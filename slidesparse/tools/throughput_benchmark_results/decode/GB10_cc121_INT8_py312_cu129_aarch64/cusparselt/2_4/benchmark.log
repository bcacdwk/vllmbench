
========== M=16 ==========
Time: 2026-01-25 18:38:46
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4
Params: prompt_len=16, output_len=256, num_prompts=16, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4 --dataset-name random --input-len 16 --output-len 256 --num-prompts 16 --max-num-seqs 16 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_4/json/Llama3.2-1B-INT8_M16.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 18:38:49 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 18:38:49 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=287800) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=287800) 
(EngineCore_DP0 pid=287800) 
(EngineCore_DP0 pid=287800) ================================================================
(EngineCore_DP0 pid=287800) Internal Triton PTX codegen error
(EngineCore_DP0 pid=287800) `ptxas` stderr:
(EngineCore_DP0 pid=287800) ptxas fatal   : Value 'sm_121a' is not defined for option 'gpu-name'
(EngineCore_DP0 pid=287800) 
(EngineCore_DP0 pid=287800) Repro command: /usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas -lineinfo -v --gpu-name=sm_121a /tmp/tmph7eaa1au.ptx -o /tmp/tmph7eaa1au.ptx.o
(EngineCore_DP0 pid=287800) 
(EngineCore_DP0 pid=287800) 
(EngineCore_DP0 pid=287800) //
(EngineCore_DP0 pid=287800) // Generated by LLVM NVPTX Back-End
(EngineCore_DP0 pid=287800) //
(EngineCore_DP0 pid=287800) 
(EngineCore_DP0 pid=287800) .version 8.7
(EngineCore_DP0 pid=287800) .target sm_121a
(EngineCore_DP0 pid=287800) .address_size 64
(EngineCore_DP0 pid=287800) 
(EngineCore_DP0 pid=287800) 	// .globl	_quant_slide_int8_kernel // -- Begin function _quant_slide_int8_kernel
(EngineCore_DP0 pid=287800) .extern .shared .align 16 .b8 global_smem[];
(EngineCore_DP0 pid=287800)                                         // @_quant_slide_int8_kernel
(EngineCore_DP0 pid=287800) .visible .entry _quant_slide_int8_kernel(
(EngineCore_DP0 pid=287800) 	.param .u64 .ptr .global .align 1 _quant_slide_int8_kernel_param_0,
(EngineCore_DP0 pid=287800) 	.param .u64 .ptr .global .align 1 _quant_slide_int8_kernel_param_1,
(EngineCore_DP0 pid=287800) 	.param .u64 .ptr .global .align 1 _quant_slide_int8_kernel_param_2,
(EngineCore_DP0 pid=287800) 	.param .u32 _quant_slide_int8_kernel_param_3,
(EngineCore_DP0 pid=287800) 	.param .u32 _quant_slide_int8_kernel_param_4,
(EngineCore_DP0 pid=287800) 	.param .u32 _quant_slide_int8_kernel_param_5,
(EngineCore_DP0 pid=287800) 	.param .u32 _quant_slide_int8_kernel_param_6,
(EngineCore_DP0 pid=287800) 	.param .u32 _quant_slide_int8_kernel_param_7,
(EngineCore_DP0 pid=287800) 	.param .u32 _quant_slide_int8_kernel_param_8,
(EngineCore_DP0 pid=287800) 	.param .u32 _quant_slide_int8_kernel_param_9,
(EngineCore_DP0 pid=287800) 	.param .u64 .ptr .global .align 1 _quant_slide_int8_kernel_param_10,
(EngineCore_DP0 pid=287800) 	.param .u64 .ptr .global .align 1 _quant_slide_int8_kernel_param_11
(EngineCore_DP0 pid=287800) )
(EngineCore_DP0 pid=287800) .reqntid 1024
(EngineCore_DP0 pid=287800) {
(EngineCore_DP0 pid=287800) 	.reg .pred 	%p<20>;
(EngineCore_DP0 pid=287800) 	.reg .b16 	%rs<20>;
(EngineCore_DP0 pid=287800) 	.reg .b32 	%r<113>;
(EngineCore_DP0 pid=287800) 	.reg .b64 	%rd<13>;
(EngineCore_DP0 pid=287800) 	.loc	1 278 0                         // quant_slide_tuned_Llama3.2-1B.py:278:0
(EngineCore_DP0 pid=287800) $L__func_begin0:
(EngineCore_DP0 pid=287800) 	.loc	1 278 0                         // quant_slide_tuned_Llama3.2-1B.py:278:0
(EngineCore_DP0 pid=287800) 
(EngineCore_DP0 pid=287800) // %bb.0:
(EngineCore_DP0 pid=287800) 	ld.param.b32 	%r20, [_quant_slide_int8_kernel_param_7];
(EngineCore_DP0 pid=287800) 	ld.param.b32 	%r19, [_quant_slide_int8_kernel_param_5];
(EngineCore_DP0 pid=287800) 	ld.param.b32 	%r18, [_quant_slide_int8_kernel_param_4];
(EngineCore_DP0 pid=287800) 	ld.param.b64 	%rd3, [_quant_slide_int8_kernel_param_2];
(EngineCore_DP0 pid=287800) 	ld.param.b64 	%rd4, [_quant_slide_int8_kernel_param_0];
(EngineCore_DP0 pid=287800) $L__tmp0:
(EngineCore_DP0 pid=287800) 	.loc	1 288 24                        // quant_slide_tuned_Llama3.2-1B.py:288:24
(EngineCore_DP0 pid=287800) 	mov.u32 	%r1, %ctaid.x;
(EngineCore_DP0 pid=287800) 	ld.param.b32 	%r22, [_quant_slide_int8_kernel_param_8];
(EngineCore_DP0 pid=287800) 	.loc	1 293 26                        // quant_slide_tuned_Llama3.2-1B.py:293:26
(EngineCore_DP0 pid=287800) 	mul.lo.s32 	%r23, %r22, %r1;
(EngineCore_DP0 pid=287800) 	.loc	1 293 20                        // quant_slide_tuned_Llama3.2-1B.py:293:20
(EngineCore_DP0 pid=287800) 	mad.wide.s32 	%rd1, %r23, 2, %rd4;
(EngineCore_DP0 pid=287800) 	.loc	1 299 32                        // quant_slide_tuned_Llama3.2-1B.py:299:32
(EngineCore_DP0 pid=287800) 	mov.u32 	%r2, %tid.x;
(EngineCore_DP0 pid=287800) 	shl.b32 	%r111, %r2, 2;
(EngineCore_DP0 pid=287800) 	.loc	1 298 35                        // quant_slide_tuned_Llama3.2-1B.py:298:35
(EngineCore_DP0 pid=287800) 	setp.lt.s32 	%p1, %r19, 1;
(EngineCore_DP0 pid=287800) 	mov.b32 	%r110, 0f2B8CBCCC;
(EngineCore_DP0 pid=287800) 	setp.eq.b32 	%p19, %r2, 0;
(EngineCore_DP0 pid=287800) 	@%p1 bra 	$L__BB0_4;
(EngineCore_DP0 pid=287800) // %bb.1:                               // %.lr.ph
(EngineCore_DP0 pid=287800) 	.loc	1 299 32                        // quant_slide_tuned_Llama3.2-1B.py:299:32
(EngineCore_DP0 pid=287800) 	and.b32 	%r4, %r2, 31;
(EngineCore_DP0 pid=287800) 	shr.u32 	%r32, %r2, 3;
(EngineCore_DP0 pid=287800) 	and.b32 	%r33, %r32, 124;
(EngineCore_DP0 pid=287800) 	mov.b32 	%r34, global_smem;
(EngineCore_DP0 pid=287800) 	add.s32 	%r40, %r34, %r33;
(EngineCore_DP0 pid=287800) 	shl.b32 	%r35, %r2, 2;
(EngineCore_DP0 pid=287800) 	add.s32 	%r43, %r34, %r35;
(EngineCore_DP0 pid=287800) 	mov.b32 	%r38, 0;
(EngineCore_DP0 pid=287800) 	mov.b32 	%r108, 0f00000000;
(EngineCore_DP0 pid=287800) 	setp.lt.u32 	%p4, %r2, 32;
(EngineCore_DP0 pid=287800) 	setp.eq.b32 	%p3, %r4, 0;
(EngineCore_DP0 pid=287800) 	mov.b32 	%r109, %r38;
(EngineCore_DP0 pid=287800) $L__BB0_2:                              // =>This Inner Loop Header: Depth=1
(EngineCore_DP0 pid=287800) 	.loc	1 300 22                        // quant_slide_tuned_Llama3.2-1B.py:300:22
(EngineCore_DP0 pid=287800) 	add.s32 	%r46, %r111, %r109;
(EngineCore_DP0 pid=287800) 	setp.lt.s32 	%p2, %r46, %r18;
(EngineCore_DP0 pid=287800) 	.loc	1 301 29                        // quant_slide_tuned_Llama3.2-1B.py:301:29
(EngineCore_DP0 pid=287800) 	mad.wide.s32 	%rd6, %r46, 2, %rd1;
(EngineCore_DP0 pid=287800) 	.loc	1 301 21                        // quant_slide_tuned_Llama3.2-1B.py:301:21
(EngineCore_DP0 pid=287800) 	// begin inline asm
(EngineCore_DP0 pid=287800) 	mov.u32 %r36, %r38;
(EngineCore_DP0 pid=287800) 	mov.u32 %r37, %r38;
(EngineCore_DP0 pid=287800) 	@%p2 ld.global.v2.b32 { %r36, %r37 }, [ %rd6 + 0 ];
(EngineCore_DP0 pid=287800) 	// end inline asm
(EngineCore_DP0 pid=287800) 	mov.b32 	{%rs1, %rs2}, %r36;
(EngineCore_DP0 pid=287800) 	mov.b32 	{%rs3, %rs4}, %r37;
(EngineCore_DP0 pid=287800) 	.loc	1 302 50                        // quant_slide_tuned_Llama3.2-1B.py:302:50
(EngineCore_DP0 pid=287800) 	abs.bf16 	%rs5, %rs1;
(EngineCore_DP0 pid=287800) 	abs.bf16 	%rs6, %rs2;
(EngineCore_DP0 pid=287800) 	abs.bf16 	%rs7, %rs3;
(EngineCore_DP0 pid=287800) 	abs.bf16 	%rs8, %rs4;
(EngineCore_DP0 pid=287800) $L__tmp1:
(EngineCore_DP0 pid=287800) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=287800) 	bar.sync 	0;
(EngineCore_DP0 pid=287800) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=287800) 	max.bf16 	%rs9, %rs5, %rs6;
(EngineCore_DP0 pid=287800) 	max.bf16 	%rs10, %rs9, %rs7;
(EngineCore_DP0 pid=287800) 	max.bf16 	%rs11, %rs10, %rs8;
(EngineCore_DP0 pid=287800) 	cvt.f32.bf16 	%r47, %rs11;
(EngineCore_DP0 pid=287800) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=287800) 	shfl.sync.bfly.b32 	%r48, %r47, 16, 31, -1;
(EngineCore_DP0 pid=287800) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=287800) 	max.f32 	%r49, %r47, %r48;
(EngineCore_DP0 pid=287800) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=287800) 	shfl.sync.bfly.b32 	%r50, %r49, 8, 31, -1;
(EngineCore_DP0 pid=287800) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=287800) 	max.f32 	%r51, %r49, %r50;
(EngineCore_DP0 pid=287800) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=287800) 	shfl.sync.bfly.b32 	%r52, %r51, 4, 31, -1;
(EngineCore_DP0 pid=287800) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=287800) 	max.f32 	%r53, %r51, %r52;
(EngineCore_DP0 pid=287800) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=287800) 	shfl.sync.bfly.b32 	%r54, %r53, 2, 31, -1;
(EngineCore_DP0 pid=287800) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=287800) 	max.f32 	%r55, %r53, %r54;
(EngineCore_DP0 pid=287800) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=287800) 	shfl.sync.bfly.b32 	%r56, %r55, 1, 31, -1;
(EngineCore_DP0 pid=287800) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=287800) 	max.f32 	%r41, %r55, %r56;
(EngineCore_DP0 pid=287800) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=287800) 	// begin inline asm
(EngineCore_DP0 pid=287800) 	@%p3 st.shared.b32 [ %r40 + 0 ], %r41;
(EngineCore_DP0 pid=287800) 	// end inline asm
(EngineCore_DP0 pid=287800) 	bar.sync 	0;
(EngineCore_DP0 pid=287800) 	// begin inline asm
(EngineCore_DP0 pid=287800) 	@%p4 ld.shared.b32 %r42, [ %r43 + 0 ];
(EngineCore_DP0 pid=287800) 	// end inline asm
(EngineCore_DP0 pid=287800) 	shfl.sync.bfly.b32 	%r57, %r42, 16, 31, -1;
(EngineCore_DP0 pid=287800) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=287800) 	max.f32 	%r58, %r42, %r57;
(EngineCore_DP0 pid=287800) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=287800) 	shfl.sync.bfly.b32 	%r59, %r58, 8, 31, -1;
(EngineCore_DP0 pid=287800) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=287800) 	max.f32 	%r60, %r58, %r59;
(EngineCore_DP0 pid=287800) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=287800) 	shfl.sync.bfly.b32 	%r61, %r60, 4, 31, -1;
(EngineCore_DP0 pid=287800) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=287800) 	max.f32 	%r62, %r60, %r61;
(EngineCore_DP0 pid=287800) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=287800) 	shfl.sync.bfly.b32 	%r63, %r62, 2, 31, -1;
(EngineCore_DP0 pid=287800) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=287800) 	max.f32 	%r64, %r62, %r63;
(EngineCore_DP0 pid=287800) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=287800) 	shfl.sync.bfly.b32 	%r65, %r64, 1, 31, -1;
(EngineCore_DP0 pid=287800) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=287800) 	max.f32 	%r45, %r64, %r65;
(EngineCore_DP0 pid=287800) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=287800) 	// begin inline asm
(EngineCore_DP0 pid=287800) 	@%p19 st.shared.b32 [ %r43 + 0 ], %r45;
(EngineCore_DP0 pid=287800) 	// end inline asm
(EngineCore_DP0 pid=287800) 	bar.sync 	0;
(EngineCore_DP0 pid=287800) 	ld.shared.b32 	%r66, [global_smem];
(EngineCore_DP0 pid=287800) $L__tmp2:
(EngineCore_DP0 pid=287800) 	.loc	1 302 36                        // quant_slide_tuned_Llama3.2-1B.py:302:36
(EngineCore_DP0 pid=287800) 	max.f32 	%r108, %r108, %r66;
(EngineCore_DP0 pid=287800) 	.loc	1 298 35                        // quant_slide_tuned_Llama3.2-1B.py:298:35
(EngineCore_DP0 pid=287800) 	add.s32 	%r109, %r109, 4096;
(EngineCore_DP0 pid=287800) 	setp.lt.s32 	%p6, %r109, %r19;
(EngineCore_DP0 pid=287800) 	@%p6 bra 	$L__BB0_2;
(EngineCore_DP0 pid=287800) // %bb.3:                               // %._crit_edge.loopexit
(EngineCore_DP0 pid=287800) 	.loc	1 304 32                        // quant_slide_tuned_Llama3.2-1B.py:304:32
(EngineCore_DP0 pid=287800) 	max.f32 	%r110, %r108, 0f2B8CBCCC;
(EngineCore_DP0 pid=287800) $L__BB0_4:                              // %._crit_edge
(EngineCore_DP0 pid=287800) 	.loc	1 0 32                          // quant_slide_tuned_Llama3.2-1B.py:0:32
(EngineCore_DP0 pid=287800) 	mov.b32 	%r68, 0f42FE0000;
(EngineCore_DP0 pid=287800) 	.loc	1 305 32                        // quant_slide_tuned_Llama3.2-1B.py:305:32
(EngineCore_DP0 pid=287800) 	div.full.f32 	%r69, %r110, %r68;
(EngineCore_DP0 pid=287800) 	.loc	1 305 42                        // quant_slide_tuned_Llama3.2-1B.py:305:42
(EngineCore_DP0 pid=287800) 	max.f32 	%r67, %r69, 0f37810204;
(EngineCore_DP0 pid=287800) 	.loc	1 307 25                        // quant_slide_tuned_Llama3.2-1B.py:307:25
(EngineCore_DP0 pid=287800) 	mad.wide.u32 	%rd7, %r1, 4, %rd3;
(EngineCore_DP0 pid=287800) 	.loc	1 307 30                        // quant_slide_tuned_Llama3.2-1B.py:307:30
(EngineCore_DP0 pid=287800) 	// begin inline asm
(EngineCore_DP0 pid=287800) 	@%p19 st.global.b32 [ %rd7 + 0 ], { %r67 };
(EngineCore_DP0 pid=287800) 	// end inline asm
(EngineCore_DP0 pid=287800) 	.loc	1 312 41                        // quant_slide_tuned_Llama3.2-1B.py:312:41
(EngineCore_DP0 pid=287800) 	setp.lt.s32 	%p8, %r20, 1;
(EngineCore_DP0 pid=287800) 	@%p8 bra 	$L__BB0_7;
(EngineCore_DP0 pid=287800) // %bb.5:                               // %.lr.ph4.preheader
(EngineCore_DP0 pid=287800) 	.loc	1 0 41                          // quant_slide_tuned_Llama3.2-1B.py:0:41
(EngineCore_DP0 pid=287800) 	ld.param.b32 	%r24, [_quant_slide_int8_kernel_param_9];
(EngineCore_DP0 pid=287800) 	shr.s32 	%r25, %r24, 31;
(EngineCore_DP0 pid=287800) 	shr.u32 	%r26, %r25, 30;
(EngineCore_DP0 pid=287800) 	add.s32 	%r27, %r24, %r26;
(EngineCore_DP0 pid=287800) 	shr.s32 	%r28, %r27, 2;
(EngineCore_DP0 pid=287800) 	ld.param.b64 	%rd5, [_quant_slide_int8_kernel_param_1];
(EngineCore_DP0 pid=287800) 	mul.lo.s32 	%r29, %r28, %r1;
(EngineCore_DP0 pid=287800) 	mad.wide.s32 	%rd2, %r29, 4, %rd5;
(EngineCore_DP0 pid=287800) 	div.full.f32 	%r13, %r68, %r110;
(EngineCore_DP0 pid=287800) 	mov.b32 	%r112, 0;
(EngineCore_DP0 pid=287800) $L__BB0_6:                              // %.lr.ph4
(EngineCore_DP0 pid=287800)                                         // =>This Inner Loop Header: Depth=1
(EngineCore_DP0 pid=287800) 	.loc	1 314 30                        // quant_slide_tuned_Llama3.2-1B.py:314:30
(EngineCore_DP0 pid=287800) 	add.s32 	%r72, %r2, %r112;
(EngineCore_DP0 pid=287800) 	setp.lt.s32 	%p13, %r72, %r20;
(EngineCore_DP0 pid=287800) 	.loc	1 322 53                        // quant_slide_tuned_Llama3.2-1B.py:322:53
(EngineCore_DP0 pid=287800) 	setp.lt.s32 	%p14, %r111, %r18;
(EngineCore_DP0 pid=287800) 	.loc	1 322 37                        // quant_slide_tuned_Llama3.2-1B.py:322:37
(EngineCore_DP0 pid=287800) 	and.pred 	%p9, %p13, %p14;
(EngineCore_DP0 pid=287800) 	.loc	1 321 29                        // quant_slide_tuned_Llama3.2-1B.py:321:29
(EngineCore_DP0 pid=287800) 	mad.wide.s32 	%rd8, %r111, 2, %rd1;
(EngineCore_DP0 pid=287800) 	mov.b16 	%rs13, 0;
(EngineCore_DP0 pid=287800) 	.loc	1 321 21                        // quant_slide_tuned_Llama3.2-1B.py:321:21
(EngineCore_DP0 pid=287800) 	// begin inline asm
(EngineCore_DP0 pid=287800) 	mov.u16 %rs12, %rs13;
(EngineCore_DP0 pid=287800) 	@%p9 ld.global.b16 { %rs12 }, [ %rd8 + 0 ];
(EngineCore_DP0 pid=287800) 	// end inline asm
(EngineCore_DP0 pid=287800) 	.loc	1 322 79                        // quant_slide_tuned_Llama3.2-1B.py:322:79
(EngineCore_DP0 pid=287800) 	cvt.f32.bf16 	%r73, %rs12;
(EngineCore_DP0 pid=287800) 	.loc	1 324 53                        // quant_slide_tuned_Llama3.2-1B.py:324:53
(EngineCore_DP0 pid=287800) 	add.s32 	%r74, %r111, 1;
(EngineCore_DP0 pid=287800) 	setp.lt.s32 	%p15, %r74, %r18;
(EngineCore_DP0 pid=287800) 	.loc	1 324 37                        // quant_slide_tuned_Llama3.2-1B.py:324:37
(EngineCore_DP0 pid=287800) 	and.pred 	%p10, %p13, %p15;
(EngineCore_DP0 pid=287800) 	.loc	1 323 39                        // quant_slide_tuned_Llama3.2-1B.py:323:39
(EngineCore_DP0 pid=287800) 	add.s64 	%rd9, %rd8, 2;
(EngineCore_DP0 pid=287800) 	.loc	1 323 21                        // quant_slide_tuned_Llama3.2-1B.py:323:21
(EngineCore_DP0 pid=287800) 	// begin inline asm
(EngineCore_DP0 pid=287800) 	mov.u16 %rs14, %rs13;
(EngineCore_DP0 pid=287800) 	@%p10 ld.global.b16 { %rs14 }, [ %rd9 + 0 ];
(EngineCore_DP0 pid=287800) 	// end inline asm
(EngineCore_DP0 pid=287800) 	.loc	1 324 79                        // quant_slide_tuned_Llama3.2-1B.py:324:79
(EngineCore_DP0 pid=287800) 	cvt.f32.bf16 	%r75, %rs14;
(EngineCore_DP0 pid=287800) 	.loc	1 326 53                        // quant_slide_tuned_Llama3.2-1B.py:326:53
(EngineCore_DP0 pid=287800) 	add.s32 	%r76, %r111, 2;
(EngineCore_DP0 pid=287800) 	setp.lt.s32 	%p16, %r76, %r18;
(EngineCore_DP0 pid=287800) 	.loc	1 326 37                        // quant_slide_tuned_Llama3.2-1B.py:326:37
(EngineCore_DP0 pid=287800) 	and.pred 	%p11, %p13, %p16;
(EngineCore_DP0 pid=287800) 	.loc	1 325 39                        // quant_slide_tuned_Llama3.2-1B.py:325:39
(EngineCore_DP0 pid=287800) 	add.s64 	%rd10, %rd8, 4;
(EngineCore_DP0 pid=287800) 	.loc	1 325 21                        // quant_slide_tuned_Llama3.2-1B.py:325:21
(EngineCore_DP0 pid=287800) 	// begin inline asm
(EngineCore_DP0 pid=287800) 	mov.u16 %rs16, %rs13;
(EngineCore_DP0 pid=287800) 	@%p11 ld.global.b16 { %rs16 }, [ %rd10 + 0 ];
(EngineCore_DP0 pid=287800) 	// end inline asm
(EngineCore_DP0 pid=287800) 	.loc	1 326 79                        // quant_slide_tuned_Llama3.2-1B.py:326:79
(EngineCore_DP0 pid=287800) 	cvt.f32.bf16 	%r77, %rs16;
(EngineCore_DP0 pid=287800) 	.loc	1 328 53                        // quant_slide_tuned_Llama3.2-1B.py:328:53
(EngineCore_DP0 pid=287800) 	add.s32 	%r78, %r111, 3;
(EngineCore_DP0 pid=287800) 	setp.lt.s32 	%p17, %r78, %r18;
(EngineCore_DP0 pid=287800) 	.loc	1 328 37                        // quant_slide_tuned_Llama3.2-1B.py:328:37
(EngineCore_DP0 pid=287800) 	and.pred 	%p12, %p13, %p17;
(EngineCore_DP0 pid=287800) 	.loc	1 327 39                        // quant_slide_tuned_Llama3.2-1B.py:327:39
(EngineCore_DP0 pid=287800) 	add.s64 	%rd11, %rd8, 6;
(EngineCore_DP0 pid=287800) 	.loc	1 327 21                        // quant_slide_tuned_Llama3.2-1B.py:327:21
(EngineCore_DP0 pid=287800) 	// begin inline asm
(EngineCore_DP0 pid=287800) 	mov.u16 %rs18, %rs13;
(EngineCore_DP0 pid=287800) 	@%p12 ld.global.b16 { %rs18 }, [ %rd11 + 0 ];
(EngineCore_DP0 pid=287800) 	// end inline asm
(EngineCore_DP0 pid=287800) 	.loc	1 328 79                        // quant_slide_tuned_Llama3.2-1B.py:328:79
(EngineCore_DP0 pid=287800) 	cvt.f32.bf16 	%r79, %rs18;
(EngineCore_DP0 pid=287800) 	.loc	1 330 56                        // quant_slide_tuned_Llama3.2-1B.py:330:56
(EngineCore_DP0 pid=287800) 	mul.f32 	%r80, %r13, %r73;
(EngineCore_DP0 pid=287800) 	.loc	1 330 51                        // quant_slide_tuned_Llama3.2-1B.py:330:51
(EngineCore_DP0 pid=287800) 	cvt.rni.f32.f32 	%r81, %r80;
(EngineCore_DP0 pid=287800) 	.loc	1 330 76                        // quant_slide_tuned_Llama3.2-1B.py:330:76
(EngineCore_DP0 pid=287800) 	max.f32 	%r82, %r81, 0fC3000000;
(EngineCore_DP0 pid=287800) 	min.f32 	%r83, %r82, 0f42FE0000;
(EngineCore_DP0 pid=287800) 	.loc	1 330 86                        // quant_slide_tuned_Llama3.2-1B.py:330:86
(EngineCore_DP0 pid=287800) 	cvt.rzi.s32.f32 	%r84, %r83;
(EngineCore_DP0 pid=287800) 	.loc	1 330 98                        // quant_slide_tuned_Llama3.2-1B.py:330:98
(EngineCore_DP0 pid=287800) 	and.b32 	%r85, %r84, 255;
(EngineCore_DP0 pid=287800) 	.loc	1 331 56                        // quant_slide_tuned_Llama3.2-1B.py:331:56
(EngineCore_DP0 pid=287800) 	mul.f32 	%r86, %r13, %r75;
(EngineCore_DP0 pid=287800) 	.loc	1 331 51                        // quant_slide_tuned_Llama3.2-1B.py:331:51
(EngineCore_DP0 pid=287800) 	cvt.rni.f32.f32 	%r87, %r86;
(EngineCore_DP0 pid=287800) 	.loc	1 332 56                        // quant_slide_tuned_Llama3.2-1B.py:332:56
(EngineCore_DP0 pid=287800) 	mul.f32 	%r88, %r13, %r77;
(EngineCore_DP0 pid=287800) 	.loc	1 332 51                        // quant_slide_tuned_Llama3.2-1B.py:332:51
(EngineCore_DP0 pid=287800) 	cvt.rni.f32.f32 	%r89, %r88;
(EngineCore_DP0 pid=287800) 	.loc	1 333 56                        // quant_slide_tuned_Llama3.2-1B.py:333:56
(EngineCore_DP0 pid=287800) 	mul.f32 	%r90, %r13, %r79;
(EngineCore_DP0 pid=287800) 	.loc	1 333 51                        // quant_slide_tuned_Llama3.2-1B.py:333:51
(EngineCore_DP0 pid=287800) 	cvt.rni.f32.f32 	%r91, %r90;
(EngineCore_DP0 pid=287800) 	.loc	1 333 76                        // quant_slide_tuned_Llama3.2-1B.py:333:76
(EngineCore_DP0 pid=287800) 	max.f32 	%r92, %r91, 0fC3000000;
(EngineCore_DP0 pid=287800) 	min.f32 	%r93, %r92, 0f42FE0000;
(EngineCore_DP0 pid=287800) 	.loc	1 333 86                        // quant_slide_tuned_Llama3.2-1B.py:333:86
(EngineCore_DP0 pid=287800) 	cvt.rzi.s32.f32 	%r94, %r93;
(EngineCore_DP0 pid=287800) 	.loc	1 331 76                        // quant_slide_tuned_Llama3.2-1B.py:331:76
(EngineCore_DP0 pid=287800) 	max.f32 	%r95, %r89, 0fC3000000;
(EngineCore_DP0 pid=287800) 	max.f32 	%r96, %r87, 0fC3000000;
(EngineCore_DP0 pid=287800) 	min.f32 	%r97, %r96, 0f42FE0000;
(EngineCore_DP0 pid=287800) 	min.f32 	%r98, %r95, 0f42FE0000;
(EngineCore_DP0 pid=287800) 	.loc	1 331 86                        // quant_slide_tuned_Llama3.2-1B.py:331:86
(EngineCore_DP0 pid=287800) 	cvt.rzi.s32.f32 	%r99, %r98;
(EngineCore_DP0 pid=287800) 	cvt.rzi.s32.f32 	%r100, %r97;
(EngineCore_DP0 pid=287800) 	.loc	1 335 30                        // quant_slide_tuned_Llama3.2-1B.py:335:30
(EngineCore_DP0 pid=287800) 	shl.b32 	%r101, %r100, 8;
(EngineCore_DP0 pid=287800) 	shl.b32 	%r102, %r99, 16;
(EngineCore_DP0 pid=287800) 	and.b32 	%r103, %r102, 16711680;
(EngineCore_DP0 pid=287800) 	and.b32 	%r104, %r101, 65280;
(EngineCore_DP0 pid=287800) 	.loc	1 335 24                        // quant_slide_tuned_Llama3.2-1B.py:335:24
(EngineCore_DP0 pid=287800) 	or.b32 	%r105, %r104, %r85;
(EngineCore_DP0 pid=287800) 	.loc	1 335 36                        // quant_slide_tuned_Llama3.2-1B.py:335:36
(EngineCore_DP0 pid=287800) 	or.b32 	%r106, %r105, %r103;
(EngineCore_DP0 pid=287800) 	.loc	1 335 55                        // quant_slide_tuned_Llama3.2-1B.py:335:55
(EngineCore_DP0 pid=287800) 	shl.b32 	%r107, %r94, 24;
(EngineCore_DP0 pid=287800) 	.loc	1 335 49                        // quant_slide_tuned_Llama3.2-1B.py:335:49
(EngineCore_DP0 pid=287800) 	or.b32 	%r71, %r106, %r107;
(EngineCore_DP0 pid=287800) 	.loc	1 336 29                        // quant_slide_tuned_Llama3.2-1B.py:336:29
(EngineCore_DP0 pid=287800) 	mad.wide.s32 	%rd12, %r72, 4, %rd2;
(EngineCore_DP0 pid=287800) 	.loc	1 336 39                        // quant_slide_tuned_Llama3.2-1B.py:336:39
(EngineCore_DP0 pid=287800) 	// begin inline asm
(EngineCore_DP0 pid=287800) 	@%p13 st.global.b32 [ %rd12 + 0 ], { %r71 };
(EngineCore_DP0 pid=287800) 	// end inline asm
(EngineCore_DP0 pid=287800) 	.loc	1 312 41                        // quant_slide_tuned_Llama3.2-1B.py:312:41
(EngineCore_DP0 pid=287800) 	add.s32 	%r112, %r112, 1024;
(EngineCore_DP0 pid=287800) 	add.s32 	%r111, %r111, 4096;
(EngineCore_DP0 pid=287800) 	setp.lt.s32 	%p18, %r112, %r20;
(EngineCore_DP0 pid=287800) 	@%p18 bra 	$L__BB0_6;
(EngineCore_DP0 pid=287800) $L__BB0_7:                              // %._crit_edge5
(EngineCore_DP0 pid=287800) 	.loc	1 312 4                         // quant_slide_tuned_Llama3.2-1B.py:312:4
(EngineCore_DP0 pid=287800) 	ret;
(EngineCore_DP0 pid=287800) $L__tmp3:
(EngineCore_DP0 pid=287800) $L__func_end0:
(EngineCore_DP0 pid=287800)                                         // -- End function
(EngineCore_DP0 pid=287800) }
(EngineCore_DP0 pid=287800) 	.file	1 "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/GB10_cc121_py312_cu129_aarch64/quant_slide_tuned_Llama3.2-1B.py"
(EngineCore_DP0 pid=287800) 	.file	2 "/usr/local/lib/python3.12/dist-packages/triton/language/standard.py"
(EngineCore_DP0 pid=287800) 	.section	.debug_abbrev
(EngineCore_DP0 pid=287800) 	{
(EngineCore_DP0 pid=287800) .b8 1                                   // Abbreviation Code
(EngineCore_DP0 pid=287800) .b8 17                                  // DW_TAG_compile_unit
(EngineCore_DP0 pid=287800) .b8 1                                   // DW_CHILDREN_yes
(EngineCore_DP0 pid=287800) .b8 37                                  // DW_AT_producer
(EngineCore_DP0 pid=287800) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=287800) .b8 19                                  // DW_AT_language
(EngineCore_DP0 pid=287800) .b8 5                                   // DW_FORM_data2
(EngineCore_DP0 pid=287800) .b8 3                                   // DW_AT_name
(EngineCore_DP0 pid=287800) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=287800) .b8 16                                  // DW_AT_stmt_list
(EngineCore_DP0 pid=287800) .b8 6                                   // DW_FORM_data4
(EngineCore_DP0 pid=287800) .b8 27                                  // DW_AT_comp_dir
(EngineCore_DP0 pid=287800) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=287800) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=287800) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=287800) .b8 2                                   // Abbreviation Code
(EngineCore_DP0 pid=287800) .b8 46                                  // DW_TAG_subprogram
(EngineCore_DP0 pid=287800) .b8 0                                   // DW_CHILDREN_no
(EngineCore_DP0 pid=287800) .b8 3                                   // DW_AT_name
(EngineCore_DP0 pid=287800) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=287800) .b8 32                                  // DW_AT_inline
(EngineCore_DP0 pid=287800) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=287800) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=287800) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=287800) .b8 3                                   // Abbreviation Code
(EngineCore_DP0 pid=287800) .b8 46                                  // DW_TAG_subprogram
(EngineCore_DP0 pid=287800) .b8 1                                   // DW_CHILDREN_yes
(EngineCore_DP0 pid=287800) .b8 17                                  // DW_AT_low_pc
(EngineCore_DP0 pid=287800) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=287800) .b8 18                                  // DW_AT_high_pc
(EngineCore_DP0 pid=287800) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=287800) .b8 49                                  // DW_AT_abstract_origin
(EngineCore_DP0 pid=287800) .b8 19                                  // DW_FORM_ref4
(EngineCore_DP0 pid=287800) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=287800) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=287800) .b8 4                                   // Abbreviation Code
(EngineCore_DP0 pid=287800) .b8 29                                  // DW_TAG_inlined_subroutine
(EngineCore_DP0 pid=287800) .b8 0                                   // DW_CHILDREN_no
(EngineCore_DP0 pid=287800) .b8 49                                  // DW_AT_abstract_origin
(EngineCore_DP0 pid=287800) .b8 19                                  // DW_FORM_ref4
(EngineCore_DP0 pid=287800) .b8 17                                  // DW_AT_low_pc
(EngineCore_DP0 pid=287800) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=287800) .b8 18                                  // DW_AT_high_pc
(EngineCore_DP0 pid=287800) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=287800) .b8 88                                  // DW_AT_call_file
(EngineCore_DP0 pid=287800) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=287800) .b8 89                                  // DW_AT_call_line
(EngineCore_DP0 pid=287800) .b8 5                                   // DW_FORM_data2
(EngineCore_DP0 pid=287800) .b8 87                                  // DW_AT_call_column
(EngineCore_DP0 pid=287800) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=287800) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=287800) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=287800) .b8 0                                   // EOM(3)
(EngineCore_DP0 pid=287800) 	}
(EngineCore_DP0 pid=287800) 	.section	.debug_info
(EngineCore_DP0 pid=287800) 	{
(EngineCore_DP0 pid=287800) .b32 224                                // Length of Unit
(EngineCore_DP0 pid=287800) .b8 2                                   // DWARF version number
(EngineCore_DP0 pid=287800) .b8 0
(EngineCore_DP0 pid=287800) .b32 .debug_abbrev                      // Offset Into Abbrev. Section
(EngineCore_DP0 pid=287800) .b8 8                                   // Address Size (in bytes)
(EngineCore_DP0 pid=287800) .b8 1                                   // Abbrev [1] 0xb:0xd9 DW_TAG_compile_unit
(EngineCore_DP0 pid=287800) .b8 116                                 // DW_AT_producer
(EngineCore_DP0 pid=287800) .b8 114
(EngineCore_DP0 pid=287800) .b8 105
(EngineCore_DP0 pid=287800) .b8 116
(EngineCore_DP0 pid=287800) .b8 111
(EngineCore_DP0 pid=287800) .b8 110
(EngineCore_DP0 pid=287800) .b8 0
(EngineCore_DP0 pid=287800) .b8 2                                   // DW_AT_language
(EngineCore_DP0 pid=287800) .b8 0
(EngineCore_DP0 pid=287800) .b8 113                                 // DW_AT_name
(EngineCore_DP0 pid=287800) .b8 117
(EngineCore_DP0 pid=287800) .b8 97
(EngineCore_DP0 pid=287800) .b8 110
(EngineCore_DP0 pid=287800) .b8 116
(EngineCore_DP0 pid=287800) .b8 95
(EngineCore_DP0 pid=287800) .b8 115
(EngineCore_DP0 pid=287800) .b8 108
(EngineCore_DP0 pid=287800) .b8 105
(EngineCore_DP0 pid=287800) .b8 100
(EngineCore_DP0 pid=287800) .b8 101
(EngineCore_DP0 pid=287800) .b8 95
(EngineCore_DP0 pid=287800) .b8 116
(EngineCore_DP0 pid=287800) .b8 117
(EngineCore_DP0 pid=287800) .b8 110
(EngineCore_DP0 pid=287800) .b8 101
(EngineCore_DP0 pid=287800) .b8 100
(EngineCore_DP0 pid=287800) .b8 95
(EngineCore_DP0 pid=287800) .b8 76
(EngineCore_DP0 pid=287800) .b8 108
(EngineCore_DP0 pid=287800) .b8 97
(EngineCore_DP0 pid=287800) .b8 109
(EngineCore_DP0 pid=287800) .b8 97
(EngineCore_DP0 pid=287800) .b8 51
(EngineCore_DP0 pid=287800) .b8 46
(EngineCore_DP0 pid=287800) .b8 50
(EngineCore_DP0 pid=287800) .b8 45
(EngineCore_DP0 pid=287800) .b8 49
(EngineCore_DP0 pid=287800) .b8 66
(EngineCore_DP0 pid=287800) .b8 46
(EngineCore_DP0 pid=287800) .b8 112
(EngineCore_DP0 pid=287800) .b8 121
(EngineCore_DP0 pid=287800) .b8 0
(EngineCore_DP0 pid=287800) .b32 .debug_line                        // DW_AT_stmt_list
(EngineCore_DP0 pid=287800) .b8 47                                  // DW_AT_comp_dir
(EngineCore_DP0 pid=287800) .b8 114
(EngineCore_DP0 pid=287800) .b8 111
(EngineCore_DP0 pid=287800) .b8 111
(EngineCore_DP0 pid=287800) .b8 116
(EngineCore_DP0 pid=287800) .b8 47
(EngineCore_DP0 pid=287800) .b8 118
(EngineCore_DP0 pid=287800) .b8 108
(EngineCore_DP0 pid=287800) .b8 108
(EngineCore_DP0 pid=287800) .b8 109
(EngineCore_DP0 pid=287800) .b8 98
(EngineCore_DP0 pid=287800) .b8 101
(EngineCore_DP0 pid=287800) .b8 110
(EngineCore_DP0 pid=287800) .b8 99
(EngineCore_DP0 pid=287800) .b8 104
(EngineCore_DP0 pid=287800) .b8 47
(EngineCore_DP0 pid=287800) .b8 115
(EngineCore_DP0 pid=287800) .b8 108
(EngineCore_DP0 pid=287800) .b8 105
(EngineCore_DP0 pid=287800) .b8 100
(EngineCore_DP0 pid=287800) .b8 101
(EngineCore_DP0 pid=287800) .b8 115
(EngineCore_DP0 pid=287800) .b8 112
(EngineCore_DP0 pid=287800) .b8 97
(EngineCore_DP0 pid=287800) .b8 114
(EngineCore_DP0 pid=287800) .b8 115
(EngineCore_DP0 pid=287800) .b8 101
(EngineCore_DP0 pid=287800) .b8 47
(EngineCore_DP0 pid=287800) .b8 99
(EngineCore_DP0 pid=287800) .b8 115
(EngineCore_DP0 pid=287800) .b8 114
(EngineCore_DP0 pid=287800) .b8 99
(EngineCore_DP0 pid=287800) .b8 47
(EngineCore_DP0 pid=287800) .b8 102
(EngineCore_DP0 pid=287800) .b8 117
(EngineCore_DP0 pid=287800) .b8 115
(EngineCore_DP0 pid=287800) .b8 101
(EngineCore_DP0 pid=287800) .b8 100
(EngineCore_DP0 pid=287800) .b8 95
(EngineCore_DP0 pid=287800) .b8 113
(EngineCore_DP0 pid=287800) .b8 117
(EngineCore_DP0 pid=287800) .b8 97
(EngineCore_DP0 pid=287800) .b8 110
(EngineCore_DP0 pid=287800) .b8 116
(EngineCore_DP0 pid=287800) .b8 95
(EngineCore_DP0 pid=287800) .b8 115
(EngineCore_DP0 pid=287800) .b8 108
(EngineCore_DP0 pid=287800) .b8 105
(EngineCore_DP0 pid=287800) .b8 100
(EngineCore_DP0 pid=287800) .b8 101
(EngineCore_DP0 pid=287800) .b8 95
(EngineCore_DP0 pid=287800) .b8 116
(EngineCore_DP0 pid=287800) .b8 114
(EngineCore_DP0 pid=287800) .b8 105
(EngineCore_DP0 pid=287800) .b8 116
(EngineCore_DP0 pid=287800) .b8 111
(EngineCore_DP0 pid=287800) .b8 110
(EngineCore_DP0 pid=287800) .b8 47
(EngineCore_DP0 pid=287800) .b8 98
(EngineCore_DP0 pid=287800) .b8 117
(EngineCore_DP0 pid=287800) .b8 105
(EngineCore_DP0 pid=287800) .b8 108
(EngineCore_DP0 pid=287800) .b8 100
(EngineCore_DP0 pid=287800) .b8 47
(EngineCore_DP0 pid=287800) .b8 71
(EngineCore_DP0 pid=287800) .b8 66
(EngineCore_DP0 pid=287800) .b8 49
(EngineCore_DP0 pid=287800) .b8 48
(EngineCore_DP0 pid=287800) .b8 95
(EngineCore_DP0 pid=287800) .b8 99
(EngineCore_DP0 pid=287800) .b8 99
(EngineCore_DP0 pid=287800) .b8 49
(EngineCore_DP0 pid=287800) .b8 50
(EngineCore_DP0 pid=287800) .b8 49
(EngineCore_DP0 pid=287800) .b8 95
(EngineCore_DP0 pid=287800) .b8 112
(EngineCore_DP0 pid=287800) .b8 121
(EngineCore_DP0 pid=287800) .b8 51
(EngineCore_DP0 pid=287800) .b8 49
(EngineCore_DP0 pid=287800) .b8 50
(EngineCore_DP0 pid=287800) .b8 95
(EngineCore_DP0 pid=287800) .b8 99
(EngineCore_DP0 pid=287800) .b8 117
(EngineCore_DP0 pid=287800) .b8 49
(EngineCore_DP0 pid=287800) .b8 50
(EngineCore_DP0 pid=287800) .b8 57
(EngineCore_DP0 pid=287800) .b8 95
(EngineCore_DP0 pid=287800) .b8 97
(EngineCore_DP0 pid=287800) .b8 97
(EngineCore_DP0 pid=287800) .b8 114
(EngineCore_DP0 pid=287800) .b8 99
(EngineCore_DP0 pid=287800) .b8 104
(EngineCore_DP0 pid=287800) .b8 54
(EngineCore_DP0 pid=287800) .b8 52
(EngineCore_DP0 pid=287800) .b8 0
(EngineCore_DP0 pid=287800) .b8 2                                   // Abbrev [2] 0x99:0x1b DW_TAG_subprogram
(EngineCore_DP0 pid=287800) .b8 95                                  // DW_AT_name
(EngineCore_DP0 pid=287800) .b8 113
(EngineCore_DP0 pid=287800) .b8 117
(EngineCore_DP0 pid=287800) .b8 97
(EngineCore_DP0 pid=287800) .b8 110
(EngineCore_DP0 pid=287800) .b8 116
(EngineCore_DP0 pid=287800) .b8 95
(EngineCore_DP0 pid=287800) .b8 115
(EngineCore_DP0 pid=287800) .b8 108
(EngineCore_DP0 pid=287800) .b8 105
(EngineCore_DP0 pid=287800) .b8 100
(EngineCore_DP0 pid=287800) .b8 101
(EngineCore_DP0 pid=287800) .b8 95
(EngineCore_DP0 pid=287800) .b8 105
(EngineCore_DP0 pid=287800) .b8 110
(EngineCore_DP0 pid=287800) .b8 116
(EngineCore_DP0 pid=287800) .b8 56
(EngineCore_DP0 pid=287800) .b8 95
(EngineCore_DP0 pid=287800) .b8 107
(EngineCore_DP0 pid=287800) .b8 101
(EngineCore_DP0 pid=287800) .b8 114
(EngineCore_DP0 pid=287800) .b8 110
(EngineCore_DP0 pid=287800) .b8 101
(EngineCore_DP0 pid=287800) .b8 108
(EngineCore_DP0 pid=287800) .b8 0
(EngineCore_DP0 pid=287800) .b8 1                                   // DW_AT_inline
(EngineCore_DP0 pid=287800) .b8 3                                   // Abbrev [3] 0xb4:0x2f DW_TAG_subprogram
(EngineCore_DP0 pid=287800) .b64 $L__func_begin0                    // DW_AT_low_pc
(EngineCore_DP0 pid=287800) .b64 $L__func_end0                      // DW_AT_high_pc
(EngineCore_DP0 pid=287800) .b32 153                                // DW_AT_abstract_origin
(EngineCore_DP0 pid=287800) .b8 4                                   // Abbrev [4] 0xc9:0x19 DW_TAG_inlined_subroutine
(EngineCore_DP0 pid=287800) .b32 153                                // DW_AT_abstract_origin
(EngineCore_DP0 pid=287800) .b64 $L__tmp1                           // DW_AT_low_pc
(EngineCore_DP0 pid=287800) .b64 $L__tmp2                           // DW_AT_high_pc
(EngineCore_DP0 pid=287800) .b8 1                                   // DW_AT_call_file
(EngineCore_DP0 pid=287800) .b8 46                                  // DW_AT_call_line
(EngineCore_DP0 pid=287800) .b8 1
(EngineCore_DP0 pid=287800) .b8 43                                  // DW_AT_call_column
(EngineCore_DP0 pid=287800) .b8 0                                   // End Of Children Mark
(EngineCore_DP0 pid=287800) .b8 0                                   // End Of Children Mark
(EngineCore_DP0 pid=287800) 	}
(EngineCore_DP0 pid=287800) 	.section	.debug_macinfo	{	}
(EngineCore_DP0 pid=287800) 
(EngineCore_DP0 pid=287800) ================================================================
(EngineCore_DP0 pid=287800) please share the reproducer above with Triton project.
(EngineCore_DP0 pid=287800) 
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 468, in make_cubin
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]     subprocess.run(ptxas_cmd, check=True, close_fds=False, stderr=flog)
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]   File "/usr/lib/python3.12/subprocess.py", line 571, in run
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]     raise CalledProcessError(retcode, process.args,
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866] subprocess.CalledProcessError: Command '['/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas', '-lineinfo', '-v', '--gpu-name=sm_121a', '/tmp/tmph7eaa1au.ptx', '-o', '/tmp/tmph7eaa1au.ptx.o']' returned non-zero exit status 255.
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866] 
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866] During handling of the above exception, another exception occurred:
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866] 
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]     self.model_runner.profile_run()
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 623, in forward
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]     model_output = self.model(
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]                    ^^^^^^^^^^^
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 372, in __call__
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]     return self.forward(*args, **kwargs)
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 436, in forward
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]     hidden_states, residual = layer(positions, hidden_states, residual)
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 342, in forward
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]     hidden_states = self.self_attn(positions=positions, hidden_states=hidden_states)
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 240, in forward
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]     qkv, _ = self.qkv_proj(hidden_states)
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 565, in forward
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]     output_parallel = self.quant_method.apply(self, input_, bias)
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 957, in apply
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]     return scheme.apply_weights(layer, x, bias=bias)
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 745, in apply_weights
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]     return self.slidesparse_int8_linear.apply(
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 534, in apply
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]     return self._linear_fn(
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]            ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 289, in cuSPARSELt_INT8_linear
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]     qinput, scale_a_pad = quant_slide_int8_kernel(input, model_name, L)
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 432, in quant_slide_int8_kernel
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]     return torch.ops.slidesparse.quant_slide_int8(input, model_name, L)
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 1255, in __call__
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 564, in _quant_slide_int8_impl
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]     return fn(input, L)
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]            ^^^^^^^^^^^^
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]   File "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/GB10_cc121_py312_cu129_aarch64/quant_slide_tuned_Llama3.2-1B.py", line 365, in quant_slide_int8_triton
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]     _quant_slide_int8_kernel[(M,)](
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 733, in run
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]     kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 861, in _do_compile
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]     kernel = self.compile(src, target=target, options=options.__dict__)
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 320, in compile
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]     next_module = compile_ir(module, metadata)
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 520, in <lambda>
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]     stages["cubin"] = lambda src, metadata: self.make_cubin(src, metadata, options, self.target.arch)
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 503, in make_cubin
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866]     raise PTXASError(error)
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866] triton.runtime.errors.PTXASError: PTXAS error: Internal Triton PTX codegen error
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866] `ptxas` stderr:
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866] ptxas fatal   : Value 'sm_121a' is not defined for option 'gpu-name'
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866] 
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866] Repro command: /usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas -lineinfo -v --gpu-name=sm_121a /tmp/tmph7eaa1au.ptx -o /tmp/tmph7eaa1au.ptx.o
(EngineCore_DP0 pid=287800) ERROR 01-25 18:39:03 [core.py:866] 

STDERR:
[2026-01-25 18:38:49] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-25 18:38:49] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 18:38:49] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 18:38:49] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:38:49] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:38:49] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:38:49] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:38:49] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:38:49] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 18:38:49] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:38:49] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:38:49] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:38:49] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:38:49] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 18:38:53] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-25 18:38:53] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 18:38:53] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 18:38:53] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:38:53] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:38:53] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:38:53] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:38:53] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:38:53] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 18:38:53] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:38:53] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:38:53] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:38:53] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:38:53] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=287800) [2026-01-25 18:38:54] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=287800) [2026-01-25 18:38:54] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=287800) [2026-01-25 18:38:54] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=287800) [2026-01-25 18:38:54] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=287800) [2026-01-25 18:38:54] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=287800) [2026-01-25 18:38:54] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=287800) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=287800) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.19s/it]
(EngineCore_DP0 pid=287800) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.19s/it]
(EngineCore_DP0 pid=287800) 
(EngineCore_DP0 pid=287800) [2026-01-25 18:39:02] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2048] -> 1D uint8
(EngineCore_DP0 pid=287800) [2026-01-25 18:39:02] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4718592 bytes
(EngineCore_DP0 pid=287800) [2026-01-25 18:39:02] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2048] -> 1D uint8
(EngineCore_DP0 pid=287800) [2026-01-25 18:39:02] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 3145728 bytes
(EngineCore_DP0 pid=287800) [2026-01-25 18:39:02] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2048] -> 1D uint8
(EngineCore_DP0 pid=287800) [2026-01-25 18:39:02] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 25165824 bytes
(EngineCore_DP0 pid=287800) [2026-01-25 18:39:02] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 8192] -> 1D uint8
(EngineCore_DP0 pid=287800) [2026-01-25 18:39:02] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12582912 bytes
(EngineCore_DP0 pid=287800) Process EngineCore_DP0:
(EngineCore_DP0 pid=287800) Traceback (most recent call last):
(EngineCore_DP0 pid=287800)   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 468, in make_cubin
(EngineCore_DP0 pid=287800)     subprocess.run(ptxas_cmd, check=True, close_fds=False, stderr=flog)
(EngineCore_DP0 pid=287800)   File "/usr/lib/python3.12/subprocess.py", line 571, in run
(EngineCore_DP0 pid=287800)     raise CalledProcessError(retcode, process.args,
(EngineCore_DP0 pid=287800) subprocess.CalledProcessError: Command '['/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas', '-lineinfo', '-v', '--gpu-name=sm_121a', '/tmp/tmph7eaa1au.ptx', '-o', '/tmp/tmph7eaa1au.ptx.o']' returned non-zero exit status 255.
(EngineCore_DP0 pid=287800) 
(EngineCore_DP0 pid=287800) During handling of the above exception, another exception occurred:
(EngineCore_DP0 pid=287800) 
(EngineCore_DP0 pid=287800) Traceback (most recent call last):
(EngineCore_DP0 pid=287800)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=287800)     self.run()
(EngineCore_DP0 pid=287800)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=287800)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=287800)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=287800)     raise e
(EngineCore_DP0 pid=287800)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=287800)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=287800)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=287800)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=287800)     super().__init__(
(EngineCore_DP0 pid=287800)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=287800)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=287800)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=287800)   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=287800)     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=287800)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=287800)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=287800)     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=287800)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=287800)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=287800)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=287800)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=287800)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=287800)     return func(*args, **kwargs)
(EngineCore_DP0 pid=287800)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=287800)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=287800)     return func(*args, **kwargs)
(EngineCore_DP0 pid=287800)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=287800)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=287800)     self.model_runner.profile_run()
(EngineCore_DP0 pid=287800)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=287800)     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=287800)                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=287800)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=287800)     return func(*args, **kwargs)
(EngineCore_DP0 pid=287800)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=287800)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=287800)     outputs = self.model(
(EngineCore_DP0 pid=287800)               ^^^^^^^^^^^
(EngineCore_DP0 pid=287800)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=287800)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=287800)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=287800)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=287800)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=287800)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=287800)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 623, in forward
(EngineCore_DP0 pid=287800)     model_output = self.model(
(EngineCore_DP0 pid=287800)                    ^^^^^^^^^^^
(EngineCore_DP0 pid=287800)   File "/root/vllmbench/vllm/compilation/decorators.py", line 372, in __call__
(EngineCore_DP0 pid=287800)     return self.forward(*args, **kwargs)
(EngineCore_DP0 pid=287800)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=287800)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 436, in forward
(EngineCore_DP0 pid=287800)     hidden_states, residual = layer(positions, hidden_states, residual)
(EngineCore_DP0 pid=287800)                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=287800)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=287800)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=287800)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=287800)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=287800)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=287800)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=287800)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 342, in forward
(EngineCore_DP0 pid=287800)     hidden_states = self.self_attn(positions=positions, hidden_states=hidden_states)
(EngineCore_DP0 pid=287800)                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=287800)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=287800)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=287800)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=287800)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=287800)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=287800)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=287800)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 240, in forward
(EngineCore_DP0 pid=287800)     qkv, _ = self.qkv_proj(hidden_states)
(EngineCore_DP0 pid=287800)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=287800)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=287800)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=287800)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=287800)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=287800)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=287800)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=287800)   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 565, in forward
(EngineCore_DP0 pid=287800)     output_parallel = self.quant_method.apply(self, input_, bias)
(EngineCore_DP0 pid=287800)                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=287800)   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 957, in apply
(EngineCore_DP0 pid=287800)     return scheme.apply_weights(layer, x, bias=bias)
(EngineCore_DP0 pid=287800)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=287800)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 745, in apply_weights
(EngineCore_DP0 pid=287800)     return self.slidesparse_int8_linear.apply(
(EngineCore_DP0 pid=287800)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=287800)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 534, in apply
(EngineCore_DP0 pid=287800)     return self._linear_fn(
(EngineCore_DP0 pid=287800)            ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=287800)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 289, in cuSPARSELt_INT8_linear
(EngineCore_DP0 pid=287800)     qinput, scale_a_pad = quant_slide_int8_kernel(input, model_name, L)
(EngineCore_DP0 pid=287800)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=287800)   File "/root/vllmbench/slidesparse/core/kernels.py", line 432, in quant_slide_int8_kernel
(EngineCore_DP0 pid=287800)     return torch.ops.slidesparse.quant_slide_int8(input, model_name, L)
(EngineCore_DP0 pid=287800)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=287800)   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 1255, in __call__
(EngineCore_DP0 pid=287800)     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=287800)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=287800)   File "/root/vllmbench/slidesparse/core/kernels.py", line 564, in _quant_slide_int8_impl
(EngineCore_DP0 pid=287800)     return fn(input, L)
(EngineCore_DP0 pid=287800)            ^^^^^^^^^^^^
(EngineCore_DP0 pid=287800)   File "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/GB10_cc121_py312_cu129_aarch64/quant_slide_tuned_Llama3.2-1B.py", line 365, in quant_slide_int8_triton
(EngineCore_DP0 pid=287800)     _quant_slide_int8_kernel[(M,)](
(EngineCore_DP0 pid=287800)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=287800)     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=287800)                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=287800)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 733, in run
(EngineCore_DP0 pid=287800)     kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)
(EngineCore_DP0 pid=287800)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=287800)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 861, in _do_compile
(EngineCore_DP0 pid=287800)     kernel = self.compile(src, target=target, options=options.__dict__)
(EngineCore_DP0 pid=287800)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=287800)   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 320, in compile
(EngineCore_DP0 pid=287800)     next_module = compile_ir(module, metadata)
(EngineCore_DP0 pid=287800)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=287800)   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 520, in <lambda>
(EngineCore_DP0 pid=287800)     stages["cubin"] = lambda src, metadata: self.make_cubin(src, metadata, options, self.target.arch)
(EngineCore_DP0 pid=287800)                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=287800)   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 503, in make_cubin
(EngineCore_DP0 pid=287800)     raise PTXASError(error)
(EngineCore_DP0 pid=287800) triton.runtime.errors.PTXASError: PTXAS error: Internal Triton PTX codegen error
(EngineCore_DP0 pid=287800) `ptxas` stderr:
(EngineCore_DP0 pid=287800) ptxas fatal   : Value 'sm_121a' is not defined for option 'gpu-name'
(EngineCore_DP0 pid=287800) 
(EngineCore_DP0 pid=287800) Repro command: /usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas -lineinfo -v --gpu-name=sm_121a /tmp/tmph7eaa1au.ptx -o /tmp/tmph7eaa1au.ptx.o
(EngineCore_DP0 pid=287800) 
[rank0]:[W125 18:39:04.367479039 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=16

========== M=128 ==========
Time: 2026-01-25 18:39:05
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4
Params: prompt_len=16, output_len=256, num_prompts=128, max_num_seqs=128
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4 --dataset-name random --input-len 16 --output-len 256 --num-prompts 128 --max-num-seqs 128 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_4/json/Llama3.2-1B-INT8_M128.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 18:39:09 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 18:39:09 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=288263) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=288263) 
(EngineCore_DP0 pid=288263) 
(EngineCore_DP0 pid=288263) ================================================================
(EngineCore_DP0 pid=288263) Internal Triton PTX codegen error
(EngineCore_DP0 pid=288263) `ptxas` stderr:
(EngineCore_DP0 pid=288263) ptxas fatal   : Value 'sm_121a' is not defined for option 'gpu-name'
(EngineCore_DP0 pid=288263) 
(EngineCore_DP0 pid=288263) Repro command: /usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas -lineinfo -v --gpu-name=sm_121a /tmp/tmp5mpli33m.ptx -o /tmp/tmp5mpli33m.ptx.o
(EngineCore_DP0 pid=288263) 
(EngineCore_DP0 pid=288263) 
(EngineCore_DP0 pid=288263) //
(EngineCore_DP0 pid=288263) // Generated by LLVM NVPTX Back-End
(EngineCore_DP0 pid=288263) //
(EngineCore_DP0 pid=288263) 
(EngineCore_DP0 pid=288263) .version 8.7
(EngineCore_DP0 pid=288263) .target sm_121a
(EngineCore_DP0 pid=288263) .address_size 64
(EngineCore_DP0 pid=288263) 
(EngineCore_DP0 pid=288263) 	// .globl	_quant_slide_int8_kernel // -- Begin function _quant_slide_int8_kernel
(EngineCore_DP0 pid=288263) .extern .shared .align 16 .b8 global_smem[];
(EngineCore_DP0 pid=288263)                                         // @_quant_slide_int8_kernel
(EngineCore_DP0 pid=288263) .visible .entry _quant_slide_int8_kernel(
(EngineCore_DP0 pid=288263) 	.param .u64 .ptr .global .align 1 _quant_slide_int8_kernel_param_0,
(EngineCore_DP0 pid=288263) 	.param .u64 .ptr .global .align 1 _quant_slide_int8_kernel_param_1,
(EngineCore_DP0 pid=288263) 	.param .u64 .ptr .global .align 1 _quant_slide_int8_kernel_param_2,
(EngineCore_DP0 pid=288263) 	.param .u32 _quant_slide_int8_kernel_param_3,
(EngineCore_DP0 pid=288263) 	.param .u32 _quant_slide_int8_kernel_param_4,
(EngineCore_DP0 pid=288263) 	.param .u32 _quant_slide_int8_kernel_param_5,
(EngineCore_DP0 pid=288263) 	.param .u32 _quant_slide_int8_kernel_param_6,
(EngineCore_DP0 pid=288263) 	.param .u32 _quant_slide_int8_kernel_param_7,
(EngineCore_DP0 pid=288263) 	.param .u32 _quant_slide_int8_kernel_param_8,
(EngineCore_DP0 pid=288263) 	.param .u32 _quant_slide_int8_kernel_param_9,
(EngineCore_DP0 pid=288263) 	.param .u64 .ptr .global .align 1 _quant_slide_int8_kernel_param_10,
(EngineCore_DP0 pid=288263) 	.param .u64 .ptr .global .align 1 _quant_slide_int8_kernel_param_11
(EngineCore_DP0 pid=288263) )
(EngineCore_DP0 pid=288263) .reqntid 1024
(EngineCore_DP0 pid=288263) {
(EngineCore_DP0 pid=288263) 	.reg .pred 	%p<20>;
(EngineCore_DP0 pid=288263) 	.reg .b16 	%rs<20>;
(EngineCore_DP0 pid=288263) 	.reg .b32 	%r<113>;
(EngineCore_DP0 pid=288263) 	.reg .b64 	%rd<13>;
(EngineCore_DP0 pid=288263) 	.loc	1 278 0                         // quant_slide_tuned_Llama3.2-1B.py:278:0
(EngineCore_DP0 pid=288263) $L__func_begin0:
(EngineCore_DP0 pid=288263) 	.loc	1 278 0                         // quant_slide_tuned_Llama3.2-1B.py:278:0
(EngineCore_DP0 pid=288263) 
(EngineCore_DP0 pid=288263) // %bb.0:
(EngineCore_DP0 pid=288263) 	ld.param.b32 	%r20, [_quant_slide_int8_kernel_param_7];
(EngineCore_DP0 pid=288263) 	ld.param.b32 	%r19, [_quant_slide_int8_kernel_param_5];
(EngineCore_DP0 pid=288263) 	ld.param.b32 	%r18, [_quant_slide_int8_kernel_param_4];
(EngineCore_DP0 pid=288263) 	ld.param.b64 	%rd3, [_quant_slide_int8_kernel_param_2];
(EngineCore_DP0 pid=288263) 	ld.param.b64 	%rd4, [_quant_slide_int8_kernel_param_0];
(EngineCore_DP0 pid=288263) $L__tmp0:
(EngineCore_DP0 pid=288263) 	.loc	1 288 24                        // quant_slide_tuned_Llama3.2-1B.py:288:24
(EngineCore_DP0 pid=288263) 	mov.u32 	%r1, %ctaid.x;
(EngineCore_DP0 pid=288263) 	ld.param.b32 	%r22, [_quant_slide_int8_kernel_param_8];
(EngineCore_DP0 pid=288263) 	.loc	1 293 26                        // quant_slide_tuned_Llama3.2-1B.py:293:26
(EngineCore_DP0 pid=288263) 	mul.lo.s32 	%r23, %r22, %r1;
(EngineCore_DP0 pid=288263) 	.loc	1 293 20                        // quant_slide_tuned_Llama3.2-1B.py:293:20
(EngineCore_DP0 pid=288263) 	mad.wide.s32 	%rd1, %r23, 2, %rd4;
(EngineCore_DP0 pid=288263) 	.loc	1 299 32                        // quant_slide_tuned_Llama3.2-1B.py:299:32
(EngineCore_DP0 pid=288263) 	mov.u32 	%r2, %tid.x;
(EngineCore_DP0 pid=288263) 	shl.b32 	%r111, %r2, 2;
(EngineCore_DP0 pid=288263) 	.loc	1 298 35                        // quant_slide_tuned_Llama3.2-1B.py:298:35
(EngineCore_DP0 pid=288263) 	setp.lt.s32 	%p1, %r19, 1;
(EngineCore_DP0 pid=288263) 	mov.b32 	%r110, 0f2B8CBCCC;
(EngineCore_DP0 pid=288263) 	setp.eq.b32 	%p19, %r2, 0;
(EngineCore_DP0 pid=288263) 	@%p1 bra 	$L__BB0_4;
(EngineCore_DP0 pid=288263) // %bb.1:                               // %.lr.ph
(EngineCore_DP0 pid=288263) 	.loc	1 299 32                        // quant_slide_tuned_Llama3.2-1B.py:299:32
(EngineCore_DP0 pid=288263) 	and.b32 	%r4, %r2, 31;
(EngineCore_DP0 pid=288263) 	shr.u32 	%r32, %r2, 3;
(EngineCore_DP0 pid=288263) 	and.b32 	%r33, %r32, 124;
(EngineCore_DP0 pid=288263) 	mov.b32 	%r34, global_smem;
(EngineCore_DP0 pid=288263) 	add.s32 	%r40, %r34, %r33;
(EngineCore_DP0 pid=288263) 	shl.b32 	%r35, %r2, 2;
(EngineCore_DP0 pid=288263) 	add.s32 	%r43, %r34, %r35;
(EngineCore_DP0 pid=288263) 	mov.b32 	%r38, 0;
(EngineCore_DP0 pid=288263) 	mov.b32 	%r108, 0f00000000;
(EngineCore_DP0 pid=288263) 	setp.lt.u32 	%p4, %r2, 32;
(EngineCore_DP0 pid=288263) 	setp.eq.b32 	%p3, %r4, 0;
(EngineCore_DP0 pid=288263) 	mov.b32 	%r109, %r38;
(EngineCore_DP0 pid=288263) $L__BB0_2:                              // =>This Inner Loop Header: Depth=1
(EngineCore_DP0 pid=288263) 	.loc	1 300 22                        // quant_slide_tuned_Llama3.2-1B.py:300:22
(EngineCore_DP0 pid=288263) 	add.s32 	%r46, %r111, %r109;
(EngineCore_DP0 pid=288263) 	setp.lt.s32 	%p2, %r46, %r18;
(EngineCore_DP0 pid=288263) 	.loc	1 301 29                        // quant_slide_tuned_Llama3.2-1B.py:301:29
(EngineCore_DP0 pid=288263) 	mad.wide.s32 	%rd6, %r46, 2, %rd1;
(EngineCore_DP0 pid=288263) 	.loc	1 301 21                        // quant_slide_tuned_Llama3.2-1B.py:301:21
(EngineCore_DP0 pid=288263) 	// begin inline asm
(EngineCore_DP0 pid=288263) 	mov.u32 %r36, %r38;
(EngineCore_DP0 pid=288263) 	mov.u32 %r37, %r38;
(EngineCore_DP0 pid=288263) 	@%p2 ld.global.v2.b32 { %r36, %r37 }, [ %rd6 + 0 ];
(EngineCore_DP0 pid=288263) 	// end inline asm
(EngineCore_DP0 pid=288263) 	mov.b32 	{%rs1, %rs2}, %r36;
(EngineCore_DP0 pid=288263) 	mov.b32 	{%rs3, %rs4}, %r37;
(EngineCore_DP0 pid=288263) 	.loc	1 302 50                        // quant_slide_tuned_Llama3.2-1B.py:302:50
(EngineCore_DP0 pid=288263) 	abs.bf16 	%rs5, %rs1;
(EngineCore_DP0 pid=288263) 	abs.bf16 	%rs6, %rs2;
(EngineCore_DP0 pid=288263) 	abs.bf16 	%rs7, %rs3;
(EngineCore_DP0 pid=288263) 	abs.bf16 	%rs8, %rs4;
(EngineCore_DP0 pid=288263) $L__tmp1:
(EngineCore_DP0 pid=288263) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=288263) 	bar.sync 	0;
(EngineCore_DP0 pid=288263) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=288263) 	max.bf16 	%rs9, %rs5, %rs6;
(EngineCore_DP0 pid=288263) 	max.bf16 	%rs10, %rs9, %rs7;
(EngineCore_DP0 pid=288263) 	max.bf16 	%rs11, %rs10, %rs8;
(EngineCore_DP0 pid=288263) 	cvt.f32.bf16 	%r47, %rs11;
(EngineCore_DP0 pid=288263) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=288263) 	shfl.sync.bfly.b32 	%r48, %r47, 16, 31, -1;
(EngineCore_DP0 pid=288263) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=288263) 	max.f32 	%r49, %r47, %r48;
(EngineCore_DP0 pid=288263) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=288263) 	shfl.sync.bfly.b32 	%r50, %r49, 8, 31, -1;
(EngineCore_DP0 pid=288263) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=288263) 	max.f32 	%r51, %r49, %r50;
(EngineCore_DP0 pid=288263) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=288263) 	shfl.sync.bfly.b32 	%r52, %r51, 4, 31, -1;
(EngineCore_DP0 pid=288263) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=288263) 	max.f32 	%r53, %r51, %r52;
(EngineCore_DP0 pid=288263) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=288263) 	shfl.sync.bfly.b32 	%r54, %r53, 2, 31, -1;
(EngineCore_DP0 pid=288263) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=288263) 	max.f32 	%r55, %r53, %r54;
(EngineCore_DP0 pid=288263) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=288263) 	shfl.sync.bfly.b32 	%r56, %r55, 1, 31, -1;
(EngineCore_DP0 pid=288263) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=288263) 	max.f32 	%r41, %r55, %r56;
(EngineCore_DP0 pid=288263) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=288263) 	// begin inline asm
(EngineCore_DP0 pid=288263) 	@%p3 st.shared.b32 [ %r40 + 0 ], %r41;
(EngineCore_DP0 pid=288263) 	// end inline asm
(EngineCore_DP0 pid=288263) 	bar.sync 	0;
(EngineCore_DP0 pid=288263) 	// begin inline asm
(EngineCore_DP0 pid=288263) 	@%p4 ld.shared.b32 %r42, [ %r43 + 0 ];
(EngineCore_DP0 pid=288263) 	// end inline asm
(EngineCore_DP0 pid=288263) 	shfl.sync.bfly.b32 	%r57, %r42, 16, 31, -1;
(EngineCore_DP0 pid=288263) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=288263) 	max.f32 	%r58, %r42, %r57;
(EngineCore_DP0 pid=288263) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=288263) 	shfl.sync.bfly.b32 	%r59, %r58, 8, 31, -1;
(EngineCore_DP0 pid=288263) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=288263) 	max.f32 	%r60, %r58, %r59;
(EngineCore_DP0 pid=288263) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=288263) 	shfl.sync.bfly.b32 	%r61, %r60, 4, 31, -1;
(EngineCore_DP0 pid=288263) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=288263) 	max.f32 	%r62, %r60, %r61;
(EngineCore_DP0 pid=288263) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=288263) 	shfl.sync.bfly.b32 	%r63, %r62, 2, 31, -1;
(EngineCore_DP0 pid=288263) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=288263) 	max.f32 	%r64, %r62, %r63;
(EngineCore_DP0 pid=288263) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=288263) 	shfl.sync.bfly.b32 	%r65, %r64, 1, 31, -1;
(EngineCore_DP0 pid=288263) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=288263) 	max.f32 	%r45, %r64, %r65;
(EngineCore_DP0 pid=288263) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=288263) 	// begin inline asm
(EngineCore_DP0 pid=288263) 	@%p19 st.shared.b32 [ %r43 + 0 ], %r45;
(EngineCore_DP0 pid=288263) 	// end inline asm
(EngineCore_DP0 pid=288263) 	bar.sync 	0;
(EngineCore_DP0 pid=288263) 	ld.shared.b32 	%r66, [global_smem];
(EngineCore_DP0 pid=288263) $L__tmp2:
(EngineCore_DP0 pid=288263) 	.loc	1 302 36                        // quant_slide_tuned_Llama3.2-1B.py:302:36
(EngineCore_DP0 pid=288263) 	max.f32 	%r108, %r108, %r66;
(EngineCore_DP0 pid=288263) 	.loc	1 298 35                        // quant_slide_tuned_Llama3.2-1B.py:298:35
(EngineCore_DP0 pid=288263) 	add.s32 	%r109, %r109, 4096;
(EngineCore_DP0 pid=288263) 	setp.lt.s32 	%p6, %r109, %r19;
(EngineCore_DP0 pid=288263) 	@%p6 bra 	$L__BB0_2;
(EngineCore_DP0 pid=288263) // %bb.3:                               // %._crit_edge.loopexit
(EngineCore_DP0 pid=288263) 	.loc	1 304 32                        // quant_slide_tuned_Llama3.2-1B.py:304:32
(EngineCore_DP0 pid=288263) 	max.f32 	%r110, %r108, 0f2B8CBCCC;
(EngineCore_DP0 pid=288263) $L__BB0_4:                              // %._crit_edge
(EngineCore_DP0 pid=288263) 	.loc	1 0 32                          // quant_slide_tuned_Llama3.2-1B.py:0:32
(EngineCore_DP0 pid=288263) 	mov.b32 	%r68, 0f42FE0000;
(EngineCore_DP0 pid=288263) 	.loc	1 305 32                        // quant_slide_tuned_Llama3.2-1B.py:305:32
(EngineCore_DP0 pid=288263) 	div.full.f32 	%r69, %r110, %r68;
(EngineCore_DP0 pid=288263) 	.loc	1 305 42                        // quant_slide_tuned_Llama3.2-1B.py:305:42
(EngineCore_DP0 pid=288263) 	max.f32 	%r67, %r69, 0f37810204;
(EngineCore_DP0 pid=288263) 	.loc	1 307 25                        // quant_slide_tuned_Llama3.2-1B.py:307:25
(EngineCore_DP0 pid=288263) 	mad.wide.u32 	%rd7, %r1, 4, %rd3;
(EngineCore_DP0 pid=288263) 	.loc	1 307 30                        // quant_slide_tuned_Llama3.2-1B.py:307:30
(EngineCore_DP0 pid=288263) 	// begin inline asm
(EngineCore_DP0 pid=288263) 	@%p19 st.global.b32 [ %rd7 + 0 ], { %r67 };
(EngineCore_DP0 pid=288263) 	// end inline asm
(EngineCore_DP0 pid=288263) 	.loc	1 312 41                        // quant_slide_tuned_Llama3.2-1B.py:312:41
(EngineCore_DP0 pid=288263) 	setp.lt.s32 	%p8, %r20, 1;
(EngineCore_DP0 pid=288263) 	@%p8 bra 	$L__BB0_7;
(EngineCore_DP0 pid=288263) // %bb.5:                               // %.lr.ph4.preheader
(EngineCore_DP0 pid=288263) 	.loc	1 0 41                          // quant_slide_tuned_Llama3.2-1B.py:0:41
(EngineCore_DP0 pid=288263) 	ld.param.b32 	%r24, [_quant_slide_int8_kernel_param_9];
(EngineCore_DP0 pid=288263) 	shr.s32 	%r25, %r24, 31;
(EngineCore_DP0 pid=288263) 	shr.u32 	%r26, %r25, 30;
(EngineCore_DP0 pid=288263) 	add.s32 	%r27, %r24, %r26;
(EngineCore_DP0 pid=288263) 	shr.s32 	%r28, %r27, 2;
(EngineCore_DP0 pid=288263) 	ld.param.b64 	%rd5, [_quant_slide_int8_kernel_param_1];
(EngineCore_DP0 pid=288263) 	mul.lo.s32 	%r29, %r28, %r1;
(EngineCore_DP0 pid=288263) 	mad.wide.s32 	%rd2, %r29, 4, %rd5;
(EngineCore_DP0 pid=288263) 	div.full.f32 	%r13, %r68, %r110;
(EngineCore_DP0 pid=288263) 	mov.b32 	%r112, 0;
(EngineCore_DP0 pid=288263) $L__BB0_6:                              // %.lr.ph4
(EngineCore_DP0 pid=288263)                                         // =>This Inner Loop Header: Depth=1
(EngineCore_DP0 pid=288263) 	.loc	1 314 30                        // quant_slide_tuned_Llama3.2-1B.py:314:30
(EngineCore_DP0 pid=288263) 	add.s32 	%r72, %r2, %r112;
(EngineCore_DP0 pid=288263) 	setp.lt.s32 	%p13, %r72, %r20;
(EngineCore_DP0 pid=288263) 	.loc	1 322 53                        // quant_slide_tuned_Llama3.2-1B.py:322:53
(EngineCore_DP0 pid=288263) 	setp.lt.s32 	%p14, %r111, %r18;
(EngineCore_DP0 pid=288263) 	.loc	1 322 37                        // quant_slide_tuned_Llama3.2-1B.py:322:37
(EngineCore_DP0 pid=288263) 	and.pred 	%p9, %p13, %p14;
(EngineCore_DP0 pid=288263) 	.loc	1 321 29                        // quant_slide_tuned_Llama3.2-1B.py:321:29
(EngineCore_DP0 pid=288263) 	mad.wide.s32 	%rd8, %r111, 2, %rd1;
(EngineCore_DP0 pid=288263) 	mov.b16 	%rs13, 0;
(EngineCore_DP0 pid=288263) 	.loc	1 321 21                        // quant_slide_tuned_Llama3.2-1B.py:321:21
(EngineCore_DP0 pid=288263) 	// begin inline asm
(EngineCore_DP0 pid=288263) 	mov.u16 %rs12, %rs13;
(EngineCore_DP0 pid=288263) 	@%p9 ld.global.b16 { %rs12 }, [ %rd8 + 0 ];
(EngineCore_DP0 pid=288263) 	// end inline asm
(EngineCore_DP0 pid=288263) 	.loc	1 322 79                        // quant_slide_tuned_Llama3.2-1B.py:322:79
(EngineCore_DP0 pid=288263) 	cvt.f32.bf16 	%r73, %rs12;
(EngineCore_DP0 pid=288263) 	.loc	1 324 53                        // quant_slide_tuned_Llama3.2-1B.py:324:53
(EngineCore_DP0 pid=288263) 	add.s32 	%r74, %r111, 1;
(EngineCore_DP0 pid=288263) 	setp.lt.s32 	%p15, %r74, %r18;
(EngineCore_DP0 pid=288263) 	.loc	1 324 37                        // quant_slide_tuned_Llama3.2-1B.py:324:37
(EngineCore_DP0 pid=288263) 	and.pred 	%p10, %p13, %p15;
(EngineCore_DP0 pid=288263) 	.loc	1 323 39                        // quant_slide_tuned_Llama3.2-1B.py:323:39
(EngineCore_DP0 pid=288263) 	add.s64 	%rd9, %rd8, 2;
(EngineCore_DP0 pid=288263) 	.loc	1 323 21                        // quant_slide_tuned_Llama3.2-1B.py:323:21
(EngineCore_DP0 pid=288263) 	// begin inline asm
(EngineCore_DP0 pid=288263) 	mov.u16 %rs14, %rs13;
(EngineCore_DP0 pid=288263) 	@%p10 ld.global.b16 { %rs14 }, [ %rd9 + 0 ];
(EngineCore_DP0 pid=288263) 	// end inline asm
(EngineCore_DP0 pid=288263) 	.loc	1 324 79                        // quant_slide_tuned_Llama3.2-1B.py:324:79
(EngineCore_DP0 pid=288263) 	cvt.f32.bf16 	%r75, %rs14;
(EngineCore_DP0 pid=288263) 	.loc	1 326 53                        // quant_slide_tuned_Llama3.2-1B.py:326:53
(EngineCore_DP0 pid=288263) 	add.s32 	%r76, %r111, 2;
(EngineCore_DP0 pid=288263) 	setp.lt.s32 	%p16, %r76, %r18;
(EngineCore_DP0 pid=288263) 	.loc	1 326 37                        // quant_slide_tuned_Llama3.2-1B.py:326:37
(EngineCore_DP0 pid=288263) 	and.pred 	%p11, %p13, %p16;
(EngineCore_DP0 pid=288263) 	.loc	1 325 39                        // quant_slide_tuned_Llama3.2-1B.py:325:39
(EngineCore_DP0 pid=288263) 	add.s64 	%rd10, %rd8, 4;
(EngineCore_DP0 pid=288263) 	.loc	1 325 21                        // quant_slide_tuned_Llama3.2-1B.py:325:21
(EngineCore_DP0 pid=288263) 	// begin inline asm
(EngineCore_DP0 pid=288263) 	mov.u16 %rs16, %rs13;
(EngineCore_DP0 pid=288263) 	@%p11 ld.global.b16 { %rs16 }, [ %rd10 + 0 ];
(EngineCore_DP0 pid=288263) 	// end inline asm
(EngineCore_DP0 pid=288263) 	.loc	1 326 79                        // quant_slide_tuned_Llama3.2-1B.py:326:79
(EngineCore_DP0 pid=288263) 	cvt.f32.bf16 	%r77, %rs16;
(EngineCore_DP0 pid=288263) 	.loc	1 328 53                        // quant_slide_tuned_Llama3.2-1B.py:328:53
(EngineCore_DP0 pid=288263) 	add.s32 	%r78, %r111, 3;
(EngineCore_DP0 pid=288263) 	setp.lt.s32 	%p17, %r78, %r18;
(EngineCore_DP0 pid=288263) 	.loc	1 328 37                        // quant_slide_tuned_Llama3.2-1B.py:328:37
(EngineCore_DP0 pid=288263) 	and.pred 	%p12, %p13, %p17;
(EngineCore_DP0 pid=288263) 	.loc	1 327 39                        // quant_slide_tuned_Llama3.2-1B.py:327:39
(EngineCore_DP0 pid=288263) 	add.s64 	%rd11, %rd8, 6;
(EngineCore_DP0 pid=288263) 	.loc	1 327 21                        // quant_slide_tuned_Llama3.2-1B.py:327:21
(EngineCore_DP0 pid=288263) 	// begin inline asm
(EngineCore_DP0 pid=288263) 	mov.u16 %rs18, %rs13;
(EngineCore_DP0 pid=288263) 	@%p12 ld.global.b16 { %rs18 }, [ %rd11 + 0 ];
(EngineCore_DP0 pid=288263) 	// end inline asm
(EngineCore_DP0 pid=288263) 	.loc	1 328 79                        // quant_slide_tuned_Llama3.2-1B.py:328:79
(EngineCore_DP0 pid=288263) 	cvt.f32.bf16 	%r79, %rs18;
(EngineCore_DP0 pid=288263) 	.loc	1 330 56                        // quant_slide_tuned_Llama3.2-1B.py:330:56
(EngineCore_DP0 pid=288263) 	mul.f32 	%r80, %r13, %r73;
(EngineCore_DP0 pid=288263) 	.loc	1 330 51                        // quant_slide_tuned_Llama3.2-1B.py:330:51
(EngineCore_DP0 pid=288263) 	cvt.rni.f32.f32 	%r81, %r80;
(EngineCore_DP0 pid=288263) 	.loc	1 330 76                        // quant_slide_tuned_Llama3.2-1B.py:330:76
(EngineCore_DP0 pid=288263) 	max.f32 	%r82, %r81, 0fC3000000;
(EngineCore_DP0 pid=288263) 	min.f32 	%r83, %r82, 0f42FE0000;
(EngineCore_DP0 pid=288263) 	.loc	1 330 86                        // quant_slide_tuned_Llama3.2-1B.py:330:86
(EngineCore_DP0 pid=288263) 	cvt.rzi.s32.f32 	%r84, %r83;
(EngineCore_DP0 pid=288263) 	.loc	1 330 98                        // quant_slide_tuned_Llama3.2-1B.py:330:98
(EngineCore_DP0 pid=288263) 	and.b32 	%r85, %r84, 255;
(EngineCore_DP0 pid=288263) 	.loc	1 331 56                        // quant_slide_tuned_Llama3.2-1B.py:331:56
(EngineCore_DP0 pid=288263) 	mul.f32 	%r86, %r13, %r75;
(EngineCore_DP0 pid=288263) 	.loc	1 331 51                        // quant_slide_tuned_Llama3.2-1B.py:331:51
(EngineCore_DP0 pid=288263) 	cvt.rni.f32.f32 	%r87, %r86;
(EngineCore_DP0 pid=288263) 	.loc	1 332 56                        // quant_slide_tuned_Llama3.2-1B.py:332:56
(EngineCore_DP0 pid=288263) 	mul.f32 	%r88, %r13, %r77;
(EngineCore_DP0 pid=288263) 	.loc	1 332 51                        // quant_slide_tuned_Llama3.2-1B.py:332:51
(EngineCore_DP0 pid=288263) 	cvt.rni.f32.f32 	%r89, %r88;
(EngineCore_DP0 pid=288263) 	.loc	1 333 56                        // quant_slide_tuned_Llama3.2-1B.py:333:56
(EngineCore_DP0 pid=288263) 	mul.f32 	%r90, %r13, %r79;
(EngineCore_DP0 pid=288263) 	.loc	1 333 51                        // quant_slide_tuned_Llama3.2-1B.py:333:51
(EngineCore_DP0 pid=288263) 	cvt.rni.f32.f32 	%r91, %r90;
(EngineCore_DP0 pid=288263) 	.loc	1 333 76                        // quant_slide_tuned_Llama3.2-1B.py:333:76
(EngineCore_DP0 pid=288263) 	max.f32 	%r92, %r91, 0fC3000000;
(EngineCore_DP0 pid=288263) 	min.f32 	%r93, %r92, 0f42FE0000;
(EngineCore_DP0 pid=288263) 	.loc	1 333 86                        // quant_slide_tuned_Llama3.2-1B.py:333:86
(EngineCore_DP0 pid=288263) 	cvt.rzi.s32.f32 	%r94, %r93;
(EngineCore_DP0 pid=288263) 	.loc	1 331 76                        // quant_slide_tuned_Llama3.2-1B.py:331:76
(EngineCore_DP0 pid=288263) 	max.f32 	%r95, %r89, 0fC3000000;
(EngineCore_DP0 pid=288263) 	max.f32 	%r96, %r87, 0fC3000000;
(EngineCore_DP0 pid=288263) 	min.f32 	%r97, %r96, 0f42FE0000;
(EngineCore_DP0 pid=288263) 	min.f32 	%r98, %r95, 0f42FE0000;
(EngineCore_DP0 pid=288263) 	.loc	1 331 86                        // quant_slide_tuned_Llama3.2-1B.py:331:86
(EngineCore_DP0 pid=288263) 	cvt.rzi.s32.f32 	%r99, %r98;
(EngineCore_DP0 pid=288263) 	cvt.rzi.s32.f32 	%r100, %r97;
(EngineCore_DP0 pid=288263) 	.loc	1 335 30                        // quant_slide_tuned_Llama3.2-1B.py:335:30
(EngineCore_DP0 pid=288263) 	shl.b32 	%r101, %r100, 8;
(EngineCore_DP0 pid=288263) 	shl.b32 	%r102, %r99, 16;
(EngineCore_DP0 pid=288263) 	and.b32 	%r103, %r102, 16711680;
(EngineCore_DP0 pid=288263) 	and.b32 	%r104, %r101, 65280;
(EngineCore_DP0 pid=288263) 	.loc	1 335 24                        // quant_slide_tuned_Llama3.2-1B.py:335:24
(EngineCore_DP0 pid=288263) 	or.b32 	%r105, %r104, %r85;
(EngineCore_DP0 pid=288263) 	.loc	1 335 36                        // quant_slide_tuned_Llama3.2-1B.py:335:36
(EngineCore_DP0 pid=288263) 	or.b32 	%r106, %r105, %r103;
(EngineCore_DP0 pid=288263) 	.loc	1 335 55                        // quant_slide_tuned_Llama3.2-1B.py:335:55
(EngineCore_DP0 pid=288263) 	shl.b32 	%r107, %r94, 24;
(EngineCore_DP0 pid=288263) 	.loc	1 335 49                        // quant_slide_tuned_Llama3.2-1B.py:335:49
(EngineCore_DP0 pid=288263) 	or.b32 	%r71, %r106, %r107;
(EngineCore_DP0 pid=288263) 	.loc	1 336 29                        // quant_slide_tuned_Llama3.2-1B.py:336:29
(EngineCore_DP0 pid=288263) 	mad.wide.s32 	%rd12, %r72, 4, %rd2;
(EngineCore_DP0 pid=288263) 	.loc	1 336 39                        // quant_slide_tuned_Llama3.2-1B.py:336:39
(EngineCore_DP0 pid=288263) 	// begin inline asm
(EngineCore_DP0 pid=288263) 	@%p13 st.global.b32 [ %rd12 + 0 ], { %r71 };
(EngineCore_DP0 pid=288263) 	// end inline asm
(EngineCore_DP0 pid=288263) 	.loc	1 312 41                        // quant_slide_tuned_Llama3.2-1B.py:312:41
(EngineCore_DP0 pid=288263) 	add.s32 	%r112, %r112, 1024;
(EngineCore_DP0 pid=288263) 	add.s32 	%r111, %r111, 4096;
(EngineCore_DP0 pid=288263) 	setp.lt.s32 	%p18, %r112, %r20;
(EngineCore_DP0 pid=288263) 	@%p18 bra 	$L__BB0_6;
(EngineCore_DP0 pid=288263) $L__BB0_7:                              // %._crit_edge5
(EngineCore_DP0 pid=288263) 	.loc	1 312 4                         // quant_slide_tuned_Llama3.2-1B.py:312:4
(EngineCore_DP0 pid=288263) 	ret;
(EngineCore_DP0 pid=288263) $L__tmp3:
(EngineCore_DP0 pid=288263) $L__func_end0:
(EngineCore_DP0 pid=288263)                                         // -- End function
(EngineCore_DP0 pid=288263) }
(EngineCore_DP0 pid=288263) 	.file	1 "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/GB10_cc121_py312_cu129_aarch64/quant_slide_tuned_Llama3.2-1B.py"
(EngineCore_DP0 pid=288263) 	.file	2 "/usr/local/lib/python3.12/dist-packages/triton/language/standard.py"
(EngineCore_DP0 pid=288263) 	.section	.debug_abbrev
(EngineCore_DP0 pid=288263) 	{
(EngineCore_DP0 pid=288263) .b8 1                                   // Abbreviation Code
(EngineCore_DP0 pid=288263) .b8 17                                  // DW_TAG_compile_unit
(EngineCore_DP0 pid=288263) .b8 1                                   // DW_CHILDREN_yes
(EngineCore_DP0 pid=288263) .b8 37                                  // DW_AT_producer
(EngineCore_DP0 pid=288263) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=288263) .b8 19                                  // DW_AT_language
(EngineCore_DP0 pid=288263) .b8 5                                   // DW_FORM_data2
(EngineCore_DP0 pid=288263) .b8 3                                   // DW_AT_name
(EngineCore_DP0 pid=288263) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=288263) .b8 16                                  // DW_AT_stmt_list
(EngineCore_DP0 pid=288263) .b8 6                                   // DW_FORM_data4
(EngineCore_DP0 pid=288263) .b8 27                                  // DW_AT_comp_dir
(EngineCore_DP0 pid=288263) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=288263) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=288263) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=288263) .b8 2                                   // Abbreviation Code
(EngineCore_DP0 pid=288263) .b8 46                                  // DW_TAG_subprogram
(EngineCore_DP0 pid=288263) .b8 0                                   // DW_CHILDREN_no
(EngineCore_DP0 pid=288263) .b8 3                                   // DW_AT_name
(EngineCore_DP0 pid=288263) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=288263) .b8 32                                  // DW_AT_inline
(EngineCore_DP0 pid=288263) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=288263) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=288263) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=288263) .b8 3                                   // Abbreviation Code
(EngineCore_DP0 pid=288263) .b8 46                                  // DW_TAG_subprogram
(EngineCore_DP0 pid=288263) .b8 1                                   // DW_CHILDREN_yes
(EngineCore_DP0 pid=288263) .b8 17                                  // DW_AT_low_pc
(EngineCore_DP0 pid=288263) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=288263) .b8 18                                  // DW_AT_high_pc
(EngineCore_DP0 pid=288263) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=288263) .b8 49                                  // DW_AT_abstract_origin
(EngineCore_DP0 pid=288263) .b8 19                                  // DW_FORM_ref4
(EngineCore_DP0 pid=288263) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=288263) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=288263) .b8 4                                   // Abbreviation Code
(EngineCore_DP0 pid=288263) .b8 29                                  // DW_TAG_inlined_subroutine
(EngineCore_DP0 pid=288263) .b8 0                                   // DW_CHILDREN_no
(EngineCore_DP0 pid=288263) .b8 49                                  // DW_AT_abstract_origin
(EngineCore_DP0 pid=288263) .b8 19                                  // DW_FORM_ref4
(EngineCore_DP0 pid=288263) .b8 17                                  // DW_AT_low_pc
(EngineCore_DP0 pid=288263) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=288263) .b8 18                                  // DW_AT_high_pc
(EngineCore_DP0 pid=288263) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=288263) .b8 88                                  // DW_AT_call_file
(EngineCore_DP0 pid=288263) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=288263) .b8 89                                  // DW_AT_call_line
(EngineCore_DP0 pid=288263) .b8 5                                   // DW_FORM_data2
(EngineCore_DP0 pid=288263) .b8 87                                  // DW_AT_call_column
(EngineCore_DP0 pid=288263) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=288263) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=288263) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=288263) .b8 0                                   // EOM(3)
(EngineCore_DP0 pid=288263) 	}
(EngineCore_DP0 pid=288263) 	.section	.debug_info
(EngineCore_DP0 pid=288263) 	{
(EngineCore_DP0 pid=288263) .b32 224                                // Length of Unit
(EngineCore_DP0 pid=288263) .b8 2                                   // DWARF version number
(EngineCore_DP0 pid=288263) .b8 0
(EngineCore_DP0 pid=288263) .b32 .debug_abbrev                      // Offset Into Abbrev. Section
(EngineCore_DP0 pid=288263) .b8 8                                   // Address Size (in bytes)
(EngineCore_DP0 pid=288263) .b8 1                                   // Abbrev [1] 0xb:0xd9 DW_TAG_compile_unit
(EngineCore_DP0 pid=288263) .b8 116                                 // DW_AT_producer
(EngineCore_DP0 pid=288263) .b8 114
(EngineCore_DP0 pid=288263) .b8 105
(EngineCore_DP0 pid=288263) .b8 116
(EngineCore_DP0 pid=288263) .b8 111
(EngineCore_DP0 pid=288263) .b8 110
(EngineCore_DP0 pid=288263) .b8 0
(EngineCore_DP0 pid=288263) .b8 2                                   // DW_AT_language
(EngineCore_DP0 pid=288263) .b8 0
(EngineCore_DP0 pid=288263) .b8 113                                 // DW_AT_name
(EngineCore_DP0 pid=288263) .b8 117
(EngineCore_DP0 pid=288263) .b8 97
(EngineCore_DP0 pid=288263) .b8 110
(EngineCore_DP0 pid=288263) .b8 116
(EngineCore_DP0 pid=288263) .b8 95
(EngineCore_DP0 pid=288263) .b8 115
(EngineCore_DP0 pid=288263) .b8 108
(EngineCore_DP0 pid=288263) .b8 105
(EngineCore_DP0 pid=288263) .b8 100
(EngineCore_DP0 pid=288263) .b8 101
(EngineCore_DP0 pid=288263) .b8 95
(EngineCore_DP0 pid=288263) .b8 116
(EngineCore_DP0 pid=288263) .b8 117
(EngineCore_DP0 pid=288263) .b8 110
(EngineCore_DP0 pid=288263) .b8 101
(EngineCore_DP0 pid=288263) .b8 100
(EngineCore_DP0 pid=288263) .b8 95
(EngineCore_DP0 pid=288263) .b8 76
(EngineCore_DP0 pid=288263) .b8 108
(EngineCore_DP0 pid=288263) .b8 97
(EngineCore_DP0 pid=288263) .b8 109
(EngineCore_DP0 pid=288263) .b8 97
(EngineCore_DP0 pid=288263) .b8 51
(EngineCore_DP0 pid=288263) .b8 46
(EngineCore_DP0 pid=288263) .b8 50
(EngineCore_DP0 pid=288263) .b8 45
(EngineCore_DP0 pid=288263) .b8 49
(EngineCore_DP0 pid=288263) .b8 66
(EngineCore_DP0 pid=288263) .b8 46
(EngineCore_DP0 pid=288263) .b8 112
(EngineCore_DP0 pid=288263) .b8 121
(EngineCore_DP0 pid=288263) .b8 0
(EngineCore_DP0 pid=288263) .b32 .debug_line                        // DW_AT_stmt_list
(EngineCore_DP0 pid=288263) .b8 47                                  // DW_AT_comp_dir
(EngineCore_DP0 pid=288263) .b8 114
(EngineCore_DP0 pid=288263) .b8 111
(EngineCore_DP0 pid=288263) .b8 111
(EngineCore_DP0 pid=288263) .b8 116
(EngineCore_DP0 pid=288263) .b8 47
(EngineCore_DP0 pid=288263) .b8 118
(EngineCore_DP0 pid=288263) .b8 108
(EngineCore_DP0 pid=288263) .b8 108
(EngineCore_DP0 pid=288263) .b8 109
(EngineCore_DP0 pid=288263) .b8 98
(EngineCore_DP0 pid=288263) .b8 101
(EngineCore_DP0 pid=288263) .b8 110
(EngineCore_DP0 pid=288263) .b8 99
(EngineCore_DP0 pid=288263) .b8 104
(EngineCore_DP0 pid=288263) .b8 47
(EngineCore_DP0 pid=288263) .b8 115
(EngineCore_DP0 pid=288263) .b8 108
(EngineCore_DP0 pid=288263) .b8 105
(EngineCore_DP0 pid=288263) .b8 100
(EngineCore_DP0 pid=288263) .b8 101
(EngineCore_DP0 pid=288263) .b8 115
(EngineCore_DP0 pid=288263) .b8 112
(EngineCore_DP0 pid=288263) .b8 97
(EngineCore_DP0 pid=288263) .b8 114
(EngineCore_DP0 pid=288263) .b8 115
(EngineCore_DP0 pid=288263) .b8 101
(EngineCore_DP0 pid=288263) .b8 47
(EngineCore_DP0 pid=288263) .b8 99
(EngineCore_DP0 pid=288263) .b8 115
(EngineCore_DP0 pid=288263) .b8 114
(EngineCore_DP0 pid=288263) .b8 99
(EngineCore_DP0 pid=288263) .b8 47
(EngineCore_DP0 pid=288263) .b8 102
(EngineCore_DP0 pid=288263) .b8 117
(EngineCore_DP0 pid=288263) .b8 115
(EngineCore_DP0 pid=288263) .b8 101
(EngineCore_DP0 pid=288263) .b8 100
(EngineCore_DP0 pid=288263) .b8 95
(EngineCore_DP0 pid=288263) .b8 113
(EngineCore_DP0 pid=288263) .b8 117
(EngineCore_DP0 pid=288263) .b8 97
(EngineCore_DP0 pid=288263) .b8 110
(EngineCore_DP0 pid=288263) .b8 116
(EngineCore_DP0 pid=288263) .b8 95
(EngineCore_DP0 pid=288263) .b8 115
(EngineCore_DP0 pid=288263) .b8 108
(EngineCore_DP0 pid=288263) .b8 105
(EngineCore_DP0 pid=288263) .b8 100
(EngineCore_DP0 pid=288263) .b8 101
(EngineCore_DP0 pid=288263) .b8 95
(EngineCore_DP0 pid=288263) .b8 116
(EngineCore_DP0 pid=288263) .b8 114
(EngineCore_DP0 pid=288263) .b8 105
(EngineCore_DP0 pid=288263) .b8 116
(EngineCore_DP0 pid=288263) .b8 111
(EngineCore_DP0 pid=288263) .b8 110
(EngineCore_DP0 pid=288263) .b8 47
(EngineCore_DP0 pid=288263) .b8 98
(EngineCore_DP0 pid=288263) .b8 117
(EngineCore_DP0 pid=288263) .b8 105
(EngineCore_DP0 pid=288263) .b8 108
(EngineCore_DP0 pid=288263) .b8 100
(EngineCore_DP0 pid=288263) .b8 47
(EngineCore_DP0 pid=288263) .b8 71
(EngineCore_DP0 pid=288263) .b8 66
(EngineCore_DP0 pid=288263) .b8 49
(EngineCore_DP0 pid=288263) .b8 48
(EngineCore_DP0 pid=288263) .b8 95
(EngineCore_DP0 pid=288263) .b8 99
(EngineCore_DP0 pid=288263) .b8 99
(EngineCore_DP0 pid=288263) .b8 49
(EngineCore_DP0 pid=288263) .b8 50
(EngineCore_DP0 pid=288263) .b8 49
(EngineCore_DP0 pid=288263) .b8 95
(EngineCore_DP0 pid=288263) .b8 112
(EngineCore_DP0 pid=288263) .b8 121
(EngineCore_DP0 pid=288263) .b8 51
(EngineCore_DP0 pid=288263) .b8 49
(EngineCore_DP0 pid=288263) .b8 50
(EngineCore_DP0 pid=288263) .b8 95
(EngineCore_DP0 pid=288263) .b8 99
(EngineCore_DP0 pid=288263) .b8 117
(EngineCore_DP0 pid=288263) .b8 49
(EngineCore_DP0 pid=288263) .b8 50
(EngineCore_DP0 pid=288263) .b8 57
(EngineCore_DP0 pid=288263) .b8 95
(EngineCore_DP0 pid=288263) .b8 97
(EngineCore_DP0 pid=288263) .b8 97
(EngineCore_DP0 pid=288263) .b8 114
(EngineCore_DP0 pid=288263) .b8 99
(EngineCore_DP0 pid=288263) .b8 104
(EngineCore_DP0 pid=288263) .b8 54
(EngineCore_DP0 pid=288263) .b8 52
(EngineCore_DP0 pid=288263) .b8 0
(EngineCore_DP0 pid=288263) .b8 2                                   // Abbrev [2] 0x99:0x1b DW_TAG_subprogram
(EngineCore_DP0 pid=288263) .b8 95                                  // DW_AT_name
(EngineCore_DP0 pid=288263) .b8 113
(EngineCore_DP0 pid=288263) .b8 117
(EngineCore_DP0 pid=288263) .b8 97
(EngineCore_DP0 pid=288263) .b8 110
(EngineCore_DP0 pid=288263) .b8 116
(EngineCore_DP0 pid=288263) .b8 95
(EngineCore_DP0 pid=288263) .b8 115
(EngineCore_DP0 pid=288263) .b8 108
(EngineCore_DP0 pid=288263) .b8 105
(EngineCore_DP0 pid=288263) .b8 100
(EngineCore_DP0 pid=288263) .b8 101
(EngineCore_DP0 pid=288263) .b8 95
(EngineCore_DP0 pid=288263) .b8 105
(EngineCore_DP0 pid=288263) .b8 110
(EngineCore_DP0 pid=288263) .b8 116
(EngineCore_DP0 pid=288263) .b8 56
(EngineCore_DP0 pid=288263) .b8 95
(EngineCore_DP0 pid=288263) .b8 107
(EngineCore_DP0 pid=288263) .b8 101
(EngineCore_DP0 pid=288263) .b8 114
(EngineCore_DP0 pid=288263) .b8 110
(EngineCore_DP0 pid=288263) .b8 101
(EngineCore_DP0 pid=288263) .b8 108
(EngineCore_DP0 pid=288263) .b8 0
(EngineCore_DP0 pid=288263) .b8 1                                   // DW_AT_inline
(EngineCore_DP0 pid=288263) .b8 3                                   // Abbrev [3] 0xb4:0x2f DW_TAG_subprogram
(EngineCore_DP0 pid=288263) .b64 $L__func_begin0                    // DW_AT_low_pc
(EngineCore_DP0 pid=288263) .b64 $L__func_end0                      // DW_AT_high_pc
(EngineCore_DP0 pid=288263) .b32 153                                // DW_AT_abstract_origin
(EngineCore_DP0 pid=288263) .b8 4                                   // Abbrev [4] 0xc9:0x19 DW_TAG_inlined_subroutine
(EngineCore_DP0 pid=288263) .b32 153                                // DW_AT_abstract_origin
(EngineCore_DP0 pid=288263) .b64 $L__tmp1                           // DW_AT_low_pc
(EngineCore_DP0 pid=288263) .b64 $L__tmp2                           // DW_AT_high_pc
(EngineCore_DP0 pid=288263) .b8 1                                   // DW_AT_call_file
(EngineCore_DP0 pid=288263) .b8 46                                  // DW_AT_call_line
(EngineCore_DP0 pid=288263) .b8 1
(EngineCore_DP0 pid=288263) .b8 43                                  // DW_AT_call_column
(EngineCore_DP0 pid=288263) .b8 0                                   // End Of Children Mark
(EngineCore_DP0 pid=288263) .b8 0                                   // End Of Children Mark
(EngineCore_DP0 pid=288263) 	}
(EngineCore_DP0 pid=288263) 	.section	.debug_macinfo	{	}
(EngineCore_DP0 pid=288263) 
(EngineCore_DP0 pid=288263) ================================================================
(EngineCore_DP0 pid=288263) please share the reproducer above with Triton project.
(EngineCore_DP0 pid=288263) 
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 468, in make_cubin
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]     subprocess.run(ptxas_cmd, check=True, close_fds=False, stderr=flog)
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]   File "/usr/lib/python3.12/subprocess.py", line 571, in run
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]     raise CalledProcessError(retcode, process.args,
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866] subprocess.CalledProcessError: Command '['/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas', '-lineinfo', '-v', '--gpu-name=sm_121a', '/tmp/tmp5mpli33m.ptx', '-o', '/tmp/tmp5mpli33m.ptx.o']' returned non-zero exit status 255.
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866] 
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866] During handling of the above exception, another exception occurred:
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866] 
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]     self.model_runner.profile_run()
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 623, in forward
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]     model_output = self.model(
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]                    ^^^^^^^^^^^
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 372, in __call__
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]     return self.forward(*args, **kwargs)
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 436, in forward
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]     hidden_states, residual = layer(positions, hidden_states, residual)
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 342, in forward
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]     hidden_states = self.self_attn(positions=positions, hidden_states=hidden_states)
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 240, in forward
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]     qkv, _ = self.qkv_proj(hidden_states)
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 565, in forward
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]     output_parallel = self.quant_method.apply(self, input_, bias)
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 957, in apply
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]     return scheme.apply_weights(layer, x, bias=bias)
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 745, in apply_weights
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]     return self.slidesparse_int8_linear.apply(
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 534, in apply
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]     return self._linear_fn(
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]            ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 289, in cuSPARSELt_INT8_linear
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]     qinput, scale_a_pad = quant_slide_int8_kernel(input, model_name, L)
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 432, in quant_slide_int8_kernel
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]     return torch.ops.slidesparse.quant_slide_int8(input, model_name, L)
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 1255, in __call__
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 564, in _quant_slide_int8_impl
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]     return fn(input, L)
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]            ^^^^^^^^^^^^
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]   File "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/GB10_cc121_py312_cu129_aarch64/quant_slide_tuned_Llama3.2-1B.py", line 365, in quant_slide_int8_triton
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]     _quant_slide_int8_kernel[(M,)](
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 733, in run
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]     kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 861, in _do_compile
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]     kernel = self.compile(src, target=target, options=options.__dict__)
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 320, in compile
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]     next_module = compile_ir(module, metadata)
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 520, in <lambda>
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]     stages["cubin"] = lambda src, metadata: self.make_cubin(src, metadata, options, self.target.arch)
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 503, in make_cubin
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866]     raise PTXASError(error)
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866] triton.runtime.errors.PTXASError: PTXAS error: Internal Triton PTX codegen error
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866] `ptxas` stderr:
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866] ptxas fatal   : Value 'sm_121a' is not defined for option 'gpu-name'
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866] 
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866] Repro command: /usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas -lineinfo -v --gpu-name=sm_121a /tmp/tmp5mpli33m.ptx -o /tmp/tmp5mpli33m.ptx.o
(EngineCore_DP0 pid=288263) ERROR 01-25 18:39:22 [core.py:866] 

STDERR:
[2026-01-25 18:39:09] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-25 18:39:09] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 18:39:09] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 18:39:09] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:39:09] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:39:09] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:39:09] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:39:09] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:39:09] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 18:39:09] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:39:09] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:39:09] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:39:09] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:39:09] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 18:39:12] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-25 18:39:12] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 18:39:12] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 18:39:12] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:39:12] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:39:12] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:39:12] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:39:12] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:39:12] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 18:39:12] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:39:12] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:39:12] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:39:12] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:39:12] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=288263) [2026-01-25 18:39:13] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=288263) [2026-01-25 18:39:13] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=288263) [2026-01-25 18:39:13] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=288263) [2026-01-25 18:39:13] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=288263) [2026-01-25 18:39:13] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=288263) [2026-01-25 18:39:13] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=288263) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=288263) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.15s/it]
(EngineCore_DP0 pid=288263) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.15s/it]
(EngineCore_DP0 pid=288263) 
(EngineCore_DP0 pid=288263) [2026-01-25 18:39:22] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2048] -> 1D uint8
(EngineCore_DP0 pid=288263) [2026-01-25 18:39:22] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4718592 bytes
(EngineCore_DP0 pid=288263) [2026-01-25 18:39:22] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2048] -> 1D uint8
(EngineCore_DP0 pid=288263) [2026-01-25 18:39:22] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 3145728 bytes
(EngineCore_DP0 pid=288263) [2026-01-25 18:39:22] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2048] -> 1D uint8
(EngineCore_DP0 pid=288263) [2026-01-25 18:39:22] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 25165824 bytes
(EngineCore_DP0 pid=288263) [2026-01-25 18:39:22] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 8192] -> 1D uint8
(EngineCore_DP0 pid=288263) [2026-01-25 18:39:22] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12582912 bytes
(EngineCore_DP0 pid=288263) Process EngineCore_DP0:
(EngineCore_DP0 pid=288263) Traceback (most recent call last):
(EngineCore_DP0 pid=288263)   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 468, in make_cubin
(EngineCore_DP0 pid=288263)     subprocess.run(ptxas_cmd, check=True, close_fds=False, stderr=flog)
(EngineCore_DP0 pid=288263)   File "/usr/lib/python3.12/subprocess.py", line 571, in run
(EngineCore_DP0 pid=288263)     raise CalledProcessError(retcode, process.args,
(EngineCore_DP0 pid=288263) subprocess.CalledProcessError: Command '['/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas', '-lineinfo', '-v', '--gpu-name=sm_121a', '/tmp/tmp5mpli33m.ptx', '-o', '/tmp/tmp5mpli33m.ptx.o']' returned non-zero exit status 255.
(EngineCore_DP0 pid=288263) 
(EngineCore_DP0 pid=288263) During handling of the above exception, another exception occurred:
(EngineCore_DP0 pid=288263) 
(EngineCore_DP0 pid=288263) Traceback (most recent call last):
(EngineCore_DP0 pid=288263)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=288263)     self.run()
(EngineCore_DP0 pid=288263)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=288263)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=288263)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=288263)     raise e
(EngineCore_DP0 pid=288263)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=288263)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=288263)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288263)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=288263)     super().__init__(
(EngineCore_DP0 pid=288263)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=288263)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=288263)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288263)   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=288263)     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=288263)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288263)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=288263)     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=288263)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288263)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=288263)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=288263)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288263)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=288263)     return func(*args, **kwargs)
(EngineCore_DP0 pid=288263)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288263)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=288263)     return func(*args, **kwargs)
(EngineCore_DP0 pid=288263)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288263)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=288263)     self.model_runner.profile_run()
(EngineCore_DP0 pid=288263)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=288263)     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=288263)                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288263)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=288263)     return func(*args, **kwargs)
(EngineCore_DP0 pid=288263)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288263)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=288263)     outputs = self.model(
(EngineCore_DP0 pid=288263)               ^^^^^^^^^^^
(EngineCore_DP0 pid=288263)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=288263)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=288263)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288263)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=288263)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=288263)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288263)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 623, in forward
(EngineCore_DP0 pid=288263)     model_output = self.model(
(EngineCore_DP0 pid=288263)                    ^^^^^^^^^^^
(EngineCore_DP0 pid=288263)   File "/root/vllmbench/vllm/compilation/decorators.py", line 372, in __call__
(EngineCore_DP0 pid=288263)     return self.forward(*args, **kwargs)
(EngineCore_DP0 pid=288263)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288263)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 436, in forward
(EngineCore_DP0 pid=288263)     hidden_states, residual = layer(positions, hidden_states, residual)
(EngineCore_DP0 pid=288263)                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288263)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=288263)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=288263)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288263)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=288263)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=288263)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288263)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 342, in forward
(EngineCore_DP0 pid=288263)     hidden_states = self.self_attn(positions=positions, hidden_states=hidden_states)
(EngineCore_DP0 pid=288263)                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288263)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=288263)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=288263)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288263)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=288263)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=288263)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288263)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 240, in forward
(EngineCore_DP0 pid=288263)     qkv, _ = self.qkv_proj(hidden_states)
(EngineCore_DP0 pid=288263)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288263)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=288263)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=288263)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288263)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=288263)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=288263)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288263)   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 565, in forward
(EngineCore_DP0 pid=288263)     output_parallel = self.quant_method.apply(self, input_, bias)
(EngineCore_DP0 pid=288263)                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288263)   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 957, in apply
(EngineCore_DP0 pid=288263)     return scheme.apply_weights(layer, x, bias=bias)
(EngineCore_DP0 pid=288263)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288263)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 745, in apply_weights
(EngineCore_DP0 pid=288263)     return self.slidesparse_int8_linear.apply(
(EngineCore_DP0 pid=288263)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288263)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 534, in apply
(EngineCore_DP0 pid=288263)     return self._linear_fn(
(EngineCore_DP0 pid=288263)            ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288263)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 289, in cuSPARSELt_INT8_linear
(EngineCore_DP0 pid=288263)     qinput, scale_a_pad = quant_slide_int8_kernel(input, model_name, L)
(EngineCore_DP0 pid=288263)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288263)   File "/root/vllmbench/slidesparse/core/kernels.py", line 432, in quant_slide_int8_kernel
(EngineCore_DP0 pid=288263)     return torch.ops.slidesparse.quant_slide_int8(input, model_name, L)
(EngineCore_DP0 pid=288263)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288263)   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 1255, in __call__
(EngineCore_DP0 pid=288263)     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=288263)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288263)   File "/root/vllmbench/slidesparse/core/kernels.py", line 564, in _quant_slide_int8_impl
(EngineCore_DP0 pid=288263)     return fn(input, L)
(EngineCore_DP0 pid=288263)            ^^^^^^^^^^^^
(EngineCore_DP0 pid=288263)   File "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/GB10_cc121_py312_cu129_aarch64/quant_slide_tuned_Llama3.2-1B.py", line 365, in quant_slide_int8_triton
(EngineCore_DP0 pid=288263)     _quant_slide_int8_kernel[(M,)](
(EngineCore_DP0 pid=288263)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=288263)     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=288263)                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288263)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 733, in run
(EngineCore_DP0 pid=288263)     kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)
(EngineCore_DP0 pid=288263)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288263)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 861, in _do_compile
(EngineCore_DP0 pid=288263)     kernel = self.compile(src, target=target, options=options.__dict__)
(EngineCore_DP0 pid=288263)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288263)   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 320, in compile
(EngineCore_DP0 pid=288263)     next_module = compile_ir(module, metadata)
(EngineCore_DP0 pid=288263)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288263)   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 520, in <lambda>
(EngineCore_DP0 pid=288263)     stages["cubin"] = lambda src, metadata: self.make_cubin(src, metadata, options, self.target.arch)
(EngineCore_DP0 pid=288263)                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288263)   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 503, in make_cubin
(EngineCore_DP0 pid=288263)     raise PTXASError(error)
(EngineCore_DP0 pid=288263) triton.runtime.errors.PTXASError: PTXAS error: Internal Triton PTX codegen error
(EngineCore_DP0 pid=288263) `ptxas` stderr:
(EngineCore_DP0 pid=288263) ptxas fatal   : Value 'sm_121a' is not defined for option 'gpu-name'
(EngineCore_DP0 pid=288263) 
(EngineCore_DP0 pid=288263) Repro command: /usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas -lineinfo -v --gpu-name=sm_121a /tmp/tmp5mpli33m.ptx -o /tmp/tmp5mpli33m.ptx.o
(EngineCore_DP0 pid=288263) 
[rank0]:[W125 18:39:23.419633560 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=128

========== M=256 ==========
Time: 2026-01-25 18:39:24
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4
Params: prompt_len=16, output_len=256, num_prompts=256, max_num_seqs=256
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4 --dataset-name random --input-len 16 --output-len 256 --num-prompts 256 --max-num-seqs 256 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_4/json/Llama3.2-1B-INT8_M256.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 18:39:28 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 18:39:28 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=288725) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=288725) 
(EngineCore_DP0 pid=288725) 
(EngineCore_DP0 pid=288725) ================================================================
(EngineCore_DP0 pid=288725) Internal Triton PTX codegen error
(EngineCore_DP0 pid=288725) `ptxas` stderr:
(EngineCore_DP0 pid=288725) ptxas fatal   : Value 'sm_121a' is not defined for option 'gpu-name'
(EngineCore_DP0 pid=288725) 
(EngineCore_DP0 pid=288725) Repro command: /usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas -lineinfo -v --gpu-name=sm_121a /tmp/tmpca1zryxw.ptx -o /tmp/tmpca1zryxw.ptx.o
(EngineCore_DP0 pid=288725) 
(EngineCore_DP0 pid=288725) 
(EngineCore_DP0 pid=288725) //
(EngineCore_DP0 pid=288725) // Generated by LLVM NVPTX Back-End
(EngineCore_DP0 pid=288725) //
(EngineCore_DP0 pid=288725) 
(EngineCore_DP0 pid=288725) .version 8.7
(EngineCore_DP0 pid=288725) .target sm_121a
(EngineCore_DP0 pid=288725) .address_size 64
(EngineCore_DP0 pid=288725) 
(EngineCore_DP0 pid=288725) 	// .globl	_quant_slide_int8_kernel // -- Begin function _quant_slide_int8_kernel
(EngineCore_DP0 pid=288725) .extern .shared .align 16 .b8 global_smem[];
(EngineCore_DP0 pid=288725)                                         // @_quant_slide_int8_kernel
(EngineCore_DP0 pid=288725) .visible .entry _quant_slide_int8_kernel(
(EngineCore_DP0 pid=288725) 	.param .u64 .ptr .global .align 1 _quant_slide_int8_kernel_param_0,
(EngineCore_DP0 pid=288725) 	.param .u64 .ptr .global .align 1 _quant_slide_int8_kernel_param_1,
(EngineCore_DP0 pid=288725) 	.param .u64 .ptr .global .align 1 _quant_slide_int8_kernel_param_2,
(EngineCore_DP0 pid=288725) 	.param .u32 _quant_slide_int8_kernel_param_3,
(EngineCore_DP0 pid=288725) 	.param .u32 _quant_slide_int8_kernel_param_4,
(EngineCore_DP0 pid=288725) 	.param .u32 _quant_slide_int8_kernel_param_5,
(EngineCore_DP0 pid=288725) 	.param .u32 _quant_slide_int8_kernel_param_6,
(EngineCore_DP0 pid=288725) 	.param .u32 _quant_slide_int8_kernel_param_7,
(EngineCore_DP0 pid=288725) 	.param .u32 _quant_slide_int8_kernel_param_8,
(EngineCore_DP0 pid=288725) 	.param .u32 _quant_slide_int8_kernel_param_9,
(EngineCore_DP0 pid=288725) 	.param .u64 .ptr .global .align 1 _quant_slide_int8_kernel_param_10,
(EngineCore_DP0 pid=288725) 	.param .u64 .ptr .global .align 1 _quant_slide_int8_kernel_param_11
(EngineCore_DP0 pid=288725) )
(EngineCore_DP0 pid=288725) .reqntid 1024
(EngineCore_DP0 pid=288725) {
(EngineCore_DP0 pid=288725) 	.reg .pred 	%p<20>;
(EngineCore_DP0 pid=288725) 	.reg .b16 	%rs<20>;
(EngineCore_DP0 pid=288725) 	.reg .b32 	%r<113>;
(EngineCore_DP0 pid=288725) 	.reg .b64 	%rd<13>;
(EngineCore_DP0 pid=288725) 	.loc	1 278 0                         // quant_slide_tuned_Llama3.2-1B.py:278:0
(EngineCore_DP0 pid=288725) $L__func_begin0:
(EngineCore_DP0 pid=288725) 	.loc	1 278 0                         // quant_slide_tuned_Llama3.2-1B.py:278:0
(EngineCore_DP0 pid=288725) 
(EngineCore_DP0 pid=288725) // %bb.0:
(EngineCore_DP0 pid=288725) 	ld.param.b32 	%r20, [_quant_slide_int8_kernel_param_7];
(EngineCore_DP0 pid=288725) 	ld.param.b32 	%r19, [_quant_slide_int8_kernel_param_5];
(EngineCore_DP0 pid=288725) 	ld.param.b32 	%r18, [_quant_slide_int8_kernel_param_4];
(EngineCore_DP0 pid=288725) 	ld.param.b64 	%rd3, [_quant_slide_int8_kernel_param_2];
(EngineCore_DP0 pid=288725) 	ld.param.b64 	%rd4, [_quant_slide_int8_kernel_param_0];
(EngineCore_DP0 pid=288725) $L__tmp0:
(EngineCore_DP0 pid=288725) 	.loc	1 288 24                        // quant_slide_tuned_Llama3.2-1B.py:288:24
(EngineCore_DP0 pid=288725) 	mov.u32 	%r1, %ctaid.x;
(EngineCore_DP0 pid=288725) 	ld.param.b32 	%r22, [_quant_slide_int8_kernel_param_8];
(EngineCore_DP0 pid=288725) 	.loc	1 293 26                        // quant_slide_tuned_Llama3.2-1B.py:293:26
(EngineCore_DP0 pid=288725) 	mul.lo.s32 	%r23, %r22, %r1;
(EngineCore_DP0 pid=288725) 	.loc	1 293 20                        // quant_slide_tuned_Llama3.2-1B.py:293:20
(EngineCore_DP0 pid=288725) 	mad.wide.s32 	%rd1, %r23, 2, %rd4;
(EngineCore_DP0 pid=288725) 	.loc	1 299 32                        // quant_slide_tuned_Llama3.2-1B.py:299:32
(EngineCore_DP0 pid=288725) 	mov.u32 	%r2, %tid.x;
(EngineCore_DP0 pid=288725) 	shl.b32 	%r111, %r2, 2;
(EngineCore_DP0 pid=288725) 	.loc	1 298 35                        // quant_slide_tuned_Llama3.2-1B.py:298:35
(EngineCore_DP0 pid=288725) 	setp.lt.s32 	%p1, %r19, 1;
(EngineCore_DP0 pid=288725) 	mov.b32 	%r110, 0f2B8CBCCC;
(EngineCore_DP0 pid=288725) 	setp.eq.b32 	%p19, %r2, 0;
(EngineCore_DP0 pid=288725) 	@%p1 bra 	$L__BB0_4;
(EngineCore_DP0 pid=288725) // %bb.1:                               // %.lr.ph
(EngineCore_DP0 pid=288725) 	.loc	1 299 32                        // quant_slide_tuned_Llama3.2-1B.py:299:32
(EngineCore_DP0 pid=288725) 	and.b32 	%r4, %r2, 31;
(EngineCore_DP0 pid=288725) 	shr.u32 	%r32, %r2, 3;
(EngineCore_DP0 pid=288725) 	and.b32 	%r33, %r32, 124;
(EngineCore_DP0 pid=288725) 	mov.b32 	%r34, global_smem;
(EngineCore_DP0 pid=288725) 	add.s32 	%r40, %r34, %r33;
(EngineCore_DP0 pid=288725) 	shl.b32 	%r35, %r2, 2;
(EngineCore_DP0 pid=288725) 	add.s32 	%r43, %r34, %r35;
(EngineCore_DP0 pid=288725) 	mov.b32 	%r38, 0;
(EngineCore_DP0 pid=288725) 	mov.b32 	%r108, 0f00000000;
(EngineCore_DP0 pid=288725) 	setp.lt.u32 	%p4, %r2, 32;
(EngineCore_DP0 pid=288725) 	setp.eq.b32 	%p3, %r4, 0;
(EngineCore_DP0 pid=288725) 	mov.b32 	%r109, %r38;
(EngineCore_DP0 pid=288725) $L__BB0_2:                              // =>This Inner Loop Header: Depth=1
(EngineCore_DP0 pid=288725) 	.loc	1 300 22                        // quant_slide_tuned_Llama3.2-1B.py:300:22
(EngineCore_DP0 pid=288725) 	add.s32 	%r46, %r111, %r109;
(EngineCore_DP0 pid=288725) 	setp.lt.s32 	%p2, %r46, %r18;
(EngineCore_DP0 pid=288725) 	.loc	1 301 29                        // quant_slide_tuned_Llama3.2-1B.py:301:29
(EngineCore_DP0 pid=288725) 	mad.wide.s32 	%rd6, %r46, 2, %rd1;
(EngineCore_DP0 pid=288725) 	.loc	1 301 21                        // quant_slide_tuned_Llama3.2-1B.py:301:21
(EngineCore_DP0 pid=288725) 	// begin inline asm
(EngineCore_DP0 pid=288725) 	mov.u32 %r36, %r38;
(EngineCore_DP0 pid=288725) 	mov.u32 %r37, %r38;
(EngineCore_DP0 pid=288725) 	@%p2 ld.global.v2.b32 { %r36, %r37 }, [ %rd6 + 0 ];
(EngineCore_DP0 pid=288725) 	// end inline asm
(EngineCore_DP0 pid=288725) 	mov.b32 	{%rs1, %rs2}, %r36;
(EngineCore_DP0 pid=288725) 	mov.b32 	{%rs3, %rs4}, %r37;
(EngineCore_DP0 pid=288725) 	.loc	1 302 50                        // quant_slide_tuned_Llama3.2-1B.py:302:50
(EngineCore_DP0 pid=288725) 	abs.bf16 	%rs5, %rs1;
(EngineCore_DP0 pid=288725) 	abs.bf16 	%rs6, %rs2;
(EngineCore_DP0 pid=288725) 	abs.bf16 	%rs7, %rs3;
(EngineCore_DP0 pid=288725) 	abs.bf16 	%rs8, %rs4;
(EngineCore_DP0 pid=288725) $L__tmp1:
(EngineCore_DP0 pid=288725) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=288725) 	bar.sync 	0;
(EngineCore_DP0 pid=288725) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=288725) 	max.bf16 	%rs9, %rs5, %rs6;
(EngineCore_DP0 pid=288725) 	max.bf16 	%rs10, %rs9, %rs7;
(EngineCore_DP0 pid=288725) 	max.bf16 	%rs11, %rs10, %rs8;
(EngineCore_DP0 pid=288725) 	cvt.f32.bf16 	%r47, %rs11;
(EngineCore_DP0 pid=288725) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=288725) 	shfl.sync.bfly.b32 	%r48, %r47, 16, 31, -1;
(EngineCore_DP0 pid=288725) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=288725) 	max.f32 	%r49, %r47, %r48;
(EngineCore_DP0 pid=288725) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=288725) 	shfl.sync.bfly.b32 	%r50, %r49, 8, 31, -1;
(EngineCore_DP0 pid=288725) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=288725) 	max.f32 	%r51, %r49, %r50;
(EngineCore_DP0 pid=288725) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=288725) 	shfl.sync.bfly.b32 	%r52, %r51, 4, 31, -1;
(EngineCore_DP0 pid=288725) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=288725) 	max.f32 	%r53, %r51, %r52;
(EngineCore_DP0 pid=288725) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=288725) 	shfl.sync.bfly.b32 	%r54, %r53, 2, 31, -1;
(EngineCore_DP0 pid=288725) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=288725) 	max.f32 	%r55, %r53, %r54;
(EngineCore_DP0 pid=288725) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=288725) 	shfl.sync.bfly.b32 	%r56, %r55, 1, 31, -1;
(EngineCore_DP0 pid=288725) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=288725) 	max.f32 	%r41, %r55, %r56;
(EngineCore_DP0 pid=288725) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=288725) 	// begin inline asm
(EngineCore_DP0 pid=288725) 	@%p3 st.shared.b32 [ %r40 + 0 ], %r41;
(EngineCore_DP0 pid=288725) 	// end inline asm
(EngineCore_DP0 pid=288725) 	bar.sync 	0;
(EngineCore_DP0 pid=288725) 	// begin inline asm
(EngineCore_DP0 pid=288725) 	@%p4 ld.shared.b32 %r42, [ %r43 + 0 ];
(EngineCore_DP0 pid=288725) 	// end inline asm
(EngineCore_DP0 pid=288725) 	shfl.sync.bfly.b32 	%r57, %r42, 16, 31, -1;
(EngineCore_DP0 pid=288725) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=288725) 	max.f32 	%r58, %r42, %r57;
(EngineCore_DP0 pid=288725) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=288725) 	shfl.sync.bfly.b32 	%r59, %r58, 8, 31, -1;
(EngineCore_DP0 pid=288725) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=288725) 	max.f32 	%r60, %r58, %r59;
(EngineCore_DP0 pid=288725) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=288725) 	shfl.sync.bfly.b32 	%r61, %r60, 4, 31, -1;
(EngineCore_DP0 pid=288725) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=288725) 	max.f32 	%r62, %r60, %r61;
(EngineCore_DP0 pid=288725) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=288725) 	shfl.sync.bfly.b32 	%r63, %r62, 2, 31, -1;
(EngineCore_DP0 pid=288725) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=288725) 	max.f32 	%r64, %r62, %r63;
(EngineCore_DP0 pid=288725) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=288725) 	shfl.sync.bfly.b32 	%r65, %r64, 1, 31, -1;
(EngineCore_DP0 pid=288725) 	.loc	2 168 27                        // standard.py:168:27 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=288725) 	max.f32 	%r45, %r64, %r65;
(EngineCore_DP0 pid=288725) 	.loc	2 189 40                        // standard.py:189:40 @[ quant_slide_tuned_Llama3.2-1B.py:302:43 ]
(EngineCore_DP0 pid=288725) 	// begin inline asm
(EngineCore_DP0 pid=288725) 	@%p19 st.shared.b32 [ %r43 + 0 ], %r45;
(EngineCore_DP0 pid=288725) 	// end inline asm
(EngineCore_DP0 pid=288725) 	bar.sync 	0;
(EngineCore_DP0 pid=288725) 	ld.shared.b32 	%r66, [global_smem];
(EngineCore_DP0 pid=288725) $L__tmp2:
(EngineCore_DP0 pid=288725) 	.loc	1 302 36                        // quant_slide_tuned_Llama3.2-1B.py:302:36
(EngineCore_DP0 pid=288725) 	max.f32 	%r108, %r108, %r66;
(EngineCore_DP0 pid=288725) 	.loc	1 298 35                        // quant_slide_tuned_Llama3.2-1B.py:298:35
(EngineCore_DP0 pid=288725) 	add.s32 	%r109, %r109, 4096;
(EngineCore_DP0 pid=288725) 	setp.lt.s32 	%p6, %r109, %r19;
(EngineCore_DP0 pid=288725) 	@%p6 bra 	$L__BB0_2;
(EngineCore_DP0 pid=288725) // %bb.3:                               // %._crit_edge.loopexit
(EngineCore_DP0 pid=288725) 	.loc	1 304 32                        // quant_slide_tuned_Llama3.2-1B.py:304:32
(EngineCore_DP0 pid=288725) 	max.f32 	%r110, %r108, 0f2B8CBCCC;
(EngineCore_DP0 pid=288725) $L__BB0_4:                              // %._crit_edge
(EngineCore_DP0 pid=288725) 	.loc	1 0 32                          // quant_slide_tuned_Llama3.2-1B.py:0:32
(EngineCore_DP0 pid=288725) 	mov.b32 	%r68, 0f42FE0000;
(EngineCore_DP0 pid=288725) 	.loc	1 305 32                        // quant_slide_tuned_Llama3.2-1B.py:305:32
(EngineCore_DP0 pid=288725) 	div.full.f32 	%r69, %r110, %r68;
(EngineCore_DP0 pid=288725) 	.loc	1 305 42                        // quant_slide_tuned_Llama3.2-1B.py:305:42
(EngineCore_DP0 pid=288725) 	max.f32 	%r67, %r69, 0f37810204;
(EngineCore_DP0 pid=288725) 	.loc	1 307 25                        // quant_slide_tuned_Llama3.2-1B.py:307:25
(EngineCore_DP0 pid=288725) 	mad.wide.u32 	%rd7, %r1, 4, %rd3;
(EngineCore_DP0 pid=288725) 	.loc	1 307 30                        // quant_slide_tuned_Llama3.2-1B.py:307:30
(EngineCore_DP0 pid=288725) 	// begin inline asm
(EngineCore_DP0 pid=288725) 	@%p19 st.global.b32 [ %rd7 + 0 ], { %r67 };
(EngineCore_DP0 pid=288725) 	// end inline asm
(EngineCore_DP0 pid=288725) 	.loc	1 312 41                        // quant_slide_tuned_Llama3.2-1B.py:312:41
(EngineCore_DP0 pid=288725) 	setp.lt.s32 	%p8, %r20, 1;
(EngineCore_DP0 pid=288725) 	@%p8 bra 	$L__BB0_7;
(EngineCore_DP0 pid=288725) // %bb.5:                               // %.lr.ph4.preheader
(EngineCore_DP0 pid=288725) 	.loc	1 0 41                          // quant_slide_tuned_Llama3.2-1B.py:0:41
(EngineCore_DP0 pid=288725) 	ld.param.b32 	%r24, [_quant_slide_int8_kernel_param_9];
(EngineCore_DP0 pid=288725) 	shr.s32 	%r25, %r24, 31;
(EngineCore_DP0 pid=288725) 	shr.u32 	%r26, %r25, 30;
(EngineCore_DP0 pid=288725) 	add.s32 	%r27, %r24, %r26;
(EngineCore_DP0 pid=288725) 	shr.s32 	%r28, %r27, 2;
(EngineCore_DP0 pid=288725) 	ld.param.b64 	%rd5, [_quant_slide_int8_kernel_param_1];
(EngineCore_DP0 pid=288725) 	mul.lo.s32 	%r29, %r28, %r1;
(EngineCore_DP0 pid=288725) 	mad.wide.s32 	%rd2, %r29, 4, %rd5;
(EngineCore_DP0 pid=288725) 	div.full.f32 	%r13, %r68, %r110;
(EngineCore_DP0 pid=288725) 	mov.b32 	%r112, 0;
(EngineCore_DP0 pid=288725) $L__BB0_6:                              // %.lr.ph4
(EngineCore_DP0 pid=288725)                                         // =>This Inner Loop Header: Depth=1
(EngineCore_DP0 pid=288725) 	.loc	1 314 30                        // quant_slide_tuned_Llama3.2-1B.py:314:30
(EngineCore_DP0 pid=288725) 	add.s32 	%r72, %r2, %r112;
(EngineCore_DP0 pid=288725) 	setp.lt.s32 	%p13, %r72, %r20;
(EngineCore_DP0 pid=288725) 	.loc	1 322 53                        // quant_slide_tuned_Llama3.2-1B.py:322:53
(EngineCore_DP0 pid=288725) 	setp.lt.s32 	%p14, %r111, %r18;
(EngineCore_DP0 pid=288725) 	.loc	1 322 37                        // quant_slide_tuned_Llama3.2-1B.py:322:37
(EngineCore_DP0 pid=288725) 	and.pred 	%p9, %p13, %p14;
(EngineCore_DP0 pid=288725) 	.loc	1 321 29                        // quant_slide_tuned_Llama3.2-1B.py:321:29
(EngineCore_DP0 pid=288725) 	mad.wide.s32 	%rd8, %r111, 2, %rd1;
(EngineCore_DP0 pid=288725) 	mov.b16 	%rs13, 0;
(EngineCore_DP0 pid=288725) 	.loc	1 321 21                        // quant_slide_tuned_Llama3.2-1B.py:321:21
(EngineCore_DP0 pid=288725) 	// begin inline asm
(EngineCore_DP0 pid=288725) 	mov.u16 %rs12, %rs13;
(EngineCore_DP0 pid=288725) 	@%p9 ld.global.b16 { %rs12 }, [ %rd8 + 0 ];
(EngineCore_DP0 pid=288725) 	// end inline asm
(EngineCore_DP0 pid=288725) 	.loc	1 322 79                        // quant_slide_tuned_Llama3.2-1B.py:322:79
(EngineCore_DP0 pid=288725) 	cvt.f32.bf16 	%r73, %rs12;
(EngineCore_DP0 pid=288725) 	.loc	1 324 53                        // quant_slide_tuned_Llama3.2-1B.py:324:53
(EngineCore_DP0 pid=288725) 	add.s32 	%r74, %r111, 1;
(EngineCore_DP0 pid=288725) 	setp.lt.s32 	%p15, %r74, %r18;
(EngineCore_DP0 pid=288725) 	.loc	1 324 37                        // quant_slide_tuned_Llama3.2-1B.py:324:37
(EngineCore_DP0 pid=288725) 	and.pred 	%p10, %p13, %p15;
(EngineCore_DP0 pid=288725) 	.loc	1 323 39                        // quant_slide_tuned_Llama3.2-1B.py:323:39
(EngineCore_DP0 pid=288725) 	add.s64 	%rd9, %rd8, 2;
(EngineCore_DP0 pid=288725) 	.loc	1 323 21                        // quant_slide_tuned_Llama3.2-1B.py:323:21
(EngineCore_DP0 pid=288725) 	// begin inline asm
(EngineCore_DP0 pid=288725) 	mov.u16 %rs14, %rs13;
(EngineCore_DP0 pid=288725) 	@%p10 ld.global.b16 { %rs14 }, [ %rd9 + 0 ];
(EngineCore_DP0 pid=288725) 	// end inline asm
(EngineCore_DP0 pid=288725) 	.loc	1 324 79                        // quant_slide_tuned_Llama3.2-1B.py:324:79
(EngineCore_DP0 pid=288725) 	cvt.f32.bf16 	%r75, %rs14;
(EngineCore_DP0 pid=288725) 	.loc	1 326 53                        // quant_slide_tuned_Llama3.2-1B.py:326:53
(EngineCore_DP0 pid=288725) 	add.s32 	%r76, %r111, 2;
(EngineCore_DP0 pid=288725) 	setp.lt.s32 	%p16, %r76, %r18;
(EngineCore_DP0 pid=288725) 	.loc	1 326 37                        // quant_slide_tuned_Llama3.2-1B.py:326:37
(EngineCore_DP0 pid=288725) 	and.pred 	%p11, %p13, %p16;
(EngineCore_DP0 pid=288725) 	.loc	1 325 39                        // quant_slide_tuned_Llama3.2-1B.py:325:39
(EngineCore_DP0 pid=288725) 	add.s64 	%rd10, %rd8, 4;
(EngineCore_DP0 pid=288725) 	.loc	1 325 21                        // quant_slide_tuned_Llama3.2-1B.py:325:21
(EngineCore_DP0 pid=288725) 	// begin inline asm
(EngineCore_DP0 pid=288725) 	mov.u16 %rs16, %rs13;
(EngineCore_DP0 pid=288725) 	@%p11 ld.global.b16 { %rs16 }, [ %rd10 + 0 ];
(EngineCore_DP0 pid=288725) 	// end inline asm
(EngineCore_DP0 pid=288725) 	.loc	1 326 79                        // quant_slide_tuned_Llama3.2-1B.py:326:79
(EngineCore_DP0 pid=288725) 	cvt.f32.bf16 	%r77, %rs16;
(EngineCore_DP0 pid=288725) 	.loc	1 328 53                        // quant_slide_tuned_Llama3.2-1B.py:328:53
(EngineCore_DP0 pid=288725) 	add.s32 	%r78, %r111, 3;
(EngineCore_DP0 pid=288725) 	setp.lt.s32 	%p17, %r78, %r18;
(EngineCore_DP0 pid=288725) 	.loc	1 328 37                        // quant_slide_tuned_Llama3.2-1B.py:328:37
(EngineCore_DP0 pid=288725) 	and.pred 	%p12, %p13, %p17;
(EngineCore_DP0 pid=288725) 	.loc	1 327 39                        // quant_slide_tuned_Llama3.2-1B.py:327:39
(EngineCore_DP0 pid=288725) 	add.s64 	%rd11, %rd8, 6;
(EngineCore_DP0 pid=288725) 	.loc	1 327 21                        // quant_slide_tuned_Llama3.2-1B.py:327:21
(EngineCore_DP0 pid=288725) 	// begin inline asm
(EngineCore_DP0 pid=288725) 	mov.u16 %rs18, %rs13;
(EngineCore_DP0 pid=288725) 	@%p12 ld.global.b16 { %rs18 }, [ %rd11 + 0 ];
(EngineCore_DP0 pid=288725) 	// end inline asm
(EngineCore_DP0 pid=288725) 	.loc	1 328 79                        // quant_slide_tuned_Llama3.2-1B.py:328:79
(EngineCore_DP0 pid=288725) 	cvt.f32.bf16 	%r79, %rs18;
(EngineCore_DP0 pid=288725) 	.loc	1 330 56                        // quant_slide_tuned_Llama3.2-1B.py:330:56
(EngineCore_DP0 pid=288725) 	mul.f32 	%r80, %r13, %r73;
(EngineCore_DP0 pid=288725) 	.loc	1 330 51                        // quant_slide_tuned_Llama3.2-1B.py:330:51
(EngineCore_DP0 pid=288725) 	cvt.rni.f32.f32 	%r81, %r80;
(EngineCore_DP0 pid=288725) 	.loc	1 330 76                        // quant_slide_tuned_Llama3.2-1B.py:330:76
(EngineCore_DP0 pid=288725) 	max.f32 	%r82, %r81, 0fC3000000;
(EngineCore_DP0 pid=288725) 	min.f32 	%r83, %r82, 0f42FE0000;
(EngineCore_DP0 pid=288725) 	.loc	1 330 86                        // quant_slide_tuned_Llama3.2-1B.py:330:86
(EngineCore_DP0 pid=288725) 	cvt.rzi.s32.f32 	%r84, %r83;
(EngineCore_DP0 pid=288725) 	.loc	1 330 98                        // quant_slide_tuned_Llama3.2-1B.py:330:98
(EngineCore_DP0 pid=288725) 	and.b32 	%r85, %r84, 255;
(EngineCore_DP0 pid=288725) 	.loc	1 331 56                        // quant_slide_tuned_Llama3.2-1B.py:331:56
(EngineCore_DP0 pid=288725) 	mul.f32 	%r86, %r13, %r75;
(EngineCore_DP0 pid=288725) 	.loc	1 331 51                        // quant_slide_tuned_Llama3.2-1B.py:331:51
(EngineCore_DP0 pid=288725) 	cvt.rni.f32.f32 	%r87, %r86;
(EngineCore_DP0 pid=288725) 	.loc	1 332 56                        // quant_slide_tuned_Llama3.2-1B.py:332:56
(EngineCore_DP0 pid=288725) 	mul.f32 	%r88, %r13, %r77;
(EngineCore_DP0 pid=288725) 	.loc	1 332 51                        // quant_slide_tuned_Llama3.2-1B.py:332:51
(EngineCore_DP0 pid=288725) 	cvt.rni.f32.f32 	%r89, %r88;
(EngineCore_DP0 pid=288725) 	.loc	1 333 56                        // quant_slide_tuned_Llama3.2-1B.py:333:56
(EngineCore_DP0 pid=288725) 	mul.f32 	%r90, %r13, %r79;
(EngineCore_DP0 pid=288725) 	.loc	1 333 51                        // quant_slide_tuned_Llama3.2-1B.py:333:51
(EngineCore_DP0 pid=288725) 	cvt.rni.f32.f32 	%r91, %r90;
(EngineCore_DP0 pid=288725) 	.loc	1 333 76                        // quant_slide_tuned_Llama3.2-1B.py:333:76
(EngineCore_DP0 pid=288725) 	max.f32 	%r92, %r91, 0fC3000000;
(EngineCore_DP0 pid=288725) 	min.f32 	%r93, %r92, 0f42FE0000;
(EngineCore_DP0 pid=288725) 	.loc	1 333 86                        // quant_slide_tuned_Llama3.2-1B.py:333:86
(EngineCore_DP0 pid=288725) 	cvt.rzi.s32.f32 	%r94, %r93;
(EngineCore_DP0 pid=288725) 	.loc	1 331 76                        // quant_slide_tuned_Llama3.2-1B.py:331:76
(EngineCore_DP0 pid=288725) 	max.f32 	%r95, %r89, 0fC3000000;
(EngineCore_DP0 pid=288725) 	max.f32 	%r96, %r87, 0fC3000000;
(EngineCore_DP0 pid=288725) 	min.f32 	%r97, %r96, 0f42FE0000;
(EngineCore_DP0 pid=288725) 	min.f32 	%r98, %r95, 0f42FE0000;
(EngineCore_DP0 pid=288725) 	.loc	1 331 86                        // quant_slide_tuned_Llama3.2-1B.py:331:86
(EngineCore_DP0 pid=288725) 	cvt.rzi.s32.f32 	%r99, %r98;
(EngineCore_DP0 pid=288725) 	cvt.rzi.s32.f32 	%r100, %r97;
(EngineCore_DP0 pid=288725) 	.loc	1 335 30                        // quant_slide_tuned_Llama3.2-1B.py:335:30
(EngineCore_DP0 pid=288725) 	shl.b32 	%r101, %r100, 8;
(EngineCore_DP0 pid=288725) 	shl.b32 	%r102, %r99, 16;
(EngineCore_DP0 pid=288725) 	and.b32 	%r103, %r102, 16711680;
(EngineCore_DP0 pid=288725) 	and.b32 	%r104, %r101, 65280;
(EngineCore_DP0 pid=288725) 	.loc	1 335 24                        // quant_slide_tuned_Llama3.2-1B.py:335:24
(EngineCore_DP0 pid=288725) 	or.b32 	%r105, %r104, %r85;
(EngineCore_DP0 pid=288725) 	.loc	1 335 36                        // quant_slide_tuned_Llama3.2-1B.py:335:36
(EngineCore_DP0 pid=288725) 	or.b32 	%r106, %r105, %r103;
(EngineCore_DP0 pid=288725) 	.loc	1 335 55                        // quant_slide_tuned_Llama3.2-1B.py:335:55
(EngineCore_DP0 pid=288725) 	shl.b32 	%r107, %r94, 24;
(EngineCore_DP0 pid=288725) 	.loc	1 335 49                        // quant_slide_tuned_Llama3.2-1B.py:335:49
(EngineCore_DP0 pid=288725) 	or.b32 	%r71, %r106, %r107;
(EngineCore_DP0 pid=288725) 	.loc	1 336 29                        // quant_slide_tuned_Llama3.2-1B.py:336:29
(EngineCore_DP0 pid=288725) 	mad.wide.s32 	%rd12, %r72, 4, %rd2;
(EngineCore_DP0 pid=288725) 	.loc	1 336 39                        // quant_slide_tuned_Llama3.2-1B.py:336:39
(EngineCore_DP0 pid=288725) 	// begin inline asm
(EngineCore_DP0 pid=288725) 	@%p13 st.global.b32 [ %rd12 + 0 ], { %r71 };
(EngineCore_DP0 pid=288725) 	// end inline asm
(EngineCore_DP0 pid=288725) 	.loc	1 312 41                        // quant_slide_tuned_Llama3.2-1B.py:312:41
(EngineCore_DP0 pid=288725) 	add.s32 	%r112, %r112, 1024;
(EngineCore_DP0 pid=288725) 	add.s32 	%r111, %r111, 4096;
(EngineCore_DP0 pid=288725) 	setp.lt.s32 	%p18, %r112, %r20;
(EngineCore_DP0 pid=288725) 	@%p18 bra 	$L__BB0_6;
(EngineCore_DP0 pid=288725) $L__BB0_7:                              // %._crit_edge5
(EngineCore_DP0 pid=288725) 	.loc	1 312 4                         // quant_slide_tuned_Llama3.2-1B.py:312:4
(EngineCore_DP0 pid=288725) 	ret;
(EngineCore_DP0 pid=288725) $L__tmp3:
(EngineCore_DP0 pid=288725) $L__func_end0:
(EngineCore_DP0 pid=288725)                                         // -- End function
(EngineCore_DP0 pid=288725) }
(EngineCore_DP0 pid=288725) 	.file	1 "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/GB10_cc121_py312_cu129_aarch64/quant_slide_tuned_Llama3.2-1B.py"
(EngineCore_DP0 pid=288725) 	.file	2 "/usr/local/lib/python3.12/dist-packages/triton/language/standard.py"
(EngineCore_DP0 pid=288725) 	.section	.debug_abbrev
(EngineCore_DP0 pid=288725) 	{
(EngineCore_DP0 pid=288725) .b8 1                                   // Abbreviation Code
(EngineCore_DP0 pid=288725) .b8 17                                  // DW_TAG_compile_unit
(EngineCore_DP0 pid=288725) .b8 1                                   // DW_CHILDREN_yes
(EngineCore_DP0 pid=288725) .b8 37                                  // DW_AT_producer
(EngineCore_DP0 pid=288725) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=288725) .b8 19                                  // DW_AT_language
(EngineCore_DP0 pid=288725) .b8 5                                   // DW_FORM_data2
(EngineCore_DP0 pid=288725) .b8 3                                   // DW_AT_name
(EngineCore_DP0 pid=288725) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=288725) .b8 16                                  // DW_AT_stmt_list
(EngineCore_DP0 pid=288725) .b8 6                                   // DW_FORM_data4
(EngineCore_DP0 pid=288725) .b8 27                                  // DW_AT_comp_dir
(EngineCore_DP0 pid=288725) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=288725) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=288725) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=288725) .b8 2                                   // Abbreviation Code
(EngineCore_DP0 pid=288725) .b8 46                                  // DW_TAG_subprogram
(EngineCore_DP0 pid=288725) .b8 0                                   // DW_CHILDREN_no
(EngineCore_DP0 pid=288725) .b8 3                                   // DW_AT_name
(EngineCore_DP0 pid=288725) .b8 8                                   // DW_FORM_string
(EngineCore_DP0 pid=288725) .b8 32                                  // DW_AT_inline
(EngineCore_DP0 pid=288725) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=288725) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=288725) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=288725) .b8 3                                   // Abbreviation Code
(EngineCore_DP0 pid=288725) .b8 46                                  // DW_TAG_subprogram
(EngineCore_DP0 pid=288725) .b8 1                                   // DW_CHILDREN_yes
(EngineCore_DP0 pid=288725) .b8 17                                  // DW_AT_low_pc
(EngineCore_DP0 pid=288725) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=288725) .b8 18                                  // DW_AT_high_pc
(EngineCore_DP0 pid=288725) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=288725) .b8 49                                  // DW_AT_abstract_origin
(EngineCore_DP0 pid=288725) .b8 19                                  // DW_FORM_ref4
(EngineCore_DP0 pid=288725) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=288725) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=288725) .b8 4                                   // Abbreviation Code
(EngineCore_DP0 pid=288725) .b8 29                                  // DW_TAG_inlined_subroutine
(EngineCore_DP0 pid=288725) .b8 0                                   // DW_CHILDREN_no
(EngineCore_DP0 pid=288725) .b8 49                                  // DW_AT_abstract_origin
(EngineCore_DP0 pid=288725) .b8 19                                  // DW_FORM_ref4
(EngineCore_DP0 pid=288725) .b8 17                                  // DW_AT_low_pc
(EngineCore_DP0 pid=288725) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=288725) .b8 18                                  // DW_AT_high_pc
(EngineCore_DP0 pid=288725) .b8 1                                   // DW_FORM_addr
(EngineCore_DP0 pid=288725) .b8 88                                  // DW_AT_call_file
(EngineCore_DP0 pid=288725) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=288725) .b8 89                                  // DW_AT_call_line
(EngineCore_DP0 pid=288725) .b8 5                                   // DW_FORM_data2
(EngineCore_DP0 pid=288725) .b8 87                                  // DW_AT_call_column
(EngineCore_DP0 pid=288725) .b8 11                                  // DW_FORM_data1
(EngineCore_DP0 pid=288725) .b8 0                                   // EOM(1)
(EngineCore_DP0 pid=288725) .b8 0                                   // EOM(2)
(EngineCore_DP0 pid=288725) .b8 0                                   // EOM(3)
(EngineCore_DP0 pid=288725) 	}
(EngineCore_DP0 pid=288725) 	.section	.debug_info
(EngineCore_DP0 pid=288725) 	{
(EngineCore_DP0 pid=288725) .b32 224                                // Length of Unit
(EngineCore_DP0 pid=288725) .b8 2                                   // DWARF version number
(EngineCore_DP0 pid=288725) .b8 0
(EngineCore_DP0 pid=288725) .b32 .debug_abbrev                      // Offset Into Abbrev. Section
(EngineCore_DP0 pid=288725) .b8 8                                   // Address Size (in bytes)
(EngineCore_DP0 pid=288725) .b8 1                                   // Abbrev [1] 0xb:0xd9 DW_TAG_compile_unit
(EngineCore_DP0 pid=288725) .b8 116                                 // DW_AT_producer
(EngineCore_DP0 pid=288725) .b8 114
(EngineCore_DP0 pid=288725) .b8 105
(EngineCore_DP0 pid=288725) .b8 116
(EngineCore_DP0 pid=288725) .b8 111
(EngineCore_DP0 pid=288725) .b8 110
(EngineCore_DP0 pid=288725) .b8 0
(EngineCore_DP0 pid=288725) .b8 2                                   // DW_AT_language
(EngineCore_DP0 pid=288725) .b8 0
(EngineCore_DP0 pid=288725) .b8 113                                 // DW_AT_name
(EngineCore_DP0 pid=288725) .b8 117
(EngineCore_DP0 pid=288725) .b8 97
(EngineCore_DP0 pid=288725) .b8 110
(EngineCore_DP0 pid=288725) .b8 116
(EngineCore_DP0 pid=288725) .b8 95
(EngineCore_DP0 pid=288725) .b8 115
(EngineCore_DP0 pid=288725) .b8 108
(EngineCore_DP0 pid=288725) .b8 105
(EngineCore_DP0 pid=288725) .b8 100
(EngineCore_DP0 pid=288725) .b8 101
(EngineCore_DP0 pid=288725) .b8 95
(EngineCore_DP0 pid=288725) .b8 116
(EngineCore_DP0 pid=288725) .b8 117
(EngineCore_DP0 pid=288725) .b8 110
(EngineCore_DP0 pid=288725) .b8 101
(EngineCore_DP0 pid=288725) .b8 100
(EngineCore_DP0 pid=288725) .b8 95
(EngineCore_DP0 pid=288725) .b8 76
(EngineCore_DP0 pid=288725) .b8 108
(EngineCore_DP0 pid=288725) .b8 97
(EngineCore_DP0 pid=288725) .b8 109
(EngineCore_DP0 pid=288725) .b8 97
(EngineCore_DP0 pid=288725) .b8 51
(EngineCore_DP0 pid=288725) .b8 46
(EngineCore_DP0 pid=288725) .b8 50
(EngineCore_DP0 pid=288725) .b8 45
(EngineCore_DP0 pid=288725) .b8 49
(EngineCore_DP0 pid=288725) .b8 66
(EngineCore_DP0 pid=288725) .b8 46
(EngineCore_DP0 pid=288725) .b8 112
(EngineCore_DP0 pid=288725) .b8 121
(EngineCore_DP0 pid=288725) .b8 0
(EngineCore_DP0 pid=288725) .b32 .debug_line                        // DW_AT_stmt_list
(EngineCore_DP0 pid=288725) .b8 47                                  // DW_AT_comp_dir
(EngineCore_DP0 pid=288725) .b8 114
(EngineCore_DP0 pid=288725) .b8 111
(EngineCore_DP0 pid=288725) .b8 111
(EngineCore_DP0 pid=288725) .b8 116
(EngineCore_DP0 pid=288725) .b8 47
(EngineCore_DP0 pid=288725) .b8 118
(EngineCore_DP0 pid=288725) .b8 108
(EngineCore_DP0 pid=288725) .b8 108
(EngineCore_DP0 pid=288725) .b8 109
(EngineCore_DP0 pid=288725) .b8 98
(EngineCore_DP0 pid=288725) .b8 101
(EngineCore_DP0 pid=288725) .b8 110
(EngineCore_DP0 pid=288725) .b8 99
(EngineCore_DP0 pid=288725) .b8 104
(EngineCore_DP0 pid=288725) .b8 47
(EngineCore_DP0 pid=288725) .b8 115
(EngineCore_DP0 pid=288725) .b8 108
(EngineCore_DP0 pid=288725) .b8 105
(EngineCore_DP0 pid=288725) .b8 100
(EngineCore_DP0 pid=288725) .b8 101
(EngineCore_DP0 pid=288725) .b8 115
(EngineCore_DP0 pid=288725) .b8 112
(EngineCore_DP0 pid=288725) .b8 97
(EngineCore_DP0 pid=288725) .b8 114
(EngineCore_DP0 pid=288725) .b8 115
(EngineCore_DP0 pid=288725) .b8 101
(EngineCore_DP0 pid=288725) .b8 47
(EngineCore_DP0 pid=288725) .b8 99
(EngineCore_DP0 pid=288725) .b8 115
(EngineCore_DP0 pid=288725) .b8 114
(EngineCore_DP0 pid=288725) .b8 99
(EngineCore_DP0 pid=288725) .b8 47
(EngineCore_DP0 pid=288725) .b8 102
(EngineCore_DP0 pid=288725) .b8 117
(EngineCore_DP0 pid=288725) .b8 115
(EngineCore_DP0 pid=288725) .b8 101
(EngineCore_DP0 pid=288725) .b8 100
(EngineCore_DP0 pid=288725) .b8 95
(EngineCore_DP0 pid=288725) .b8 113
(EngineCore_DP0 pid=288725) .b8 117
(EngineCore_DP0 pid=288725) .b8 97
(EngineCore_DP0 pid=288725) .b8 110
(EngineCore_DP0 pid=288725) .b8 116
(EngineCore_DP0 pid=288725) .b8 95
(EngineCore_DP0 pid=288725) .b8 115
(EngineCore_DP0 pid=288725) .b8 108
(EngineCore_DP0 pid=288725) .b8 105
(EngineCore_DP0 pid=288725) .b8 100
(EngineCore_DP0 pid=288725) .b8 101
(EngineCore_DP0 pid=288725) .b8 95
(EngineCore_DP0 pid=288725) .b8 116
(EngineCore_DP0 pid=288725) .b8 114
(EngineCore_DP0 pid=288725) .b8 105
(EngineCore_DP0 pid=288725) .b8 116
(EngineCore_DP0 pid=288725) .b8 111
(EngineCore_DP0 pid=288725) .b8 110
(EngineCore_DP0 pid=288725) .b8 47
(EngineCore_DP0 pid=288725) .b8 98
(EngineCore_DP0 pid=288725) .b8 117
(EngineCore_DP0 pid=288725) .b8 105
(EngineCore_DP0 pid=288725) .b8 108
(EngineCore_DP0 pid=288725) .b8 100
(EngineCore_DP0 pid=288725) .b8 47
(EngineCore_DP0 pid=288725) .b8 71
(EngineCore_DP0 pid=288725) .b8 66
(EngineCore_DP0 pid=288725) .b8 49
(EngineCore_DP0 pid=288725) .b8 48
(EngineCore_DP0 pid=288725) .b8 95
(EngineCore_DP0 pid=288725) .b8 99
(EngineCore_DP0 pid=288725) .b8 99
(EngineCore_DP0 pid=288725) .b8 49
(EngineCore_DP0 pid=288725) .b8 50
(EngineCore_DP0 pid=288725) .b8 49
(EngineCore_DP0 pid=288725) .b8 95
(EngineCore_DP0 pid=288725) .b8 112
(EngineCore_DP0 pid=288725) .b8 121
(EngineCore_DP0 pid=288725) .b8 51
(EngineCore_DP0 pid=288725) .b8 49
(EngineCore_DP0 pid=288725) .b8 50
(EngineCore_DP0 pid=288725) .b8 95
(EngineCore_DP0 pid=288725) .b8 99
(EngineCore_DP0 pid=288725) .b8 117
(EngineCore_DP0 pid=288725) .b8 49
(EngineCore_DP0 pid=288725) .b8 50
(EngineCore_DP0 pid=288725) .b8 57
(EngineCore_DP0 pid=288725) .b8 95
(EngineCore_DP0 pid=288725) .b8 97
(EngineCore_DP0 pid=288725) .b8 97
(EngineCore_DP0 pid=288725) .b8 114
(EngineCore_DP0 pid=288725) .b8 99
(EngineCore_DP0 pid=288725) .b8 104
(EngineCore_DP0 pid=288725) .b8 54
(EngineCore_DP0 pid=288725) .b8 52
(EngineCore_DP0 pid=288725) .b8 0
(EngineCore_DP0 pid=288725) .b8 2                                   // Abbrev [2] 0x99:0x1b DW_TAG_subprogram
(EngineCore_DP0 pid=288725) .b8 95                                  // DW_AT_name
(EngineCore_DP0 pid=288725) .b8 113
(EngineCore_DP0 pid=288725) .b8 117
(EngineCore_DP0 pid=288725) .b8 97
(EngineCore_DP0 pid=288725) .b8 110
(EngineCore_DP0 pid=288725) .b8 116
(EngineCore_DP0 pid=288725) .b8 95
(EngineCore_DP0 pid=288725) .b8 115
(EngineCore_DP0 pid=288725) .b8 108
(EngineCore_DP0 pid=288725) .b8 105
(EngineCore_DP0 pid=288725) .b8 100
(EngineCore_DP0 pid=288725) .b8 101
(EngineCore_DP0 pid=288725) .b8 95
(EngineCore_DP0 pid=288725) .b8 105
(EngineCore_DP0 pid=288725) .b8 110
(EngineCore_DP0 pid=288725) .b8 116
(EngineCore_DP0 pid=288725) .b8 56
(EngineCore_DP0 pid=288725) .b8 95
(EngineCore_DP0 pid=288725) .b8 107
(EngineCore_DP0 pid=288725) .b8 101
(EngineCore_DP0 pid=288725) .b8 114
(EngineCore_DP0 pid=288725) .b8 110
(EngineCore_DP0 pid=288725) .b8 101
(EngineCore_DP0 pid=288725) .b8 108
(EngineCore_DP0 pid=288725) .b8 0
(EngineCore_DP0 pid=288725) .b8 1                                   // DW_AT_inline
(EngineCore_DP0 pid=288725) .b8 3                                   // Abbrev [3] 0xb4:0x2f DW_TAG_subprogram
(EngineCore_DP0 pid=288725) .b64 $L__func_begin0                    // DW_AT_low_pc
(EngineCore_DP0 pid=288725) .b64 $L__func_end0                      // DW_AT_high_pc
(EngineCore_DP0 pid=288725) .b32 153                                // DW_AT_abstract_origin
(EngineCore_DP0 pid=288725) .b8 4                                   // Abbrev [4] 0xc9:0x19 DW_TAG_inlined_subroutine
(EngineCore_DP0 pid=288725) .b32 153                                // DW_AT_abstract_origin
(EngineCore_DP0 pid=288725) .b64 $L__tmp1                           // DW_AT_low_pc
(EngineCore_DP0 pid=288725) .b64 $L__tmp2                           // DW_AT_high_pc
(EngineCore_DP0 pid=288725) .b8 1                                   // DW_AT_call_file
(EngineCore_DP0 pid=288725) .b8 46                                  // DW_AT_call_line
(EngineCore_DP0 pid=288725) .b8 1
(EngineCore_DP0 pid=288725) .b8 43                                  // DW_AT_call_column
(EngineCore_DP0 pid=288725) .b8 0                                   // End Of Children Mark
(EngineCore_DP0 pid=288725) .b8 0                                   // End Of Children Mark
(EngineCore_DP0 pid=288725) 	}
(EngineCore_DP0 pid=288725) 	.section	.debug_macinfo	{	}
(EngineCore_DP0 pid=288725) 
(EngineCore_DP0 pid=288725) ================================================================
(EngineCore_DP0 pid=288725) please share the reproducer above with Triton project.
(EngineCore_DP0 pid=288725) 
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 468, in make_cubin
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]     subprocess.run(ptxas_cmd, check=True, close_fds=False, stderr=flog)
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]   File "/usr/lib/python3.12/subprocess.py", line 571, in run
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]     raise CalledProcessError(retcode, process.args,
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866] subprocess.CalledProcessError: Command '['/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas', '-lineinfo', '-v', '--gpu-name=sm_121a', '/tmp/tmpca1zryxw.ptx', '-o', '/tmp/tmpca1zryxw.ptx.o']' returned non-zero exit status 255.
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866] 
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866] During handling of the above exception, another exception occurred:
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866] 
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]     self.model_runner.profile_run()
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 623, in forward
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]     model_output = self.model(
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]                    ^^^^^^^^^^^
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 372, in __call__
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]     return self.forward(*args, **kwargs)
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 436, in forward
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]     hidden_states, residual = layer(positions, hidden_states, residual)
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 342, in forward
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]     hidden_states = self.self_attn(positions=positions, hidden_states=hidden_states)
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 240, in forward
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]     qkv, _ = self.qkv_proj(hidden_states)
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 565, in forward
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]     output_parallel = self.quant_method.apply(self, input_, bias)
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 957, in apply
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]     return scheme.apply_weights(layer, x, bias=bias)
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 745, in apply_weights
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]     return self.slidesparse_int8_linear.apply(
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 534, in apply
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]     return self._linear_fn(
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]            ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 289, in cuSPARSELt_INT8_linear
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]     qinput, scale_a_pad = quant_slide_int8_kernel(input, model_name, L)
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 432, in quant_slide_int8_kernel
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]     return torch.ops.slidesparse.quant_slide_int8(input, model_name, L)
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 1255, in __call__
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 564, in _quant_slide_int8_impl
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]     return fn(input, L)
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]            ^^^^^^^^^^^^
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]   File "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/GB10_cc121_py312_cu129_aarch64/quant_slide_tuned_Llama3.2-1B.py", line 365, in quant_slide_int8_triton
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]     _quant_slide_int8_kernel[(M,)](
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 733, in run
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]     kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 861, in _do_compile
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]     kernel = self.compile(src, target=target, options=options.__dict__)
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 320, in compile
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]     next_module = compile_ir(module, metadata)
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 520, in <lambda>
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]     stages["cubin"] = lambda src, metadata: self.make_cubin(src, metadata, options, self.target.arch)
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 503, in make_cubin
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866]     raise PTXASError(error)
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866] triton.runtime.errors.PTXASError: PTXAS error: Internal Triton PTX codegen error
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866] `ptxas` stderr:
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866] ptxas fatal   : Value 'sm_121a' is not defined for option 'gpu-name'
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866] 
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866] Repro command: /usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas -lineinfo -v --gpu-name=sm_121a /tmp/tmpca1zryxw.ptx -o /tmp/tmpca1zryxw.ptx.o
(EngineCore_DP0 pid=288725) ERROR 01-25 18:39:42 [core.py:866] 

STDERR:
[2026-01-25 18:39:28] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-25 18:39:28] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 18:39:28] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 18:39:28] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:39:28] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:39:28] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:39:28] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:39:28] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:39:28] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 18:39:28] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:39:28] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:39:28] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:39:28] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:39:28] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 18:39:31] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-25 18:39:31] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-25 18:39:31] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 18:39:31] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:39:31] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:39:31] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:39:31] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:39:31] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 18:39:31] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-25 18:39:31] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:39:31] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:39:31] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:39:31] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:39:31] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=288725) [2026-01-25 18:39:32] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=288725) [2026-01-25 18:39:32] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=288725) [2026-01-25 18:39:32] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=288725) [2026-01-25 18:39:32] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=288725) [2026-01-25 18:39:32] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=288725) [2026-01-25 18:39:32] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=288725) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=288725) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.28s/it]
(EngineCore_DP0 pid=288725) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.28s/it]
(EngineCore_DP0 pid=288725) 
(EngineCore_DP0 pid=288725) [2026-01-25 18:39:41] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2048] -> 1D uint8
(EngineCore_DP0 pid=288725) [2026-01-25 18:39:41] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4718592 bytes
(EngineCore_DP0 pid=288725) [2026-01-25 18:39:41] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2048] -> 1D uint8
(EngineCore_DP0 pid=288725) [2026-01-25 18:39:41] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 3145728 bytes
(EngineCore_DP0 pid=288725) [2026-01-25 18:39:41] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2048] -> 1D uint8
(EngineCore_DP0 pid=288725) [2026-01-25 18:39:41] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 25165824 bytes
(EngineCore_DP0 pid=288725) [2026-01-25 18:39:41] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 8192] -> 1D uint8
(EngineCore_DP0 pid=288725) [2026-01-25 18:39:41] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12582912 bytes
(EngineCore_DP0 pid=288725) Process EngineCore_DP0:
(EngineCore_DP0 pid=288725) Traceback (most recent call last):
(EngineCore_DP0 pid=288725)   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 468, in make_cubin
(EngineCore_DP0 pid=288725)     subprocess.run(ptxas_cmd, check=True, close_fds=False, stderr=flog)
(EngineCore_DP0 pid=288725)   File "/usr/lib/python3.12/subprocess.py", line 571, in run
(EngineCore_DP0 pid=288725)     raise CalledProcessError(retcode, process.args,
(EngineCore_DP0 pid=288725) subprocess.CalledProcessError: Command '['/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas', '-lineinfo', '-v', '--gpu-name=sm_121a', '/tmp/tmpca1zryxw.ptx', '-o', '/tmp/tmpca1zryxw.ptx.o']' returned non-zero exit status 255.
(EngineCore_DP0 pid=288725) 
(EngineCore_DP0 pid=288725) During handling of the above exception, another exception occurred:
(EngineCore_DP0 pid=288725) 
(EngineCore_DP0 pid=288725) Traceback (most recent call last):
(EngineCore_DP0 pid=288725)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=288725)     self.run()
(EngineCore_DP0 pid=288725)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=288725)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=288725)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=288725)     raise e
(EngineCore_DP0 pid=288725)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=288725)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=288725)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288725)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=288725)     super().__init__(
(EngineCore_DP0 pid=288725)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=288725)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=288725)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288725)   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=288725)     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=288725)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288725)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=288725)     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=288725)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288725)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=288725)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=288725)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288725)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=288725)     return func(*args, **kwargs)
(EngineCore_DP0 pid=288725)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288725)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=288725)     return func(*args, **kwargs)
(EngineCore_DP0 pid=288725)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288725)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=288725)     self.model_runner.profile_run()
(EngineCore_DP0 pid=288725)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=288725)     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=288725)                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288725)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=288725)     return func(*args, **kwargs)
(EngineCore_DP0 pid=288725)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288725)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=288725)     outputs = self.model(
(EngineCore_DP0 pid=288725)               ^^^^^^^^^^^
(EngineCore_DP0 pid=288725)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=288725)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=288725)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288725)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=288725)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=288725)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288725)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 623, in forward
(EngineCore_DP0 pid=288725)     model_output = self.model(
(EngineCore_DP0 pid=288725)                    ^^^^^^^^^^^
(EngineCore_DP0 pid=288725)   File "/root/vllmbench/vllm/compilation/decorators.py", line 372, in __call__
(EngineCore_DP0 pid=288725)     return self.forward(*args, **kwargs)
(EngineCore_DP0 pid=288725)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288725)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 436, in forward
(EngineCore_DP0 pid=288725)     hidden_states, residual = layer(positions, hidden_states, residual)
(EngineCore_DP0 pid=288725)                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288725)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=288725)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=288725)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288725)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=288725)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=288725)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288725)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 342, in forward
(EngineCore_DP0 pid=288725)     hidden_states = self.self_attn(positions=positions, hidden_states=hidden_states)
(EngineCore_DP0 pid=288725)                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288725)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=288725)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=288725)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288725)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=288725)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=288725)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288725)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 240, in forward
(EngineCore_DP0 pid=288725)     qkv, _ = self.qkv_proj(hidden_states)
(EngineCore_DP0 pid=288725)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288725)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=288725)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=288725)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288725)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=288725)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=288725)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288725)   File "/root/vllmbench/vllm/model_executor/layers/linear.py", line 565, in forward
(EngineCore_DP0 pid=288725)     output_parallel = self.quant_method.apply(self, input_, bias)
(EngineCore_DP0 pid=288725)                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288725)   File "/root/vllmbench/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py", line 957, in apply
(EngineCore_DP0 pid=288725)     return scheme.apply_weights(layer, x, bias=bias)
(EngineCore_DP0 pid=288725)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288725)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 745, in apply_weights
(EngineCore_DP0 pid=288725)     return self.slidesparse_int8_linear.apply(
(EngineCore_DP0 pid=288725)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288725)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 534, in apply
(EngineCore_DP0 pid=288725)     return self._linear_fn(
(EngineCore_DP0 pid=288725)            ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288725)   File "/root/vllmbench/slidesparse/core/SlideSparseLinearMethod_INT8.py", line 289, in cuSPARSELt_INT8_linear
(EngineCore_DP0 pid=288725)     qinput, scale_a_pad = quant_slide_int8_kernel(input, model_name, L)
(EngineCore_DP0 pid=288725)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288725)   File "/root/vllmbench/slidesparse/core/kernels.py", line 432, in quant_slide_int8_kernel
(EngineCore_DP0 pid=288725)     return torch.ops.slidesparse.quant_slide_int8(input, model_name, L)
(EngineCore_DP0 pid=288725)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288725)   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 1255, in __call__
(EngineCore_DP0 pid=288725)     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=288725)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288725)   File "/root/vllmbench/slidesparse/core/kernels.py", line 564, in _quant_slide_int8_impl
(EngineCore_DP0 pid=288725)     return fn(input, L)
(EngineCore_DP0 pid=288725)            ^^^^^^^^^^^^
(EngineCore_DP0 pid=288725)   File "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/GB10_cc121_py312_cu129_aarch64/quant_slide_tuned_Llama3.2-1B.py", line 365, in quant_slide_int8_triton
(EngineCore_DP0 pid=288725)     _quant_slide_int8_kernel[(M,)](
(EngineCore_DP0 pid=288725)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=288725)     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=288725)                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288725)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 733, in run
(EngineCore_DP0 pid=288725)     kernel = self._do_compile(key, signature, device, constexprs, options, attrs, warmup)
(EngineCore_DP0 pid=288725)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288725)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 861, in _do_compile
(EngineCore_DP0 pid=288725)     kernel = self.compile(src, target=target, options=options.__dict__)
(EngineCore_DP0 pid=288725)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288725)   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 320, in compile
(EngineCore_DP0 pid=288725)     next_module = compile_ir(module, metadata)
(EngineCore_DP0 pid=288725)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288725)   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 520, in <lambda>
(EngineCore_DP0 pid=288725)     stages["cubin"] = lambda src, metadata: self.make_cubin(src, metadata, options, self.target.arch)
(EngineCore_DP0 pid=288725)                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=288725)   File "/usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/compiler.py", line 503, in make_cubin
(EngineCore_DP0 pid=288725)     raise PTXASError(error)
(EngineCore_DP0 pid=288725) triton.runtime.errors.PTXASError: PTXAS error: Internal Triton PTX codegen error
(EngineCore_DP0 pid=288725) `ptxas` stderr:
(EngineCore_DP0 pid=288725) ptxas fatal   : Value 'sm_121a' is not defined for option 'gpu-name'
(EngineCore_DP0 pid=288725) 
(EngineCore_DP0 pid=288725) Repro command: /usr/local/lib/python3.12/dist-packages/triton/backends/nvidia/bin/ptxas -lineinfo -v --gpu-name=sm_121a /tmp/tmpca1zryxw.ptx -o /tmp/tmpca1zryxw.ptx.o
(EngineCore_DP0 pid=288725) 
[rank0]:[W125 18:39:42.679112141 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=256

========== M=16 ==========
Time: 2026-01-26 02:25:22
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4
Params: prompt_len=16, output_len=256, num_prompts=16, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4 --dataset-name random --input-len 16 --output-len 256 --num-prompts 16 --max-num-seqs 16 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_4/json/Llama3.2-1B-INT8_M16.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:25:25 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 02:25:25 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=739223) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=739223) WARNING 01-26 02:25:43 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 6.63 requests/s, 1803.14 total tokens/s, 1697.07 output tokens/s
Total num prompt tokens:  256
Total num output tokens:  4096

STDERR:
[2026-01-26 02:25:25] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:25:25] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 02:25:25] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 02:25:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:25:25] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:25:25] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:25:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:25:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:25:25] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 02:25:25] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:25:25] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:25:25] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:25:25] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:25:25] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:25:28] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:25:28] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 02:25:28] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 02:25:28] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:25:28] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:25:28] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:25:28] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:25:28] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:25:28] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 02:25:28] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:25:28] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:25:28] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:25:28] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:25:28] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=739223) [2026-01-26 02:25:29] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=739223) [2026-01-26 02:25:29] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=739223) [2026-01-26 02:25:29] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=739223) [2026-01-26 02:25:29] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=739223) [2026-01-26 02:25:29] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=739223) [2026-01-26 02:25:29] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=739223) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=739223) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.03s/it]
(EngineCore_DP0 pid=739223) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.03s/it]
(EngineCore_DP0 pid=739223) 
(EngineCore_DP0 pid=739223) [2026-01-26 02:25:38] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2048] -> 1D uint8
(EngineCore_DP0 pid=739223) [2026-01-26 02:25:38] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4718592 bytes
(EngineCore_DP0 pid=739223) [2026-01-26 02:25:38] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2048] -> 1D uint8
(EngineCore_DP0 pid=739223) [2026-01-26 02:25:38] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 3145728 bytes
(EngineCore_DP0 pid=739223) [2026-01-26 02:25:38] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2048] -> 1D uint8
(EngineCore_DP0 pid=739223) [2026-01-26 02:25:38] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 25165824 bytes
(EngineCore_DP0 pid=739223) [2026-01-26 02:25:38] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 8192] -> 1D uint8
(EngineCore_DP0 pid=739223) [2026-01-26 02:25:38] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12582912 bytes
(EngineCore_DP0 pid=739223) 2026-01-26 02:25:43,431 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=739223) 2026-01-26 02:25:43,439 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/16 [00:00<?, ?it/s]
Adding requests: 100%|| 16/16 [00:00<00:00, 8488.35it/s]

Processed prompts:   0%|          | 0/16 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   6%|         | 1/16 [00:02<00:35,  2.40s/it, est. speed input: 6.67 toks/s, output: 106.73 toks/s]
Processed prompts: 100%|| 16/16 [00:02<00:00,  2.40s/it, est. speed input: 106.19 toks/s, output: 1699.04 toks/s]
Processed prompts: 100%|| 16/16 [00:02<00:00,  6.64it/s, est. speed input: 106.19 toks/s, output: 1699.04 toks/s]
[rank0]:[W126 02:25:46.748792624 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-26 02:25:48
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4
Params: prompt_len=16, output_len=256, num_prompts=128, max_num_seqs=128
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4 --dataset-name random --input-len 16 --output-len 256 --num-prompts 128 --max-num-seqs 128 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_4/json/Llama3.2-1B-INT8_M128.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:25:52 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 02:25:52 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=739770) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=739770) WARNING 01-26 02:26:10 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 26.31 requests/s, 7156.64 total tokens/s, 6735.66 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768

STDERR:
[2026-01-26 02:25:52] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:25:52] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 02:25:52] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 02:25:52] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:25:52] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:25:52] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:25:52] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:25:52] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:25:52] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 02:25:52] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:25:52] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:25:52] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:25:52] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:25:52] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:25:55] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:25:55] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 02:25:55] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 02:25:55] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:25:55] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:25:55] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:25:55] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:25:55] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:25:55] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 02:25:55] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:25:55] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:25:55] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:25:55] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:25:55] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=739770) [2026-01-26 02:25:56] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=739770) [2026-01-26 02:25:56] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=739770) [2026-01-26 02:25:56] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=739770) [2026-01-26 02:25:56] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=739770) [2026-01-26 02:25:56] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=739770) [2026-01-26 02:25:56] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=739770) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=739770) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.22s/it]
(EngineCore_DP0 pid=739770) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.22s/it]
(EngineCore_DP0 pid=739770) 
(EngineCore_DP0 pid=739770) [2026-01-26 02:26:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2048] -> 1D uint8
(EngineCore_DP0 pid=739770) [2026-01-26 02:26:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4718592 bytes
(EngineCore_DP0 pid=739770) [2026-01-26 02:26:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2048] -> 1D uint8
(EngineCore_DP0 pid=739770) [2026-01-26 02:26:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 3145728 bytes
(EngineCore_DP0 pid=739770) [2026-01-26 02:26:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2048] -> 1D uint8
(EngineCore_DP0 pid=739770) [2026-01-26 02:26:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 25165824 bytes
(EngineCore_DP0 pid=739770) [2026-01-26 02:26:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 8192] -> 1D uint8
(EngineCore_DP0 pid=739770) [2026-01-26 02:26:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12582912 bytes
(EngineCore_DP0 pid=739770) 2026-01-26 02:26:10,249 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=739770) 2026-01-26 02:26:10,257 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|| 128/128 [00:00<00:00, 13294.15it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:04<09:33,  4.52s/it, est. speed input: 3.54 toks/s, output: 56.65 toks/s]
Processed prompts:   2%|         | 2/128 [00:04<04:03,  1.93s/it, est. speed input: 6.90 toks/s, output: 110.33 toks/s]
Processed prompts:  48%|     | 62/128 [00:04<00:02, 25.41it/s, est. speed input: 208.86 toks/s, output: 3341.72 toks/s]
Processed prompts:  99%|| 127/128 [00:04<00:00, 60.22it/s, est. speed input: 418.59 toks/s, output: 6697.45 toks/s]
Processed prompts: 100%|| 128/128 [00:04<00:00, 60.22it/s, est. speed input: 421.88 toks/s, output: 6750.06 toks/s]
Processed prompts: 100%|| 128/128 [00:04<00:00, 26.37it/s, est. speed input: 421.88 toks/s, output: 6750.06 toks/s]
[rank0]:[W126 02:26:15.046349540 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-26 02:26:18
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4
Params: prompt_len=16, output_len=256, num_prompts=256, max_num_seqs=256
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4 --dataset-name random --input-len 16 --output-len 256 --num-prompts 256 --max-num-seqs 256 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_4/json/Llama3.2-1B-INT8_M256.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:26:21 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 02:26:21 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=740379) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=740379) WARNING 01-26 02:26:40 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 32.08 requests/s, 8724.69 total tokens/s, 8211.47 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536

STDERR:
[2026-01-26 02:26:21] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:26:21] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 02:26:21] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 02:26:21] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:26:21] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:26:21] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:26:21] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:26:21] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:26:21] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 02:26:21] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:26:21] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:26:21] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:26:21] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:26:21] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:26:24] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:26:24] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 02:26:24] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 02:26:24] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:26:24] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:26:24] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:26:24] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:26:24] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:26:24] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 02:26:24] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:26:24] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:26:24] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:26:24] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:26:24] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=740379) [2026-01-26 02:26:25] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=740379) [2026-01-26 02:26:25] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=740379) [2026-01-26 02:26:25] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=740379) [2026-01-26 02:26:25] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=740379) [2026-01-26 02:26:25] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=740379) [2026-01-26 02:26:25] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=740379) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=740379) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.08s/it]
(EngineCore_DP0 pid=740379) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.08s/it]
(EngineCore_DP0 pid=740379) 
(EngineCore_DP0 pid=740379) [2026-01-26 02:26:34] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2048] -> 1D uint8
(EngineCore_DP0 pid=740379) [2026-01-26 02:26:34] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4718592 bytes
(EngineCore_DP0 pid=740379) [2026-01-26 02:26:34] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2048] -> 1D uint8
(EngineCore_DP0 pid=740379) [2026-01-26 02:26:34] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 3145728 bytes
(EngineCore_DP0 pid=740379) [2026-01-26 02:26:34] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2048] -> 1D uint8
(EngineCore_DP0 pid=740379) [2026-01-26 02:26:34] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 25165824 bytes
(EngineCore_DP0 pid=740379) [2026-01-26 02:26:34] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 8192] -> 1D uint8
(EngineCore_DP0 pid=740379) [2026-01-26 02:26:34] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12582912 bytes
(EngineCore_DP0 pid=740379) 2026-01-26 02:26:39,453 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=740379) 2026-01-26 02:26:39,461 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|| 256/256 [00:00<00:00, 13656.84it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:06<29:36,  6.97s/it, est. speed input: 2.30 toks/s, output: 36.74 toks/s]
Processed prompts:  13%|        | 34/256 [00:07<00:32,  6.74it/s, est. speed input: 76.73 toks/s, output: 1227.60 toks/s]
Processed prompts:  29%|       | 75/256 [00:07<00:10, 17.94it/s, est. speed input: 166.87 toks/s, output: 2669.97 toks/s]
Processed prompts:  46%|     | 118/256 [00:07<00:04, 33.27it/s, est. speed input: 257.87 toks/s, output: 4125.90 toks/s]
Processed prompts:  59%|    | 152/256 [00:07<00:02, 48.88it/s, est. speed input: 327.55 toks/s, output: 5240.78 toks/s]
Processed prompts:  71%|  | 183/256 [00:07<00:01, 66.44it/s, est. speed input: 388.90 toks/s, output: 6222.41 toks/s]
Processed prompts:  84%| | 214/256 [00:07<00:00, 86.60it/s, est. speed input: 447.79 toks/s, output: 7164.66 toks/s]
Processed prompts:  95%|| 244/256 [00:07<00:00, 104.55it/s, est. speed input: 500.83 toks/s, output: 8013.22 toks/s]
Processed prompts: 100%|| 256/256 [00:07<00:00, 104.55it/s, est. speed input: 514.48 toks/s, output: 8231.66 toks/s]
Processed prompts: 100%|| 256/256 [00:07<00:00, 32.15it/s, est. speed input: 514.48 toks/s, output: 8231.66 toks/s] 
[rank0]:[W126 02:26:48.391072798 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=64 ==========
Time: 2026-01-27 16:51:35
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4
Params: prompt_len=16, output_len=256, num_prompts=64, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4 --dataset-name random --input-len 16 --output-len 256 --num-prompts 64 --max-num-seqs 64 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_4/json/Llama3.2-1B-INT8_M64.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 16:51:38 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 16:51:38 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2830438) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2830438) WARNING 01-27 16:51:57 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 18.00 requests/s, 4896.98 total tokens/s, 4608.93 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384

STDERR:
[2026-01-27 16:51:38] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 16:51:38] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-27 16:51:38] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-27 16:51:38] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:51:38] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:51:38] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:51:38] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:51:38] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:51:38] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-27 16:51:38] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 16:51:38] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 16:51:38] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 16:51:38] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 16:51:38] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 16:51:41] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 16:51:41] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-27 16:51:41] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-27 16:51:41] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:51:41] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:51:41] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:51:41] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:51:41] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:51:41] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-27 16:51:41] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 16:51:41] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 16:51:41] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 16:51:41] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 16:51:41] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2830438) [2026-01-27 16:51:42] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2830438) [2026-01-27 16:51:42] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2830438) [2026-01-27 16:51:42] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2830438) [2026-01-27 16:51:42] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=2830438) [2026-01-27 16:51:42] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=2830438) [2026-01-27 16:51:42] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2830438) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2830438) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.05s/it]
(EngineCore_DP0 pid=2830438) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.05s/it]
(EngineCore_DP0 pid=2830438) 
(EngineCore_DP0 pid=2830438) [2026-01-27 16:51:51] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2048] -> 1D uint8
(EngineCore_DP0 pid=2830438) [2026-01-27 16:51:51] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4718592 bytes
(EngineCore_DP0 pid=2830438) [2026-01-27 16:51:51] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2048] -> 1D uint8
(EngineCore_DP0 pid=2830438) [2026-01-27 16:51:51] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 3145728 bytes
(EngineCore_DP0 pid=2830438) [2026-01-27 16:51:51] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2048] -> 1D uint8
(EngineCore_DP0 pid=2830438) [2026-01-27 16:51:51] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 25165824 bytes
(EngineCore_DP0 pid=2830438) [2026-01-27 16:51:51] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 8192] -> 1D uint8
(EngineCore_DP0 pid=2830438) [2026-01-27 16:51:51] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12582912 bytes
(EngineCore_DP0 pid=2830438) 2026-01-27 16:51:56,813 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2830438) 2026-01-27 16:51:56,822 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|| 64/64 [00:00<00:00, 12243.35it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|         | 1/64 [00:03<03:37,  3.45s/it, est. speed input: 4.64 toks/s, output: 74.20 toks/s]
Processed prompts: 100%|| 64/64 [00:03<00:00,  3.45s/it, est. speed input: 288.54 toks/s, output: 4616.63 toks/s]
Processed prompts: 100%|| 64/64 [00:03<00:00, 18.03it/s, est. speed input: 288.54 toks/s, output: 4616.63 toks/s]
[rank0]:[W127 16:52:01.145781989 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-27 16:52:03
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4
Params: prompt_len=16, output_len=256, num_prompts=128, max_num_seqs=128
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4 --dataset-name random --input-len 16 --output-len 256 --num-prompts 128 --max-num-seqs 128 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_4/json/Llama3.2-1B-INT8_M128.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 16:52:07 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 16:52:07 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2831029) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2831029) WARNING 01-27 16:52:26 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 27.04 requests/s, 7354.59 total tokens/s, 6921.97 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768

STDERR:
[2026-01-27 16:52:07] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 16:52:07] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-27 16:52:07] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-27 16:52:07] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:52:07] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:52:07] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:52:07] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:52:07] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:52:07] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-27 16:52:07] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 16:52:07] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 16:52:07] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 16:52:07] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 16:52:07] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 16:52:10] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 16:52:10] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-27 16:52:10] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-27 16:52:10] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:52:10] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:52:10] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:52:10] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:52:10] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:52:10] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-27 16:52:10] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 16:52:10] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 16:52:10] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 16:52:10] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 16:52:10] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2831029) [2026-01-27 16:52:11] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2831029) [2026-01-27 16:52:11] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2831029) [2026-01-27 16:52:11] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2831029) [2026-01-27 16:52:11] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=2831029) [2026-01-27 16:52:11] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=2831029) [2026-01-27 16:52:11] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2831029) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2831029) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:07<00:00,  8.00s/it]
(EngineCore_DP0 pid=2831029) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:07<00:00,  8.00s/it]
(EngineCore_DP0 pid=2831029) 
(EngineCore_DP0 pid=2831029) [2026-01-27 16:52:20] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2048] -> 1D uint8
(EngineCore_DP0 pid=2831029) [2026-01-27 16:52:20] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4718592 bytes
(EngineCore_DP0 pid=2831029) [2026-01-27 16:52:20] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2048] -> 1D uint8
(EngineCore_DP0 pid=2831029) [2026-01-27 16:52:20] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 3145728 bytes
(EngineCore_DP0 pid=2831029) [2026-01-27 16:52:20] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2048] -> 1D uint8
(EngineCore_DP0 pid=2831029) [2026-01-27 16:52:20] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 25165824 bytes
(EngineCore_DP0 pid=2831029) [2026-01-27 16:52:20] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 8192] -> 1D uint8
(EngineCore_DP0 pid=2831029) [2026-01-27 16:52:20] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12582912 bytes
(EngineCore_DP0 pid=2831029) 2026-01-27 16:52:25,665 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2831029) 2026-01-27 16:52:25,672 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|| 128/128 [00:00<00:00, 13106.56it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:04<09:30,  4.50s/it, est. speed input: 3.56 toks/s, output: 56.95 toks/s]
Processed prompts:  48%|     | 62/128 [00:04<00:03, 18.85it/s, est. speed input: 214.61 toks/s, output: 3433.81 toks/s]
Processed prompts:  99%|| 127/128 [00:04<00:00, 45.23it/s, est. speed input: 430.25 toks/s, output: 6883.96 toks/s]
Processed prompts: 100%|| 128/128 [00:04<00:00, 45.23it/s, est. speed input: 433.62 toks/s, output: 6937.97 toks/s]
Processed prompts: 100%|| 128/128 [00:04<00:00, 27.10it/s, est. speed input: 433.62 toks/s, output: 6937.97 toks/s]
[rank0]:[W127 16:52:31.170555765 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-27 16:52:33
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4
Params: prompt_len=16, output_len=256, num_prompts=256, max_num_seqs=256
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4 --dataset-name random --input-len 16 --output-len 256 --num-prompts 256 --max-num-seqs 256 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_4/json/Llama3.2-1B-INT8_M256.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 16:52:36 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 16:52:37 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2831638) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2831638) WARNING 01-27 16:52:55 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 32.21 requests/s, 8759.79 total tokens/s, 8244.51 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536

STDERR:
[2026-01-27 16:52:36] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 16:52:36] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-27 16:52:36] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-27 16:52:36] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:52:36] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:52:36] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:52:36] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:52:36] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:52:36] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-27 16:52:36] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 16:52:36] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 16:52:36] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 16:52:36] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 16:52:36] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 16:52:40] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 16:52:40] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-27 16:52:40] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-27 16:52:40] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:52:40] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:52:40] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:52:40] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:52:40] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:52:40] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-27 16:52:40] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 16:52:40] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 16:52:40] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 16:52:40] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 16:52:40] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2831638) [2026-01-27 16:52:41] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2831638) [2026-01-27 16:52:41] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2831638) [2026-01-27 16:52:41] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2831638) [2026-01-27 16:52:41] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=2831638) [2026-01-27 16:52:41] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=2831638) [2026-01-27 16:52:41] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2831638) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2831638) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.02s/it]
(EngineCore_DP0 pid=2831638) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.02s/it]
(EngineCore_DP0 pid=2831638) 
(EngineCore_DP0 pid=2831638) [2026-01-27 16:52:49] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2048] -> 1D uint8
(EngineCore_DP0 pid=2831638) [2026-01-27 16:52:49] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4718592 bytes
(EngineCore_DP0 pid=2831638) [2026-01-27 16:52:49] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2048] -> 1D uint8
(EngineCore_DP0 pid=2831638) [2026-01-27 16:52:49] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 3145728 bytes
(EngineCore_DP0 pid=2831638) [2026-01-27 16:52:49] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2048] -> 1D uint8
(EngineCore_DP0 pid=2831638) [2026-01-27 16:52:49] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 25165824 bytes
(EngineCore_DP0 pid=2831638) [2026-01-27 16:52:49] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 8192] -> 1D uint8
(EngineCore_DP0 pid=2831638) [2026-01-27 16:52:49] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12582912 bytes
(EngineCore_DP0 pid=2831638) 2026-01-27 16:52:55,163 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2831638) 2026-01-27 16:52:55,170 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|| 256/256 [00:00<00:00, 13455.58it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:06<29:21,  6.91s/it, est. speed input: 2.32 toks/s, output: 37.06 toks/s]
Processed prompts:  14%|        | 35/256 [00:07<00:31,  7.00it/s, est. speed input: 79.64 toks/s, output: 1274.19 toks/s]
Processed prompts:  30%|       | 76/256 [00:07<00:09, 18.20it/s, est. speed input: 170.11 toks/s, output: 2721.83 toks/s]
Processed prompts:  46%|     | 119/256 [00:07<00:04, 33.66it/s, est. speed input: 261.84 toks/s, output: 4189.43 toks/s]
Processed prompts:  60%|    | 153/256 [00:07<00:02, 49.36it/s, est. speed input: 331.98 toks/s, output: 5311.67 toks/s]
Processed prompts:  72%|  | 184/256 [00:07<00:01, 67.17it/s, est. speed input: 393.89 toks/s, output: 6302.29 toks/s]
Processed prompts:  84%| | 215/256 [00:07<00:00, 88.51it/s, est. speed input: 453.98 toks/s, output: 7263.68 toks/s]
Processed prompts:  96%|| 246/256 [00:07<00:00, 103.96it/s, est. speed input: 507.32 toks/s, output: 8117.15 toks/s]
Processed prompts: 100%|| 256/256 [00:07<00:00, 103.96it/s, est. speed input: 516.58 toks/s, output: 8265.31 toks/s]
Processed prompts: 100%|| 256/256 [00:07<00:00, 32.29it/s, est. speed input: 516.58 toks/s, output: 8265.31 toks/s] 
[rank0]:[W127 16:53:03.910841261 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-27 16:53:06
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4
Params: prompt_len=16, output_len=256, num_prompts=512, max_num_seqs=512
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4 --dataset-name random --input-len 16 --output-len 256 --num-prompts 512 --max-num-seqs 512 --max-model-len 272 --max-num-batched-tokens 512 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_4/json/Llama3.2-1B-INT8_M512.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 16:53:09 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 16:53:09 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2832274) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2832274) WARNING 01-27 16:53:28 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 33.14 requests/s, 9014.55 total tokens/s, 8484.28 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072

STDERR:
[2026-01-27 16:53:09] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 16:53:09] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-27 16:53:09] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-27 16:53:09] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:53:09] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:53:09] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:53:09] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:53:09] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:53:09] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-27 16:53:09] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 16:53:09] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 16:53:09] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 16:53:09] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 16:53:09] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 16:53:12] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 16:53:12] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-27 16:53:12] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-27 16:53:12] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:53:12] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:53:12] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:53:12] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:53:12] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-27 16:53:12] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-27 16:53:12] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 16:53:12] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 16:53:12] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 16:53:12] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 16:53:12] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2832274) [2026-01-27 16:53:13] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2832274) [2026-01-27 16:53:13] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2832274) [2026-01-27 16:53:13] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2832274) [2026-01-27 16:53:13] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=2832274) [2026-01-27 16:53:13] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=2832274) [2026-01-27 16:53:13] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2832274) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2832274) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.23s/it]
(EngineCore_DP0 pid=2832274) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.23s/it]
(EngineCore_DP0 pid=2832274) 
(EngineCore_DP0 pid=2832274) [2026-01-27 16:53:22] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2048] -> 1D uint8
(EngineCore_DP0 pid=2832274) [2026-01-27 16:53:22] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4718592 bytes
(EngineCore_DP0 pid=2832274) [2026-01-27 16:53:22] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2048] -> 1D uint8
(EngineCore_DP0 pid=2832274) [2026-01-27 16:53:22] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 3145728 bytes
(EngineCore_DP0 pid=2832274) [2026-01-27 16:53:22] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2048] -> 1D uint8
(EngineCore_DP0 pid=2832274) [2026-01-27 16:53:22] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 25165824 bytes
(EngineCore_DP0 pid=2832274) [2026-01-27 16:53:22] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 8192] -> 1D uint8
(EngineCore_DP0 pid=2832274) [2026-01-27 16:53:22] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12582912 bytes
(EngineCore_DP0 pid=2832274) 2026-01-27 16:53:27,886 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2832274) 2026-01-27 16:53:27,893 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests: 100%|| 512/512 [00:00<00:00, 14170.51it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:11<1:42:08, 11.99s/it, est. speed input: 1.33 toks/s, output: 21.35 toks/s]
Processed prompts:   1%|          | 5/512 [00:12<15:16,  1.81s/it, est. speed input: 6.61 toks/s, output: 105.82 toks/s] 
Processed prompts:  13%|        | 65/512 [00:12<00:43, 10.31it/s, est. speed input: 84.65 toks/s, output: 1354.44 toks/s]
Processed prompts:  23%|       | 119/512 [00:12<00:17, 22.07it/s, est. speed input: 153.02 toks/s, output: 2448.36 toks/s]
Processed prompts:  32%|      | 166/512 [00:12<00:09, 35.95it/s, est. speed input: 211.69 toks/s, output: 3387.04 toks/s]
Processed prompts:  44%|     | 226/512 [00:12<00:04, 58.83it/s, est. speed input: 284.97 toks/s, output: 4559.45 toks/s]
Processed prompts:  54%|    | 274/512 [00:12<00:02, 80.90it/s, est. speed input: 341.84 toks/s, output: 5469.48 toks/s]
Processed prompts:  62%|   | 315/512 [00:12<00:01, 104.91it/s, est. speed input: 389.95 toks/s, output: 6239.13 toks/s]
Processed prompts:  70%|   | 358/512 [00:13<00:01, 131.77it/s, est. speed input: 438.73 toks/s, output: 7019.73 toks/s]
Processed prompts:  77%|  | 395/512 [00:13<00:00, 158.73it/s, est. speed input: 480.27 toks/s, output: 7684.30 toks/s]
Processed prompts:  84%| | 432/512 [00:13<00:00, 180.22it/s, est. speed input: 520.00 toks/s, output: 8320.02 toks/s]
Processed prompts:  91%| | 466/512 [00:13<00:00, 188.27it/s, est. speed input: 554.32 toks/s, output: 8869.12 toks/s]
Processed prompts:  97%|| 496/512 [00:13<00:00, 170.58it/s, est. speed input: 580.30 toks/s, output: 9284.80 toks/s]
Processed prompts: 100%|| 512/512 [00:15<00:00, 170.58it/s, est. speed input: 531.55 toks/s, output: 8504.74 toks/s]
Processed prompts: 100%|| 512/512 [00:15<00:00, 33.22it/s, est. speed input: 531.55 toks/s, output: 8504.74 toks/s] 
[rank0]:[W127 16:53:44.184420201 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=64 ==========
Time: 2026-01-27 17:17:39
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_4
Params: prompt_len=16, output_len=256, num_prompts=64, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_4 --dataset-name random --input-len 16 --output-len 256 --num-prompts 64 --max-num-seqs 64 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_4/json/Llama3.2-3B-INT8_M64.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:17:42 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:17:43 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2859879) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2859879) WARNING 01-27 17:18:14 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 8.92 requests/s, 2427.17 total tokens/s, 2284.40 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384

STDERR:
[2026-01-27 17:17:42] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:17:42] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-27 17:17:42] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-27 17:17:42] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:17:42] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:17:42] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:17:42] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:17:42] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:17:42] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-27 17:17:42] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:17:42] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:17:42] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:17:42] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:17:42] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:17:46] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:17:46] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-27 17:17:46] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-27 17:17:46] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:17:46] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:17:46] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:17:46] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:17:46] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:17:46] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-27 17:17:46] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:17:46] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:17:46] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:17:46] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:17:46] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2859879) [2026-01-27 17:17:47] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2859879) [2026-01-27 17:17:47] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2859879) [2026-01-27 17:17:47] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2859879) [2026-01-27 17:17:47] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=2859879) [2026-01-27 17:17:47] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=2859879) [2026-01-27 17:17:47] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2859879) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2859879) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:20<00:00, 20.20s/it]
(EngineCore_DP0 pid=2859879) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:20<00:00, 20.20s/it]
(EngineCore_DP0 pid=2859879) 
(EngineCore_DP0 pid=2859879) [2026-01-27 17:18:08] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 3072] -> 1D uint8
(EngineCore_DP0 pid=2859879) [2026-01-27 17:18:08] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11796480 bytes
(EngineCore_DP0 pid=2859879) [2026-01-27 17:18:08] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=2859879) [2026-01-27 17:18:08] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7077888 bytes
(EngineCore_DP0 pid=2859879) [2026-01-27 17:18:08] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=2859879) [2026-01-27 17:18:08] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 37748736 bytes
(EngineCore_DP0 pid=2859879) [2026-01-27 17:18:08] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 8192] -> 1D uint8
(EngineCore_DP0 pid=2859879) [2026-01-27 17:18:08] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 18874368 bytes
(EngineCore_DP0 pid=2859879) 2026-01-27 17:18:14,216 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2859879) 2026-01-27 17:18:14,233 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|| 64/64 [00:00<00:00, 10972.67it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|         | 1/64 [00:06<07:14,  6.90s/it, est. speed input: 2.32 toks/s, output: 37.12 toks/s]
Processed prompts:  58%|    | 37/64 [00:07<00:03,  7.42it/s, est. speed input: 84.39 toks/s, output: 1350.31 toks/s]
Processed prompts: 100%|| 64/64 [00:07<00:00, 14.58it/s, est. speed input: 142.91 toks/s, output: 2286.55 toks/s]
Processed prompts: 100%|| 64/64 [00:07<00:00, 14.58it/s, est. speed input: 142.91 toks/s, output: 2286.55 toks/s]
Processed prompts: 100%|| 64/64 [00:07<00:00,  8.93it/s, est. speed input: 142.91 toks/s, output: 2286.55 toks/s]
[rank0]:[W127 17:18:22.145569219 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-27 17:18:24
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_4
Params: prompt_len=16, output_len=256, num_prompts=128, max_num_seqs=128
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_4 --dataset-name random --input-len 16 --output-len 256 --num-prompts 128 --max-num-seqs 128 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_4/json/Llama3.2-3B-INT8_M128.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:18:28 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:18:28 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2860699) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2860699) WARNING 01-27 17:18:59 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 13.66 requests/s, 3714.47 total tokens/s, 3495.97 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768

STDERR:
[2026-01-27 17:18:27] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:18:27] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-27 17:18:27] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-27 17:18:27] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:18:27] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:18:27] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:18:28] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:18:28] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:18:28] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-27 17:18:28] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:18:28] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:18:28] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:18:28] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:18:28] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:18:31] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:18:31] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-27 17:18:31] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-27 17:18:31] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:18:31] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:18:31] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:18:31] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:18:31] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:18:31] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-27 17:18:31] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:18:31] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:18:31] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:18:31] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:18:31] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2860699) [2026-01-27 17:18:32] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2860699) [2026-01-27 17:18:32] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2860699) [2026-01-27 17:18:32] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2860699) [2026-01-27 17:18:32] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=2860699) [2026-01-27 17:18:32] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=2860699) [2026-01-27 17:18:32] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2860699) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2860699) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:19<00:00, 19.60s/it]
(EngineCore_DP0 pid=2860699) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:19<00:00, 19.60s/it]
(EngineCore_DP0 pid=2860699) 
(EngineCore_DP0 pid=2860699) [2026-01-27 17:18:52] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 3072] -> 1D uint8
(EngineCore_DP0 pid=2860699) [2026-01-27 17:18:52] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11796480 bytes
(EngineCore_DP0 pid=2860699) [2026-01-27 17:18:52] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=2860699) [2026-01-27 17:18:52] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7077888 bytes
(EngineCore_DP0 pid=2860699) [2026-01-27 17:18:52] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=2860699) [2026-01-27 17:18:52] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 37748736 bytes
(EngineCore_DP0 pid=2860699) [2026-01-27 17:18:52] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 8192] -> 1D uint8
(EngineCore_DP0 pid=2860699) [2026-01-27 17:18:52] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 18874368 bytes
(EngineCore_DP0 pid=2860699) 2026-01-27 17:18:58,590 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2860699) 2026-01-27 17:18:58,601 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|| 128/128 [00:00<00:00, 4454.81it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:08<18:57,  8.96s/it, est. speed input: 1.79 toks/s, output: 28.57 toks/s]
Processed prompts:  26%|       | 33/128 [00:09<00:18,  5.10it/s, est. speed input: 58.00 toks/s, output: 927.98 toks/s]
Processed prompts:  59%|    | 75/128 [00:09<00:03, 14.09it/s, est. speed input: 130.19 toks/s, output: 2083.00 toks/s]
Processed prompts:  92%|| 118/128 [00:09<00:00, 26.53it/s, est. speed input: 202.43 toks/s, output: 3238.82 toks/s]
Processed prompts: 100%|| 128/128 [00:09<00:00, 26.53it/s, est. speed input: 219.20 toks/s, output: 3507.20 toks/s]
Processed prompts: 100%|| 128/128 [00:09<00:00, 13.70it/s, est. speed input: 219.20 toks/s, output: 3507.20 toks/s]
[rank0]:[W127 17:19:08.741535879 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-27 17:19:11
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_4
Params: prompt_len=16, output_len=256, num_prompts=256, max_num_seqs=256
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_4 --dataset-name random --input-len 16 --output-len 256 --num-prompts 256 --max-num-seqs 256 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_4/json/Llama3.2-3B-INT8_M256.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:19:14 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:19:14 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2861523) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2861523) WARNING 01-27 17:19:45 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 16.39 requests/s, 4456.91 total tokens/s, 4194.74 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536

STDERR:
[2026-01-27 17:19:14] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:19:14] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-27 17:19:14] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-27 17:19:14] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:19:14] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:19:14] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:19:14] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:19:14] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:19:14] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-27 17:19:14] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:19:14] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:19:14] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:19:14] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:19:14] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:19:17] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:19:17] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-27 17:19:17] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-27 17:19:17] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:19:17] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:19:17] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:19:17] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:19:17] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:19:17] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-27 17:19:17] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:19:17] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:19:17] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:19:17] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:19:17] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2861523) [2026-01-27 17:19:18] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2861523) [2026-01-27 17:19:18] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2861523) [2026-01-27 17:19:18] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2861523) [2026-01-27 17:19:18] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=2861523) [2026-01-27 17:19:18] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=2861523) [2026-01-27 17:19:18] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2861523) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2861523) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:19<00:00, 19.67s/it]
(EngineCore_DP0 pid=2861523) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:19<00:00, 19.67s/it]
(EngineCore_DP0 pid=2861523) 
(EngineCore_DP0 pid=2861523) [2026-01-27 17:19:39] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 3072] -> 1D uint8
(EngineCore_DP0 pid=2861523) [2026-01-27 17:19:39] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11796480 bytes
(EngineCore_DP0 pid=2861523) [2026-01-27 17:19:39] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=2861523) [2026-01-27 17:19:39] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7077888 bytes
(EngineCore_DP0 pid=2861523) [2026-01-27 17:19:39] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=2861523) [2026-01-27 17:19:39] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 37748736 bytes
(EngineCore_DP0 pid=2861523) [2026-01-27 17:19:39] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 8192] -> 1D uint8
(EngineCore_DP0 pid=2861523) [2026-01-27 17:19:39] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 18874368 bytes
(EngineCore_DP0 pid=2861523) 2026-01-27 17:19:44,655 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2861523) 2026-01-27 17:19:44,666 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|| 256/256 [00:00<00:00, 13291.52it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:13<58:28, 13.76s/it, est. speed input: 1.16 toks/s, output: 18.61 toks/s]
Processed prompts:   7%|         | 19/256 [00:13<02:03,  1.91it/s, est. speed input: 21.86 toks/s, output: 349.83 toks/s]
Processed prompts:  19%|        | 48/256 [00:14<00:34,  6.03it/s, est. speed input: 54.70 toks/s, output: 875.27 toks/s]
Processed prompts:  29%|       | 75/256 [00:14<00:16, 11.24it/s, est. speed input: 84.63 toks/s, output: 1354.10 toks/s]
Processed prompts:  38%|      | 98/256 [00:14<00:09, 17.16it/s, est. speed input: 109.61 toks/s, output: 1753.78 toks/s]
Processed prompts:  46%|     | 118/256 [00:14<00:05, 24.02it/s, est. speed input: 131.03 toks/s, output: 2096.49 toks/s]
Processed prompts:  56%|    | 144/256 [00:14<00:03, 35.40it/s, est. speed input: 158.39 toks/s, output: 2534.30 toks/s]
Processed prompts:  65%|   | 166/256 [00:14<00:01, 46.68it/s, est. speed input: 180.93 toks/s, output: 2894.86 toks/s]
Processed prompts:  72%|  | 185/256 [00:14<00:01, 58.33it/s, est. speed input: 200.11 toks/s, output: 3201.79 toks/s]
Processed prompts:  79%|  | 203/256 [00:14<00:00, 69.20it/s, est. speed input: 217.67 toks/s, output: 3482.72 toks/s]
Processed prompts:  86%| | 220/256 [00:15<00:00, 78.35it/s, est. speed input: 233.73 toks/s, output: 3739.62 toks/s]
Processed prompts:  92%|| 236/256 [00:15<00:00, 83.45it/s, est. speed input: 248.14 toks/s, output: 3970.18 toks/s]
Processed prompts:  98%|| 250/256 [00:15<00:00, 78.36it/s, est. speed input: 259.25 toks/s, output: 4148.02 toks/s]
Processed prompts: 100%|| 256/256 [00:15<00:00, 78.36it/s, est. speed input: 262.51 toks/s, output: 4200.20 toks/s]
Processed prompts: 100%|| 256/256 [00:15<00:00, 16.41it/s, est. speed input: 262.51 toks/s, output: 4200.20 toks/s]
[rank0]:[W127 17:20:01.069403111 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-27 17:20:03
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_4
Params: prompt_len=16, output_len=256, num_prompts=512, max_num_seqs=512
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_4 --dataset-name random --input-len 16 --output-len 256 --num-prompts 512 --max-num-seqs 512 --max-model-len 272 --max-num-batched-tokens 512 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_4/json/Llama3.2-3B-INT8_M512.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:20:06 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:20:06 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2862452) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2862452) WARNING 01-27 17:20:37 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 16.99 requests/s, 4622.24 total tokens/s, 4350.34 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072

STDERR:
[2026-01-27 17:20:06] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:20:06] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-27 17:20:06] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-27 17:20:06] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:20:06] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:20:06] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:20:06] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:20:06] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:20:06] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-27 17:20:06] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:20:06] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:20:06] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:20:06] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:20:06] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:20:10] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:20:10] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-27 17:20:10] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-27 17:20:10] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:20:10] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:20:10] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:20:10] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:20:10] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-27 17:20:10] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-27 17:20:10] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:20:10] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:20:10] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:20:10] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:20:10] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2862452) [2026-01-27 17:20:10] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2862452) [2026-01-27 17:20:10] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2862452) [2026-01-27 17:20:10] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2862452) [2026-01-27 17:20:10] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=2862452) [2026-01-27 17:20:10] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=2862452) [2026-01-27 17:20:10] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2862452) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2862452) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:19<00:00, 19.67s/it]
(EngineCore_DP0 pid=2862452) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:19<00:00, 19.67s/it]
(EngineCore_DP0 pid=2862452) 
(EngineCore_DP0 pid=2862452) [2026-01-27 17:20:31] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 3072] -> 1D uint8
(EngineCore_DP0 pid=2862452) [2026-01-27 17:20:31] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11796480 bytes
(EngineCore_DP0 pid=2862452) [2026-01-27 17:20:31] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=2862452) [2026-01-27 17:20:31] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7077888 bytes
(EngineCore_DP0 pid=2862452) [2026-01-27 17:20:31] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=2862452) [2026-01-27 17:20:31] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 37748736 bytes
(EngineCore_DP0 pid=2862452) [2026-01-27 17:20:31] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 8192] -> 1D uint8
(EngineCore_DP0 pid=2862452) [2026-01-27 17:20:31] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 18874368 bytes
(EngineCore_DP0 pid=2862452) 2026-01-27 17:20:36,901 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2862452) 2026-01-27 17:20:36,911 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests: 100%|| 512/512 [00:00<00:00, 12977.69it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:23<3:16:59, 23.13s/it, est. speed input: 0.69 toks/s, output: 11.07 toks/s]
Processed prompts:   1%|         | 7/512 [00:23<20:38,  2.45s/it, est. speed input: 4.79 toks/s, output: 76.68 toks/s]  
Processed prompts:   7%|         | 38/512 [00:23<02:35,  3.04it/s, est. speed input: 25.76 toks/s, output: 412.19 toks/s]
Processed prompts:  13%|        | 67/512 [00:23<01:09,  6.40it/s, est. speed input: 45.01 toks/s, output: 720.08 toks/s]
Processed prompts:  18%|        | 94/512 [00:24<00:39, 10.61it/s, est. speed input: 62.60 toks/s, output: 1001.65 toks/s]
Processed prompts:  23%|       | 120/512 [00:24<00:24, 15.91it/s, est. speed input: 79.22 toks/s, output: 1267.60 toks/s]
Processed prompts:  28%|       | 144/512 [00:24<00:16, 22.80it/s, est. speed input: 94.67 toks/s, output: 1514.67 toks/s]
Processed prompts:  37%|      | 188/512 [00:24<00:08, 38.96it/s, est. speed input: 122.65 toks/s, output: 1962.32 toks/s]
Processed prompts:  44%|     | 227/512 [00:24<00:05, 55.93it/s, est. speed input: 147.04 toks/s, output: 2352.59 toks/s]
Processed prompts:  51%|     | 260/512 [00:24<00:03, 72.09it/s, est. speed input: 167.33 toks/s, output: 2677.23 toks/s]
Processed prompts:  56%|    | 289/512 [00:25<00:02, 87.27it/s, est. speed input: 184.87 toks/s, output: 2957.85 toks/s]
Processed prompts:  62%|   | 316/512 [00:25<00:01, 103.64it/s, est. speed input: 201.10 toks/s, output: 3217.64 toks/s]
Processed prompts:  66%|   | 339/512 [00:25<00:01, 114.98it/s, est. speed input: 214.60 toks/s, output: 3433.60 toks/s]
Processed prompts:  70%|   | 359/512 [00:25<00:01, 126.40it/s, est. speed input: 226.30 toks/s, output: 3620.82 toks/s]
Processed prompts:  75%|  | 385/512 [00:25<00:00, 137.16it/s, est. speed input: 241.24 toks/s, output: 3859.84 toks/s]
Processed prompts:  79%|  | 407/512 [00:25<00:00, 145.94it/s, est. speed input: 253.79 toks/s, output: 4060.61 toks/s]
Processed prompts:  83%| | 426/512 [00:25<00:00, 154.10it/s, est. speed input: 264.58 toks/s, output: 4233.26 toks/s]
Processed prompts:  87%| | 445/512 [00:25<00:00, 151.12it/s, est. speed input: 274.96 toks/s, output: 4399.43 toks/s]
Processed prompts:  90%| | 463/512 [00:26<00:00, 134.41it/s, est. speed input: 284.16 toks/s, output: 4546.55 toks/s]
Processed prompts:  94%|| 479/512 [00:26<00:00, 119.20it/s, est. speed input: 291.96 toks/s, output: 4671.43 toks/s]
Processed prompts:  96%|| 493/512 [00:26<00:00, 90.99it/s, est. speed input: 297.43 toks/s, output: 4758.92 toks/s] 
Processed prompts:  98%|| 504/512 [00:29<00:00, 13.34it/s, est. speed input: 268.97 toks/s, output: 4303.47 toks/s]
Processed prompts: 100%|| 512/512 [00:30<00:00, 15.65it/s, est. speed input: 272.26 toks/s, output: 4356.22 toks/s]
Processed prompts: 100%|| 512/512 [00:30<00:00, 15.65it/s, est. speed input: 272.26 toks/s, output: 4356.22 toks/s]
Processed prompts: 100%|| 512/512 [00:30<00:00, 17.02it/s, est. speed input: 272.26 toks/s, output: 4356.22 toks/s]
[rank0]:[W127 17:21:07.871924437 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=64 ==========
Time: 2026-01-27 17:57:52
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_4
Params: prompt_len=16, output_len=256, num_prompts=64, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_4 --dataset-name random --input-len 16 --output-len 256 --num-prompts 64 --max-num-seqs 64 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_4/json/Qwen2.5-7B-INT8_M64.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:57:55 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:57:55 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2901011) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2901011) WARNING 01-27 17:58:49 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 5.68 requests/s, 1545.53 total tokens/s, 1454.61 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384

STDERR:
[2026-01-27 17:57:55] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:57:55] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-27 17:57:55] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-27 17:57:55] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 17:57:55] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 17:57:55] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 17:57:55] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 17:57:55] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 17:57:55] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-27 17:57:55] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:57:55] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:57:55] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:57:55] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:57:55] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:57:59] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:57:59] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-27 17:57:59] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-27 17:57:59] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 17:57:59] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 17:57:59] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 17:57:59] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 17:57:59] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 17:57:59] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-27 17:57:59] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:57:59] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:57:59] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:57:59] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:57:59] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2901011) [2026-01-27 17:58:00] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2901011) [2026-01-27 17:58:00] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2901011) [2026-01-27 17:58:00] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2901011) [2026-01-27 17:58:00] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=2901011) [2026-01-27 17:58:00] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=2901011) [2026-01-27 17:58:00] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2901011) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2901011) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:20<00:20, 20.39s/it]
(EngineCore_DP0 pid=2901011) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:41<00:00, 20.60s/it]
(EngineCore_DP0 pid=2901011) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:41<00:00, 20.57s/it]
(EngineCore_DP0 pid=2901011) 
(EngineCore_DP0 pid=2901011) [2026-01-27 17:58:42] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 3584] -> 1D uint8
(EngineCore_DP0 pid=2901011) [2026-01-27 17:58:42] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12386304 bytes
(EngineCore_DP0 pid=2901011) [2026-01-27 17:58:42] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 3584] -> 1D uint8
(EngineCore_DP0 pid=2901011) [2026-01-27 17:58:42] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9633792 bytes
(EngineCore_DP0 pid=2901011) [2026-01-27 17:58:42] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 3584] -> 1D uint8
(EngineCore_DP0 pid=2901011) [2026-01-27 17:58:42] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 101842944 bytes
(EngineCore_DP0 pid=2901011) [2026-01-27 17:58:42] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 18944] -> 1D uint8
(EngineCore_DP0 pid=2901011) [2026-01-27 17:58:42] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 50921472 bytes
(EngineCore_DP0 pid=2901011) 2026-01-27 17:58:49,040 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2901011) 2026-01-27 17:58:49,054 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|| 64/64 [00:00<00:00, 8901.27it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|         | 1/64 [00:11<11:35, 11.04s/it, est. speed input: 1.45 toks/s, output: 23.18 toks/s]
Processed prompts:  53%|    | 34/64 [00:11<00:07,  4.29it/s, est. speed input: 48.67 toks/s, output: 778.72 toks/s]
Processed prompts: 100%|| 64/64 [00:11<00:00,  4.29it/s, est. speed input: 90.98 toks/s, output: 1455.74 toks/s]
Processed prompts: 100%|| 64/64 [00:11<00:00,  5.69it/s, est. speed input: 90.98 toks/s, output: 1455.74 toks/s]
[rank0]:[W127 17:59:01.158523563 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-27 17:59:04
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_4
Params: prompt_len=16, output_len=256, num_prompts=128, max_num_seqs=128
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_4 --dataset-name random --input-len 16 --output-len 256 --num-prompts 128 --max-num-seqs 128 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_4/json/Qwen2.5-7B-INT8_M128.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:59:10 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:59:10 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2902231) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2902231) WARNING 01-27 18:00:03 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 9.88 requests/s, 2688.69 total tokens/s, 2530.53 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768

STDERR:
[2026-01-27 17:59:10] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:59:10] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-27 17:59:10] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-27 17:59:10] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 17:59:10] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 17:59:10] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 17:59:10] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 17:59:10] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 17:59:10] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-27 17:59:10] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:59:10] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:59:10] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:59:10] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:59:10] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:59:13] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:59:13] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-27 17:59:13] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-27 17:59:13] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 17:59:13] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 17:59:13] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 17:59:13] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 17:59:13] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 17:59:13] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-27 17:59:13] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:59:13] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:59:13] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:59:13] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:59:13] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2902231) [2026-01-27 17:59:14] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2902231) [2026-01-27 17:59:14] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2902231) [2026-01-27 17:59:14] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2902231) [2026-01-27 17:59:14] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=2902231) [2026-01-27 17:59:14] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=2902231) [2026-01-27 17:59:14] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2902231) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2902231) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:19<00:19, 19.50s/it]
(EngineCore_DP0 pid=2902231) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:39<00:00, 20.09s/it]
(EngineCore_DP0 pid=2902231) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:39<00:00, 20.00s/it]
(EngineCore_DP0 pid=2902231) 
(EngineCore_DP0 pid=2902231) [2026-01-27 17:59:55] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 3584] -> 1D uint8
(EngineCore_DP0 pid=2902231) [2026-01-27 17:59:55] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12386304 bytes
(EngineCore_DP0 pid=2902231) [2026-01-27 17:59:55] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 3584] -> 1D uint8
(EngineCore_DP0 pid=2902231) [2026-01-27 17:59:55] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9633792 bytes
(EngineCore_DP0 pid=2902231) [2026-01-27 17:59:55] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 3584] -> 1D uint8
(EngineCore_DP0 pid=2902231) [2026-01-27 17:59:55] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 101842944 bytes
(EngineCore_DP0 pid=2902231) [2026-01-27 17:59:55] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 18944] -> 1D uint8
(EngineCore_DP0 pid=2902231) [2026-01-27 17:59:55] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 50921472 bytes
(EngineCore_DP0 pid=2902231) 2026-01-27 18:00:02,363 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2902231) 2026-01-27 18:00:02,375 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|| 128/128 [00:00<00:00, 4239.82it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:12<26:11, 12.38s/it, est. speed input: 1.29 toks/s, output: 20.68 toks/s]
Processed prompts:  14%|        | 18/128 [00:12<00:54,  2.01it/s, est. speed input: 23.04 toks/s, output: 368.69 toks/s]
Processed prompts:  38%|      | 48/128 [00:12<00:11,  6.78it/s, est. speed input: 60.95 toks/s, output: 975.14 toks/s]
Processed prompts:  59%|    | 75/128 [00:12<00:04, 12.61it/s, est. speed input: 94.36 toks/s, output: 1509.73 toks/s]
Processed prompts:  77%|  | 99/128 [00:12<00:01, 19.58it/s, est. speed input: 123.51 toks/s, output: 1976.23 toks/s]
Processed prompts: 100%|| 128/128 [00:12<00:00, 19.58it/s, est. speed input: 158.54 toks/s, output: 2536.70 toks/s]
Processed prompts: 100%|| 128/128 [00:12<00:00,  9.91it/s, est. speed input: 158.54 toks/s, output: 2536.70 toks/s]
[rank0]:[W127 18:00:16.359664263 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-27 18:00:18
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_4
Params: prompt_len=16, output_len=256, num_prompts=256, max_num_seqs=256
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_4 --dataset-name random --input-len 16 --output-len 256 --num-prompts 256 --max-num-seqs 256 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_4/json/Qwen2.5-7B-INT8_M256.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 18:00:22 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 18:00:22 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2903423) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2903423) WARNING 01-27 18:01:16 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 12.79 requests/s, 3479.25 total tokens/s, 3274.59 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536

STDERR:
[2026-01-27 18:00:22] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:00:22] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-27 18:00:22] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-27 18:00:22] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:00:22] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:00:22] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:00:22] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:00:22] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:00:22] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-27 18:00:22] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:00:22] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:00:22] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:00:22] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:00:22] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 18:00:25] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:00:25] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-27 18:00:25] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-27 18:00:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:00:25] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:00:25] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:00:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:00:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:00:25] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-27 18:00:25] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:00:25] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:00:25] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:00:25] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:00:25] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2903423) [2026-01-27 18:00:26] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2903423) [2026-01-27 18:00:26] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2903423) [2026-01-27 18:00:26] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2903423) [2026-01-27 18:00:26] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=2903423) [2026-01-27 18:00:26] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=2903423) [2026-01-27 18:00:26] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2903423) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2903423) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:20<00:20, 20.87s/it]
(EngineCore_DP0 pid=2903423) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:41<00:00, 20.95s/it]
(EngineCore_DP0 pid=2903423) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:41<00:00, 20.94s/it]
(EngineCore_DP0 pid=2903423) 
(EngineCore_DP0 pid=2903423) [2026-01-27 18:01:09] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 3584] -> 1D uint8
(EngineCore_DP0 pid=2903423) [2026-01-27 18:01:09] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12386304 bytes
(EngineCore_DP0 pid=2903423) [2026-01-27 18:01:09] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 3584] -> 1D uint8
(EngineCore_DP0 pid=2903423) [2026-01-27 18:01:09] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9633792 bytes
(EngineCore_DP0 pid=2903423) [2026-01-27 18:01:09] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 3584] -> 1D uint8
(EngineCore_DP0 pid=2903423) [2026-01-27 18:01:09] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 101842944 bytes
(EngineCore_DP0 pid=2903423) [2026-01-27 18:01:09] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 18944] -> 1D uint8
(EngineCore_DP0 pid=2903423) [2026-01-27 18:01:09] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 50921472 bytes
(EngineCore_DP0 pid=2903423) 2026-01-27 18:01:15,615 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2903423) 2026-01-27 18:01:15,625 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|| 256/256 [00:00<00:00, 11410.28it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:17<1:14:31, 17.53s/it, est. speed input: 0.91 toks/s, output: 14.60 toks/s]
Processed prompts:   7%|         | 19/256 [00:17<02:37,  1.50it/s, est. speed input: 17.18 toks/s, output: 274.82 toks/s]
Processed prompts:  19%|        | 48/256 [00:17<00:43,  4.75it/s, est. speed input: 43.03 toks/s, output: 688.45 toks/s]
Processed prompts:  29%|       | 75/256 [00:18<00:20,  8.89it/s, est. speed input: 66.66 toks/s, output: 1066.61 toks/s]
Processed prompts:  38%|      | 98/256 [00:18<00:11, 13.59it/s, est. speed input: 86.37 toks/s, output: 1381.90 toks/s]
Processed prompts:  46%|     | 118/256 [00:18<00:07, 19.06it/s, est. speed input: 103.28 toks/s, output: 1652.55 toks/s]
Processed prompts:  53%|    | 136/256 [00:18<00:04, 25.45it/s, est. speed input: 118.27 toks/s, output: 1892.24 toks/s]
Processed prompts:  59%|    | 152/256 [00:18<00:03, 32.66it/s, est. speed input: 131.40 toks/s, output: 2102.32 toks/s]
Processed prompts:  66%|   | 168/256 [00:18<00:02, 41.68it/s, est. speed input: 144.40 toks/s, output: 2310.36 toks/s]
Processed prompts:  72%|  | 184/256 [00:18<00:01, 50.71it/s, est. speed input: 156.95 toks/s, output: 2511.18 toks/s]
Processed prompts:  77%|  | 198/256 [00:18<00:00, 58.28it/s, est. speed input: 167.65 toks/s, output: 2682.40 toks/s]
Processed prompts:  82%| | 211/256 [00:19<00:00, 65.25it/s, est. speed input: 177.43 toks/s, output: 2838.91 toks/s]
Processed prompts:  87%| | 223/256 [00:19<00:00, 68.55it/s, est. speed input: 186.07 toks/s, output: 2977.05 toks/s]
Processed prompts:  91%|| 234/256 [00:19<00:00, 67.58it/s, est. speed input: 193.53 toks/s, output: 3096.50 toks/s]
Processed prompts:  95%|| 244/256 [00:19<00:00, 61.82it/s, est. speed input: 199.69 toks/s, output: 3195.07 toks/s]
Processed prompts:  99%|| 253/256 [00:19<00:00, 51.21it/s, est. speed input: 204.22 toks/s, output: 3267.51 toks/s]
Processed prompts: 100%|| 256/256 [00:19<00:00, 51.21it/s, est. speed input: 204.90 toks/s, output: 3278.42 toks/s]
Processed prompts: 100%|| 256/256 [00:19<00:00, 12.81it/s, est. speed input: 204.90 toks/s, output: 3278.42 toks/s]
[rank0]:[W127 18:01:36.635440605 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-27 18:01:38
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_4
Params: prompt_len=16, output_len=256, num_prompts=512, max_num_seqs=512
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_4 --dataset-name random --input-len 16 --output-len 256 --num-prompts 512 --max-num-seqs 512 --max-model-len 272 --max-num-batched-tokens 512 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_4/json/Qwen2.5-7B-INT8_M512.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 18:01:42 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 18:01:42 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2904713) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2904713) WARNING 01-27 18:02:34 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 12.73 requests/s, 3462.61 total tokens/s, 3258.93 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072

STDERR:
[2026-01-27 18:01:42] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:01:42] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-27 18:01:42] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-27 18:01:42] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:01:42] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:01:42] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:01:42] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:01:42] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:01:42] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-27 18:01:42] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:01:42] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:01:42] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:01:42] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:01:42] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 18:01:45] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:01:45] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-27 18:01:45] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-27 18:01:45] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:01:45] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:01:45] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:01:45] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:01:45] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-27 18:01:45] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-27 18:01:45] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:01:45] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:01:45] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:01:45] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:01:45] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2904713) [2026-01-27 18:01:46] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2904713) [2026-01-27 18:01:46] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2904713) [2026-01-27 18:01:46] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2904713) [2026-01-27 18:01:46] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=2904713) [2026-01-27 18:01:46] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=2904713) [2026-01-27 18:01:46] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2904713) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2904713) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:19<00:19, 19.83s/it]
(EngineCore_DP0 pid=2904713) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:39<00:00, 20.00s/it]
(EngineCore_DP0 pid=2904713) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:39<00:00, 19.97s/it]
(EngineCore_DP0 pid=2904713) 
(EngineCore_DP0 pid=2904713) [2026-01-27 18:02:27] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 3584] -> 1D uint8
(EngineCore_DP0 pid=2904713) [2026-01-27 18:02:27] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12386304 bytes
(EngineCore_DP0 pid=2904713) [2026-01-27 18:02:27] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 3584] -> 1D uint8
(EngineCore_DP0 pid=2904713) [2026-01-27 18:02:27] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9633792 bytes
(EngineCore_DP0 pid=2904713) [2026-01-27 18:02:27] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 3584] -> 1D uint8
(EngineCore_DP0 pid=2904713) [2026-01-27 18:02:27] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 101842944 bytes
(EngineCore_DP0 pid=2904713) [2026-01-27 18:02:27] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 18944] -> 1D uint8
(EngineCore_DP0 pid=2904713) [2026-01-27 18:02:27] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 50921472 bytes
(EngineCore_DP0 pid=2904713) 2026-01-27 18:02:33,686 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2904713) 2026-01-27 18:02:33,706 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests: 100%|| 512/512 [00:00<00:00, 6181.64it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:29<4:12:32, 29.65s/it, est. speed input: 0.54 toks/s, output: 8.63 toks/s]
Processed prompts:   0%|          | 2/512 [00:29<1:45:21, 12.39s/it, est. speed input: 1.07 toks/s, output: 17.09 toks/s]
Processed prompts:   6%|         | 33/512 [00:30<03:44,  2.14it/s, est. speed input: 17.44 toks/s, output: 279.08 toks/s]
Processed prompts:  12%|        | 63/512 [00:30<01:32,  4.85it/s, est. speed input: 32.99 toks/s, output: 527.84 toks/s]
Processed prompts:  18%|        | 91/512 [00:30<00:51,  8.25it/s, est. speed input: 47.23 toks/s, output: 755.69 toks/s]
Processed prompts:  23%|       | 117/512 [00:31<00:31, 12.38it/s, est. speed input: 60.20 toks/s, output: 963.22 toks/s]
Processed prompts:  28%|       | 141/512 [00:31<00:20, 17.83it/s, est. speed input: 72.29 toks/s, output: 1156.61 toks/s]
Processed prompts:  32%|      | 164/512 [00:31<00:14, 24.80it/s, est. speed input: 83.78 toks/s, output: 1340.47 toks/s]
Processed prompts:  36%|      | 185/512 [00:31<00:09, 33.11it/s, est. speed input: 94.19 toks/s, output: 1507.06 toks/s]
Processed prompts:  40%|      | 205/512 [00:31<00:07, 43.14it/s, est. speed input: 104.03 toks/s, output: 1664.53 toks/s]
Processed prompts:  47%|     | 242/512 [00:31<00:04, 64.35it/s, est. speed input: 122.06 toks/s, output: 1952.97 toks/s]
Processed prompts:  53%|    | 273/512 [00:31<00:02, 80.95it/s, est. speed input: 136.88 toks/s, output: 2190.10 toks/s]
Processed prompts:  59%|    | 302/512 [00:32<00:02, 99.02it/s, est. speed input: 150.72 toks/s, output: 2411.56 toks/s]
Processed prompts:  64%|   | 327/512 [00:32<00:01, 111.17it/s, est. speed input: 162.43 toks/s, output: 2598.81 toks/s]
Processed prompts:  68%|   | 348/512 [00:32<00:01, 118.39it/s, est. speed input: 172.09 toks/s, output: 2753.51 toks/s]
Processed prompts:  72%|  | 367/512 [00:32<00:01, 124.78it/s, est. speed input: 180.78 toks/s, output: 2892.51 toks/s]
Processed prompts:  75%|  | 384/512 [00:32<00:00, 128.73it/s, est. speed input: 188.47 toks/s, output: 3015.48 toks/s]
Processed prompts:  78%|  | 400/512 [00:32<00:00, 127.74it/s, est. speed input: 195.55 toks/s, output: 3128.81 toks/s]
Processed prompts:  81%|  | 415/512 [00:32<00:00, 130.38it/s, est. speed input: 202.22 toks/s, output: 3235.50 toks/s]
Processed prompts:  84%| | 430/512 [00:32<00:00, 122.23it/s, est. speed input: 208.61 toks/s, output: 3337.74 toks/s]
Processed prompts:  87%| | 444/512 [00:33<00:00, 115.15it/s, est. speed input: 214.48 toks/s, output: 3431.64 toks/s]
Processed prompts:  89%| | 457/512 [00:33<00:00, 98.99it/s, est. speed input: 219.53 toks/s, output: 3512.45 toks/s] 
Processed prompts:  91%|| 468/512 [00:33<00:00, 88.44it/s, est. speed input: 223.69 toks/s, output: 3578.99 toks/s]
Processed prompts:  93%|| 478/512 [00:33<00:00, 78.60it/s, est. speed input: 227.29 toks/s, output: 3636.60 toks/s]
Processed prompts:  95%|| 487/512 [00:33<00:00, 61.74it/s, est. speed input: 229.86 toks/s, output: 3677.77 toks/s]
Processed prompts:  96%|| 494/512 [00:34<00:00, 51.40it/s, est. speed input: 231.63 toks/s, output: 3706.04 toks/s]
Processed prompts:  98%|| 500/512 [00:40<00:02,  4.69it/s, est. speed input: 199.76 toks/s, output: 3196.23 toks/s]
Processed prompts: 100%|| 512/512 [00:40<00:00,  4.69it/s, est. speed input: 204.11 toks/s, output: 3265.78 toks/s]
Processed prompts: 100%|| 512/512 [00:40<00:00, 12.76it/s, est. speed input: 204.11 toks/s, output: 3265.78 toks/s]
[rank0]:[W127 18:03:15.071633639 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=64 ==========
Time: 2026-01-27 19:09:13
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_4
Params: prompt_len=16, output_len=256, num_prompts=64, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_4 --dataset-name random --input-len 16 --output-len 256 --num-prompts 64 --max-num-seqs 64 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_4/json/Qwen2.5-14B-INT8_M64.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 19:09:20 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 19:09:20 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2975201) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2975201) WARNING 01-27 19:10:59 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 3.21 requests/s, 873.24 total tokens/s, 821.88 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384

STDERR:
[2026-01-27 19:09:20] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 19:09:20] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:09:20] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-27 19:09:20] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:09:20] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:09:20] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:09:20] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:09:20] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:09:20] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:09:20] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 19:09:20] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 19:09:20] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 19:09:20] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 19:09:20] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 19:09:23] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 19:09:23] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:09:23] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-27 19:09:23] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:09:23] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:09:23] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:09:23] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:09:23] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:09:23] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:09:23] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 19:09:23] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 19:09:23] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 19:09:23] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 19:09:23] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2975201) [2026-01-27 19:09:24] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2975201) [2026-01-27 19:09:24] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2975201) [2026-01-27 19:09:24] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2975201) [2026-01-27 19:09:24] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=2975201) [2026-01-27 19:09:24] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=2975201) [2026-01-27 19:09:24] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2975201) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2975201) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:08<00:25,  8.41s/it]
(EngineCore_DP0 pid=2975201) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:34<00:37, 18.62s/it]
(EngineCore_DP0 pid=2975201) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:55<00:19, 19.86s/it]
(EngineCore_DP0 pid=2975201) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:23<00:00, 22.90s/it]
(EngineCore_DP0 pid=2975201) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:23<00:00, 20.77s/it]
(EngineCore_DP0 pid=2975201) 
(EngineCore_DP0 pid=2975201) [2026-01-27 19:10:49] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [7168, 5120] -> 1D uint8
(EngineCore_DP0 pid=2975201) [2026-01-27 19:10:49] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 27525120 bytes
(EngineCore_DP0 pid=2975201) [2026-01-27 19:10:49] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 5120] -> 1D uint8
(EngineCore_DP0 pid=2975201) [2026-01-27 19:10:49] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19660800 bytes
(EngineCore_DP0 pid=2975201) [2026-01-27 19:10:49] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [27648, 5120] -> 1D uint8
(EngineCore_DP0 pid=2975201) [2026-01-27 19:10:49] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 106168320 bytes
(EngineCore_DP0 pid=2975201) [2026-01-27 19:10:49] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 13824] -> 1D uint8
(EngineCore_DP0 pid=2975201) [2026-01-27 19:10:49] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 53084160 bytes
(EngineCore_DP0 pid=2975201) 2026-01-27 19:10:58,687 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2975201) 2026-01-27 19:10:58,716 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests:   2%|         | 1/64 [00:00<00:08,  7.66it/s]
Adding requests:   5%|         | 3/64 [00:00<00:04, 13.89it/s]
Adding requests:  11%|         | 7/64 [00:00<00:02, 23.92it/s]
Adding requests:  19%|        | 12/64 [00:00<00:01, 32.18it/s]
Adding requests:  31%|      | 20/64 [00:00<00:00, 46.79it/s]
Adding requests:  52%|    | 33/64 [00:00<00:00, 72.70it/s]
Adding requests:  78%|  | 50/64 [00:00<00:00, 101.91it/s]
Adding requests: 100%|| 64/64 [00:00<00:00, 76.87it/s] 

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|         | 1/64 [00:18<19:34, 18.65s/it, est. speed input: 0.86 toks/s, output: 13.73 toks/s]
Processed prompts:  11%|         | 7/64 [00:18<01:52,  1.97s/it, est. speed input: 5.95 toks/s, output: 95.28 toks/s]
Processed prompts:  59%|    | 38/64 [00:18<00:06,  3.79it/s, est. speed input: 32.05 toks/s, output: 512.78 toks/s]
Processed prompts: 100%|| 64/64 [00:19<00:00,  3.79it/s, est. speed input: 53.81 toks/s, output: 860.93 toks/s]
Processed prompts: 100%|| 64/64 [00:19<00:00,  3.36it/s, est. speed input: 53.81 toks/s, output: 860.93 toks/s]
[rank0]:[W127 19:11:19.872897545 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-27 19:11:33
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_4
Params: prompt_len=16, output_len=256, num_prompts=128, max_num_seqs=128
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_4 --dataset-name random --input-len 16 --output-len 256 --num-prompts 128 --max-num-seqs 128 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_4/json/Qwen2.5-14B-INT8_M128.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 19:11:39 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 19:11:39 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2977766) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2977766) WARNING 01-27 19:13:17 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 5.26 requests/s, 1430.16 total tokens/s, 1346.03 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768

STDERR:
[2026-01-27 19:11:39] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 19:11:39] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:11:39] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-27 19:11:39] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:11:39] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:11:39] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:11:39] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:11:39] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:11:39] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:11:39] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 19:11:39] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 19:11:39] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 19:11:39] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 19:11:39] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 19:11:42] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 19:11:42] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:11:42] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-27 19:11:42] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:11:42] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:11:42] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:11:42] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:11:42] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:11:42] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:11:42] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 19:11:42] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 19:11:42] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 19:11:42] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 19:11:42] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2977766) [2026-01-27 19:11:43] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2977766) [2026-01-27 19:11:43] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2977766) [2026-01-27 19:11:43] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2977766) [2026-01-27 19:11:43] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=2977766) [2026-01-27 19:11:43] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=2977766) [2026-01-27 19:11:43] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2977766) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2977766) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:08<00:25,  8.41s/it]
(EngineCore_DP0 pid=2977766) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:35<00:38, 19.11s/it]
(EngineCore_DP0 pid=2977766) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:56<00:20, 20.10s/it]
(EngineCore_DP0 pid=2977766) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:23<00:00, 22.84s/it]
(EngineCore_DP0 pid=2977766) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:23<00:00, 20.83s/it]
(EngineCore_DP0 pid=2977766) 
(EngineCore_DP0 pid=2977766) [2026-01-27 19:13:08] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [7168, 5120] -> 1D uint8
(EngineCore_DP0 pid=2977766) [2026-01-27 19:13:08] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 27525120 bytes
(EngineCore_DP0 pid=2977766) [2026-01-27 19:13:08] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 5120] -> 1D uint8
(EngineCore_DP0 pid=2977766) [2026-01-27 19:13:08] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19660800 bytes
(EngineCore_DP0 pid=2977766) [2026-01-27 19:13:08] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [27648, 5120] -> 1D uint8
(EngineCore_DP0 pid=2977766) [2026-01-27 19:13:08] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 106168320 bytes
(EngineCore_DP0 pid=2977766) [2026-01-27 19:13:08] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 13824] -> 1D uint8
(EngineCore_DP0 pid=2977766) [2026-01-27 19:13:08] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 53084160 bytes
(EngineCore_DP0 pid=2977766) 2026-01-27 19:13:16,624 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2977766) 2026-01-27 19:13:16,654 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|| 128/128 [00:00<00:00, 3620.14it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:23<49:22, 23.33s/it, est. speed input: 0.69 toks/s, output: 10.97 toks/s]
Processed prompts:   2%|         | 2/128 [00:23<20:18,  9.67s/it, est. speed input: 1.37 toks/s, output: 21.84 toks/s]
Processed prompts:  14%|        | 18/128 [00:23<01:14,  1.48it/s, est. speed input: 12.23 toks/s, output: 195.65 toks/s]
Processed prompts:  26%|       | 33/128 [00:23<00:28,  3.28it/s, est. speed input: 22.32 toks/s, output: 357.06 toks/s]
Processed prompts:  48%|     | 62/128 [00:23<00:08,  8.06it/s, est. speed input: 41.62 toks/s, output: 665.85 toks/s]
Processed prompts:  68%|   | 87/128 [00:24<00:03, 13.47it/s, est. speed input: 57.95 toks/s, output: 927.25 toks/s]
Processed prompts:  84%| | 108/128 [00:24<00:01, 19.43it/s, est. speed input: 71.50 toks/s, output: 1143.99 toks/s]
Processed prompts:  99%|| 127/128 [00:24<00:00, 26.56it/s, est. speed input: 83.68 toks/s, output: 1338.82 toks/s]
Processed prompts: 100%|| 128/128 [00:24<00:00, 26.56it/s, est. speed input: 84.33 toks/s, output: 1349.36 toks/s]
Processed prompts: 100%|| 128/128 [00:24<00:00,  5.27it/s, est. speed input: 84.33 toks/s, output: 1349.36 toks/s]
[rank0]:[W127 19:13:42.257676139 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-27 19:13:45
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_4
Params: prompt_len=16, output_len=256, num_prompts=256, max_num_seqs=256
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_4 --dataset-name random --input-len 16 --output-len 256 --num-prompts 256 --max-num-seqs 256 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_4/json/Qwen2.5-14B-INT8_M256.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 19:13:51 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 19:13:51 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2980166) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2980166) WARNING 01-27 19:15:31 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 6.69 requests/s, 1820.81 total tokens/s, 1713.71 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536

STDERR:
[2026-01-27 19:13:51] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 19:13:51] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:13:51] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-27 19:13:51] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:13:51] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:13:51] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:13:51] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:13:51] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:13:51] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:13:51] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 19:13:51] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 19:13:51] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 19:13:51] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 19:13:51] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 19:13:54] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 19:13:54] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:13:54] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-27 19:13:54] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:13:54] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:13:54] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:13:54] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:13:54] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:13:54] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:13:54] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 19:13:54] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 19:13:54] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 19:13:54] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 19:13:54] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2980166) [2026-01-27 19:13:55] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2980166) [2026-01-27 19:13:55] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2980166) [2026-01-27 19:13:55] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2980166) [2026-01-27 19:13:55] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=2980166) [2026-01-27 19:13:55] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=2980166) [2026-01-27 19:13:55] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2980166) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2980166) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:08<00:25,  8.61s/it]
(EngineCore_DP0 pid=2980166) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:35<00:38, 19.43s/it]
(EngineCore_DP0 pid=2980166) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:57<00:20, 20.37s/it]
(EngineCore_DP0 pid=2980166) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:24<00:00, 23.15s/it]
(EngineCore_DP0 pid=2980166) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:24<00:00, 21.13s/it]
(EngineCore_DP0 pid=2980166) 
(EngineCore_DP0 pid=2980166) [2026-01-27 19:15:21] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [7168, 5120] -> 1D uint8
(EngineCore_DP0 pid=2980166) [2026-01-27 19:15:21] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 27525120 bytes
(EngineCore_DP0 pid=2980166) [2026-01-27 19:15:21] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 5120] -> 1D uint8
(EngineCore_DP0 pid=2980166) [2026-01-27 19:15:21] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19660800 bytes
(EngineCore_DP0 pid=2980166) [2026-01-27 19:15:21] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [27648, 5120] -> 1D uint8
(EngineCore_DP0 pid=2980166) [2026-01-27 19:15:21] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 106168320 bytes
(EngineCore_DP0 pid=2980166) [2026-01-27 19:15:21] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 13824] -> 1D uint8
(EngineCore_DP0 pid=2980166) [2026-01-27 19:15:21] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 53084160 bytes
(EngineCore_DP0 pid=2980166) 2026-01-27 19:15:29,887 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2980166) 2026-01-27 19:15:29,917 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   0%|          | 1/256 [00:00<00:58,  4.39it/s]
Adding requests:   1%|          | 3/256 [00:00<00:26,  9.56it/s]
Adding requests:   2%|         | 6/256 [00:00<00:15, 16.42it/s]
Adding requests:   4%|         | 11/256 [00:00<00:09, 25.17it/s]
Adding requests:   7%|         | 17/256 [00:00<00:06, 35.32it/s]
Adding requests:  11%|         | 28/256 [00:00<00:04, 56.31it/s]
Adding requests:  17%|        | 44/256 [00:00<00:02, 86.12it/s]
Adding requests:  26%|       | 67/256 [00:00<00:01, 127.76it/s]
Adding requests:  37%|      | 94/256 [00:01<00:00, 169.08it/s]
Adding requests:  52%|    | 132/256 [00:01<00:00, 230.85it/s]
Adding requests:  63%|   | 161/256 [00:01<00:00, 159.58it/s]
Adding requests:  77%|  | 198/256 [00:01<00:00, 203.58it/s]
Adding requests:  99%|| 253/256 [00:01<00:00, 284.08it/s]
Adding requests: 100%|| 256/256 [00:01<00:00, 151.42it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:31<2:12:58, 31.29s/it, est. speed input: 0.51 toks/s, output: 8.18 toks/s]
Processed prompts:   1%|          | 2/256 [00:31<54:55, 12.97s/it, est. speed input: 1.02 toks/s, output: 16.28 toks/s] 
Processed prompts:   2%|         | 5/256 [00:31<15:33,  3.72s/it, est. speed input: 2.53 toks/s, output: 40.52 toks/s]
Processed prompts:   3%|         | 7/256 [00:31<09:17,  2.24s/it, est. speed input: 3.53 toks/s, output: 56.47 toks/s]
Processed prompts:   4%|         | 11/256 [00:31<04:19,  1.06s/it, est. speed input: 5.52 toks/s, output: 88.33 toks/s]
Processed prompts:   5%|         | 14/256 [00:32<02:46,  1.45it/s, est. speed input: 6.99 toks/s, output: 111.90 toks/s]
Processed prompts:   7%|         | 18/256 [00:32<01:40,  2.36it/s, est. speed input: 8.95 toks/s, output: 143.13 toks/s]
Processed prompts:  10%|         | 25/256 [00:32<00:50,  4.56it/s, est. speed input: 12.37 toks/s, output: 197.92 toks/s]
Processed prompts:  14%|        | 36/256 [00:32<00:24,  9.06it/s, est. speed input: 17.73 toks/s, output: 283.62 toks/s]
Processed prompts:  20%|        | 50/256 [00:32<00:12, 16.48it/s, est. speed input: 24.52 toks/s, output: 392.35 toks/s]
Processed prompts:  25%|       | 63/256 [00:32<00:07, 24.58it/s, est. speed input: 30.77 toks/s, output: 492.39 toks/s]
Processed prompts:  30%|       | 76/256 [00:32<00:05, 33.41it/s, est. speed input: 36.97 toks/s, output: 591.44 toks/s]
Processed prompts:  34%|      | 88/256 [00:33<00:04, 41.42it/s, est. speed input: 42.62 toks/s, output: 681.85 toks/s]
Processed prompts:  39%|      | 99/256 [00:33<00:03, 48.83it/s, est. speed input: 47.75 toks/s, output: 764.08 toks/s]
Processed prompts:  43%|     | 109/256 [00:33<00:02, 56.19it/s, est. speed input: 52.41 toks/s, output: 838.52 toks/s]
Processed prompts:  46%|     | 119/256 [00:33<00:02, 63.40it/s, est. speed input: 57.03 toks/s, output: 912.52 toks/s]
Processed prompts:  50%|     | 128/256 [00:33<00:01, 68.13it/s, est. speed input: 61.15 toks/s, output: 978.45 toks/s]
Processed prompts:  54%|    | 137/256 [00:33<00:01, 70.82it/s, est. speed input: 65.23 toks/s, output: 1043.70 toks/s]
Processed prompts:  57%|    | 146/256 [00:33<00:01, 73.01it/s, est. speed input: 69.28 toks/s, output: 1108.52 toks/s]
Processed prompts:  61%|    | 155/256 [00:33<00:01, 76.12it/s, est. speed input: 73.32 toks/s, output: 1173.17 toks/s]
Processed prompts:  65%|   | 167/256 [00:34<00:01, 71.99it/s, est. speed input: 78.57 toks/s, output: 1257.18 toks/s]
Processed prompts:  70%|   | 179/256 [00:34<00:01, 70.97it/s, est. speed input: 83.79 toks/s, output: 1340.67 toks/s]
Processed prompts:  74%|  | 189/256 [00:34<00:00, 67.28it/s, est. speed input: 88.04 toks/s, output: 1408.64 toks/s]
Processed prompts:  77%|  | 198/256 [00:34<00:00, 64.42it/s, est. speed input: 91.82 toks/s, output: 1469.05 toks/s]
Processed prompts:  80%|  | 206/256 [00:34<00:00, 61.01it/s, est. speed input: 95.11 toks/s, output: 1521.73 toks/s]
Processed prompts:  83%| | 213/256 [00:34<00:00, 57.59it/s, est. speed input: 97.94 toks/s, output: 1566.97 toks/s]
Processed prompts:  86%| | 219/256 [00:34<00:00, 53.33it/s, est. speed input: 100.29 toks/s, output: 1604.63 toks/s]
Processed prompts:  88%| | 225/256 [00:35<00:00, 49.38it/s, est. speed input: 102.60 toks/s, output: 1641.59 toks/s]
Processed prompts:  90%| | 230/256 [00:35<00:00, 46.21it/s, est. speed input: 104.49 toks/s, output: 1671.78 toks/s]
Processed prompts:  92%|| 235/256 [00:35<00:00, 43.75it/s, est. speed input: 106.36 toks/s, output: 1701.69 toks/s]
Processed prompts:  94%|| 240/256 [00:35<00:00, 37.14it/s, est. speed input: 108.03 toks/s, output: 1728.46 toks/s]
Processed prompts:  95%|| 244/256 [00:35<00:00, 32.31it/s, est. speed input: 109.28 toks/s, output: 1748.50 toks/s]
Processed prompts:  97%|| 248/256 [00:35<00:00, 26.62it/s, est. speed input: 110.36 toks/s, output: 1765.73 toks/s]
Processed prompts:  98%|| 251/256 [00:36<00:00, 24.08it/s, est. speed input: 111.17 toks/s, output: 1778.70 toks/s]
Processed prompts:  99%|| 254/256 [00:36<00:00, 22.32it/s, est. speed input: 111.98 toks/s, output: 1791.66 toks/s]
Processed prompts: 100%|| 256/256 [00:36<00:00, 22.32it/s, est. speed input: 112.41 toks/s, output: 1798.60 toks/s]
Processed prompts: 100%|| 256/256 [00:36<00:00,  7.03it/s, est. speed input: 112.41 toks/s, output: 1798.60 toks/s]
[rank0]:[W127 19:16:10.395655239 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-27 19:16:25
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_4
Params: prompt_len=16, output_len=256, num_prompts=512, max_num_seqs=512
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_4 --dataset-name random --input-len 16 --output-len 256 --num-prompts 512 --max-num-seqs 512 --max-model-len 272 --max-num-batched-tokens 512 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_4/json/Qwen2.5-14B-INT8_M512.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-27 19:16:28 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 19:16:28 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2982989) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2982989) WARNING 01-27 19:18:06 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 6.81 requests/s, 1853.35 total tokens/s, 1744.33 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072

STDERR:
[2026-01-27 19:16:28] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 19:16:28] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:16:28] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-27 19:16:28] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:16:28] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:16:28] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:16:28] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:16:28] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:16:28] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:16:28] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 19:16:28] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 19:16:28] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 19:16:28] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 19:16:28] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 19:16:31] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 19:16:31] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:16:31] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-27 19:16:31] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:16:31] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:16:31] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:16:31] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:16:31] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:16:31] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:16:31] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 19:16:31] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 19:16:31] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 19:16:31] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 19:16:31] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2982989) [2026-01-27 19:16:32] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2982989) [2026-01-27 19:16:32] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2982989) [2026-01-27 19:16:32] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2982989) [2026-01-27 19:16:32] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=2982989) [2026-01-27 19:16:32] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=2982989) [2026-01-27 19:16:32] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2982989) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2982989) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:08<00:25,  8.60s/it]
(EngineCore_DP0 pid=2982989) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:35<00:38, 19.11s/it]
(EngineCore_DP0 pid=2982989) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:55<00:19, 19.93s/it]
(EngineCore_DP0 pid=2982989) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:23<00:00, 22.95s/it]
(EngineCore_DP0 pid=2982989) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:23<00:00, 20.89s/it]
(EngineCore_DP0 pid=2982989) 
(EngineCore_DP0 pid=2982989) [2026-01-27 19:17:57] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [7168, 5120] -> 1D uint8
(EngineCore_DP0 pid=2982989) [2026-01-27 19:17:57] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 27525120 bytes
(EngineCore_DP0 pid=2982989) [2026-01-27 19:17:57] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 5120] -> 1D uint8
(EngineCore_DP0 pid=2982989) [2026-01-27 19:17:57] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19660800 bytes
(EngineCore_DP0 pid=2982989) [2026-01-27 19:17:57] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [27648, 5120] -> 1D uint8
(EngineCore_DP0 pid=2982989) [2026-01-27 19:17:57] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 106168320 bytes
(EngineCore_DP0 pid=2982989) [2026-01-27 19:17:57] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 13824] -> 1D uint8
(EngineCore_DP0 pid=2982989) [2026-01-27 19:17:57] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 53084160 bytes
(EngineCore_DP0 pid=2982989) 2026-01-27 19:18:05,374 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2982989) 2026-01-27 19:18:05,411 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests: 100%|| 512/512 [00:00<00:00, 11496.78it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:54<7:46:39, 54.79s/it, est. speed input: 0.29 toks/s, output: 4.67 toks/s]
Processed prompts:   0%|          | 2/512 [00:55<3:15:15, 22.97s/it, est. speed input: 0.58 toks/s, output: 9.23 toks/s]
Processed prompts:   6%|         | 33/512 [00:56<06:56,  1.15it/s, est. speed input: 9.40 toks/s, output: 150.47 toks/s]
Processed prompts:  12%|        | 63/512 [00:56<02:52,  2.60it/s, est. speed input: 17.76 toks/s, output: 284.09 toks/s]
Processed prompts:  18%|        | 91/512 [00:57<01:35,  4.39it/s, est. speed input: 25.39 toks/s, output: 406.16 toks/s]
Processed prompts:  23%|       | 117/512 [00:57<01:00,  6.56it/s, est. speed input: 32.32 toks/s, output: 517.13 toks/s]
Processed prompts:  28%|       | 141/512 [00:58<00:39,  9.44it/s, est. speed input: 38.81 toks/s, output: 620.91 toks/s]
Processed prompts:  32%|      | 164/512 [00:58<00:26, 13.15it/s, est. speed input: 44.98 toks/s, output: 719.69 toks/s]
Processed prompts:  36%|      | 185/512 [00:58<00:18, 17.57it/s, est. speed input: 50.57 toks/s, output: 809.15 toks/s]
Processed prompts:  40%|      | 205/512 [00:58<00:13, 22.90it/s, est. speed input: 55.85 toks/s, output: 893.66 toks/s]
Processed prompts:  44%|     | 224/512 [00:58<00:09, 29.23it/s, est. speed input: 60.85 toks/s, output: 973.54 toks/s]
Processed prompts:  47%|     | 242/512 [00:59<00:07, 36.33it/s, est. speed input: 65.54 toks/s, output: 1048.70 toks/s]
Processed prompts:  50%|     | 258/512 [00:59<00:05, 42.49it/s, est. speed input: 69.65 toks/s, output: 1114.42 toks/s]
Processed prompts:  53%|    | 273/512 [00:59<00:04, 48.69it/s, est. speed input: 73.48 toks/s, output: 1175.70 toks/s]
Processed prompts:  56%|    | 288/512 [00:59<00:03, 57.25it/s, est. speed input: 77.34 toks/s, output: 1237.47 toks/s]
Processed prompts:  59%|    | 302/512 [00:59<00:03, 63.24it/s, est. speed input: 80.89 toks/s, output: 1294.22 toks/s]
Processed prompts:  62%|   | 315/512 [00:59<00:02, 68.75it/s, est. speed input: 84.17 toks/s, output: 1346.74 toks/s]
Processed prompts:  64%|   | 327/512 [01:00<00:02, 72.86it/s, est. speed input: 87.18 toks/s, output: 1394.87 toks/s]
Processed prompts:  66%|   | 338/512 [01:00<00:02, 74.73it/s, est. speed input: 89.91 toks/s, output: 1438.54 toks/s]
Processed prompts:  68%|   | 348/512 [01:00<00:02, 77.30it/s, est. speed input: 92.39 toks/s, output: 1478.26 toks/s]
Processed prompts:  70%|   | 358/512 [01:00<00:01, 77.53it/s, est. speed input: 94.84 toks/s, output: 1517.52 toks/s]
Processed prompts:  72%|  | 367/512 [01:00<00:01, 78.79it/s, est. speed input: 97.05 toks/s, output: 1552.87 toks/s]
Processed prompts:  73%|  | 376/512 [01:00<00:01, 77.53it/s, est. speed input: 99.24 toks/s, output: 1587.77 toks/s]
Processed prompts:  75%|  | 385/512 [01:00<00:01, 79.18it/s, est. speed input: 101.43 toks/s, output: 1622.91 toks/s]
Processed prompts:  77%|  | 394/512 [01:00<00:01, 76.30it/s, est. speed input: 103.58 toks/s, output: 1657.33 toks/s]
Processed prompts:  79%|  | 406/512 [01:01<00:01, 69.53it/s, est. speed input: 106.39 toks/s, output: 1702.18 toks/s]
Processed prompts:  82%| | 418/512 [01:01<00:01, 66.81it/s, est. speed input: 109.18 toks/s, output: 1746.94 toks/s]
Processed prompts:  84%| | 428/512 [01:01<00:01, 64.38it/s, est. speed input: 111.49 toks/s, output: 1783.80 toks/s]
Processed prompts:  85%| | 435/512 [01:01<00:01, 65.50it/s, est. speed input: 113.13 toks/s, output: 1810.02 toks/s]
Processed prompts:  86%| | 442/512 [01:01<00:01, 58.51it/s, est. speed input: 114.65 toks/s, output: 1834.37 toks/s]
Processed prompts:  88%| | 450/512 [01:01<00:01, 52.69it/s, est. speed input: 116.36 toks/s, output: 1861.82 toks/s]
Processed prompts:  89%| | 456/512 [01:02<00:01, 49.20it/s, est. speed input: 117.63 toks/s, output: 1882.12 toks/s]
Processed prompts:  90%| | 462/512 [01:02<00:01, 46.97it/s, est. speed input: 118.90 toks/s, output: 1902.43 toks/s]
Processed prompts:  91%|| 468/512 [01:02<00:00, 44.22it/s, est. speed input: 120.14 toks/s, output: 1922.25 toks/s]
Processed prompts:  92%|| 473/512 [01:02<00:00, 42.04it/s, est. speed input: 121.16 toks/s, output: 1938.51 toks/s]
Processed prompts:  93%|| 478/512 [01:02<00:00, 35.86it/s, est. speed input: 122.05 toks/s, output: 1952.75 toks/s]
Processed prompts:  94%|| 482/512 [01:02<00:00, 33.36it/s, est. speed input: 122.78 toks/s, output: 1964.43 toks/s]
Processed prompts:  95%|| 486/512 [01:03<00:00, 26.28it/s, est. speed input: 123.30 toks/s, output: 1972.80 toks/s]
Processed prompts:  96%|| 489/512 [01:03<00:00, 23.22it/s, est. speed input: 123.69 toks/s, output: 1979.10 toks/s]
Processed prompts:  96%|| 492/512 [01:03<00:00, 21.25it/s, est. speed input: 124.10 toks/s, output: 1985.54 toks/s]
Processed prompts:  97%|| 495/512 [01:03<00:00, 19.75it/s, est. speed input: 124.49 toks/s, output: 1991.84 toks/s]
Processed prompts:  97%|| 498/512 [01:14<00:14,  1.01s/it, est. speed input: 106.43 toks/s, output: 1702.83 toks/s]
Processed prompts:  98%|| 501/512 [01:14<00:08,  1.34it/s, est. speed input: 106.90 toks/s, output: 1710.44 toks/s]
Processed prompts: 100%|| 511/512 [01:15<00:00,  3.00it/s, est. speed input: 108.87 toks/s, output: 1741.99 toks/s]
Processed prompts: 100%|| 512/512 [01:15<00:00,  3.00it/s, est. speed input: 109.09 toks/s, output: 1745.40 toks/s]
Processed prompts: 100%|| 512/512 [01:15<00:00,  6.82it/s, est. speed input: 109.09 toks/s, output: 1745.40 toks/s]
[rank0]:[W127 19:19:22.127372634 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=64 ==========
Time: 2026-01-28 09:48:20
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_4
Params: prompt_len=16, output_len=256, num_prompts=64, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_4 --dataset-name random --input-len 16 --output-len 256 --num-prompts 64 --max-num-seqs 64 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_4/json/BitNet-2B-INT8_M64.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-28 09:48:23 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 09:48:23 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3766491) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3766491) WARNING 01-28 09:48:47 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 11.09 requests/s, 3015.79 total tokens/s, 2838.39 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384

STDERR:
[2026-01-28 09:48:23] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:48:23] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 09:48:23] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 09:48:23] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:48:23] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:48:23] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:48:23] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:48:23] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:48:23] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 09:48:23] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:48:23] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:48:23] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:48:23] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:48:23] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 09:48:27] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:48:27] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 09:48:27] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 09:48:27] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:48:27] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:48:27] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:48:27] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:48:27] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:48:27] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 09:48:27] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:48:27] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:48:27] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:48:27] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:48:27] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3766491) [2026-01-28 09:48:28] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3766491) [2026-01-28 09:48:28] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3766491) [2026-01-28 09:48:28] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3766491) [2026-01-28 09:48:28] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3766491) [2026-01-28 09:48:28] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3766491) [2026-01-28 09:48:28] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3766491) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3766491) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:12<00:00, 12.09s/it]
(EngineCore_DP0 pid=3766491) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:12<00:00, 12.09s/it]
(EngineCore_DP0 pid=3766491) 
(EngineCore_DP0 pid=3766491) [2026-01-28 09:48:40] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3766491) [2026-01-28 09:48:40] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=3766491) [2026-01-28 09:48:40] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3766491) [2026-01-28 09:48:40] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4915200 bytes
(EngineCore_DP0 pid=3766491) [2026-01-28 09:48:40] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3766491) [2026-01-28 09:48:40] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 26542080 bytes
(EngineCore_DP0 pid=3766491) [2026-01-28 09:48:40] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3766491) [2026-01-28 09:48:40] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 13271040 bytes
(EngineCore_DP0 pid=3766491) 2026-01-28 09:48:46,518 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3766491) 2026-01-28 09:48:46,543 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|| 64/64 [00:00<00:00, 12167.32it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|         | 1/64 [00:05<05:53,  5.62s/it, est. speed input: 2.85 toks/s, output: 45.58 toks/s]
Processed prompts:  56%|    | 36/64 [00:05<00:03,  8.84it/s, est. speed input: 100.67 toks/s, output: 1610.68 toks/s]
Processed prompts: 100%|| 64/64 [00:05<00:00,  8.84it/s, est. speed input: 177.59 toks/s, output: 2841.46 toks/s]
Processed prompts: 100%|| 64/64 [00:05<00:00, 11.10it/s, est. speed input: 177.59 toks/s, output: 2841.46 toks/s]
[rank0]:[W128 09:48:53.023144263 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-28 09:48:55
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_4
Params: prompt_len=16, output_len=256, num_prompts=128, max_num_seqs=128
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_4 --dataset-name random --input-len 16 --output-len 256 --num-prompts 128 --max-num-seqs 128 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_4/json/BitNet-2B-INT8_M128.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-28 09:48:58 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 09:48:58 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3767175) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3767175) WARNING 01-28 09:49:21 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 16.14 requests/s, 4389.73 total tokens/s, 4131.51 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768

STDERR:
[2026-01-28 09:48:58] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:48:58] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 09:48:58] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 09:48:58] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:48:58] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:48:58] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:48:58] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:48:58] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:48:58] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 09:48:58] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:48:58] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:48:58] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:48:58] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:48:58] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 09:49:02] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:49:02] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 09:49:02] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 09:49:02] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:49:02] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:49:02] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:49:02] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:49:02] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:49:02] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 09:49:02] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:49:02] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:49:02] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:49:02] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:49:02] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3767175) [2026-01-28 09:49:03] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3767175) [2026-01-28 09:49:03] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3767175) [2026-01-28 09:49:03] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3767175) [2026-01-28 09:49:03] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3767175) [2026-01-28 09:49:03] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3767175) [2026-01-28 09:49:03] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3767175) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3767175) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.58s/it]
(EngineCore_DP0 pid=3767175) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.58s/it]
(EngineCore_DP0 pid=3767175) 
(EngineCore_DP0 pid=3767175) [2026-01-28 09:49:15] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3767175) [2026-01-28 09:49:15] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=3767175) [2026-01-28 09:49:15] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3767175) [2026-01-28 09:49:15] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4915200 bytes
(EngineCore_DP0 pid=3767175) [2026-01-28 09:49:15] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3767175) [2026-01-28 09:49:15] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 26542080 bytes
(EngineCore_DP0 pid=3767175) [2026-01-28 09:49:15] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3767175) [2026-01-28 09:49:15] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 13271040 bytes
(EngineCore_DP0 pid=3767175) 2026-01-28 09:49:20,973 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3767175) 2026-01-28 09:49:20,985 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|| 128/128 [00:00<00:00, 12873.37it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:07<15:50,  7.48s/it, est. speed input: 2.14 toks/s, output: 34.21 toks/s]
Processed prompts:  28%|       | 36/128 [00:07<00:13,  6.65it/s, est. speed input: 75.64 toks/s, output: 1210.17 toks/s]
Processed prompts:  69%|   | 88/128 [00:07<00:02, 19.83it/s, est. speed input: 181.94 toks/s, output: 2911.01 toks/s]
Processed prompts: 100%|| 128/128 [00:07<00:00, 32.38it/s, est. speed input: 258.57 toks/s, output: 4137.16 toks/s]
Processed prompts: 100%|| 128/128 [00:07<00:00, 32.38it/s, est. speed input: 258.57 toks/s, output: 4137.16 toks/s]
Processed prompts: 100%|| 128/128 [00:07<00:00, 16.16it/s, est. speed input: 258.57 toks/s, output: 4137.16 toks/s]
[rank0]:[W128 09:49:29.779493924 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-28 09:49:32
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_4
Params: prompt_len=16, output_len=256, num_prompts=256, max_num_seqs=256
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_4 --dataset-name random --input-len 16 --output-len 256 --num-prompts 256 --max-num-seqs 256 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_4/json/BitNet-2B-INT8_M256.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-28 09:49:35 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 09:49:35 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3767892) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3767892) WARNING 01-28 09:49:57 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 20.06 requests/s, 5455.62 total tokens/s, 5134.70 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536

STDERR:
[2026-01-28 09:49:35] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:49:35] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 09:49:35] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 09:49:35] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:49:35] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:49:35] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:49:35] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:49:35] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:49:35] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 09:49:35] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:49:35] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:49:35] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:49:35] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:49:35] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 09:49:38] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:49:38] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 09:49:38] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 09:49:38] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:49:38] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:49:38] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:49:38] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:49:38] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:49:38] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 09:49:38] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:49:38] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:49:38] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:49:38] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:49:38] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3767892) [2026-01-28 09:49:39] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3767892) [2026-01-28 09:49:39] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3767892) [2026-01-28 09:49:39] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3767892) [2026-01-28 09:49:39] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3767892) [2026-01-28 09:49:39] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3767892) [2026-01-28 09:49:39] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3767892) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3767892) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.33s/it]
(EngineCore_DP0 pid=3767892) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.33s/it]
(EngineCore_DP0 pid=3767892) 
(EngineCore_DP0 pid=3767892) [2026-01-28 09:49:51] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3767892) [2026-01-28 09:49:51] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=3767892) [2026-01-28 09:49:51] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3767892) [2026-01-28 09:49:51] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4915200 bytes
(EngineCore_DP0 pid=3767892) [2026-01-28 09:49:51] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3767892) [2026-01-28 09:49:51] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 26542080 bytes
(EngineCore_DP0 pid=3767892) [2026-01-28 09:49:51] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3767892) [2026-01-28 09:49:51] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 13271040 bytes
(EngineCore_DP0 pid=3767892) 2026-01-28 09:49:57,067 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3767892) 2026-01-28 09:49:57,079 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|| 256/256 [00:00<00:00, 13279.19it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:11<47:32, 11.18s/it, est. speed input: 1.43 toks/s, output: 22.89 toks/s]
Processed prompts:   9%|         | 22/256 [00:11<01:25,  2.73it/s, est. speed input: 31.15 toks/s, output: 498.34 toks/s]
Processed prompts:  20%|        | 51/256 [00:11<00:26,  7.78it/s, est. speed input: 71.44 toks/s, output: 1143.08 toks/s]
Processed prompts:  35%|      | 89/256 [00:11<00:10, 16.69it/s, est. speed input: 123.05 toks/s, output: 1968.72 toks/s]
Processed prompts:  47%|     | 120/256 [00:11<00:05, 26.10it/s, est. speed input: 163.83 toks/s, output: 2621.22 toks/s]
Processed prompts:  57%|    | 146/256 [00:11<00:03, 36.08it/s, est. speed input: 197.10 toks/s, output: 3153.64 toks/s]
Processed prompts:  66%|   | 168/256 [00:11<00:01, 46.96it/s, est. speed input: 224.86 toks/s, output: 3597.74 toks/s]
Processed prompts:  74%|  | 190/256 [00:12<00:01, 60.39it/s, est. speed input: 252.14 toks/s, output: 4034.31 toks/s]
Processed prompts:  83%| | 212/256 [00:12<00:00, 73.67it/s, est. speed input: 278.18 toks/s, output: 4450.86 toks/s]
Processed prompts:  91%| | 232/256 [00:12<00:00, 82.60it/s, est. speed input: 300.38 toks/s, output: 4806.15 toks/s]
Processed prompts:  98%|| 250/256 [00:12<00:00, 82.55it/s, est. speed input: 318.07 toks/s, output: 5089.08 toks/s]
Processed prompts: 100%|| 256/256 [00:12<00:00, 82.55it/s, est. speed input: 321.43 toks/s, output: 5142.92 toks/s]
Processed prompts: 100%|| 256/256 [00:12<00:00, 20.09it/s, est. speed input: 321.43 toks/s, output: 5142.92 toks/s]
[rank0]:[W128 09:50:10.711031711 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-28 09:50:12
Backend: cuSPARSELt (2:4)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_4
Params: prompt_len=16, output_len=256, num_prompts=512, max_num_seqs=512
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_4 --dataset-name random --input-len 16 --output-len 256 --num-prompts 512 --max-num-seqs 512 --max-model-len 272 --max-num-batched-tokens 512 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_4/json/BitNet-2B-INT8_M512.json --enforce-eager

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-28 09:50:16 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-28 09:50:16 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3768701) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3768701) WARNING 01-28 09:50:38 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 20.59 requests/s, 5599.42 total tokens/s, 5270.05 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072

STDERR:
[2026-01-28 09:50:16] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:50:16] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 09:50:16] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 09:50:16] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:50:16] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:50:16] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:50:16] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:50:16] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:50:16] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 09:50:16] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:50:16] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:50:16] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:50:16] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:50:16] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 09:50:19] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-28 09:50:19] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 09:50:19] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 09:50:19] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:50:19] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:50:19] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:50:19] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:50:19] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 09:50:19] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 09:50:19] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:50:19] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:50:19] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:50:19] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:50:19] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3768701) [2026-01-28 09:50:20] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3768701) [2026-01-28 09:50:20] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3768701) [2026-01-28 09:50:20] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3768701) [2026-01-28 09:50:20] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3768701) [2026-01-28 09:50:20] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3768701) [2026-01-28 09:50:20] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3768701) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3768701) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.50s/it]
(EngineCore_DP0 pid=3768701) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:11<00:00, 11.50s/it]
(EngineCore_DP0 pid=3768701) 
(EngineCore_DP0 pid=3768701) [2026-01-28 09:50:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3768701) [2026-01-28 09:50:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7372800 bytes
(EngineCore_DP0 pid=3768701) [2026-01-28 09:50:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3768701) [2026-01-28 09:50:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4915200 bytes
(EngineCore_DP0 pid=3768701) [2026-01-28 09:50:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3768701) [2026-01-28 09:50:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 26542080 bytes
(EngineCore_DP0 pid=3768701) [2026-01-28 09:50:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3768701) [2026-01-28 09:50:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 13271040 bytes
(EngineCore_DP0 pid=3768701) 2026-01-28 09:50:38,334 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3768701) 2026-01-28 09:50:38,345 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests: 100%|| 512/512 [00:00<00:00, 14515.32it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:18<2:41:00, 18.91s/it, est. speed input: 0.85 toks/s, output: 13.54 toks/s]
Processed prompts:   1%|          | 6/512 [00:19<19:54,  2.36s/it, est. speed input: 5.02 toks/s, output: 80.32 toks/s]  
Processed prompts:   7%|         | 37/512 [00:19<02:10,  3.64it/s, est. speed input: 30.63 toks/s, output: 490.12 toks/s]
Processed prompts:  13%|        | 66/512 [00:19<00:57,  7.72it/s, est. speed input: 54.10 toks/s, output: 865.61 toks/s]
Processed prompts:  18%|        | 93/512 [00:19<00:32, 12.82it/s, est. speed input: 75.54 toks/s, output: 1208.65 toks/s]
Processed prompts:  23%|       | 119/512 [00:19<00:20, 19.22it/s, est. speed input: 95.78 toks/s, output: 1532.54 toks/s]
Processed prompts:  32%|      | 166/512 [00:20<00:09, 35.06it/s, est. speed input: 132.48 toks/s, output: 2119.63 toks/s]
Processed prompts:  40%|      | 207/512 [00:20<00:05, 52.24it/s, est. speed input: 163.97 toks/s, output: 2623.56 toks/s]
Processed prompts:  47%|     | 243/512 [00:20<00:03, 70.25it/s, est. speed input: 191.18 toks/s, output: 3058.82 toks/s]
Processed prompts:  54%|    | 274/512 [00:20<00:02, 87.92it/s, est. speed input: 214.22 toks/s, output: 3427.47 toks/s]
Processed prompts:  59%|    | 302/512 [00:20<00:01, 106.84it/s, est. speed input: 234.87 toks/s, output: 3757.93 toks/s]
Processed prompts:  64%|   | 327/512 [00:20<00:01, 123.21it/s, est. speed input: 252.94 toks/s, output: 4047.02 toks/s]
Processed prompts:  70%|   | 358/512 [00:20<00:01, 142.12it/s, est. speed input: 275.01 toks/s, output: 4400.09 toks/s]
Processed prompts:  75%|  | 384/512 [00:20<00:00, 157.92it/s, est. speed input: 293.35 toks/s, output: 4693.62 toks/s]
Processed prompts:  79%|  | 407/512 [00:21<00:00, 162.87it/s, est. speed input: 309.03 toks/s, output: 4944.46 toks/s]
Processed prompts:  84%| | 429/512 [00:21<00:00, 167.20it/s, est. speed input: 323.87 toks/s, output: 5181.84 toks/s]
Processed prompts:  88%| | 450/512 [00:21<00:00, 165.38it/s, est. speed input: 337.63 toks/s, output: 5402.13 toks/s]
Processed prompts:  92%|| 470/512 [00:21<00:00, 152.18it/s, est. speed input: 350.00 toks/s, output: 5599.95 toks/s]
Processed prompts:  95%|| 488/512 [00:21<00:00, 124.03it/s, est. speed input: 359.64 toks/s, output: 5754.19 toks/s]
Processed prompts:  98%|| 503/512 [00:24<00:00, 19.00it/s, est. speed input: 325.45 toks/s, output: 5207.15 toks/s] 
Processed prompts: 100%|| 512/512 [00:24<00:00, 19.00it/s, est. speed input: 329.86 toks/s, output: 5277.80 toks/s]
Processed prompts: 100%|| 512/512 [00:24<00:00, 20.62it/s, est. speed input: 329.86 toks/s, output: 5277.80 toks/s]
[rank0]:[W128 09:51:04.039842412 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

