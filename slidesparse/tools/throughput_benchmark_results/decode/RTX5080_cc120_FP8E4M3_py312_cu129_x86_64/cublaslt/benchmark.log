
========== M=16 ==========
Time: 2026-01-25 18:10:15
Backend: cuBLASLt
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-FP8
Params: prompt_len=16, output_len=256, num_prompts=16, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-FP8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 16 --max-num-seqs 16 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cublaslt/json/Llama3.2-1B-FP8_M16.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 18:10:19 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=276415) WARNING 01-25 18:10:27 [backends.py:609] Failed to read file <frozen os>
Throughput: 17.65 requests/s, 4800.58 total tokens/s, 4518.19 output tokens/s
Total num prompt tokens:  256
Total num output tokens:  4096

STDERR:
[2026-01-25 18:10:19] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 18:10:19] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 18:10:19] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 18:10:19] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:10:19] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:10:19] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:10:19] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:10:19] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:10:19] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 18:10:19] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:10:19] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:10:19] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:10:19] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:10:19] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 18:10:23] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 18:10:23] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 18:10:23] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 18:10:23] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:10:23] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:10:23] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:10:23] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:10:23] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:10:23] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 18:10:23] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:10:23] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:10:23] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:10:23] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:10:23] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=276415) [2026-01-25 18:10:24] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=276415) [2026-01-25 18:10:24] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=276415) [2026-01-25 18:10:24] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=276415) [2026-01-25 18:10:24] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=276415) [2026-01-25 18:10:24] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=276415) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=276415) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.41it/s]
(EngineCore_DP0 pid=276415) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.40it/s]
(EngineCore_DP0 pid=276415) 
(EngineCore_DP0 pid=276415) 2026-01-25 18:10:31,272 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=276415) 2026-01-25 18:10:31,285 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=276415) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00, 35.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 34.24it/s]
(EngineCore_DP0 pid=276415) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 1/5 [00:00<00:00,  6.95it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 22.81it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 20.06it/s]

Adding requests:   0%|          | 0/16 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 16/16 [00:00<00:00, 1605.67it/s]

Processed prompts:   0%|          | 0/16 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   6%|▋         | 1/16 [00:00<00:13,  1.12it/s, est. speed input: 17.96 toks/s, output: 287.38 toks/s]
Processed prompts: 100%|██████████| 16/16 [00:00<00:00,  1.12it/s, est. speed input: 286.20 toks/s, output: 4579.25 toks/s]
Processed prompts: 100%|██████████| 16/16 [00:00<00:00, 17.89it/s, est. speed input: 286.20 toks/s, output: 4579.25 toks/s]
[rank0]:[W125 18:10:33.544887359 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-25 18:10:34
Backend: cuBLASLt
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-FP8
Params: prompt_len=16, output_len=256, num_prompts=128, max_num_seqs=128
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-FP8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 128 --max-num-seqs 128 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cublaslt/json/Llama3.2-1B-FP8_M128.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 18:10:38 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=276959) WARNING 01-25 18:10:46 [backends.py:609] Failed to read file <frozen os>
Throughput: 75.84 requests/s, 20627.64 total tokens/s, 19414.25 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768

STDERR:
[2026-01-25 18:10:38] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 18:10:38] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 18:10:38] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 18:10:38] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:10:38] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:10:38] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:10:38] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:10:38] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:10:38] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 18:10:38] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:10:38] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:10:38] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:10:38] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:10:38] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 18:10:42] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 18:10:42] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 18:10:42] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 18:10:42] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:10:42] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:10:42] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:10:42] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:10:42] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:10:42] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 18:10:42] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:10:42] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:10:42] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:10:42] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:10:42] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=276959) [2026-01-25 18:10:43] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=276959) [2026-01-25 18:10:43] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=276959) [2026-01-25 18:10:43] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=276959) [2026-01-25 18:10:43] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=276959) [2026-01-25 18:10:43] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=276959) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=276959) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.39it/s]
(EngineCore_DP0 pid=276959) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.39it/s]
(EngineCore_DP0 pid=276959) 
(EngineCore_DP0 pid=276959) 2026-01-25 18:10:48,954 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=276959) 2026-01-25 18:10:48,968 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=276959) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█▏        | 4/35 [00:00<00:00, 34.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  23%|██▎       | 8/35 [00:00<00:00, 32.44it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  34%|███▍      | 12/35 [00:00<00:00, 34.34it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  46%|████▌     | 16/35 [00:00<00:00, 35.16it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 20/35 [00:00<00:00, 35.15it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 24/35 [00:00<00:00, 35.48it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 28/35 [00:00<00:00, 35.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████▏| 32/35 [00:00<00:00, 36.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:00<00:00, 35.06it/s]
(EngineCore_DP0 pid=276959) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   5%|▌         | 1/19 [00:00<00:02,  6.90it/s]
Capturing CUDA graphs (decode, FULL):  26%|██▋       | 5/19 [00:00<00:00, 22.45it/s]
Capturing CUDA graphs (decode, FULL):  47%|████▋     | 9/19 [00:00<00:00, 26.70it/s]
Capturing CUDA graphs (decode, FULL):  68%|██████▊   | 13/19 [00:00<00:00, 30.58it/s]
Capturing CUDA graphs (decode, FULL):  89%|████████▉ | 17/19 [00:00<00:00, 33.02it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:00<00:00, 29.50it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 4523.19it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:01<03:23,  1.61s/it, est. speed input: 9.97 toks/s, output: 159.44 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00,  1.61s/it, est. speed input: 1235.47 toks/s, output: 19767.50 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 77.21it/s, est. speed input: 1235.47 toks/s, output: 19767.50 toks/s]
[rank0]:[W125 18:10:53.128936072 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-25 18:10:54
Backend: cuBLASLt
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-FP8
Params: prompt_len=16, output_len=256, num_prompts=256, max_num_seqs=256
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-FP8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 256 --max-num-seqs 256 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cublaslt/json/Llama3.2-1B-FP8_M256.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 18:10:58 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=277496) WARNING 01-25 18:11:05 [backends.py:609] Failed to read file <frozen os>
Throughput: 78.88 requests/s, 21456.18 total tokens/s, 20194.05 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536

STDERR:
[2026-01-25 18:10:58] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 18:10:58] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 18:10:58] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 18:10:58] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:10:58] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:10:58] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:10:58] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:10:58] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:10:58] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 18:10:58] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:10:58] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:10:58] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:10:58] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:10:58] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 18:11:02] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 18:11:02] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 18:11:02] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 18:11:02] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:11:02] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:11:02] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:11:02] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:11:02] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 18:11:02] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 18:11:02] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 18:11:02] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:11:02] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:11:02] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:11:02] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=277496) [2026-01-25 18:11:02] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=277496) [2026-01-25 18:11:02] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=277496) [2026-01-25 18:11:02] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=277496) [2026-01-25 18:11:02] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=277496) [2026-01-25 18:11:02] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=277496) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=277496) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.47it/s]
(EngineCore_DP0 pid=277496) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.46it/s]
(EngineCore_DP0 pid=277496) 
(EngineCore_DP0 pid=277496) 2026-01-25 18:11:08,229 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=277496) 2026-01-25 18:11:08,241 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=277496) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/36 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 4/36 [00:00<00:00, 35.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 8/36 [00:00<00:00, 37.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 12/36 [00:00<00:00, 38.14it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  44%|████▍     | 16/36 [00:00<00:00, 35.93it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  56%|█████▌    | 20/36 [00:00<00:00, 36.05it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 24/36 [00:00<00:00, 33.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 28/36 [00:00<00:00, 29.70it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 32/36 [00:01<00:00, 25.72it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:01<00:00, 27.48it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:01<00:00, 30.63it/s]
(EngineCore_DP0 pid=277496) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   3%|▎         | 1/35 [00:00<00:04,  7.01it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 5/35 [00:00<00:01, 23.25it/s]
Capturing CUDA graphs (decode, FULL):  26%|██▌       | 9/35 [00:00<00:00, 28.43it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 13/35 [00:00<00:00, 32.32it/s]
Capturing CUDA graphs (decode, FULL):  49%|████▊     | 17/35 [00:00<00:00, 34.67it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 21/35 [00:00<00:00, 35.95it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 25/35 [00:00<00:00, 36.92it/s]
Capturing CUDA graphs (decode, FULL):  83%|████████▎ | 29/35 [00:00<00:00, 37.58it/s]
Capturing CUDA graphs (decode, FULL):  94%|█████████▍| 33/35 [00:00<00:00, 38.31it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:01<00:00, 34.23it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   0%|          | 1/256 [00:00<00:37,  6.78it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 1462.57it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:02<11:53,  2.80s/it, est. speed input: 5.72 toks/s, output: 91.46 toks/s]
Processed prompts:  50%|████▉     | 127/256 [00:02<00:02, 61.47it/s, est. speed input: 700.44 toks/s, output: 11207.02 toks/s]
Processed prompts:  91%|█████████ | 232/256 [00:03<00:00, 125.37it/s, est. speed input: 1236.09 toks/s, output: 19777.39 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 125.37it/s, est. speed input: 1334.82 toks/s, output: 21357.03 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 83.42it/s, est. speed input: 1334.82 toks/s, output: 21357.03 toks/s] 
[rank0]:[W125 18:11:14.517866259 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=64 ==========
Time: 2026-01-25 22:28:29
Backend: cuBLASLt
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-FP8
Params: prompt_len=16, output_len=256, num_prompts=64, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-FP8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 64 --max-num-seqs 64 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cublaslt/json/Llama3.2-1B-FP8_M64.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 22:28:33 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=592086) WARNING 01-25 22:28:41 [backends.py:609] Failed to read file <frozen os>
Throughput: 53.60 requests/s, 14580.52 total tokens/s, 13722.85 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384

STDERR:
[2026-01-25 22:28:33] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:28:33] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 22:28:33] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 22:28:33] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 22:28:33] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 22:28:33] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 22:28:33] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 22:28:33] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 22:28:33] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 22:28:33] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:28:33] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:28:33] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:28:33] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:28:33] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 22:28:37] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:28:37] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 22:28:37] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 22:28:37] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 22:28:37] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 22:28:37] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 22:28:37] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 22:28:37] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 22:28:37] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 22:28:37] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:28:37] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:28:37] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:28:37] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:28:37] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=592086) [2026-01-25 22:28:38] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=592086) [2026-01-25 22:28:38] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=592086) [2026-01-25 22:28:38] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=592086) [2026-01-25 22:28:38] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=592086) [2026-01-25 22:28:38] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=592086) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=592086) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.96it/s]
(EngineCore_DP0 pid=592086) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.96it/s]
(EngineCore_DP0 pid=592086) 
(EngineCore_DP0 pid=592086) 2026-01-25 22:28:44,114 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=592086) 2026-01-25 22:28:44,129 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=592086) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 3/19 [00:00<00:00, 29.67it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 7/19 [00:00<00:00, 34.60it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 11/19 [00:00<00:00, 36.01it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  79%|███████▉  | 15/19 [00:00<00:00, 36.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:00<00:00, 31.81it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:00<00:00, 33.14it/s]
(EngineCore_DP0 pid=592086) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   9%|▉         | 1/11 [00:00<00:01,  6.76it/s]
Capturing CUDA graphs (decode, FULL):  36%|███▋      | 4/11 [00:00<00:00, 17.13it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 6/11 [00:00<00:00, 16.69it/s]
Capturing CUDA graphs (decode, FULL):  82%|████████▏ | 9/11 [00:00<00:00, 20.81it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:00<00:00, 18.91it/s]

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 64/64 [00:00<00:00, 2949.52it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 1/64 [00:01<01:12,  1.15s/it, est. speed input: 13.91 toks/s, output: 222.48 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:01<00:00,  1.15s/it, est. speed input: 874.85 toks/s, output: 13997.54 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:01<00:00, 54.68it/s, est. speed input: 874.85 toks/s, output: 13997.54 toks/s]
[rank0]:[W125 22:28:47.310635908 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-25 22:28:48
Backend: cuBLASLt
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-FP8
Params: prompt_len=16, output_len=256, num_prompts=128, max_num_seqs=128
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-FP8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 128 --max-num-seqs 128 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cublaslt/json/Llama3.2-1B-FP8_M128.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 22:28:52 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=592619) WARNING 01-25 22:28:59 [backends.py:609] Failed to read file <frozen os>
Throughput: 76.58 requests/s, 20829.48 total tokens/s, 19604.22 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768

STDERR:
[2026-01-25 22:28:52] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:28:52] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 22:28:52] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 22:28:52] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 22:28:52] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 22:28:52] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 22:28:52] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 22:28:52] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 22:28:52] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 22:28:52] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:28:52] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:28:52] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:28:52] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:28:52] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 22:28:56] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:28:56] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 22:28:56] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 22:28:56] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 22:28:56] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 22:28:56] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 22:28:56] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 22:28:56] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 22:28:56] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 22:28:56] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:28:56] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:28:56] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:28:56] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:28:56] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=592619) [2026-01-25 22:28:57] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=592619) [2026-01-25 22:28:57] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=592619) [2026-01-25 22:28:57] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=592619) [2026-01-25 22:28:57] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=592619) [2026-01-25 22:28:57] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=592619) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=592619) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  7.13it/s]
(EngineCore_DP0 pid=592619) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  7.13it/s]
(EngineCore_DP0 pid=592619) 
(EngineCore_DP0 pid=592619) 2026-01-25 22:29:01,968 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=592619) 2026-01-25 22:29:01,982 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=592619) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█▏        | 4/35 [00:00<00:00, 33.00it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  23%|██▎       | 8/35 [00:00<00:00, 34.70it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  34%|███▍      | 12/35 [00:00<00:00, 34.96it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  46%|████▌     | 16/35 [00:00<00:00, 35.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 20/35 [00:00<00:00, 35.04it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 24/35 [00:00<00:00, 35.32it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 28/35 [00:00<00:00, 35.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████▏| 32/35 [00:00<00:00, 33.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:01<00:00, 32.87it/s]
(EngineCore_DP0 pid=592619) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   5%|▌         | 1/19 [00:00<00:03,  5.71it/s]
Capturing CUDA graphs (decode, FULL):  21%|██        | 4/19 [00:00<00:01, 14.91it/s]
Capturing CUDA graphs (decode, FULL):  42%|████▏     | 8/19 [00:00<00:00, 23.03it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 12/19 [00:00<00:00, 27.40it/s]
Capturing CUDA graphs (decode, FULL):  84%|████████▍ | 16/19 [00:00<00:00, 30.15it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:00<00:00, 26.29it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 5070.90it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:01<03:22,  1.59s/it, est. speed input: 10.05 toks/s, output: 160.74 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00,  1.59s/it, est. speed input: 1245.28 toks/s, output: 19924.42 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 77.83it/s, est. speed input: 1245.28 toks/s, output: 19924.42 toks/s]
[rank0]:[W125 22:29:06.277903853 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-25 22:29:07
Backend: cuBLASLt
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-FP8
Params: prompt_len=16, output_len=256, num_prompts=256, max_num_seqs=256
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-FP8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 256 --max-num-seqs 256 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cublaslt/json/Llama3.2-1B-FP8_M256.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 22:29:11 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=593151) WARNING 01-25 22:29:18 [backends.py:609] Failed to read file <frozen os>
Throughput: 78.54 requests/s, 21361.55 total tokens/s, 20104.99 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536

STDERR:
[2026-01-25 22:29:11] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:29:11] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 22:29:11] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 22:29:11] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 22:29:11] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 22:29:11] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 22:29:11] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 22:29:11] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 22:29:11] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 22:29:11] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:29:11] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:29:11] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:29:11] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:29:11] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 22:29:15] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:29:15] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 22:29:15] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 22:29:15] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 22:29:15] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 22:29:15] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 22:29:15] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 22:29:15] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 22:29:15] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 22:29:15] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:29:15] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:29:15] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:29:15] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:29:15] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=593151) [2026-01-25 22:29:16] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=593151) [2026-01-25 22:29:16] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=593151) [2026-01-25 22:29:16] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=593151) [2026-01-25 22:29:16] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=593151) [2026-01-25 22:29:16] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=593151) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=593151) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.99it/s]
(EngineCore_DP0 pid=593151) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.98it/s]
(EngineCore_DP0 pid=593151) 
(EngineCore_DP0 pid=593151) 2026-01-25 22:29:21,083 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=593151) 2026-01-25 22:29:21,118 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=593151) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/36 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|▎         | 1/36 [00:00<00:03,  8.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 3/36 [00:00<00:02, 13.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  19%|█▉        | 7/36 [00:00<00:01, 21.01it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███       | 11/36 [00:00<00:00, 26.18it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 15/36 [00:00<00:00, 28.99it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 19/36 [00:00<00:00, 30.99it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▍   | 23/36 [00:00<00:00, 32.48it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 27/36 [00:00<00:00, 33.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 31/36 [00:01<00:00, 34.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  97%|█████████▋| 35/36 [00:01<00:00, 35.00it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:01<00:00, 29.82it/s]
(EngineCore_DP0 pid=593151) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   3%|▎         | 1/35 [00:00<00:04,  6.97it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 5/35 [00:00<00:01, 20.73it/s]
Capturing CUDA graphs (decode, FULL):  26%|██▌       | 9/35 [00:00<00:00, 27.11it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 13/35 [00:00<00:00, 30.46it/s]
Capturing CUDA graphs (decode, FULL):  49%|████▊     | 17/35 [00:00<00:00, 32.34it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 21/35 [00:00<00:00, 33.30it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 25/35 [00:00<00:00, 34.14it/s]
Capturing CUDA graphs (decode, FULL):  83%|████████▎ | 29/35 [00:00<00:00, 34.67it/s]
Capturing CUDA graphs (decode, FULL):  94%|█████████▍| 33/35 [00:01<00:00, 35.26it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:01<00:00, 31.74it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   0%|          | 1/256 [00:00<00:39,  6.41it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 1393.46it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:02<11:55,  2.81s/it, est. speed input: 5.70 toks/s, output: 91.18 toks/s]
Processed prompts:  50%|████▉     | 127/256 [00:02<00:02, 61.28it/s, est. speed input: 698.29 toks/s, output: 11172.62 toks/s]
Processed prompts:  91%|█████████ | 232/256 [00:03<00:00, 125.01it/s, est. speed input: 1232.39 toks/s, output: 19718.22 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 125.01it/s, est. speed input: 1332.01 toks/s, output: 21312.04 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 83.25it/s, est. speed input: 1332.01 toks/s, output: 21312.04 toks/s] 
[rank0]:[W125 22:29:27.532531678 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-25 22:29:28
Backend: cuBLASLt
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-FP8
Params: prompt_len=16, output_len=256, num_prompts=512, max_num_seqs=512
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-FP8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 512 --max-num-seqs 512 --max-model-len 272 --max-num-batched-tokens 512 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cublaslt/json/Llama3.2-1B-FP8_M512.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 22:29:32 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=593713) WARNING 01-25 22:29:40 [backends.py:609] Failed to read file <frozen os>
Throughput: 78.83 requests/s, 21442.78 total tokens/s, 20181.44 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072

STDERR:
[2026-01-25 22:29:32] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:29:32] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 22:29:32] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 22:29:32] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 22:29:32] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 22:29:32] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 22:29:32] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 22:29:32] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 22:29:32] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 22:29:32] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:29:32] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:29:32] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:29:32] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:29:32] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 22:29:36] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:29:36] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 22:29:36] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 22:29:36] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 22:29:36] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 22:29:36] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 22:29:36] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 22:29:36] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 22:29:36] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 22:29:36] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:29:36] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:29:36] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:29:36] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:29:36] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=593713) [2026-01-25 22:29:37] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=593713) [2026-01-25 22:29:37] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=593713) [2026-01-25 22:29:37] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=593713) [2026-01-25 22:29:37] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=593713) [2026-01-25 22:29:37] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=593713) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=593713) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  7.16it/s]
(EngineCore_DP0 pid=593713) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  7.16it/s]
(EngineCore_DP0 pid=593713) 
(EngineCore_DP0 pid=593713) 2026-01-25 22:29:44,078 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=593713) 2026-01-25 22:29:44,091 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=593713) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 4/51 [00:00<00:01, 28.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 8/51 [00:00<00:01, 33.34it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|██▎       | 12/51 [00:00<00:01, 35.45it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 16/51 [00:00<00:00, 36.48it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|███▉      | 20/51 [00:00<00:00, 35.73it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 24/51 [00:00<00:00, 35.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 28/51 [00:00<00:00, 31.24it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 32/51 [00:01<00:00, 23.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 35/51 [00:01<00:00, 22.34it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|███████▋  | 39/51 [00:01<00:00, 25.79it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 43/51 [00:01<00:00, 28.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 47/51 [00:01<00:00, 31.00it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:01<00:00, 31.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:01<00:00, 30.04it/s]
(EngineCore_DP0 pid=593713) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   2%|▏         | 1/51 [00:00<00:07,  6.86it/s]
Capturing CUDA graphs (decode, FULL):  10%|▉         | 5/51 [00:00<00:02, 22.69it/s]
Capturing CUDA graphs (decode, FULL):  18%|█▊        | 9/51 [00:00<00:01, 29.12it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 13/51 [00:00<00:01, 32.46it/s]
Capturing CUDA graphs (decode, FULL):  33%|███▎      | 17/51 [00:00<00:00, 34.29it/s]
Capturing CUDA graphs (decode, FULL):  41%|████      | 21/51 [00:00<00:00, 35.15it/s]
Capturing CUDA graphs (decode, FULL):  49%|████▉     | 25/51 [00:00<00:00, 35.98it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 29/51 [00:00<00:00, 36.61it/s]
Capturing CUDA graphs (decode, FULL):  65%|██████▍   | 33/51 [00:00<00:00, 36.90it/s]
Capturing CUDA graphs (decode, FULL):  73%|███████▎  | 37/51 [00:01<00:00, 37.40it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 41/51 [00:01<00:00, 37.68it/s]
Capturing CUDA graphs (decode, FULL):  88%|████████▊ | 45/51 [00:01<00:00, 37.61it/s]
Capturing CUDA graphs (decode, FULL):  96%|█████████▌| 49/51 [00:01<00:00, 38.01it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:01<00:00, 34.87it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 8487.24it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:05<44:38,  5.24s/it, est. speed input: 3.05 toks/s, output: 48.85 toks/s]
Processed prompts:  12%|█▏        | 63/512 [00:05<00:27, 16.59it/s, est. speed input: 188.36 toks/s, output: 3013.80 toks/s]
Processed prompts:  36%|███▌      | 185/512 [00:05<00:05, 60.14it/s, est. speed input: 541.70 toks/s, output: 8667.25 toks/s]
Processed prompts:  64%|██████▍   | 327/512 [00:05<00:01, 127.03it/s, est. speed input: 940.08 toks/s, output: 15041.19 toks/s]
Processed prompts:  86%|████████▋ | 442/512 [00:05<00:00, 193.77it/s, est. speed input: 1247.47 toks/s, output: 19959.46 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:06<00:00, 193.77it/s, est. speed input: 1273.51 toks/s, output: 20376.20 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:06<00:00, 79.59it/s, est. speed input: 1273.51 toks/s, output: 20376.20 toks/s] 
[rank0]:[W125 22:29:54.657914159 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=64 ==========
Time: 2026-01-25 22:45:21
Backend: cuBLASLt
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-3B-FP8
Params: prompt_len=16, output_len=256, num_prompts=64, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-3B-FP8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 64 --max-num-seqs 64 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cublaslt/json/Llama3.2-3B-FP8_M64.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 22:45:25 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=617251) WARNING 01-25 22:45:36 [backends.py:609] Failed to read file <frozen os>
Throughput: 28.70 requests/s, 7807.41 total tokens/s, 7348.15 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384

STDERR:
[2026-01-25 22:45:25] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:45:25] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-25 22:45:25] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-25 22:45:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:45:25] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:45:25] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:45:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:45:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:45:25] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-25 22:45:25] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:45:25] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:45:25] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:45:25] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:45:25] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 22:45:29] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:45:29] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-25 22:45:29] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-25 22:45:29] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:45:29] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:45:29] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:45:29] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:45:29] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:45:29] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-25 22:45:29] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:45:29] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:45:29] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:45:29] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:45:29] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=617251) [2026-01-25 22:45:30] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=617251) [2026-01-25 22:45:30] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=617251) [2026-01-25 22:45:30] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=617251) [2026-01-25 22:45:30] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=617251) [2026-01-25 22:45:30] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=617251) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=617251) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:02<00:00,  2.10s/it]
(EngineCore_DP0 pid=617251) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:02<00:00,  2.10s/it]
(EngineCore_DP0 pid=617251) 
(EngineCore_DP0 pid=617251) 2026-01-25 22:45:42,256 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=617251) 2026-01-25 22:45:42,274 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=617251) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 3/19 [00:00<00:00, 24.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  32%|███▏      | 6/19 [00:00<00:00, 25.34it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 9/19 [00:00<00:00, 26.92it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 12/19 [00:00<00:00, 26.54it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 16/19 [00:00<00:00, 28.00it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:00<00:00, 26.73it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:00<00:00, 26.67it/s]
(EngineCore_DP0 pid=617251) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   9%|▉         | 1/11 [00:00<00:01,  7.05it/s]
Capturing CUDA graphs (decode, FULL):  45%|████▌     | 5/11 [00:00<00:00, 20.51it/s]
Capturing CUDA graphs (decode, FULL):  82%|████████▏ | 9/11 [00:00<00:00, 25.14it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:00<00:00, 23.61it/s]

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 64/64 [00:00<00:00, 3000.32it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 1/64 [00:02<02:16,  2.17s/it, est. speed input: 7.39 toks/s, output: 118.16 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:02<00:00,  2.17s/it, est. speed input: 464.07 toks/s, output: 7425.11 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:02<00:00, 29.00it/s, est. speed input: 464.07 toks/s, output: 7425.11 toks/s]
[rank0]:[W125 22:45:46.718544508 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-25 22:45:48
Backend: cuBLASLt
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-3B-FP8
Params: prompt_len=16, output_len=256, num_prompts=128, max_num_seqs=128
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-3B-FP8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 128 --max-num-seqs 128 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cublaslt/json/Llama3.2-3B-FP8_M128.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 22:45:51 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=617886) WARNING 01-25 22:46:00 [backends.py:609] Failed to read file <frozen os>
Throughput: 42.02 requests/s, 11430.66 total tokens/s, 10758.27 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768

STDERR:
[2026-01-25 22:45:51] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:45:51] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-25 22:45:51] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-25 22:45:51] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:45:51] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:45:51] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:45:51] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:45:51] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:45:51] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-25 22:45:51] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:45:51] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:45:51] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:45:51] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:45:51] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 22:45:55] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:45:56] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-25 22:45:56] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-25 22:45:56] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:45:56] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:45:56] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:45:56] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:45:56] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:45:56] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-25 22:45:56] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:45:56] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:45:56] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:45:56] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:45:56] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=617886) [2026-01-25 22:45:56] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=617886) [2026-01-25 22:45:56] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=617886) [2026-01-25 22:45:56] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=617886) [2026-01-25 22:45:56] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=617886) [2026-01-25 22:45:56] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=617886) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=617886) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.96it/s]
(EngineCore_DP0 pid=617886) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.96it/s]
(EngineCore_DP0 pid=617886) 
(EngineCore_DP0 pid=617886) 2026-01-25 22:46:04,723 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=617886) 2026-01-25 22:46:04,738 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=617886) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▊         | 3/35 [00:00<00:01, 26.73it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 6/35 [00:00<00:01, 27.97it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▌       | 9/35 [00:00<00:00, 28.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  34%|███▍      | 12/35 [00:00<00:00, 29.01it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 15/35 [00:00<00:00, 29.13it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████▏    | 18/35 [00:00<00:00, 28.99it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 21/35 [00:00<00:00, 26.15it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 24/35 [00:00<00:00, 26.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  77%|███████▋  | 27/35 [00:00<00:00, 27.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 30/35 [00:01<00:00, 28.13it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 33/35 [00:01<00:00, 26.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:01<00:00, 26.59it/s]
(EngineCore_DP0 pid=617886) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   5%|▌         | 1/19 [00:00<00:03,  5.16it/s]
Capturing CUDA graphs (decode, FULL):  16%|█▌        | 3/19 [00:00<00:01, 11.35it/s]
Capturing CUDA graphs (decode, FULL):  32%|███▏      | 6/19 [00:00<00:00, 18.28it/s]
Capturing CUDA graphs (decode, FULL):  47%|████▋     | 9/19 [00:00<00:00, 20.99it/s]
Capturing CUDA graphs (decode, FULL):  68%|██████▊   | 13/19 [00:00<00:00, 24.38it/s]
Capturing CUDA graphs (decode, FULL):  84%|████████▍ | 16/19 [00:00<00:00, 26.01it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:00<00:00, 27.13it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:00<00:00, 22.33it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 9033.52it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:02<06:11,  2.93s/it, est. speed input: 5.47 toks/s, output: 87.48 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:03<00:00, 58.85it/s, est. speed input: 670.43 toks/s, output: 10726.92 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 58.85it/s, est. speed input: 675.70 toks/s, output: 10811.12 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 42.23it/s, est. speed input: 675.70 toks/s, output: 10811.12 toks/s]
[rank0]:[W125 22:46:11.836725647 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-25 22:46:12
Backend: cuBLASLt
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-3B-FP8
Params: prompt_len=16, output_len=256, num_prompts=256, max_num_seqs=256
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-3B-FP8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 256 --max-num-seqs 256 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cublaslt/json/Llama3.2-3B-FP8_M256.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 22:46:16 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=618531) WARNING 01-25 22:46:24 [backends.py:609] Failed to read file <frozen os>
Throughput: 43.17 requests/s, 11741.64 total tokens/s, 11050.96 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536

STDERR:
[2026-01-25 22:46:16] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:46:16] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-25 22:46:16] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-25 22:46:16] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:46:16] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:46:16] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:46:16] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:46:16] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:46:16] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-25 22:46:16] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:46:16] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:46:16] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:46:16] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:46:16] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 22:46:19] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:46:19] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-25 22:46:19] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-25 22:46:19] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:46:19] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:46:19] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:46:19] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:46:19] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:46:19] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-25 22:46:19] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:46:19] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:46:19] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:46:19] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:46:19] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=618531) [2026-01-25 22:46:20] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=618531) [2026-01-25 22:46:20] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=618531) [2026-01-25 22:46:20] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=618531) [2026-01-25 22:46:20] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=618531) [2026-01-25 22:46:20] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=618531) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=618531) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.93it/s]
(EngineCore_DP0 pid=618531) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.93it/s]
(EngineCore_DP0 pid=618531) 
(EngineCore_DP0 pid=618531) 2026-01-25 22:46:28,685 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=618531) 2026-01-25 22:46:28,700 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=618531) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/36 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 3/36 [00:00<00:01, 28.02it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 6/36 [00:00<00:01, 28.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  28%|██▊       | 10/36 [00:00<00:00, 29.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▌      | 13/36 [00:00<00:00, 27.41it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  44%|████▍     | 16/36 [00:00<00:00, 28.10it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 19/36 [00:00<00:00, 28.44it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|██████    | 22/36 [00:00<00:00, 28.71it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▉   | 25/36 [00:00<00:00, 28.92it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 28/36 [00:00<00:00, 29.07it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 31/36 [00:01<00:00, 29.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 34/36 [00:01<00:00, 27.06it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:01<00:00, 27.44it/s]
(EngineCore_DP0 pid=618531) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   3%|▎         | 1/35 [00:00<00:05,  5.68it/s]
Capturing CUDA graphs (decode, FULL):   9%|▊         | 3/35 [00:00<00:02, 12.00it/s]
Capturing CUDA graphs (decode, FULL):  17%|█▋        | 6/35 [00:00<00:01, 18.92it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 10/35 [00:00<00:01, 23.80it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 14/35 [00:00<00:00, 26.20it/s]
Capturing CUDA graphs (decode, FULL):  49%|████▊     | 17/35 [00:00<00:00, 27.26it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 21/35 [00:00<00:00, 28.29it/s]
Capturing CUDA graphs (decode, FULL):  69%|██████▊   | 24/35 [00:00<00:00, 28.74it/s]
Capturing CUDA graphs (decode, FULL):  77%|███████▋  | 27/35 [00:01<00:00, 28.97it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 30/35 [00:01<00:00, 29.23it/s]
Capturing CUDA graphs (decode, FULL):  94%|█████████▍| 33/35 [00:01<00:00, 29.33it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:01<00:00, 25.83it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   0%|          | 1/256 [00:00<00:34,  7.37it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 1564.55it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:05<22:12,  5.22s/it, est. speed input: 3.06 toks/s, output: 49.00 toks/s]
Processed prompts:  24%|██▍       | 62/256 [00:05<00:11, 16.39it/s, est. speed input: 186.04 toks/s, output: 2976.59 toks/s]
Processed prompts:  50%|████▉     | 127/256 [00:05<00:03, 39.54it/s, est. speed input: 374.00 toks/s, output: 5984.00 toks/s]
Processed prompts:  73%|███████▎  | 188/256 [00:05<00:01, 67.83it/s, est. speed input: 543.59 toks/s, output: 8697.37 toks/s]
Processed prompts:  94%|█████████▍| 241/256 [00:05<00:00, 96.23it/s, est. speed input: 680.33 toks/s, output: 10885.27 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:05<00:00, 96.23it/s, est. speed input: 710.37 toks/s, output: 11365.97 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:05<00:00, 44.40it/s, est. speed input: 710.37 toks/s, output: 11365.97 toks/s]
[rank0]:[W125 22:46:38.266090818 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-25 22:46:39
Backend: cuBLASLt
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-3B-FP8
Params: prompt_len=16, output_len=256, num_prompts=512, max_num_seqs=512
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-3B-FP8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 512 --max-num-seqs 512 --max-model-len 272 --max-num-batched-tokens 512 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cublaslt/json/Llama3.2-3B-FP8_M512.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 22:46:43 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=619176) WARNING 01-25 22:46:52 [backends.py:609] Failed to read file <frozen os>
Throughput: 36.74 requests/s, 9992.34 total tokens/s, 9404.55 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072

STDERR:
[2026-01-25 22:46:43] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:46:43] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-25 22:46:43] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-25 22:46:43] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:46:43] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:46:43] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:46:43] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:46:43] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:46:43] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-25 22:46:43] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:46:43] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:46:43] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:46:43] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:46:43] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 22:46:47] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:46:47] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-25 22:46:47] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-25 22:46:47] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:46:47] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:46:47] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:46:47] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:46:47] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-25 22:46:47] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-25 22:46:47] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:46:47] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:46:47] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:46:47] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:46:47] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=619176) [2026-01-25 22:46:48] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=619176) [2026-01-25 22:46:48] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=619176) [2026-01-25 22:46:48] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=619176) [2026-01-25 22:46:48] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=619176) [2026-01-25 22:46:48] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=619176) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=619176) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.99it/s]
(EngineCore_DP0 pid=619176) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.98it/s]
(EngineCore_DP0 pid=619176) 
(EngineCore_DP0 pid=619176) 2026-01-25 22:46:58,174 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=619176) 2026-01-25 22:46:58,190 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=619176) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 3/51 [00:00<00:01, 26.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|█▏        | 6/51 [00:00<00:01, 25.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 9/51 [00:00<00:01, 27.05it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|██▎       | 12/51 [00:00<00:01, 27.90it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 16/51 [00:00<00:01, 28.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 19/51 [00:00<00:01, 28.41it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 23/51 [00:00<00:00, 29.06it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 27/51 [00:00<00:00, 29.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  59%|█████▉    | 30/51 [00:01<00:00, 29.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 34/51 [00:01<00:00, 29.63it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 37/51 [00:01<00:00, 27.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 40/51 [00:01<00:00, 28.12it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▋ | 44/51 [00:01<00:00, 28.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 47/51 [00:01<00:00, 26.60it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  98%|█████████▊| 50/51 [00:01<00:00, 24.90it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:01<00:00, 27.27it/s]
(EngineCore_DP0 pid=619176) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   2%|▏         | 1/51 [00:00<00:09,  5.31it/s]
Capturing CUDA graphs (decode, FULL):   8%|▊         | 4/51 [00:00<00:03, 14.44it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▎        | 7/51 [00:00<00:02, 18.58it/s]
Capturing CUDA graphs (decode, FULL):  20%|█▉        | 10/51 [00:00<00:01, 20.99it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 13/51 [00:00<00:01, 23.27it/s]
Capturing CUDA graphs (decode, FULL):  31%|███▏      | 16/51 [00:00<00:01, 25.21it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 19/51 [00:00<00:01, 26.53it/s]
Capturing CUDA graphs (decode, FULL):  45%|████▌     | 23/51 [00:00<00:01, 27.86it/s]
Capturing CUDA graphs (decode, FULL):  53%|█████▎    | 27/51 [00:01<00:00, 29.02it/s]
Capturing CUDA graphs (decode, FULL):  61%|██████    | 31/51 [00:01<00:00, 29.60it/s]
Capturing CUDA graphs (decode, FULL):  69%|██████▊   | 35/51 [00:01<00:00, 30.06it/s]
Capturing CUDA graphs (decode, FULL):  76%|███████▋  | 39/51 [00:01<00:00, 29.57it/s]
Capturing CUDA graphs (decode, FULL):  82%|████████▏ | 42/51 [00:01<00:00, 29.36it/s]
Capturing CUDA graphs (decode, FULL):  90%|█████████ | 46/51 [00:01<00:00, 29.85it/s]
Capturing CUDA graphs (decode, FULL):  98%|█████████▊| 50/51 [00:01<00:00, 30.31it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:01<00:00, 26.64it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 7909.47it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:07<1:07:11,  7.89s/it, est. speed input: 2.03 toks/s, output: 32.45 toks/s]
Processed prompts:  12%|█▏        | 63/512 [00:08<00:40, 11.12it/s, est. speed input: 125.94 toks/s, output: 2015.08 toks/s]
Processed prompts:  32%|███▏      | 164/512 [00:08<00:09, 35.77it/s, est. speed input: 323.63 toks/s, output: 5178.11 toks/s]
Processed prompts:  44%|████▍     | 226/512 [00:08<00:05, 52.97it/s, est. speed input: 432.65 toks/s, output: 6922.32 toks/s]
Processed prompts:  53%|█████▎    | 273/512 [00:10<00:05, 42.06it/s, est. speed input: 435.26 toks/s, output: 6964.10 toks/s]
Processed prompts:  60%|█████▉    | 305/512 [00:11<00:05, 37.90it/s, est. speed input: 436.70 toks/s, output: 6987.19 toks/s]
Processed prompts:  64%|██████▍   | 327/512 [00:11<00:04, 39.50it/s, est. speed input: 450.14 toks/s, output: 7202.30 toks/s]
Processed prompts:  67%|██████▋   | 344/512 [00:11<00:03, 42.24it/s, est. speed input: 463.16 toks/s, output: 7410.51 toks/s]
Processed prompts:  70%|██████▉   | 358/512 [00:12<00:03, 43.71it/s, est. speed input: 471.58 toks/s, output: 7545.21 toks/s]
Processed prompts:  72%|███████▏  | 369/512 [00:12<00:03, 45.78it/s, est. speed input: 479.06 toks/s, output: 7664.94 toks/s]
Processed prompts:  74%|███████▍  | 379/512 [00:12<00:02, 49.65it/s, est. speed input: 487.45 toks/s, output: 7799.14 toks/s]
Processed prompts:  76%|███████▌  | 389/512 [00:12<00:02, 53.37it/s, est. speed input: 495.20 toks/s, output: 7923.18 toks/s]
Processed prompts:  78%|███████▊  | 398/512 [00:12<00:02, 56.87it/s, est. speed input: 502.01 toks/s, output: 8032.22 toks/s]
Processed prompts:  80%|███████▉  | 408/512 [00:12<00:01, 63.28it/s, est. speed input: 510.55 toks/s, output: 8168.73 toks/s]
Processed prompts:  83%|████████▎ | 423/512 [00:12<00:01, 76.34it/s, est. speed input: 524.50 toks/s, output: 8392.01 toks/s]
Processed prompts:  86%|████████▌ | 438/512 [00:13<00:00, 86.02it/s, est. speed input: 537.74 toks/s, output: 8603.78 toks/s]
Processed prompts:  88%|████████▊ | 450/512 [00:13<00:00, 93.02it/s, est. speed input: 548.23 toks/s, output: 8771.60 toks/s]
Processed prompts:  91%|█████████ | 465/512 [00:13<00:00, 103.99it/s, est. speed input: 561.83 toks/s, output: 8989.28 toks/s]
Processed prompts:  93%|█████████▎| 478/512 [00:13<00:00, 109.91it/s, est. speed input: 573.14 toks/s, output: 9170.21 toks/s]
Processed prompts:  96%|█████████▌| 491/512 [00:13<00:00, 110.35it/s, est. speed input: 583.62 toks/s, output: 9337.96 toks/s]
Processed prompts:  98%|█████████▊| 503/512 [00:13<00:00, 62.41it/s, est. speed input: 580.23 toks/s, output: 9283.64 toks/s] 
Processed prompts: 100%|██████████| 512/512 [00:13<00:00, 62.41it/s, est. speed input: 590.60 toks/s, output: 9449.67 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:13<00:00, 36.91it/s, est. speed input: 590.60 toks/s, output: 9449.67 toks/s]
[rank0]:[W125 22:47:17.884698418 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=64 ==========
Time: 2026-01-25 23:05:22
Backend: cuBLASLt
Checkpoint: /root/vllmbench/checkpoints/Qwen2.5-7B-FP8
Params: prompt_len=16, output_len=256, num_prompts=64, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Qwen2.5-7B-FP8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 64 --max-num-seqs 64 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cublaslt/json/Qwen2.5-7B-FP8_M64.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 23:05:26 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=645103) WARNING 01-25 23:05:37 [backends.py:609] Failed to read file <frozen os>
Throughput: 18.70 requests/s, 5087.66 total tokens/s, 4788.39 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384

STDERR:
[2026-01-25 23:05:26] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 23:05:26] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-25 23:05:26] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-25 23:05:26] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-25 23:05:26] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-25 23:05:26] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-25 23:05:26] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-25 23:05:26] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-25 23:05:26] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-25 23:05:26] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 23:05:26] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 23:05:26] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 23:05:26] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 23:05:26] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 23:05:30] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 23:05:30] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-25 23:05:30] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-25 23:05:30] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-25 23:05:30] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-25 23:05:30] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-25 23:05:30] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-25 23:05:30] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-25 23:05:30] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-25 23:05:30] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 23:05:30] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 23:05:30] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 23:05:30] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 23:05:30] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=645103) [2026-01-25 23:05:30] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=645103) [2026-01-25 23:05:31] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=645103) [2026-01-25 23:05:31] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=645103) [2026-01-25 23:05:31] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=645103) [2026-01-25 23:05:31] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=645103) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=645103) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.11it/s]
(EngineCore_DP0 pid=645103) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.17s/it]
(EngineCore_DP0 pid=645103) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.13s/it]
(EngineCore_DP0 pid=645103) 
(EngineCore_DP0 pid=645103) 2026-01-25 23:05:42,818 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=645103) 2026-01-25 23:05:42,833 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=645103) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 3/19 [00:00<00:00, 26.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  32%|███▏      | 6/19 [00:00<00:00, 28.12it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 10/19 [00:00<00:00, 29.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  68%|██████▊   | 13/19 [00:00<00:00, 29.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 16/19 [00:00<00:00, 29.59it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:00<00:00, 26.68it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:00<00:00, 27.79it/s]
(EngineCore_DP0 pid=645103) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   9%|▉         | 1/11 [00:00<00:01,  7.25it/s]
Capturing CUDA graphs (decode, FULL):  45%|████▌     | 5/11 [00:00<00:00, 20.63it/s]
Capturing CUDA graphs (decode, FULL):  82%|████████▏ | 9/11 [00:00<00:00, 25.11it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:00<00:00, 23.63it/s]

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 64/64 [00:00<00:00, 4337.30it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 1/64 [00:03<03:30,  3.34s/it, est. speed input: 4.78 toks/s, output: 76.55 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:03<00:00,  3.34s/it, est. speed input: 300.72 toks/s, output: 4811.53 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:03<00:00, 18.79it/s, est. speed input: 300.72 toks/s, output: 4811.53 toks/s]
[rank0]:[W125 23:05:48.340777858 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-25 23:05:49
Backend: cuBLASLt
Checkpoint: /root/vllmbench/checkpoints/Qwen2.5-7B-FP8
Params: prompt_len=16, output_len=256, num_prompts=128, max_num_seqs=128
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Qwen2.5-7B-FP8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 128 --max-num-seqs 128 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cublaslt/json/Qwen2.5-7B-FP8_M128.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 23:05:53 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=645748) WARNING 01-25 23:06:03 [backends.py:609] Failed to read file <frozen os>
Throughput: 29.20 requests/s, 7943.60 total tokens/s, 7476.33 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768

STDERR:
[2026-01-25 23:05:53] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 23:05:53] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-25 23:05:53] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-25 23:05:53] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-25 23:05:53] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-25 23:05:53] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-25 23:05:53] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-25 23:05:53] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-25 23:05:53] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-25 23:05:53] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 23:05:53] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 23:05:53] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 23:05:53] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 23:05:53] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 23:05:57] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 23:05:57] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-25 23:05:57] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-25 23:05:57] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-25 23:05:57] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-25 23:05:57] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-25 23:05:57] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-25 23:05:57] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-25 23:05:57] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-25 23:05:57] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 23:05:57] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 23:05:57] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 23:05:57] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 23:05:57] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=645748) [2026-01-25 23:05:58] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=645748) [2026-01-25 23:05:58] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=645748) [2026-01-25 23:05:58] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=645748) [2026-01-25 23:05:58] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=645748) [2026-01-25 23:05:58] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=645748) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=645748) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  3.06it/s]
(EngineCore_DP0 pid=645748) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.25it/s]
(EngineCore_DP0 pid=645748) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.35it/s]
(EngineCore_DP0 pid=645748) 
(EngineCore_DP0 pid=645748) 2026-01-25 23:06:07,142 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=645748) 2026-01-25 23:06:07,157 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=645748) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▊         | 3/35 [00:00<00:01, 20.77it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 6/35 [00:00<00:01, 23.34it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▌       | 9/35 [00:00<00:01, 24.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  34%|███▍      | 12/35 [00:00<00:00, 26.10it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 15/35 [00:00<00:00, 26.88it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████▏    | 18/35 [00:00<00:00, 27.39it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 21/35 [00:00<00:00, 27.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 24/35 [00:00<00:00, 28.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  77%|███████▋  | 27/35 [00:01<00:00, 28.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 30/35 [00:01<00:00, 28.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 33/35 [00:01<00:00, 26.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:01<00:00, 26.53it/s]
(EngineCore_DP0 pid=645748) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   5%|▌         | 1/19 [00:00<00:02,  7.32it/s]
Capturing CUDA graphs (decode, FULL):  21%|██        | 4/19 [00:00<00:00, 18.68it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 7/19 [00:00<00:00, 23.17it/s]
Capturing CUDA graphs (decode, FULL):  53%|█████▎    | 10/19 [00:00<00:00, 20.65it/s]
Capturing CUDA graphs (decode, FULL):  68%|██████▊   | 13/19 [00:00<00:00, 19.37it/s]
Capturing CUDA graphs (decode, FULL):  84%|████████▍ | 16/19 [00:00<00:00, 19.14it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:00<00:00, 21.66it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:00<00:00, 20.20it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 7684.40it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:04<08:55,  4.21s/it, est. speed input: 3.80 toks/s, output: 60.74 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:04<00:01, 28.38it/s, est. speed input: 322.30 toks/s, output: 5156.73 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:04<00:00, 28.38it/s, est. speed input: 469.13 toks/s, output: 7506.01 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:04<00:00, 29.32it/s, est. speed input: 469.13 toks/s, output: 7506.01 toks/s]
[rank0]:[W125 23:06:14.649117760 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-25 23:06:16
Backend: cuBLASLt
Checkpoint: /root/vllmbench/checkpoints/Qwen2.5-7B-FP8
Params: prompt_len=16, output_len=256, num_prompts=256, max_num_seqs=256
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Qwen2.5-7B-FP8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 256 --max-num-seqs 256 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cublaslt/json/Qwen2.5-7B-FP8_M256.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 23:06:19 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=646364) WARNING 01-25 23:06:29 [backends.py:609] Failed to read file <frozen os>
Throughput: 24.52 requests/s, 6668.63 total tokens/s, 6276.36 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536

STDERR:
[2026-01-25 23:06:19] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 23:06:19] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-25 23:06:19] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-25 23:06:19] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-25 23:06:19] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-25 23:06:19] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-25 23:06:19] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-25 23:06:19] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-25 23:06:19] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-25 23:06:19] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 23:06:19] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 23:06:19] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 23:06:19] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 23:06:19] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 23:06:23] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 23:06:23] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-25 23:06:23] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-25 23:06:23] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-25 23:06:23] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-25 23:06:23] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-25 23:06:23] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-25 23:06:23] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-25 23:06:23] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-25 23:06:23] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 23:06:23] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 23:06:23] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 23:06:23] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 23:06:23] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=646364) [2026-01-25 23:06:24] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=646364) [2026-01-25 23:06:24] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=646364) [2026-01-25 23:06:24] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=646364) [2026-01-25 23:06:24] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=646364) [2026-01-25 23:06:24] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=646364) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=646364) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  3.08it/s]
(EngineCore_DP0 pid=646364) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.26it/s]
(EngineCore_DP0 pid=646364) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.35it/s]
(EngineCore_DP0 pid=646364) 
(EngineCore_DP0 pid=646364) 2026-01-25 23:06:33,241 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=646364) 2026-01-25 23:06:33,257 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=646364) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/36 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 2/36 [00:00<00:01, 18.34it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 5/36 [00:00<00:01, 22.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 8/36 [00:00<00:01, 23.84it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███       | 11/36 [00:00<00:00, 25.15it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|███▉      | 14/36 [00:00<00:00, 26.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 17/36 [00:00<00:00, 27.15it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  56%|█████▌    | 20/36 [00:00<00:00, 27.55it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▍   | 23/36 [00:00<00:00, 28.07it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  72%|███████▏  | 26/36 [00:00<00:00, 28.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  81%|████████  | 29/36 [00:01<00:00, 28.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 32/36 [00:01<00:00, 28.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:01<00:00, 27.72it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:01<00:00, 26.92it/s]
(EngineCore_DP0 pid=646364) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   3%|▎         | 1/35 [00:00<00:04,  7.13it/s]
Capturing CUDA graphs (decode, FULL):  11%|█▏        | 4/35 [00:00<00:01, 17.34it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 7/35 [00:00<00:01, 21.20it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 10/35 [00:00<00:01, 23.75it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 13/35 [00:00<00:00, 25.62it/s]
Capturing CUDA graphs (decode, FULL):  46%|████▌     | 16/35 [00:00<00:00, 26.92it/s]
Capturing CUDA graphs (decode, FULL):  54%|█████▍    | 19/35 [00:00<00:00, 27.82it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 22/35 [00:00<00:00, 27.09it/s]
Capturing CUDA graphs (decode, FULL):  74%|███████▍  | 26/35 [00:01<00:00, 28.04it/s]
Capturing CUDA graphs (decode, FULL):  83%|████████▎ | 29/35 [00:01<00:00, 28.51it/s]
Capturing CUDA graphs (decode, FULL):  91%|█████████▏| 32/35 [00:01<00:00, 28.28it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:01<00:00, 25.35it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:01<00:00, 25.21it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  38%|███▊      | 98/256 [00:00<00:00, 629.37it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 1465.28it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:07<30:24,  7.16s/it, est. speed input: 2.24 toks/s, output: 35.78 toks/s]
Processed prompts:  24%|██▍       | 62/256 [00:07<00:16, 12.07it/s, est. speed input: 136.64 toks/s, output: 2186.21 toks/s]
Processed prompts:  53%|█████▎    | 136/256 [00:07<00:03, 31.82it/s, est. speed input: 295.49 toks/s, output: 4727.90 toks/s]
Processed prompts:  73%|███████▎  | 188/256 [00:07<00:01, 49.21it/s, est. speed input: 400.57 toks/s, output: 6409.13 toks/s]
Processed prompts:  91%|█████████▏| 234/256 [00:09<00:00, 34.74it/s, est. speed input: 388.86 toks/s, output: 6221.81 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:10<00:00, 34.74it/s, est. speed input: 398.98 toks/s, output: 6383.75 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:10<00:00, 24.94it/s, est. speed input: 398.98 toks/s, output: 6383.75 toks/s]
[rank0]:[W125 23:06:47.335211832 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-25 23:06:48
Backend: cuBLASLt
Checkpoint: /root/vllmbench/checkpoints/Qwen2.5-7B-FP8
Params: prompt_len=16, output_len=256, num_prompts=512, max_num_seqs=512
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Qwen2.5-7B-FP8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 512 --max-num-seqs 512 --max-model-len 272 --max-num-batched-tokens 512 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cublaslt/json/Qwen2.5-7B-FP8_M512.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 23:06:52 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=647102) WARNING 01-25 23:07:02 [backends.py:609] Failed to read file <frozen os>
Throughput: 21.65 requests/s, 5889.95 total tokens/s, 5543.49 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072

STDERR:
[2026-01-25 23:06:52] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 23:06:52] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-25 23:06:52] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-25 23:06:52] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-25 23:06:52] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-25 23:06:52] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-25 23:06:52] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-25 23:06:52] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-25 23:06:52] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-25 23:06:52] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 23:06:52] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 23:06:52] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 23:06:52] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 23:06:52] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 23:06:56] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 23:06:56] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-25 23:06:56] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-25 23:06:56] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-25 23:06:56] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-25 23:06:56] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-25 23:06:56] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-25 23:06:56] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-25 23:06:56] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-25 23:06:56] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 23:06:56] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 23:06:56] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 23:06:56] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 23:06:56] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=647102) [2026-01-25 23:06:57] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=647102) [2026-01-25 23:06:57] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=647102) [2026-01-25 23:06:57] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=647102) [2026-01-25 23:06:57] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=647102) [2026-01-25 23:06:57] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=647102) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=647102) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  3.02it/s]
(EngineCore_DP0 pid=647102) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.23it/s]
(EngineCore_DP0 pid=647102) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.33it/s]
(EngineCore_DP0 pid=647102) 
(EngineCore_DP0 pid=647102) 2026-01-25 23:07:07,713 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=647102) 2026-01-25 23:07:07,728 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=647102) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|▍         | 2/51 [00:00<00:03, 15.85it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 4/51 [00:00<00:02, 16.97it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|█▏        | 6/51 [00:00<00:02, 17.79it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 8/51 [00:00<00:02, 18.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 11/51 [00:00<00:02, 19.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 14/51 [00:00<00:01, 20.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 17/51 [00:00<00:01, 21.32it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|███▉      | 20/51 [00:00<00:01, 22.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 23/51 [00:01<00:01, 20.19it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████     | 26/51 [00:01<00:01, 18.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 28/51 [00:01<00:01, 16.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|██████    | 31/51 [00:01<00:01, 18.19it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 34/51 [00:01<00:00, 20.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 37/51 [00:01<00:00, 22.66it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 41/51 [00:01<00:00, 24.97it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  88%|████████▊ | 45/51 [00:02<00:00, 26.60it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 48/51 [00:02<00:00, 26.87it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:02<00:00, 26.36it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:02<00:00, 21.75it/s]
(EngineCore_DP0 pid=647102) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   2%|▏         | 1/51 [00:00<00:07,  6.36it/s]
Capturing CUDA graphs (decode, FULL):   6%|▌         | 3/51 [00:00<00:03, 12.12it/s]
Capturing CUDA graphs (decode, FULL):  10%|▉         | 5/51 [00:00<00:03, 14.62it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▎        | 7/51 [00:00<00:02, 16.10it/s]
Capturing CUDA graphs (decode, FULL):  18%|█▊        | 9/51 [00:00<00:02, 17.33it/s]
Capturing CUDA graphs (decode, FULL):  24%|██▎       | 12/51 [00:00<00:02, 19.07it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▉       | 15/51 [00:00<00:01, 19.93it/s]
Capturing CUDA graphs (decode, FULL):  35%|███▌      | 18/51 [00:00<00:01, 21.12it/s]
Capturing CUDA graphs (decode, FULL):  41%|████      | 21/51 [00:01<00:01, 22.65it/s]
Capturing CUDA graphs (decode, FULL):  47%|████▋     | 24/51 [00:01<00:01, 23.78it/s]
Capturing CUDA graphs (decode, FULL):  53%|█████▎    | 27/51 [00:01<00:00, 25.16it/s]
Capturing CUDA graphs (decode, FULL):  59%|█████▉    | 30/51 [00:01<00:00, 26.49it/s]
Capturing CUDA graphs (decode, FULL):  65%|██████▍   | 33/51 [00:01<00:00, 27.36it/s]
Capturing CUDA graphs (decode, FULL):  73%|███████▎  | 37/51 [00:01<00:00, 28.35it/s]
Capturing CUDA graphs (decode, FULL):  78%|███████▊  | 40/51 [00:01<00:00, 26.74it/s]
Capturing CUDA graphs (decode, FULL):  84%|████████▍ | 43/51 [00:01<00:00, 22.63it/s]
Capturing CUDA graphs (decode, FULL):  90%|█████████ | 46/51 [00:02<00:00, 20.24it/s]
Capturing CUDA graphs (decode, FULL):  96%|█████████▌| 49/51 [00:02<00:00, 21.41it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:02<00:00, 21.79it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 7044.83it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:07<1:00:54,  7.15s/it, est. speed input: 2.24 toks/s, output: 35.80 toks/s]
Processed prompts:  12%|█▏        | 63/512 [00:07<00:36, 12.27it/s, est. speed input: 138.93 toks/s, output: 2222.85 toks/s]
Processed prompts:  19%|█▉        | 99/512 [00:08<00:24, 16.61it/s, est. speed input: 186.60 toks/s, output: 2985.59 toks/s]
Processed prompts:  23%|██▎       | 120/512 [00:10<00:26, 15.00it/s, est. speed input: 187.31 toks/s, output: 2996.98 toks/s]
Processed prompts:  26%|██▌       | 133/512 [00:11<00:25, 14.86it/s, est. speed input: 190.73 toks/s, output: 3051.60 toks/s]
Processed prompts:  28%|██▊       | 142/512 [00:11<00:22, 16.36it/s, est. speed input: 198.97 toks/s, output: 3183.54 toks/s]
Processed prompts:  29%|██▉       | 149/512 [00:11<00:22, 16.39it/s, est. speed input: 201.30 toks/s, output: 3220.87 toks/s]
Processed prompts:  30%|███       | 155/512 [00:12<00:20, 17.78it/s, est. speed input: 206.21 toks/s, output: 3299.43 toks/s]
Processed prompts:  31%|███▏      | 160/512 [00:12<00:20, 17.50it/s, est. speed input: 207.53 toks/s, output: 3320.49 toks/s]
Processed prompts:  32%|███▏      | 164/512 [00:12<00:18, 18.61it/s, est. speed input: 210.34 toks/s, output: 3365.47 toks/s]
Processed prompts:  33%|███▎      | 169/512 [00:12<00:16, 20.83it/s, est. speed input: 214.43 toks/s, output: 3430.94 toks/s]
Processed prompts:  34%|███▍      | 174/512 [00:12<00:17, 19.76it/s, est. speed input: 215.72 toks/s, output: 3451.53 toks/s]
Processed prompts:  35%|███▍      | 178/512 [00:13<00:15, 21.50it/s, est. speed input: 218.57 toks/s, output: 3497.11 toks/s]
Processed prompts:  36%|███▌      | 182/512 [00:13<00:14, 23.43it/s, est. speed input: 221.44 toks/s, output: 3543.07 toks/s]
Processed prompts:  36%|███▋      | 186/512 [00:13<00:12, 25.74it/s, est. speed input: 224.45 toks/s, output: 3591.16 toks/s]
Processed prompts:  37%|███▋      | 190/512 [00:13<00:12, 26.66it/s, est. speed input: 226.97 toks/s, output: 3631.44 toks/s]
Processed prompts:  38%|███▊      | 196/512 [00:13<00:13, 22.63it/s, est. speed input: 228.39 toks/s, output: 3654.31 toks/s]
Processed prompts:  39%|███▉      | 201/512 [00:13<00:12, 25.47it/s, est. speed input: 231.88 toks/s, output: 3710.04 toks/s]
Processed prompts:  40%|████      | 205/512 [00:14<00:11, 25.79it/s, est. speed input: 233.97 toks/s, output: 3743.55 toks/s]
Processed prompts:  41%|████      | 211/512 [00:14<00:10, 28.35it/s, est. speed input: 237.88 toks/s, output: 3806.09 toks/s]
Processed prompts:  42%|████▏     | 215/512 [00:14<00:09, 30.50it/s, est. speed input: 240.67 toks/s, output: 3850.68 toks/s]
Processed prompts:  43%|████▎     | 219/512 [00:14<00:09, 32.39it/s, est. speed input: 243.40 toks/s, output: 3894.45 toks/s]
Processed prompts:  44%|████▎     | 223/512 [00:14<00:08, 33.90it/s, est. speed input: 246.08 toks/s, output: 3937.31 toks/s]
Processed prompts:  44%|████▍     | 227/512 [00:14<00:11, 24.07it/s, est. speed input: 245.60 toks/s, output: 3929.57 toks/s]
Processed prompts:  45%|████▌     | 231/512 [00:14<00:10, 26.98it/s, est. speed input: 248.19 toks/s, output: 3970.96 toks/s]
Processed prompts:  46%|████▌     | 235/512 [00:14<00:09, 29.47it/s, est. speed input: 250.71 toks/s, output: 4011.40 toks/s]
Processed prompts:  47%|████▋     | 239/512 [00:15<00:08, 31.44it/s, est. speed input: 253.18 toks/s, output: 4050.86 toks/s]
Processed prompts:  47%|████▋     | 243/512 [00:15<00:08, 33.14it/s, est. speed input: 255.64 toks/s, output: 4090.17 toks/s]
Processed prompts:  48%|████▊     | 247/512 [00:15<00:07, 33.87it/s, est. speed input: 257.95 toks/s, output: 4127.14 toks/s]
Processed prompts:  49%|████▉     | 253/512 [00:15<00:06, 38.76it/s, est. speed input: 262.16 toks/s, output: 4194.62 toks/s]
Processed prompts:  50%|█████     | 258/512 [00:16<00:13, 18.30it/s, est. speed input: 257.58 toks/s, output: 4121.36 toks/s]
Processed prompts:  51%|█████     | 262/512 [00:16<00:20, 12.36it/s, est. speed input: 251.93 toks/s, output: 4030.82 toks/s]
Processed prompts:  52%|█████▏    | 265/512 [00:17<00:23, 10.42it/s, est. speed input: 248.14 toks/s, output: 3970.31 toks/s]
Processed prompts:  52%|█████▏    | 267/512 [00:17<00:21, 11.14it/s, est. speed input: 248.27 toks/s, output: 3972.28 toks/s]
Processed prompts:  53%|█████▎    | 269/512 [00:17<00:22, 10.68it/s, est. speed input: 246.99 toks/s, output: 3951.87 toks/s]
Processed prompts:  53%|█████▎    | 272/512 [00:17<00:18, 12.72it/s, est. speed input: 247.89 toks/s, output: 3966.29 toks/s]
Processed prompts:  54%|█████▎    | 274/512 [00:17<00:17, 13.53it/s, est. speed input: 248.09 toks/s, output: 3969.43 toks/s]
Processed prompts:  54%|█████▍    | 277/512 [00:17<00:15, 15.48it/s, est. speed input: 248.91 toks/s, output: 3982.53 toks/s]
Processed prompts:  54%|█████▍    | 279/512 [00:17<00:14, 16.34it/s, est. speed input: 249.30 toks/s, output: 3988.81 toks/s]
Processed prompts:  55%|█████▌    | 282/512 [00:18<00:12, 17.90it/s, est. speed input: 250.08 toks/s, output: 4001.36 toks/s]
Processed prompts:  56%|█████▌    | 285/512 [00:18<00:11, 20.28it/s, est. speed input: 251.26 toks/s, output: 4020.12 toks/s]
Processed prompts:  56%|█████▋    | 288/512 [00:18<00:10, 21.02it/s, est. speed input: 252.08 toks/s, output: 4033.25 toks/s]
Processed prompts:  57%|█████▋    | 294/512 [00:18<00:07, 28.14it/s, est. speed input: 255.47 toks/s, output: 4087.48 toks/s]
Processed prompts:  58%|█████▊    | 298/512 [00:18<00:07, 28.51it/s, est. speed input: 257.04 toks/s, output: 4112.65 toks/s]
Processed prompts:  59%|█████▉    | 301/512 [00:18<00:07, 28.00it/s, est. speed input: 258.06 toks/s, output: 4128.92 toks/s]
Processed prompts:  59%|█████▉    | 304/512 [00:18<00:07, 26.06it/s, est. speed input: 258.72 toks/s, output: 4139.58 toks/s]
Processed prompts:  60%|█████▉    | 307/512 [00:18<00:08, 24.85it/s, est. speed input: 259.41 toks/s, output: 4150.48 toks/s]
Processed prompts:  61%|██████    | 310/512 [00:19<00:07, 25.42it/s, est. speed input: 260.41 toks/s, output: 4166.61 toks/s]
Processed prompts:  61%|██████    | 313/512 [00:19<00:07, 25.49it/s, est. speed input: 261.33 toks/s, output: 4181.28 toks/s]
Processed prompts:  62%|██████▏   | 316/512 [00:19<00:07, 26.57it/s, est. speed input: 262.45 toks/s, output: 4199.16 toks/s]
Processed prompts:  63%|██████▎   | 323/512 [00:19<00:05, 34.07it/s, est. speed input: 266.25 toks/s, output: 4259.99 toks/s]
Processed prompts:  64%|██████▍   | 327/512 [00:19<00:06, 30.31it/s, est. speed input: 267.23 toks/s, output: 4275.70 toks/s]
Processed prompts:  65%|██████▍   | 331/512 [00:19<00:05, 31.11it/s, est. speed input: 268.85 toks/s, output: 4301.57 toks/s]
Processed prompts:  66%|██████▌   | 337/512 [00:19<00:04, 37.58it/s, est. speed input: 272.25 toks/s, output: 4356.03 toks/s]
Processed prompts:  67%|██████▋   | 342/512 [00:19<00:04, 36.62it/s, est. speed input: 274.30 toks/s, output: 4388.73 toks/s]
Processed prompts:  68%|██████▊   | 347/512 [00:20<00:04, 39.81it/s, est. speed input: 276.90 toks/s, output: 4430.41 toks/s]
Processed prompts:  69%|██████▉   | 352/512 [00:20<00:04, 38.38it/s, est. speed input: 278.93 toks/s, output: 4462.90 toks/s]
Processed prompts:  71%|███████   | 361/512 [00:20<00:03, 48.92it/s, est. speed input: 284.37 toks/s, output: 4549.99 toks/s]
Processed prompts:  73%|███████▎  | 375/512 [00:20<00:01, 68.95it/s, est. speed input: 293.70 toks/s, output: 4699.12 toks/s]
Processed prompts:  76%|███████▌  | 387/512 [00:20<00:01, 72.31it/s, est. speed input: 300.85 toks/s, output: 4813.53 toks/s]
Processed prompts:  77%|███████▋  | 395/512 [00:21<00:03, 30.11it/s, est. speed input: 297.00 toks/s, output: 4752.05 toks/s]
Processed prompts:  78%|███████▊  | 401/512 [00:21<00:04, 25.96it/s, est. speed input: 296.72 toks/s, output: 4747.59 toks/s]
Processed prompts:  79%|███████▉  | 406/512 [00:21<00:04, 26.17it/s, est. speed input: 297.87 toks/s, output: 4765.93 toks/s]
Processed prompts:  80%|████████  | 410/512 [00:21<00:03, 27.75it/s, est. speed input: 299.33 toks/s, output: 4789.28 toks/s]
Processed prompts:  81%|████████  | 414/512 [00:22<00:03, 26.83it/s, est. speed input: 299.96 toks/s, output: 4799.41 toks/s]
Processed prompts:  82%|████████▏ | 418/512 [00:22<00:03, 25.00it/s, est. speed input: 300.21 toks/s, output: 4803.29 toks/s]
Processed prompts:  82%|████████▏ | 422/512 [00:22<00:03, 24.78it/s, est. speed input: 300.84 toks/s, output: 4813.47 toks/s]
Processed prompts:  83%|████████▎ | 426/512 [00:22<00:03, 25.32it/s, est. speed input: 301.70 toks/s, output: 4827.19 toks/s]
Processed prompts:  84%|████████▍ | 430/512 [00:22<00:02, 28.00it/s, est. speed input: 303.15 toks/s, output: 4850.43 toks/s]
Processed prompts:  85%|████████▍ | 435/512 [00:22<00:02, 32.65it/s, est. speed input: 305.32 toks/s, output: 4885.04 toks/s]
Processed prompts:  86%|████████▌ | 440/512 [00:22<00:02, 35.57it/s, est. speed input: 307.29 toks/s, output: 4916.65 toks/s]
Processed prompts:  87%|████████▋ | 446/512 [00:23<00:01, 40.08it/s, est. speed input: 309.92 toks/s, output: 4958.78 toks/s]
Processed prompts:  88%|████████▊ | 453/512 [00:23<00:01, 46.91it/s, est. speed input: 313.34 toks/s, output: 5013.51 toks/s]
Processed prompts:  90%|████████▉ | 460/512 [00:23<00:00, 52.68it/s, est. speed input: 316.79 toks/s, output: 5068.63 toks/s]
Processed prompts:  92%|█████████▏| 470/512 [00:23<00:00, 63.37it/s, est. speed input: 322.12 toks/s, output: 5153.85 toks/s]
Processed prompts:  95%|█████████▍| 485/512 [00:23<00:00, 84.75it/s, est. speed input: 330.84 toks/s, output: 5293.46 toks/s]
Processed prompts:  99%|█████████▉| 507/512 [00:23<00:00, 120.84it/s, est. speed input: 344.33 toks/s, output: 5509.30 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:23<00:00, 120.84it/s, est. speed input: 347.56 toks/s, output: 5561.03 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:23<00:00, 21.72it/s, est. speed input: 347.56 toks/s, output: 5561.03 toks/s] 
[rank0]:[W125 23:07:37.992154037 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=64 ==========
Time: 2026-01-25 23:20:11
Backend: cuBLASLt
Checkpoint: /root/vllmbench/checkpoints/Qwen2.5-14B-FP8
Params: prompt_len=16, output_len=256, num_prompts=64, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Qwen2.5-14B-FP8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 64 --max-num-seqs 64 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cublaslt/json/Qwen2.5-14B-FP8_M64.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 23:20:15 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=667859) ERROR 01-25 23:20:20 [gpu_model_runner.py:3657] Failed to load model - not enough GPU memory. Try lowering --gpu-memory-utilization to free memory for weights, increasing --tensor-parallel-size, or using --quantization. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more tips. (original error: CUDA out of memory. Tried to allocate 1.45 GiB. GPU 0 has a total capacity of 15.46 GiB of which 948.94 MiB is free. Including non-PyTorch memory, this process has 14.31 GiB memory in use. Of the allocated memory 13.94 GiB is allocated by PyTorch, and 11.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables))
(EngineCore_DP0 pid=667859) ERROR 01-25 23:20:20 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=667859) ERROR 01-25 23:20:20 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=667859) ERROR 01-25 23:20:20 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=667859) ERROR 01-25 23:20:20 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=667859) ERROR 01-25 23:20:20 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=667859) ERROR 01-25 23:20:20 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=667859) ERROR 01-25 23:20:20 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=667859) ERROR 01-25 23:20:20 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=667859) ERROR 01-25 23:20:20 [core.py:866]     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=667859) ERROR 01-25 23:20:20 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=667859) ERROR 01-25 23:20:20 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=667859) ERROR 01-25 23:20:20 [core.py:866]     self._init_executor()
(EngineCore_DP0 pid=667859) ERROR 01-25 23:20:20 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
(EngineCore_DP0 pid=667859) ERROR 01-25 23:20:20 [core.py:866]     self.driver_worker.load_model()
(EngineCore_DP0 pid=667859) ERROR 01-25 23:20:20 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 289, in load_model
(EngineCore_DP0 pid=667859) ERROR 01-25 23:20:20 [core.py:866]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
(EngineCore_DP0 pid=667859) ERROR 01-25 23:20:20 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 3658, in load_model
(EngineCore_DP0 pid=667859) ERROR 01-25 23:20:20 [core.py:866]     raise e
(EngineCore_DP0 pid=667859) ERROR 01-25 23:20:20 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 3581, in load_model
(EngineCore_DP0 pid=667859) ERROR 01-25 23:20:20 [core.py:866]     self.model = model_loader.load_model(
(EngineCore_DP0 pid=667859) ERROR 01-25 23:20:20 [core.py:866]                  ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=667859) ERROR 01-25 23:20:20 [core.py:866]   File "/root/vllmbench/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
(EngineCore_DP0 pid=667859) ERROR 01-25 23:20:20 [core.py:866]     model = initialize_model(
(EngineCore_DP0 pid=667859) ERROR 01-25 23:20:20 [core.py:866]             ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=667859) ERROR 01-25 23:20:20 [core.py:866]   File "/root/vllmbench/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
(EngineCore_DP0 pid=667859) ERROR 01-25 23:20:20 [core.py:866]     return model_class(vllm_config=vllm_config, prefix=prefix)
(EngineCore_DP0 pid=667859) ERROR 01-25 23:20:20 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=667859) ERROR 01-25 23:20:20 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 551, in __init__
(EngineCore_DP0 pid=667859) ERROR 01-25 23:20:20 [core.py:866]     self.lm_head = ParallelLMHead(
(EngineCore_DP0 pid=667859) ERROR 01-25 23:20:20 [core.py:866]                    ^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=667859) ERROR 01-25 23:20:20 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/vocab_parallel_embedding.py", line 523, in __init__
(EngineCore_DP0 pid=667859) ERROR 01-25 23:20:20 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=667859) ERROR 01-25 23:20:20 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/vocab_parallel_embedding.py", line 301, in __init__
(EngineCore_DP0 pid=667859) ERROR 01-25 23:20:20 [core.py:866]     self.quant_method.create_weights(
(EngineCore_DP0 pid=667859) ERROR 01-25 23:20:20 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/vocab_parallel_embedding.py", line 46, in create_weights
(EngineCore_DP0 pid=667859) ERROR 01-25 23:20:20 [core.py:866]     torch.empty(
(EngineCore_DP0 pid=667859) ERROR 01-25 23:20:20 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_device.py", line 103, in __torch_function__
(EngineCore_DP0 pid=667859) ERROR 01-25 23:20:20 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=667859) ERROR 01-25 23:20:20 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=667859) ERROR 01-25 23:20:20 [core.py:866] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.45 GiB. GPU 0 has a total capacity of 15.46 GiB of which 948.94 MiB is free. Including non-PyTorch memory, this process has 14.31 GiB memory in use. Of the allocated memory 13.94 GiB is allocated by PyTorch, and 11.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

STDERR:
[2026-01-25 23:20:15] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 23:20:15] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-25 23:20:15] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-25 23:20:15] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-25 23:20:15] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-25 23:20:15] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-25 23:20:15] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-25 23:20:15] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-25 23:20:15] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-25 23:20:15] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 23:20:15] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 23:20:15] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 23:20:15] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 23:20:15] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 23:20:19] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 23:20:19] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-25 23:20:19] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-25 23:20:19] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-25 23:20:19] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-25 23:20:19] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-25 23:20:19] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-25 23:20:19] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-25 23:20:19] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-25 23:20:19] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 23:20:19] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 23:20:19] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 23:20:19] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 23:20:19] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=667859) [2026-01-25 23:20:19] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=667859) [2026-01-25 23:20:19] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=667859) [2026-01-25 23:20:19] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=667859) [2026-01-25 23:20:19] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=667859) [2026-01-25 23:20:19] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=667859) Process EngineCore_DP0:
(EngineCore_DP0 pid=667859) Traceback (most recent call last):
(EngineCore_DP0 pid=667859)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=667859)     self.run()
(EngineCore_DP0 pid=667859)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=667859)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=667859)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=667859)     raise e
(EngineCore_DP0 pid=667859)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=667859)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=667859)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=667859)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=667859)     super().__init__(
(EngineCore_DP0 pid=667859)   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=667859)     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=667859)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=667859)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=667859)     self._init_executor()
(EngineCore_DP0 pid=667859)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
(EngineCore_DP0 pid=667859)     self.driver_worker.load_model()
(EngineCore_DP0 pid=667859)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 289, in load_model
(EngineCore_DP0 pid=667859)     self.model_runner.load_model(eep_scale_up=eep_scale_up)
(EngineCore_DP0 pid=667859)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 3658, in load_model
(EngineCore_DP0 pid=667859)     raise e
(EngineCore_DP0 pid=667859)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 3581, in load_model
(EngineCore_DP0 pid=667859)     self.model = model_loader.load_model(
(EngineCore_DP0 pid=667859)                  ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=667859)   File "/root/vllmbench/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
(EngineCore_DP0 pid=667859)     model = initialize_model(
(EngineCore_DP0 pid=667859)             ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=667859)   File "/root/vllmbench/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
(EngineCore_DP0 pid=667859)     return model_class(vllm_config=vllm_config, prefix=prefix)
(EngineCore_DP0 pid=667859)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=667859)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 551, in __init__
(EngineCore_DP0 pid=667859)     self.lm_head = ParallelLMHead(
(EngineCore_DP0 pid=667859)                    ^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=667859)   File "/root/vllmbench/vllm/model_executor/layers/vocab_parallel_embedding.py", line 523, in __init__
(EngineCore_DP0 pid=667859)     super().__init__(
(EngineCore_DP0 pid=667859)   File "/root/vllmbench/vllm/model_executor/layers/vocab_parallel_embedding.py", line 301, in __init__
(EngineCore_DP0 pid=667859)     self.quant_method.create_weights(
(EngineCore_DP0 pid=667859)   File "/root/vllmbench/vllm/model_executor/layers/vocab_parallel_embedding.py", line 46, in create_weights
(EngineCore_DP0 pid=667859)     torch.empty(
(EngineCore_DP0 pid=667859)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_device.py", line 103, in __torch_function__
(EngineCore_DP0 pid=667859)     return func(*args, **kwargs)
(EngineCore_DP0 pid=667859)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=667859) torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.45 GiB. GPU 0 has a total capacity of 15.46 GiB of which 948.94 MiB is free. Including non-PyTorch memory, this process has 14.31 GiB memory in use. Of the allocated memory 13.94 GiB is allocated by PyTorch, and 11.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W125 23:20:20.594636681 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=64

========== M=128 ==========
Time: 2026-01-25 23:20:22
Backend: cuBLASLt
Checkpoint: /root/vllmbench/checkpoints/Qwen2.5-14B-FP8
Params: prompt_len=16, output_len=256, num_prompts=128, max_num_seqs=128
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Qwen2.5-14B-FP8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 128 --max-num-seqs 128 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cublaslt/json/Qwen2.5-14B-FP8_M128.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 23:20:26 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=668252) ERROR 01-25 23:20:31 [gpu_model_runner.py:3657] Failed to load model - not enough GPU memory. Try lowering --gpu-memory-utilization to free memory for weights, increasing --tensor-parallel-size, or using --quantization. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more tips. (original error: CUDA out of memory. Tried to allocate 1.45 GiB. GPU 0 has a total capacity of 15.46 GiB of which 948.94 MiB is free. Including non-PyTorch memory, this process has 14.31 GiB memory in use. Of the allocated memory 13.94 GiB is allocated by PyTorch, and 11.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables))
(EngineCore_DP0 pid=668252) ERROR 01-25 23:20:31 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=668252) ERROR 01-25 23:20:31 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=668252) ERROR 01-25 23:20:31 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=668252) ERROR 01-25 23:20:31 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=668252) ERROR 01-25 23:20:31 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=668252) ERROR 01-25 23:20:31 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=668252) ERROR 01-25 23:20:31 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=668252) ERROR 01-25 23:20:31 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=668252) ERROR 01-25 23:20:31 [core.py:866]     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=668252) ERROR 01-25 23:20:31 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=668252) ERROR 01-25 23:20:31 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=668252) ERROR 01-25 23:20:31 [core.py:866]     self._init_executor()
(EngineCore_DP0 pid=668252) ERROR 01-25 23:20:31 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
(EngineCore_DP0 pid=668252) ERROR 01-25 23:20:31 [core.py:866]     self.driver_worker.load_model()
(EngineCore_DP0 pid=668252) ERROR 01-25 23:20:31 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 289, in load_model
(EngineCore_DP0 pid=668252) ERROR 01-25 23:20:31 [core.py:866]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
(EngineCore_DP0 pid=668252) ERROR 01-25 23:20:31 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 3658, in load_model
(EngineCore_DP0 pid=668252) ERROR 01-25 23:20:31 [core.py:866]     raise e
(EngineCore_DP0 pid=668252) ERROR 01-25 23:20:31 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 3581, in load_model
(EngineCore_DP0 pid=668252) ERROR 01-25 23:20:31 [core.py:866]     self.model = model_loader.load_model(
(EngineCore_DP0 pid=668252) ERROR 01-25 23:20:31 [core.py:866]                  ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=668252) ERROR 01-25 23:20:31 [core.py:866]   File "/root/vllmbench/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
(EngineCore_DP0 pid=668252) ERROR 01-25 23:20:31 [core.py:866]     model = initialize_model(
(EngineCore_DP0 pid=668252) ERROR 01-25 23:20:31 [core.py:866]             ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=668252) ERROR 01-25 23:20:31 [core.py:866]   File "/root/vllmbench/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
(EngineCore_DP0 pid=668252) ERROR 01-25 23:20:31 [core.py:866]     return model_class(vllm_config=vllm_config, prefix=prefix)
(EngineCore_DP0 pid=668252) ERROR 01-25 23:20:31 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=668252) ERROR 01-25 23:20:31 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 551, in __init__
(EngineCore_DP0 pid=668252) ERROR 01-25 23:20:31 [core.py:866]     self.lm_head = ParallelLMHead(
(EngineCore_DP0 pid=668252) ERROR 01-25 23:20:31 [core.py:866]                    ^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=668252) ERROR 01-25 23:20:31 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/vocab_parallel_embedding.py", line 523, in __init__
(EngineCore_DP0 pid=668252) ERROR 01-25 23:20:31 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=668252) ERROR 01-25 23:20:31 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/vocab_parallel_embedding.py", line 301, in __init__
(EngineCore_DP0 pid=668252) ERROR 01-25 23:20:31 [core.py:866]     self.quant_method.create_weights(
(EngineCore_DP0 pid=668252) ERROR 01-25 23:20:31 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/vocab_parallel_embedding.py", line 46, in create_weights
(EngineCore_DP0 pid=668252) ERROR 01-25 23:20:31 [core.py:866]     torch.empty(
(EngineCore_DP0 pid=668252) ERROR 01-25 23:20:31 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_device.py", line 103, in __torch_function__
(EngineCore_DP0 pid=668252) ERROR 01-25 23:20:31 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=668252) ERROR 01-25 23:20:31 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=668252) ERROR 01-25 23:20:31 [core.py:866] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.45 GiB. GPU 0 has a total capacity of 15.46 GiB of which 948.94 MiB is free. Including non-PyTorch memory, this process has 14.31 GiB memory in use. Of the allocated memory 13.94 GiB is allocated by PyTorch, and 11.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

STDERR:
[2026-01-25 23:20:26] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 23:20:26] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-25 23:20:26] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-25 23:20:26] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-25 23:20:26] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-25 23:20:26] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-25 23:20:26] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-25 23:20:26] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-25 23:20:26] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-25 23:20:26] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 23:20:26] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 23:20:26] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 23:20:26] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 23:20:26] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 23:20:29] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 23:20:30] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-25 23:20:30] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-25 23:20:30] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-25 23:20:30] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-25 23:20:30] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-25 23:20:30] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-25 23:20:30] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-25 23:20:30] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-25 23:20:30] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 23:20:30] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 23:20:30] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 23:20:30] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 23:20:30] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=668252) [2026-01-25 23:20:30] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=668252) [2026-01-25 23:20:31] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=668252) [2026-01-25 23:20:31] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=668252) [2026-01-25 23:20:31] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=668252) [2026-01-25 23:20:31] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=668252) Process EngineCore_DP0:
(EngineCore_DP0 pid=668252) Traceback (most recent call last):
(EngineCore_DP0 pid=668252)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=668252)     self.run()
(EngineCore_DP0 pid=668252)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=668252)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=668252)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=668252)     raise e
(EngineCore_DP0 pid=668252)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=668252)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=668252)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=668252)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=668252)     super().__init__(
(EngineCore_DP0 pid=668252)   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=668252)     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=668252)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=668252)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=668252)     self._init_executor()
(EngineCore_DP0 pid=668252)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
(EngineCore_DP0 pid=668252)     self.driver_worker.load_model()
(EngineCore_DP0 pid=668252)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 289, in load_model
(EngineCore_DP0 pid=668252)     self.model_runner.load_model(eep_scale_up=eep_scale_up)
(EngineCore_DP0 pid=668252)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 3658, in load_model
(EngineCore_DP0 pid=668252)     raise e
(EngineCore_DP0 pid=668252)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 3581, in load_model
(EngineCore_DP0 pid=668252)     self.model = model_loader.load_model(
(EngineCore_DP0 pid=668252)                  ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=668252)   File "/root/vllmbench/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
(EngineCore_DP0 pid=668252)     model = initialize_model(
(EngineCore_DP0 pid=668252)             ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=668252)   File "/root/vllmbench/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
(EngineCore_DP0 pid=668252)     return model_class(vllm_config=vllm_config, prefix=prefix)
(EngineCore_DP0 pid=668252)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=668252)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 551, in __init__
(EngineCore_DP0 pid=668252)     self.lm_head = ParallelLMHead(
(EngineCore_DP0 pid=668252)                    ^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=668252)   File "/root/vllmbench/vllm/model_executor/layers/vocab_parallel_embedding.py", line 523, in __init__
(EngineCore_DP0 pid=668252)     super().__init__(
(EngineCore_DP0 pid=668252)   File "/root/vllmbench/vllm/model_executor/layers/vocab_parallel_embedding.py", line 301, in __init__
(EngineCore_DP0 pid=668252)     self.quant_method.create_weights(
(EngineCore_DP0 pid=668252)   File "/root/vllmbench/vllm/model_executor/layers/vocab_parallel_embedding.py", line 46, in create_weights
(EngineCore_DP0 pid=668252)     torch.empty(
(EngineCore_DP0 pid=668252)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_device.py", line 103, in __torch_function__
(EngineCore_DP0 pid=668252)     return func(*args, **kwargs)
(EngineCore_DP0 pid=668252)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=668252) torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.45 GiB. GPU 0 has a total capacity of 15.46 GiB of which 948.94 MiB is free. Including non-PyTorch memory, this process has 14.31 GiB memory in use. Of the allocated memory 13.94 GiB is allocated by PyTorch, and 11.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W125 23:20:31.646606265 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=128

========== M=256 ==========
Time: 2026-01-25 23:20:33
Backend: cuBLASLt
Checkpoint: /root/vllmbench/checkpoints/Qwen2.5-14B-FP8
Params: prompt_len=16, output_len=256, num_prompts=256, max_num_seqs=256
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Qwen2.5-14B-FP8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 256 --max-num-seqs 256 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cublaslt/json/Qwen2.5-14B-FP8_M256.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 23:20:37 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=668656) ERROR 01-25 23:20:41 [gpu_model_runner.py:3657] Failed to load model - not enough GPU memory. Try lowering --gpu-memory-utilization to free memory for weights, increasing --tensor-parallel-size, or using --quantization. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more tips. (original error: CUDA out of memory. Tried to allocate 1.45 GiB. GPU 0 has a total capacity of 15.46 GiB of which 948.94 MiB is free. Including non-PyTorch memory, this process has 14.31 GiB memory in use. Of the allocated memory 13.94 GiB is allocated by PyTorch, and 11.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables))
(EngineCore_DP0 pid=668656) ERROR 01-25 23:20:41 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=668656) ERROR 01-25 23:20:41 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=668656) ERROR 01-25 23:20:41 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=668656) ERROR 01-25 23:20:41 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=668656) ERROR 01-25 23:20:41 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=668656) ERROR 01-25 23:20:41 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=668656) ERROR 01-25 23:20:41 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=668656) ERROR 01-25 23:20:41 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=668656) ERROR 01-25 23:20:41 [core.py:866]     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=668656) ERROR 01-25 23:20:41 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=668656) ERROR 01-25 23:20:41 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=668656) ERROR 01-25 23:20:41 [core.py:866]     self._init_executor()
(EngineCore_DP0 pid=668656) ERROR 01-25 23:20:41 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
(EngineCore_DP0 pid=668656) ERROR 01-25 23:20:41 [core.py:866]     self.driver_worker.load_model()
(EngineCore_DP0 pid=668656) ERROR 01-25 23:20:41 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 289, in load_model
(EngineCore_DP0 pid=668656) ERROR 01-25 23:20:41 [core.py:866]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
(EngineCore_DP0 pid=668656) ERROR 01-25 23:20:41 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 3658, in load_model
(EngineCore_DP0 pid=668656) ERROR 01-25 23:20:41 [core.py:866]     raise e
(EngineCore_DP0 pid=668656) ERROR 01-25 23:20:41 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 3581, in load_model
(EngineCore_DP0 pid=668656) ERROR 01-25 23:20:41 [core.py:866]     self.model = model_loader.load_model(
(EngineCore_DP0 pid=668656) ERROR 01-25 23:20:41 [core.py:866]                  ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=668656) ERROR 01-25 23:20:41 [core.py:866]   File "/root/vllmbench/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
(EngineCore_DP0 pid=668656) ERROR 01-25 23:20:41 [core.py:866]     model = initialize_model(
(EngineCore_DP0 pid=668656) ERROR 01-25 23:20:41 [core.py:866]             ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=668656) ERROR 01-25 23:20:41 [core.py:866]   File "/root/vllmbench/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
(EngineCore_DP0 pid=668656) ERROR 01-25 23:20:41 [core.py:866]     return model_class(vllm_config=vllm_config, prefix=prefix)
(EngineCore_DP0 pid=668656) ERROR 01-25 23:20:41 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=668656) ERROR 01-25 23:20:41 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 551, in __init__
(EngineCore_DP0 pid=668656) ERROR 01-25 23:20:41 [core.py:866]     self.lm_head = ParallelLMHead(
(EngineCore_DP0 pid=668656) ERROR 01-25 23:20:41 [core.py:866]                    ^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=668656) ERROR 01-25 23:20:41 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/vocab_parallel_embedding.py", line 523, in __init__
(EngineCore_DP0 pid=668656) ERROR 01-25 23:20:41 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=668656) ERROR 01-25 23:20:41 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/vocab_parallel_embedding.py", line 301, in __init__
(EngineCore_DP0 pid=668656) ERROR 01-25 23:20:41 [core.py:866]     self.quant_method.create_weights(
(EngineCore_DP0 pid=668656) ERROR 01-25 23:20:41 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/vocab_parallel_embedding.py", line 46, in create_weights
(EngineCore_DP0 pid=668656) ERROR 01-25 23:20:41 [core.py:866]     torch.empty(
(EngineCore_DP0 pid=668656) ERROR 01-25 23:20:41 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_device.py", line 103, in __torch_function__
(EngineCore_DP0 pid=668656) ERROR 01-25 23:20:41 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=668656) ERROR 01-25 23:20:41 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=668656) ERROR 01-25 23:20:41 [core.py:866] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.45 GiB. GPU 0 has a total capacity of 15.46 GiB of which 948.94 MiB is free. Including non-PyTorch memory, this process has 14.31 GiB memory in use. Of the allocated memory 13.94 GiB is allocated by PyTorch, and 11.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

STDERR:
[2026-01-25 23:20:37] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 23:20:37] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-25 23:20:37] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-25 23:20:37] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-25 23:20:37] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-25 23:20:37] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-25 23:20:37] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-25 23:20:37] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-25 23:20:37] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-25 23:20:37] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 23:20:37] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 23:20:37] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 23:20:37] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 23:20:37] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 23:20:40] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 23:20:40] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-25 23:20:40] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-25 23:20:40] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-25 23:20:40] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-25 23:20:40] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-25 23:20:40] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-25 23:20:40] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-25 23:20:40] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-25 23:20:40] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 23:20:40] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 23:20:40] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 23:20:40] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 23:20:40] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=668656) [2026-01-25 23:20:41] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=668656) [2026-01-25 23:20:41] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=668656) [2026-01-25 23:20:41] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=668656) [2026-01-25 23:20:41] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=668656) [2026-01-25 23:20:41] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=668656) Process EngineCore_DP0:
(EngineCore_DP0 pid=668656) Traceback (most recent call last):
(EngineCore_DP0 pid=668656)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=668656)     self.run()
(EngineCore_DP0 pid=668656)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=668656)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=668656)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=668656)     raise e
(EngineCore_DP0 pid=668656)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=668656)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=668656)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=668656)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=668656)     super().__init__(
(EngineCore_DP0 pid=668656)   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=668656)     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=668656)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=668656)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=668656)     self._init_executor()
(EngineCore_DP0 pid=668656)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
(EngineCore_DP0 pid=668656)     self.driver_worker.load_model()
(EngineCore_DP0 pid=668656)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 289, in load_model
(EngineCore_DP0 pid=668656)     self.model_runner.load_model(eep_scale_up=eep_scale_up)
(EngineCore_DP0 pid=668656)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 3658, in load_model
(EngineCore_DP0 pid=668656)     raise e
(EngineCore_DP0 pid=668656)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 3581, in load_model
(EngineCore_DP0 pid=668656)     self.model = model_loader.load_model(
(EngineCore_DP0 pid=668656)                  ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=668656)   File "/root/vllmbench/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
(EngineCore_DP0 pid=668656)     model = initialize_model(
(EngineCore_DP0 pid=668656)             ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=668656)   File "/root/vllmbench/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
(EngineCore_DP0 pid=668656)     return model_class(vllm_config=vllm_config, prefix=prefix)
(EngineCore_DP0 pid=668656)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=668656)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 551, in __init__
(EngineCore_DP0 pid=668656)     self.lm_head = ParallelLMHead(
(EngineCore_DP0 pid=668656)                    ^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=668656)   File "/root/vllmbench/vllm/model_executor/layers/vocab_parallel_embedding.py", line 523, in __init__
(EngineCore_DP0 pid=668656)     super().__init__(
(EngineCore_DP0 pid=668656)   File "/root/vllmbench/vllm/model_executor/layers/vocab_parallel_embedding.py", line 301, in __init__
(EngineCore_DP0 pid=668656)     self.quant_method.create_weights(
(EngineCore_DP0 pid=668656)   File "/root/vllmbench/vllm/model_executor/layers/vocab_parallel_embedding.py", line 46, in create_weights
(EngineCore_DP0 pid=668656)     torch.empty(
(EngineCore_DP0 pid=668656)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_device.py", line 103, in __torch_function__
(EngineCore_DP0 pid=668656)     return func(*args, **kwargs)
(EngineCore_DP0 pid=668656)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=668656) torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.45 GiB. GPU 0 has a total capacity of 15.46 GiB of which 948.94 MiB is free. Including non-PyTorch memory, this process has 14.31 GiB memory in use. Of the allocated memory 13.94 GiB is allocated by PyTorch, and 11.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W125 23:20:42.095802172 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=256

========== M=512 ==========
Time: 2026-01-25 23:20:43
Backend: cuBLASLt
Checkpoint: /root/vllmbench/checkpoints/Qwen2.5-14B-FP8
Params: prompt_len=16, output_len=256, num_prompts=512, max_num_seqs=512
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Qwen2.5-14B-FP8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 512 --max-num-seqs 512 --max-model-len 272 --max-num-batched-tokens 512 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cublaslt/json/Qwen2.5-14B-FP8_M512.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 23:20:47 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=669050) ERROR 01-25 23:20:52 [gpu_model_runner.py:3657] Failed to load model - not enough GPU memory. Try lowering --gpu-memory-utilization to free memory for weights, increasing --tensor-parallel-size, or using --quantization. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more tips. (original error: CUDA out of memory. Tried to allocate 1.45 GiB. GPU 0 has a total capacity of 15.46 GiB of which 948.94 MiB is free. Including non-PyTorch memory, this process has 14.31 GiB memory in use. Of the allocated memory 13.94 GiB is allocated by PyTorch, and 8.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables))
(EngineCore_DP0 pid=669050) ERROR 01-25 23:20:52 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=669050) ERROR 01-25 23:20:52 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=669050) ERROR 01-25 23:20:52 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=669050) ERROR 01-25 23:20:52 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=669050) ERROR 01-25 23:20:52 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=669050) ERROR 01-25 23:20:52 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=669050) ERROR 01-25 23:20:52 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=669050) ERROR 01-25 23:20:52 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=669050) ERROR 01-25 23:20:52 [core.py:866]     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=669050) ERROR 01-25 23:20:52 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=669050) ERROR 01-25 23:20:52 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=669050) ERROR 01-25 23:20:52 [core.py:866]     self._init_executor()
(EngineCore_DP0 pid=669050) ERROR 01-25 23:20:52 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
(EngineCore_DP0 pid=669050) ERROR 01-25 23:20:52 [core.py:866]     self.driver_worker.load_model()
(EngineCore_DP0 pid=669050) ERROR 01-25 23:20:52 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 289, in load_model
(EngineCore_DP0 pid=669050) ERROR 01-25 23:20:52 [core.py:866]     self.model_runner.load_model(eep_scale_up=eep_scale_up)
(EngineCore_DP0 pid=669050) ERROR 01-25 23:20:52 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 3658, in load_model
(EngineCore_DP0 pid=669050) ERROR 01-25 23:20:52 [core.py:866]     raise e
(EngineCore_DP0 pid=669050) ERROR 01-25 23:20:52 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 3581, in load_model
(EngineCore_DP0 pid=669050) ERROR 01-25 23:20:52 [core.py:866]     self.model = model_loader.load_model(
(EngineCore_DP0 pid=669050) ERROR 01-25 23:20:52 [core.py:866]                  ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=669050) ERROR 01-25 23:20:52 [core.py:866]   File "/root/vllmbench/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
(EngineCore_DP0 pid=669050) ERROR 01-25 23:20:52 [core.py:866]     model = initialize_model(
(EngineCore_DP0 pid=669050) ERROR 01-25 23:20:52 [core.py:866]             ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=669050) ERROR 01-25 23:20:52 [core.py:866]   File "/root/vllmbench/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
(EngineCore_DP0 pid=669050) ERROR 01-25 23:20:52 [core.py:866]     return model_class(vllm_config=vllm_config, prefix=prefix)
(EngineCore_DP0 pid=669050) ERROR 01-25 23:20:52 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=669050) ERROR 01-25 23:20:52 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 551, in __init__
(EngineCore_DP0 pid=669050) ERROR 01-25 23:20:52 [core.py:866]     self.lm_head = ParallelLMHead(
(EngineCore_DP0 pid=669050) ERROR 01-25 23:20:52 [core.py:866]                    ^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=669050) ERROR 01-25 23:20:52 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/vocab_parallel_embedding.py", line 523, in __init__
(EngineCore_DP0 pid=669050) ERROR 01-25 23:20:52 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=669050) ERROR 01-25 23:20:52 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/vocab_parallel_embedding.py", line 301, in __init__
(EngineCore_DP0 pid=669050) ERROR 01-25 23:20:52 [core.py:866]     self.quant_method.create_weights(
(EngineCore_DP0 pid=669050) ERROR 01-25 23:20:52 [core.py:866]   File "/root/vllmbench/vllm/model_executor/layers/vocab_parallel_embedding.py", line 46, in create_weights
(EngineCore_DP0 pid=669050) ERROR 01-25 23:20:52 [core.py:866]     torch.empty(
(EngineCore_DP0 pid=669050) ERROR 01-25 23:20:52 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_device.py", line 103, in __torch_function__
(EngineCore_DP0 pid=669050) ERROR 01-25 23:20:52 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=669050) ERROR 01-25 23:20:52 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=669050) ERROR 01-25 23:20:52 [core.py:866] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.45 GiB. GPU 0 has a total capacity of 15.46 GiB of which 948.94 MiB is free. Including non-PyTorch memory, this process has 14.31 GiB memory in use. Of the allocated memory 13.94 GiB is allocated by PyTorch, and 8.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

STDERR:
[2026-01-25 23:20:47] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 23:20:47] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-25 23:20:47] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-25 23:20:47] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-25 23:20:47] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-25 23:20:47] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-25 23:20:47] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-25 23:20:47] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-25 23:20:47] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-25 23:20:47] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 23:20:47] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 23:20:47] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 23:20:47] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 23:20:47] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 23:20:51] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 23:20:51] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-25 23:20:51] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-25 23:20:51] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-25 23:20:51] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-25 23:20:51] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-25 23:20:51] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-25 23:20:51] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-25 23:20:51] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-25 23:20:51] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 23:20:51] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 23:20:51] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 23:20:51] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 23:20:51] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=669050) [2026-01-25 23:20:52] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=669050) [2026-01-25 23:20:52] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=669050) [2026-01-25 23:20:52] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=669050) [2026-01-25 23:20:52] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=669050) [2026-01-25 23:20:52] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=669050) Process EngineCore_DP0:
(EngineCore_DP0 pid=669050) Traceback (most recent call last):
(EngineCore_DP0 pid=669050)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=669050)     self.run()
(EngineCore_DP0 pid=669050)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=669050)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=669050)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=669050)     raise e
(EngineCore_DP0 pid=669050)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=669050)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=669050)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=669050)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=669050)     super().__init__(
(EngineCore_DP0 pid=669050)   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=669050)     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=669050)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=669050)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=669050)     self._init_executor()
(EngineCore_DP0 pid=669050)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 48, in _init_executor
(EngineCore_DP0 pid=669050)     self.driver_worker.load_model()
(EngineCore_DP0 pid=669050)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 289, in load_model
(EngineCore_DP0 pid=669050)     self.model_runner.load_model(eep_scale_up=eep_scale_up)
(EngineCore_DP0 pid=669050)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 3658, in load_model
(EngineCore_DP0 pid=669050)     raise e
(EngineCore_DP0 pid=669050)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 3581, in load_model
(EngineCore_DP0 pid=669050)     self.model = model_loader.load_model(
(EngineCore_DP0 pid=669050)                  ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=669050)   File "/root/vllmbench/vllm/model_executor/model_loader/base_loader.py", line 49, in load_model
(EngineCore_DP0 pid=669050)     model = initialize_model(
(EngineCore_DP0 pid=669050)             ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=669050)   File "/root/vllmbench/vllm/model_executor/model_loader/utils.py", line 48, in initialize_model
(EngineCore_DP0 pid=669050)     return model_class(vllm_config=vllm_config, prefix=prefix)
(EngineCore_DP0 pid=669050)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=669050)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 551, in __init__
(EngineCore_DP0 pid=669050)     self.lm_head = ParallelLMHead(
(EngineCore_DP0 pid=669050)                    ^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=669050)   File "/root/vllmbench/vllm/model_executor/layers/vocab_parallel_embedding.py", line 523, in __init__
(EngineCore_DP0 pid=669050)     super().__init__(
(EngineCore_DP0 pid=669050)   File "/root/vllmbench/vllm/model_executor/layers/vocab_parallel_embedding.py", line 301, in __init__
(EngineCore_DP0 pid=669050)     self.quant_method.create_weights(
(EngineCore_DP0 pid=669050)   File "/root/vllmbench/vllm/model_executor/layers/vocab_parallel_embedding.py", line 46, in create_weights
(EngineCore_DP0 pid=669050)     torch.empty(
(EngineCore_DP0 pid=669050)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_device.py", line 103, in __torch_function__
(EngineCore_DP0 pid=669050)     return func(*args, **kwargs)
(EngineCore_DP0 pid=669050)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=669050) torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.45 GiB. GPU 0 has a total capacity of 15.46 GiB of which 948.94 MiB is free. Including non-PyTorch memory, this process has 14.31 GiB memory in use. Of the allocated memory 13.94 GiB is allocated by PyTorch, and 8.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W125 23:20:53.891310450 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=512
