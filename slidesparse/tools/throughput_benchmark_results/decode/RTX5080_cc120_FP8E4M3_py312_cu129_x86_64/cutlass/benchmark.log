
========== M=16 ==========
Time: 2026-01-25 18:08:16
Backend: CUTLASS (SlideSparse fallback)
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-FP8
Params: prompt_len=16, output_len=256, num_prompts=16, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-FP8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 16 --max-num-seqs 16 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cutlass/json/Llama3.2-1B-FP8_M16.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 18:08:20 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
Throughput: 12.62 requests/s, 3432.54 total tokens/s, 3230.62 output tokens/s
Total num prompt tokens:  256
Total num output tokens:  4096

STDERR:
[2026-01-25 18:08:20] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 18:08:20] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B
[2026-01-25 18:08:20] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B
[2026-01-25 18:08:20] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B
[2026-01-25 18:08:20] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B
[2026-01-25 18:08:20] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B
[2026-01-25 18:08:20] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B
[2026-01-25 18:08:20] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B
[2026-01-25 18:08:20] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B
[2026-01-25 18:08:20] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-25 18:08:20] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-25 18:08:20] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-25 18:08:20] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-25 18:08:20] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B
[2026-01-25 18:08:20] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B
[2026-01-25 18:08:20] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B
[2026-01-25 18:08:20] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B
[2026-01-25 18:08:20] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-25 18:08:20] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-25 18:08:20] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-25 18:08:20] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-25 18:08:20] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-25 18:08:20] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-25 18:08:20] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-25 18:08:20] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-25 18:08:20] INFO kernels.py:719: Preloaded 20 Triton kernels from RTX5080_cc120_py312_cu129_x86_64
[2026-01-25 18:08:20] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=8, cuSPARSELt=8 models
[2026-01-25 18:08:20] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:08:20] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:08:20] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:08:20] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 18:08:24] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 18:08:24] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B
[2026-01-25 18:08:24] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B
[2026-01-25 18:08:24] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B
[2026-01-25 18:08:24] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B
[2026-01-25 18:08:24] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B
[2026-01-25 18:08:24] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B
[2026-01-25 18:08:24] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B
[2026-01-25 18:08:24] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B
[2026-01-25 18:08:24] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-25 18:08:24] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-25 18:08:24] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-25 18:08:24] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-25 18:08:24] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B
[2026-01-25 18:08:24] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B
[2026-01-25 18:08:24] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B
[2026-01-25 18:08:24] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B
[2026-01-25 18:08:24] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-25 18:08:24] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-25 18:08:24] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-25 18:08:24] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-25 18:08:24] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-25 18:08:24] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-25 18:08:24] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-25 18:08:24] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-25 18:08:24] INFO kernels.py:719: Preloaded 20 Triton kernels from RTX5080_cc120_py312_cu129_x86_64
[2026-01-25 18:08:24] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=8, cuSPARSELt=8 models
[2026-01-25 18:08:24] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:08:24] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:08:24] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:08:24] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=273091) [2026-01-25 18:08:25] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (CUTLASS)
(EngineCore_DP0 pid=273091) [2026-01-25 18:08:25] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=CUTLASS)
(EngineCore_DP0 pid=273091) [2026-01-25 18:08:25] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=CUTLASS
(EngineCore_DP0 pid=273091) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=273091) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.59it/s]
(EngineCore_DP0 pid=273091) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.59it/s]
(EngineCore_DP0 pid=273091) 
(EngineCore_DP0 pid=273091) 2026-01-25 18:08:32,964 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=273091) 2026-01-25 18:08:32,967 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=273091) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 3/7 [00:00<00:00, 22.73it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 31.48it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 29.98it/s]
(EngineCore_DP0 pid=273091) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 1/5 [00:00<00:00,  7.85it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 31.76it/s]

Adding requests:   0%|          | 0/16 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 16/16 [00:00<00:00, 1702.54it/s]

Processed prompts:   0%|          | 0/16 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   6%|▋         | 1/16 [00:01<00:18,  1.25s/it, est. speed input: 12.79 toks/s, output: 204.64 toks/s]
Processed prompts: 100%|██████████| 16/16 [00:01<00:00,  1.25s/it, est. speed input: 203.82 toks/s, output: 3261.17 toks/s]
Processed prompts: 100%|██████████| 16/16 [00:01<00:00, 12.74it/s, est. speed input: 203.82 toks/s, output: 3261.17 toks/s]
[rank0]:[W125 18:08:35.536555993 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-25 18:08:36
Backend: CUTLASS (SlideSparse fallback)
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-FP8
Params: prompt_len=16, output_len=256, num_prompts=128, max_num_seqs=128
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-FP8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 128 --max-num-seqs 128 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cutlass/json/Llama3.2-1B-FP8_M128.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 18:08:40 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
Throughput: 65.42 requests/s, 17794.02 total tokens/s, 16747.32 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768

STDERR:
[2026-01-25 18:08:40] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 18:08:40] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B
[2026-01-25 18:08:40] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B
[2026-01-25 18:08:40] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B
[2026-01-25 18:08:40] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B
[2026-01-25 18:08:40] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B
[2026-01-25 18:08:40] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B
[2026-01-25 18:08:40] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B
[2026-01-25 18:08:40] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B
[2026-01-25 18:08:40] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-25 18:08:40] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-25 18:08:40] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-25 18:08:40] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-25 18:08:40] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B
[2026-01-25 18:08:40] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B
[2026-01-25 18:08:40] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B
[2026-01-25 18:08:40] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B
[2026-01-25 18:08:40] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-25 18:08:40] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-25 18:08:40] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-25 18:08:40] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-25 18:08:40] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-25 18:08:40] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-25 18:08:40] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-25 18:08:40] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-25 18:08:40] INFO kernels.py:719: Preloaded 20 Triton kernels from RTX5080_cc120_py312_cu129_x86_64
[2026-01-25 18:08:40] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=8, cuSPARSELt=8 models
[2026-01-25 18:08:40] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:08:40] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:08:40] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:08:40] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 18:08:44] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 18:08:44] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B
[2026-01-25 18:08:44] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B
[2026-01-25 18:08:44] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B
[2026-01-25 18:08:44] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B
[2026-01-25 18:08:44] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B
[2026-01-25 18:08:44] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B
[2026-01-25 18:08:44] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B
[2026-01-25 18:08:44] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B
[2026-01-25 18:08:44] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-25 18:08:44] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-25 18:08:44] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-25 18:08:44] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-25 18:08:44] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B
[2026-01-25 18:08:44] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B
[2026-01-25 18:08:44] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B
[2026-01-25 18:08:44] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B
[2026-01-25 18:08:44] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-25 18:08:44] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-25 18:08:44] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-25 18:08:44] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-25 18:08:44] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-25 18:08:44] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-25 18:08:44] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-25 18:08:44] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-25 18:08:44] INFO kernels.py:719: Preloaded 20 Triton kernels from RTX5080_cc120_py312_cu129_x86_64
[2026-01-25 18:08:44] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=8, cuSPARSELt=8 models
[2026-01-25 18:08:44] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:08:44] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:08:44] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:08:44] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=273643) [2026-01-25 18:08:45] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (CUTLASS)
(EngineCore_DP0 pid=273643) [2026-01-25 18:08:45] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=CUTLASS)
(EngineCore_DP0 pid=273643) [2026-01-25 18:08:45] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=CUTLASS
(EngineCore_DP0 pid=273643) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=273643) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.54it/s]
(EngineCore_DP0 pid=273643) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.54it/s]
(EngineCore_DP0 pid=273643) 
(EngineCore_DP0 pid=273643) 2026-01-25 18:08:51,259 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=273643) 2026-01-25 18:08:51,262 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=273643) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 5/35 [00:00<00:00, 48.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▊     | 17/35 [00:00<00:00, 86.58it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 30/35 [00:00<00:00, 102.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:00<00:00, 95.67it/s] 
(EngineCore_DP0 pid=273643) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   5%|▌         | 1/19 [00:00<00:02,  8.00it/s]
Capturing CUDA graphs (decode, FULL):  74%|███████▎  | 14/19 [00:00<00:00, 72.22it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:00<00:00, 72.29it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 4455.21it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:01<03:56,  1.86s/it, est. speed input: 8.60 toks/s, output: 137.66 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00,  1.86s/it, est. speed input: 1063.20 toks/s, output: 17011.13 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 66.45it/s, est. speed input: 1063.20 toks/s, output: 17011.13 toks/s]
[rank0]:[W125 18:08:54.684513518 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-25 18:08:56
Backend: CUTLASS (SlideSparse fallback)
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-FP8
Params: prompt_len=16, output_len=256, num_prompts=256, max_num_seqs=256
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-FP8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 256 --max-num-seqs 256 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cutlass/json/Llama3.2-1B-FP8_M256.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 18:08:59 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
Throughput: 74.57 requests/s, 20282.71 total tokens/s, 19089.61 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536

STDERR:
[2026-01-25 18:08:59] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 18:08:59] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B
[2026-01-25 18:08:59] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B
[2026-01-25 18:08:59] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B
[2026-01-25 18:08:59] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B
[2026-01-25 18:08:59] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B
[2026-01-25 18:08:59] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B
[2026-01-25 18:08:59] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B
[2026-01-25 18:08:59] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B
[2026-01-25 18:08:59] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-25 18:08:59] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-25 18:08:59] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-25 18:08:59] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-25 18:08:59] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B
[2026-01-25 18:08:59] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B
[2026-01-25 18:08:59] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B
[2026-01-25 18:08:59] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B
[2026-01-25 18:08:59] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-25 18:08:59] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-25 18:08:59] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-25 18:08:59] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-25 18:08:59] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-25 18:08:59] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-25 18:08:59] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-25 18:08:59] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-25 18:08:59] INFO kernels.py:719: Preloaded 20 Triton kernels from RTX5080_cc120_py312_cu129_x86_64
[2026-01-25 18:08:59] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=8, cuSPARSELt=8 models
[2026-01-25 18:08:59] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:08:59] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:08:59] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:08:59] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 18:09:03] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 18:09:03] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B
[2026-01-25 18:09:03] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B
[2026-01-25 18:09:03] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B
[2026-01-25 18:09:03] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B
[2026-01-25 18:09:03] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B
[2026-01-25 18:09:03] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B
[2026-01-25 18:09:03] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B
[2026-01-25 18:09:03] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B
[2026-01-25 18:09:03] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-25 18:09:03] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-25 18:09:03] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-25 18:09:03] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-25 18:09:03] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B
[2026-01-25 18:09:03] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B
[2026-01-25 18:09:03] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B
[2026-01-25 18:09:03] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B
[2026-01-25 18:09:03] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-25 18:09:03] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-25 18:09:03] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-25 18:09:03] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-25 18:09:03] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-25 18:09:03] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-25 18:09:03] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-25 18:09:03] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-25 18:09:03] INFO kernels.py:719: Preloaded 20 Triton kernels from RTX5080_cc120_py312_cu129_x86_64
[2026-01-25 18:09:03] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=8, cuSPARSELt=8 models
[2026-01-25 18:09:03] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 18:09:03] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 18:09:03] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 18:09:03] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=274176) [2026-01-25 18:09:04] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (CUTLASS)
(EngineCore_DP0 pid=274176) [2026-01-25 18:09:04] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=CUTLASS)
(EngineCore_DP0 pid=274176) [2026-01-25 18:09:04] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=CUTLASS
(EngineCore_DP0 pid=274176) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=274176) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.95it/s]
(EngineCore_DP0 pid=274176) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.94it/s]
(EngineCore_DP0 pid=274176) 
(EngineCore_DP0 pid=274176) 2026-01-25 18:09:10,286 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=274176) 2026-01-25 18:09:10,289 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=274176) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/36 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███       | 11/36 [00:00<00:00, 108.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▍   | 23/36 [00:00<00:00, 112.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:00<00:00, 112.97it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:00<00:00, 112.42it/s]
(EngineCore_DP0 pid=274176) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   3%|▎         | 1/35 [00:00<00:04,  7.45it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 10/35 [00:00<00:00, 49.49it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 22/35 [00:00<00:00, 77.12it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:00<00:00, 79.56it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 7563.69it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:03<13:00,  3.06s/it, est. speed input: 5.23 toks/s, output: 83.66 toks/s]
Processed prompts:  46%|████▌     | 118/256 [00:03<00:02, 52.33it/s, est. speed input: 596.12 toks/s, output: 9537.98 toks/s]
Processed prompts:  83%|████████▎ | 213/256 [00:03<00:00, 105.73it/s, est. speed input: 1042.04 toks/s, output: 16672.60 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 105.73it/s, est. speed input: 1205.35 toks/s, output: 19285.64 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 75.33it/s, est. speed input: 1205.35 toks/s, output: 19285.64 toks/s] 
[rank0]:[W125 18:09:15.317799704 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

