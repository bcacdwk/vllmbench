
========== M=16 ==========
Time: 2026-01-26 13:11:33
Backend: CUTLASS (SlideSparse fallback)
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-FP8
Params: prompt_len=16, output_len=256, num_prompts=16, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-FP8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 16 --max-num-seqs 16 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/B200_cc100_FP8E4M3_py312_cu129_x86_64/cutlass/json/Llama3.2-1B-FP8_M16.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:11:40 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=464185) WARNING 01-26 13:12:00 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 33.24 requests/s, 9042.25 total tokens/s, 8510.35 output tokens/s
Total num prompt tokens:  256
Total num output tokens:  4096

STDERR:
[2026-01-26 13:11:39] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:11:40] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B
[2026-01-26 13:11:40] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B
[2026-01-26 13:11:40] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B
[2026-01-26 13:11:40] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B
[2026-01-26 13:11:40] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B
[2026-01-26 13:11:40] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B
[2026-01-26 13:11:40] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B
[2026-01-26 13:11:40] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B
[2026-01-26 13:11:40] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B
[2026-01-26 13:11:40] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B
[2026-01-26 13:11:40] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B
[2026-01-26 13:11:40] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B
[2026-01-26 13:11:40] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-26 13:11:40] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-26 13:11:40] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-26 13:11:40] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-26 13:11:40] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-26 13:11:40] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-26 13:11:40] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-26 13:11:40] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-26 13:11:40] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-26 13:11:40] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-26 13:11:40] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-26 13:11:40] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-26 13:11:40] INFO kernels.py:719: Preloaded 20 Triton kernels from B200_cc100_py312_cu129_x86_64
[2026-01-26 13:11:40] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=8, cuSPARSELt=8 models
[2026-01-26 13:11:40] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:11:40] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:11:40] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:11:40] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:11:46] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:11:46] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B
[2026-01-26 13:11:46] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B
[2026-01-26 13:11:46] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B
[2026-01-26 13:11:46] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B
[2026-01-26 13:11:46] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B
[2026-01-26 13:11:46] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B
[2026-01-26 13:11:46] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B
[2026-01-26 13:11:46] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B
[2026-01-26 13:11:46] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B
[2026-01-26 13:11:46] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B
[2026-01-26 13:11:46] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B
[2026-01-26 13:11:46] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B
[2026-01-26 13:11:46] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-26 13:11:46] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-26 13:11:46] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-26 13:11:46] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-26 13:11:46] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-26 13:11:46] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-26 13:11:46] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-26 13:11:46] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-26 13:11:46] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-26 13:11:46] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-26 13:11:46] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-26 13:11:46] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-26 13:11:46] INFO kernels.py:719: Preloaded 20 Triton kernels from B200_cc100_py312_cu129_x86_64
[2026-01-26 13:11:46] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=8, cuSPARSELt=8 models
[2026-01-26 13:11:46] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:11:46] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:11:46] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:11:46] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=464185) [2026-01-26 13:11:47] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (CUTLASS)
(EngineCore_DP0 pid=464185) [2026-01-26 13:11:47] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=CUTLASS)
(EngineCore_DP0 pid=464185) [2026-01-26 13:11:47] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=CUTLASS
(EngineCore_DP0 pid=464185) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=464185) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.49it/s]
(EngineCore_DP0 pid=464185) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.49it/s]
(EngineCore_DP0 pid=464185) 
(EngineCore_DP0 pid=464185) 2026-01-26 13:12:00,484 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=464185) 2026-01-26 13:12:00,489 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=464185) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 71.98it/s]
(EngineCore_DP0 pid=464185) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 75.46it/s]

Adding requests:   0%|          | 0/16 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 16/16 [00:00<00:00, 3917.16it/s]

Processed prompts:   0%|          | 0/16 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   6%|▋         | 1/16 [00:00<00:07,  2.11it/s, est. speed input: 33.76 toks/s, output: 540.17 toks/s]
Processed prompts: 100%|██████████| 16/16 [00:00<00:00,  2.11it/s, est. speed input: 537.59 toks/s, output: 8601.26 toks/s]
Processed prompts: 100%|██████████| 16/16 [00:00<00:00, 33.59it/s, est. speed input: 537.59 toks/s, output: 8601.26 toks/s]
[rank0]:[W126 13:12:02.183356140 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-26 13:12:04
Backend: CUTLASS (SlideSparse fallback)
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-FP8
Params: prompt_len=16, output_len=256, num_prompts=128, max_num_seqs=128
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-FP8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 128 --max-num-seqs 128 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/B200_cc100_FP8E4M3_py312_cu129_x86_64/cutlass/json/Llama3.2-1B-FP8_M128.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:12:11 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=465165) WARNING 01-26 13:12:29 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 140.55 requests/s, 38228.92 total tokens/s, 35980.16 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768

STDERR:
[2026-01-26 13:12:11] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:12:11] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B
[2026-01-26 13:12:11] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B
[2026-01-26 13:12:11] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B
[2026-01-26 13:12:11] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B
[2026-01-26 13:12:11] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B
[2026-01-26 13:12:11] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B
[2026-01-26 13:12:11] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B
[2026-01-26 13:12:11] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B
[2026-01-26 13:12:11] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B
[2026-01-26 13:12:11] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B
[2026-01-26 13:12:11] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B
[2026-01-26 13:12:11] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B
[2026-01-26 13:12:11] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-26 13:12:11] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-26 13:12:11] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-26 13:12:11] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-26 13:12:11] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-26 13:12:11] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-26 13:12:11] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-26 13:12:11] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-26 13:12:11] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-26 13:12:11] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-26 13:12:11] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-26 13:12:11] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-26 13:12:11] INFO kernels.py:719: Preloaded 20 Triton kernels from B200_cc100_py312_cu129_x86_64
[2026-01-26 13:12:11] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=8, cuSPARSELt=8 models
[2026-01-26 13:12:11] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:12:11] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:12:11] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:12:11] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:12:18] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:12:18] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B
[2026-01-26 13:12:18] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B
[2026-01-26 13:12:18] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B
[2026-01-26 13:12:18] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B
[2026-01-26 13:12:18] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B
[2026-01-26 13:12:18] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B
[2026-01-26 13:12:18] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B
[2026-01-26 13:12:18] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B
[2026-01-26 13:12:18] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B
[2026-01-26 13:12:18] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B
[2026-01-26 13:12:18] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B
[2026-01-26 13:12:18] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B
[2026-01-26 13:12:18] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-26 13:12:18] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-26 13:12:18] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-26 13:12:18] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-26 13:12:18] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-26 13:12:18] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-26 13:12:18] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-26 13:12:18] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-26 13:12:18] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-26 13:12:18] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-26 13:12:18] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-26 13:12:18] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-26 13:12:18] INFO kernels.py:719: Preloaded 20 Triton kernels from B200_cc100_py312_cu129_x86_64
[2026-01-26 13:12:18] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=8, cuSPARSELt=8 models
[2026-01-26 13:12:18] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:12:18] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:12:18] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:12:18] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=465165) [2026-01-26 13:12:19] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (CUTLASS)
(EngineCore_DP0 pid=465165) [2026-01-26 13:12:19] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=CUTLASS)
(EngineCore_DP0 pid=465165) [2026-01-26 13:12:19] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=CUTLASS
(EngineCore_DP0 pid=465165) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=465165) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.59it/s]
(EngineCore_DP0 pid=465165) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.59it/s]
(EngineCore_DP0 pid=465165) 
(EngineCore_DP0 pid=465165) 2026-01-26 13:12:29,355 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=465165) 2026-01-26 13:12:29,360 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=465165) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▌       | 9/35 [00:00<00:00, 85.60it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████▏    | 18/35 [00:00<00:00, 86.41it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  77%|███████▋  | 27/35 [00:00<00:00, 87.44it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:00<00:00, 83.58it/s]
(EngineCore_DP0 pid=465165) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  21%|██        | 4/19 [00:00<00:00, 26.82it/s]
Capturing CUDA graphs (decode, FULL):  58%|█████▊    | 11/19 [00:00<00:00, 44.40it/s]
Capturing CUDA graphs (decode, FULL):  95%|█████████▍| 18/19 [00:00<00:00, 52.54it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:00<00:00, 48.99it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 5182.70it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<01:48,  1.17it/s, est. speed input: 18.73 toks/s, output: 299.69 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00,  1.17it/s, est. speed input: 2314.23 toks/s, output: 37027.48 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 144.62it/s, est. speed input: 2314.23 toks/s, output: 37027.48 toks/s]
[rank0]:[W126 13:12:32.947352471 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-26 13:12:34
Backend: CUTLASS (SlideSparse fallback)
Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-FP8
Params: prompt_len=16, output_len=256, num_prompts=256, max_num_seqs=256
Command: vllm bench throughput --model /root/vllmbench/checkpoints/Llama3.2-1B-FP8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 256 --max-num-seqs 256 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/B200_cc100_FP8E4M3_py312_cu129_x86_64/cutlass/json/Llama3.2-1B-FP8_M256.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:12:41 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=466130) WARNING 01-26 13:12:59 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 180.09 requests/s, 48985.03 total tokens/s, 46103.56 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536

STDERR:
[2026-01-26 13:12:40] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:12:41] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B
[2026-01-26 13:12:41] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B
[2026-01-26 13:12:41] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B
[2026-01-26 13:12:41] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B
[2026-01-26 13:12:41] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B
[2026-01-26 13:12:41] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B
[2026-01-26 13:12:41] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B
[2026-01-26 13:12:41] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B
[2026-01-26 13:12:41] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B
[2026-01-26 13:12:41] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B
[2026-01-26 13:12:41] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B
[2026-01-26 13:12:41] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B
[2026-01-26 13:12:41] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-26 13:12:41] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-26 13:12:41] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-26 13:12:41] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-26 13:12:41] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-26 13:12:41] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-26 13:12:41] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-26 13:12:41] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-26 13:12:41] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-26 13:12:41] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-26 13:12:41] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-26 13:12:41] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-26 13:12:41] INFO kernels.py:719: Preloaded 20 Triton kernels from B200_cc100_py312_cu129_x86_64
[2026-01-26 13:12:41] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=8, cuSPARSELt=8 models
[2026-01-26 13:12:41] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:12:41] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:12:41] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:12:41] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:12:47] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:12:48] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B
[2026-01-26 13:12:48] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B
[2026-01-26 13:12:48] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B
[2026-01-26 13:12:48] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B
[2026-01-26 13:12:48] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B
[2026-01-26 13:12:48] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B
[2026-01-26 13:12:48] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B
[2026-01-26 13:12:48] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B
[2026-01-26 13:12:48] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B
[2026-01-26 13:12:48] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B
[2026-01-26 13:12:48] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B
[2026-01-26 13:12:48] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B
[2026-01-26 13:12:48] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-26 13:12:48] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B
[2026-01-26 13:12:48] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-26 13:12:48] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B
[2026-01-26 13:12:48] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-26 13:12:48] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B
[2026-01-26 13:12:48] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-26 13:12:48] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B
[2026-01-26 13:12:48] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-26 13:12:48] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B
[2026-01-26 13:12:48] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-26 13:12:48] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B
[2026-01-26 13:12:48] INFO kernels.py:719: Preloaded 20 Triton kernels from B200_cc100_py312_cu129_x86_64
[2026-01-26 13:12:48] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=8, cuSPARSELt=8 models
[2026-01-26 13:12:48] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:12:48] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:12:48] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:12:48] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=466130) [2026-01-26 13:12:49] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (CUTLASS)
(EngineCore_DP0 pid=466130) [2026-01-26 13:12:49] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=CUTLASS)
(EngineCore_DP0 pid=466130) [2026-01-26 13:12:49] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=CUTLASS
(EngineCore_DP0 pid=466130) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=466130) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.46it/s]
(EngineCore_DP0 pid=466130) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.46it/s]
(EngineCore_DP0 pid=466130) 
(EngineCore_DP0 pid=466130) 2026-01-26 13:12:59,167 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=466130) 2026-01-26 13:12:59,172 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=466130) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/36 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 5/36 [00:00<00:00, 47.22it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███       | 11/36 [00:00<00:00, 51.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 17/36 [00:00<00:00, 54.58it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▉   | 25/36 [00:00<00:00, 64.22it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 34/36 [00:00<00:00, 73.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:00<00:00, 63.86it/s]
(EngineCore_DP0 pid=466130) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 7/35 [00:00<00:00, 66.02it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 14/35 [00:00<00:00, 60.11it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 22/35 [00:00<00:00, 65.48it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 30/35 [00:00<00:00, 69.31it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:00<00:00, 68.19it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 4952.02it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:01<05:11,  1.22s/it, est. speed input: 13.08 toks/s, output: 209.28 toks/s]
Processed prompts:  88%|████████▊ | 225/256 [00:01<00:00, 234.55it/s, est. speed input: 2717.03 toks/s, output: 43472.36 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:01<00:00, 234.55it/s, est. speed input: 2992.88 toks/s, output: 47885.91 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:01<00:00, 187.04it/s, est. speed input: 2992.88 toks/s, output: 47885.91 toks/s]
[rank0]:[W126 13:13:03.666727510 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

