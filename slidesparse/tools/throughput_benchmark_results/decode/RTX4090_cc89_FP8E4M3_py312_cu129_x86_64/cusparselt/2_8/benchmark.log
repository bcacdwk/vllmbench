
========== M=64 ==========
Time: 2026-01-26 09:12:30
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_8
Params: prompt_len=16, output_len=256, num_prompts=64, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 64 --max-num-seqs 64 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Llama3.2-1B-FP8_M64.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 09:12:37 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 09:12:38 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=446926) WARNING 01-26 09:12:46 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=446926) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=446926) WARNING 01-26 09:12:55 [backends.py:609] Failed to read file <frozen os>
Throughput: 31.37 requests/s, 8533.12 total tokens/s, 8031.17 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384

STDERR:
[2026-01-26 09:12:36] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 09:12:37] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 09:12:37] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 09:12:37] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 09:12:37] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 09:12:37] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 09:12:37] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 09:12:37] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 09:12:37] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 09:12:37] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 09:12:37] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 09:12:37] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 09:12:37] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 09:12:37] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 09:12:45] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 09:12:45] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 09:12:45] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 09:12:45] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 09:12:45] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 09:12:45] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 09:12:45] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 09:12:45] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 09:12:45] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 09:12:45] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 09:12:45] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 09:12:45] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 09:12:45] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 09:12:45] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=446926) [2026-01-26 09:12:47] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=446926) [2026-01-26 09:12:47] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=446926) [2026-01-26 09:12:47] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=446926) [2026-01-26 09:12:47] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=446926) [2026-01-26 09:12:47] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=446926) [2026-01-26 09:12:47] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=446926) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=446926) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.59s/it]
(EngineCore_DP0 pid=446926) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.59s/it]
(EngineCore_DP0 pid=446926) 
(EngineCore_DP0 pid=446926) [2026-01-26 09:12:49] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=446926) [2026-01-26 09:12:49] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 7077888 bytes
(EngineCore_DP0 pid=446926) [2026-01-26 09:12:49] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 3072] -> 1D uint8
(EngineCore_DP0 pid=446926) [2026-01-26 09:12:49] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4718592 bytes
(EngineCore_DP0 pid=446926) [2026-01-26 09:12:49] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=446926) [2026-01-26 09:12:49] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 37748736 bytes
(EngineCore_DP0 pid=446926) [2026-01-26 09:12:49] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 12288] -> 1D uint8
(EngineCore_DP0 pid=446926) [2026-01-26 09:12:49] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 18874368 bytes
(EngineCore_DP0 pid=446926) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   5%|▌         | 1/19 [00:00<00:02,  7.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 2/19 [00:00<00:02,  8.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 3/19 [00:00<00:02,  7.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  21%|██        | 4/19 [00:00<00:04,  3.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▋       | 5/19 [00:01<00:03,  4.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  32%|███▏      | 6/19 [00:01<00:04,  3.17it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 7/19 [00:01<00:03,  3.74it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 9/19 [00:01<00:01,  5.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 10/19 [00:02<00:01,  5.93it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 11/19 [00:02<00:01,  6.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  68%|██████▊   | 13/19 [00:02<00:00,  7.81it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  79%|███████▉  | 15/19 [00:02<00:00,  8.54it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 17/19 [00:02<00:00,  8.99it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:02<00:00,  8.93it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:02<00:00,  6.48it/s]
(EngineCore_DP0 pid=446926) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   9%|▉         | 1/11 [00:00<00:01,  8.33it/s]
Capturing CUDA graphs (decode, FULL):  27%|██▋       | 3/11 [00:00<00:00,  9.65it/s]
Capturing CUDA graphs (decode, FULL):  45%|████▌     | 5/11 [00:00<00:00,  9.71it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 6/11 [00:00<00:00,  8.48it/s]
Capturing CUDA graphs (decode, FULL):  64%|██████▎   | 7/11 [00:00<00:00,  5.99it/s]
Capturing CUDA graphs (decode, FULL):  82%|████████▏ | 9/11 [00:01<00:00,  7.33it/s]
Capturing CUDA graphs (decode, FULL):  91%|█████████ | 10/11 [00:01<00:00,  7.28it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:01<00:00,  7.84it/s]

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 64/64 [00:00<00:00, 2704.88it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 1/64 [00:01<02:05,  1.99s/it, est. speed input: 8.05 toks/s, output: 128.84 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:02<00:00,  1.99s/it, est. speed input: 508.25 toks/s, output: 8131.96 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:02<00:00, 31.76it/s, est. speed input: 508.25 toks/s, output: 8131.96 toks/s]
[rank0]:[W126 09:13:13.342235433 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-26 09:13:16
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_8
Params: prompt_len=16, output_len=256, num_prompts=128, max_num_seqs=128
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 128 --max-num-seqs 128 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Llama3.2-1B-FP8_M128.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 09:13:24 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 09:13:26 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=447800) WARNING 01-26 09:13:34 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=447800) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=447800) WARNING 01-26 09:13:40 [backends.py:609] Failed to read file <frozen os>
Throughput: 49.20 requests/s, 13382.57 total tokens/s, 12595.36 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768

STDERR:
[2026-01-26 09:13:24] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 09:13:24] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 09:13:24] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 09:13:24] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 09:13:24] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 09:13:24] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 09:13:24] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 09:13:24] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 09:13:24] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 09:13:24] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 09:13:24] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 09:13:24] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 09:13:24] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 09:13:24] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 09:13:33] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 09:13:33] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 09:13:33] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 09:13:33] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 09:13:33] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 09:13:33] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 09:13:33] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 09:13:33] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 09:13:33] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 09:13:33] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 09:13:33] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 09:13:33] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 09:13:33] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 09:13:33] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=447800) [2026-01-26 09:13:35] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=447800) [2026-01-26 09:13:35] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=447800) [2026-01-26 09:13:35] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=447800) [2026-01-26 09:13:35] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=447800) [2026-01-26 09:13:35] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=447800) [2026-01-26 09:13:35] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=447800) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=447800) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.40it/s]
(EngineCore_DP0 pid=447800) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.40it/s]
(EngineCore_DP0 pid=447800) 
(EngineCore_DP0 pid=447800) [2026-01-26 09:13:35] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=447800) [2026-01-26 09:13:35] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 7077888 bytes
(EngineCore_DP0 pid=447800) [2026-01-26 09:13:35] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 3072] -> 1D uint8
(EngineCore_DP0 pid=447800) [2026-01-26 09:13:35] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4718592 bytes
(EngineCore_DP0 pid=447800) [2026-01-26 09:13:35] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=447800) [2026-01-26 09:13:35] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 37748736 bytes
(EngineCore_DP0 pid=447800) [2026-01-26 09:13:35] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 12288] -> 1D uint8
(EngineCore_DP0 pid=447800) [2026-01-26 09:13:35] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 18874368 bytes
(EngineCore_DP0 pid=447800) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|▎         | 1/35 [00:00<00:03,  8.80it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 2/35 [00:00<00:03,  8.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▊         | 3/35 [00:00<00:03,  8.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█▏        | 4/35 [00:00<00:03,  9.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 5/35 [00:00<00:03,  9.41it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 6/35 [00:00<00:03,  9.29it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 7/35 [00:00<00:02,  9.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  23%|██▎       | 8/35 [00:00<00:02,  9.54it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▌       | 9/35 [00:00<00:02,  9.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 11/35 [00:01<00:02,  9.82it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  34%|███▍      | 12/35 [00:01<00:02,  9.86it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 13/35 [00:01<00:02,  7.88it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 14/35 [00:01<00:03,  5.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 15/35 [00:02<00:03,  5.24it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  46%|████▌     | 16/35 [00:02<00:03,  5.80it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▊     | 17/35 [00:02<00:04,  3.71it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████▏    | 18/35 [00:02<00:03,  4.44it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  54%|█████▍    | 19/35 [00:02<00:03,  5.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 20/35 [00:02<00:02,  6.06it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 21/35 [00:03<00:02,  6.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  66%|██████▌   | 23/35 [00:03<00:01,  7.93it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 25/35 [00:03<00:01,  8.60it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▍  | 26/35 [00:03<00:01,  8.79it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  77%|███████▋  | 27/35 [00:03<00:00,  8.97it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 28/35 [00:03<00:00,  9.18it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  83%|████████▎ | 29/35 [00:03<00:00,  9.32it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▊ | 31/35 [00:04<00:00,  9.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 33/35 [00:04<00:00,  9.86it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  97%|█████████▋| 34/35 [00:04<00:00,  9.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:04<00:00,  7.97it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:04<00:00,  7.61it/s]
(EngineCore_DP0 pid=447800) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   5%|▌         | 1/19 [00:00<00:07,  2.35it/s]
Capturing CUDA graphs (decode, FULL):  16%|█▌        | 3/19 [00:00<00:03,  5.09it/s]
Capturing CUDA graphs (decode, FULL):  21%|██        | 4/19 [00:00<00:03,  4.15it/s]
Capturing CUDA graphs (decode, FULL):  26%|██▋       | 5/19 [00:01<00:03,  4.08it/s]
Capturing CUDA graphs (decode, FULL):  32%|███▏      | 6/19 [00:01<00:02,  5.02it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 7/19 [00:01<00:02,  5.94it/s]
Capturing CUDA graphs (decode, FULL):  42%|████▏     | 8/19 [00:01<00:01,  6.78it/s]
Capturing CUDA graphs (decode, FULL):  53%|█████▎    | 10/19 [00:01<00:01,  8.05it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 12/19 [00:01<00:00,  8.82it/s]
Capturing CUDA graphs (decode, FULL):  74%|███████▎  | 14/19 [00:02<00:00,  9.24it/s]
Capturing CUDA graphs (decode, FULL):  84%|████████▍ | 16/19 [00:02<00:00,  9.57it/s]
Capturing CUDA graphs (decode, FULL):  95%|█████████▍| 18/19 [00:02<00:00,  9.84it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:02<00:00,  7.28it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 2781.12it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:02<05:15,  2.48s/it, est. speed input: 6.45 toks/s, output: 103.17 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00,  2.48s/it, est. speed input: 801.96 toks/s, output: 12831.31 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 50.12it/s, est. speed input: 801.96 toks/s, output: 12831.31 toks/s]
[rank0]:[W126 09:14:01.919742327 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-26 09:14:04
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_8
Params: prompt_len=16, output_len=256, num_prompts=256, max_num_seqs=256
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 256 --max-num-seqs 256 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Llama3.2-1B-FP8_M256.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 09:14:11 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 09:14:12 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=448667) WARNING 01-26 09:14:20 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=448667) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=448667) WARNING 01-26 09:14:28 [backends.py:609] Failed to read file <frozen os>
Throughput: 73.05 requests/s, 19868.94 total tokens/s, 18700.18 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536

STDERR:
[2026-01-26 09:14:11] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 09:14:11] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 09:14:11] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 09:14:11] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 09:14:11] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 09:14:11] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 09:14:11] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 09:14:11] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 09:14:11] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 09:14:11] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 09:14:11] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 09:14:11] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 09:14:11] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 09:14:11] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 09:14:19] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 09:14:19] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 09:14:19] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 09:14:19] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 09:14:19] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 09:14:19] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 09:14:19] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 09:14:19] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 09:14:19] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 09:14:19] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 09:14:19] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 09:14:19] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 09:14:19] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 09:14:19] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=448667) [2026-01-26 09:14:20] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=448667) [2026-01-26 09:14:20] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=448667) [2026-01-26 09:14:20] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=448667) [2026-01-26 09:14:20] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=448667) [2026-01-26 09:14:20] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=448667) [2026-01-26 09:14:20] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=448667) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=448667) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.35it/s]
(EngineCore_DP0 pid=448667) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.35it/s]
(EngineCore_DP0 pid=448667) 
(EngineCore_DP0 pid=448667) [2026-01-26 09:14:21] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=448667) [2026-01-26 09:14:21] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 7077888 bytes
(EngineCore_DP0 pid=448667) [2026-01-26 09:14:21] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 3072] -> 1D uint8
(EngineCore_DP0 pid=448667) [2026-01-26 09:14:21] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4718592 bytes
(EngineCore_DP0 pid=448667) [2026-01-26 09:14:21] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=448667) [2026-01-26 09:14:21] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 37748736 bytes
(EngineCore_DP0 pid=448667) [2026-01-26 09:14:21] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 12288] -> 1D uint8
(EngineCore_DP0 pid=448667) [2026-01-26 09:14:21] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 18874368 bytes
(EngineCore_DP0 pid=448667) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/36 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|▎         | 1/36 [00:00<00:12,  2.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 2/36 [00:00<00:07,  4.81it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 17/36 [00:00<00:00, 44.68it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 24/36 [00:02<00:01,  9.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 28/36 [00:02<00:00,  9.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 31/36 [00:03<00:00,  9.25it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 34/36 [00:03<00:00,  9.19it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:03<00:00,  8.94it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:03<00:00,  9.92it/s]
(EngineCore_DP0 pid=448667) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   3%|▎         | 1/35 [00:00<00:04,  7.35it/s]
Capturing CUDA graphs (decode, FULL):   6%|▌         | 2/35 [00:00<00:05,  6.50it/s]
Capturing CUDA graphs (decode, FULL):   9%|▊         | 3/35 [00:00<00:08,  3.66it/s]
Capturing CUDA graphs (decode, FULL):  11%|█▏        | 4/35 [00:00<00:07,  4.35it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 5/35 [00:01<00:08,  3.42it/s]
Capturing CUDA graphs (decode, FULL):  17%|█▋        | 6/35 [00:01<00:08,  3.46it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 7/35 [00:01<00:06,  4.30it/s]
Capturing CUDA graphs (decode, FULL):  23%|██▎       | 8/35 [00:01<00:05,  5.10it/s]
Capturing CUDA graphs (decode, FULL):  26%|██▌       | 9/35 [00:01<00:04,  5.88it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 10/35 [00:02<00:03,  6.55it/s]
Capturing CUDA graphs (decode, FULL):  31%|███▏      | 11/35 [00:02<00:03,  7.21it/s]
Capturing CUDA graphs (decode, FULL):  34%|███▍      | 12/35 [00:02<00:02,  7.77it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 13/35 [00:02<00:02,  8.16it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 14/35 [00:02<00:02,  8.50it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 15/35 [00:02<00:02,  8.81it/s]
Capturing CUDA graphs (decode, FULL):  49%|████▊     | 17/35 [00:02<00:01,  9.37it/s]
Capturing CUDA graphs (decode, FULL):  51%|█████▏    | 18/35 [00:02<00:01,  9.44it/s]
Capturing CUDA graphs (decode, FULL):  54%|█████▍    | 19/35 [00:02<00:01,  9.57it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 20/35 [00:03<00:01,  9.67it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 21/35 [00:03<00:01,  8.76it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 22/35 [00:03<00:01,  8.04it/s]
Capturing CUDA graphs (decode, FULL):  66%|██████▌   | 23/35 [00:03<00:02,  4.29it/s]
Capturing CUDA graphs (decode, FULL):  69%|██████▊   | 24/35 [00:03<00:02,  5.08it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 25/35 [00:04<00:01,  5.23it/s]
Capturing CUDA graphs (decode, FULL):  74%|███████▍  | 26/35 [00:04<00:02,  4.01it/s]
Capturing CUDA graphs (decode, FULL):  77%|███████▋  | 27/35 [00:04<00:01,  4.42it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 28/35 [00:04<00:01,  5.28it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 30/35 [00:04<00:00,  6.80it/s]
Capturing CUDA graphs (decode, FULL):  91%|█████████▏| 32/35 [00:05<00:00,  7.87it/s]
Capturing CUDA graphs (decode, FULL):  97%|█████████▋| 34/35 [00:05<00:00,  8.66it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:05<00:00,  6.39it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 2892.35it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:03<13:01,  3.07s/it, est. speed input: 5.22 toks/s, output: 83.50 toks/s]
Processed prompts:  46%|████▌     | 118/256 [00:03<00:02, 52.20it/s, est. speed input: 594.74 toks/s, output: 9515.72 toks/s]
Processed prompts:  82%|████████▏ | 210/256 [00:03<00:00, 103.61it/s, est. speed input: 1024.63 toks/s, output: 16393.95 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 103.61it/s, est. speed input: 1199.63 toks/s, output: 19193.95 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 74.97it/s, est. speed input: 1199.63 toks/s, output: 19193.95 toks/s] 
[rank0]:[W126 09:14:52.501857765 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-26 09:14:55
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_8
Params: prompt_len=16, output_len=256, num_prompts=512, max_num_seqs=512
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 512 --max-num-seqs 512 --max-model-len 272 --max-num-batched-tokens 512 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Llama3.2-1B-FP8_M512.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 09:15:04 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 09:15:05 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=449596) WARNING 01-26 09:15:11 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=449596) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=449596) WARNING 01-26 09:15:19 [backends.py:609] Failed to read file <frozen os>
Throughput: 70.92 requests/s, 19289.06 total tokens/s, 18154.41 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072

STDERR:
[2026-01-26 09:15:03] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 09:15:04] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 09:15:04] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 09:15:04] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 09:15:04] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 09:15:04] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 09:15:04] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 09:15:04] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 09:15:04] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 09:15:04] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 09:15:04] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 09:15:04] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 09:15:04] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 09:15:04] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 09:15:11] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 09:15:11] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 09:15:11] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 09:15:11] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 09:15:11] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 09:15:11] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 09:15:11] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 09:15:11] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 09:15:11] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 09:15:11] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 09:15:11] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 09:15:11] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 09:15:11] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 09:15:11] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=449596) [2026-01-26 09:15:12] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=449596) [2026-01-26 09:15:12] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=449596) [2026-01-26 09:15:12] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=449596) [2026-01-26 09:15:12] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=449596) [2026-01-26 09:15:12] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=449596) [2026-01-26 09:15:12] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=449596) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=449596) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.50it/s]
(EngineCore_DP0 pid=449596) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.50it/s]
(EngineCore_DP0 pid=449596) 
(EngineCore_DP0 pid=449596) [2026-01-26 09:15:13] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=449596) [2026-01-26 09:15:13] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 7077888 bytes
(EngineCore_DP0 pid=449596) [2026-01-26 09:15:13] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 3072] -> 1D uint8
(EngineCore_DP0 pid=449596) [2026-01-26 09:15:13] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4718592 bytes
(EngineCore_DP0 pid=449596) [2026-01-26 09:15:13] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=449596) [2026-01-26 09:15:13] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 37748736 bytes
(EngineCore_DP0 pid=449596) [2026-01-26 09:15:13] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 12288] -> 1D uint8
(EngineCore_DP0 pid=449596) [2026-01-26 09:15:13] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 18874368 bytes
(EngineCore_DP0 pid=449596) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|▏         | 1/51 [00:00<00:05,  9.91it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|▍         | 2/51 [00:00<00:05,  9.63it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 4/51 [00:00<00:04,  9.92it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|█▏        | 6/51 [00:00<00:04, 10.08it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 8/51 [00:00<00:04, 10.10it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|█▉        | 10/51 [00:00<00:04, 10.02it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 11/51 [00:01<00:04,  9.96it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|██▎       | 12/51 [00:01<00:03,  9.93it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 13/51 [00:01<00:03,  9.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 14/51 [00:01<00:04,  8.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▉       | 15/51 [00:01<00:07,  4.82it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 16/51 [00:02<00:06,  5.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 17/51 [00:02<00:10,  3.24it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  35%|███▌      | 18/51 [00:02<00:08,  3.94it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 19/51 [00:02<00:06,  4.74it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|███▉      | 20/51 [00:02<00:05,  5.60it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|████      | 21/51 [00:03<00:04,  6.42it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 23/51 [00:03<00:03,  7.67it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 24/51 [00:03<00:03,  8.11it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▉     | 25/51 [00:03<00:03,  8.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████     | 26/51 [00:03<00:02,  8.82it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 27/51 [00:03<00:02,  9.03it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 29/51 [00:03<00:02,  9.42it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  59%|█████▉    | 30/51 [00:04<00:02,  9.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|██████    | 31/51 [00:04<00:02,  9.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 32/51 [00:04<00:02,  9.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  65%|██████▍   | 33/51 [00:04<00:01,  9.03it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 34/51 [00:04<00:01,  8.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 35/51 [00:04<00:02,  5.74it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████   | 36/51 [00:05<00:03,  4.24it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 37/51 [00:05<00:02,  4.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▍  | 38/51 [00:05<00:03,  3.35it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|███████▋  | 39/51 [00:05<00:02,  4.11it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 40/51 [00:06<00:02,  4.92it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 41/51 [00:06<00:01,  5.66it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 42/51 [00:06<00:01,  6.34it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 43/51 [00:06<00:01,  6.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▋ | 44/51 [00:06<00:00,  7.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  88%|████████▊ | 45/51 [00:06<00:00,  7.92it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|█████████ | 46/51 [00:06<00:00,  8.32it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 47/51 [00:06<00:00,  8.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 48/51 [00:06<00:00,  8.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  96%|█████████▌| 49/51 [00:07<00:00,  8.94it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  98%|█████████▊| 50/51 [00:07<00:00,  9.15it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:07<00:00,  8.32it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:07<00:00,  6.99it/s]
(EngineCore_DP0 pid=449596) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   2%|▏         | 1/51 [00:00<00:08,  5.66it/s]
Capturing CUDA graphs (decode, FULL):   4%|▍         | 2/51 [00:00<00:08,  5.56it/s]
Capturing CUDA graphs (decode, FULL):   6%|▌         | 3/51 [00:00<00:08,  5.72it/s]
Capturing CUDA graphs (decode, FULL):   8%|▊         | 4/51 [00:00<00:11,  3.98it/s]
Capturing CUDA graphs (decode, FULL):  27%|██▋       | 14/51 [00:01<00:01, 21.05it/s]
Capturing CUDA graphs (decode, FULL):  35%|███▌      | 18/51 [00:01<00:02, 14.58it/s]
Capturing CUDA graphs (decode, FULL):  41%|████      | 21/51 [00:01<00:02, 12.99it/s]
Capturing CUDA graphs (decode, FULL):  47%|████▋     | 24/51 [00:02<00:02, 11.29it/s]
Capturing CUDA graphs (decode, FULL):  51%|█████     | 26/51 [00:02<00:03,  7.82it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 28/51 [00:03<00:03,  6.67it/s]
Capturing CUDA graphs (decode, FULL):  59%|█████▉    | 30/51 [00:03<00:03,  6.20it/s]
Capturing CUDA graphs (decode, FULL):  61%|██████    | 31/51 [00:03<00:03,  6.51it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 32/51 [00:03<00:02,  6.87it/s]
Capturing CUDA graphs (decode, FULL):  65%|██████▍   | 33/51 [00:03<00:02,  7.29it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 34/51 [00:03<00:02,  7.73it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████   | 36/51 [00:04<00:01,  8.52it/s]
Capturing CUDA graphs (decode, FULL):  75%|███████▍  | 38/51 [00:04<00:01,  9.04it/s]
Capturing CUDA graphs (decode, FULL):  78%|███████▊  | 40/51 [00:04<00:01,  9.42it/s]
Capturing CUDA graphs (decode, FULL):  82%|████████▏ | 42/51 [00:04<00:00,  9.70it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▋ | 44/51 [00:04<00:00,  9.80it/s]
Capturing CUDA graphs (decode, FULL):  90%|█████████ | 46/51 [00:05<00:00,  9.97it/s]
Capturing CUDA graphs (decode, FULL):  94%|█████████▍| 48/51 [00:05<00:00,  7.40it/s]
Capturing CUDA graphs (decode, FULL):  96%|█████████▌| 49/51 [00:05<00:00,  7.00it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:05<00:00,  7.41it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:05<00:00,  8.55it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:  57%|█████▋    | 294/512 [00:00<00:00, 2938.69it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 2947.12it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:05<45:16,  5.32s/it, est. speed input: 3.01 toks/s, output: 48.15 toks/s]
Processed prompts:  12%|█▏        | 63/512 [00:05<00:27, 16.38it/s, est. speed input: 185.87 toks/s, output: 2973.85 toks/s]
Processed prompts:  36%|███▌      | 185/512 [00:05<00:05, 59.39it/s, est. speed input: 534.62 toks/s, output: 8553.90 toks/s]
Processed prompts:  62%|██████▏   | 315/512 [00:05<00:01, 119.51it/s, est. speed input: 893.50 toks/s, output: 14296.03 toks/s]
Processed prompts:  82%|████████▏ | 418/512 [00:05<00:00, 178.06it/s, est. speed input: 1164.10 toks/s, output: 18625.48 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:07<00:00, 120.41it/s, est. speed input: 1162.96 toks/s, output: 18607.32 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:07<00:00, 120.41it/s, est. speed input: 1162.96 toks/s, output: 18607.32 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:07<00:00, 72.68it/s, est. speed input: 1162.96 toks/s, output: 18607.32 toks/s] 
[rank0]:[W126 09:15:54.766797141 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=64 ==========
Time: 2026-01-26 09:54:00
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_8
Params: prompt_len=16, output_len=256, num_prompts=64, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 64 --max-num-seqs 64 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Llama3.2-3B-FP8_M64.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 09:54:08 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 09:54:09 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=490036) WARNING 01-26 09:54:17 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=490036) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=490036) WARNING 01-26 09:54:30 [backends.py:609] Failed to read file <frozen os>
Throughput: 18.43 requests/s, 5013.25 total tokens/s, 4718.35 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384

STDERR:
[2026-01-26 09:54:08] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 09:54:08] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-26 09:54:08] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-26 09:54:08] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 09:54:08] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 09:54:08] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 09:54:08] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 09:54:08] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 09:54:08] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-26 09:54:08] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 09:54:08] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 09:54:08] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 09:54:08] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 09:54:08] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 09:54:16] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 09:54:16] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-26 09:54:16] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-26 09:54:16] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 09:54:16] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 09:54:16] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 09:54:16] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 09:54:16] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 09:54:16] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-26 09:54:16] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 09:54:16] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 09:54:16] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 09:54:16] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 09:54:16] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=490036) [2026-01-26 09:54:17] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=490036) [2026-01-26 09:54:17] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=490036) [2026-01-26 09:54:17] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=490036) [2026-01-26 09:54:17] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=490036) [2026-01-26 09:54:17] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=490036) [2026-01-26 09:54:17] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=490036) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=490036) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.83s/it]
(EngineCore_DP0 pid=490036) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.83s/it]
(EngineCore_DP0 pid=490036) 
(EngineCore_DP0 pid=490036) [2026-01-26 09:54:22] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 4608] -> 1D uint8
(EngineCore_DP0 pid=490036) [2026-01-26 09:54:22] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 17694720 bytes
(EngineCore_DP0 pid=490036) [2026-01-26 09:54:22] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 4608] -> 1D uint8
(EngineCore_DP0 pid=490036) [2026-01-26 09:54:22] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10616832 bytes
(EngineCore_DP0 pid=490036) [2026-01-26 09:54:22] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 4608] -> 1D uint8
(EngineCore_DP0 pid=490036) [2026-01-26 09:54:22] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 56623104 bytes
(EngineCore_DP0 pid=490036) [2026-01-26 09:54:22] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 12288] -> 1D uint8
(EngineCore_DP0 pid=490036) [2026-01-26 09:54:22] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 28311552 bytes
(EngineCore_DP0 pid=490036) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   5%|▌         | 1/19 [00:00<00:02,  7.29it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 2/19 [00:00<00:02,  7.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 3/19 [00:00<00:02,  7.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  21%|██        | 4/19 [00:00<00:01,  8.36it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▋       | 5/19 [00:00<00:01,  8.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  32%|███▏      | 6/19 [00:00<00:01,  8.55it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 7/19 [00:00<00:01,  8.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 8/19 [00:00<00:01,  8.67it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 9/19 [00:01<00:01,  7.68it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 10/19 [00:01<00:02,  4.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 11/19 [00:01<00:02,  3.99it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 12/19 [00:02<00:01,  4.09it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  68%|██████▊   | 13/19 [00:02<00:01,  3.34it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▎  | 14/19 [00:02<00:01,  3.91it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  79%|███████▉  | 15/19 [00:02<00:00,  4.71it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 16/19 [00:02<00:00,  5.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 17/19 [00:03<00:00,  6.22it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  95%|█████████▍| 18/19 [00:03<00:00,  6.81it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:03<00:00,  6.81it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:03<00:00,  5.81it/s]
(EngineCore_DP0 pid=490036) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   9%|▉         | 1/11 [00:00<00:01,  7.47it/s]
Capturing CUDA graphs (decode, FULL):  18%|█▊        | 2/11 [00:00<00:01,  8.36it/s]
Capturing CUDA graphs (decode, FULL):  27%|██▋       | 3/11 [00:00<00:00,  8.61it/s]
Capturing CUDA graphs (decode, FULL):  36%|███▋      | 4/11 [00:00<00:00,  8.81it/s]
Capturing CUDA graphs (decode, FULL):  45%|████▌     | 5/11 [00:00<00:00,  8.92it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 6/11 [00:00<00:00,  9.00it/s]
Capturing CUDA graphs (decode, FULL):  64%|██████▎   | 7/11 [00:00<00:00,  8.98it/s]
Capturing CUDA graphs (decode, FULL):  73%|███████▎  | 8/11 [00:00<00:00,  8.18it/s]
Capturing CUDA graphs (decode, FULL):  82%|████████▏ | 9/11 [00:01<00:00,  6.45it/s]
Capturing CUDA graphs (decode, FULL):  91%|█████████ | 10/11 [00:01<00:00,  4.08it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:01<00:00,  4.91it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:01<00:00,  6.38it/s]

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 64/64 [00:00<00:00, 2602.96it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 1/64 [00:03<03:34,  3.40s/it, est. speed input: 4.71 toks/s, output: 75.36 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:03<00:00,  3.40s/it, est. speed input: 297.13 toks/s, output: 4754.12 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:03<00:00, 18.57it/s, est. speed input: 297.13 toks/s, output: 4754.12 toks/s]
[rank0]:[W126 09:54:57.311314152 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-26 09:55:00
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_8
Params: prompt_len=16, output_len=256, num_prompts=128, max_num_seqs=128
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 128 --max-num-seqs 128 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Llama3.2-3B-FP8_M128.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 09:55:08 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 09:55:09 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=491059) WARNING 01-26 09:55:17 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=491059) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=491059) WARNING 01-26 09:55:29 [backends.py:609] Failed to read file <frozen os>
Throughput: 30.87 requests/s, 8395.81 total tokens/s, 7901.94 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768

STDERR:
[2026-01-26 09:55:08] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 09:55:08] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-26 09:55:08] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-26 09:55:08] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 09:55:08] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 09:55:08] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 09:55:08] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 09:55:08] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 09:55:08] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-26 09:55:08] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 09:55:08] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 09:55:08] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 09:55:08] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 09:55:08] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 09:55:16] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 09:55:16] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-26 09:55:16] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-26 09:55:16] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 09:55:16] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 09:55:16] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 09:55:16] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 09:55:16] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 09:55:16] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-26 09:55:16] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 09:55:16] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 09:55:16] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 09:55:16] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 09:55:16] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=491059) [2026-01-26 09:55:17] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=491059) [2026-01-26 09:55:17] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=491059) [2026-01-26 09:55:17] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=491059) [2026-01-26 09:55:17] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=491059) [2026-01-26 09:55:17] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=491059) [2026-01-26 09:55:17] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=491059) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=491059) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.03s/it]
(EngineCore_DP0 pid=491059) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.03s/it]
(EngineCore_DP0 pid=491059) 
(EngineCore_DP0 pid=491059) [2026-01-26 09:55:19] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 4608] -> 1D uint8
(EngineCore_DP0 pid=491059) [2026-01-26 09:55:19] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 17694720 bytes
(EngineCore_DP0 pid=491059) [2026-01-26 09:55:19] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 4608] -> 1D uint8
(EngineCore_DP0 pid=491059) [2026-01-26 09:55:19] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10616832 bytes
(EngineCore_DP0 pid=491059) [2026-01-26 09:55:19] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 4608] -> 1D uint8
(EngineCore_DP0 pid=491059) [2026-01-26 09:55:19] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 56623104 bytes
(EngineCore_DP0 pid=491059) [2026-01-26 09:55:19] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 12288] -> 1D uint8
(EngineCore_DP0 pid=491059) [2026-01-26 09:55:19] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 28311552 bytes
(EngineCore_DP0 pid=491059) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|▎         | 1/35 [00:00<00:09,  3.45it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 2/35 [00:00<00:09,  3.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▊         | 3/35 [00:00<00:06,  4.77it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█▏        | 4/35 [00:00<00:05,  5.81it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 5/35 [00:00<00:04,  6.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 6/35 [00:01<00:04,  7.11it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 7/35 [00:01<00:03,  7.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  23%|██▎       | 8/35 [00:01<00:03,  7.90it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▌       | 9/35 [00:01<00:03,  8.09it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 10/35 [00:01<00:03,  7.45it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 11/35 [00:01<00:04,  6.00it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  34%|███▍      | 12/35 [00:02<00:06,  3.59it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 13/35 [00:02<00:08,  2.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 14/35 [00:03<00:06,  3.07it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 15/35 [00:03<00:05,  3.81it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  46%|████▌     | 16/35 [00:03<00:04,  4.55it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▊     | 17/35 [00:03<00:03,  5.25it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████▏    | 18/35 [00:03<00:02,  5.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  54%|█████▍    | 19/35 [00:03<00:02,  6.34it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 20/35 [00:03<00:02,  6.81it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 21/35 [00:03<00:01,  7.15it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 22/35 [00:04<00:01,  7.45it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  66%|██████▌   | 23/35 [00:04<00:01,  7.68it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 24/35 [00:04<00:01,  7.94it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 25/35 [00:04<00:01,  7.90it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▍  | 26/35 [00:04<00:01,  7.42it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  77%|███████▋  | 27/35 [00:04<00:01,  5.67it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 28/35 [00:05<00:01,  3.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  83%|████████▎ | 29/35 [00:05<00:01,  3.72it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 30/35 [00:06<00:01,  3.02it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▊ | 31/35 [00:06<00:01,  3.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████▏| 32/35 [00:06<00:00,  4.39it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 33/35 [00:06<00:00,  5.01it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  97%|█████████▋| 34/35 [00:06<00:00,  5.63it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:06<00:00,  5.72it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:06<00:00,  5.13it/s]
(EngineCore_DP0 pid=491059) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   5%|▌         | 1/19 [00:00<00:02,  6.92it/s]
Capturing CUDA graphs (decode, FULL):  11%|█         | 2/19 [00:00<00:02,  7.81it/s]
Capturing CUDA graphs (decode, FULL):  16%|█▌        | 3/19 [00:00<00:01,  8.13it/s]
Capturing CUDA graphs (decode, FULL):  21%|██        | 4/19 [00:00<00:01,  8.26it/s]
Capturing CUDA graphs (decode, FULL):  26%|██▋       | 5/19 [00:00<00:01,  8.43it/s]
Capturing CUDA graphs (decode, FULL):  32%|███▏      | 6/19 [00:00<00:01,  8.51it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 7/19 [00:00<00:01,  8.55it/s]
Capturing CUDA graphs (decode, FULL):  42%|████▏     | 8/19 [00:00<00:01,  8.24it/s]
Capturing CUDA graphs (decode, FULL):  47%|████▋     | 9/19 [00:01<00:01,  7.05it/s]
Capturing CUDA graphs (decode, FULL):  53%|█████▎    | 10/19 [00:01<00:02,  3.95it/s]
Capturing CUDA graphs (decode, FULL):  58%|█████▊    | 11/19 [00:01<00:01,  4.66it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 12/19 [00:02<00:01,  4.59it/s]
Capturing CUDA graphs (decode, FULL):  68%|██████▊   | 13/19 [00:02<00:01,  4.37it/s]
Capturing CUDA graphs (decode, FULL):  74%|███████▎  | 14/19 [00:02<00:01,  4.33it/s]
Capturing CUDA graphs (decode, FULL):  79%|███████▉  | 15/19 [00:02<00:00,  5.12it/s]
Capturing CUDA graphs (decode, FULL):  84%|████████▍ | 16/19 [00:02<00:00,  5.88it/s]
Capturing CUDA graphs (decode, FULL):  89%|████████▉ | 17/19 [00:02<00:00,  6.55it/s]
Capturing CUDA graphs (decode, FULL):  95%|█████████▍| 18/19 [00:02<00:00,  7.19it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:03<00:00,  7.68it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:03<00:00,  6.21it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 2859.39it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:03<08:24,  3.97s/it, est. speed input: 4.03 toks/s, output: 64.43 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:04<00:00, 33.90it/s, est. speed input: 384.90 toks/s, output: 6158.40 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:04<00:00, 33.90it/s, est. speed input: 499.46 toks/s, output: 7991.33 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:04<00:00, 31.21it/s, est. speed input: 499.46 toks/s, output: 7991.33 toks/s]
[rank0]:[W126 09:55:56.755553585 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-26 09:55:59
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_8
Params: prompt_len=16, output_len=256, num_prompts=256, max_num_seqs=256
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 256 --max-num-seqs 256 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Llama3.2-3B-FP8_M256.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 09:56:06 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 09:56:07 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=492091) WARNING 01-26 09:56:15 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=492091) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=492091) WARNING 01-26 09:56:28 [backends.py:609] Failed to read file <frozen os>
Throughput: 43.20 requests/s, 11751.02 total tokens/s, 11059.78 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536

STDERR:
[2026-01-26 09:56:05] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 09:56:06] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-26 09:56:06] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-26 09:56:06] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 09:56:06] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 09:56:06] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 09:56:06] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 09:56:06] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 09:56:06] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-26 09:56:06] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 09:56:06] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 09:56:06] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 09:56:06] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 09:56:06] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 09:56:14] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 09:56:14] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-26 09:56:14] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-26 09:56:14] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 09:56:14] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 09:56:14] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 09:56:14] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 09:56:14] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 09:56:14] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-26 09:56:14] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 09:56:14] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 09:56:14] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 09:56:14] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 09:56:14] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=492091) [2026-01-26 09:56:16] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=492091) [2026-01-26 09:56:16] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=492091) [2026-01-26 09:56:16] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=492091) [2026-01-26 09:56:16] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=492091) [2026-01-26 09:56:16] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=492091) [2026-01-26 09:56:16] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=492091) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=492091) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.03s/it]
(EngineCore_DP0 pid=492091) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.03s/it]
(EngineCore_DP0 pid=492091) 
(EngineCore_DP0 pid=492091) [2026-01-26 09:56:18] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 4608] -> 1D uint8
(EngineCore_DP0 pid=492091) [2026-01-26 09:56:18] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 17694720 bytes
(EngineCore_DP0 pid=492091) [2026-01-26 09:56:18] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 4608] -> 1D uint8
(EngineCore_DP0 pid=492091) [2026-01-26 09:56:18] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10616832 bytes
(EngineCore_DP0 pid=492091) [2026-01-26 09:56:18] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 4608] -> 1D uint8
(EngineCore_DP0 pid=492091) [2026-01-26 09:56:18] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 56623104 bytes
(EngineCore_DP0 pid=492091) [2026-01-26 09:56:18] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 12288] -> 1D uint8
(EngineCore_DP0 pid=492091) [2026-01-26 09:56:18] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 28311552 bytes
(EngineCore_DP0 pid=492091) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/36 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|▎         | 1/36 [00:00<00:19,  1.81it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 2/36 [00:01<00:19,  1.79it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 3/36 [00:01<00:12,  2.67it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 4/36 [00:01<00:08,  3.66it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 5/36 [00:01<00:06,  4.63it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 6/36 [00:01<00:05,  5.45it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  19%|█▉        | 7/36 [00:01<00:04,  6.16it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 8/36 [00:01<00:04,  6.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 9/36 [00:01<00:03,  7.29it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  28%|██▊       | 10/36 [00:02<00:03,  7.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███       | 11/36 [00:02<00:03,  8.03it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 12/36 [00:02<00:02,  8.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▌      | 13/36 [00:02<00:02,  8.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|███▉      | 14/36 [00:02<00:02,  8.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 15/36 [00:02<00:02,  8.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  44%|████▍     | 16/36 [00:02<00:02,  8.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 17/36 [00:02<00:02,  7.74it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 18/36 [00:03<00:05,  3.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 19/36 [00:03<00:04,  3.72it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  56%|█████▌    | 20/36 [00:04<00:05,  2.88it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 21/36 [00:04<00:04,  3.58it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|██████    | 22/36 [00:04<00:03,  4.32it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▍   | 23/36 [00:04<00:02,  5.08it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 24/36 [00:04<00:02,  5.77it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▉   | 25/36 [00:04<00:01,  6.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  72%|███████▏  | 26/36 [00:05<00:01,  6.68it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 27/36 [00:05<00:01,  7.02it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 28/36 [00:05<00:01,  7.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  81%|████████  | 29/36 [00:05<00:00,  7.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  83%|████████▎ | 30/36 [00:05<00:00,  7.73it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 31/36 [00:05<00:00,  8.01it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 32/36 [00:05<00:00,  8.19it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 33/36 [00:05<00:00,  8.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 34/36 [00:06<00:00,  7.66it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  97%|█████████▋| 35/36 [00:06<00:00,  6.86it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:06<00:00,  4.00it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:06<00:00,  5.34it/s]
(EngineCore_DP0 pid=492091) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   3%|▎         | 1/35 [00:00<00:08,  4.06it/s]
Capturing CUDA graphs (decode, FULL):   6%|▌         | 2/35 [00:00<00:10,  3.01it/s]
Capturing CUDA graphs (decode, FULL):   9%|▊         | 3/35 [00:00<00:08,  3.91it/s]
Capturing CUDA graphs (decode, FULL):  11%|█▏        | 4/35 [00:00<00:06,  4.93it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 5/35 [00:01<00:05,  5.76it/s]
Capturing CUDA graphs (decode, FULL):  17%|█▋        | 6/35 [00:01<00:04,  6.40it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 7/35 [00:01<00:04,  6.92it/s]
Capturing CUDA graphs (decode, FULL):  23%|██▎       | 8/35 [00:01<00:03,  7.32it/s]
Capturing CUDA graphs (decode, FULL):  26%|██▌       | 9/35 [00:01<00:03,  7.69it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 10/35 [00:01<00:03,  7.97it/s]
Capturing CUDA graphs (decode, FULL):  31%|███▏      | 11/35 [00:01<00:02,  8.13it/s]
Capturing CUDA graphs (decode, FULL):  34%|███▍      | 12/35 [00:01<00:02,  8.24it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 13/35 [00:01<00:02,  8.32it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 14/35 [00:02<00:02,  8.35it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 15/35 [00:02<00:02,  8.42it/s]
Capturing CUDA graphs (decode, FULL):  46%|████▌     | 16/35 [00:02<00:02,  7.94it/s]
Capturing CUDA graphs (decode, FULL):  49%|████▊     | 17/35 [00:02<00:02,  6.94it/s]
Capturing CUDA graphs (decode, FULL):  51%|█████▏    | 18/35 [00:03<00:04,  4.03it/s]
Capturing CUDA graphs (decode, FULL):  54%|█████▍    | 19/35 [00:03<00:03,  4.77it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 20/35 [00:03<00:02,  5.23it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 21/35 [00:03<00:03,  3.65it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 22/35 [00:03<00:03,  4.01it/s]
Capturing CUDA graphs (decode, FULL):  66%|██████▌   | 23/35 [00:04<00:02,  4.78it/s]
Capturing CUDA graphs (decode, FULL):  69%|██████▊   | 24/35 [00:04<00:01,  5.52it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 25/35 [00:04<00:01,  6.21it/s]
Capturing CUDA graphs (decode, FULL):  74%|███████▍  | 26/35 [00:04<00:01,  6.81it/s]
Capturing CUDA graphs (decode, FULL):  77%|███████▋  | 27/35 [00:04<00:01,  7.35it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 28/35 [00:04<00:00,  7.68it/s]
Capturing CUDA graphs (decode, FULL):  83%|████████▎ | 29/35 [00:04<00:00,  8.01it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 30/35 [00:04<00:00,  8.28it/s]
Capturing CUDA graphs (decode, FULL):  89%|████████▊ | 31/35 [00:04<00:00,  8.51it/s]
Capturing CUDA graphs (decode, FULL):  91%|█████████▏| 32/35 [00:05<00:00,  8.76it/s]
Capturing CUDA graphs (decode, FULL):  94%|█████████▍| 33/35 [00:05<00:00,  8.92it/s]
Capturing CUDA graphs (decode, FULL):  97%|█████████▋| 34/35 [00:05<00:00,  8.99it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:05<00:00,  8.97it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:05<00:00,  6.45it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 2850.25it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:05<22:06,  5.20s/it, est. speed input: 3.08 toks/s, output: 49.23 toks/s]
Processed prompts:  24%|██▍       | 62/256 [00:05<00:11, 16.44it/s, est. speed input: 186.73 toks/s, output: 2987.71 toks/s]
Processed prompts:  50%|████▉     | 127/256 [00:05<00:03, 39.58it/s, est. speed input: 374.99 toks/s, output: 5999.77 toks/s]
Processed prompts:  71%|███████▏  | 183/256 [00:05<00:01, 65.22it/s, est. speed input: 529.90 toks/s, output: 8478.42 toks/s]
Processed prompts:  91%|█████████ | 232/256 [00:05<00:00, 90.43it/s, est. speed input: 654.31 toks/s, output: 10468.90 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:05<00:00, 90.43it/s, est. speed input: 702.09 toks/s, output: 11233.36 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:05<00:00, 43.88it/s, est. speed input: 702.09 toks/s, output: 11233.36 toks/s]
[rank0]:[W126 09:56:59.677415232 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-26 09:57:02
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_8
Params: prompt_len=16, output_len=256, num_prompts=512, max_num_seqs=512
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 512 --max-num-seqs 512 --max-model-len 272 --max-num-batched-tokens 512 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Llama3.2-3B-FP8_M512.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 09:57:09 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 09:57:10 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=493170) WARNING 01-26 09:57:18 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=493170) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=493170) WARNING 01-26 09:57:31 [backends.py:609] Failed to read file <frozen os>
Throughput: 41.61 requests/s, 11316.59 total tokens/s, 10650.91 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072

STDERR:
[2026-01-26 09:57:08] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 09:57:09] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-26 09:57:09] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-26 09:57:09] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 09:57:09] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 09:57:09] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 09:57:09] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 09:57:09] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 09:57:09] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-26 09:57:09] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 09:57:09] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 09:57:09] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 09:57:09] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 09:57:09] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 09:57:17] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 09:57:17] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-26 09:57:17] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-26 09:57:17] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 09:57:17] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 09:57:17] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 09:57:17] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 09:57:17] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 09:57:17] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-26 09:57:17] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 09:57:17] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 09:57:17] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 09:57:17] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 09:57:17] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=493170) [2026-01-26 09:57:19] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=493170) [2026-01-26 09:57:19] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=493170) [2026-01-26 09:57:19] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=493170) [2026-01-26 09:57:19] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=493170) [2026-01-26 09:57:19] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=493170) [2026-01-26 09:57:19] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=493170) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=493170) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.01s/it]
(EngineCore_DP0 pid=493170) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.01s/it]
(EngineCore_DP0 pid=493170) 
(EngineCore_DP0 pid=493170) [2026-01-26 09:57:21] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 4608] -> 1D uint8
(EngineCore_DP0 pid=493170) [2026-01-26 09:57:21] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 17694720 bytes
(EngineCore_DP0 pid=493170) [2026-01-26 09:57:21] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 4608] -> 1D uint8
(EngineCore_DP0 pid=493170) [2026-01-26 09:57:21] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10616832 bytes
(EngineCore_DP0 pid=493170) [2026-01-26 09:57:21] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 4608] -> 1D uint8
(EngineCore_DP0 pid=493170) [2026-01-26 09:57:21] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 56623104 bytes
(EngineCore_DP0 pid=493170) [2026-01-26 09:57:21] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 12288] -> 1D uint8
(EngineCore_DP0 pid=493170) [2026-01-26 09:57:21] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 28311552 bytes
(EngineCore_DP0 pid=493170) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|▏         | 1/51 [00:00<00:06,  8.22it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|▍         | 2/51 [00:00<00:05,  8.19it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 3/51 [00:00<00:05,  8.39it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 4/51 [00:00<00:05,  8.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|▉         | 5/51 [00:00<00:05,  8.48it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|█▏        | 6/51 [00:00<00:05,  8.48it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▎        | 7/51 [00:00<00:05,  8.39it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 8/51 [00:00<00:05,  8.39it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 9/51 [00:01<00:05,  8.35it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|█▉        | 10/51 [00:01<00:12,  3.37it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 11/51 [00:02<00:11,  3.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|██▎       | 12/51 [00:02<00:13,  2.85it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 13/51 [00:02<00:10,  3.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 14/51 [00:02<00:08,  4.32it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▉       | 15/51 [00:02<00:07,  5.06it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 16/51 [00:03<00:06,  5.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 17/51 [00:03<00:05,  6.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  35%|███▌      | 18/51 [00:03<00:04,  6.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 19/51 [00:03<00:04,  7.14it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|███▉      | 20/51 [00:03<00:04,  7.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|████      | 21/51 [00:03<00:03,  7.68it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 22/51 [00:03<00:03,  7.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 23/51 [00:03<00:03,  8.06it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 24/51 [00:03<00:03,  8.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▉     | 25/51 [00:04<00:03,  8.29it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████     | 26/51 [00:04<00:03,  8.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 27/51 [00:04<00:03,  6.35it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 28/51 [00:04<00:06,  3.82it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 29/51 [00:05<00:05,  3.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  59%|█████▉    | 30/51 [00:05<00:06,  3.12it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|██████    | 31/51 [00:05<00:05,  3.70it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 32/51 [00:05<00:04,  4.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  65%|██████▍   | 33/51 [00:06<00:03,  4.79it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 34/51 [00:06<00:03,  5.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 35/51 [00:06<00:02,  5.65it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████   | 36/51 [00:06<00:02,  6.07it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 37/51 [00:06<00:02,  6.32it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▍  | 38/51 [00:06<00:01,  6.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|███████▋  | 39/51 [00:06<00:01,  6.67it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 40/51 [00:07<00:01,  6.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 41/51 [00:07<00:01,  7.00it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 42/51 [00:07<00:01,  6.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 43/51 [00:08<00:02,  3.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▋ | 44/51 [00:08<00:01,  3.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  88%|████████▊ | 45/51 [00:08<00:02,  2.90it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|█████████ | 46/51 [00:08<00:01,  3.44it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 47/51 [00:09<00:00,  4.13it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 48/51 [00:09<00:00,  4.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  96%|█████████▌| 49/51 [00:09<00:00,  5.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  98%|█████████▊| 50/51 [00:09<00:00,  6.02it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:09<00:00,  6.04it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:09<00:00,  5.29it/s]
(EngineCore_DP0 pid=493170) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   2%|▏         | 1/51 [00:00<00:08,  5.59it/s]
Capturing CUDA graphs (decode, FULL):   4%|▍         | 2/51 [00:00<00:07,  6.45it/s]
Capturing CUDA graphs (decode, FULL):   6%|▌         | 3/51 [00:00<00:07,  6.83it/s]
Capturing CUDA graphs (decode, FULL):   8%|▊         | 4/51 [00:00<00:06,  7.13it/s]
Capturing CUDA graphs (decode, FULL):  10%|▉         | 5/51 [00:00<00:06,  7.22it/s]
Capturing CUDA graphs (decode, FULL):  12%|█▏        | 6/51 [00:00<00:06,  7.33it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▎        | 7/51 [00:01<00:07,  5.99it/s]
Capturing CUDA graphs (decode, FULL):  16%|█▌        | 8/51 [00:01<00:10,  4.04it/s]
Capturing CUDA graphs (decode, FULL):  18%|█▊        | 9/51 [00:01<00:09,  4.24it/s]
Capturing CUDA graphs (decode, FULL):  20%|█▉        | 10/51 [00:02<00:13,  3.12it/s]
Capturing CUDA graphs (decode, FULL):  22%|██▏       | 11/51 [00:02<00:11,  3.52it/s]
Capturing CUDA graphs (decode, FULL):  24%|██▎       | 12/51 [00:02<00:09,  4.22it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 13/51 [00:02<00:07,  4.94it/s]
Capturing CUDA graphs (decode, FULL):  27%|██▋       | 14/51 [00:02<00:06,  5.73it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▉       | 15/51 [00:02<00:05,  6.38it/s]
Capturing CUDA graphs (decode, FULL):  31%|███▏      | 16/51 [00:03<00:05,  6.95it/s]
Capturing CUDA graphs (decode, FULL):  33%|███▎      | 17/51 [00:03<00:04,  7.39it/s]
Capturing CUDA graphs (decode, FULL):  35%|███▌      | 18/51 [00:03<00:04,  7.76it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 19/51 [00:03<00:03,  8.01it/s]
Capturing CUDA graphs (decode, FULL):  39%|███▉      | 20/51 [00:03<00:03,  8.22it/s]
Capturing CUDA graphs (decode, FULL):  41%|████      | 21/51 [00:03<00:03,  8.33it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 22/51 [00:03<00:03,  8.44it/s]
Capturing CUDA graphs (decode, FULL):  45%|████▌     | 23/51 [00:03<00:03,  8.43it/s]
Capturing CUDA graphs (decode, FULL):  47%|████▋     | 24/51 [00:03<00:03,  8.45it/s]
Capturing CUDA graphs (decode, FULL):  49%|████▉     | 25/51 [00:04<00:03,  7.14it/s]
Capturing CUDA graphs (decode, FULL):  51%|█████     | 26/51 [00:04<00:06,  4.05it/s]
Capturing CUDA graphs (decode, FULL):  53%|█████▎    | 27/51 [00:04<00:04,  4.82it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 28/51 [00:04<00:04,  4.91it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 29/51 [00:05<00:05,  4.27it/s]
Capturing CUDA graphs (decode, FULL):  59%|█████▉    | 30/51 [00:05<00:05,  3.85it/s]
Capturing CUDA graphs (decode, FULL):  61%|██████    | 31/51 [00:05<00:04,  4.63it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 32/51 [00:05<00:03,  5.37it/s]
Capturing CUDA graphs (decode, FULL):  65%|██████▍   | 33/51 [00:05<00:02,  6.09it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 34/51 [00:06<00:02,  6.70it/s]
Capturing CUDA graphs (decode, FULL):  69%|██████▊   | 35/51 [00:06<00:02,  7.25it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████   | 36/51 [00:06<00:01,  7.72it/s]
Capturing CUDA graphs (decode, FULL):  73%|███████▎  | 37/51 [00:06<00:01,  8.04it/s]
Capturing CUDA graphs (decode, FULL):  75%|███████▍  | 38/51 [00:06<00:01,  8.26it/s]
Capturing CUDA graphs (decode, FULL):  76%|███████▋  | 39/51 [00:06<00:01,  8.32it/s]
Capturing CUDA graphs (decode, FULL):  78%|███████▊  | 40/51 [00:06<00:01,  8.33it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 41/51 [00:06<00:01,  8.47it/s]
Capturing CUDA graphs (decode, FULL):  82%|████████▏ | 42/51 [00:06<00:01,  8.63it/s]
Capturing CUDA graphs (decode, FULL):  84%|████████▍ | 43/51 [00:07<00:01,  7.80it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▋ | 44/51 [00:07<00:01,  6.83it/s]
Capturing CUDA graphs (decode, FULL):  88%|████████▊ | 45/51 [00:07<00:01,  4.47it/s]
Capturing CUDA graphs (decode, FULL):  90%|█████████ | 46/51 [00:07<00:01,  4.52it/s]
Capturing CUDA graphs (decode, FULL):  92%|█████████▏| 47/51 [00:08<00:00,  4.74it/s]
Capturing CUDA graphs (decode, FULL):  94%|█████████▍| 48/51 [00:08<00:00,  3.60it/s]
Capturing CUDA graphs (decode, FULL):  96%|█████████▌| 49/51 [00:08<00:00,  4.36it/s]
Capturing CUDA graphs (decode, FULL):  98%|█████████▊| 50/51 [00:08<00:00,  5.17it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:08<00:00,  5.93it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:08<00:00,  5.76it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:  57%|█████▋    | 294/512 [00:00<00:00, 2939.27it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 2917.83it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:08<1:15:53,  8.91s/it, est. speed input: 1.80 toks/s, output: 28.73 toks/s]
Processed prompts:  12%|█▏        | 63/512 [00:09<00:45,  9.87it/s, est. speed input: 111.67 toks/s, output: 1786.79 toks/s]
Processed prompts:  28%|██▊       | 141/512 [00:09<00:13, 26.80it/s, est. speed input: 247.18 toks/s, output: 3954.81 toks/s]
Processed prompts:  40%|████      | 205/512 [00:09<00:06, 45.24it/s, est. speed input: 354.83 toks/s, output: 5677.26 toks/s]
Processed prompts:  51%|█████     | 259/512 [00:09<00:03, 65.49it/s, est. speed input: 443.00 toks/s, output: 7088.05 toks/s]
Processed prompts:  62%|██████▏   | 315/512 [00:09<00:02, 91.72it/s, est. speed input: 531.64 toks/s, output: 8506.15 toks/s]
Processed prompts:  71%|███████▏  | 365/512 [00:09<00:01, 120.77it/s, est. speed input: 609.12 toks/s, output: 9745.84 toks/s]
Processed prompts:  81%|████████  | 414/512 [00:09<00:00, 147.57it/s, est. speed input: 679.80 toks/s, output: 10876.73 toks/s]
Processed prompts:  89%|████████▉ | 457/512 [00:09<00:00, 165.27it/s, est. speed input: 737.10 toks/s, output: 11793.51 toks/s]
Processed prompts:  97%|█████████▋| 495/512 [00:10<00:00, 107.77it/s, est. speed input: 744.91 toks/s, output: 11918.56 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:12<00:00, 107.77it/s, est. speed input: 675.41 toks/s, output: 10806.62 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:12<00:00, 42.21it/s, est. speed input: 675.41 toks/s, output: 10806.62 toks/s] 
[rank0]:[W126 09:58:18.047409042 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=64 ==========
Time: 2026-01-26 10:40:20
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_8
Params: prompt_len=16, output_len=256, num_prompts=64, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 64 --max-num-seqs 64 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Qwen2.5-7B-FP8_M64.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 10:40:29 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 10:40:30 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=537395) WARNING 01-26 10:40:38 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=537395) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=537395) WARNING 01-26 10:40:59 [backends.py:609] Failed to read file <frozen os>
Throughput: 13.23 requests/s, 3598.58 total tokens/s, 3386.90 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384

STDERR:
[2026-01-26 10:40:28] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 10:40:28] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 10:40:28] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 10:40:28] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 10:40:28] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 10:40:28] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 10:40:28] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 10:40:28] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 10:40:28] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 10:40:28] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 10:40:28] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 10:40:28] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 10:40:28] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 10:40:28] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 10:40:37] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 10:40:37] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 10:40:37] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 10:40:37] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 10:40:37] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 10:40:37] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 10:40:38] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 10:40:38] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 10:40:38] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 10:40:38] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 10:40:38] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 10:40:38] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 10:40:38] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 10:40:38] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=537395) [2026-01-26 10:40:39] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=537395) [2026-01-26 10:40:39] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=537395) [2026-01-26 10:40:39] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=537395) [2026-01-26 10:40:39] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=537395) [2026-01-26 10:40:39] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=537395) [2026-01-26 10:40:39] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=537395) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=537395) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:03<00:03,  3.94s/it]
(EngineCore_DP0 pid=537395) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:07<00:00,  3.89s/it]
(EngineCore_DP0 pid=537395) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:07<00:00,  3.90s/it]
(EngineCore_DP0 pid=537395) 
(EngineCore_DP0 pid=537395) [2026-01-26 10:40:49] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5376] -> 1D uint8
(EngineCore_DP0 pid=537395) [2026-01-26 10:40:49] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 18579456 bytes
(EngineCore_DP0 pid=537395) [2026-01-26 10:40:49] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5376] -> 1D uint8
(EngineCore_DP0 pid=537395) [2026-01-26 10:40:49] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14450688 bytes
(EngineCore_DP0 pid=537395) [2026-01-26 10:40:49] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5376] -> 1D uint8
(EngineCore_DP0 pid=537395) [2026-01-26 10:40:49] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 152764416 bytes
(EngineCore_DP0 pid=537395) [2026-01-26 10:40:49] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 28416] -> 1D uint8
(EngineCore_DP0 pid=537395) [2026-01-26 10:40:49] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 76382208 bytes
(EngineCore_DP0 pid=537395) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   5%|▌         | 1/19 [00:00<00:05,  3.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 2/19 [00:00<00:04,  3.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 3/19 [00:00<00:03,  4.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  21%|██        | 4/19 [00:00<00:03,  4.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▋       | 5/19 [00:01<00:04,  2.87it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  32%|███▏      | 6/19 [00:01<00:03,  3.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 7/19 [00:02<00:04,  2.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 8/19 [00:02<00:03,  3.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 9/19 [00:02<00:02,  3.97it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 10/19 [00:02<00:01,  4.71it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 11/19 [00:02<00:01,  5.45it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 12/19 [00:02<00:01,  6.15it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  68%|██████▊   | 13/19 [00:03<00:00,  6.70it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▎  | 14/19 [00:03<00:00,  7.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  79%|███████▉  | 15/19 [00:03<00:00,  7.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:02<00:00,  8.51it/s]
(EngineCore_DP0 pid=537395) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   9%|▉         | 1/11 [00:00<00:01,  7.57it/s]
Capturing CUDA graphs (decode, FULL):  18%|█▊        | 2/11 [00:00<00:01,  5.89it/s]
Capturing CUDA graphs (decode, FULL):  27%|██▋       | 3/11 [00:00<00:02,  3.14it/s]
Capturing CUDA graphs (decode, FULL):  36%|███▋      | 4/11 [00:00<00:01,  4.20it/s]
Capturing CUDA graphs (decode, FULL):  45%|████▌     | 5/11 [00:01<00:01,  4.73it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 6/11 [00:01<00:01,  3.76it/s]
Capturing CUDA graphs (decode, FULL):  64%|██████▎   | 7/11 [00:01<00:01,  3.80it/s]
Capturing CUDA graphs (decode, FULL):  73%|███████▎  | 8/11 [00:01<00:00,  4.64it/s]
Capturing CUDA graphs (decode, FULL):  82%|████████▏ | 9/11 [00:01<00:00,  5.47it/s]
Capturing CUDA graphs (decode, FULL):  91%|█████████ | 10/11 [00:02<00:00,  6.13it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:02<00:00,  6.76it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:02<00:00,  5.02it/s]

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 64/64 [00:00<00:00, 2515.89it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 1/64 [00:04<04:58,  4.73s/it, est. speed input: 3.38 toks/s, output: 54.08 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:04<00:00,  4.73s/it, est. speed input: 212.88 toks/s, output: 3406.10 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:04<00:00, 13.30it/s, est. speed input: 212.88 toks/s, output: 3406.10 toks/s]
[rank0]:[W126 10:41:26.044533861 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-26 10:41:29
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_8
Params: prompt_len=16, output_len=256, num_prompts=128, max_num_seqs=128
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 128 --max-num-seqs 128 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Qwen2.5-7B-FP8_M128.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 10:41:37 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 10:41:38 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=538589) WARNING 01-26 10:41:47 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=538589) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=538589) WARNING 01-26 10:42:01 [backends.py:609] Failed to read file <frozen os>
Throughput: 22.67 requests/s, 6165.45 total tokens/s, 5802.77 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768

STDERR:
[2026-01-26 10:41:37] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 10:41:37] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 10:41:37] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 10:41:37] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 10:41:37] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 10:41:37] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 10:41:37] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 10:41:37] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 10:41:37] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 10:41:37] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 10:41:37] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 10:41:37] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 10:41:37] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 10:41:37] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 10:41:46] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 10:41:46] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 10:41:46] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 10:41:46] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 10:41:46] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 10:41:46] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 10:41:46] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 10:41:46] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 10:41:46] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 10:41:46] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 10:41:46] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 10:41:46] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 10:41:46] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 10:41:46] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=538589) [2026-01-26 10:41:48] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=538589) [2026-01-26 10:41:48] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=538589) [2026-01-26 10:41:48] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=538589) [2026-01-26 10:41:48] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=538589) [2026-01-26 10:41:48] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=538589) [2026-01-26 10:41:48] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=538589) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=538589) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.02it/s]
(EngineCore_DP0 pid=538589) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.39s/it]
(EngineCore_DP0 pid=538589) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.33s/it]
(EngineCore_DP0 pid=538589) 
(EngineCore_DP0 pid=538589) [2026-01-26 10:41:51] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5376] -> 1D uint8
(EngineCore_DP0 pid=538589) [2026-01-26 10:41:51] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 18579456 bytes
(EngineCore_DP0 pid=538589) [2026-01-26 10:41:51] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5376] -> 1D uint8
(EngineCore_DP0 pid=538589) [2026-01-26 10:41:51] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14450688 bytes
(EngineCore_DP0 pid=538589) [2026-01-26 10:41:51] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5376] -> 1D uint8
(EngineCore_DP0 pid=538589) [2026-01-26 10:41:51] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 152764416 bytes
(EngineCore_DP0 pid=538589) [2026-01-26 10:41:51] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 28416] -> 1D uint8
(EngineCore_DP0 pid=538589) [2026-01-26 10:41:51] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 76382208 bytes
(EngineCore_DP0 pid=538589) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|▎         | 1/35 [00:00<00:11,  2.91it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 2/35 [00:00<00:12,  2.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▊         | 3/35 [00:00<00:08,  3.80it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█▏        | 4/35 [00:00<00:06,  4.85it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 5/35 [00:01<00:05,  5.66it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 6/35 [00:01<00:04,  6.32it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 7/35 [00:01<00:04,  6.82it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  23%|██▎       | 8/35 [00:01<00:03,  7.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▌       | 9/35 [00:01<00:03,  7.60it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 10/35 [00:01<00:04,  5.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 11/35 [00:02<00:06,  3.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  34%|███▍      | 12/35 [00:02<00:06,  3.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 13/35 [00:03<00:07,  2.88it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 14/35 [00:03<00:06,  3.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 15/35 [00:03<00:04,  4.18it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  46%|████▌     | 16/35 [00:03<00:03,  4.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▊     | 17/35 [00:03<00:03,  5.63it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████▏    | 18/35 [00:03<00:02,  6.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  54%|█████▍    | 19/35 [00:03<00:02,  6.70it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 20/35 [00:04<00:02,  7.15it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 21/35 [00:04<00:01,  7.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 22/35 [00:04<00:01,  7.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  66%|██████▌   | 23/35 [00:04<00:01,  7.87it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 24/35 [00:04<00:01,  8.10it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 25/35 [00:04<00:01,  7.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▍  | 26/35 [00:04<00:01,  5.58it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  77%|███████▋  | 27/35 [00:05<00:02,  3.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 28/35 [00:05<00:01,  3.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:05<00:00,  6.29it/s]
(EngineCore_DP0 pid=538589) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   5%|▌         | 1/19 [00:00<00:02,  7.02it/s]
Capturing CUDA graphs (decode, FULL):  11%|█         | 2/19 [00:00<00:02,  7.96it/s]
Capturing CUDA graphs (decode, FULL):  16%|█▌        | 3/19 [00:00<00:01,  8.31it/s]
Capturing CUDA graphs (decode, FULL):  21%|██        | 4/19 [00:00<00:01,  8.51it/s]
Capturing CUDA graphs (decode, FULL):  26%|██▋       | 5/19 [00:00<00:01,  8.50it/s]
Capturing CUDA graphs (decode, FULL):  32%|███▏      | 6/19 [00:00<00:01,  8.59it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 7/19 [00:00<00:01,  8.72it/s]
Capturing CUDA graphs (decode, FULL):  42%|████▏     | 8/19 [00:01<00:01,  7.17it/s]
Capturing CUDA graphs (decode, FULL):  47%|████▋     | 9/19 [00:01<00:02,  4.06it/s]
Capturing CUDA graphs (decode, FULL):  53%|█████▎    | 10/19 [00:01<00:01,  4.84it/s]
Capturing CUDA graphs (decode, FULL):  58%|█████▊    | 11/19 [00:01<00:01,  5.20it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 12/19 [00:02<00:01,  3.96it/s]
Capturing CUDA graphs (decode, FULL):  68%|██████▊   | 13/19 [00:02<00:01,  3.72it/s]
Capturing CUDA graphs (decode, FULL):  74%|███████▎  | 14/19 [00:02<00:01,  4.48it/s]
Capturing CUDA graphs (decode, FULL):  79%|███████▉  | 15/19 [00:02<00:00,  5.23it/s]
Capturing CUDA graphs (decode, FULL):  84%|████████▍ | 16/19 [00:02<00:00,  5.98it/s]
Capturing CUDA graphs (decode, FULL):  89%|████████▉ | 17/19 [00:02<00:00,  6.67it/s]
Capturing CUDA graphs (decode, FULL):  95%|█████████▍| 18/19 [00:03<00:00,  7.27it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:03<00:00,  7.76it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:03<00:00,  6.04it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 2632.91it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:05<11:26,  5.41s/it, est. speed input: 2.96 toks/s, output: 47.33 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:05<00:02, 19.17it/s, est. speed input: 217.41 toks/s, output: 3478.47 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:05<00:00, 19.17it/s, est. speed input: 365.93 toks/s, output: 5854.93 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:05<00:00, 22.87it/s, est. speed input: 365.93 toks/s, output: 5854.93 toks/s]
[rank0]:[W126 10:42:30.038900286 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-26 10:42:33
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_8
Params: prompt_len=16, output_len=256, num_prompts=256, max_num_seqs=256
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 256 --max-num-seqs 256 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Qwen2.5-7B-FP8_M256.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 10:42:43 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 10:42:44 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=539727) WARNING 01-26 10:42:52 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=539727) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=539727) WARNING 01-26 10:43:08 [backends.py:609] Failed to read file <frozen os>
Throughput: 30.42 requests/s, 8274.45 total tokens/s, 7787.72 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536

STDERR:
[2026-01-26 10:42:42] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 10:42:43] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 10:42:43] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 10:42:43] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 10:42:43] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 10:42:43] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 10:42:43] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 10:42:43] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 10:42:43] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 10:42:43] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 10:42:43] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 10:42:43] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 10:42:43] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 10:42:43] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 10:42:50] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 10:42:50] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 10:42:50] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 10:42:50] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 10:42:50] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 10:42:50] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 10:42:50] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 10:42:50] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 10:42:50] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 10:42:50] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 10:42:50] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 10:42:50] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 10:42:50] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 10:42:50] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=539727) [2026-01-26 10:42:53] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=539727) [2026-01-26 10:42:53] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=539727) [2026-01-26 10:42:53] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=539727) [2026-01-26 10:42:53] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=539727) [2026-01-26 10:42:53] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=539727) [2026-01-26 10:42:53] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=539727) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=539727) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.00it/s]
(EngineCore_DP0 pid=539727) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.29s/it]
(EngineCore_DP0 pid=539727) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.25s/it]
(EngineCore_DP0 pid=539727) 
(EngineCore_DP0 pid=539727) [2026-01-26 10:42:57] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5376] -> 1D uint8
(EngineCore_DP0 pid=539727) [2026-01-26 10:42:57] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 18579456 bytes
(EngineCore_DP0 pid=539727) [2026-01-26 10:42:57] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5376] -> 1D uint8
(EngineCore_DP0 pid=539727) [2026-01-26 10:42:57] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14450688 bytes
(EngineCore_DP0 pid=539727) [2026-01-26 10:42:57] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5376] -> 1D uint8
(EngineCore_DP0 pid=539727) [2026-01-26 10:42:57] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 152764416 bytes
(EngineCore_DP0 pid=539727) [2026-01-26 10:42:57] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 28416] -> 1D uint8
(EngineCore_DP0 pid=539727) [2026-01-26 10:42:57] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 76382208 bytes
(EngineCore_DP0 pid=539727) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/36 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|▎         | 1/36 [00:00<00:04,  8.34it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 2/36 [00:00<00:04,  8.17it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 3/36 [00:00<00:04,  8.17it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 4/36 [00:00<00:03,  8.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 5/36 [00:00<00:03,  8.15it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 6/36 [00:00<00:03,  8.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  19%|█▉        | 7/36 [00:00<00:03,  7.73it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 8/36 [00:01<00:05,  5.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▌      | 13/36 [00:01<00:01, 13.24it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 15/36 [00:01<00:01, 11.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 17/36 [00:01<00:01, 10.33it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 19/36 [00:02<00:01,  9.59it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 21/36 [00:02<00:01,  9.11it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|██████    | 22/36 [00:02<00:01,  8.84it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▍   | 23/36 [00:02<00:01,  8.02it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 24/36 [00:03<00:03,  3.99it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▉   | 25/36 [00:03<00:02,  4.54it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  72%|███████▏  | 26/36 [00:04<00:03,  2.91it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 27/36 [00:04<00:02,  3.24it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 28/36 [00:04<00:02,  3.91it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  81%|████████  | 29/36 [00:04<00:01,  4.60it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  83%|████████▎ | 30/36 [00:04<00:01,  5.26it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 31/36 [00:04<00:00,  5.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 32/36 [00:04<00:00,  6.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 33/36 [00:05<00:00,  6.99it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 34/36 [00:05<00:00,  7.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  97%|█████████▋| 35/36 [00:05<00:00,  7.77it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:05<00:00,  7.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:05<00:00,  6.67it/s]
(EngineCore_DP0 pid=539727) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   3%|▎         | 1/35 [00:00<00:05,  6.52it/s]
Capturing CUDA graphs (decode, FULL):   6%|▌         | 2/35 [00:00<00:04,  7.39it/s]
Capturing CUDA graphs (decode, FULL):   9%|▊         | 3/35 [00:00<00:04,  6.42it/s]
Capturing CUDA graphs (decode, FULL):  11%|█▏        | 4/35 [00:01<00:09,  3.16it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 5/35 [00:01<00:07,  4.06it/s]
Capturing CUDA graphs (decode, FULL):  17%|█▋        | 6/35 [00:01<00:06,  4.36it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 7/35 [00:01<00:08,  3.11it/s]
Capturing CUDA graphs (decode, FULL):  23%|██▎       | 8/35 [00:02<00:07,  3.50it/s]
Capturing CUDA graphs (decode, FULL):  26%|██▌       | 9/35 [00:02<00:06,  4.17it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 10/35 [00:02<00:05,  4.94it/s]
Capturing CUDA graphs (decode, FULL):  31%|███▏      | 11/35 [00:02<00:04,  5.68it/s]
Capturing CUDA graphs (decode, FULL):  34%|███▍      | 12/35 [00:02<00:03,  6.34it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 13/35 [00:02<00:03,  6.82it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 14/35 [00:02<00:02,  7.28it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 15/35 [00:02<00:02,  7.58it/s]
Capturing CUDA graphs (decode, FULL):  46%|████▌     | 16/35 [00:03<00:02,  7.82it/s]
Capturing CUDA graphs (decode, FULL):  49%|████▊     | 17/35 [00:03<00:02,  8.03it/s]
Capturing CUDA graphs (decode, FULL):  51%|█████▏    | 18/35 [00:03<00:02,  8.08it/s]
Capturing CUDA graphs (decode, FULL):  54%|█████▍    | 19/35 [00:03<00:02,  7.68it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 20/35 [00:03<00:02,  6.52it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 21/35 [00:04<00:03,  3.65it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 22/35 [00:04<00:02,  4.36it/s]
Capturing CUDA graphs (decode, FULL):  66%|██████▌   | 23/35 [00:04<00:02,  4.53it/s]
Capturing CUDA graphs (decode, FULL):  69%|██████▊   | 24/35 [00:04<00:02,  3.83it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 25/35 [00:05<00:02,  3.75it/s]
Capturing CUDA graphs (decode, FULL):  74%|███████▍  | 26/35 [00:05<00:01,  4.54it/s]
Capturing CUDA graphs (decode, FULL):  77%|███████▋  | 27/35 [00:05<00:01,  5.30it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 28/35 [00:05<00:01,  5.95it/s]
Capturing CUDA graphs (decode, FULL):  83%|████████▎ | 29/35 [00:05<00:00,  6.55it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 30/35 [00:05<00:00,  7.04it/s]
Capturing CUDA graphs (decode, FULL):  89%|████████▊ | 31/35 [00:05<00:00,  7.54it/s]
Capturing CUDA graphs (decode, FULL):  91%|█████████▏| 32/35 [00:05<00:00,  7.89it/s]
Capturing CUDA graphs (decode, FULL):  94%|█████████▍| 33/35 [00:06<00:00,  8.10it/s]
Capturing CUDA graphs (decode, FULL):  97%|█████████▋| 34/35 [00:06<00:00,  8.35it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:06<00:00,  8.55it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:06<00:00,  5.58it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 2659.38it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:07<31:25,  7.39s/it, est. speed input: 2.16 toks/s, output: 34.63 toks/s]
Processed prompts:  19%|█▉        | 48/256 [00:07<00:23,  9.02it/s, est. speed input: 102.34 toks/s, output: 1637.37 toks/s]
Processed prompts:  42%|████▏     | 108/256 [00:07<00:06, 24.42it/s, est. speed input: 226.53 toks/s, output: 3624.52 toks/s]
Processed prompts:  59%|█████▉    | 152/256 [00:07<00:02, 39.35it/s, est. speed input: 314.58 toks/s, output: 5033.24 toks/s]
Processed prompts:  75%|███████▍  | 191/256 [00:07<00:01, 56.29it/s, est. speed input: 389.89 toks/s, output: 6238.30 toks/s]
Processed prompts:  89%|████████▉ | 229/256 [00:08<00:00, 73.78it/s, est. speed input: 457.28 toks/s, output: 7316.54 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:08<00:00, 73.78it/s, est. speed input: 492.48 toks/s, output: 7879.64 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:08<00:00, 30.78it/s, est. speed input: 492.48 toks/s, output: 7879.64 toks/s]
[rank0]:[W126 10:43:42.665039030 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-26 10:43:47
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_8
Params: prompt_len=16, output_len=256, num_prompts=512, max_num_seqs=512
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 512 --max-num-seqs 512 --max-model-len 272 --max-num-batched-tokens 512 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Qwen2.5-7B-FP8_M512.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 10:43:54 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 10:43:55 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=540947) WARNING 01-26 10:44:05 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=540947) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=540947) WARNING 01-26 10:44:20 [backends.py:609] Failed to read file <frozen os>
Throughput: 30.76 requests/s, 8365.69 total tokens/s, 7873.59 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072

STDERR:
[2026-01-26 10:43:54] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 10:43:54] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 10:43:54] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 10:43:54] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 10:43:54] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 10:43:54] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 10:43:54] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 10:43:54] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 10:43:54] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 10:43:54] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 10:43:54] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 10:43:54] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 10:43:54] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 10:43:54] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 10:44:03] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 10:44:03] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 10:44:03] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 10:44:03] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 10:44:03] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 10:44:03] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 10:44:03] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 10:44:03] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 10:44:03] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 10:44:03] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 10:44:03] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 10:44:03] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 10:44:03] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 10:44:03] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=540947) [2026-01-26 10:44:05] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=540947) [2026-01-26 10:44:05] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=540947) [2026-01-26 10:44:05] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=540947) [2026-01-26 10:44:05] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=540947) [2026-01-26 10:44:05] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=540947) [2026-01-26 10:44:05] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=540947) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=540947) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.00s/it]
(EngineCore_DP0 pid=540947) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.28s/it]
(EngineCore_DP0 pid=540947) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.24s/it]
(EngineCore_DP0 pid=540947) 
(EngineCore_DP0 pid=540947) [2026-01-26 10:44:09] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5376] -> 1D uint8
(EngineCore_DP0 pid=540947) [2026-01-26 10:44:09] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 18579456 bytes
(EngineCore_DP0 pid=540947) [2026-01-26 10:44:09] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5376] -> 1D uint8
(EngineCore_DP0 pid=540947) [2026-01-26 10:44:09] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14450688 bytes
(EngineCore_DP0 pid=540947) [2026-01-26 10:44:09] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5376] -> 1D uint8
(EngineCore_DP0 pid=540947) [2026-01-26 10:44:10] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 152764416 bytes
(EngineCore_DP0 pid=540947) [2026-01-26 10:44:10] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 28416] -> 1D uint8
(EngineCore_DP0 pid=540947) [2026-01-26 10:44:10] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 76382208 bytes
(EngineCore_DP0 pid=540947) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|▏         | 1/51 [00:00<00:21,  2.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|▍         | 2/51 [00:00<00:12,  3.91it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 3/51 [00:00<00:09,  5.18it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 4/51 [00:00<00:07,  6.09it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|▉         | 5/51 [00:00<00:06,  6.77it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|█▏        | 6/51 [00:01<00:06,  7.17it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▎        | 7/51 [00:01<00:05,  7.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 8/51 [00:01<00:05,  7.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 9/51 [00:01<00:05,  7.81it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|█▉        | 10/51 [00:01<00:05,  7.99it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 11/51 [00:01<00:05,  7.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|██▎       | 12/51 [00:02<00:07,  5.34it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 13/51 [00:02<00:10,  3.59it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 14/51 [00:02<00:08,  4.33it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▉       | 15/51 [00:03<00:11,  3.06it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 16/51 [00:03<00:11,  3.06it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 17/51 [00:03<00:09,  3.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  35%|███▌      | 18/51 [00:03<00:07,  4.48it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 19/51 [00:03<00:06,  5.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|███▉      | 20/51 [00:03<00:05,  5.90it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|████      | 21/51 [00:04<00:04,  6.37it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 22/51 [00:04<00:04,  6.80it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 23/51 [00:04<00:03,  7.17it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 24/51 [00:04<00:03,  7.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▉     | 25/51 [00:04<00:03,  7.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████     | 26/51 [00:04<00:03,  7.93it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 27/51 [00:04<00:03,  7.16it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 28/51 [00:05<00:04,  5.29it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 29/51 [00:05<00:06,  3.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  59%|█████▉    | 30/51 [00:06<00:06,  3.39it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|██████    | 31/51 [00:06<00:07,  2.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 32/51 [00:06<00:05,  3.18it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  65%|██████▍   | 33/51 [00:06<00:04,  3.90it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 34/51 [00:07<00:03,  4.63it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 35/51 [00:07<00:03,  5.33it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████   | 36/51 [00:07<00:02,  5.99it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 37/51 [00:07<00:02,  6.54it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▍  | 38/51 [00:07<00:01,  7.04it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|███████▋  | 39/51 [00:07<00:01,  7.33it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 40/51 [00:07<00:01,  7.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 41/51 [00:07<00:01,  7.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 42/51 [00:08<00:01,  7.13it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 43/51 [00:08<00:01,  5.58it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▋ | 44/51 [00:08<00:01,  3.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  88%|████████▊ | 45/51 [00:08<00:01,  4.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|█████████ | 46/51 [00:09<00:01,  4.35it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 47/51 [00:09<00:01,  3.01it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 48/51 [00:09<00:00,  3.73it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  96%|█████████▌| 49/51 [00:09<00:00,  4.45it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  98%|█████████▊| 50/51 [00:10<00:00,  5.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:10<00:00,  5.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:10<00:00,  4.99it/s]
(EngineCore_DP0 pid=540947) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   2%|▏         | 1/51 [00:00<00:10,  4.59it/s]
Capturing CUDA graphs (decode, FULL):   4%|▍         | 2/51 [00:00<00:08,  5.74it/s]
Capturing CUDA graphs (decode, FULL):   6%|▌         | 3/51 [00:00<00:07,  6.44it/s]
Capturing CUDA graphs (decode, FULL):   8%|▊         | 4/51 [00:00<00:06,  7.09it/s]
Capturing CUDA graphs (decode, FULL):  10%|▉         | 5/51 [00:00<00:06,  7.45it/s]
Capturing CUDA graphs (decode, FULL):  12%|█▏        | 6/51 [00:00<00:05,  7.75it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▎        | 7/51 [00:00<00:05,  8.02it/s]
Capturing CUDA graphs (decode, FULL):  16%|█▌        | 8/51 [00:01<00:05,  7.31it/s]
Capturing CUDA graphs (decode, FULL):  18%|█▊        | 9/51 [00:01<00:11,  3.67it/s]
Capturing CUDA graphs (decode, FULL):  20%|█▉        | 10/51 [00:01<00:09,  4.46it/s]
Capturing CUDA graphs (decode, FULL):  22%|██▏       | 11/51 [00:02<00:09,  4.34it/s]
Capturing CUDA graphs (decode, FULL):  24%|██▎       | 12/51 [00:02<00:11,  3.28it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 13/51 [00:02<00:10,  3.61it/s]
Capturing CUDA graphs (decode, FULL):  27%|██▋       | 14/51 [00:02<00:08,  4.36it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▉       | 15/51 [00:02<00:07,  5.09it/s]
Capturing CUDA graphs (decode, FULL):  31%|███▏      | 16/51 [00:03<00:06,  5.78it/s]
Capturing CUDA graphs (decode, FULL):  33%|███▎      | 17/51 [00:03<00:05,  6.37it/s]
Capturing CUDA graphs (decode, FULL):  35%|███▌      | 18/51 [00:03<00:04,  6.89it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 19/51 [00:03<00:04,  7.31it/s]
Capturing CUDA graphs (decode, FULL):  39%|███▉      | 20/51 [00:03<00:04,  7.63it/s]
Capturing CUDA graphs (decode, FULL):  41%|████      | 21/51 [00:03<00:03,  7.87it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 22/51 [00:03<00:03,  8.05it/s]
Capturing CUDA graphs (decode, FULL):  45%|████▌     | 23/51 [00:03<00:03,  8.20it/s]
Capturing CUDA graphs (decode, FULL):  47%|████▋     | 24/51 [00:04<00:03,  7.80it/s]
Capturing CUDA graphs (decode, FULL):  49%|████▉     | 25/51 [00:04<00:03,  6.67it/s]
Capturing CUDA graphs (decode, FULL):  51%|█████     | 26/51 [00:04<00:06,  3.60it/s]
Capturing CUDA graphs (decode, FULL):  53%|█████▎    | 27/51 [00:05<00:05,  4.11it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 28/51 [00:05<00:06,  3.57it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 29/51 [00:05<00:06,  3.16it/s]
Capturing CUDA graphs (decode, FULL):  59%|█████▉    | 30/51 [00:05<00:05,  3.90it/s]
Capturing CUDA graphs (decode, FULL):  61%|██████    | 31/51 [00:06<00:04,  4.63it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 32/51 [00:06<00:03,  5.36it/s]
Capturing CUDA graphs (decode, FULL):  65%|██████▍   | 33/51 [00:06<00:02,  6.05it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 34/51 [00:06<00:02,  6.54it/s]
Capturing CUDA graphs (decode, FULL):  69%|██████▊   | 35/51 [00:06<00:02,  7.02it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████   | 36/51 [00:06<00:02,  7.43it/s]
Capturing CUDA graphs (decode, FULL):  73%|███████▎  | 37/51 [00:06<00:01,  7.74it/s]
Capturing CUDA graphs (decode, FULL):  75%|███████▍  | 38/51 [00:06<00:01,  7.95it/s]
Capturing CUDA graphs (decode, FULL):  76%|███████▋  | 39/51 [00:06<00:01,  8.12it/s]
Capturing CUDA graphs (decode, FULL):  78%|███████▊  | 40/51 [00:07<00:01,  8.17it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 41/51 [00:07<00:01,  7.55it/s]
Capturing CUDA graphs (decode, FULL):  82%|████████▏ | 42/51 [00:07<00:01,  6.64it/s]
Capturing CUDA graphs (decode, FULL):  96%|█████████▌| 49/51 [00:07<00:00, 18.22it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:07<00:00, 14.54it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:07<00:00,  6.52it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:  52%|█████▏    | 268/512 [00:00<00:00, 2678.02it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 2708.97it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:12<1:42:49, 12.07s/it, est. speed input: 1.33 toks/s, output: 21.20 toks/s]
Processed prompts:   6%|▋         | 33/512 [00:12<02:06,  3.79it/s, est. speed input: 43.14 toks/s, output: 690.19 toks/s]
Processed prompts:  18%|█▊        | 91/512 [00:12<00:32, 13.08it/s, est. speed input: 117.51 toks/s, output: 1880.23 toks/s]
Processed prompts:  28%|██▊       | 141/512 [00:12<00:15, 24.03it/s, est. speed input: 180.48 toks/s, output: 2887.72 toks/s]
Processed prompts:  40%|████      | 205/512 [00:12<00:07, 43.05it/s, est. speed input: 260.03 toks/s, output: 4160.50 toks/s]
Processed prompts:  50%|█████     | 258/512 [00:12<00:03, 63.71it/s, est. speed input: 324.66 toks/s, output: 5194.55 toks/s]
Processed prompts:  62%|██████▏   | 315/512 [00:12<00:02, 92.19it/s, est. speed input: 393.02 toks/s, output: 6288.34 toks/s]
Processed prompts:  72%|███████▏  | 367/512 [00:12<00:01, 122.85it/s, est. speed input: 453.79 toks/s, output: 7260.56 toks/s]
Processed prompts:  81%|████████  | 415/512 [00:13<00:00, 154.22it/s, est. speed input: 508.44 toks/s, output: 8135.06 toks/s]
Processed prompts:  90%|█████████ | 461/512 [00:13<00:00, 172.14it/s, est. speed input: 556.71 toks/s, output: 8907.29 toks/s]
Processed prompts:  98%|█████████▊| 500/512 [00:16<00:00, 39.68it/s, est. speed input: 487.40 toks/s, output: 7798.47 toks/s] 
Processed prompts: 100%|██████████| 512/512 [00:16<00:00, 39.68it/s, est. speed input: 497.81 toks/s, output: 7964.89 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:16<00:00, 31.11it/s, est. speed input: 497.81 toks/s, output: 7964.89 toks/s]
[rank0]:[W126 10:45:14.555935198 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=64 ==========
Time: 2026-01-26 11:49:12
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_8
Params: prompt_len=16, output_len=256, num_prompts=64, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 64 --max-num-seqs 64 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Qwen2.5-14B-FP8_M64.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 11:49:20 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 11:49:21 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=603836) WARNING 01-26 11:49:30 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=603836) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=603836) WARNING 01-26 11:50:05 [backends.py:609] Failed to read file <frozen os>
Throughput: 3.34 requests/s, 909.31 total tokens/s, 855.82 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384

STDERR:
[2026-01-26 11:49:20] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:49:20] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 11:49:20] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 11:49:20] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 11:49:20] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 11:49:20] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 11:49:20] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 11:49:20] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 11:49:20] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 11:49:20] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:49:20] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:49:20] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:49:20] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:49:20] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 11:49:29] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:49:29] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 11:49:29] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 11:49:29] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 11:49:29] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 11:49:29] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 11:49:29] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 11:49:29] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 11:49:29] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 11:49:29] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:49:29] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:49:29] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:49:29] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:49:29] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=603836) [2026-01-26 11:49:30] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=603836) [2026-01-26 11:49:30] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=603836) [2026-01-26 11:49:30] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=603836) [2026-01-26 11:49:30] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=603836) [2026-01-26 11:49:30] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=603836) [2026-01-26 11:49:30] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=603836) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=603836) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:04<00:12,  4.15s/it]
(EngineCore_DP0 pid=603836) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:09<00:09,  4.76s/it]
(EngineCore_DP0 pid=603836) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:10<00:03,  3.19s/it]
(EngineCore_DP0 pid=603836) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:16<00:00,  4.13s/it]
(EngineCore_DP0 pid=603836) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:16<00:00,  4.06s/it]
(EngineCore_DP0 pid=603836) 
(EngineCore_DP0 pid=603836) [2026-01-26 11:49:49] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 7680] -> 1D uint8
(EngineCore_DP0 pid=603836) [2026-01-26 11:49:49] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 41287680 bytes
(EngineCore_DP0 pid=603836) [2026-01-26 11:49:49] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 7680] -> 1D uint8
(EngineCore_DP0 pid=603836) [2026-01-26 11:49:49] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 29491200 bytes
(EngineCore_DP0 pid=603836) [2026-01-26 11:49:49] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 7680] -> 1D uint8
(EngineCore_DP0 pid=603836) [2026-01-26 11:49:50] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 159252480 bytes
(EngineCore_DP0 pid=603836) [2026-01-26 11:49:50] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 20736] -> 1D uint8
(EngineCore_DP0 pid=603836) [2026-01-26 11:49:50] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 79626240 bytes
(EngineCore_DP0 pid=603836) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   5%|▌         | 1/19 [00:00<00:08,  2.18it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 2/19 [00:00<00:06,  2.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 3/19 [00:00<00:04,  3.34it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  21%|██        | 4/19 [00:01<00:03,  3.88it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▋       | 5/19 [00:01<00:03,  3.58it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  32%|███▏      | 6/19 [00:02<00:05,  2.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 7/19 [00:03<00:06,  1.77it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 8/19 [00:03<00:04,  2.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 9/19 [00:03<00:03,  2.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 10/19 [00:03<00:02,  3.15it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 11/19 [00:03<00:02,  3.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 12/19 [00:04<00:01,  3.92it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  68%|██████▊   | 13/19 [00:04<00:01,  4.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▎  | 14/19 [00:04<00:01,  4.16it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  79%|███████▉  | 15/19 [00:05<00:01,  2.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 16/19 [00:05<00:01,  2.60it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 17/19 [00:06<00:00,  2.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  95%|█████████▍| 18/19 [00:06<00:00,  2.66it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:06<00:00,  3.04it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:06<00:00,  2.85it/s]
(EngineCore_DP0 pid=603836) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   9%|▉         | 1/11 [00:00<00:02,  4.75it/s]
Capturing CUDA graphs (decode, FULL):  18%|█▊        | 2/11 [00:00<00:01,  5.08it/s]
Capturing CUDA graphs (decode, FULL):  27%|██▋       | 3/11 [00:00<00:01,  5.18it/s]
Capturing CUDA graphs (decode, FULL):  36%|███▋      | 4/11 [00:00<00:01,  5.18it/s]
Capturing CUDA graphs (decode, FULL):  45%|████▌     | 5/11 [00:01<00:01,  4.79it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 6/11 [00:01<00:01,  2.67it/s]
Capturing CUDA graphs (decode, FULL):  64%|██████▎   | 7/11 [00:01<00:01,  3.03it/s]
Capturing CUDA graphs (decode, FULL):  73%|███████▎  | 8/11 [00:02<00:01,  2.66it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:01<00:00,  6.85it/s]

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 64/64 [00:00<00:00, 2443.41it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 1/64 [00:07<07:23,  7.03s/it, est. speed input: 2.27 toks/s, output: 36.40 toks/s]
Processed prompts:  28%|██▊       | 18/64 [00:07<00:13,  3.50it/s, est. speed input: 40.18 toks/s, output: 642.90 toks/s]
Processed prompts:  41%|████      | 26/64 [00:11<00:15,  2.53it/s, est. speed input: 35.62 toks/s, output: 569.94 toks/s]
Processed prompts:  48%|████▊     | 31/64 [00:13<00:12,  2.74it/s, est. speed input: 37.93 toks/s, output: 606.93 toks/s]
Processed prompts:  53%|█████▎    | 34/64 [00:13<00:10,  2.92it/s, est. speed input: 39.39 toks/s, output: 630.31 toks/s]
Processed prompts:  56%|█████▋    | 36/64 [00:14<00:09,  2.92it/s, est. speed input: 39.76 toks/s, output: 636.11 toks/s]
Processed prompts:  59%|█████▉    | 38/64 [00:14<00:07,  3.37it/s, est. speed input: 41.50 toks/s, output: 663.98 toks/s]
Processed prompts:  64%|██████▍   | 41/64 [00:15<00:05,  3.98it/s, est. speed input: 43.63 toks/s, output: 698.12 toks/s]
Processed prompts:  72%|███████▏  | 46/64 [00:15<00:03,  5.38it/s, est. speed input: 47.60 toks/s, output: 761.58 toks/s]
Processed prompts:  75%|███████▌  | 48/64 [00:15<00:02,  6.06it/s, est. speed input: 49.22 toks/s, output: 787.49 toks/s]
Processed prompts:  78%|███████▊  | 50/64 [00:16<00:03,  4.17it/s, est. speed input: 48.02 toks/s, output: 768.39 toks/s]
Processed prompts:  80%|███████▉  | 51/64 [00:17<00:03,  3.63it/s, est. speed input: 47.52 toks/s, output: 760.37 toks/s]
Processed prompts:  81%|████████▏ | 52/64 [00:17<00:03,  3.67it/s, est. speed input: 47.74 toks/s, output: 763.85 toks/s]
Processed prompts:  83%|████████▎ | 53/64 [00:17<00:02,  3.96it/s, est. speed input: 48.20 toks/s, output: 771.26 toks/s]
Processed prompts:  84%|████████▍ | 54/64 [00:17<00:02,  3.67it/s, est. speed input: 48.16 toks/s, output: 770.52 toks/s]
Processed prompts:  86%|████████▌ | 55/64 [00:18<00:02,  3.88it/s, est. speed input: 48.49 toks/s, output: 775.82 toks/s]
Processed prompts:  89%|████████▉ | 57/64 [00:18<00:01,  5.09it/s, est. speed input: 49.69 toks/s, output: 795.06 toks/s]
Processed prompts:  91%|█████████ | 58/64 [00:18<00:01,  5.46it/s, est. speed input: 50.19 toks/s, output: 803.05 toks/s]
Processed prompts:  94%|█████████▍| 60/64 [00:18<00:00,  6.78it/s, est. speed input: 51.42 toks/s, output: 822.69 toks/s]
Processed prompts:  95%|█████████▌| 61/64 [00:18<00:00,  6.68it/s, est. speed input: 51.84 toks/s, output: 829.41 toks/s]
Processed prompts:  97%|█████████▋| 62/64 [00:18<00:00,  6.85it/s, est. speed input: 52.32 toks/s, output: 837.04 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:19<00:00,  8.45it/s, est. speed input: 53.57 toks/s, output: 857.07 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:19<00:00,  8.45it/s, est. speed input: 53.57 toks/s, output: 857.07 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:19<00:00,  3.35it/s, est. speed input: 53.57 toks/s, output: 857.07 toks/s]
[rank0]:[W126 11:50:55.900807565 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-26 11:50:59
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_8
Params: prompt_len=16, output_len=256, num_prompts=128, max_num_seqs=128
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 128 --max-num-seqs 128 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Qwen2.5-14B-FP8_M128.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 11:51:06 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 11:51:07 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=605537) WARNING 01-26 11:51:25 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=605537) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=605537) WARNING 01-26 11:52:00 [backends.py:609] Failed to read file <frozen os>
Throughput: 6.44 requests/s, 1751.41 total tokens/s, 1648.38 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768

STDERR:
[2026-01-26 11:51:05] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:51:06] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 11:51:06] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 11:51:06] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 11:51:06] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 11:51:06] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 11:51:06] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 11:51:06] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 11:51:06] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 11:51:06] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:51:06] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:51:06] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:51:06] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:51:06] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 11:51:14] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:51:14] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 11:51:14] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 11:51:14] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 11:51:14] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 11:51:14] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 11:51:14] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 11:51:14] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 11:51:14] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 11:51:14] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:51:14] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:51:14] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:51:14] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:51:14] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[W126 11:51:24.349447071 socket.cpp:209] [c10d] The hostname of the client socket cannot be retrieved. err=-3
(EngineCore_DP0 pid=605537) [2026-01-26 11:51:26] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=605537) [2026-01-26 11:51:26] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=605537) [2026-01-26 11:51:26] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=605537) [2026-01-26 11:51:26] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=605537) [2026-01-26 11:51:26] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=605537) [2026-01-26 11:51:26] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=605537) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=605537) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:05<00:17,  5.76s/it]
(EngineCore_DP0 pid=605537) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:09<00:09,  4.63s/it]
(EngineCore_DP0 pid=605537) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:10<00:03,  3.14s/it]
(EngineCore_DP0 pid=605537) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:16<00:00,  4.14s/it]
(EngineCore_DP0 pid=605537) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:16<00:00,  4.16s/it]
(EngineCore_DP0 pid=605537) 
(EngineCore_DP0 pid=605537) [2026-01-26 11:51:43] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 7680] -> 1D uint8
(EngineCore_DP0 pid=605537) [2026-01-26 11:51:43] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 41287680 bytes
(EngineCore_DP0 pid=605537) [2026-01-26 11:51:43] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 7680] -> 1D uint8
(EngineCore_DP0 pid=605537) [2026-01-26 11:51:43] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 29491200 bytes
(EngineCore_DP0 pid=605537) [2026-01-26 11:51:43] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 7680] -> 1D uint8
(EngineCore_DP0 pid=605537) [2026-01-26 11:51:44] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 159252480 bytes
(EngineCore_DP0 pid=605537) [2026-01-26 11:51:44] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 20736] -> 1D uint8
(EngineCore_DP0 pid=605537) [2026-01-26 11:51:44] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 79626240 bytes
(EngineCore_DP0 pid=605537) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|▎         | 1/35 [00:00<00:32,  1.05it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 2/35 [00:01<00:27,  1.18it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▊         | 3/35 [00:01<00:17,  1.80it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█▏        | 4/35 [00:02<00:12,  2.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 5/35 [00:02<00:09,  3.01it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 6/35 [00:02<00:08,  3.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 7/35 [00:02<00:07,  3.92it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  23%|██▎       | 8/35 [00:02<00:06,  4.24it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▌       | 9/35 [00:03<00:06,  4.22it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 10/35 [00:03<00:10,  2.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 11/35 [00:04<00:12,  1.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  34%|███▍      | 12/35 [00:04<00:10,  2.25it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 13/35 [00:05<00:08,  2.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 14/35 [00:05<00:06,  3.15it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 15/35 [00:05<00:05,  3.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  46%|████▌     | 16/35 [00:05<00:04,  3.93it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▊     | 17/35 [00:05<00:04,  4.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████▏    | 18/35 [00:06<00:04,  4.15it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  54%|█████▍    | 19/35 [00:07<00:06,  2.36it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 20/35 [00:07<00:07,  2.06it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 21/35 [00:08<00:06,  2.10it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 22/35 [00:08<00:05,  2.55it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  66%|██████▌   | 23/35 [00:08<00:03,  3.00it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 24/35 [00:08<00:03,  3.41it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 25/35 [00:08<00:02,  3.79it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▍  | 26/35 [00:09<00:02,  4.13it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  77%|███████▋  | 27/35 [00:09<00:01,  4.17it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 28/35 [00:10<00:02,  2.48it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  83%|████████▎ | 29/35 [00:10<00:02,  2.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 30/35 [00:11<00:02,  2.09it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▊ | 31/35 [00:11<00:01,  2.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████▏| 32/35 [00:11<00:01,  2.99it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 33/35 [00:11<00:00,  3.44it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  97%|█████████▋| 34/35 [00:11<00:00,  3.84it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:12<00:00,  4.06it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:12<00:00,  2.87it/s]
(EngineCore_DP0 pid=605537) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   5%|▌         | 1/19 [00:00<00:04,  3.81it/s]
Capturing CUDA graphs (decode, FULL):  11%|█         | 2/19 [00:00<00:05,  3.39it/s]
Capturing CUDA graphs (decode, FULL):  16%|█▌        | 3/19 [00:01<00:06,  2.58it/s]
Capturing CUDA graphs (decode, FULL):  21%|██        | 4/19 [00:01<00:04,  3.03it/s]
Capturing CUDA graphs (decode, FULL):  26%|██▋       | 5/19 [00:01<00:05,  2.62it/s]
Capturing CUDA graphs (decode, FULL):  32%|███▏      | 6/19 [00:02<00:05,  2.56it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 7/19 [00:02<00:03,  3.07it/s]
Capturing CUDA graphs (decode, FULL):  42%|████▏     | 8/19 [00:02<00:03,  3.55it/s]
Capturing CUDA graphs (decode, FULL):  47%|████▋     | 9/19 [00:02<00:02,  3.99it/s]
Capturing CUDA graphs (decode, FULL):  53%|█████▎    | 10/19 [00:02<00:02,  4.33it/s]
Capturing CUDA graphs (decode, FULL):  58%|█████▊    | 11/19 [00:03<00:01,  4.61it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 12/19 [00:03<00:01,  4.78it/s]
Capturing CUDA graphs (decode, FULL):  68%|██████▊   | 13/19 [00:03<00:01,  4.32it/s]
Capturing CUDA graphs (decode, FULL):  74%|███████▎  | 14/19 [00:04<00:01,  2.93it/s]
Capturing CUDA graphs (decode, FULL):  79%|███████▉  | 15/19 [00:04<00:01,  3.07it/s]
Capturing CUDA graphs (decode, FULL):  84%|████████▍ | 16/19 [00:05<00:01,  2.49it/s]
Capturing CUDA graphs (decode, FULL):  89%|████████▉ | 17/19 [00:05<00:00,  2.97it/s]
Capturing CUDA graphs (decode, FULL):  95%|█████████▍| 18/19 [00:05<00:00,  3.44it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:05<00:00,  3.88it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:05<00:00,  3.38it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 2600.33it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:06<13:23,  6.33s/it, est. speed input: 2.53 toks/s, output: 40.45 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:06<00:28,  3.88it/s, est. speed input: 44.54 toks/s, output: 712.65 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:06<00:12,  7.49it/s, est. speed input: 76.21 toks/s, output: 1219.41 toks/s]
Processed prompts:  30%|███       | 39/128 [00:08<00:14,  6.13it/s, est. speed input: 73.63 toks/s, output: 1178.01 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:09<00:15,  5.59it/s, est. speed input: 72.67 toks/s, output: 1162.74 toks/s]
Processed prompts:  36%|███▌      | 46/128 [00:09<00:13,  6.17it/s, est. speed input: 75.88 toks/s, output: 1214.13 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:10<00:14,  5.63it/s, est. speed input: 74.92 toks/s, output: 1198.70 toks/s]
Processed prompts:  39%|███▉      | 50/128 [00:10<00:12,  6.37it/s, est. speed input: 77.27 toks/s, output: 1236.38 toks/s]
Processed prompts:  41%|████      | 52/128 [00:10<00:12,  5.93it/s, est. speed input: 77.11 toks/s, output: 1233.70 toks/s]
Processed prompts:  42%|████▏     | 54/128 [00:10<00:10,  6.80it/s, est. speed input: 79.09 toks/s, output: 1265.44 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:11<00:11,  6.22it/s, est. speed input: 79.04 toks/s, output: 1264.70 toks/s]
Processed prompts:  45%|████▌     | 58/128 [00:11<00:09,  7.51it/s, est. speed input: 81.14 toks/s, output: 1298.28 toks/s]
Processed prompts:  47%|████▋     | 60/128 [00:11<00:08,  8.03it/s, est. speed input: 82.50 toks/s, output: 1319.93 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:12<00:11,  5.94it/s, est. speed input: 81.26 toks/s, output: 1300.13 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:12<00:08,  7.39it/s, est. speed input: 83.20 toks/s, output: 1331.16 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:12<00:07,  8.30it/s, est. speed input: 84.65 toks/s, output: 1354.36 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:12<00:06,  9.95it/s, est. speed input: 86.50 toks/s, output: 1383.97 toks/s]
Processed prompts:  55%|█████▍    | 70/128 [00:12<00:07,  8.07it/s, est. speed input: 86.56 toks/s, output: 1384.88 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:13<00:05, 10.59it/s, est. speed input: 89.28 toks/s, output: 1428.53 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:13<00:03, 14.96it/s, est. speed input: 93.34 toks/s, output: 1493.43 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:13<00:03, 12.43it/s, est. speed input: 94.60 toks/s, output: 1513.63 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:13<00:02, 15.16it/s, est. speed input: 97.43 toks/s, output: 1558.80 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:13<00:02, 18.85it/s, est. speed input: 101.20 toks/s, output: 1619.18 toks/s]
Processed prompts:  70%|███████   | 90/128 [00:14<00:04,  8.49it/s, est. speed input: 98.64 toks/s, output: 1578.25 toks/s] 
Processed prompts:  72%|███████▏  | 92/128 [00:15<00:06,  5.78it/s, est. speed input: 95.88 toks/s, output: 1534.11 toks/s]
Processed prompts:  73%|███████▎  | 94/128 [00:16<00:07,  4.74it/s, est. speed input: 93.85 toks/s, output: 1501.60 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:16<00:05,  5.45it/s, est. speed input: 94.66 toks/s, output: 1514.52 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:16<00:04,  6.07it/s, est. speed input: 95.32 toks/s, output: 1525.08 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:16<00:04,  6.18it/s, est. speed input: 95.43 toks/s, output: 1526.95 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:16<00:03,  7.44it/s, est. speed input: 96.51 toks/s, output: 1544.15 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:16<00:02,  8.94it/s, est. speed input: 97.71 toks/s, output: 1563.30 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:16<00:02, 10.35it/s, est. speed input: 98.88 toks/s, output: 1582.00 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:17<00:02, 10.25it/s, est. speed input: 99.59 toks/s, output: 1593.45 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:17<00:01, 10.26it/s, est. speed input: 100.32 toks/s, output: 1605.07 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:17<00:01, 10.01it/s, est. speed input: 101.26 toks/s, output: 1620.23 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:17<00:00, 16.25it/s, est. speed input: 105.71 toks/s, output: 1691.36 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:18<00:00, 15.73it/s, est. speed input: 107.15 toks/s, output: 1714.47 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:18<00:00, 16.07it/s, est. speed input: 108.24 toks/s, output: 1731.87 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:18<00:00, 17.53it/s, est. speed input: 110.06 toks/s, output: 1760.93 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:18<00:00, 17.53it/s, est. speed input: 111.40 toks/s, output: 1782.34 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:18<00:00,  6.96it/s, est. speed input: 111.40 toks/s, output: 1782.34 toks/s]
[rank0]:[W126 11:52:56.285920375 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-26 11:52:59
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_8
Params: prompt_len=16, output_len=256, num_prompts=256, max_num_seqs=256
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 256 --max-num-seqs 256 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Qwen2.5-14B-FP8_M256.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 11:53:07 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 11:53:08 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=607394) WARNING 01-26 11:53:26 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=607394) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=607394) WARNING 01-26 11:53:49 [backends.py:609] Failed to read file <frozen os>
Throughput: 4.67 requests/s, 1270.05 total tokens/s, 1195.34 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536

STDERR:
[2026-01-26 11:53:07] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:53:07] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 11:53:07] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 11:53:07] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 11:53:07] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 11:53:07] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 11:53:07] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 11:53:07] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 11:53:07] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 11:53:07] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:53:07] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:53:07] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:53:07] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:53:07] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 11:53:14] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:53:14] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 11:53:14] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 11:53:14] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 11:53:14] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 11:53:14] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 11:53:14] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 11:53:14] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 11:53:14] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 11:53:14] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:53:14] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:53:14] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:53:14] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:53:14] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[W126 11:53:26.977675743 socket.cpp:209] [c10d] The hostname of the client socket cannot be retrieved. err=-3
(EngineCore_DP0 pid=607394) [2026-01-26 11:53:27] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=607394) [2026-01-26 11:53:27] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=607394) [2026-01-26 11:53:27] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=607394) [2026-01-26 11:53:27] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=607394) [2026-01-26 11:53:27] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=607394) [2026-01-26 11:53:27] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=607394) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=607394) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.45s/it]
(EngineCore_DP0 pid=607394) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:02<00:02,  1.45s/it]
(EngineCore_DP0 pid=607394) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:00,  1.00it/s]
(EngineCore_DP0 pid=607394) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.15s/it]
(EngineCore_DP0 pid=607394) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.19s/it]
(EngineCore_DP0 pid=607394) 
(EngineCore_DP0 pid=607394) [2026-01-26 11:53:34] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 7680] -> 1D uint8
(EngineCore_DP0 pid=607394) [2026-01-26 11:53:34] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 41287680 bytes
(EngineCore_DP0 pid=607394) [2026-01-26 11:53:34] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 7680] -> 1D uint8
(EngineCore_DP0 pid=607394) [2026-01-26 11:53:34] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 29491200 bytes
(EngineCore_DP0 pid=607394) [2026-01-26 11:53:34] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 7680] -> 1D uint8
(EngineCore_DP0 pid=607394) [2026-01-26 11:53:35] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 159252480 bytes
(EngineCore_DP0 pid=607394) [2026-01-26 11:53:35] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 20736] -> 1D uint8
(EngineCore_DP0 pid=607394) [2026-01-26 11:53:35] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 79626240 bytes
(EngineCore_DP0 pid=607394) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/36 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|▎         | 1/36 [00:00<00:10,  3.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 2/36 [00:00<00:08,  4.17it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 3/36 [00:00<00:07,  4.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 4/36 [00:00<00:06,  4.65it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 5/36 [00:01<00:06,  4.82it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 6/36 [00:01<00:08,  3.54it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  19%|█▉        | 7/36 [00:02<00:11,  2.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 8/36 [00:03<00:15,  1.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 9/36 [00:03<00:11,  2.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  28%|██▊       | 10/36 [00:03<00:09,  2.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███       | 11/36 [00:03<00:07,  3.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 12/36 [00:03<00:06,  3.59it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▌      | 13/36 [00:04<00:05,  3.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 17/36 [00:04<00:03,  5.42it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 18/36 [00:04<00:03,  5.13it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 19/36 [00:05<00:03,  5.12it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  56%|█████▌    | 20/36 [00:05<00:03,  5.11it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 21/36 [00:05<00:02,  5.10it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|██████    | 22/36 [00:05<00:02,  5.06it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▍   | 23/36 [00:05<00:02,  5.05it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 24/36 [00:06<00:02,  4.15it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▉   | 25/36 [00:06<00:04,  2.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  72%|███████▏  | 26/36 [00:07<00:05,  1.92it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 27/36 [00:08<00:03,  2.26it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 28/36 [00:08<00:02,  2.68it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  81%|████████  | 29/36 [00:08<00:02,  3.11it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  83%|████████▎ | 30/36 [00:08<00:01,  3.48it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 31/36 [00:08<00:01,  3.84it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 32/36 [00:09<00:00,  4.13it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 33/36 [00:09<00:00,  4.00it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 34/36 [00:10<00:00,  2.48it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  97%|█████████▋| 35/36 [00:10<00:00,  2.34it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:11<00:00,  2.24it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:11<00:00,  3.26it/s]
(EngineCore_DP0 pid=607394) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   3%|▎         | 1/35 [00:00<00:11,  3.02it/s]
Capturing CUDA graphs (decode, FULL):   6%|▌         | 2/35 [00:00<00:08,  4.03it/s]
Capturing CUDA graphs (decode, FULL):   9%|▊         | 3/35 [00:00<00:07,  4.48it/s]
Capturing CUDA graphs (decode, FULL):  11%|█▏        | 4/35 [00:00<00:06,  4.74it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 5/35 [00:01<00:06,  4.82it/s]
Capturing CUDA graphs (decode, FULL):  17%|█▋        | 6/35 [00:01<00:05,  4.92it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 7/35 [00:01<00:06,  4.38it/s]
Capturing CUDA graphs (decode, FULL):  23%|██▎       | 8/35 [00:02<00:08,  3.12it/s]
Capturing CUDA graphs (decode, FULL):  26%|██▌       | 9/35 [00:02<00:08,  3.24it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 10/35 [00:03<00:10,  2.41it/s]
Capturing CUDA graphs (decode, FULL):  31%|███▏      | 11/35 [00:03<00:08,  2.76it/s]
Capturing CUDA graphs (decode, FULL):  34%|███▍      | 12/35 [00:03<00:07,  3.23it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 13/35 [00:03<00:06,  3.65it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 14/35 [00:03<00:05,  4.02it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 15/35 [00:04<00:04,  4.25it/s]
Capturing CUDA graphs (decode, FULL):  46%|████▌     | 16/35 [00:04<00:04,  4.46it/s]
Capturing CUDA graphs (decode, FULL):  49%|████▊     | 17/35 [00:04<00:03,  4.67it/s]
Capturing CUDA graphs (decode, FULL):  51%|█████▏    | 18/35 [00:04<00:03,  4.33it/s]
Capturing CUDA graphs (decode, FULL):  54%|█████▍    | 19/35 [00:05<00:05,  3.18it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 20/35 [00:05<00:05,  2.96it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 21/35 [00:05<00:04,  2.99it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 22/35 [00:06<00:04,  2.67it/s]
Capturing CUDA graphs (decode, FULL):  66%|██████▌   | 23/35 [00:06<00:03,  3.15it/s]
Capturing CUDA graphs (decode, FULL):  69%|██████▊   | 24/35 [00:06<00:03,  3.59it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 25/35 [00:06<00:02,  3.97it/s]
Capturing CUDA graphs (decode, FULL):  74%|███████▍  | 26/35 [00:07<00:02,  4.30it/s]
Capturing CUDA graphs (decode, FULL):  77%|███████▋  | 27/35 [00:07<00:01,  4.57it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 28/35 [00:07<00:01,  4.63it/s]
Capturing CUDA graphs (decode, FULL):  83%|████████▎ | 29/35 [00:07<00:01,  4.29it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 30/35 [00:08<00:01,  3.03it/s]
Capturing CUDA graphs (decode, FULL):  89%|████████▊ | 31/35 [00:08<00:01,  3.15it/s]
Capturing CUDA graphs (decode, FULL):  91%|█████████▏| 32/35 [00:09<00:01,  2.83it/s]
Capturing CUDA graphs (decode, FULL):  94%|█████████▍| 33/35 [00:09<00:00,  3.03it/s]
Capturing CUDA graphs (decode, FULL):  97%|█████████▋| 34/35 [00:09<00:00,  3.50it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:09<00:00,  3.92it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:09<00:00,  3.59it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 2704.90it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:06<28:50,  6.79s/it, est. speed input: 2.36 toks/s, output: 37.73 toks/s]
Processed prompts:   7%|▋         | 19/256 [00:07<01:06,  3.55it/s, est. speed input: 41.63 toks/s, output: 666.00 toks/s]
Processed prompts:   9%|▊         | 22/256 [00:08<01:08,  3.44it/s, est. speed input: 42.45 toks/s, output: 679.17 toks/s]
Processed prompts:   9%|▉         | 24/256 [00:08<01:05,  3.55it/s, est. speed input: 43.86 toks/s, output: 701.72 toks/s]
Processed prompts:  10%|▉         | 25/256 [00:09<01:07,  3.41it/s, est. speed input: 43.67 toks/s, output: 698.73 toks/s]
Processed prompts:  11%|█         | 27/256 [00:09<01:04,  3.58it/s, est. speed input: 44.92 toks/s, output: 718.74 toks/s]
Processed prompts:  11%|█▏        | 29/256 [00:09<00:51,  4.39it/s, est. speed input: 47.64 toks/s, output: 762.18 toks/s]
Processed prompts:  12%|█▏        | 30/256 [00:10<00:56,  3.97it/s, est. speed input: 47.37 toks/s, output: 757.86 toks/s]
Processed prompts:  12%|█▎        | 32/256 [00:10<00:55,  4.04it/s, est. speed input: 48.26 toks/s, output: 772.18 toks/s]
Processed prompts:  13%|█▎        | 34/256 [00:10<00:43,  5.15it/s, est. speed input: 50.64 toks/s, output: 810.27 toks/s]
Processed prompts:  14%|█▍        | 36/256 [00:11<00:44,  4.91it/s, est. speed input: 51.45 toks/s, output: 823.24 toks/s]
Processed prompts:  16%|█▌        | 40/256 [00:11<00:35,  6.07it/s, est. speed input: 54.85 toks/s, output: 877.56 toks/s]
Processed prompts:  17%|█▋        | 44/256 [00:11<00:24,  8.67it/s, est. speed input: 59.51 toks/s, output: 952.24 toks/s]
Processed prompts:  20%|█▉        | 50/256 [00:12<00:17, 11.46it/s, est. speed input: 65.82 toks/s, output: 1053.16 toks/s]
Processed prompts:  20%|██        | 52/256 [00:12<00:28,  7.14it/s, est. speed input: 64.41 toks/s, output: 1030.63 toks/s]
Processed prompts:  21%|██        | 54/256 [00:13<00:34,  5.78it/s, est. speed input: 63.90 toks/s, output: 1022.46 toks/s]
Processed prompts:  21%|██▏       | 55/256 [00:13<00:36,  5.45it/s, est. speed input: 63.89 toks/s, output: 1022.19 toks/s]
Processed prompts:  22%|██▏       | 56/256 [00:13<00:37,  5.35it/s, est. speed input: 64.09 toks/s, output: 1025.45 toks/s]
Processed prompts:  22%|██▏       | 57/256 [00:14<00:36,  5.52it/s, est. speed input: 64.52 toks/s, output: 1032.33 toks/s]
Processed prompts:  23%|██▎       | 58/256 [00:14<00:33,  5.85it/s, est. speed input: 65.05 toks/s, output: 1040.74 toks/s]
Processed prompts:  24%|██▍       | 61/256 [00:14<00:21,  8.93it/s, est. speed input: 67.77 toks/s, output: 1084.35 toks/s]
Processed prompts:  25%|██▍       | 63/256 [00:14<00:25,  7.45it/s, est. speed input: 68.25 toks/s, output: 1092.02 toks/s]
Processed prompts:  25%|██▌       | 64/256 [00:15<00:33,  5.81it/s, est. speed input: 67.75 toks/s, output: 1084.00 toks/s]
Processed prompts:  25%|██▌       | 65/256 [00:15<00:30,  6.28it/s, est. speed input: 68.32 toks/s, output: 1093.08 toks/s]
Processed prompts:  27%|██▋       | 69/256 [00:15<00:17, 10.54it/s, est. speed input: 71.77 toks/s, output: 1148.31 toks/s]
Processed prompts:  28%|██▊       | 71/256 [00:15<00:18, 10.08it/s, est. speed input: 72.80 toks/s, output: 1164.87 toks/s]
Processed prompts:  29%|██▊       | 73/256 [00:15<00:19,  9.28it/s, est. speed input: 73.63 toks/s, output: 1178.14 toks/s]
Processed prompts:  30%|██▉       | 76/256 [00:16<00:23,  7.80it/s, est. speed input: 74.38 toks/s, output: 1190.15 toks/s]
Processed prompts:  30%|███       | 77/256 [00:17<00:40,  4.46it/s, est. speed input: 72.03 toks/s, output: 1152.48 toks/s]
Processed prompts:  30%|███       | 78/256 [00:17<00:55,  3.23it/s, est. speed input: 70.10 toks/s, output: 1121.58 toks/s]
Processed prompts:  31%|███       | 79/256 [00:18<00:58,  3.04it/s, est. speed input: 69.42 toks/s, output: 1110.66 toks/s]
Processed prompts:  31%|███▏      | 80/256 [00:18<01:04,  2.74it/s, est. speed input: 68.45 toks/s, output: 1095.19 toks/s]
Processed prompts:  32%|███▏      | 81/256 [00:18<00:53,  3.30it/s, est. speed input: 68.90 toks/s, output: 1102.42 toks/s]
Processed prompts:  32%|███▏      | 82/256 [00:18<00:46,  3.77it/s, est. speed input: 69.18 toks/s, output: 1106.88 toks/s]
Processed prompts:  32%|███▏      | 83/256 [00:19<00:43,  3.94it/s, est. speed input: 69.21 toks/s, output: 1107.30 toks/s]
Processed prompts:  33%|███▎      | 84/256 [00:19<00:39,  4.40it/s, est. speed input: 69.47 toks/s, output: 1111.53 toks/s]
Processed prompts:  33%|███▎      | 85/256 [00:19<00:36,  4.66it/s, est. speed input: 69.64 toks/s, output: 1114.22 toks/s]
Processed prompts:  34%|███▍      | 87/256 [00:19<00:31,  5.31it/s, est. speed input: 70.16 toks/s, output: 1122.53 toks/s]
Processed prompts:  34%|███▍      | 88/256 [00:20<00:33,  5.04it/s, est. speed input: 70.15 toks/s, output: 1122.39 toks/s]
Processed prompts:  35%|███▍      | 89/256 [00:20<00:32,  5.16it/s, est. speed input: 70.31 toks/s, output: 1125.03 toks/s]
Processed prompts:  35%|███▌      | 90/256 [00:20<00:36,  4.53it/s, est. speed input: 70.09 toks/s, output: 1121.43 toks/s]
Processed prompts:  36%|███▌      | 91/256 [00:20<00:34,  4.75it/s, est. speed input: 70.24 toks/s, output: 1123.82 toks/s]
Processed prompts:  36%|███▌      | 92/256 [00:20<00:35,  4.63it/s, est. speed input: 70.23 toks/s, output: 1123.67 toks/s]
Processed prompts:  36%|███▋      | 93/256 [00:21<00:31,  5.21it/s, est. speed input: 70.55 toks/s, output: 1128.74 toks/s]
Processed prompts:  37%|███▋      | 95/256 [00:21<00:21,  7.64it/s, est. speed input: 71.68 toks/s, output: 1146.86 toks/s]
Processed prompts:  38%|███▊      | 98/256 [00:21<00:14, 11.22it/s, est. speed input: 73.46 toks/s, output: 1175.44 toks/s]
Processed prompts:  40%|███▉      | 102/256 [00:21<00:09, 16.94it/s, est. speed input: 76.08 toks/s, output: 1217.27 toks/s]
Processed prompts:  41%|████      | 105/256 [00:22<00:16,  8.91it/s, est. speed input: 76.03 toks/s, output: 1216.54 toks/s]
Processed prompts:  42%|████▏     | 107/256 [00:23<00:36,  4.06it/s, est. speed input: 73.12 toks/s, output: 1169.98 toks/s]
Processed prompts:  43%|████▎     | 109/256 [00:23<00:36,  4.01it/s, est. speed input: 72.88 toks/s, output: 1166.14 toks/s]
Processed prompts:  43%|████▎     | 110/256 [00:24<00:34,  4.19it/s, est. speed input: 73.00 toks/s, output: 1167.92 toks/s]
Processed prompts:  43%|████▎     | 111/256 [00:24<00:37,  3.91it/s, est. speed input: 72.66 toks/s, output: 1162.50 toks/s]
Processed prompts:  44%|████▍     | 112/256 [00:24<00:44,  3.27it/s, est. speed input: 71.84 toks/s, output: 1149.51 toks/s]
Processed prompts:  44%|████▍     | 113/256 [00:25<00:48,  2.97it/s, est. speed input: 71.23 toks/s, output: 1139.66 toks/s]
Processed prompts:  45%|████▍     | 114/256 [00:25<00:49,  2.85it/s, est. speed input: 70.76 toks/s, output: 1132.10 toks/s]
Processed prompts:  45%|████▍     | 115/256 [00:25<00:42,  3.33it/s, est. speed input: 70.95 toks/s, output: 1135.19 toks/s]
Processed prompts:  45%|████▌     | 116/256 [00:26<00:39,  3.55it/s, est. speed input: 70.93 toks/s, output: 1134.92 toks/s]
Processed prompts:  46%|████▌     | 117/256 [00:26<00:36,  3.85it/s, est. speed input: 71.00 toks/s, output: 1135.93 toks/s]
Processed prompts:  46%|████▋     | 119/256 [00:26<00:27,  5.06it/s, est. speed input: 71.56 toks/s, output: 1144.94 toks/s]
Processed prompts:  47%|████▋     | 120/256 [00:26<00:23,  5.67it/s, est. speed input: 71.86 toks/s, output: 1149.83 toks/s]
Processed prompts:  47%|████▋     | 121/256 [00:26<00:21,  6.30it/s, est. speed input: 72.17 toks/s, output: 1154.75 toks/s]
Processed prompts:  48%|████▊     | 122/256 [00:26<00:20,  6.69it/s, est. speed input: 72.43 toks/s, output: 1158.94 toks/s]
Processed prompts:  48%|████▊     | 123/256 [00:27<00:19,  6.89it/s, est. speed input: 72.67 toks/s, output: 1162.66 toks/s]
Processed prompts:  49%|████▉     | 126/256 [00:27<00:12, 10.01it/s, est. speed input: 73.93 toks/s, output: 1182.93 toks/s]
Processed prompts:  50%|█████     | 128/256 [00:27<00:12, 10.22it/s, est. speed input: 74.59 toks/s, output: 1193.49 toks/s]
Processed prompts:  51%|█████     | 130/256 [00:27<00:12, 10.04it/s, est. speed input: 75.19 toks/s, output: 1203.10 toks/s]
Processed prompts:  52%|█████▏    | 134/256 [00:27<00:08, 14.18it/s, est. speed input: 77.07 toks/s, output: 1233.08 toks/s]
Processed prompts:  53%|█████▎    | 136/256 [00:28<00:14,  8.53it/s, est. speed input: 76.82 toks/s, output: 1229.18 toks/s]
Processed prompts:  54%|█████▍    | 138/256 [00:29<00:33,  3.48it/s, est. speed input: 74.02 toks/s, output: 1184.40 toks/s]
Processed prompts:  54%|█████▍    | 139/256 [00:30<00:39,  2.93it/s, est. speed input: 73.03 toks/s, output: 1168.55 toks/s]
Processed prompts:  55%|█████▍    | 140/256 [00:31<00:45,  2.55it/s, est. speed input: 72.10 toks/s, output: 1153.64 toks/s]
Processed prompts:  55%|█████▌    | 141/256 [00:31<00:44,  2.61it/s, est. speed input: 71.81 toks/s, output: 1148.94 toks/s]
Processed prompts:  56%|█████▌    | 143/256 [00:31<00:32,  3.46it/s, est. speed input: 72.22 toks/s, output: 1155.57 toks/s]
Processed prompts:  56%|█████▋    | 144/256 [00:31<00:29,  3.75it/s, est. speed input: 72.31 toks/s, output: 1156.99 toks/s]
Processed prompts:  57%|█████▋    | 145/256 [00:31<00:25,  4.35it/s, est. speed input: 72.56 toks/s, output: 1161.01 toks/s]
Processed prompts:  57%|█████▋    | 147/256 [00:32<00:20,  5.37it/s, est. speed input: 73.02 toks/s, output: 1168.24 toks/s]
Processed prompts:  58%|█████▊    | 148/256 [00:32<00:20,  5.26it/s, est. speed input: 73.05 toks/s, output: 1168.78 toks/s]
Processed prompts:  58%|█████▊    | 149/256 [00:32<00:18,  5.67it/s, est. speed input: 73.24 toks/s, output: 1171.86 toks/s]
Processed prompts:  59%|█████▉    | 151/256 [00:32<00:14,  7.45it/s, est. speed input: 73.89 toks/s, output: 1182.30 toks/s]
Processed prompts:  59%|█████▉    | 152/256 [00:32<00:13,  7.77it/s, est. speed input: 74.13 toks/s, output: 1186.14 toks/s]
Processed prompts:  61%|██████    | 155/256 [00:32<00:09, 10.92it/s, est. speed input: 75.23 toks/s, output: 1203.71 toks/s]
Processed prompts:  61%|██████▏   | 157/256 [00:33<00:08, 11.58it/s, est. speed input: 75.86 toks/s, output: 1213.70 toks/s]
Processed prompts:  62%|██████▏   | 159/256 [00:33<00:07, 12.65it/s, est. speed input: 76.53 toks/s, output: 1224.51 toks/s]
Processed prompts:  63%|██████▎   | 161/256 [00:33<00:08, 11.61it/s, est. speed input: 77.02 toks/s, output: 1232.35 toks/s]
Processed prompts:  64%|██████▎   | 163/256 [00:34<00:15,  5.86it/s, est. speed input: 76.33 toks/s, output: 1221.24 toks/s]
Processed prompts:  64%|██████▍   | 164/256 [00:34<00:20,  4.44it/s, est. speed input: 75.74 toks/s, output: 1211.87 toks/s]
Processed prompts:  64%|██████▍   | 165/256 [00:35<00:26,  3.37it/s, est. speed input: 74.96 toks/s, output: 1199.34 toks/s]
Processed prompts:  65%|██████▍   | 166/256 [00:35<00:27,  3.22it/s, est. speed input: 74.66 toks/s, output: 1194.53 toks/s]
Processed prompts:  65%|██████▌   | 167/256 [00:35<00:26,  3.37it/s, est. speed input: 74.57 toks/s, output: 1193.11 toks/s]
Processed prompts:  66%|██████▌   | 168/256 [00:36<00:27,  3.17it/s, est. speed input: 74.25 toks/s, output: 1188.04 toks/s]
Processed prompts:  66%|██████▌   | 169/256 [00:36<00:25,  3.42it/s, est. speed input: 74.22 toks/s, output: 1187.57 toks/s]
Processed prompts:  66%|██████▋   | 170/256 [00:36<00:24,  3.55it/s, est. speed input: 74.15 toks/s, output: 1186.33 toks/s]
Processed prompts:  67%|██████▋   | 171/256 [00:36<00:23,  3.56it/s, est. speed input: 74.02 toks/s, output: 1184.30 toks/s]
Processed prompts:  67%|██████▋   | 172/256 [00:37<00:24,  3.49it/s, est. speed input: 73.85 toks/s, output: 1181.63 toks/s]
Processed prompts:  68%|██████▊   | 173/256 [00:37<00:20,  4.02it/s, est. speed input: 73.97 toks/s, output: 1183.50 toks/s]
Processed prompts:  68%|██████▊   | 174/256 [00:37<00:18,  4.37it/s, est. speed input: 74.04 toks/s, output: 1184.61 toks/s]
Processed prompts:  68%|██████▊   | 175/256 [00:37<00:16,  4.83it/s, est. speed input: 74.16 toks/s, output: 1186.52 toks/s]
Processed prompts:  69%|██████▉   | 176/256 [00:37<00:15,  5.28it/s, est. speed input: 74.29 toks/s, output: 1188.64 toks/s]
Processed prompts:  69%|██████▉   | 177/256 [00:38<00:13,  6.05it/s, est. speed input: 74.50 toks/s, output: 1191.98 toks/s]
Processed prompts:  70%|██████▉   | 179/256 [00:38<00:10,  7.15it/s, est. speed input: 74.91 toks/s, output: 1198.53 toks/s]
Processed prompts:  70%|███████   | 180/256 [00:38<00:10,  7.57it/s, est. speed input: 75.11 toks/s, output: 1201.81 toks/s]
Processed prompts:  71%|███████   | 181/256 [00:38<00:09,  7.91it/s, est. speed input: 75.31 toks/s, output: 1205.02 toks/s]
Processed prompts:  71%|███████▏  | 183/256 [00:38<00:07,  9.61it/s, est. speed input: 75.85 toks/s, output: 1213.66 toks/s]
Processed prompts:  72%|███████▏  | 185/256 [00:38<00:06, 10.77it/s, est. speed input: 76.39 toks/s, output: 1222.21 toks/s]
Processed prompts:  73%|███████▎  | 187/256 [00:38<00:06, 10.37it/s, est. speed input: 76.81 toks/s, output: 1228.89 toks/s]
Processed prompts:  74%|███████▍  | 190/256 [00:39<00:05, 12.34it/s, est. speed input: 77.67 toks/s, output: 1242.80 toks/s]
Processed prompts:  75%|███████▌  | 192/256 [00:39<00:04, 13.07it/s, est. speed input: 78.23 toks/s, output: 1251.69 toks/s]
Processed prompts:  76%|███████▌  | 194/256 [00:41<00:19,  3.15it/s, est. speed input: 75.59 toks/s, output: 1209.49 toks/s]
Processed prompts:  76%|███████▌  | 195/256 [00:41<00:23,  2.64it/s, est. speed input: 74.76 toks/s, output: 1196.10 toks/s]
Processed prompts:  77%|███████▋  | 196/256 [00:41<00:21,  2.82it/s, est. speed input: 74.68 toks/s, output: 1194.94 toks/s]
Processed prompts:  77%|███████▋  | 197/256 [00:42<00:19,  2.97it/s, est. speed input: 74.57 toks/s, output: 1193.15 toks/s]
Processed prompts:  77%|███████▋  | 198/256 [00:42<00:17,  3.34it/s, est. speed input: 74.64 toks/s, output: 1194.16 toks/s]
Processed prompts:  78%|███████▊  | 199/256 [00:42<00:17,  3.26it/s, est. speed input: 74.44 toks/s, output: 1190.96 toks/s]
Processed prompts:  78%|███████▊  | 200/256 [00:42<00:14,  3.75it/s, est. speed input: 74.53 toks/s, output: 1192.55 toks/s]
Processed prompts:  79%|███████▊  | 201/256 [00:43<00:13,  4.23it/s, est. speed input: 74.64 toks/s, output: 1194.17 toks/s]
Processed prompts:  84%|████████▎ | 214/256 [00:43<00:01, 21.17it/s, est. speed input: 79.21 toks/s, output: 1267.44 toks/s]
Processed prompts:  85%|████████▌ | 218/256 [00:43<00:02, 18.33it/s, est. speed input: 80.14 toks/s, output: 1282.29 toks/s]
Processed prompts:  86%|████████▋ | 221/256 [00:44<00:03,  9.76it/s, est. speed input: 79.77 toks/s, output: 1276.31 toks/s]
Processed prompts:  88%|████████▊ | 224/256 [00:45<00:06,  5.27it/s, est. speed input: 78.40 toks/s, output: 1254.39 toks/s]
Processed prompts:  88%|████████▊ | 226/256 [00:46<00:05,  5.09it/s, est. speed input: 78.32 toks/s, output: 1253.13 toks/s]
Processed prompts:  89%|████████▉ | 228/256 [00:46<00:05,  4.85it/s, est. speed input: 78.19 toks/s, output: 1251.11 toks/s]
Processed prompts:  89%|████████▉ | 229/256 [00:46<00:05,  4.86it/s, est. speed input: 78.20 toks/s, output: 1251.19 toks/s]
Processed prompts:  90%|████████▉ | 230/256 [00:47<00:05,  4.96it/s, est. speed input: 78.24 toks/s, output: 1251.87 toks/s]
Processed prompts:  90%|█████████ | 231/256 [00:47<00:04,  5.06it/s, est. speed input: 78.28 toks/s, output: 1252.47 toks/s]
Processed prompts:  91%|█████████ | 232/256 [00:47<00:04,  5.33it/s, est. speed input: 78.37 toks/s, output: 1253.96 toks/s]
Processed prompts:  91%|█████████ | 233/256 [00:47<00:03,  5.92it/s, est. speed input: 78.53 toks/s, output: 1256.52 toks/s]
Processed prompts:  92%|█████████▏| 235/256 [00:47<00:02,  7.38it/s, est. speed input: 78.93 toks/s, output: 1262.82 toks/s]
Processed prompts:  93%|█████████▎| 237/256 [00:47<00:02,  8.50it/s, est. speed input: 79.31 toks/s, output: 1268.99 toks/s]
Processed prompts:  93%|█████████▎| 238/256 [00:47<00:02,  8.47it/s, est. speed input: 79.45 toks/s, output: 1271.16 toks/s]
Processed prompts:  93%|█████████▎| 239/256 [00:48<00:02,  7.42it/s, est. speed input: 79.46 toks/s, output: 1271.41 toks/s]
Processed prompts:  94%|█████████▍| 241/256 [00:48<00:01,  9.53it/s, est. speed input: 79.93 toks/s, output: 1278.92 toks/s]
Processed prompts:  95%|█████████▍| 243/256 [00:48<00:01,  9.51it/s, est. speed input: 80.24 toks/s, output: 1283.92 toks/s]
Processed prompts:  96%|█████████▌| 245/256 [00:48<00:01,  9.22it/s, est. speed input: 80.52 toks/s, output: 1288.38 toks/s]
Processed prompts:  96%|█████████▌| 246/256 [00:48<00:01,  8.75it/s, est. speed input: 80.62 toks/s, output: 1289.97 toks/s]
Processed prompts:  97%|█████████▋| 249/256 [00:49<00:00,  8.78it/s, est. speed input: 81.04 toks/s, output: 1296.66 toks/s]
Processed prompts:  98%|█████████▊| 250/256 [00:49<00:01,  4.50it/s, est. speed input: 80.19 toks/s, output: 1283.03 toks/s]
Processed prompts:  98%|█████████▊| 251/256 [00:50<00:01,  3.49it/s, est. speed input: 79.65 toks/s, output: 1274.35 toks/s]
Processed prompts:  98%|█████████▊| 252/256 [00:50<00:01,  3.14it/s, est. speed input: 79.29 toks/s, output: 1268.69 toks/s]
Processed prompts:  99%|█████████▉| 253/256 [00:51<00:00,  3.26it/s, est. speed input: 79.19 toks/s, output: 1267.01 toks/s]
Processed prompts:  99%|█████████▉| 254/256 [00:51<00:00,  3.44it/s, est. speed input: 79.12 toks/s, output: 1265.91 toks/s]
Processed prompts: 100%|█████████▉| 255/256 [00:51<00:00,  3.68it/s, est. speed input: 79.09 toks/s, output: 1265.50 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:51<00:00,  3.99it/s, est. speed input: 79.10 toks/s, output: 1265.67 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:51<00:00,  3.99it/s, est. speed input: 79.10 toks/s, output: 1265.67 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:51<00:00,  4.94it/s, est. speed input: 79.10 toks/s, output: 1265.67 toks/s]
[rank0]:[W126 11:55:23.691640245 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-26 11:55:26
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_8
Params: prompt_len=16, output_len=256, num_prompts=512, max_num_seqs=512
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 512 --max-num-seqs 512 --max-model-len 272 --max-num-batched-tokens 512 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Qwen2.5-14B-FP8_M512.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 11:55:35 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 11:55:36 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=609645) WARNING 01-26 11:55:53 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=609645) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=609645) WARNING 01-26 11:56:16 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=609645) ERROR 01-26 11:56:37 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=609645) ERROR 01-26 11:56:37 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=609645) ERROR 01-26 11:56:37 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=609645) ERROR 01-26 11:56:37 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=609645) ERROR 01-26 11:56:37 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=609645) ERROR 01-26 11:56:37 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=609645) ERROR 01-26 11:56:37 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=609645) ERROR 01-26 11:56:37 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=609645) ERROR 01-26 11:56:37 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=609645) ERROR 01-26 11:56:37 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=609645) ERROR 01-26 11:56:37 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 248, in _initialize_kv_caches
(EngineCore_DP0 pid=609645) ERROR 01-26 11:56:37 [core.py:866]     kv_cache_configs = get_kv_cache_configs(
(EngineCore_DP0 pid=609645) ERROR 01-26 11:56:37 [core.py:866]                        ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=609645) ERROR 01-26 11:56:37 [core.py:866]   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 1340, in get_kv_cache_configs
(EngineCore_DP0 pid=609645) ERROR 01-26 11:56:37 [core.py:866]     check_enough_kv_cache_memory(
(EngineCore_DP0 pid=609645) ERROR 01-26 11:56:37 [core.py:866]   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 687, in check_enough_kv_cache_memory
(EngineCore_DP0 pid=609645) ERROR 01-26 11:56:37 [core.py:866]     raise ValueError(
(EngineCore_DP0 pid=609645) ERROR 01-26 11:56:37 [core.py:866] ValueError: No available memory for the cache blocks. Try increasing `gpu_memory_utilization` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.

STDERR:
[2026-01-26 11:55:34] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:55:35] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 11:55:35] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 11:55:35] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 11:55:35] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 11:55:35] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 11:55:35] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 11:55:35] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 11:55:35] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 11:55:35] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:55:35] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:55:35] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:55:35] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:55:35] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 11:55:43] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:55:43] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 11:55:43] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 11:55:43] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 11:55:43] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 11:55:43] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 11:55:43] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 11:55:43] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 11:55:43] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 11:55:43] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:55:43] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:55:43] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:55:43] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:55:43] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[W126 11:55:53.054610915 socket.cpp:209] [c10d] The hostname of the client socket cannot be retrieved. err=-3
(EngineCore_DP0 pid=609645) [2026-01-26 11:55:54] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=609645) [2026-01-26 11:55:54] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=609645) [2026-01-26 11:55:54] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=609645) [2026-01-26 11:55:54] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=609645) [2026-01-26 11:55:54] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=609645) [2026-01-26 11:55:54] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=609645) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=609645) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:04,  1.45s/it]
(EngineCore_DP0 pid=609645) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:02<00:02,  1.47s/it]
(EngineCore_DP0 pid=609645) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.01s/it]
(EngineCore_DP0 pid=609645) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.17s/it]
(EngineCore_DP0 pid=609645) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.20s/it]
(EngineCore_DP0 pid=609645) 
(EngineCore_DP0 pid=609645) [2026-01-26 11:56:01] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 7680] -> 1D uint8
(EngineCore_DP0 pid=609645) [2026-01-26 11:56:01] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 41287680 bytes
(EngineCore_DP0 pid=609645) [2026-01-26 11:56:01] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 7680] -> 1D uint8
(EngineCore_DP0 pid=609645) [2026-01-26 11:56:01] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 29491200 bytes
(EngineCore_DP0 pid=609645) [2026-01-26 11:56:01] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 7680] -> 1D uint8
(EngineCore_DP0 pid=609645) [2026-01-26 11:56:01] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 159252480 bytes
(EngineCore_DP0 pid=609645) [2026-01-26 11:56:01] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 20736] -> 1D uint8
(EngineCore_DP0 pid=609645) [2026-01-26 11:56:01] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 79626240 bytes
(EngineCore_DP0 pid=609645) Process EngineCore_DP0:
(EngineCore_DP0 pid=609645) Traceback (most recent call last):
(EngineCore_DP0 pid=609645)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=609645)     self.run()
(EngineCore_DP0 pid=609645)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=609645)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=609645)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=609645)     raise e
(EngineCore_DP0 pid=609645)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=609645)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=609645)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=609645)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=609645)     super().__init__(
(EngineCore_DP0 pid=609645)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=609645)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=609645)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=609645)   File "/root/vllmbench/vllm/v1/engine/core.py", line 248, in _initialize_kv_caches
(EngineCore_DP0 pid=609645)     kv_cache_configs = get_kv_cache_configs(
(EngineCore_DP0 pid=609645)                        ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=609645)   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 1340, in get_kv_cache_configs
(EngineCore_DP0 pid=609645)     check_enough_kv_cache_memory(
(EngineCore_DP0 pid=609645)   File "/root/vllmbench/vllm/v1/core/kv_cache_utils.py", line 687, in check_enough_kv_cache_memory
(EngineCore_DP0 pid=609645)     raise ValueError(
(EngineCore_DP0 pid=609645) ValueError: No available memory for the cache blocks. Try increasing `gpu_memory_utilization` when initializing the engine. See https://docs.vllm.ai/en/latest/configuration/conserving_memory/ for more details.
[rank0]:[W126 11:56:39.497442936 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=512

========== M=512 ==========
Time: 2026-01-26 21:52:17
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_8
Params: prompt_len=16, output_len=256, num_prompts=512, max_num_seqs=512
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 512 --max-num-seqs 512 --max-model-len 272 --max-num-batched-tokens 512 --no-enable-chunked-prefill --gpu-memory-utilization 0.95 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Qwen2.5-14B-FP8_M512.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 21:52:26 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 21:52:27 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=1119835) ERROR 01-26 21:52:39 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=1119835) ERROR 01-26 21:52:39 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=1119835) ERROR 01-26 21:52:39 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1119835) ERROR 01-26 21:52:39 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1119835) ERROR 01-26 21:52:39 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1119835) ERROR 01-26 21:52:39 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1119835) ERROR 01-26 21:52:39 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=1119835) ERROR 01-26 21:52:39 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=1119835) ERROR 01-26 21:52:39 [core.py:866]     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=1119835) ERROR 01-26 21:52:39 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1119835) ERROR 01-26 21:52:39 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=1119835) ERROR 01-26 21:52:39 [core.py:866]     self._init_executor()
(EngineCore_DP0 pid=1119835) ERROR 01-26 21:52:39 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=1119835) ERROR 01-26 21:52:39 [core.py:866]     self.driver_worker.init_device()
(EngineCore_DP0 pid=1119835) ERROR 01-26 21:52:39 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
(EngineCore_DP0 pid=1119835) ERROR 01-26 21:52:39 [core.py:866]     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=1119835) ERROR 01-26 21:52:39 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1119835) ERROR 01-26 21:52:39 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
(EngineCore_DP0 pid=1119835) ERROR 01-26 21:52:39 [core.py:866]     raise ValueError(
(EngineCore_DP0 pid=1119835) ERROR 01-26 21:52:39 [core.py:866] ValueError: Free memory on device (22.39/23.99 GiB) on startup is less than desired GPU memory utilization (0.95, 22.79 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.

STDERR:
[2026-01-26 21:52:26] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 21:52:26] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:52:26] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 21:52:26] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:52:26] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:52:26] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:52:26] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:52:26] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:52:26] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:52:26] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 21:52:26] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 21:52:26] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 21:52:26] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 21:52:26] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 21:52:35] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 21:52:35] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:52:35] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 21:52:35] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:52:35] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:52:35] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:52:35] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:52:35] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:52:35] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:52:35] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 21:52:35] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 21:52:35] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 21:52:35] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 21:52:35] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1119835) Process EngineCore_DP0:
(EngineCore_DP0 pid=1119835) Traceback (most recent call last):
(EngineCore_DP0 pid=1119835)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=1119835)     self.run()
(EngineCore_DP0 pid=1119835)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=1119835)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=1119835)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=1119835)     raise e
(EngineCore_DP0 pid=1119835)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1119835)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1119835)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1119835)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1119835)     super().__init__(
(EngineCore_DP0 pid=1119835)   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=1119835)     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=1119835)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1119835)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=1119835)     self._init_executor()
(EngineCore_DP0 pid=1119835)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=1119835)     self.driver_worker.init_device()
(EngineCore_DP0 pid=1119835)   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
(EngineCore_DP0 pid=1119835)     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=1119835)     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1119835)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
(EngineCore_DP0 pid=1119835)     raise ValueError(
(EngineCore_DP0 pid=1119835) ValueError: Free memory on device (22.39/23.99 GiB) on startup is less than desired GPU memory utilization (0.95, 22.79 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W126 21:52:40.608392419 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=512

========== M=512 ==========
Time: 2026-01-26 21:53:07
Backend: cuSPARSELt (2:8)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_8
Params: prompt_len=16, output_len=256, num_prompts=512, max_num_seqs=512
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_8 --dataset-name random --input-len 16 --output-len 256 --num-prompts 512 --max-num-seqs 512 --max-model-len 272 --max-num-batched-tokens 512 --no-enable-chunked-prefill --gpu-memory-utilization 0.98 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/json/Qwen2.5-14B-FP8_M512.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 21:53:15 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 21:53:16 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=1120783) ERROR 01-26 21:53:28 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=1120783) ERROR 01-26 21:53:28 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=1120783) ERROR 01-26 21:53:28 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1120783) ERROR 01-26 21:53:28 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1120783) ERROR 01-26 21:53:28 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1120783) ERROR 01-26 21:53:28 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1120783) ERROR 01-26 21:53:28 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=1120783) ERROR 01-26 21:53:28 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=1120783) ERROR 01-26 21:53:28 [core.py:866]     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=1120783) ERROR 01-26 21:53:28 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1120783) ERROR 01-26 21:53:28 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=1120783) ERROR 01-26 21:53:28 [core.py:866]     self._init_executor()
(EngineCore_DP0 pid=1120783) ERROR 01-26 21:53:28 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=1120783) ERROR 01-26 21:53:28 [core.py:866]     self.driver_worker.init_device()
(EngineCore_DP0 pid=1120783) ERROR 01-26 21:53:28 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
(EngineCore_DP0 pid=1120783) ERROR 01-26 21:53:28 [core.py:866]     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=1120783) ERROR 01-26 21:53:28 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1120783) ERROR 01-26 21:53:28 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
(EngineCore_DP0 pid=1120783) ERROR 01-26 21:53:28 [core.py:866]     raise ValueError(
(EngineCore_DP0 pid=1120783) ERROR 01-26 21:53:28 [core.py:866] ValueError: Free memory on device (22.39/23.99 GiB) on startup is less than desired GPU memory utilization (0.98, 23.51 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.

STDERR:
[2026-01-26 21:53:15] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 21:53:15] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:53:15] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 21:53:15] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:53:15] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:53:15] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:53:15] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:53:15] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:53:15] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:53:15] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 21:53:15] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 21:53:15] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 21:53:15] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 21:53:15] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 21:53:24] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 21:53:25] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:53:25] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 21:53:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:53:25] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:53:25] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:53:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:53:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 21:53:25] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 21:53:25] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 21:53:25] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 21:53:25] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 21:53:25] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 21:53:25] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1120783) Process EngineCore_DP0:
(EngineCore_DP0 pid=1120783) Traceback (most recent call last):
(EngineCore_DP0 pid=1120783)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=1120783)     self.run()
(EngineCore_DP0 pid=1120783)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=1120783)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=1120783)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=1120783)     raise e
(EngineCore_DP0 pid=1120783)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1120783)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1120783)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1120783)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1120783)     super().__init__(
(EngineCore_DP0 pid=1120783)   File "/root/vllmbench/vllm/v1/engine/core.py", line 102, in __init__
(EngineCore_DP0 pid=1120783)     self.model_executor = executor_class(vllm_config)
(EngineCore_DP0 pid=1120783)                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1120783)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 101, in __init__
(EngineCore_DP0 pid=1120783)     self._init_executor()
(EngineCore_DP0 pid=1120783)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 47, in _init_executor
(EngineCore_DP0 pid=1120783)     self.driver_worker.init_device()
(EngineCore_DP0 pid=1120783)   File "/root/vllmbench/vllm/v1/worker/worker_base.py", line 326, in init_device
(EngineCore_DP0 pid=1120783)     self.worker.init_device()  # type: ignore
(EngineCore_DP0 pid=1120783)     ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1120783)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 247, in init_device
(EngineCore_DP0 pid=1120783)     raise ValueError(
(EngineCore_DP0 pid=1120783) ValueError: Free memory on device (22.39/23.99 GiB) on startup is less than desired GPU memory utilization (0.98, 23.51 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.
[rank0]:[W126 21:53:29.787164159 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

ERROR: Test failed for M=512
