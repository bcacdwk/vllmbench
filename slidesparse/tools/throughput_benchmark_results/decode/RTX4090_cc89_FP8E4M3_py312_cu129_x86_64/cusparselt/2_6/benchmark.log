
========== M=16 ==========
Time: 2026-01-25 17:32:19
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6
Params: prompt_len=16, output_len=256, num_prompts=16, max_num_seqs=16
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 256 --num-prompts 16 --max-num-seqs 16 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/json/Llama3.2-1B-FP8_M16.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 17:32:25 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 17:32:27 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=195458) WARNING 01-25 17:32:34 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=195458) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=195458) WARNING 01-25 17:32:42 [backends.py:609] Failed to read file <frozen os>
Throughput: 10.83 requests/s, 2945.66 total tokens/s, 2772.38 output tokens/s
Total num prompt tokens:  256
Total num output tokens:  4096

STDERR:
[2026-01-25 17:32:25] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 17:32:25] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 17:32:25] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 17:32:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 17:32:25] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 17:32:25] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 17:32:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 17:32:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 17:32:25] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 17:32:25] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 17:32:25] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 17:32:25] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 17:32:25] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 17:32:25] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 17:32:34] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 17:32:34] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 17:32:34] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 17:32:34] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 17:32:34] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 17:32:34] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 17:32:34] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 17:32:34] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 17:32:34] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 17:32:34] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 17:32:34] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 17:32:34] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 17:32:34] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 17:32:34] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=195458) [2026-01-25 17:32:35] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=195458) [2026-01-25 17:32:35] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=195458) [2026-01-25 17:32:35] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=195458) [2026-01-25 17:32:35] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=195458) [2026-01-25 17:32:35] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=195458) [2026-01-25 17:32:35] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=195458) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=195458) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.44it/s]
(EngineCore_DP0 pid=195458) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.44it/s]
(EngineCore_DP0 pid=195458) 
(EngineCore_DP0 pid=195458) [2026-01-25 17:32:36] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=195458) [2026-01-25 17:32:36] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6389760 bytes
(EngineCore_DP0 pid=195458) [2026-01-25 17:32:36] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=195458) [2026-01-25 17:32:36] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4259840 bytes
(EngineCore_DP0 pid=195458) [2026-01-25 17:32:36] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=195458) [2026-01-25 17:32:36] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 34078720 bytes
(EngineCore_DP0 pid=195458) [2026-01-25 17:32:36] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=195458) [2026-01-25 17:32:36] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16842752 bytes
(EngineCore_DP0 pid=195458) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 1/7 [00:00<00:00,  6.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 3/7 [00:00<00:00,  9.12it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 5/7 [00:00<00:00, 10.05it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00,  9.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00,  9.65it/s]
(EngineCore_DP0 pid=195458) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 1/5 [00:00<00:00,  8.80it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00,  8.96it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 3/5 [00:00<00:00,  7.04it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00,  7.85it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00,  7.71it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00,  7.80it/s]

Adding requests:   0%|          | 0/16 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 16/16 [00:00<00:00, 2126.19it/s]

Processed prompts:   0%|          | 0/16 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts: 100%|██████████| 16/16 [00:00<00:00, -96.55it/s, est. speed input: -1544.32 toks/s, output: -24713.01 toks/s]
Processed prompts: 100%|██████████| 16/16 [00:00<00:00, -96.65it/s, est. speed input: -1544.32 toks/s, output: -24713.01 toks/s]
[rank0]:[W125 17:32:56.900999214 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-25 17:32:59
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6
Params: prompt_len=16, output_len=256, num_prompts=128, max_num_seqs=128
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 256 --num-prompts 128 --max-num-seqs 128 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/json/Llama3.2-1B-FP8_M128.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 17:33:07 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 17:33:08 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=196249) WARNING 01-25 17:33:16 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=196249) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=196249) WARNING 01-25 17:33:23 [backends.py:609] Failed to read file <frozen os>
Throughput: 48.32 requests/s, 13144.33 total tokens/s, 12371.14 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768

STDERR:
[2026-01-25 17:33:07] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 17:33:07] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 17:33:07] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 17:33:07] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 17:33:07] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 17:33:07] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 17:33:07] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 17:33:07] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 17:33:07] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 17:33:07] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 17:33:07] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 17:33:07] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 17:33:07] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 17:33:07] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 17:33:15] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 17:33:15] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 17:33:15] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 17:33:15] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 17:33:15] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 17:33:15] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 17:33:15] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 17:33:15] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 17:33:15] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 17:33:15] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 17:33:15] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 17:33:15] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 17:33:15] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 17:33:15] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=196249) [2026-01-25 17:33:17] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=196249) [2026-01-25 17:33:17] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=196249) [2026-01-25 17:33:17] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=196249) [2026-01-25 17:33:17] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=196249) [2026-01-25 17:33:17] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=196249) [2026-01-25 17:33:17] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=196249) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=196249) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.65it/s]
(EngineCore_DP0 pid=196249) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.65it/s]
(EngineCore_DP0 pid=196249) 
(EngineCore_DP0 pid=196249) [2026-01-25 17:33:17] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=196249) [2026-01-25 17:33:17] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6389760 bytes
(EngineCore_DP0 pid=196249) [2026-01-25 17:33:17] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=196249) [2026-01-25 17:33:17] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4259840 bytes
(EngineCore_DP0 pid=196249) [2026-01-25 17:33:17] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=196249) [2026-01-25 17:33:17] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 34078720 bytes
(EngineCore_DP0 pid=196249) [2026-01-25 17:33:17] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=196249) [2026-01-25 17:33:17] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16842752 bytes
(EngineCore_DP0 pid=196249) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|▎         | 1/35 [00:00<00:03,  8.92it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 2/35 [00:00<00:03,  8.94it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▊         | 3/35 [00:00<00:03,  9.07it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█▏        | 4/35 [00:00<00:03,  9.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 5/35 [00:00<00:03,  9.36it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 6/35 [00:00<00:03,  9.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 7/35 [00:00<00:02,  9.65it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  23%|██▎       | 8/35 [00:00<00:02,  9.60it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▌       | 9/35 [00:00<00:02,  9.55it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 10/35 [00:01<00:02,  9.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 11/35 [00:01<00:02,  9.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  34%|███▍      | 12/35 [00:01<00:02,  9.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 13/35 [00:01<00:02,  9.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 14/35 [00:01<00:02,  9.66it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 15/35 [00:01<00:02,  9.66it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▊     | 17/35 [00:01<00:01,  9.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████▏    | 18/35 [00:02<00:03,  5.15it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  54%|█████▍    | 19/35 [00:02<00:02,  5.67it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 20/35 [00:02<00:02,  5.22it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 21/35 [00:02<00:02,  5.81it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 22/35 [00:02<00:01,  6.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  66%|██████▌   | 23/35 [00:02<00:01,  7.25it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 24/35 [00:03<00:01,  7.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 25/35 [00:03<00:01,  8.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▍  | 26/35 [00:03<00:01,  8.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  77%|███████▋  | 27/35 [00:03<00:00,  8.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 28/35 [00:03<00:00,  9.05it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  83%|████████▎ | 29/35 [00:03<00:00,  9.24it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 30/35 [00:03<00:00,  9.44it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████▏| 32/35 [00:03<00:00,  9.88it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  97%|█████████▋| 34/35 [00:04<00:00, 10.14it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:04<00:00,  8.40it/s]
(EngineCore_DP0 pid=196249) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   5%|▌         | 1/19 [00:00<00:02,  8.48it/s]
Capturing CUDA graphs (decode, FULL):  16%|█▌        | 3/19 [00:00<00:01,  9.82it/s]
Capturing CUDA graphs (decode, FULL):  26%|██▋       | 5/19 [00:00<00:01, 10.22it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 7/19 [00:00<00:01, 10.37it/s]
Capturing CUDA graphs (decode, FULL):  47%|████▋     | 9/19 [00:00<00:01,  9.93it/s]
Capturing CUDA graphs (decode, FULL):  53%|█████▎    | 10/19 [00:01<00:01,  8.73it/s]
Capturing CUDA graphs (decode, FULL):  58%|█████▊    | 11/19 [00:01<00:00,  8.98it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 12/19 [00:01<00:00,  7.87it/s]
Capturing CUDA graphs (decode, FULL):  68%|██████▊   | 13/19 [00:01<00:00,  7.78it/s]
Capturing CUDA graphs (decode, FULL):  79%|███████▉  | 15/19 [00:01<00:00,  8.74it/s]
Capturing CUDA graphs (decode, FULL):  89%|████████▉ | 17/19 [00:01<00:00,  9.44it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:02<00:00,  9.93it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:02<00:00,  9.34it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 2700.25it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:02<05:21,  2.53s/it, est. speed input: 6.33 toks/s, output: 101.23 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00,  2.53s/it, est. speed input: 787.75 toks/s, output: 12603.95 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 49.23it/s, est. speed input: 787.75 toks/s, output: 12603.95 toks/s]
[rank0]:[W125 17:33:40.936812255 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-25 17:33:43
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6
Params: prompt_len=16, output_len=256, num_prompts=256, max_num_seqs=256
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 256 --num-prompts 256 --max-num-seqs 256 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/json/Llama3.2-1B-FP8_M256.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-25 17:33:51 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-25 17:33:53 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=197111) WARNING 01-25 17:33:59 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=197111) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=197111) WARNING 01-25 17:34:06 [backends.py:609] Failed to read file <frozen os>
Throughput: 67.63 requests/s, 18396.21 total tokens/s, 17314.08 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536

STDERR:
[2026-01-25 17:33:51] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 17:33:51] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 17:33:51] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 17:33:51] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 17:33:51] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 17:33:51] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 17:33:51] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 17:33:51] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 17:33:51] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 17:33:51] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 17:33:51] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 17:33:51] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 17:33:51] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 17:33:51] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 17:33:58] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 17:33:58] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-25 17:33:58] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 17:33:58] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 17:33:58] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 17:33:58] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 17:33:58] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 17:33:58] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 17:33:58] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-25 17:33:58] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 17:33:58] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 17:33:58] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 17:33:58] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 17:33:58] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=197111) [2026-01-25 17:33:59] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=197111) [2026-01-25 17:33:59] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=197111) [2026-01-25 17:33:59] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=197111) [2026-01-25 17:33:59] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=197111) [2026-01-25 17:33:59] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=197111) [2026-01-25 17:33:59] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=197111) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=197111) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.58it/s]
(EngineCore_DP0 pid=197111) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.58it/s]
(EngineCore_DP0 pid=197111) 
(EngineCore_DP0 pid=197111) [2026-01-25 17:34:00] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=197111) [2026-01-25 17:34:00] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6389760 bytes
(EngineCore_DP0 pid=197111) [2026-01-25 17:34:00] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=197111) [2026-01-25 17:34:00] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4259840 bytes
(EngineCore_DP0 pid=197111) [2026-01-25 17:34:00] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=197111) [2026-01-25 17:34:00] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 34078720 bytes
(EngineCore_DP0 pid=197111) [2026-01-25 17:34:00] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=197111) [2026-01-25 17:34:00] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16842752 bytes
(EngineCore_DP0 pid=197111) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/36 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|▎         | 1/36 [00:00<00:05,  5.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 2/36 [00:00<00:04,  7.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 3/36 [00:00<00:03,  8.29it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 4/36 [00:00<00:03,  8.70it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 6/36 [00:00<00:03,  9.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 8/36 [00:00<00:02,  9.82it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  28%|██▊       | 10/36 [00:01<00:02,  9.96it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 12/36 [00:01<00:02, 10.08it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|███▉      | 14/36 [00:01<00:02, 10.18it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  44%|████▍     | 16/36 [00:01<00:01, 10.09it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 18/36 [00:01<00:01,  9.65it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 19/36 [00:02<00:01,  9.45it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  56%|█████▌    | 20/36 [00:02<00:01,  9.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 21/36 [00:02<00:01,  9.36it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|██████    | 22/36 [00:02<00:01,  9.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▍   | 23/36 [00:02<00:01,  9.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 24/36 [00:02<00:01,  9.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▉   | 25/36 [00:02<00:01,  8.87it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  72%|███████▏  | 26/36 [00:02<00:01,  7.42it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 27/36 [00:03<00:01,  7.14it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 28/36 [00:03<00:01,  6.13it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  81%|████████  | 29/36 [00:03<00:01,  6.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  83%|████████▎ | 30/36 [00:03<00:00,  7.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 32/36 [00:03<00:00,  8.34it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 34/36 [00:03<00:00,  8.92it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  97%|█████████▋| 35/36 [00:03<00:00,  9.08it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:04<00:00,  8.67it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:04<00:00,  8.79it/s]
(EngineCore_DP0 pid=197111) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   3%|▎         | 1/35 [00:00<00:04,  7.87it/s]
Capturing CUDA graphs (decode, FULL):   6%|▌         | 2/35 [00:00<00:03,  8.84it/s]
Capturing CUDA graphs (decode, FULL):   9%|▊         | 3/35 [00:00<00:03,  8.97it/s]
Capturing CUDA graphs (decode, FULL):  11%|█▏        | 4/35 [00:00<00:03,  9.19it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 5/35 [00:00<00:03,  9.33it/s]
Capturing CUDA graphs (decode, FULL):  17%|█▋        | 6/35 [00:00<00:03,  9.25it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 7/35 [00:00<00:02,  9.40it/s]
Capturing CUDA graphs (decode, FULL):  23%|██▎       | 8/35 [00:00<00:02,  9.48it/s]
Capturing CUDA graphs (decode, FULL):  26%|██▌       | 9/35 [00:00<00:02,  9.60it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 10/35 [00:01<00:02,  9.63it/s]
Capturing CUDA graphs (decode, FULL):  31%|███▏      | 11/35 [00:01<00:02,  9.62it/s]
Capturing CUDA graphs (decode, FULL):  34%|███▍      | 12/35 [00:01<00:02,  9.58it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 13/35 [00:01<00:02,  9.37it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 14/35 [00:01<00:02,  9.41it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 15/35 [00:01<00:02,  9.37it/s]
Capturing CUDA graphs (decode, FULL):  46%|████▌     | 16/35 [00:01<00:02,  9.49it/s]
Capturing CUDA graphs (decode, FULL):  49%|████▊     | 17/35 [00:01<00:02,  8.92it/s]
Capturing CUDA graphs (decode, FULL):  51%|█████▏    | 18/35 [00:02<00:02,  7.43it/s]
Capturing CUDA graphs (decode, FULL):  54%|█████▍    | 19/35 [00:02<00:02,  7.79it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 20/35 [00:02<00:02,  6.79it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 21/35 [00:02<00:02,  6.40it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 22/35 [00:02<00:01,  7.08it/s]
Capturing CUDA graphs (decode, FULL):  66%|██████▌   | 23/35 [00:02<00:01,  7.74it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 25/35 [00:02<00:01,  8.85it/s]
Capturing CUDA graphs (decode, FULL):  77%|███████▋  | 27/35 [00:03<00:00,  9.52it/s]
Capturing CUDA graphs (decode, FULL):  83%|████████▎ | 29/35 [00:03<00:00,  9.91it/s]
Capturing CUDA graphs (decode, FULL):  89%|████████▊ | 31/35 [00:03<00:00, 10.16it/s]
Capturing CUDA graphs (decode, FULL):  94%|█████████▍| 33/35 [00:03<00:00, 10.28it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:03<00:00, 10.44it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:03<00:00,  9.13it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 2708.91it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:03<14:08,  3.33s/it, est. speed input: 4.81 toks/s, output: 76.92 toks/s]
Processed prompts:  42%|████▏     | 108/256 [00:03<00:03, 44.22it/s, est. speed input: 503.22 toks/s, output: 8051.42 toks/s]
Processed prompts:  77%|███████▋  | 198/256 [00:03<00:00, 91.59it/s, est. speed input: 896.20 toks/s, output: 14339.11 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 91.59it/s, est. speed input: 1110.39 toks/s, output: 17766.10 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 69.39it/s, est. speed input: 1110.39 toks/s, output: 17766.10 toks/s]
[rank0]:[W125 17:34:28.250853574 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=64 ==========
Time: 2026-01-26 09:09:06
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6
Params: prompt_len=16, output_len=256, num_prompts=64, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 256 --num-prompts 64 --max-num-seqs 64 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/json/Llama3.2-1B-FP8_M64.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 09:09:14 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 09:09:15 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=443322) WARNING 01-26 09:09:23 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=443322) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=443322) WARNING 01-26 09:09:31 [backends.py:609] Failed to read file <frozen os>
Throughput: 30.88 requests/s, 8399.45 total tokens/s, 7905.37 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384

STDERR:
[2026-01-26 09:09:14] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 09:09:14] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 09:09:14] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 09:09:14] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 09:09:14] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 09:09:14] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 09:09:14] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 09:09:14] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 09:09:14] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 09:09:14] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 09:09:14] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 09:09:14] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 09:09:14] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 09:09:14] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 09:09:23] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 09:09:23] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 09:09:23] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 09:09:23] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 09:09:23] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 09:09:23] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 09:09:23] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 09:09:23] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 09:09:23] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 09:09:23] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 09:09:23] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 09:09:23] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 09:09:23] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 09:09:23] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=443322) [2026-01-26 09:09:24] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=443322) [2026-01-26 09:09:24] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=443322) [2026-01-26 09:09:24] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=443322) [2026-01-26 09:09:24] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=443322) [2026-01-26 09:09:24] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=443322) [2026-01-26 09:09:24] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=443322) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=443322) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.51s/it]
(EngineCore_DP0 pid=443322) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.51s/it]
(EngineCore_DP0 pid=443322) 
(EngineCore_DP0 pid=443322) [2026-01-26 09:09:26] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=443322) [2026-01-26 09:09:26] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6389760 bytes
(EngineCore_DP0 pid=443322) [2026-01-26 09:09:26] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=443322) [2026-01-26 09:09:26] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4259840 bytes
(EngineCore_DP0 pid=443322) [2026-01-26 09:09:26] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=443322) [2026-01-26 09:09:26] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 34078720 bytes
(EngineCore_DP0 pid=443322) [2026-01-26 09:09:26] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=443322) [2026-01-26 09:09:26] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16842752 bytes
(EngineCore_DP0 pid=443322) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   5%|▌         | 1/19 [00:00<00:08,  2.14it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 2/19 [00:00<00:05,  3.05it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 3/19 [00:01<00:05,  2.91it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  21%|██        | 4/19 [00:01<00:04,  3.58it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  32%|███▏      | 6/19 [00:01<00:02,  5.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 7/19 [00:01<00:01,  6.22it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 8/19 [00:01<00:01,  6.91it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 9/19 [00:01<00:01,  7.32it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 10/19 [00:01<00:01,  7.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 11/19 [00:01<00:01,  7.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 12/19 [00:02<00:00,  8.39it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  68%|██████▊   | 13/19 [00:02<00:00,  8.68it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  79%|███████▉  | 15/19 [00:02<00:00,  9.14it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 17/19 [00:02<00:00,  9.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  95%|█████████▍| 18/19 [00:02<00:00,  9.55it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:02<00:00,  8.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:02<00:00,  6.63it/s]
(EngineCore_DP0 pid=443322) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   9%|▉         | 1/11 [00:00<00:01,  6.07it/s]
Capturing CUDA graphs (decode, FULL):  18%|█▊        | 2/11 [00:00<00:02,  3.52it/s]
Capturing CUDA graphs (decode, FULL):  27%|██▋       | 3/11 [00:00<00:01,  4.17it/s]
Capturing CUDA graphs (decode, FULL):  36%|███▋      | 4/11 [00:00<00:01,  4.96it/s]
Capturing CUDA graphs (decode, FULL):  45%|████▌     | 5/11 [00:01<00:01,  4.14it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 6/11 [00:01<00:01,  4.35it/s]
Capturing CUDA graphs (decode, FULL):  73%|███████▎  | 8/11 [00:01<00:00,  6.11it/s]
Capturing CUDA graphs (decode, FULL):  91%|█████████ | 10/11 [00:01<00:00,  7.31it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:01<00:00,  5.89it/s]

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 64/64 [00:00<00:00, 2630.43it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 1/64 [00:02<02:07,  2.02s/it, est. speed input: 7.92 toks/s, output: 126.77 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:02<00:00,  2.02s/it, est. speed input: 500.34 toks/s, output: 8005.43 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:02<00:00, 31.27it/s, est. speed input: 500.34 toks/s, output: 8005.43 toks/s]
[rank0]:[W126 09:09:47.447135942 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-26 09:09:50
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6
Params: prompt_len=16, output_len=256, num_prompts=128, max_num_seqs=128
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 256 --num-prompts 128 --max-num-seqs 128 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/json/Llama3.2-1B-FP8_M128.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 09:09:57 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 09:09:58 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=444136) WARNING 01-26 09:10:07 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=444136) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=444136) WARNING 01-26 09:10:16 [backends.py:609] Failed to read file <frozen os>
Throughput: 48.45 requests/s, 13177.62 total tokens/s, 12402.47 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768

STDERR:
[2026-01-26 09:09:58] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 09:09:57] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 09:09:57] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 09:09:57] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 09:09:57] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 09:09:57] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 09:09:57] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 09:09:57] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 09:09:57] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 09:09:57] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 09:09:57] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 09:09:57] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 09:09:57] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 09:09:57] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 09:10:05] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 09:10:06] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 09:10:06] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 09:10:06] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 09:10:06] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 09:10:06] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 09:10:06] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 09:10:06] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 09:10:06] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 09:10:06] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 09:10:06] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 09:10:06] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 09:10:06] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 09:10:06] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=444136) [2026-01-26 09:10:08] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=444136) [2026-01-26 09:10:08] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=444136) [2026-01-26 09:10:08] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=444136) [2026-01-26 09:10:08] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=444136) [2026-01-26 09:10:08] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=444136) [2026-01-26 09:10:08] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=444136) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=444136) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.51it/s]
(EngineCore_DP0 pid=444136) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.51it/s]
(EngineCore_DP0 pid=444136) 
(EngineCore_DP0 pid=444136) [2026-01-26 09:10:08] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=444136) [2026-01-26 09:10:09] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6389760 bytes
(EngineCore_DP0 pid=444136) [2026-01-26 09:10:09] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=444136) [2026-01-26 09:10:09] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4259840 bytes
(EngineCore_DP0 pid=444136) [2026-01-26 09:10:09] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=444136) [2026-01-26 09:10:09] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 34078720 bytes
(EngineCore_DP0 pid=444136) [2026-01-26 09:10:09] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=444136) [2026-01-26 09:10:09] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16842752 bytes
(EngineCore_DP0 pid=444136) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|▎         | 1/35 [00:00<00:03,  8.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 2/35 [00:00<00:03,  8.60it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▊         | 3/35 [00:00<00:03,  8.84it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█▏        | 4/35 [00:00<00:03,  9.18it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 5/35 [00:00<00:03,  9.22it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 6/35 [00:00<00:03,  9.32it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 7/35 [00:00<00:02,  9.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▌       | 9/35 [00:00<00:02,  9.73it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 11/35 [00:01<00:02,  9.72it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  34%|███▍      | 12/35 [00:01<00:02,  8.27it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 13/35 [00:01<00:04,  4.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 14/35 [00:01<00:03,  5.36it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 15/35 [00:02<00:03,  5.13it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  46%|████▌     | 16/35 [00:02<00:04,  3.87it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▊     | 17/35 [00:02<00:03,  4.59it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████▏    | 18/35 [00:02<00:03,  5.35it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  54%|█████▍    | 19/35 [00:02<00:02,  6.11it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 20/35 [00:03<00:02,  6.81it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 21/35 [00:03<00:01,  7.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  66%|██████▌   | 23/35 [00:03<00:01,  8.44it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 24/35 [00:03<00:01,  8.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 25/35 [00:03<00:01,  8.79it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▍  | 26/35 [00:03<00:01,  8.93it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  77%|███████▋  | 27/35 [00:03<00:00,  9.15it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 28/35 [00:03<00:00,  9.36it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  83%|████████▎ | 29/35 [00:03<00:00,  9.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 30/35 [00:04<00:00,  9.42it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▊ | 31/35 [00:04<00:00,  9.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 33/35 [00:04<00:00,  9.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  97%|█████████▋| 34/35 [00:04<00:00,  7.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:04<00:00,  5.14it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:04<00:00,  7.03it/s]
(EngineCore_DP0 pid=444136) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   5%|▌         | 1/19 [00:00<00:02,  6.04it/s]
Capturing CUDA graphs (decode, FULL):  11%|█         | 2/19 [00:00<00:02,  5.82it/s]
Capturing CUDA graphs (decode, FULL):  16%|█▌        | 3/19 [00:00<00:04,  3.35it/s]
Capturing CUDA graphs (decode, FULL):  21%|██        | 4/19 [00:00<00:03,  4.47it/s]
Capturing CUDA graphs (decode, FULL):  26%|██▋       | 5/19 [00:01<00:02,  5.56it/s]
Capturing CUDA graphs (decode, FULL):  32%|███▏      | 6/19 [00:01<00:02,  6.50it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 7/19 [00:01<00:01,  7.33it/s]
Capturing CUDA graphs (decode, FULL):  47%|████▋     | 9/19 [00:01<00:01,  8.47it/s]
Capturing CUDA graphs (decode, FULL):  53%|█████▎    | 10/19 [00:01<00:01,  8.79it/s]
Capturing CUDA graphs (decode, FULL):  58%|█████▊    | 11/19 [00:01<00:00,  9.08it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:00<00:00, 20.61it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 2830.54it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:02<05:20,  2.53s/it, est. speed input: 6.34 toks/s, output: 101.38 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00,  2.53s/it, est. speed input: 789.14 toks/s, output: 12626.19 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 49.32it/s, est. speed input: 789.14 toks/s, output: 12626.19 toks/s]
[rank0]:[W126 09:10:34.666528230 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-26 09:10:36
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6
Params: prompt_len=16, output_len=256, num_prompts=256, max_num_seqs=256
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 256 --num-prompts 256 --max-num-seqs 256 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/json/Llama3.2-1B-FP8_M256.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 09:10:45 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 09:10:46 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=444990) WARNING 01-26 09:10:54 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=444990) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=444990) WARNING 01-26 09:11:01 [backends.py:609] Failed to read file <frozen os>
Throughput: 70.11 requests/s, 19068.89 total tokens/s, 17947.20 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536

STDERR:
[2026-01-26 09:10:45] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 09:10:45] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 09:10:45] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 09:10:45] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 09:10:45] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 09:10:45] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 09:10:45] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 09:10:45] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 09:10:45] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 09:10:45] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 09:10:45] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 09:10:45] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 09:10:45] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 09:10:45] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 09:10:54] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 09:10:54] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 09:10:54] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 09:10:54] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 09:10:54] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 09:10:54] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 09:10:54] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 09:10:54] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 09:10:54] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 09:10:54] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 09:10:54] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 09:10:54] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 09:10:54] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 09:10:54] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=444990) [2026-01-26 09:10:55] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=444990) [2026-01-26 09:10:55] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=444990) [2026-01-26 09:10:55] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=444990) [2026-01-26 09:10:55] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=444990) [2026-01-26 09:10:55] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=444990) [2026-01-26 09:10:55] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=444990) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=444990) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.52it/s]
(EngineCore_DP0 pid=444990) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.52it/s]
(EngineCore_DP0 pid=444990) 
(EngineCore_DP0 pid=444990) [2026-01-26 09:10:56] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=444990) [2026-01-26 09:10:56] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6389760 bytes
(EngineCore_DP0 pid=444990) [2026-01-26 09:10:56] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=444990) [2026-01-26 09:10:56] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4259840 bytes
(EngineCore_DP0 pid=444990) [2026-01-26 09:10:56] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=444990) [2026-01-26 09:10:56] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 34078720 bytes
(EngineCore_DP0 pid=444990) [2026-01-26 09:10:56] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=444990) [2026-01-26 09:10:56] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16842752 bytes
(EngineCore_DP0 pid=444990) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/36 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|▎         | 1/36 [00:00<00:15,  2.29it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 2/36 [00:00<00:13,  2.45it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 3/36 [00:01<00:13,  2.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 4/36 [00:01<00:09,  3.48it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 6/36 [00:01<00:05,  5.27it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 8/36 [00:01<00:04,  6.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  28%|██▊       | 10/36 [00:01<00:03,  7.58it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███       | 11/36 [00:02<00:03,  7.96it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 12/36 [00:02<00:02,  8.34it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▌      | 13/36 [00:02<00:02,  8.70it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|███▉      | 14/36 [00:02<00:02,  8.94it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 15/36 [00:02<00:02,  9.18it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  44%|████▍     | 16/36 [00:02<00:02,  9.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 17/36 [00:02<00:02,  9.41it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 18/36 [00:02<00:01,  9.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 19/36 [00:02<00:01,  8.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  56%|█████▌    | 20/36 [00:03<00:02,  7.94it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 21/36 [00:03<00:03,  4.18it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|██████    | 22/36 [00:03<00:02,  4.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▍   | 23/36 [00:03<00:02,  4.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 24/36 [00:04<00:03,  3.81it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▉   | 25/36 [00:04<00:02,  4.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  72%|███████▏  | 26/36 [00:04<00:01,  5.33it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 27/36 [00:04<00:01,  6.01it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 28/36 [00:04<00:01,  6.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  81%|████████  | 29/36 [00:04<00:00,  7.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  83%|████████▎ | 30/36 [00:04<00:00,  7.55it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 31/36 [00:05<00:00,  7.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 32/36 [00:05<00:00,  8.16it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 33/36 [00:05<00:00,  8.45it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 34/36 [00:05<00:00,  8.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  97%|█████████▋| 35/36 [00:05<00:00,  8.82it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:05<00:00,  8.15it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:05<00:00,  6.32it/s]
(EngineCore_DP0 pid=444990) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   3%|▎         | 1/35 [00:00<00:05,  6.62it/s]
Capturing CUDA graphs (decode, FULL):   6%|▌         | 2/35 [00:00<00:04,  7.54it/s]
Capturing CUDA graphs (decode, FULL):   9%|▊         | 3/35 [00:00<00:04,  7.11it/s]
Capturing CUDA graphs (decode, FULL):  11%|█▏        | 4/35 [00:00<00:08,  3.86it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 5/35 [00:01<00:07,  4.23it/s]
Capturing CUDA graphs (decode, FULL):  17%|█▋        | 6/35 [00:01<00:06,  4.73it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 7/35 [00:01<00:06,  4.31it/s]
Capturing CUDA graphs (decode, FULL):  23%|██▎       | 8/35 [00:01<00:06,  4.11it/s]
Capturing CUDA graphs (decode, FULL):  26%|██▌       | 9/35 [00:01<00:05,  5.00it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 10/35 [00:01<00:04,  5.90it/s]
Capturing CUDA graphs (decode, FULL):  31%|███▏      | 11/35 [00:02<00:03,  6.71it/s]
Capturing CUDA graphs (decode, FULL):  34%|███▍      | 12/35 [00:02<00:03,  7.40it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 13/35 [00:02<00:02,  8.00it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 14/35 [00:02<00:02,  8.51it/s]
Capturing CUDA graphs (decode, FULL):  46%|████▌     | 16/35 [00:02<00:02,  9.14it/s]
Capturing CUDA graphs (decode, FULL):  49%|████▊     | 17/35 [00:02<00:01,  9.31it/s]
Capturing CUDA graphs (decode, FULL):  51%|█████▏    | 18/35 [00:02<00:01,  9.46it/s]
Capturing CUDA graphs (decode, FULL):  54%|█████▍    | 19/35 [00:02<00:01,  9.53it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 20/35 [00:02<00:01,  9.64it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 22/35 [00:03<00:01,  9.73it/s]
Capturing CUDA graphs (decode, FULL):  69%|██████▊   | 24/35 [00:03<00:01,  9.84it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 25/35 [00:03<00:01,  9.87it/s]
Capturing CUDA graphs (decode, FULL):  74%|███████▍  | 26/35 [00:03<00:01,  8.40it/s]
Capturing CUDA graphs (decode, FULL):  77%|███████▋  | 27/35 [00:03<00:01,  5.88it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 28/35 [00:04<00:01,  5.69it/s]
Capturing CUDA graphs (decode, FULL):  83%|████████▎ | 29/35 [00:04<00:01,  5.96it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 30/35 [00:04<00:01,  4.77it/s]
Capturing CUDA graphs (decode, FULL):  89%|████████▊ | 31/35 [00:04<00:00,  5.14it/s]
Capturing CUDA graphs (decode, FULL):  91%|█████████▏| 32/35 [00:04<00:00,  5.97it/s]
Capturing CUDA graphs (decode, FULL):  97%|█████████▋| 34/35 [00:05<00:00,  7.37it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:05<00:00,  6.75it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 2891.17it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:03<13:42,  3.22s/it, est. speed input: 4.96 toks/s, output: 79.41 toks/s]
Processed prompts:  46%|████▌     | 118/256 [00:03<00:02, 49.80it/s, est. speed input: 566.87 toks/s, output: 9069.91 toks/s]
Processed prompts:  82%|████████▏ | 210/256 [00:03<00:00, 99.37it/s, est. speed input: 979.28 toks/s, output: 15668.37 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 99.37it/s, est. speed input: 1150.13 toks/s, output: 18402.04 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 71.88it/s, est. speed input: 1150.13 toks/s, output: 18402.04 toks/s]
[rank0]:[W126 09:11:25.417111711 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-26 09:11:28
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6
Params: prompt_len=16, output_len=256, num_prompts=512, max_num_seqs=512
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 256 --num-prompts 512 --max-num-seqs 512 --max-model-len 272 --max-num-batched-tokens 512 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/json/Llama3.2-1B-FP8_M512.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 09:11:34 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 09:11:36 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=445879) WARNING 01-26 09:11:44 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=445879) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=445879) WARNING 01-26 09:11:52 [backends.py:609] Failed to read file <frozen os>
Throughput: 71.92 requests/s, 19562.76 total tokens/s, 18412.01 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072

STDERR:
[2026-01-26 09:11:34] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 09:11:34] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 09:11:34] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 09:11:34] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 09:11:34] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 09:11:34] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 09:11:34] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 09:11:34] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 09:11:34] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 09:11:34] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 09:11:34] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 09:11:34] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 09:11:34] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 09:11:34] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 09:11:43] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 09:11:43] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-FP8'
[2026-01-26 09:11:43] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-26 09:11:43] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 09:11:43] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 09:11:43] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 09:11:43] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 09:11:43] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-26 09:11:43] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-FP8'
[2026-01-26 09:11:43] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 09:11:43] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 09:11:43] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 09:11:43] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 09:11:43] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=445879) [2026-01-26 09:11:44] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=445879) [2026-01-26 09:11:44] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=445879) [2026-01-26 09:11:44] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=445879) [2026-01-26 09:11:44] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=445879) [2026-01-26 09:11:44] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8
(EngineCore_DP0 pid=445879) [2026-01-26 09:11:44] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=445879) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=445879) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.60it/s]
(EngineCore_DP0 pid=445879) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.60it/s]
(EngineCore_DP0 pid=445879) 
(EngineCore_DP0 pid=445879) [2026-01-26 09:11:45] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=445879) [2026-01-26 09:11:45] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6389760 bytes
(EngineCore_DP0 pid=445879) [2026-01-26 09:11:45] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=445879) [2026-01-26 09:11:45] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4259840 bytes
(EngineCore_DP0 pid=445879) [2026-01-26 09:11:45] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=445879) [2026-01-26 09:11:45] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 34078720 bytes
(EngineCore_DP0 pid=445879) [2026-01-26 09:11:45] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=445879) [2026-01-26 09:11:45] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16842752 bytes
(EngineCore_DP0 pid=445879) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|▏         | 1/51 [00:00<00:14,  3.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|▍         | 2/51 [00:00<00:08,  5.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 3/51 [00:00<00:06,  6.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 4/51 [00:00<00:06,  7.71it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|▉         | 5/51 [00:00<00:05,  8.33it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▎        | 7/51 [00:00<00:04,  9.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 9/51 [00:01<00:04,  9.58it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 11/51 [00:01<00:04,  9.73it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|██▎       | 12/51 [00:01<00:03,  9.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 13/51 [00:01<00:03,  9.82it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▉       | 15/51 [00:01<00:03,  9.86it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 17/51 [00:01<00:03,  9.22it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  35%|███▌      | 18/51 [00:02<00:03,  8.91it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 19/51 [00:02<00:06,  4.79it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|███▉      | 20/51 [00:02<00:06,  4.96it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|████      | 21/51 [00:03<00:07,  3.84it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 22/51 [00:03<00:06,  4.22it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 23/51 [00:03<00:05,  5.03it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▉     | 25/51 [00:03<00:04,  6.41it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████     | 26/51 [00:03<00:03,  6.93it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 27/51 [00:03<00:03,  7.35it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 28/51 [00:04<00:02,  7.80it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 29/51 [00:04<00:02,  8.09it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  59%|█████▉    | 30/51 [00:04<00:02,  8.34it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|██████    | 31/51 [00:04<00:02,  8.58it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 32/51 [00:04<00:02,  8.66it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  65%|██████▍   | 33/51 [00:04<00:02,  8.63it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 34/51 [00:04<00:01,  8.54it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 35/51 [00:04<00:01,  8.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████   | 36/51 [00:04<00:01,  8.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 37/51 [00:05<00:01,  8.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▍  | 38/51 [00:05<00:01,  7.88it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|███████▋  | 39/51 [00:05<00:02,  4.25it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 40/51 [00:05<00:02,  5.06it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 41/51 [00:05<00:01,  5.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 42/51 [00:06<00:02,  3.71it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 43/51 [00:06<00:01,  4.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▋ | 44/51 [00:06<00:01,  5.39it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  88%|████████▊ | 45/51 [00:06<00:00,  6.08it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|█████████ | 46/51 [00:06<00:00,  6.80it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 47/51 [00:06<00:00,  7.39it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  96%|█████████▌| 49/51 [00:07<00:00,  8.34it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  98%|█████████▊| 50/51 [00:07<00:00,  8.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:07<00:00,  8.15it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:07<00:00,  6.88it/s]
(EngineCore_DP0 pid=445879) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   2%|▏         | 1/51 [00:00<00:08,  5.68it/s]
Capturing CUDA graphs (decode, FULL):   4%|▍         | 2/51 [00:00<00:07,  6.56it/s]
Capturing CUDA graphs (decode, FULL):   6%|▌         | 3/51 [00:00<00:06,  6.95it/s]
Capturing CUDA graphs (decode, FULL):   8%|▊         | 4/51 [00:00<00:06,  7.38it/s]
Capturing CUDA graphs (decode, FULL):  10%|▉         | 5/51 [00:00<00:05,  7.70it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▎        | 7/51 [00:00<00:05,  7.65it/s]
Capturing CUDA graphs (decode, FULL):  16%|█▌        | 8/51 [00:01<00:09,  4.62it/s]
Capturing CUDA graphs (decode, FULL):  18%|█▊        | 9/51 [00:01<00:07,  5.26it/s]
Capturing CUDA graphs (decode, FULL):  20%|█▉        | 10/51 [00:01<00:08,  5.06it/s]
Capturing CUDA graphs (decode, FULL):  22%|██▏       | 11/51 [00:02<00:09,  4.17it/s]
Capturing CUDA graphs (decode, FULL):  24%|██▎       | 12/51 [00:02<00:08,  4.51it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 13/51 [00:02<00:07,  5.28it/s]
Capturing CUDA graphs (decode, FULL):  27%|██▋       | 14/51 [00:02<00:06,  6.01it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▉       | 15/51 [00:02<00:05,  6.63it/s]
Capturing CUDA graphs (decode, FULL):  31%|███▏      | 16/51 [00:02<00:04,  7.23it/s]
Capturing CUDA graphs (decode, FULL):  33%|███▎      | 17/51 [00:02<00:04,  7.80it/s]
Capturing CUDA graphs (decode, FULL):  35%|███▌      | 18/51 [00:02<00:04,  8.24it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 19/51 [00:03<00:03,  8.64it/s]
Capturing CUDA graphs (decode, FULL):  39%|███▉      | 20/51 [00:03<00:03,  8.97it/s]
Capturing CUDA graphs (decode, FULL):  41%|████      | 21/51 [00:03<00:03,  9.19it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 22/51 [00:03<00:03,  9.21it/s]
Capturing CUDA graphs (decode, FULL):  47%|████▋     | 24/51 [00:03<00:02,  9.61it/s]
Capturing CUDA graphs (decode, FULL):  51%|█████     | 26/51 [00:03<00:02,  9.83it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 28/51 [00:03<00:02,  9.75it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 29/51 [00:04<00:02,  9.09it/s]
Capturing CUDA graphs (decode, FULL):  59%|█████▉    | 30/51 [00:04<00:04,  5.07it/s]
Capturing CUDA graphs (decode, FULL):  61%|██████    | 31/51 [00:04<00:03,  5.69it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 32/51 [00:04<00:03,  6.01it/s]
Capturing CUDA graphs (decode, FULL):  65%|██████▍   | 33/51 [00:05<00:03,  4.81it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 34/51 [00:05<00:03,  4.37it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████   | 36/51 [00:05<00:02,  5.85it/s]
Capturing CUDA graphs (decode, FULL):  75%|███████▍  | 38/51 [00:05<00:01,  6.97it/s]
Capturing CUDA graphs (decode, FULL):  78%|███████▊  | 40/51 [00:06<00:01,  7.91it/s]
Capturing CUDA graphs (decode, FULL):  82%|████████▏ | 42/51 [00:06<00:01,  8.63it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▋ | 44/51 [00:06<00:00,  9.10it/s]
Capturing CUDA graphs (decode, FULL):  90%|█████████ | 46/51 [00:06<00:00,  9.49it/s]
Capturing CUDA graphs (decode, FULL):  94%|█████████▍| 48/51 [00:06<00:00,  9.76it/s]
Capturing CUDA graphs (decode, FULL):  98%|█████████▊| 50/51 [00:06<00:00,  9.99it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:07<00:00,  7.22it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:  59%|█████▊    | 300/512 [00:00<00:00, 2997.50it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 3056.55it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:05<46:13,  5.43s/it, est. speed input: 2.95 toks/s, output: 47.17 toks/s]
Processed prompts:  12%|█▏        | 63/512 [00:05<00:27, 16.04it/s, est. speed input: 182.02 toks/s, output: 2912.36 toks/s]
Processed prompts:  36%|███▌      | 185/512 [00:05<00:05, 58.17it/s, est. speed input: 523.64 toks/s, output: 8378.23 toks/s]
Processed prompts:  62%|██████▏   | 315/512 [00:05<00:01, 117.10it/s, est. speed input: 875.24 toks/s, output: 14003.84 toks/s]
Processed prompts:  82%|████████▏ | 418/512 [00:05<00:00, 175.10it/s, est. speed input: 1141.44 toks/s, output: 18262.94 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:06<00:00, 130.87it/s, est. speed input: 1178.78 toks/s, output: 18860.51 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:06<00:00, 130.87it/s, est. speed input: 1178.78 toks/s, output: 18860.51 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:06<00:00, 73.67it/s, est. speed input: 1178.78 toks/s, output: 18860.51 toks/s] 
[rank0]:[W126 09:12:27.279511365 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=64 ==========
Time: 2026-01-26 09:49:25
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_6
Params: prompt_len=16, output_len=256, num_prompts=64, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 256 --num-prompts 64 --max-num-seqs 64 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/json/Llama3.2-3B-FP8_M64.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 09:49:34 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 09:49:35 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=485337) WARNING 01-26 09:49:53 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=485337) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=485337) WARNING 01-26 09:50:10 [backends.py:609] Failed to read file <frozen os>
Throughput: 19.07 requests/s, 5187.83 total tokens/s, 4882.66 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384

STDERR:
[2026-01-26 09:49:34] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 09:49:34] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-26 09:49:34] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-26 09:49:34] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 09:49:34] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 09:49:34] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 09:49:34] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 09:49:34] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 09:49:34] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-26 09:49:34] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 09:49:34] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 09:49:34] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 09:49:34] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 09:49:34] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 09:49:42] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 09:49:43] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-26 09:49:43] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-26 09:49:43] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 09:49:43] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 09:49:43] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 09:49:43] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 09:49:43] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 09:49:43] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-26 09:49:43] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 09:49:43] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 09:49:43] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 09:49:43] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 09:49:43] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[W126 09:49:52.944390096 socket.cpp:209] [c10d] The hostname of the client socket cannot be retrieved. err=-3
(EngineCore_DP0 pid=485337) [2026-01-26 09:49:53] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=485337) [2026-01-26 09:49:53] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=485337) [2026-01-26 09:49:53] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=485337) [2026-01-26 09:49:53] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=485337) [2026-01-26 09:49:53] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=485337) [2026-01-26 09:49:53] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=485337) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=485337) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:04<00:00,  4.20s/it]
(EngineCore_DP0 pid=485337) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:04<00:00,  4.20s/it]
(EngineCore_DP0 pid=485337) 
(EngineCore_DP0 pid=485337) [2026-01-26 09:49:59] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 4096] -> 1D uint8
(EngineCore_DP0 pid=485337) [2026-01-26 09:49:59] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15728640 bytes
(EngineCore_DP0 pid=485337) [2026-01-26 09:49:59] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 4096] -> 1D uint8
(EngineCore_DP0 pid=485337) [2026-01-26 09:49:59] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9437184 bytes
(EngineCore_DP0 pid=485337) [2026-01-26 09:49:59] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 4096] -> 1D uint8
(EngineCore_DP0 pid=485337) [2026-01-26 09:49:59] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 50331648 bytes
(EngineCore_DP0 pid=485337) [2026-01-26 09:49:59] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 10944] -> 1D uint8
(EngineCore_DP0 pid=485337) [2026-01-26 09:49:59] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 25264128 bytes
(EngineCore_DP0 pid=485337) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   5%|▌         | 1/19 [00:00<00:02,  7.39it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 2/19 [00:00<00:02,  7.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 3/19 [00:00<00:02,  7.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  21%|██        | 4/19 [00:00<00:01,  8.08it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▋       | 5/19 [00:00<00:01,  8.26it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  32%|███▏      | 6/19 [00:00<00:01,  8.41it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 7/19 [00:00<00:01,  8.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 8/19 [00:00<00:01,  8.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 9/19 [00:01<00:01,  8.22it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 10/19 [00:01<00:01,  4.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 11/19 [00:01<00:02,  3.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 12/19 [00:02<00:02,  2.86it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  68%|██████▊   | 13/19 [00:02<00:01,  3.44it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▎  | 14/19 [00:02<00:01,  4.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  79%|███████▉  | 15/19 [00:02<00:00,  4.97it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 16/19 [00:02<00:00,  5.72it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 17/19 [00:03<00:00,  6.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  95%|█████████▍| 18/19 [00:03<00:00,  7.03it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:03<00:00,  6.93it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:03<00:00,  5.72it/s]
(EngineCore_DP0 pid=485337) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   9%|▉         | 1/11 [00:00<00:01,  7.27it/s]
Capturing CUDA graphs (decode, FULL):  18%|█▊        | 2/11 [00:00<00:01,  8.05it/s]
Capturing CUDA graphs (decode, FULL):  27%|██▋       | 3/11 [00:00<00:00,  8.36it/s]
Capturing CUDA graphs (decode, FULL):  36%|███▋      | 4/11 [00:00<00:00,  8.54it/s]
Capturing CUDA graphs (decode, FULL):  45%|████▌     | 5/11 [00:00<00:00,  8.75it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 6/11 [00:00<00:00,  8.78it/s]
Capturing CUDA graphs (decode, FULL):  64%|██████▎   | 7/11 [00:00<00:00,  8.92it/s]
Capturing CUDA graphs (decode, FULL):  73%|███████▎  | 8/11 [00:00<00:00,  8.99it/s]
Capturing CUDA graphs (decode, FULL):  82%|████████▏ | 9/11 [00:01<00:00,  7.53it/s]
Capturing CUDA graphs (decode, FULL):  91%|█████████ | 10/11 [00:01<00:00,  4.37it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:01<00:00,  5.18it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:01<00:00,  6.64it/s]

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 64/64 [00:00<00:00, 2750.81it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 1/64 [00:03<03:26,  3.28s/it, est. speed input: 4.87 toks/s, output: 77.99 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:03<00:00,  3.28s/it, est. speed input: 307.45 toks/s, output: 4919.12 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:03<00:00, 19.21it/s, est. speed input: 307.45 toks/s, output: 4919.12 toks/s]
[rank0]:[W126 09:50:36.903210451 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-26 09:50:39
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_6
Params: prompt_len=16, output_len=256, num_prompts=128, max_num_seqs=128
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 256 --num-prompts 128 --max-num-seqs 128 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/json/Llama3.2-3B-FP8_M128.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 09:50:48 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 09:50:49 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=486595) WARNING 01-26 09:50:55 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=486595) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=486595) WARNING 01-26 09:51:08 [backends.py:609] Failed to read file <frozen os>
Throughput: 31.92 requests/s, 8683.35 total tokens/s, 8172.56 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768

STDERR:
[2026-01-26 09:50:47] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 09:50:48] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-26 09:50:48] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-26 09:50:48] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 09:50:48] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 09:50:48] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 09:50:48] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 09:50:48] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 09:50:48] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-26 09:50:48] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 09:50:48] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 09:50:48] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 09:50:48] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 09:50:48] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 09:50:54] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 09:50:54] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-26 09:50:54] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-26 09:50:54] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 09:50:54] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 09:50:54] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 09:50:54] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 09:50:54] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 09:50:54] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-26 09:50:54] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 09:50:54] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 09:50:54] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 09:50:54] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 09:50:54] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=486595) [2026-01-26 09:50:56] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=486595) [2026-01-26 09:50:56] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=486595) [2026-01-26 09:50:56] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=486595) [2026-01-26 09:50:56] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=486595) [2026-01-26 09:50:56] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=486595) [2026-01-26 09:50:56] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=486595) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=486595) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.11it/s]
(EngineCore_DP0 pid=486595) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.11it/s]
(EngineCore_DP0 pid=486595) 
(EngineCore_DP0 pid=486595) [2026-01-26 09:50:58] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 4096] -> 1D uint8
(EngineCore_DP0 pid=486595) [2026-01-26 09:50:58] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15728640 bytes
(EngineCore_DP0 pid=486595) [2026-01-26 09:50:58] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 4096] -> 1D uint8
(EngineCore_DP0 pid=486595) [2026-01-26 09:50:58] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9437184 bytes
(EngineCore_DP0 pid=486595) [2026-01-26 09:50:58] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 4096] -> 1D uint8
(EngineCore_DP0 pid=486595) [2026-01-26 09:50:58] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 50331648 bytes
(EngineCore_DP0 pid=486595) [2026-01-26 09:50:58] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 10944] -> 1D uint8
(EngineCore_DP0 pid=486595) [2026-01-26 09:50:58] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 25264128 bytes
(EngineCore_DP0 pid=486595) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|▎         | 1/35 [00:00<00:09,  3.45it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 2/35 [00:00<00:09,  3.45it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▊         | 3/35 [00:00<00:06,  4.71it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█▏        | 4/35 [00:00<00:05,  5.74it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 5/35 [00:00<00:04,  6.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 6/35 [00:01<00:04,  6.34it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 7/35 [00:01<00:05,  5.11it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 13/35 [00:01<00:01, 14.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 15/35 [00:01<00:01, 12.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▊     | 17/35 [00:01<00:01, 11.04it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  54%|█████▍    | 19/35 [00:02<00:01, 10.05it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 21/35 [00:02<00:01,  9.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  66%|██████▌   | 23/35 [00:02<00:01,  9.17it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 25/35 [00:03<00:01,  5.25it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▍  | 26/35 [00:03<00:01,  5.36it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  77%|███████▋  | 27/35 [00:04<00:02,  3.84it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 28/35 [00:04<00:01,  4.05it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  83%|████████▎ | 29/35 [00:04<00:01,  4.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 30/35 [00:04<00:00,  5.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▊ | 31/35 [00:04<00:00,  5.80it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████▏| 32/35 [00:04<00:00,  6.39it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 33/35 [00:04<00:00,  6.88it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  97%|█████████▋| 34/35 [00:05<00:00,  7.32it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:05<00:00,  7.13it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:05<00:00,  6.67it/s]
(EngineCore_DP0 pid=486595) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   5%|▌         | 1/19 [00:00<00:02,  6.74it/s]
Capturing CUDA graphs (decode, FULL):  11%|█         | 2/19 [00:00<00:02,  7.61it/s]
Capturing CUDA graphs (decode, FULL):  16%|█▌        | 3/19 [00:00<00:01,  8.06it/s]
Capturing CUDA graphs (decode, FULL):  21%|██        | 4/19 [00:00<00:01,  8.22it/s]
Capturing CUDA graphs (decode, FULL):  26%|██▋       | 5/19 [00:00<00:01,  8.37it/s]
Capturing CUDA graphs (decode, FULL):  32%|███▏      | 6/19 [00:00<00:01,  8.50it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 7/19 [00:01<00:03,  3.58it/s]
Capturing CUDA graphs (decode, FULL):  42%|████▏     | 8/19 [00:01<00:02,  4.12it/s]
Capturing CUDA graphs (decode, FULL):  47%|████▋     | 9/19 [00:01<00:03,  3.33it/s]
Capturing CUDA graphs (decode, FULL):  53%|█████▎    | 10/19 [00:02<00:02,  3.39it/s]
Capturing CUDA graphs (decode, FULL):  58%|█████▊    | 11/19 [00:02<00:01,  4.19it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 12/19 [00:02<00:01,  4.96it/s]
Capturing CUDA graphs (decode, FULL):  68%|██████▊   | 13/19 [00:02<00:01,  5.72it/s]
Capturing CUDA graphs (decode, FULL):  74%|███████▎  | 14/19 [00:02<00:00,  6.41it/s]
Capturing CUDA graphs (decode, FULL):  79%|███████▉  | 15/19 [00:02<00:00,  6.96it/s]
Capturing CUDA graphs (decode, FULL):  84%|████████▍ | 16/19 [00:02<00:00,  6.88it/s]
Capturing CUDA graphs (decode, FULL):  89%|████████▉ | 17/19 [00:03<00:00,  7.41it/s]
Capturing CUDA graphs (decode, FULL):  95%|█████████▍| 18/19 [00:03<00:00,  7.80it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:03<00:00,  7.91it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:03<00:00,  5.79it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 2860.02it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:03<08:08,  3.84s/it, est. speed input: 4.16 toks/s, output: 66.61 toks/s]
Processed prompts:  84%|████████▍ | 108/128 [00:03<00:00, 38.54it/s, est. speed input: 437.76 toks/s, output: 7004.08 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 38.54it/s, est. speed input: 516.76 toks/s, output: 8268.08 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 32.30it/s, est. speed input: 516.76 toks/s, output: 8268.08 toks/s]
[rank0]:[W126 09:51:35.882467463 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-26 09:51:38
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_6
Params: prompt_len=16, output_len=256, num_prompts=256, max_num_seqs=256
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 256 --num-prompts 256 --max-num-seqs 256 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/json/Llama3.2-3B-FP8_M256.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 09:51:46 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 09:51:48 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=487633) WARNING 01-26 09:51:55 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=487633) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=487633) WARNING 01-26 09:52:07 [backends.py:609] Failed to read file <frozen os>
Throughput: 44.61 requests/s, 12134.81 total tokens/s, 11420.99 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536

STDERR:
[2026-01-26 09:51:46] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 09:51:46] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-26 09:51:46] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-26 09:51:46] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 09:51:46] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 09:51:46] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 09:51:46] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 09:51:46] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 09:51:46] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-26 09:51:46] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 09:51:46] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 09:51:46] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 09:51:46] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 09:51:46] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 09:51:53] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 09:51:54] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-26 09:51:54] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-26 09:51:54] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 09:51:54] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 09:51:54] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 09:51:54] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 09:51:54] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 09:51:54] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-26 09:51:54] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 09:51:54] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 09:51:54] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 09:51:54] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 09:51:54] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=487633) [2026-01-26 09:51:55] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=487633) [2026-01-26 09:51:55] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=487633) [2026-01-26 09:51:55] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=487633) [2026-01-26 09:51:55] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=487633) [2026-01-26 09:51:55] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=487633) [2026-01-26 09:51:55] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=487633) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=487633) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.12it/s]
(EngineCore_DP0 pid=487633) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.11it/s]
(EngineCore_DP0 pid=487633) 
(EngineCore_DP0 pid=487633) [2026-01-26 09:51:57] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 4096] -> 1D uint8
(EngineCore_DP0 pid=487633) [2026-01-26 09:51:57] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15728640 bytes
(EngineCore_DP0 pid=487633) [2026-01-26 09:51:57] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 4096] -> 1D uint8
(EngineCore_DP0 pid=487633) [2026-01-26 09:51:57] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9437184 bytes
(EngineCore_DP0 pid=487633) [2026-01-26 09:51:57] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 4096] -> 1D uint8
(EngineCore_DP0 pid=487633) [2026-01-26 09:51:57] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 50331648 bytes
(EngineCore_DP0 pid=487633) [2026-01-26 09:51:57] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 10944] -> 1D uint8
(EngineCore_DP0 pid=487633) [2026-01-26 09:51:57] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 25264128 bytes
(EngineCore_DP0 pid=487633) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/36 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|▎         | 1/36 [00:00<00:04,  8.45it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 2/36 [00:00<00:04,  8.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 3/36 [00:00<00:03,  8.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 4/36 [00:00<00:03,  8.34it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 5/36 [00:00<00:03,  8.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 6/36 [00:00<00:03,  8.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  19%|█▉        | 7/36 [00:00<00:03,  8.59it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 8/36 [00:00<00:03,  8.59it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 9/36 [00:01<00:03,  8.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  28%|██▊       | 10/36 [00:01<00:03,  7.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███       | 11/36 [00:01<00:04,  6.24it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 12/36 [00:01<00:06,  3.94it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▌      | 13/36 [00:02<00:07,  3.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|███▉      | 14/36 [00:02<00:07,  2.90it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 15/36 [00:02<00:05,  3.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  44%|████▍     | 16/36 [00:03<00:04,  4.22it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 17/36 [00:03<00:03,  4.90it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 18/36 [00:03<00:03,  5.39it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 19/36 [00:03<00:02,  5.80it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  56%|█████▌    | 20/36 [00:03<00:02,  6.13it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 21/36 [00:03<00:02,  6.55it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|██████    | 22/36 [00:03<00:02,  6.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  81%|████████  | 29/36 [00:04<00:00, 11.37it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  83%|████████▎ | 30/36 [00:04<00:00, 10.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 31/36 [00:04<00:00,  9.32it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 32/36 [00:04<00:00,  8.34it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 33/36 [00:04<00:00,  7.82it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 34/36 [00:05<00:00,  7.81it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  97%|█████████▋| 35/36 [00:05<00:00,  7.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:05<00:00,  7.22it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:05<00:00,  6.66it/s]
(EngineCore_DP0 pid=487633) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   3%|▎         | 1/35 [00:00<00:05,  6.66it/s]
Capturing CUDA graphs (decode, FULL):   6%|▌         | 2/35 [00:00<00:05,  5.85it/s]
Capturing CUDA graphs (decode, FULL):   9%|▊         | 3/35 [00:00<00:04,  6.72it/s]
Capturing CUDA graphs (decode, FULL):  11%|█▏        | 4/35 [00:00<00:04,  7.28it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 5/35 [00:00<00:04,  6.50it/s]
Capturing CUDA graphs (decode, FULL):  17%|█▋        | 6/35 [00:01<00:07,  3.72it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 7/35 [00:01<00:06,  4.54it/s]
Capturing CUDA graphs (decode, FULL):  23%|██▎       | 8/35 [00:01<00:05,  4.55it/s]
Capturing CUDA graphs (decode, FULL):  26%|██▌       | 9/35 [00:01<00:07,  3.67it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 10/35 [00:02<00:05,  4.46it/s]
Capturing CUDA graphs (decode, FULL):  31%|███▏      | 11/35 [00:02<00:04,  5.21it/s]
Capturing CUDA graphs (decode, FULL):  34%|███▍      | 12/35 [00:02<00:03,  5.92it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 13/35 [00:02<00:03,  6.52it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 14/35 [00:02<00:02,  7.05it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 15/35 [00:02<00:02,  7.53it/s]
Capturing CUDA graphs (decode, FULL):  46%|████▌     | 16/35 [00:02<00:02,  7.78it/s]
Capturing CUDA graphs (decode, FULL):  49%|████▊     | 17/35 [00:02<00:02,  8.00it/s]
Capturing CUDA graphs (decode, FULL):  51%|█████▏    | 18/35 [00:03<00:02,  8.20it/s]
Capturing CUDA graphs (decode, FULL):  54%|█████▍    | 19/35 [00:03<00:01,  8.36it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 20/35 [00:03<00:01,  8.37it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 21/35 [00:03<00:01,  8.43it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 22/35 [00:03<00:01,  8.43it/s]
Capturing CUDA graphs (decode, FULL):  66%|██████▌   | 23/35 [00:03<00:01,  8.29it/s]
Capturing CUDA graphs (decode, FULL):  69%|██████▊   | 24/35 [00:03<00:01,  8.36it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 25/35 [00:03<00:01,  8.13it/s]
Capturing CUDA graphs (decode, FULL):  74%|███████▍  | 26/35 [00:04<00:02,  4.07it/s]
Capturing CUDA graphs (decode, FULL):  77%|███████▋  | 27/35 [00:04<00:01,  4.81it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 28/35 [00:04<00:01,  4.68it/s]
Capturing CUDA graphs (decode, FULL):  83%|████████▎ | 29/35 [00:04<00:01,  4.47it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 30/35 [00:05<00:01,  4.39it/s]
Capturing CUDA graphs (decode, FULL):  89%|████████▊ | 31/35 [00:05<00:00,  5.17it/s]
Capturing CUDA graphs (decode, FULL):  91%|█████████▏| 32/35 [00:05<00:00,  5.95it/s]
Capturing CUDA graphs (decode, FULL):  94%|█████████▍| 33/35 [00:05<00:00,  6.61it/s]
Capturing CUDA graphs (decode, FULL):  97%|█████████▋| 34/35 [00:05<00:00,  7.19it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:05<00:00,  7.57it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:05<00:00,  6.04it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 2812.24it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:05<21:26,  5.05s/it, est. speed input: 3.17 toks/s, output: 50.74 toks/s]
Processed prompts:  24%|██▍       | 62/256 [00:05<00:11, 16.96it/s, est. speed input: 192.55 toks/s, output: 3080.78 toks/s]
Processed prompts:  50%|████▉     | 127/256 [00:05<00:03, 40.84it/s, est. speed input: 386.72 toks/s, output: 6187.47 toks/s]
Processed prompts:  71%|███████▏  | 183/256 [00:05<00:01, 67.29it/s, est. speed input: 546.52 toks/s, output: 8744.29 toks/s]
Processed prompts:  91%|█████████▏| 234/256 [00:05<00:00, 94.52it/s, est. speed input: 680.44 toks/s, output: 10886.93 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:05<00:00, 94.52it/s, est. speed input: 725.54 toks/s, output: 11608.58 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:05<00:00, 45.34it/s, est. speed input: 725.54 toks/s, output: 11608.58 toks/s]
[rank0]:[W126 09:52:39.036548252 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-26 09:52:42
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_6
Params: prompt_len=16, output_len=256, num_prompts=512, max_num_seqs=512
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 256 --num-prompts 512 --max-num-seqs 512 --max-model-len 272 --max-num-batched-tokens 512 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/json/Llama3.2-3B-FP8_M512.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 09:52:51 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 09:52:52 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=488703) WARNING 01-26 09:52:58 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=488703) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=488703) WARNING 01-26 09:53:10 [backends.py:609] Failed to read file <frozen os>
Throughput: 43.26 requests/s, 11766.57 total tokens/s, 11074.42 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072

STDERR:
[2026-01-26 09:52:50] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 09:52:51] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-26 09:52:51] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-26 09:52:51] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 09:52:51] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 09:52:51] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 09:52:51] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 09:52:51] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 09:52:51] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-26 09:52:51] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 09:52:51] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 09:52:51] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 09:52:51] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 09:52:51] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 09:52:57] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 09:52:57] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-26 09:52:57] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-26 09:52:57] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 09:52:57] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 09:52:57] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 09:52:57] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 09:52:57] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-26 09:52:57] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-26 09:52:57] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 09:52:57] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 09:52:57] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 09:52:57] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 09:52:57] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=488703) [2026-01-26 09:52:59] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=488703) [2026-01-26 09:52:59] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=488703) [2026-01-26 09:52:59] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=488703) [2026-01-26 09:52:59] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=488703) [2026-01-26 09:52:59] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=488703) [2026-01-26 09:52:59] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=488703) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=488703) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.08it/s]
(EngineCore_DP0 pid=488703) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.08it/s]
(EngineCore_DP0 pid=488703) 
(EngineCore_DP0 pid=488703) [2026-01-26 09:53:00] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 4096] -> 1D uint8
(EngineCore_DP0 pid=488703) [2026-01-26 09:53:00] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15728640 bytes
(EngineCore_DP0 pid=488703) [2026-01-26 09:53:00] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 4096] -> 1D uint8
(EngineCore_DP0 pid=488703) [2026-01-26 09:53:00] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9437184 bytes
(EngineCore_DP0 pid=488703) [2026-01-26 09:53:00] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 4096] -> 1D uint8
(EngineCore_DP0 pid=488703) [2026-01-26 09:53:00] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 50331648 bytes
(EngineCore_DP0 pid=488703) [2026-01-26 09:53:00] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 10944] -> 1D uint8
(EngineCore_DP0 pid=488703) [2026-01-26 09:53:00] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 25264128 bytes
(EngineCore_DP0 pid=488703) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|▏         | 1/51 [00:00<00:05,  8.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|▍         | 2/51 [00:00<00:05,  8.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 3/51 [00:00<00:05,  8.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 4/51 [00:00<00:05,  8.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|▉         | 5/51 [00:00<00:05,  8.60it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|█▏        | 6/51 [00:00<00:05,  8.59it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▎        | 7/51 [00:00<00:05,  8.59it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 8/51 [00:00<00:05,  8.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 9/51 [00:01<00:05,  8.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|█▉        | 10/51 [00:01<00:08,  4.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 11/51 [00:01<00:09,  4.01it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|██▎       | 12/51 [00:02<00:09,  4.06it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 13/51 [00:02<00:11,  3.24it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 14/51 [00:02<00:09,  3.99it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▉       | 15/51 [00:02<00:07,  4.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 16/51 [00:02<00:06,  5.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 17/51 [00:02<00:05,  6.07it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  35%|███▌      | 18/51 [00:03<00:04,  6.60it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 19/51 [00:03<00:04,  7.05it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|███▉      | 20/51 [00:03<00:04,  7.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|████      | 21/51 [00:03<00:03,  7.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 22/51 [00:03<00:03,  7.90it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 23/51 [00:03<00:03,  8.04it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 24/51 [00:03<00:03,  8.25it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▉     | 25/51 [00:03<00:03,  8.22it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████     | 26/51 [00:04<00:02,  8.35it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 27/51 [00:04<00:03,  7.84it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 28/51 [00:04<00:06,  3.55it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 29/51 [00:04<00:05,  4.07it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  59%|█████▉    | 30/51 [00:05<00:07,  2.91it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|██████    | 31/51 [00:05<00:06,  3.24it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 32/51 [00:05<00:04,  3.88it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  65%|██████▍   | 33/51 [00:06<00:04,  4.42it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 34/51 [00:06<00:03,  4.94it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 35/51 [00:06<00:02,  5.44it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████   | 36/51 [00:06<00:02,  5.91it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 37/51 [00:06<00:02,  6.29it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▍  | 38/51 [00:06<00:01,  6.67it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|███████▋  | 39/51 [00:06<00:01,  6.93it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 40/51 [00:07<00:01,  7.25it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 41/51 [00:07<00:01,  7.26it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 42/51 [00:07<00:01,  6.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 43/51 [00:07<00:01,  5.70it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▋ | 44/51 [00:07<00:01,  4.07it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  88%|████████▊ | 45/51 [00:08<00:01,  4.32it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|█████████ | 46/51 [00:08<00:01,  2.93it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 47/51 [00:08<00:01,  3.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 48/51 [00:09<00:00,  3.91it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  96%|█████████▌| 49/51 [00:09<00:00,  4.34it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  98%|█████████▊| 50/51 [00:09<00:00,  4.70it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:09<00:00,  5.04it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:09<00:00,  5.29it/s]
(EngineCore_DP0 pid=488703) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   2%|▏         | 1/51 [00:00<00:08,  5.84it/s]
Capturing CUDA graphs (decode, FULL):   4%|▍         | 2/51 [00:00<00:07,  6.55it/s]
Capturing CUDA graphs (decode, FULL):   6%|▌         | 3/51 [00:00<00:06,  6.96it/s]
Capturing CUDA graphs (decode, FULL):   8%|▊         | 4/51 [00:00<00:06,  6.86it/s]
Capturing CUDA graphs (decode, FULL):  10%|▉         | 5/51 [00:00<00:06,  7.07it/s]
Capturing CUDA graphs (decode, FULL):  12%|█▏        | 6/51 [00:00<00:06,  6.63it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▎        | 7/51 [00:01<00:07,  6.24it/s]
Capturing CUDA graphs (decode, FULL):  16%|█▌        | 8/51 [00:01<00:10,  4.25it/s]
Capturing CUDA graphs (decode, FULL):  18%|█▊        | 9/51 [00:01<00:09,  4.63it/s]
Capturing CUDA graphs (decode, FULL):  20%|█▉        | 10/51 [00:02<00:13,  3.08it/s]
Capturing CUDA graphs (decode, FULL):  22%|██▏       | 11/51 [00:02<00:11,  3.47it/s]
Capturing CUDA graphs (decode, FULL):  24%|██▎       | 12/51 [00:02<00:09,  4.21it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 13/51 [00:02<00:07,  4.94it/s]
Capturing CUDA graphs (decode, FULL):  27%|██▋       | 14/51 [00:02<00:06,  5.61it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▉       | 15/51 [00:02<00:05,  6.19it/s]
Capturing CUDA graphs (decode, FULL):  31%|███▏      | 16/51 [00:03<00:05,  6.74it/s]
Capturing CUDA graphs (decode, FULL):  33%|███▎      | 17/51 [00:03<00:04,  7.25it/s]
Capturing CUDA graphs (decode, FULL):  35%|███▌      | 18/51 [00:03<00:04,  7.67it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 19/51 [00:03<00:04,  7.99it/s]
Capturing CUDA graphs (decode, FULL):  39%|███▉      | 20/51 [00:03<00:03,  8.25it/s]
Capturing CUDA graphs (decode, FULL):  41%|████      | 21/51 [00:03<00:03,  8.53it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 22/51 [00:03<00:03,  8.71it/s]
Capturing CUDA graphs (decode, FULL):  45%|████▌     | 23/51 [00:03<00:03,  8.61it/s]
Capturing CUDA graphs (decode, FULL):  47%|████▋     | 24/51 [00:03<00:03,  8.63it/s]
Capturing CUDA graphs (decode, FULL):  49%|████▉     | 25/51 [00:04<00:03,  7.25it/s]
Capturing CUDA graphs (decode, FULL):  51%|█████     | 26/51 [00:04<00:06,  4.10it/s]
Capturing CUDA graphs (decode, FULL):  53%|█████▎    | 27/51 [00:04<00:04,  4.88it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 28/51 [00:04<00:04,  5.08it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 29/51 [00:05<00:05,  4.38it/s]
Capturing CUDA graphs (decode, FULL):  59%|█████▉    | 30/51 [00:05<00:05,  3.92it/s]
Capturing CUDA graphs (decode, FULL):  61%|██████    | 31/51 [00:05<00:04,  4.70it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 32/51 [00:05<00:03,  5.37it/s]
Capturing CUDA graphs (decode, FULL):  65%|██████▍   | 33/51 [00:05<00:02,  6.04it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 34/51 [00:05<00:02,  6.65it/s]
Capturing CUDA graphs (decode, FULL):  69%|██████▊   | 35/51 [00:06<00:02,  7.11it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████   | 36/51 [00:06<00:01,  7.52it/s]
Capturing CUDA graphs (decode, FULL):  73%|███████▎  | 37/51 [00:06<00:01,  7.75it/s]
Capturing CUDA graphs (decode, FULL):  75%|███████▍  | 38/51 [00:06<00:01,  8.01it/s]
Capturing CUDA graphs (decode, FULL):  76%|███████▋  | 39/51 [00:06<00:01,  8.11it/s]
Capturing CUDA graphs (decode, FULL):  78%|███████▊  | 40/51 [00:06<00:01,  8.20it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 41/51 [00:06<00:01,  8.31it/s]
Capturing CUDA graphs (decode, FULL):  82%|████████▏ | 42/51 [00:06<00:01,  8.41it/s]
Capturing CUDA graphs (decode, FULL):  84%|████████▍ | 43/51 [00:07<00:00,  8.15it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▋ | 44/51 [00:07<00:01,  6.96it/s]
Capturing CUDA graphs (decode, FULL):  88%|████████▊ | 45/51 [00:07<00:01,  5.25it/s]
Capturing CUDA graphs (decode, FULL):  90%|█████████ | 46/51 [00:07<00:01,  4.51it/s]
Capturing CUDA graphs (decode, FULL):  92%|█████████▏| 47/51 [00:07<00:00,  5.06it/s]
Capturing CUDA graphs (decode, FULL):  94%|█████████▍| 48/51 [00:08<00:00,  3.53it/s]
Capturing CUDA graphs (decode, FULL):  96%|█████████▌| 49/51 [00:08<00:00,  4.30it/s]
Capturing CUDA graphs (decode, FULL):  98%|█████████▊| 50/51 [00:08<00:00,  5.09it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:08<00:00,  5.47it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:08<00:00,  5.76it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:  56%|█████▋    | 289/512 [00:00<00:00, 2883.02it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 2883.63it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:08<1:13:49,  8.67s/it, est. speed input: 1.85 toks/s, output: 29.53 toks/s]
Processed prompts:  12%|█▏        | 63/512 [00:08<00:44, 10.14it/s, est. speed input: 114.75 toks/s, output: 1836.05 toks/s]
Processed prompts:  28%|██▊       | 141/512 [00:08<00:13, 27.52it/s, est. speed input: 253.93 toks/s, output: 4062.89 toks/s]
Processed prompts:  40%|████      | 205/512 [00:08<00:06, 46.66it/s, est. speed input: 365.07 toks/s, output: 5841.07 toks/s]
Processed prompts:  51%|█████▏    | 263/512 [00:09<00:03, 69.16it/s, est. speed input: 462.70 toks/s, output: 7403.24 toks/s]
Processed prompts:  62%|██████▏   | 319/512 [00:09<00:02, 95.94it/s, est. speed input: 553.65 toks/s, output: 8858.37 toks/s]
Processed prompts:  72%|███████▏  | 371/512 [00:09<00:01, 124.71it/s, est. speed input: 634.86 toks/s, output: 10157.75 toks/s]
Processed prompts:  82%|████████▏ | 419/512 [00:09<00:00, 150.11it/s, est. speed input: 705.12 toks/s, output: 11281.89 toks/s]
Processed prompts:  90%|█████████ | 462/512 [00:09<00:00, 174.96it/s, est. speed input: 766.62 toks/s, output: 12265.85 toks/s]
Processed prompts:  98%|█████████▊| 502/512 [00:10<00:00, 125.71it/s, est. speed input: 786.53 toks/s, output: 12584.53 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:10<00:00, 125.71it/s, est. speed input: 802.17 toks/s, output: 12834.75 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:10<00:00, 50.13it/s, est. speed input: 802.17 toks/s, output: 12834.75 toks/s] 
[rank0]:[W126 09:53:56.349160167 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=64 ==========
Time: 2026-01-26 10:35:29
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_6
Params: prompt_len=16, output_len=256, num_prompts=64, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 256 --num-prompts 64 --max-num-seqs 64 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/json/Qwen2.5-7B-FP8_M64.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 10:35:36 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 10:35:37 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=532459) WARNING 01-26 10:35:46 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=532459) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=532459) WARNING 01-26 10:36:05 [backends.py:609] Failed to read file <frozen os>
Throughput: 14.04 requests/s, 3819.66 total tokens/s, 3594.97 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384

STDERR:
[2026-01-26 10:35:35] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 10:35:36] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 10:35:36] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 10:35:36] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 10:35:36] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 10:35:36] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 10:35:36] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 10:35:36] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 10:35:36] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 10:35:36] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 10:35:36] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 10:35:36] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 10:35:36] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 10:35:36] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 10:35:44] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 10:35:44] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 10:35:44] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 10:35:44] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 10:35:44] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 10:35:44] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 10:35:44] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 10:35:44] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 10:35:44] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 10:35:44] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 10:35:44] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 10:35:44] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 10:35:44] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 10:35:44] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=532459) [2026-01-26 10:35:46] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=532459) [2026-01-26 10:35:46] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=532459) [2026-01-26 10:35:46] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=532459) [2026-01-26 10:35:46] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=532459) [2026-01-26 10:35:46] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=532459) [2026-01-26 10:35:46] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=532459) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=532459) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:03<00:03,  3.73s/it]
(EngineCore_DP0 pid=532459) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:08<00:00,  4.37s/it]
(EngineCore_DP0 pid=532459) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:08<00:00,  4.27s/it]
(EngineCore_DP0 pid=532459) 
(EngineCore_DP0 pid=532459) [2026-01-26 10:35:55] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 4800] -> 1D uint8
(EngineCore_DP0 pid=532459) [2026-01-26 10:35:56] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16662528 bytes
(EngineCore_DP0 pid=532459) [2026-01-26 10:35:56] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 4800] -> 1D uint8
(EngineCore_DP0 pid=532459) [2026-01-26 10:35:56] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 12959744 bytes
(EngineCore_DP0 pid=532459) [2026-01-26 10:35:56] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 4800] -> 1D uint8
(EngineCore_DP0 pid=532459) [2026-01-26 10:35:56] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 137003008 bytes
(EngineCore_DP0 pid=532459) [2026-01-26 10:35:56] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 25280] -> 1D uint8
(EngineCore_DP0 pid=532459) [2026-01-26 10:35:56] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 68009984 bytes
(EngineCore_DP0 pid=532459) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   5%|▌         | 1/19 [00:00<00:05,  3.59it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 2/19 [00:00<00:04,  3.68it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 3/19 [00:00<00:03,  4.45it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  21%|██        | 4/19 [00:01<00:03,  3.92it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▋       | 5/19 [00:01<00:05,  2.80it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  32%|███▏      | 6/19 [00:01<00:03,  3.34it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 7/19 [00:02<00:04,  2.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 8/19 [00:02<00:03,  3.18it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 9/19 [00:02<00:02,  3.94it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 10/19 [00:02<00:01,  4.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 11/19 [00:02<00:01,  5.36it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 12/19 [00:02<00:01,  6.00it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  68%|██████▊   | 13/19 [00:03<00:00,  6.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▎  | 14/19 [00:03<00:00,  7.06it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  79%|███████▉  | 15/19 [00:03<00:00,  7.35it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 16/19 [00:03<00:00,  7.73it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 17/19 [00:03<00:00,  7.94it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  95%|█████████▍| 18/19 [00:03<00:00,  8.19it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:03<00:00,  7.19it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:03<00:00,  4.92it/s]
(EngineCore_DP0 pid=532459) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   9%|▉         | 1/11 [00:00<00:02,  3.87it/s]
Capturing CUDA graphs (decode, FULL):  18%|█▊        | 2/11 [00:00<00:02,  3.02it/s]
Capturing CUDA graphs (decode, FULL):  27%|██▋       | 3/11 [00:00<00:02,  3.53it/s]
Capturing CUDA graphs (decode, FULL):  36%|███▋      | 4/11 [00:01<00:01,  3.78it/s]
Capturing CUDA graphs (decode, FULL):  45%|████▌     | 5/11 [00:01<00:01,  3.73it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 6/11 [00:01<00:01,  3.54it/s]
Capturing CUDA graphs (decode, FULL):  64%|██████▎   | 7/11 [00:01<00:00,  4.42it/s]
Capturing CUDA graphs (decode, FULL):  73%|███████▎  | 8/11 [00:01<00:00,  5.23it/s]
Capturing CUDA graphs (decode, FULL):  82%|████████▏ | 9/11 [00:02<00:00,  6.01it/s]
Capturing CUDA graphs (decode, FULL):  91%|█████████ | 10/11 [00:02<00:00,  6.67it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:02<00:00,  7.24it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:02<00:00,  4.89it/s]

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 64/64 [00:00<00:00, 2537.34it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 1/64 [00:04<04:40,  4.46s/it, est. speed input: 3.59 toks/s, output: 57.40 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:04<00:00,  4.46s/it, est. speed input: 226.01 toks/s, output: 3616.23 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:04<00:00, 14.13it/s, est. speed input: 226.01 toks/s, output: 3616.23 toks/s]
[rank0]:[W126 10:36:33.028218742 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-26 10:36:35
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_6
Params: prompt_len=16, output_len=256, num_prompts=128, max_num_seqs=128
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 256 --num-prompts 128 --max-num-seqs 128 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/json/Qwen2.5-7B-FP8_M128.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 10:36:42 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 10:36:43 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=533623) WARNING 01-26 10:36:52 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=533623) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=533623) WARNING 01-26 10:37:12 [backends.py:609] Failed to read file <frozen os>
Throughput: 24.60 requests/s, 6690.94 total tokens/s, 6297.36 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768

STDERR:
[2026-01-26 10:36:42] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 10:36:42] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 10:36:42] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 10:36:42] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 10:36:42] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 10:36:42] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 10:36:42] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 10:36:42] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 10:36:42] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 10:36:42] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 10:36:42] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 10:36:42] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 10:36:42] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 10:36:42] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 10:36:51] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 10:36:51] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 10:36:51] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 10:36:51] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 10:36:51] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 10:36:51] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 10:36:51] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 10:36:51] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 10:36:51] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 10:36:51] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 10:36:51] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 10:36:51] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 10:36:51] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 10:36:51] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=533623) [2026-01-26 10:36:53] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=533623) [2026-01-26 10:36:53] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=533623) [2026-01-26 10:36:53] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=533623) [2026-01-26 10:36:53] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=533623) [2026-01-26 10:36:53] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=533623) [2026-01-26 10:36:53] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=533623) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=533623) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:03<00:03,  3.84s/it]
(EngineCore_DP0 pid=533623) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:08<00:00,  4.51s/it]
(EngineCore_DP0 pid=533623) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:08<00:00,  4.41s/it]
(EngineCore_DP0 pid=533623) 
(EngineCore_DP0 pid=533623) [2026-01-26 10:37:02] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 4800] -> 1D uint8
(EngineCore_DP0 pid=533623) [2026-01-26 10:37:03] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16662528 bytes
(EngineCore_DP0 pid=533623) [2026-01-26 10:37:03] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 4800] -> 1D uint8
(EngineCore_DP0 pid=533623) [2026-01-26 10:37:03] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 12959744 bytes
(EngineCore_DP0 pid=533623) [2026-01-26 10:37:03] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 4800] -> 1D uint8
(EngineCore_DP0 pid=533623) [2026-01-26 10:37:03] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 137003008 bytes
(EngineCore_DP0 pid=533623) [2026-01-26 10:37:03] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 25280] -> 1D uint8
(EngineCore_DP0 pid=533623) [2026-01-26 10:37:03] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 68009984 bytes
(EngineCore_DP0 pid=533623) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|▎         | 1/35 [00:00<00:09,  3.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 2/35 [00:00<00:11,  2.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▊         | 3/35 [00:00<00:08,  3.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█▏        | 4/35 [00:00<00:06,  5.00it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 5/35 [00:01<00:05,  5.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 6/35 [00:01<00:06,  4.77it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 7/35 [00:01<00:09,  3.01it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  23%|██▎       | 8/35 [00:02<00:08,  3.10it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▌       | 9/35 [00:02<00:10,  2.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 10/35 [00:02<00:07,  3.27it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 11/35 [00:03<00:06,  3.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  34%|███▍      | 12/35 [00:03<00:04,  4.73it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 13/35 [00:03<00:04,  5.41it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 14/35 [00:03<00:03,  6.04it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 15/35 [00:03<00:03,  6.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  46%|████▌     | 16/35 [00:03<00:02,  6.84it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▊     | 17/35 [00:03<00:02,  7.01it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████▏    | 18/35 [00:03<00:02,  7.16it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  54%|█████▍    | 19/35 [00:04<00:02,  7.42it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 20/35 [00:04<00:01,  7.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 21/35 [00:04<00:01,  7.73it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 22/35 [00:04<00:01,  7.88it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  66%|██████▌   | 23/35 [00:04<00:01,  6.05it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 24/35 [00:05<00:02,  3.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 25/35 [00:05<00:02,  4.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▍  | 26/35 [00:05<00:02,  3.27it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  77%|███████▋  | 27/35 [00:06<00:02,  3.26it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 28/35 [00:06<00:01,  3.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  83%|████████▎ | 29/35 [00:06<00:01,  4.66it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 30/35 [00:06<00:00,  5.39it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▊ | 31/35 [00:06<00:00,  5.92it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████▏| 32/35 [00:06<00:00,  6.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 33/35 [00:06<00:00,  7.00it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  97%|█████████▋| 34/35 [00:06<00:00,  7.41it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:07<00:00,  7.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:07<00:00,  4.94it/s]
(EngineCore_DP0 pid=533623) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   5%|▌         | 1/19 [00:00<00:02,  7.34it/s]
Capturing CUDA graphs (decode, FULL):  11%|█         | 2/19 [00:00<00:02,  8.07it/s]
Capturing CUDA graphs (decode, FULL):  16%|█▌        | 3/19 [00:00<00:01,  8.21it/s]
Capturing CUDA graphs (decode, FULL):  21%|██        | 4/19 [00:00<00:01,  7.96it/s]
Capturing CUDA graphs (decode, FULL):  26%|██▋       | 5/19 [00:00<00:02,  6.04it/s]
Capturing CUDA graphs (decode, FULL):  32%|███▏      | 6/19 [00:01<00:03,  3.71it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 7/19 [00:01<00:02,  4.56it/s]
Capturing CUDA graphs (decode, FULL):  42%|████▏     | 8/19 [00:01<00:02,  4.76it/s]
Capturing CUDA graphs (decode, FULL):  47%|████▋     | 9/19 [00:01<00:02,  4.10it/s]
Capturing CUDA graphs (decode, FULL):  53%|█████▎    | 10/19 [00:02<00:02,  4.05it/s]
Capturing CUDA graphs (decode, FULL):  58%|█████▊    | 11/19 [00:02<00:01,  4.86it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 12/19 [00:02<00:01,  5.65it/s]
Capturing CUDA graphs (decode, FULL):  68%|██████▊   | 13/19 [00:02<00:00,  6.33it/s]
Capturing CUDA graphs (decode, FULL):  74%|███████▎  | 14/19 [00:02<00:00,  6.86it/s]
Capturing CUDA graphs (decode, FULL):  79%|███████▉  | 15/19 [00:02<00:00,  7.33it/s]
Capturing CUDA graphs (decode, FULL):  84%|████████▍ | 16/19 [00:02<00:00,  7.79it/s]
Capturing CUDA graphs (decode, FULL):  89%|████████▉ | 17/19 [00:02<00:00,  8.16it/s]
Capturing CUDA graphs (decode, FULL):  95%|█████████▍| 18/19 [00:02<00:00,  8.42it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:03<00:00,  8.54it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:03<00:00,  6.13it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 2669.80it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:03<07:26,  3.52s/it, est. speed input: 4.55 toks/s, output: 72.80 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:03<00:01, 29.15it/s, est. speed input: 331.74 toks/s, output: 5307.89 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 29.15it/s, est. speed input: 555.63 toks/s, output: 8890.06 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 34.72it/s, est. speed input: 555.63 toks/s, output: 8890.06 toks/s]
[rank0]:[W126 10:37:41.150045309 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-26 10:37:45
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_6
Params: prompt_len=16, output_len=256, num_prompts=256, max_num_seqs=256
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 256 --num-prompts 256 --max-num-seqs 256 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/json/Qwen2.5-7B-FP8_M256.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 10:37:53 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 10:37:54 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=534839) WARNING 01-26 10:38:06 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=534839) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=534839) WARNING 01-26 10:38:19 [backends.py:609] Failed to read file <frozen os>
Throughput: 31.52 requests/s, 8574.78 total tokens/s, 8070.38 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536

STDERR:
[2026-01-26 10:37:53] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 10:37:53] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 10:37:53] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 10:37:53] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 10:37:53] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 10:37:53] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 10:37:53] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 10:37:53] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 10:37:53] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 10:37:53] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 10:37:53] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 10:37:53] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 10:37:53] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 10:37:53] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 10:38:01] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 10:38:01] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 10:38:01] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 10:38:01] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 10:38:01] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 10:38:01] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 10:38:01] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 10:38:01] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 10:38:01] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 10:38:01] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 10:38:01] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 10:38:01] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 10:38:01] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 10:38:01] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=534839) [2026-01-26 10:38:07] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=534839) [2026-01-26 10:38:07] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=534839) [2026-01-26 10:38:07] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=534839) [2026-01-26 10:38:07] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=534839) [2026-01-26 10:38:07] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=534839) [2026-01-26 10:38:07] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=534839) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=534839) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.14it/s]
(EngineCore_DP0 pid=534839) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.15s/it]
(EngineCore_DP0 pid=534839) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.11s/it]
(EngineCore_DP0 pid=534839) 
(EngineCore_DP0 pid=534839) [2026-01-26 10:38:10] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 4800] -> 1D uint8
(EngineCore_DP0 pid=534839) [2026-01-26 10:38:10] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16662528 bytes
(EngineCore_DP0 pid=534839) [2026-01-26 10:38:10] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 4800] -> 1D uint8
(EngineCore_DP0 pid=534839) [2026-01-26 10:38:10] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 12959744 bytes
(EngineCore_DP0 pid=534839) [2026-01-26 10:38:10] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 4800] -> 1D uint8
(EngineCore_DP0 pid=534839) [2026-01-26 10:38:11] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 137003008 bytes
(EngineCore_DP0 pid=534839) [2026-01-26 10:38:11] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 25280] -> 1D uint8
(EngineCore_DP0 pid=534839) [2026-01-26 10:38:11] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 68009984 bytes
(EngineCore_DP0 pid=534839) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/36 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|▎         | 1/36 [00:00<00:04,  8.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 2/36 [00:00<00:04,  8.37it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 3/36 [00:00<00:03,  8.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 4/36 [00:00<00:03,  8.33it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 5/36 [00:00<00:03,  8.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 6/36 [00:00<00:03,  8.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  19%|█▉        | 7/36 [00:00<00:03,  8.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 8/36 [00:00<00:03,  8.44it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 9/36 [00:01<00:03,  7.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  28%|██▊       | 10/36 [00:01<00:04,  5.99it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███       | 11/36 [00:01<00:06,  3.71it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 12/36 [00:02<00:06,  3.59it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▌      | 13/36 [00:02<00:08,  2.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|███▉      | 14/36 [00:02<00:06,  3.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 15/36 [00:02<00:04,  4.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  44%|████▍     | 16/36 [00:03<00:03,  5.04it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 17/36 [00:03<00:03,  5.77it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 18/36 [00:03<00:02,  6.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 19/36 [00:03<00:02,  6.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  56%|█████▌    | 20/36 [00:03<00:02,  7.18it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 21/36 [00:03<00:01,  7.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|██████    | 22/36 [00:03<00:01,  7.84it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▍   | 23/36 [00:03<00:01,  8.09it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 24/36 [00:03<00:01,  8.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▉   | 25/36 [00:04<00:01,  8.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  72%|███████▏  | 26/36 [00:04<00:01,  7.96it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 27/36 [00:04<00:01,  6.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 28/36 [00:05<00:02,  3.68it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  81%|████████  | 29/36 [00:05<00:01,  3.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  83%|████████▎ | 30/36 [00:05<00:01,  3.02it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 31/36 [00:05<00:01,  3.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 32/36 [00:06<00:00,  4.32it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 33/36 [00:06<00:00,  5.09it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 34/36 [00:06<00:00,  5.85it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  97%|█████████▋| 35/36 [00:06<00:00,  6.55it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:06<00:00,  6.65it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:06<00:00,  5.53it/s]
(EngineCore_DP0 pid=534839) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   3%|▎         | 1/35 [00:00<00:05,  6.49it/s]
Capturing CUDA graphs (decode, FULL):   6%|▌         | 2/35 [00:00<00:04,  7.42it/s]
Capturing CUDA graphs (decode, FULL):   9%|▊         | 3/35 [00:00<00:03,  8.05it/s]
Capturing CUDA graphs (decode, FULL):  11%|█▏        | 4/35 [00:00<00:03,  8.31it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 5/35 [00:00<00:03,  8.44it/s]
Capturing CUDA graphs (decode, FULL):  17%|█▋        | 6/35 [00:00<00:03,  8.50it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 7/35 [00:00<00:03,  8.65it/s]
Capturing CUDA graphs (decode, FULL):  23%|██▎       | 8/35 [00:01<00:03,  7.59it/s]
Capturing CUDA graphs (decode, FULL):  26%|██▌       | 9/35 [00:01<00:06,  3.94it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 10/35 [00:01<00:05,  4.72it/s]
Capturing CUDA graphs (decode, FULL):  31%|███▏      | 11/35 [00:01<00:05,  4.46it/s]
Capturing CUDA graphs (decode, FULL):  34%|███▍      | 12/35 [00:02<00:06,  3.52it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 13/35 [00:02<00:05,  3.87it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 14/35 [00:02<00:04,  4.68it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 15/35 [00:02<00:03,  5.43it/s]
Capturing CUDA graphs (decode, FULL):  46%|████▌     | 16/35 [00:02<00:03,  6.12it/s]
Capturing CUDA graphs (decode, FULL):  49%|████▊     | 17/35 [00:02<00:02,  6.75it/s]
Capturing CUDA graphs (decode, FULL):  51%|█████▏    | 18/35 [00:03<00:02,  7.27it/s]
Capturing CUDA graphs (decode, FULL):  54%|█████▍    | 19/35 [00:03<00:02,  7.65it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 20/35 [00:03<00:01,  7.96it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 21/35 [00:03<00:01,  8.21it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 22/35 [00:03<00:01,  8.35it/s]
Capturing CUDA graphs (decode, FULL):  66%|██████▌   | 23/35 [00:03<00:01,  8.57it/s]
Capturing CUDA graphs (decode, FULL):  69%|██████▊   | 24/35 [00:03<00:01,  8.66it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 25/35 [00:03<00:01,  8.76it/s]
Capturing CUDA graphs (decode, FULL):  74%|███████▍  | 26/35 [00:03<00:01,  8.85it/s]
Capturing CUDA graphs (decode, FULL):  77%|███████▋  | 27/35 [00:04<00:01,  7.33it/s]
Capturing CUDA graphs (decode, FULL):  97%|█████████▋| 34/35 [00:04<00:00, 19.04it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:04<00:00,  7.84it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 2718.11it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:07<30:30,  7.18s/it, est. speed input: 2.23 toks/s, output: 35.67 toks/s]
Processed prompts:  19%|█▉        | 48/256 [00:07<00:22,  9.30it/s, est. speed input: 105.46 toks/s, output: 1687.34 toks/s]
Processed prompts:  42%|████▏     | 108/256 [00:07<00:05, 25.22it/s, est. speed input: 233.66 toks/s, output: 3738.56 toks/s]
Processed prompts:  62%|██████▏   | 159/256 [00:07<00:02, 43.05it/s, est. speed input: 338.92 toks/s, output: 5422.70 toks/s]
Processed prompts:  79%|███████▊  | 201/256 [00:07<00:00, 61.60it/s, est. speed input: 422.16 toks/s, output: 6754.49 toks/s]
Processed prompts:  95%|█████████▍| 242/256 [00:07<00:00, 78.19it/s, est. speed input: 493.58 toks/s, output: 7897.27 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:08<00:00, 78.19it/s, est. speed input: 510.43 toks/s, output: 8166.83 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:08<00:00, 31.90it/s, est. speed input: 510.43 toks/s, output: 8166.83 toks/s]
[rank0]:[W126 10:38:53.533001962 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-26 10:38:56
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_6
Params: prompt_len=16, output_len=256, num_prompts=512, max_num_seqs=512
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 256 --num-prompts 512 --max-num-seqs 512 --max-model-len 272 --max-num-batched-tokens 512 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/json/Qwen2.5-7B-FP8_M512.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 10:39:04 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 10:39:05 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=536028) WARNING 01-26 10:39:13 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=536028) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=536028) WARNING 01-26 10:39:26 [backends.py:609] Failed to read file <frozen os>
Throughput: 32.52 requests/s, 8844.28 total tokens/s, 8324.02 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072

STDERR:
[2026-01-26 10:39:04] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 10:39:04] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 10:39:04] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 10:39:04] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 10:39:04] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 10:39:04] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 10:39:04] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 10:39:04] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 10:39:04] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 10:39:04] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 10:39:04] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 10:39:04] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 10:39:04] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 10:39:04] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 10:39:13] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 10:39:13] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 10:39:13] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 10:39:13] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 10:39:13] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 10:39:13] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 10:39:13] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 10:39:13] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 10:39:13] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 10:39:13] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 10:39:13] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 10:39:13] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 10:39:13] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 10:39:13] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=536028) [2026-01-26 10:39:13] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=536028) [2026-01-26 10:39:13] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=536028) [2026-01-26 10:39:13] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=536028) [2026-01-26 10:39:13] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=536028) [2026-01-26 10:39:13] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=536028) [2026-01-26 10:39:13] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=536028) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=536028) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.11it/s]
(EngineCore_DP0 pid=536028) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.14s/it]
(EngineCore_DP0 pid=536028) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.10s/it]
(EngineCore_DP0 pid=536028) 
(EngineCore_DP0 pid=536028) [2026-01-26 10:39:16] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 4800] -> 1D uint8
(EngineCore_DP0 pid=536028) [2026-01-26 10:39:16] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16662528 bytes
(EngineCore_DP0 pid=536028) [2026-01-26 10:39:16] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 4800] -> 1D uint8
(EngineCore_DP0 pid=536028) [2026-01-26 10:39:16] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 12959744 bytes
(EngineCore_DP0 pid=536028) [2026-01-26 10:39:16] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 4800] -> 1D uint8
(EngineCore_DP0 pid=536028) [2026-01-26 10:39:16] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 137003008 bytes
(EngineCore_DP0 pid=536028) [2026-01-26 10:39:16] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 25280] -> 1D uint8
(EngineCore_DP0 pid=536028) [2026-01-26 10:39:16] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 68009984 bytes
(EngineCore_DP0 pid=536028) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|▏         | 1/51 [00:00<00:06,  7.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|▍         | 2/51 [00:00<00:06,  7.91it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 3/51 [00:00<00:05,  8.22it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 4/51 [00:00<00:05,  8.42it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|▉         | 5/51 [00:00<00:05,  8.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|█▏        | 6/51 [00:00<00:05,  8.55it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▎        | 7/51 [00:00<00:05,  8.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 8/51 [00:00<00:05,  8.48it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 17/51 [00:01<00:02, 11.67it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  35%|███▌      | 18/51 [00:01<00:03, 10.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 19/51 [00:01<00:03, 10.54it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|███▉      | 20/51 [00:02<00:03, 10.19it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|████      | 21/51 [00:02<00:03,  9.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 22/51 [00:02<00:03,  9.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 23/51 [00:02<00:02,  9.37it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 24/51 [00:02<00:02,  9.27it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▉     | 25/51 [00:02<00:02,  9.09it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████     | 26/51 [00:02<00:02,  8.96it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 27/51 [00:02<00:02,  8.84it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 28/51 [00:02<00:02,  8.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 29/51 [00:03<00:02,  8.68it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  59%|█████▉    | 30/51 [00:03<00:02,  8.08it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|██████    | 31/51 [00:03<00:06,  3.16it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 32/51 [00:04<00:05,  3.24it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  65%|██████▍   | 33/51 [00:04<00:06,  2.71it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 34/51 [00:04<00:05,  3.36it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 35/51 [00:05<00:03,  4.11it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████   | 36/51 [00:05<00:03,  4.87it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 37/51 [00:05<00:02,  5.59it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▍  | 38/51 [00:05<00:02,  6.24it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|███████▋  | 39/51 [00:05<00:01,  6.80it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 40/51 [00:05<00:01,  7.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 41/51 [00:05<00:01,  7.58it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 42/51 [00:05<00:01,  7.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 43/51 [00:05<00:00,  8.05it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▋ | 44/51 [00:06<00:00,  8.24it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  88%|████████▊ | 45/51 [00:06<00:00,  8.32it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|█████████ | 46/51 [00:06<00:00,  7.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 47/51 [00:06<00:00,  5.88it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 48/51 [00:07<00:00,  3.65it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  96%|█████████▌| 49/51 [00:07<00:00,  3.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  98%|█████████▊| 50/51 [00:07<00:00,  2.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:08<00:00,  3.34it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:08<00:00,  6.31it/s]
(EngineCore_DP0 pid=536028) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   2%|▏         | 1/51 [00:00<00:14,  3.50it/s]
Capturing CUDA graphs (decode, FULL):   4%|▍         | 2/51 [00:00<00:10,  4.50it/s]
Capturing CUDA graphs (decode, FULL):   6%|▌         | 3/51 [00:00<00:08,  5.34it/s]
Capturing CUDA graphs (decode, FULL):   8%|▊         | 4/51 [00:00<00:07,  6.30it/s]
Capturing CUDA graphs (decode, FULL):  10%|▉         | 5/51 [00:00<00:06,  6.97it/s]
Capturing CUDA graphs (decode, FULL):  12%|█▏        | 6/51 [00:00<00:06,  7.28it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▎        | 7/51 [00:01<00:05,  7.73it/s]
Capturing CUDA graphs (decode, FULL):  16%|█▌        | 8/51 [00:01<00:05,  8.02it/s]
Capturing CUDA graphs (decode, FULL):  18%|█▊        | 9/51 [00:01<00:05,  8.11it/s]
Capturing CUDA graphs (decode, FULL):  20%|█▉        | 10/51 [00:01<00:06,  6.15it/s]
Capturing CUDA graphs (decode, FULL):  22%|██▏       | 11/51 [00:02<00:10,  3.65it/s]
Capturing CUDA graphs (decode, FULL):  24%|██▎       | 12/51 [00:02<00:08,  4.40it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 13/51 [00:02<00:08,  4.31it/s]
Capturing CUDA graphs (decode, FULL):  27%|██▋       | 14/51 [00:02<00:10,  3.47it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▉       | 15/51 [00:03<00:09,  3.99it/s]
Capturing CUDA graphs (decode, FULL):  31%|███▏      | 16/51 [00:03<00:07,  4.77it/s]
Capturing CUDA graphs (decode, FULL):  33%|███▎      | 17/51 [00:03<00:06,  5.46it/s]
Capturing CUDA graphs (decode, FULL):  35%|███▌      | 18/51 [00:03<00:05,  6.16it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 19/51 [00:03<00:04,  6.78it/s]
Capturing CUDA graphs (decode, FULL):  39%|███▉      | 20/51 [00:03<00:04,  7.19it/s]
Capturing CUDA graphs (decode, FULL):  41%|████      | 21/51 [00:03<00:03,  7.60it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 22/51 [00:03<00:03,  7.94it/s]
Capturing CUDA graphs (decode, FULL):  45%|████▌     | 23/51 [00:03<00:03,  8.22it/s]
Capturing CUDA graphs (decode, FULL):  47%|████▋     | 24/51 [00:04<00:03,  8.44it/s]
Capturing CUDA graphs (decode, FULL):  49%|████▉     | 25/51 [00:04<00:03,  8.60it/s]
Capturing CUDA graphs (decode, FULL):  51%|█████     | 26/51 [00:04<00:02,  8.56it/s]
Capturing CUDA graphs (decode, FULL):  53%|█████▎    | 27/51 [00:04<00:02,  8.59it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 28/51 [00:04<00:02,  8.67it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 29/51 [00:04<00:03,  7.18it/s]
Capturing CUDA graphs (decode, FULL):  59%|█████▉    | 30/51 [00:05<00:05,  3.90it/s]
Capturing CUDA graphs (decode, FULL):  61%|██████    | 31/51 [00:05<00:04,  4.62it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 32/51 [00:05<00:03,  4.76it/s]
Capturing CUDA graphs (decode, FULL):  65%|██████▍   | 33/51 [00:05<00:04,  4.11it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 34/51 [00:06<00:04,  4.00it/s]
Capturing CUDA graphs (decode, FULL):  69%|██████▊   | 35/51 [00:06<00:03,  4.78it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████   | 36/51 [00:06<00:02,  5.54it/s]
Capturing CUDA graphs (decode, FULL):  73%|███████▎  | 37/51 [00:06<00:02,  6.27it/s]
Capturing CUDA graphs (decode, FULL):  75%|███████▍  | 38/51 [00:06<00:01,  6.92it/s]
Capturing CUDA graphs (decode, FULL):  76%|███████▋  | 39/51 [00:06<00:01,  7.44it/s]
Capturing CUDA graphs (decode, FULL):  78%|███████▊  | 40/51 [00:06<00:01,  7.84it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 41/51 [00:06<00:01,  8.19it/s]
Capturing CUDA graphs (decode, FULL):  82%|████████▏ | 42/51 [00:07<00:01,  8.34it/s]
Capturing CUDA graphs (decode, FULL):  84%|████████▍ | 43/51 [00:07<00:00,  8.31it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▋ | 44/51 [00:07<00:00,  8.46it/s]
Capturing CUDA graphs (decode, FULL):  88%|████████▊ | 45/51 [00:07<00:00,  8.52it/s]
Capturing CUDA graphs (decode, FULL):  90%|█████████ | 46/51 [00:07<00:00,  8.64it/s]
Capturing CUDA graphs (decode, FULL):  92%|█████████▏| 47/51 [00:07<00:00,  8.15it/s]
Capturing CUDA graphs (decode, FULL):  94%|█████████▍| 48/51 [00:07<00:00,  7.60it/s]
Capturing CUDA graphs (decode, FULL):  96%|█████████▌| 49/51 [00:08<00:00,  4.25it/s]
Capturing CUDA graphs (decode, FULL):  98%|█████████▊| 50/51 [00:08<00:00,  4.38it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:08<00:00,  4.53it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:08<00:00,  5.87it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:  53%|█████▎    | 273/512 [00:00<00:00, 2718.74it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 2795.97it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:11<1:38:23, 11.55s/it, est. speed input: 1.39 toks/s, output: 22.16 toks/s]
Processed prompts:   6%|▋         | 33/512 [00:11<02:00,  3.97it/s, est. speed input: 45.08 toks/s, output: 721.33 toks/s]
Processed prompts:  18%|█▊        | 91/512 [00:11<00:30, 13.67it/s, est. speed input: 122.81 toks/s, output: 1965.02 toks/s]
Processed prompts:  28%|██▊       | 141/512 [00:11<00:14, 25.12it/s, est. speed input: 188.65 toks/s, output: 3018.46 toks/s]
Processed prompts:  40%|████      | 205/512 [00:12<00:06, 45.06it/s, est. speed input: 271.87 toks/s, output: 4349.98 toks/s]
Processed prompts:  53%|█████▎    | 273/512 [00:12<00:03, 72.59it/s, est. speed input: 358.50 toks/s, output: 5735.95 toks/s]
Processed prompts:  66%|██████▌   | 338/512 [00:12<00:01, 105.56it/s, est. speed input: 439.57 toks/s, output: 7033.18 toks/s]
Processed prompts:  76%|███████▋  | 391/512 [00:12<00:00, 138.14it/s, est. speed input: 504.35 toks/s, output: 8069.54 toks/s]
Processed prompts:  87%|████████▋ | 444/512 [00:12<00:00, 165.52it/s, est. speed input: 564.97 toks/s, output: 9039.59 toks/s]
Processed prompts:  96%|█████████▌| 490/512 [00:12<00:00, 157.63it/s, est. speed input: 607.58 toks/s, output: 9721.30 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:14<00:00, 157.63it/s, est. speed input: 582.33 toks/s, output: 9317.23 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:14<00:00, 36.39it/s, est. speed input: 582.33 toks/s, output: 9317.23 toks/s] 
[rank0]:[W126 10:40:17.139145314 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=64 ==========
Time: 2026-01-26 11:41:11
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_6
Params: prompt_len=16, output_len=256, num_prompts=64, max_num_seqs=64
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 256 --num-prompts 64 --max-num-seqs 64 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/json/Qwen2.5-14B-FP8_M64.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 11:41:18 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 11:41:19 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=596302) WARNING 01-26 11:41:28 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=596302) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=596302) WARNING 01-26 11:42:00 [backends.py:609] Failed to read file <frozen os>
Throughput: 6.17 requests/s, 1678.35 total tokens/s, 1579.62 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384

STDERR:
[2026-01-26 11:41:18] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:41:18] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 11:41:18] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 11:41:18] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 11:41:18] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 11:41:18] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 11:41:18] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 11:41:18] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 11:41:18] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 11:41:18] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:41:18] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:41:18] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:41:18] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:41:18] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 11:41:26] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:41:26] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 11:41:26] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 11:41:26] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 11:41:26] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 11:41:26] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 11:41:26] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 11:41:26] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 11:41:26] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 11:41:26] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:41:26] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:41:26] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:41:26] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:41:26] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=596302) [2026-01-26 11:41:29] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=596302) [2026-01-26 11:41:29] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=596302) [2026-01-26 11:41:29] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=596302) [2026-01-26 11:41:29] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=596302) [2026-01-26 11:41:29] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=596302) [2026-01-26 11:41:29] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=596302) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=596302) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:05<00:15,  5.08s/it]
(EngineCore_DP0 pid=596302) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:10<00:10,  5.00s/it]
(EngineCore_DP0 pid=596302) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:11<00:03,  3.30s/it]
(EngineCore_DP0 pid=596302) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:16<00:00,  3.92s/it]
(EngineCore_DP0 pid=596302) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:16<00:00,  4.04s/it]
(EngineCore_DP0 pid=596302) 
(EngineCore_DP0 pid=596302) [2026-01-26 11:41:46] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 6848] -> 1D uint8
(EngineCore_DP0 pid=596302) [2026-01-26 11:41:46] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 36929536 bytes
(EngineCore_DP0 pid=596302) [2026-01-26 11:41:46] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 6848] -> 1D uint8
(EngineCore_DP0 pid=596302) [2026-01-26 11:41:46] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 26378240 bytes
(EngineCore_DP0 pid=596302) [2026-01-26 11:41:46] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 6848] -> 1D uint8
(EngineCore_DP0 pid=596302) [2026-01-26 11:41:46] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 142442496 bytes
(EngineCore_DP0 pid=596302) [2026-01-26 11:41:46] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 18432] -> 1D uint8
(EngineCore_DP0 pid=596302) [2026-01-26 11:41:46] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 70778880 bytes
(EngineCore_DP0 pid=596302) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   5%|▌         | 1/19 [00:01<00:20,  1.15s/it]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 2/19 [00:02<00:17,  1.00s/it]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 3/19 [00:02<00:10,  1.58it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  21%|██        | 4/19 [00:02<00:06,  2.18it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▋       | 5/19 [00:02<00:05,  2.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  32%|███▏      | 6/19 [00:02<00:03,  3.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 7/19 [00:03<00:03,  3.71it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 8/19 [00:03<00:02,  4.11it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 9/19 [00:03<00:02,  4.13it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 10/19 [00:04<00:03,  2.45it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 11/19 [00:04<00:03,  2.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 12/19 [00:05<00:03,  2.12it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  68%|██████▊   | 13/19 [00:05<00:02,  2.58it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▎  | 14/19 [00:05<00:01,  3.05it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  79%|███████▉  | 15/19 [00:05<00:01,  3.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 16/19 [00:06<00:00,  3.88it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 17/19 [00:06<00:00,  4.24it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  95%|█████████▍| 18/19 [00:06<00:00,  4.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:06<00:00,  3.82it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:06<00:00,  2.82it/s]
(EngineCore_DP0 pid=596302) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   9%|▉         | 1/11 [00:00<00:06,  1.65it/s]
Capturing CUDA graphs (decode, FULL):  18%|█▊        | 2/11 [00:00<00:03,  2.45it/s]
Capturing CUDA graphs (decode, FULL):  27%|██▋       | 3/11 [00:01<00:03,  2.42it/s]
Capturing CUDA graphs (decode, FULL):  36%|███▋      | 4/11 [00:01<00:02,  2.36it/s]
Capturing CUDA graphs (decode, FULL):  45%|████▌     | 5/11 [00:01<00:02,  2.96it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 6/11 [00:02<00:01,  3.48it/s]
Capturing CUDA graphs (decode, FULL):  64%|██████▎   | 7/11 [00:02<00:01,  3.93it/s]
Capturing CUDA graphs (decode, FULL):  73%|███████▎  | 8/11 [00:02<00:00,  4.31it/s]
Capturing CUDA graphs (decode, FULL):  82%|████████▏ | 9/11 [00:02<00:00,  4.59it/s]
Capturing CUDA graphs (decode, FULL):  91%|█████████ | 10/11 [00:02<00:00,  4.79it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:03<00:00,  4.41it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:03<00:00,  3.52it/s]

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 64/64 [00:00<00:00, 2522.37it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 1/64 [00:06<07:10,  6.84s/it, est. speed input: 2.34 toks/s, output: 37.42 toks/s]
Processed prompts:   3%|▎         | 2/64 [00:07<03:03,  2.96s/it, est. speed input: 4.52 toks/s, output: 72.25 toks/s]
Processed prompts:  52%|█████▏    | 33/64 [00:07<00:03,  8.81it/s, est. speed input: 73.15 toks/s, output: 1170.38 toks/s]
Processed prompts:  75%|███████▌  | 48/64 [00:07<00:01, 14.09it/s, est. speed input: 104.87 toks/s, output: 1677.96 toks/s]
Processed prompts:  95%|█████████▌| 61/64 [00:09<00:00,  8.72it/s, est. speed input: 97.99 toks/s, output: 1567.79 toks/s] 
Processed prompts: 100%|██████████| 64/64 [00:10<00:00,  8.72it/s, est. speed input: 98.98 toks/s, output: 1583.74 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:10<00:00,  6.19it/s, est. speed input: 98.98 toks/s, output: 1583.74 toks/s]
[rank0]:[W126 11:42:42.396850120 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=128 ==========
Time: 2026-01-26 11:42:46
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_6
Params: prompt_len=16, output_len=256, num_prompts=128, max_num_seqs=128
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 256 --num-prompts 128 --max-num-seqs 128 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/json/Qwen2.5-14B-FP8_M128.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 11:42:53 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 11:42:54 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=597843) WARNING 01-26 11:43:02 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=597843) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=597843) WARNING 01-26 11:43:24 [backends.py:609] Failed to read file <frozen os>
Throughput: 9.33 requests/s, 2538.85 total tokens/s, 2389.51 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768

STDERR:
[2026-01-26 11:42:52] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:42:53] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 11:42:53] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 11:42:53] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 11:42:53] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 11:42:53] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 11:42:53] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 11:42:53] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 11:42:53] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 11:42:53] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:42:53] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:42:53] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:42:53] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:42:53] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 11:43:01] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:43:01] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 11:43:01] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 11:43:01] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 11:43:01] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 11:43:01] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 11:43:01] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 11:43:01] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 11:43:01] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 11:43:01] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:43:01] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:43:01] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:43:01] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:43:01] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=597843) [2026-01-26 11:43:02] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=597843) [2026-01-26 11:43:02] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=597843) [2026-01-26 11:43:02] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=597843) [2026-01-26 11:43:02] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=597843) [2026-01-26 11:43:02] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=597843) [2026-01-26 11:43:02] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=597843) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=597843) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.27s/it]
(EngineCore_DP0 pid=597843) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:02<00:02,  1.46s/it]
(EngineCore_DP0 pid=597843) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:01,  1.11s/it]
(EngineCore_DP0 pid=597843) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.20s/it]
(EngineCore_DP0 pid=597843) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.22s/it]
(EngineCore_DP0 pid=597843) 
(EngineCore_DP0 pid=597843) [2026-01-26 11:43:10] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 6848] -> 1D uint8
(EngineCore_DP0 pid=597843) [2026-01-26 11:43:10] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 36929536 bytes
(EngineCore_DP0 pid=597843) [2026-01-26 11:43:10] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 6848] -> 1D uint8
(EngineCore_DP0 pid=597843) [2026-01-26 11:43:10] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 26378240 bytes
(EngineCore_DP0 pid=597843) [2026-01-26 11:43:10] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 6848] -> 1D uint8
(EngineCore_DP0 pid=597843) [2026-01-26 11:43:11] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 142442496 bytes
(EngineCore_DP0 pid=597843) [2026-01-26 11:43:11] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 18432] -> 1D uint8
(EngineCore_DP0 pid=597843) [2026-01-26 11:43:11] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 70778880 bytes
(EngineCore_DP0 pid=597843) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|▎         | 1/35 [00:00<00:22,  1.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 2/35 [00:00<00:12,  2.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▊         | 3/35 [00:01<00:09,  3.27it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█▏        | 4/35 [00:01<00:08,  3.79it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 5/35 [00:01<00:07,  4.16it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 6/35 [00:01<00:09,  3.13it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 7/35 [00:02<00:11,  2.42it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  23%|██▎       | 8/35 [00:03<00:14,  1.90it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▌       | 9/35 [00:03<00:11,  2.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 10/35 [00:03<00:09,  2.68it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 11/35 [00:03<00:07,  3.16it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  34%|███▍      | 12/35 [00:04<00:06,  3.58it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 13/35 [00:04<00:05,  3.94it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 14/35 [00:04<00:04,  4.25it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 15/35 [00:04<00:04,  4.24it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  46%|████▌     | 16/35 [00:05<00:07,  2.48it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▊     | 17/35 [00:06<00:08,  2.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████▏    | 18/35 [00:06<00:07,  2.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  54%|█████▍    | 19/35 [00:06<00:06,  2.65it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 20/35 [00:07<00:04,  3.11it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 21/35 [00:07<00:04,  3.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 22/35 [00:07<00:03,  3.88it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  66%|██████▌   | 23/35 [00:07<00:02,  4.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 24/35 [00:07<00:02,  4.44it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 28/35 [00:08<00:01,  5.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  83%|████████▎ | 29/35 [00:08<00:01,  5.66it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 30/35 [00:08<00:00,  5.54it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▊ | 31/35 [00:08<00:00,  5.45it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████▏| 32/35 [00:09<00:00,  5.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 33/35 [00:09<00:00,  5.35it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  97%|█████████▋| 34/35 [00:09<00:00,  5.22it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:09<00:00,  3.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:09<00:00,  3.52it/s]
(EngineCore_DP0 pid=597843) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   5%|▌         | 1/19 [00:00<00:12,  1.40it/s]
Capturing CUDA graphs (decode, FULL):  11%|█         | 2/19 [00:01<00:11,  1.52it/s]
Capturing CUDA graphs (decode, FULL):  16%|█▌        | 3/19 [00:01<00:07,  2.26it/s]
Capturing CUDA graphs (decode, FULL):  21%|██        | 4/19 [00:01<00:05,  2.93it/s]
Capturing CUDA graphs (decode, FULL):  26%|██▋       | 5/19 [00:01<00:04,  3.48it/s]
Capturing CUDA graphs (decode, FULL):  32%|███▏      | 6/19 [00:02<00:03,  3.93it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 7/19 [00:02<00:02,  4.30it/s]
Capturing CUDA graphs (decode, FULL):  42%|████▏     | 8/19 [00:02<00:02,  4.59it/s]
Capturing CUDA graphs (decode, FULL):  47%|████▋     | 9/19 [00:02<00:02,  4.78it/s]
Capturing CUDA graphs (decode, FULL):  53%|█████▎    | 10/19 [00:02<00:01,  4.95it/s]
Capturing CUDA graphs (decode, FULL):  58%|█████▊    | 11/19 [00:03<00:02,  2.70it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 12/19 [00:03<00:02,  2.90it/s]
Capturing CUDA graphs (decode, FULL):  68%|██████▊   | 13/19 [00:04<00:02,  2.72it/s]
Capturing CUDA graphs (decode, FULL):  74%|███████▎  | 14/19 [00:04<00:01,  2.68it/s]
Capturing CUDA graphs (decode, FULL):  79%|███████▉  | 15/19 [00:04<00:01,  3.16it/s]
Capturing CUDA graphs (decode, FULL):  84%|████████▍ | 16/19 [00:05<00:00,  3.60it/s]
Capturing CUDA graphs (decode, FULL):  89%|████████▉ | 17/19 [00:05<00:00,  4.02it/s]
Capturing CUDA graphs (decode, FULL):  95%|█████████▍| 18/19 [00:05<00:00,  4.39it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:05<00:00,  4.70it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:05<00:00,  3.40it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 2606.28it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:07<16:33,  7.82s/it, est. speed input: 2.05 toks/s, output: 32.72 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:08<00:09,  8.41it/s, est. speed input: 95.74 toks/s, output: 1531.91 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:08<00:05, 11.63it/s, est. speed input: 122.09 toks/s, output: 1953.46 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:10<00:05,  8.98it/s, est. speed input: 115.55 toks/s, output: 1848.78 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:11<00:04,  9.72it/s, est. speed input: 121.75 toks/s, output: 1947.93 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:11<00:03, 10.01it/s, est. speed input: 124.62 toks/s, output: 1993.99 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:11<00:02, 11.19it/s, est. speed input: 129.43 toks/s, output: 2070.88 toks/s]
Processed prompts:  78%|███████▊  | 100/128 [00:12<00:02, 11.04it/s, est. speed input: 130.56 toks/s, output: 2088.95 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:12<00:02, 11.92it/s, est. speed input: 133.00 toks/s, output: 2128.06 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:12<00:01, 12.09it/s, est. speed input: 134.75 toks/s, output: 2155.98 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:12<00:01, 13.01it/s, est. speed input: 136.85 toks/s, output: 2189.63 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:12<00:01, 14.66it/s, est. speed input: 139.41 toks/s, output: 2230.48 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:13<00:00, 17.30it/s, est. speed input: 142.92 toks/s, output: 2286.69 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:13<00:00, 15.17it/s, est. speed input: 143.56 toks/s, output: 2296.90 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:13<00:00, 17.23it/s, est. speed input: 146.01 toks/s, output: 2336.14 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:13<00:00, 19.34it/s, est. speed input: 148.44 toks/s, output: 2375.06 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:13<00:00, 19.34it/s, est. speed input: 149.90 toks/s, output: 2398.42 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:13<00:00,  9.37it/s, est. speed input: 149.90 toks/s, output: 2398.42 toks/s]
[rank0]:[W126 11:44:15.347859223 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=256 ==========
Time: 2026-01-26 11:44:18
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_6
Params: prompt_len=16, output_len=256, num_prompts=256, max_num_seqs=256
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 256 --num-prompts 256 --max-num-seqs 256 --max-model-len 272 --max-num-batched-tokens 272 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/json/Qwen2.5-14B-FP8_M256.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 11:44:25 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 11:44:26 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=599338) WARNING 01-26 11:44:35 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=599338) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=599338) WARNING 01-26 11:44:56 [backends.py:609] Failed to read file <frozen os>
Throughput: 9.23 requests/s, 2511.17 total tokens/s, 2363.45 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536

STDERR:
[2026-01-26 11:44:25] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:44:25] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 11:44:25] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 11:44:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 11:44:25] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 11:44:25] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 11:44:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 11:44:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 11:44:25] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 11:44:25] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:44:25] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:44:25] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:44:25] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:44:25] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 11:44:33] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:44:33] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 11:44:33] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 11:44:33] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 11:44:33] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 11:44:33] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 11:44:33] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 11:44:33] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 11:44:33] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 11:44:33] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:44:33] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:44:33] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:44:33] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:44:33] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=599338) [2026-01-26 11:44:35] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=599338) [2026-01-26 11:44:35] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=599338) [2026-01-26 11:44:35] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=599338) [2026-01-26 11:44:35] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=599338) [2026-01-26 11:44:35] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=599338) [2026-01-26 11:44:35] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=599338) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=599338) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.33s/it]
(EngineCore_DP0 pid=599338) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:02<00:02,  1.36s/it]
(EngineCore_DP0 pid=599338) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:00,  1.06it/s]
(EngineCore_DP0 pid=599338) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.09s/it]
(EngineCore_DP0 pid=599338) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.12s/it]
(EngineCore_DP0 pid=599338) 
(EngineCore_DP0 pid=599338) [2026-01-26 11:44:42] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 6848] -> 1D uint8
(EngineCore_DP0 pid=599338) [2026-01-26 11:44:43] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 36929536 bytes
(EngineCore_DP0 pid=599338) [2026-01-26 11:44:43] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 6848] -> 1D uint8
(EngineCore_DP0 pid=599338) [2026-01-26 11:44:43] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 26378240 bytes
(EngineCore_DP0 pid=599338) [2026-01-26 11:44:43] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 6848] -> 1D uint8
(EngineCore_DP0 pid=599338) [2026-01-26 11:44:43] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 142442496 bytes
(EngineCore_DP0 pid=599338) [2026-01-26 11:44:43] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 18432] -> 1D uint8
(EngineCore_DP0 pid=599338) [2026-01-26 11:44:43] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 70778880 bytes
(EngineCore_DP0 pid=599338) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/36 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|▎         | 1/36 [00:00<00:22,  1.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 2/36 [00:00<00:12,  2.63it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 3/36 [00:01<00:09,  3.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 4/36 [00:01<00:08,  3.79it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 5/36 [00:01<00:07,  4.19it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 6/36 [00:01<00:07,  4.07it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  19%|█▉        | 7/36 [00:02<00:12,  2.34it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 8/36 [00:03<00:15,  1.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 9/36 [00:03<00:13,  2.05it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  28%|██▊       | 10/36 [00:03<00:10,  2.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███       | 11/36 [00:04<00:08,  2.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 12/36 [00:04<00:07,  3.37it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▌      | 13/36 [00:04<00:06,  3.74it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|███▉      | 14/36 [00:04<00:05,  4.03it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 15/36 [00:04<00:05,  4.01it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  44%|████▍     | 16/36 [00:05<00:08,  2.41it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 17/36 [00:06<00:09,  1.92it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 18/36 [00:06<00:07,  2.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 19/36 [00:06<00:06,  2.74it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  56%|█████▌    | 20/36 [00:07<00:05,  3.18it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 21/36 [00:07<00:04,  3.59it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|██████    | 22/36 [00:07<00:03,  3.92it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▍   | 23/36 [00:07<00:03,  4.22it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 24/36 [00:07<00:02,  4.16it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 27/36 [00:08<00:01,  5.79it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 28/36 [00:08<00:01,  5.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  81%|████████  | 29/36 [00:08<00:01,  5.44it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  83%|████████▎ | 30/36 [00:08<00:01,  5.32it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 31/36 [00:09<00:00,  5.29it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 32/36 [00:09<00:00,  5.22it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 33/36 [00:09<00:00,  5.17it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 34/36 [00:09<00:00,  3.92it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  97%|█████████▋| 35/36 [00:10<00:00,  2.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:11<00:00,  1.93it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:11<00:00,  3.14it/s]
(EngineCore_DP0 pid=599338) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   3%|▎         | 1/35 [00:00<00:13,  2.56it/s]
Capturing CUDA graphs (decode, FULL):   6%|▌         | 2/35 [00:00<00:08,  3.68it/s]
Capturing CUDA graphs (decode, FULL):   9%|▊         | 3/35 [00:00<00:07,  4.28it/s]
Capturing CUDA graphs (decode, FULL):  11%|█▏        | 4/35 [00:00<00:06,  4.66it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 5/35 [00:01<00:06,  4.85it/s]
Capturing CUDA graphs (decode, FULL):  17%|█▋        | 6/35 [00:01<00:06,  4.78it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 7/35 [00:01<00:06,  4.08it/s]
Capturing CUDA graphs (decode, FULL):  23%|██▎       | 8/35 [00:02<00:08,  3.06it/s]
Capturing CUDA graphs (decode, FULL):  26%|██▌       | 9/35 [00:02<00:08,  3.19it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 10/35 [00:03<00:10,  2.42it/s]
Capturing CUDA graphs (decode, FULL):  31%|███▏      | 11/35 [00:03<00:08,  2.80it/s]
Capturing CUDA graphs (decode, FULL):  34%|███▍      | 12/35 [00:03<00:07,  3.27it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 13/35 [00:03<00:05,  3.70it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 14/35 [00:03<00:05,  4.08it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 15/35 [00:04<00:04,  4.38it/s]
Capturing CUDA graphs (decode, FULL):  46%|████▌     | 16/35 [00:04<00:04,  4.63it/s]
Capturing CUDA graphs (decode, FULL):  49%|████▊     | 17/35 [00:04<00:03,  4.57it/s]
Capturing CUDA graphs (decode, FULL):  51%|█████▏    | 18/35 [00:04<00:04,  4.08it/s]
Capturing CUDA graphs (decode, FULL):  54%|█████▍    | 19/35 [00:05<00:05,  3.16it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 20/35 [00:05<00:05,  2.97it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 21/35 [00:06<00:05,  2.54it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 22/35 [00:06<00:04,  2.84it/s]
Capturing CUDA graphs (decode, FULL):  66%|██████▌   | 23/35 [00:06<00:03,  3.28it/s]
Capturing CUDA graphs (decode, FULL):  69%|██████▊   | 24/35 [00:06<00:02,  3.70it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 25/35 [00:07<00:02,  4.04it/s]
Capturing CUDA graphs (decode, FULL):  74%|███████▍  | 26/35 [00:07<00:02,  4.35it/s]
Capturing CUDA graphs (decode, FULL):  77%|███████▋  | 27/35 [00:07<00:01,  4.61it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 28/35 [00:07<00:01,  4.68it/s]
Capturing CUDA graphs (decode, FULL):  83%|████████▎ | 29/35 [00:07<00:01,  4.05it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 30/35 [00:08<00:01,  2.96it/s]
Capturing CUDA graphs (decode, FULL):  89%|████████▊ | 31/35 [00:08<00:01,  3.39it/s]
Capturing CUDA graphs (decode, FULL):  91%|█████████▏| 32/35 [00:08<00:00,  3.72it/s]
Capturing CUDA graphs (decode, FULL):  94%|█████████▍| 33/35 [00:09<00:00,  3.03it/s]
Capturing CUDA graphs (decode, FULL):  97%|█████████▋| 34/35 [00:09<00:00,  3.48it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:09<00:00,  3.89it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:09<00:00,  3.60it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 2739.44it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:08<35:28,  8.35s/it, est. speed input: 1.92 toks/s, output: 30.68 toks/s]
Processed prompts:  13%|█▎        | 33/256 [00:08<00:40,  5.49it/s, est. speed input: 62.44 toks/s, output: 999.01 toks/s]
Processed prompts:  20%|█▉        | 50/256 [00:08<00:24,  8.53it/s, est. speed input: 89.23 toks/s, output: 1427.66 toks/s]
Processed prompts:  24%|██▍       | 61/256 [00:11<00:27,  7.13it/s, est. speed input: 87.55 toks/s, output: 1400.78 toks/s]
Processed prompts:  27%|██▋       | 68/256 [00:11<00:25,  7.48it/s, est. speed input: 91.35 toks/s, output: 1461.64 toks/s]
Processed prompts:  29%|██▊       | 73/256 [00:12<00:24,  7.59it/s, est. speed input: 93.30 toks/s, output: 1492.80 toks/s]
Processed prompts:  30%|███       | 77/256 [00:13<00:23,  7.56it/s, est. speed input: 94.33 toks/s, output: 1509.20 toks/s]
Processed prompts:  31%|███▏      | 80/256 [00:13<00:21,  8.21it/s, est. speed input: 96.63 toks/s, output: 1546.00 toks/s]
Processed prompts:  32%|███▏      | 83/256 [00:13<00:21,  7.92it/s, est. speed input: 97.02 toks/s, output: 1552.29 toks/s]
Processed prompts:  33%|███▎      | 85/256 [00:13<00:20,  8.52it/s, est. speed input: 98.42 toks/s, output: 1574.79 toks/s]
Processed prompts:  34%|███▍      | 87/256 [00:13<00:18,  9.14it/s, est. speed input: 99.72 toks/s, output: 1595.55 toks/s]
Processed prompts:  35%|███▍      | 89/256 [00:14<00:16, 10.08it/s, est. speed input: 101.18 toks/s, output: 1618.93 toks/s]
Processed prompts:  36%|███▌      | 91/256 [00:14<00:14, 11.07it/s, est. speed input: 102.60 toks/s, output: 1641.67 toks/s]
Processed prompts:  36%|███▋      | 93/256 [00:14<00:18,  8.89it/s, est. speed input: 102.22 toks/s, output: 1635.44 toks/s]
Processed prompts:  37%|███▋      | 95/256 [00:14<00:15, 10.28it/s, est. speed input: 103.67 toks/s, output: 1658.66 toks/s]
Processed prompts:  38%|███▊      | 97/256 [00:14<00:13, 11.48it/s, est. speed input: 105.00 toks/s, output: 1680.02 toks/s]
Processed prompts:  39%|███▊      | 99/256 [00:14<00:13, 11.77it/s, est. speed input: 106.03 toks/s, output: 1696.46 toks/s]
Processed prompts:  39%|███▉      | 101/256 [00:15<00:12, 12.62it/s, est. speed input: 107.24 toks/s, output: 1715.89 toks/s]
Processed prompts:  40%|████      | 103/256 [00:15<00:10, 13.93it/s, est. speed input: 108.60 toks/s, output: 1737.57 toks/s]
Processed prompts:  41%|████      | 105/256 [00:15<00:13, 11.03it/s, est. speed input: 108.75 toks/s, output: 1739.98 toks/s]
Processed prompts:  42%|████▏     | 108/256 [00:15<00:12, 12.04it/s, est. speed input: 110.34 toks/s, output: 1765.37 toks/s]
Processed prompts:  44%|████▍     | 112/256 [00:15<00:09, 15.37it/s, est. speed input: 113.25 toks/s, output: 1812.02 toks/s]
Processed prompts:  45%|████▍     | 114/256 [00:15<00:08, 15.94it/s, est. speed input: 114.48 toks/s, output: 1831.63 toks/s]
Processed prompts:  54%|█████▍    | 138/256 [00:16<00:02, 44.04it/s, est. speed input: 135.99 toks/s, output: 2175.84 toks/s]
Processed prompts:  55%|█████▌    | 142/256 [00:17<00:07, 15.52it/s, est. speed input: 130.79 toks/s, output: 2092.63 toks/s]
Processed prompts:  57%|█████▋    | 145/256 [00:17<00:09, 12.18it/s, est. speed input: 129.41 toks/s, output: 2070.55 toks/s]
Processed prompts:  57%|█████▋    | 147/256 [00:18<00:09, 11.18it/s, est. speed input: 129.15 toks/s, output: 2066.34 toks/s]
Processed prompts:  58%|█████▊    | 149/256 [00:18<00:09, 10.89it/s, est. speed input: 129.41 toks/s, output: 2070.54 toks/s]
Processed prompts:  59%|█████▉    | 151/256 [00:18<00:09, 11.22it/s, est. speed input: 130.06 toks/s, output: 2080.98 toks/s]
Processed prompts:  60%|█████▉    | 153/256 [00:18<00:09, 10.63it/s, est. speed input: 130.19 toks/s, output: 2083.06 toks/s]
Processed prompts:  61%|██████    | 156/256 [00:18<00:08, 11.97it/s, est. speed input: 131.51 toks/s, output: 2104.10 toks/s]
Processed prompts:  62%|██████▏   | 158/256 [00:19<00:08, 11.26it/s, est. speed input: 131.70 toks/s, output: 2107.22 toks/s]
Processed prompts:  62%|██████▎   | 160/256 [00:19<00:08, 10.90it/s, est. speed input: 131.98 toks/s, output: 2111.69 toks/s]
Processed prompts:  63%|██████▎   | 162/256 [00:19<00:08, 10.95it/s, est. speed input: 132.40 toks/s, output: 2118.40 toks/s]
Processed prompts:  64%|██████▍   | 164/256 [00:19<00:07, 11.87it/s, est. speed input: 133.15 toks/s, output: 2130.45 toks/s]
Processed prompts:  65%|██████▍   | 166/256 [00:19<00:07, 12.69it/s, est. speed input: 133.90 toks/s, output: 2142.38 toks/s]
Processed prompts:  66%|██████▌   | 168/256 [00:19<00:06, 14.05it/s, est. speed input: 134.81 toks/s, output: 2156.90 toks/s]
Processed prompts:  66%|██████▋   | 170/256 [00:20<00:06, 14.28it/s, est. speed input: 135.50 toks/s, output: 2167.96 toks/s]
Processed prompts:  68%|██████▊   | 173/256 [00:20<00:06, 13.78it/s, est. speed input: 136.33 toks/s, output: 2181.23 toks/s]
Processed prompts:  68%|██████▊   | 175/256 [00:20<00:06, 13.13it/s, est. speed input: 136.74 toks/s, output: 2187.91 toks/s]
Processed prompts:  69%|██████▉   | 177/256 [00:20<00:05, 13.66it/s, est. speed input: 137.43 toks/s, output: 2198.87 toks/s]
Processed prompts:  70%|███████   | 180/256 [00:20<00:05, 13.95it/s, est. speed input: 138.36 toks/s, output: 2213.83 toks/s]
Processed prompts:  71%|███████   | 182/256 [00:20<00:04, 15.06it/s, est. speed input: 139.21 toks/s, output: 2227.42 toks/s]
Processed prompts:  72%|███████▏  | 185/256 [00:21<00:04, 17.26it/s, est. speed input: 140.65 toks/s, output: 2250.33 toks/s]
Processed prompts:  75%|███████▍  | 191/256 [00:21<00:02, 25.43it/s, est. speed input: 144.34 toks/s, output: 2309.37 toks/s]
Processed prompts:  77%|███████▋  | 198/256 [00:21<00:01, 34.00it/s, est. speed input: 148.76 toks/s, output: 2380.09 toks/s]
Processed prompts:  80%|████████  | 206/256 [00:21<00:01, 43.05it/s, est. speed input: 153.90 toks/s, output: 2462.44 toks/s]
Processed prompts:  82%|████████▏ | 211/256 [00:22<00:03, 14.90it/s, est. speed input: 151.21 toks/s, output: 2419.39 toks/s]
Processed prompts:  84%|████████▍ | 215/256 [00:23<00:03, 10.41it/s, est. speed input: 149.07 toks/s, output: 2385.06 toks/s]
Processed prompts:  85%|████████▌ | 218/256 [00:23<00:04,  8.57it/s, est. speed input: 147.37 toks/s, output: 2357.95 toks/s]
Processed prompts:  86%|████████▌ | 220/256 [00:23<00:04,  8.15it/s, est. speed input: 146.82 toks/s, output: 2349.14 toks/s]
Processed prompts:  87%|████████▋ | 222/256 [00:24<00:04,  8.35it/s, est. speed input: 146.84 toks/s, output: 2349.51 toks/s]
Processed prompts:  88%|████████▊ | 224/256 [00:24<00:03,  8.94it/s, est. speed input: 147.16 toks/s, output: 2354.54 toks/s]
Processed prompts:  88%|████████▊ | 226/256 [00:24<00:03,  9.08it/s, est. speed input: 147.21 toks/s, output: 2355.36 toks/s]
Processed prompts:  89%|████████▉ | 228/256 [00:24<00:02, 10.30it/s, est. speed input: 147.82 toks/s, output: 2365.10 toks/s]
Processed prompts:  90%|█████████ | 231/256 [00:24<00:02, 11.86it/s, est. speed input: 148.67 toks/s, output: 2378.73 toks/s]
Processed prompts:  91%|█████████ | 233/256 [00:24<00:01, 12.49it/s, est. speed input: 149.15 toks/s, output: 2386.42 toks/s]
Processed prompts:  92%|█████████▏| 235/256 [00:25<00:01, 13.57it/s, est. speed input: 149.76 toks/s, output: 2396.22 toks/s]
Processed prompts:  93%|█████████▎| 237/256 [00:25<00:01, 13.37it/s, est. speed input: 150.11 toks/s, output: 2401.73 toks/s]
Processed prompts:  94%|█████████▍| 240/256 [00:25<00:01, 15.07it/s, est. speed input: 151.08 toks/s, output: 2417.22 toks/s]
Processed prompts:  95%|█████████▍| 243/256 [00:25<00:00, 16.44it/s, est. speed input: 152.06 toks/s, output: 2432.99 toks/s]
Processed prompts:  96%|█████████▌| 245/256 [00:25<00:00, 16.94it/s, est. speed input: 152.67 toks/s, output: 2442.76 toks/s]
Processed prompts:  96%|█████████▋| 247/256 [00:25<00:00, 15.85it/s, est. speed input: 153.03 toks/s, output: 2448.50 toks/s]
Processed prompts:  98%|█████████▊| 250/256 [00:25<00:00, 18.02it/s, est. speed input: 154.14 toks/s, output: 2466.22 toks/s]
Processed prompts:  99%|█████████▉| 253/256 [00:26<00:00, 20.70it/s, est. speed input: 155.37 toks/s, output: 2485.95 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:26<00:00, 22.84it/s, est. speed input: 156.59 toks/s, output: 2505.51 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:26<00:00, 22.84it/s, est. speed input: 156.59 toks/s, output: 2505.51 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:26<00:00,  9.79it/s, est. speed input: 156.59 toks/s, output: 2505.51 toks/s]
[rank0]:[W126 11:46:06.307949403 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


========== M=512 ==========
Time: 2026-01-26 11:46:09
Backend: cuSPARSELt (2:6)
Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_6
Params: prompt_len=16, output_len=256, num_prompts=512, max_num_seqs=512
Command: vllm bench throughput --model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_6 --dataset-name random --input-len 16 --output-len 256 --num-prompts 512 --max-num-seqs 512 --max-model-len 272 --max-num-batched-tokens 512 --no-enable-chunked-prefill --gpu-memory-utilization 0.8 --disable-log-stats --output-json /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX4090_cc89_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/json/Qwen2.5-14B-FP8_M512.json

STDOUT:
When dataset path is not set, it will default to random dataset
WARNING 01-26 11:46:17 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 11:46:19 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=601092) WARNING 01-26 11:46:25 [interface.py:465] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
(EngineCore_DP0 pid=601092) [INFO] Loading compress extension: cusparselt_compress_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=601092) WARNING 01-26 11:46:49 [backends.py:609] Failed to read file <frozen os>
Throughput: 5.66 requests/s, 1540.25 total tokens/s, 1449.65 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072

STDERR:
[2026-01-26 11:46:17] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:46:17] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 11:46:17] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 11:46:17] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 11:46:17] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 11:46:17] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 11:46:17] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 11:46:17] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 11:46:17] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 11:46:17] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:46:17] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:46:17] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:46:17] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:46:17] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 11:46:26] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:46:26] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-26 11:46:26] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-26 11:46:26] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 11:46:26] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 11:46:26] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 11:46:26] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 11:46:26] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-26 11:46:26] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-26 11:46:26] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:46:26] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:46:26] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:46:26] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:46:26] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=601092) [2026-01-26 11:46:26] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=601092) [2026-01-26 11:46:26] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX4090_cc89_py312_cu129_x86_64.so
(EngineCore_DP0 pid=601092) [2026-01-26 11:46:26] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=601092) [2026-01-26 11:46:26] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=601092) [2026-01-26 11:46:26] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=601092) [2026-01-26 11:46:26] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=601092) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=601092) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:01<00:03,  1.27s/it]
(EngineCore_DP0 pid=601092) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:02<00:02,  1.29s/it]
(EngineCore_DP0 pid=601092) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:03<00:00,  1.12it/s]
(EngineCore_DP0 pid=601092) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.04s/it]
(EngineCore_DP0 pid=601092) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:04<00:00,  1.06s/it]
(EngineCore_DP0 pid=601092) 
(EngineCore_DP0 pid=601092) [2026-01-26 11:46:32] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 6848] -> 1D uint8
(EngineCore_DP0 pid=601092) [2026-01-26 11:46:33] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 36929536 bytes
(EngineCore_DP0 pid=601092) [2026-01-26 11:46:33] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 6848] -> 1D uint8
(EngineCore_DP0 pid=601092) [2026-01-26 11:46:33] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 26378240 bytes
(EngineCore_DP0 pid=601092) [2026-01-26 11:46:33] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 6848] -> 1D uint8
(EngineCore_DP0 pid=601092) [2026-01-26 11:46:33] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 142442496 bytes
(EngineCore_DP0 pid=601092) [2026-01-26 11:46:33] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 18432] -> 1D uint8
(EngineCore_DP0 pid=601092) [2026-01-26 11:46:33] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 70778880 bytes
(EngineCore_DP0 pid=601092) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|▏         | 1/51 [00:01<01:01,  1.23s/it]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|▍         | 2/51 [00:01<00:30,  1.59it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 3/51 [00:01<00:20,  2.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 4/51 [00:01<00:15,  2.94it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|▉         | 5/51 [00:02<00:13,  3.48it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|█▏        | 6/51 [00:02<00:11,  3.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▎        | 7/51 [00:02<00:10,  4.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 8/51 [00:02<00:11,  3.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 9/51 [00:03<00:17,  2.44it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|█▉        | 10/51 [00:04<00:21,  1.94it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 11/51 [00:04<00:18,  2.22it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|██▎       | 12/51 [00:04<00:14,  2.65it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 13/51 [00:04<00:12,  3.10it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 14/51 [00:05<00:10,  3.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▉       | 15/51 [00:05<00:09,  3.85it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 16/51 [00:05<00:08,  4.11it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 17/51 [00:05<00:09,  3.68it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  35%|███▌      | 18/51 [00:06<00:13,  2.41it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 19/51 [00:07<00:16,  1.91it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|███▉      | 20/51 [00:07<00:14,  2.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|████      | 21/51 [00:07<00:11,  2.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 22/51 [00:08<00:09,  3.08it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 23/51 [00:08<00:08,  3.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 24/51 [00:08<00:07,  3.81it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▉     | 25/51 [00:08<00:06,  4.09it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████     | 26/51 [00:09<00:07,  3.27it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 27/51 [00:09<00:09,  2.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 28/51 [00:10<00:12,  1.88it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 29/51 [00:10<00:09,  2.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  59%|█████▉    | 30/51 [00:11<00:07,  2.71it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|██████    | 31/51 [00:11<00:06,  3.14it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 32/51 [00:11<00:05,  3.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  65%|██████▍   | 33/51 [00:11<00:04,  3.85it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 34/51 [00:11<00:04,  4.11it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 35/51 [00:12<00:04,  3.85it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████   | 36/51 [00:12<00:06,  2.44it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 37/51 [00:13<00:07,  1.93it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▍  | 38/51 [00:13<00:05,  2.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|███████▋  | 39/51 [00:14<00:04,  2.67it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 40/51 [00:14<00:03,  3.10it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 41/51 [00:14<00:02,  3.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 42/51 [00:14<00:02,  3.82it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 43/51 [00:14<00:01,  4.11it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▋ | 44/51 [00:15<00:01,  3.80it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  88%|████████▊ | 45/51 [00:16<00:02,  2.42it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|█████████ | 46/51 [00:16<00:02,  1.93it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 47/51 [00:17<00:01,  2.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 48/51 [00:17<00:01,  2.72it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  96%|█████████▌| 49/51 [00:17<00:00,  3.16it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  98%|█████████▊| 50/51 [00:17<00:00,  3.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:17<00:00,  3.82it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:17<00:00,  2.85it/s]
(EngineCore_DP0 pid=601092) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   2%|▏         | 1/51 [00:00<00:29,  1.67it/s]
Capturing CUDA graphs (decode, FULL):   4%|▍         | 2/51 [00:01<00:29,  1.66it/s]
Capturing CUDA graphs (decode, FULL):   6%|▌         | 3/51 [00:01<00:21,  2.20it/s]
Capturing CUDA graphs (decode, FULL):  18%|█▊        | 9/51 [00:01<00:04,  8.74it/s]
Capturing CUDA graphs (decode, FULL):  22%|██▏       | 11/51 [00:02<00:05,  7.26it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 13/51 [00:02<00:08,  4.75it/s]
Capturing CUDA graphs (decode, FULL):  27%|██▋       | 14/51 [00:03<00:08,  4.45it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▉       | 15/51 [00:03<00:10,  3.39it/s]
Capturing CUDA graphs (decode, FULL):  31%|███▏      | 16/51 [00:04<00:10,  3.40it/s]
Capturing CUDA graphs (decode, FULL):  33%|███▎      | 17/51 [00:04<00:09,  3.69it/s]
Capturing CUDA graphs (decode, FULL):  35%|███▌      | 18/51 [00:04<00:08,  3.97it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 19/51 [00:04<00:07,  4.24it/s]
Capturing CUDA graphs (decode, FULL):  39%|███▉      | 20/51 [00:04<00:06,  4.48it/s]
Capturing CUDA graphs (decode, FULL):  41%|████      | 21/51 [00:04<00:06,  4.68it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 22/51 [00:05<00:06,  4.54it/s]
Capturing CUDA graphs (decode, FULL):  45%|████▌     | 23/51 [00:05<00:06,  4.07it/s]
Capturing CUDA graphs (decode, FULL):  47%|████▋     | 24/51 [00:06<00:08,  3.09it/s]
Capturing CUDA graphs (decode, FULL):  49%|████▉     | 25/51 [00:06<00:07,  3.32it/s]
Capturing CUDA graphs (decode, FULL):  51%|█████     | 26/51 [00:06<00:09,  2.51it/s]
Capturing CUDA graphs (decode, FULL):  53%|█████▎    | 27/51 [00:07<00:08,  2.84it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 28/51 [00:07<00:07,  3.28it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 29/51 [00:07<00:05,  3.70it/s]
Capturing CUDA graphs (decode, FULL):  59%|█████▉    | 30/51 [00:07<00:05,  4.08it/s]
Capturing CUDA graphs (decode, FULL):  61%|██████    | 31/51 [00:07<00:04,  4.37it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 32/51 [00:08<00:04,  4.58it/s]
Capturing CUDA graphs (decode, FULL):  65%|██████▍   | 33/51 [00:08<00:03,  4.59it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 34/51 [00:08<00:04,  4.02it/s]
Capturing CUDA graphs (decode, FULL):  69%|██████▊   | 35/51 [00:09<00:05,  3.12it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████   | 36/51 [00:09<00:04,  3.37it/s]
Capturing CUDA graphs (decode, FULL):  73%|███████▎  | 37/51 [00:10<00:05,  2.49it/s]
Capturing CUDA graphs (decode, FULL):  75%|███████▍  | 38/51 [00:10<00:04,  2.72it/s]
Capturing CUDA graphs (decode, FULL):  76%|███████▋  | 39/51 [00:10<00:03,  3.17it/s]
Capturing CUDA graphs (decode, FULL):  78%|███████▊  | 40/51 [00:10<00:03,  3.61it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 41/51 [00:10<00:02,  4.00it/s]
Capturing CUDA graphs (decode, FULL):  82%|████████▏ | 42/51 [00:11<00:02,  4.29it/s]
Capturing CUDA graphs (decode, FULL):  84%|████████▍ | 43/51 [00:11<00:01,  4.53it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▋ | 44/51 [00:11<00:01,  4.58it/s]
Capturing CUDA graphs (decode, FULL):  88%|████████▊ | 45/51 [00:11<00:01,  4.27it/s]
Capturing CUDA graphs (decode, FULL):  90%|█████████ | 46/51 [00:12<00:01,  3.04it/s]
Capturing CUDA graphs (decode, FULL):  92%|█████████▏| 47/51 [00:12<00:01,  3.14it/s]
Capturing CUDA graphs (decode, FULL):  94%|█████████▍| 48/51 [00:13<00:01,  2.89it/s]
Capturing CUDA graphs (decode, FULL):  96%|█████████▌| 49/51 [00:13<00:00,  3.08it/s]
Capturing CUDA graphs (decode, FULL):  98%|█████████▊| 50/51 [00:13<00:00,  3.51it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:13<00:00,  3.93it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:13<00:00,  3.74it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:  53%|█████▎    | 273/512 [00:00<00:00, 2724.81it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 2760.30it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:06<52:33,  6.17s/it, est. speed input: 2.59 toks/s, output: 41.48 toks/s]
Processed prompts:   4%|▍         | 21/512 [00:06<01:58,  4.15it/s, est. speed input: 49.15 toks/s, output: 786.33 toks/s]
Processed prompts:   4%|▍         | 23/512 [00:07<01:56,  4.20it/s, est. speed input: 50.61 toks/s, output: 809.74 toks/s]
Processed prompts:   5%|▍         | 25/512 [00:07<01:52,  4.32it/s, est. speed input: 52.21 toks/s, output: 835.40 toks/s]
Processed prompts:   5%|▌         | 26/512 [00:08<02:00,  4.02it/s, est. speed input: 51.53 toks/s, output: 824.54 toks/s]
Processed prompts:   5%|▌         | 28/512 [00:08<01:56,  4.17it/s, est. speed input: 52.79 toks/s, output: 844.63 toks/s]
Processed prompts:   6%|▌         | 30/512 [00:08<01:51,  4.31it/s, est. speed input: 53.94 toks/s, output: 863.09 toks/s]
Processed prompts:   6%|▋         | 33/512 [00:09<01:38,  4.86it/s, est. speed input: 56.42 toks/s, output: 902.69 toks/s]
Processed prompts:   7%|▋         | 36/512 [00:09<01:28,  5.36it/s, est. speed input: 58.79 toks/s, output: 940.63 toks/s]
Processed prompts:   8%|▊         | 40/512 [00:10<01:13,  6.41it/s, est. speed input: 62.61 toks/s, output: 1001.69 toks/s]
Processed prompts:   9%|▊         | 44/512 [00:10<00:53,  8.80it/s, est. speed input: 67.91 toks/s, output: 1086.61 toks/s]
Processed prompts:   9%|▉         | 46/512 [00:10<00:59,  7.84it/s, est. speed input: 68.58 toks/s, output: 1097.24 toks/s]
Processed prompts:  10%|▉         | 51/512 [00:11<00:51,  8.97it/s, est. speed input: 73.00 toks/s, output: 1168.07 toks/s]
Processed prompts:  10%|█         | 53/512 [00:11<01:06,  6.93it/s, est. speed input: 72.23 toks/s, output: 1155.64 toks/s]
Processed prompts:  11%|█         | 54/512 [00:12<01:20,  5.71it/s, est. speed input: 71.17 toks/s, output: 1138.68 toks/s]
Processed prompts:  11%|█         | 55/512 [00:12<01:40,  4.53it/s, est. speed input: 69.70 toks/s, output: 1115.22 toks/s]
Processed prompts:  11%|█         | 56/512 [00:12<01:52,  4.04it/s, est. speed input: 68.93 toks/s, output: 1102.89 toks/s]
Processed prompts:  11%|█         | 57/512 [00:13<01:49,  4.17it/s, est. speed input: 69.05 toks/s, output: 1104.78 toks/s]
Processed prompts:  11%|█▏        | 58/512 [00:13<01:47,  4.23it/s, est. speed input: 69.08 toks/s, output: 1105.29 toks/s]
Processed prompts:  12%|█▏        | 59/512 [00:13<02:20,  3.23it/s, est. speed input: 67.56 toks/s, output: 1080.94 toks/s]
Processed prompts:  12%|█▏        | 61/512 [00:14<01:32,  4.89it/s, est. speed input: 69.34 toks/s, output: 1109.43 toks/s]
Processed prompts:  12%|█▏        | 62/512 [00:14<01:25,  5.25it/s, est. speed input: 69.77 toks/s, output: 1116.27 toks/s]
Processed prompts:  12%|█▏        | 63/512 [00:14<01:27,  5.15it/s, est. speed input: 69.89 toks/s, output: 1118.18 toks/s]
Processed prompts:  13%|█▎        | 65/512 [00:14<01:04,  6.90it/s, est. speed input: 71.37 toks/s, output: 1141.91 toks/s]
Processed prompts:  13%|█▎        | 68/512 [00:14<00:42, 10.50it/s, est. speed input: 74.07 toks/s, output: 1185.17 toks/s]
Processed prompts:  14%|█▎        | 70/512 [00:14<00:40, 10.95it/s, est. speed input: 75.41 toks/s, output: 1206.48 toks/s]
Processed prompts:  14%|█▍        | 72/512 [00:15<01:06,  6.57it/s, est. speed input: 74.64 toks/s, output: 1194.23 toks/s]
Processed prompts:  15%|█▌        | 79/512 [00:15<00:31, 13.72it/s, est. speed input: 81.12 toks/s, output: 1297.99 toks/s]
Processed prompts:  16%|█▌        | 82/512 [00:16<00:41, 10.37it/s, est. speed input: 81.68 toks/s, output: 1306.95 toks/s]
Processed prompts:  17%|█▋        | 85/512 [00:16<00:37, 11.52it/s, est. speed input: 83.71 toks/s, output: 1339.29 toks/s]
Processed prompts:  17%|█▋        | 87/512 [00:16<00:54,  7.84it/s, est. speed input: 82.81 toks/s, output: 1325.00 toks/s]
Processed prompts:  17%|█▋        | 89/512 [00:17<01:17,  5.43it/s, est. speed input: 81.12 toks/s, output: 1297.89 toks/s]
Processed prompts:  18%|█▊        | 90/512 [00:17<01:13,  5.70it/s, est. speed input: 81.46 toks/s, output: 1303.39 toks/s]
Processed prompts:  18%|█▊        | 91/512 [00:17<01:14,  5.62it/s, est. speed input: 81.49 toks/s, output: 1303.78 toks/s]
Processed prompts:  18%|█▊        | 93/512 [00:18<01:03,  6.59it/s, est. speed input: 82.38 toks/s, output: 1318.09 toks/s]
Processed prompts:  18%|█▊        | 94/512 [00:18<01:02,  6.64it/s, est. speed input: 82.60 toks/s, output: 1321.63 toks/s]
Processed prompts:  19%|█▉        | 96/512 [00:18<00:57,  7.24it/s, est. speed input: 83.31 toks/s, output: 1332.91 toks/s]
Processed prompts:  19%|█▉        | 97/512 [00:18<00:57,  7.17it/s, est. speed input: 83.52 toks/s, output: 1336.33 toks/s]
Processed prompts:  19%|█▉        | 99/512 [00:18<00:46,  8.85it/s, est. speed input: 84.63 toks/s, output: 1354.13 toks/s]
Processed prompts:  20%|█▉        | 101/512 [00:18<00:37, 10.86it/s, est. speed input: 85.87 toks/s, output: 1373.89 toks/s]
Processed prompts:  20%|██        | 103/512 [00:19<00:40, 10.11it/s, est. speed input: 86.53 toks/s, output: 1384.47 toks/s]
Processed prompts:  21%|██        | 105/512 [00:19<00:38, 10.50it/s, est. speed input: 87.41 toks/s, output: 1398.55 toks/s]
Processed prompts:  21%|██        | 107/512 [00:19<00:38, 10.51it/s, est. speed input: 88.20 toks/s, output: 1411.24 toks/s]
Processed prompts:  21%|██▏       | 109/512 [00:19<00:38, 10.60it/s, est. speed input: 89.00 toks/s, output: 1424.04 toks/s]
Processed prompts:  22%|██▏       | 112/512 [00:19<00:32, 12.13it/s, est. speed input: 90.56 toks/s, output: 1448.94 toks/s]
Processed prompts:  22%|██▏       | 114/512 [00:20<00:58,  6.79it/s, est. speed input: 89.31 toks/s, output: 1428.90 toks/s]
Processed prompts:  22%|██▏       | 115/512 [00:21<01:31,  4.35it/s, est. speed input: 87.31 toks/s, output: 1396.89 toks/s]
Processed prompts:  23%|██▎       | 116/512 [00:21<01:49,  3.61it/s, est. speed input: 86.11 toks/s, output: 1377.82 toks/s]
Processed prompts:  23%|██▎       | 117/512 [00:21<01:51,  3.55it/s, est. speed input: 85.66 toks/s, output: 1370.62 toks/s]
Processed prompts:  23%|██▎       | 118/512 [00:22<02:09,  3.05it/s, est. speed input: 84.55 toks/s, output: 1352.78 toks/s]
Processed prompts:  23%|██▎       | 119/512 [00:22<02:05,  3.12it/s, est. speed input: 84.14 toks/s, output: 1346.18 toks/s]
Processed prompts:  24%|██▎       | 121/512 [00:22<01:32,  4.23it/s, est. speed input: 84.66 toks/s, output: 1354.56 toks/s]
Processed prompts:  24%|██▍       | 122/512 [00:23<01:27,  4.47it/s, est. speed input: 84.69 toks/s, output: 1355.02 toks/s]
Processed prompts:  24%|██▍       | 123/512 [00:23<01:23,  4.68it/s, est. speed input: 84.71 toks/s, output: 1355.43 toks/s]
Processed prompts:  24%|██▍       | 124/512 [00:23<01:13,  5.28it/s, est. speed input: 84.96 toks/s, output: 1359.40 toks/s]
Processed prompts:  25%|██▍       | 126/512 [00:23<00:56,  6.89it/s, est. speed input: 85.70 toks/s, output: 1371.22 toks/s]
Processed prompts:  25%|██▍       | 127/512 [00:23<00:51,  7.43it/s, est. speed input: 86.02 toks/s, output: 1376.24 toks/s]
Processed prompts:  25%|██▌       | 128/512 [00:23<00:50,  7.59it/s, est. speed input: 86.24 toks/s, output: 1379.88 toks/s]
Processed prompts:  25%|██▌       | 129/512 [00:23<00:53,  7.18it/s, est. speed input: 86.33 toks/s, output: 1381.32 toks/s]
Processed prompts:  26%|██▌       | 132/512 [00:24<00:34, 11.09it/s, est. speed input: 87.83 toks/s, output: 1405.22 toks/s]
Processed prompts:  26%|██▌       | 134/512 [00:24<00:30, 12.60it/s, est. speed input: 88.73 toks/s, output: 1419.67 toks/s]
Processed prompts:  27%|██▋       | 136/512 [00:24<00:31, 12.06it/s, est. speed input: 89.39 toks/s, output: 1430.17 toks/s]
Processed prompts:  27%|██▋       | 138/512 [00:24<00:34, 10.72it/s, est. speed input: 89.85 toks/s, output: 1437.55 toks/s]
Processed prompts:  27%|██▋       | 140/512 [00:24<00:33, 11.14it/s, est. speed input: 90.54 toks/s, output: 1448.70 toks/s]
Processed prompts:  28%|██▊       | 142/512 [00:24<00:29, 12.42it/s, est. speed input: 91.40 toks/s, output: 1462.34 toks/s]
Processed prompts:  28%|██▊       | 144/512 [00:25<01:02,  5.88it/s, est. speed input: 89.98 toks/s, output: 1439.64 toks/s]
Processed prompts:  29%|██▊       | 146/512 [00:26<01:40,  3.64it/s, est. speed input: 87.71 toks/s, output: 1403.34 toks/s]
Processed prompts:  29%|██▊       | 147/512 [00:27<01:48,  3.38it/s, est. speed input: 87.02 toks/s, output: 1392.27 toks/s]
Processed prompts:  29%|██▉       | 148/512 [00:27<01:51,  3.27it/s, est. speed input: 86.51 toks/s, output: 1384.17 toks/s]
Processed prompts:  29%|██▉       | 149/512 [00:27<01:47,  3.38it/s, est. speed input: 86.27 toks/s, output: 1380.25 toks/s]
Processed prompts:  29%|██▉       | 150/512 [00:27<01:35,  3.77it/s, est. speed input: 86.32 toks/s, output: 1381.07 toks/s]
Processed prompts:  29%|██▉       | 151/512 [00:27<01:26,  4.19it/s, est. speed input: 86.39 toks/s, output: 1382.26 toks/s]
Processed prompts:  30%|██▉       | 152/512 [00:28<01:21,  4.43it/s, est. speed input: 86.37 toks/s, output: 1381.98 toks/s]
Processed prompts:  30%|██▉       | 153/512 [00:28<01:12,  4.92it/s, est. speed input: 86.50 toks/s, output: 1383.96 toks/s]
Processed prompts:  30%|███       | 154/512 [00:28<01:08,  5.23it/s, est. speed input: 86.57 toks/s, output: 1385.12 toks/s]
Processed prompts:  30%|███       | 155/512 [00:28<01:05,  5.46it/s, est. speed input: 86.64 toks/s, output: 1386.18 toks/s]
Processed prompts:  30%|███       | 156/512 [00:28<01:05,  5.46it/s, est. speed input: 86.64 toks/s, output: 1386.25 toks/s]
Processed prompts:  31%|███       | 157/512 [00:28<01:01,  5.81it/s, est. speed input: 86.76 toks/s, output: 1388.10 toks/s]
Processed prompts:  31%|███       | 158/512 [00:29<01:03,  5.62it/s, est. speed input: 86.73 toks/s, output: 1387.75 toks/s]
Processed prompts:  31%|███▏      | 160/512 [00:29<00:49,  7.08it/s, est. speed input: 87.25 toks/s, output: 1395.95 toks/s]
Processed prompts:  32%|███▏      | 162/512 [00:29<00:39,  8.89it/s, est. speed input: 87.93 toks/s, output: 1406.84 toks/s]
Processed prompts:  32%|███▏      | 164/512 [00:29<00:33, 10.33it/s, est. speed input: 88.60 toks/s, output: 1417.56 toks/s]
Processed prompts:  33%|███▎      | 167/512 [00:29<00:25, 13.62it/s, est. speed input: 89.82 toks/s, output: 1437.20 toks/s]
Processed prompts:  33%|███▎      | 169/512 [00:29<00:23, 14.67it/s, est. speed input: 90.56 toks/s, output: 1449.01 toks/s]
Processed prompts:  33%|███▎      | 171/512 [00:29<00:22, 15.00it/s, est. speed input: 91.25 toks/s, output: 1459.98 toks/s]
Processed prompts:  34%|███▍      | 173/512 [00:30<00:23, 14.39it/s, est. speed input: 91.85 toks/s, output: 1469.58 toks/s]
Processed prompts:  34%|███▍      | 175/512 [00:30<00:48,  6.88it/s, est. speed input: 90.95 toks/s, output: 1455.25 toks/s]
Processed prompts:  35%|███▍      | 177/512 [00:32<01:37,  3.43it/s, est. speed input: 88.35 toks/s, output: 1413.52 toks/s]
Processed prompts:  35%|███▍      | 178/512 [00:32<01:43,  3.22it/s, est. speed input: 87.76 toks/s, output: 1404.11 toks/s]
Processed prompts:  35%|███▍      | 179/512 [00:32<01:33,  3.55it/s, est. speed input: 87.79 toks/s, output: 1404.64 toks/s]
Processed prompts:  35%|███▌      | 180/512 [00:32<01:33,  3.57it/s, est. speed input: 87.54 toks/s, output: 1400.69 toks/s]
Processed prompts:  35%|███▌      | 181/512 [00:33<01:31,  3.63it/s, est. speed input: 87.34 toks/s, output: 1397.48 toks/s]
Processed prompts:  36%|███▌      | 182/512 [00:33<01:37,  3.40it/s, est. speed input: 86.91 toks/s, output: 1390.56 toks/s]
Processed prompts:  36%|███▌      | 183/512 [00:33<01:31,  3.59it/s, est. speed input: 86.78 toks/s, output: 1388.44 toks/s]
Processed prompts:  36%|███▌      | 184/512 [00:33<01:20,  4.07it/s, est. speed input: 86.84 toks/s, output: 1389.41 toks/s]
Processed prompts:  36%|███▌      | 185/512 [00:34<01:12,  4.51it/s, est. speed input: 86.90 toks/s, output: 1390.38 toks/s]
Processed prompts:  37%|███▋      | 187/512 [00:34<00:54,  5.96it/s, est. speed input: 87.32 toks/s, output: 1397.10 toks/s]
Processed prompts:  37%|███▋      | 188/512 [00:34<00:52,  6.14it/s, est. speed input: 87.41 toks/s, output: 1398.54 toks/s]
Processed prompts:  37%|███▋      | 189/512 [00:34<00:54,  5.96it/s, est. speed input: 87.41 toks/s, output: 1398.58 toks/s]
Processed prompts:  37%|███▋      | 190/512 [00:34<00:52,  6.18it/s, est. speed input: 87.51 toks/s, output: 1400.08 toks/s]
Processed prompts:  38%|███▊      | 192/512 [00:34<00:37,  8.58it/s, est. speed input: 88.14 toks/s, output: 1410.17 toks/s]
Processed prompts:  38%|███▊      | 193/512 [00:34<00:37,  8.43it/s, est. speed input: 88.28 toks/s, output: 1412.42 toks/s]
Processed prompts:  38%|███▊      | 195/512 [00:35<00:32,  9.86it/s, est. speed input: 88.81 toks/s, output: 1420.88 toks/s]
Processed prompts:  38%|███▊      | 197/512 [00:35<00:29, 10.65it/s, est. speed input: 89.31 toks/s, output: 1428.89 toks/s]
Processed prompts:  39%|███▉      | 199/512 [00:35<00:29, 10.48it/s, est. speed input: 89.71 toks/s, output: 1435.40 toks/s]
Processed prompts:  39%|███▉      | 201/512 [00:35<00:27, 11.49it/s, est. speed input: 90.26 toks/s, output: 1444.13 toks/s]
Processed prompts:  40%|███▉      | 203/512 [00:35<00:29, 10.46it/s, est. speed input: 90.58 toks/s, output: 1449.24 toks/s]
Processed prompts:  40%|████      | 206/512 [00:35<00:21, 14.02it/s, est. speed input: 91.64 toks/s, output: 1466.26 toks/s]
Processed prompts:  41%|████      | 208/512 [00:37<01:05,  4.65it/s, est. speed input: 89.61 toks/s, output: 1433.72 toks/s]
Processed prompts:  41%|████      | 210/512 [00:38<01:26,  3.47it/s, est. speed input: 88.23 toks/s, output: 1411.70 toks/s]
Processed prompts:  41%|████      | 211/512 [00:38<01:23,  3.63it/s, est. speed input: 88.15 toks/s, output: 1410.45 toks/s]
Processed prompts:  41%|████▏     | 212/512 [00:38<01:19,  3.79it/s, est. speed input: 88.08 toks/s, output: 1409.23 toks/s]
Processed prompts:  42%|████▏     | 213/512 [00:38<01:15,  3.95it/s, est. speed input: 88.00 toks/s, output: 1408.06 toks/s]
Processed prompts:  42%|████▏     | 214/512 [00:38<01:09,  4.28it/s, est. speed input: 88.03 toks/s, output: 1408.47 toks/s]
Processed prompts:  42%|████▏     | 215/512 [00:39<01:07,  4.37it/s, est. speed input: 87.95 toks/s, output: 1407.28 toks/s]
Processed prompts:  42%|████▏     | 216/512 [00:39<01:09,  4.24it/s, est. speed input: 87.79 toks/s, output: 1404.64 toks/s]
Processed prompts:  42%|████▏     | 217/512 [00:39<01:00,  4.88it/s, est. speed input: 87.92 toks/s, output: 1406.75 toks/s]
Processed prompts:  43%|████▎     | 219/512 [00:39<00:45,  6.48it/s, est. speed input: 88.33 toks/s, output: 1413.28 toks/s]
Processed prompts:  43%|████▎     | 221/512 [00:39<00:41,  7.06it/s, est. speed input: 88.59 toks/s, output: 1417.50 toks/s]
Processed prompts:  44%|████▎     | 223/512 [00:40<00:38,  7.57it/s, est. speed input: 88.88 toks/s, output: 1422.14 toks/s]
Processed prompts:  44%|████▍     | 226/512 [00:40<00:27, 10.40it/s, est. speed input: 89.75 toks/s, output: 1436.03 toks/s]
Processed prompts:  45%|████▍     | 228/512 [00:40<00:25, 11.35it/s, est. speed input: 90.24 toks/s, output: 1443.83 toks/s]
Processed prompts:  45%|████▍     | 230/512 [00:40<00:25, 11.11it/s, est. speed input: 90.61 toks/s, output: 1449.70 toks/s]
Processed prompts:  45%|████▌     | 232/512 [00:40<00:26, 10.69it/s, est. speed input: 90.94 toks/s, output: 1455.00 toks/s]
Processed prompts:  46%|████▌     | 234/512 [00:40<00:25, 11.03it/s, est. speed input: 91.35 toks/s, output: 1461.53 toks/s]
Processed prompts:  46%|████▋     | 237/512 [00:41<00:24, 11.45it/s, est. speed input: 91.97 toks/s, output: 1471.44 toks/s]
Processed prompts:  47%|████▋     | 239/512 [00:42<01:08,  3.98it/s, est. speed input: 89.72 toks/s, output: 1435.50 toks/s]
Processed prompts:  47%|████▋     | 240/512 [00:43<01:18,  3.45it/s, est. speed input: 89.07 toks/s, output: 1425.14 toks/s]
Processed prompts:  47%|████▋     | 241/512 [00:43<01:20,  3.37it/s, est. speed input: 88.77 toks/s, output: 1420.36 toks/s]
Processed prompts:  47%|████▋     | 242/512 [00:43<01:17,  3.47it/s, est. speed input: 88.62 toks/s, output: 1417.92 toks/s]
Processed prompts:  47%|████▋     | 243/512 [00:43<01:08,  3.93it/s, est. speed input: 88.69 toks/s, output: 1419.08 toks/s]
Processed prompts:  48%|████▊     | 244/512 [00:44<01:08,  3.90it/s, est. speed input: 88.53 toks/s, output: 1416.47 toks/s]
Processed prompts:  48%|████▊     | 245/512 [00:44<01:03,  4.18it/s, est. speed input: 88.51 toks/s, output: 1416.12 toks/s]
Processed prompts:  48%|████▊     | 246/512 [00:44<00:55,  4.82it/s, est. speed input: 88.62 toks/s, output: 1417.98 toks/s]
Processed prompts:  48%|████▊     | 247/512 [00:44<00:53,  5.00it/s, est. speed input: 88.62 toks/s, output: 1417.96 toks/s]
Processed prompts:  48%|████▊     | 248/512 [00:44<00:49,  5.29it/s, est. speed input: 88.66 toks/s, output: 1418.58 toks/s]
Processed prompts:  49%|████▊     | 249/512 [00:44<00:46,  5.68it/s, est. speed input: 88.73 toks/s, output: 1419.72 toks/s]
Processed prompts:  49%|████▉     | 250/512 [00:45<00:42,  6.22it/s, est. speed input: 88.84 toks/s, output: 1421.49 toks/s]
Processed prompts:  49%|████▉     | 252/512 [00:45<00:39,  6.64it/s, est. speed input: 89.01 toks/s, output: 1424.10 toks/s]
Processed prompts:  49%|████▉     | 253/512 [00:45<00:42,  6.11it/s, est. speed input: 88.96 toks/s, output: 1423.34 toks/s]
Processed prompts:  50%|████▉     | 255/512 [00:45<00:37,  6.78it/s, est. speed input: 89.17 toks/s, output: 1426.79 toks/s]
Processed prompts:  50%|█████     | 257/512 [00:45<00:29,  8.58it/s, est. speed input: 89.62 toks/s, output: 1433.97 toks/s]
Processed prompts:  51%|█████     | 259/512 [00:46<00:24, 10.15it/s, est. speed input: 90.07 toks/s, output: 1441.07 toks/s]
Processed prompts:  51%|█████     | 261/512 [00:46<00:22, 11.21it/s, est. speed input: 90.49 toks/s, output: 1447.78 toks/s]
Processed prompts:  51%|█████▏    | 263/512 [00:46<00:19, 12.54it/s, est. speed input: 90.94 toks/s, output: 1455.11 toks/s]
Processed prompts:  52%|█████▏    | 265/512 [00:46<00:19, 12.97it/s, est. speed input: 91.35 toks/s, output: 1461.66 toks/s]
Processed prompts:  52%|█████▏    | 268/512 [00:46<00:16, 14.82it/s, est. speed input: 92.07 toks/s, output: 1473.15 toks/s]
Processed prompts:  53%|█████▎    | 270/512 [00:47<00:49,  4.90it/s, est. speed input: 90.58 toks/s, output: 1449.26 toks/s]
Processed prompts:  54%|█████▎    | 275/512 [00:47<00:29,  8.11it/s, est. speed input: 91.85 toks/s, output: 1469.68 toks/s]
Processed prompts:  54%|█████▍    | 277/512 [00:48<00:28,  8.28it/s, est. speed input: 92.09 toks/s, output: 1473.50 toks/s]
Processed prompts:  54%|█████▍    | 279/512 [00:48<00:35,  6.49it/s, est. speed input: 91.77 toks/s, output: 1468.27 toks/s]
Processed prompts:  55%|█████▍    | 281/512 [00:49<00:37,  6.23it/s, est. speed input: 91.75 toks/s, output: 1467.99 toks/s]
Processed prompts:  55%|█████▌    | 282/512 [00:49<00:36,  6.30it/s, est. speed input: 91.80 toks/s, output: 1468.77 toks/s]
Processed prompts:  55%|█████▌    | 283/512 [00:49<00:37,  6.14it/s, est. speed input: 91.78 toks/s, output: 1468.54 toks/s]
Processed prompts:  55%|█████▌    | 284/512 [00:49<00:37,  6.09it/s, est. speed input: 91.79 toks/s, output: 1468.69 toks/s]
Processed prompts:  56%|█████▌    | 286/512 [00:49<00:29,  7.54it/s, est. speed input: 92.15 toks/s, output: 1474.33 toks/s]
Processed prompts:  56%|█████▋    | 288/512 [00:49<00:24,  9.21it/s, est. speed input: 92.56 toks/s, output: 1480.88 toks/s]
Processed prompts:  57%|█████▋    | 290/512 [00:50<00:24,  8.96it/s, est. speed input: 92.76 toks/s, output: 1484.14 toks/s]
Processed prompts:  57%|█████▋    | 292/512 [00:50<00:20, 10.58it/s, est. speed input: 93.18 toks/s, output: 1490.91 toks/s]
Processed prompts:  58%|█████▊    | 295/512 [00:50<00:16, 13.02it/s, est. speed input: 93.85 toks/s, output: 1501.64 toks/s]
Processed prompts:  58%|█████▊    | 297/512 [00:50<00:14, 14.42it/s, est. speed input: 94.30 toks/s, output: 1508.82 toks/s]
Processed prompts:  58%|█████▊    | 299/512 [00:50<00:15, 14.14it/s, est. speed input: 94.66 toks/s, output: 1514.52 toks/s]
Processed prompts:  59%|█████▉    | 301/512 [00:50<00:20, 10.15it/s, est. speed input: 94.67 toks/s, output: 1514.65 toks/s]
Processed prompts:  59%|█████▉    | 303/512 [00:51<00:45,  4.62it/s, est. speed input: 93.45 toks/s, output: 1495.28 toks/s]
Processed prompts:  59%|█████▉    | 304/512 [00:52<00:58,  3.59it/s, est. speed input: 92.73 toks/s, output: 1483.66 toks/s]
Processed prompts:  60%|█████▉    | 305/512 [00:53<01:13,  2.81it/s, est. speed input: 91.87 toks/s, output: 1469.86 toks/s]
Processed prompts:  60%|█████▉    | 306/512 [00:53<01:08,  2.99it/s, est. speed input: 91.72 toks/s, output: 1467.52 toks/s]
Processed prompts:  60%|█████▉    | 307/512 [00:53<01:05,  3.11it/s, est. speed input: 91.54 toks/s, output: 1464.59 toks/s]
Processed prompts:  60%|██████    | 308/512 [00:53<01:01,  3.32it/s, est. speed input: 91.43 toks/s, output: 1462.82 toks/s]
Processed prompts:  60%|██████    | 309/512 [00:54<01:02,  3.25it/s, est. speed input: 91.17 toks/s, output: 1458.73 toks/s]
Processed prompts:  61%|██████    | 310/512 [00:54<00:50,  3.99it/s, est. speed input: 91.30 toks/s, output: 1460.75 toks/s]
Processed prompts:  61%|██████    | 311/512 [00:54<00:45,  4.42it/s, est. speed input: 91.31 toks/s, output: 1461.01 toks/s]
Processed prompts:  61%|██████    | 313/512 [00:54<00:37,  5.33it/s, est. speed input: 91.43 toks/s, output: 1462.92 toks/s]
Processed prompts:  62%|██████▏   | 315/512 [00:54<00:27,  7.12it/s, est. speed input: 91.79 toks/s, output: 1468.70 toks/s]
Processed prompts:  62%|██████▏   | 317/512 [00:55<00:23,  8.15it/s, est. speed input: 92.07 toks/s, output: 1473.15 toks/s]
Processed prompts:  62%|██████▏   | 319/512 [00:55<00:21,  9.08it/s, est. speed input: 92.37 toks/s, output: 1477.85 toks/s]
Processed prompts:  63%|██████▎   | 321/512 [00:55<00:17, 10.94it/s, est. speed input: 92.77 toks/s, output: 1484.29 toks/s]
Processed prompts:  63%|██████▎   | 323/512 [00:55<00:17, 10.50it/s, est. speed input: 93.00 toks/s, output: 1487.97 toks/s]
Processed prompts:  63%|██████▎   | 325/512 [00:55<00:17, 10.62it/s, est. speed input: 93.27 toks/s, output: 1492.25 toks/s]
Processed prompts:  64%|██████▍   | 327/512 [00:55<00:16, 11.01it/s, est. speed input: 93.56 toks/s, output: 1496.96 toks/s]
Processed prompts:  64%|██████▍   | 329/512 [00:56<00:15, 11.95it/s, est. speed input: 93.91 toks/s, output: 1502.50 toks/s]
Processed prompts:  65%|██████▍   | 332/512 [00:56<00:13, 12.99it/s, est. speed input: 94.42 toks/s, output: 1510.80 toks/s]
Processed prompts:  65%|██████▌   | 334/512 [00:57<00:45,  3.94it/s, est. speed input: 92.61 toks/s, output: 1481.84 toks/s]
Processed prompts:  65%|██████▌   | 335/512 [00:58<00:46,  3.82it/s, est. speed input: 92.41 toks/s, output: 1478.52 toks/s]
Processed prompts:  66%|██████▌   | 336/512 [00:58<00:45,  3.89it/s, est. speed input: 92.31 toks/s, output: 1476.88 toks/s]
Processed prompts:  66%|██████▌   | 337/512 [00:58<00:48,  3.63it/s, est. speed input: 92.04 toks/s, output: 1472.59 toks/s]
Processed prompts:  66%|██████▌   | 338/512 [00:58<00:45,  3.83it/s, est. speed input: 91.98 toks/s, output: 1471.60 toks/s]
Processed prompts:  66%|██████▌   | 339/512 [00:58<00:40,  4.31it/s, est. speed input: 92.02 toks/s, output: 1472.33 toks/s]
Processed prompts:  66%|██████▋   | 340/512 [00:59<00:37,  4.55it/s, est. speed input: 92.00 toks/s, output: 1472.06 toks/s]
Processed prompts:  67%|██████▋   | 341/512 [00:59<00:33,  5.17it/s, est. speed input: 92.08 toks/s, output: 1473.32 toks/s]
Processed prompts:  67%|██████▋   | 342/512 [00:59<00:30,  5.57it/s, est. speed input: 92.13 toks/s, output: 1474.04 toks/s]
Processed prompts:  67%|██████▋   | 343/512 [00:59<00:27,  6.10it/s, est. speed input: 92.20 toks/s, output: 1475.24 toks/s]
Processed prompts:  67%|██████▋   | 344/512 [00:59<00:30,  5.47it/s, est. speed input: 92.12 toks/s, output: 1473.86 toks/s]
Processed prompts:  67%|██████▋   | 345/512 [00:59<00:29,  5.60it/s, est. speed input: 92.12 toks/s, output: 1473.99 toks/s]
Processed prompts:  68%|██████▊   | 346/512 [01:00<00:25,  6.42it/s, est. speed input: 92.24 toks/s, output: 1475.78 toks/s]
Processed prompts:  68%|██████▊   | 347/512 [01:00<00:24,  6.63it/s, est. speed input: 92.29 toks/s, output: 1476.62 toks/s]
Processed prompts:  68%|██████▊   | 348/512 [01:00<00:24,  6.79it/s, est. speed input: 92.34 toks/s, output: 1477.46 toks/s]
Processed prompts:  68%|██████▊   | 349/512 [01:00<00:24,  6.79it/s, est. speed input: 92.38 toks/s, output: 1478.09 toks/s]
Processed prompts:  69%|██████▊   | 351/512 [01:00<00:17,  9.19it/s, est. speed input: 92.71 toks/s, output: 1483.44 toks/s]
Processed prompts:  69%|██████▉   | 353/512 [01:00<00:16,  9.39it/s, est. speed input: 92.93 toks/s, output: 1486.84 toks/s]
Processed prompts:  69%|██████▉   | 355/512 [01:01<00:17,  8.97it/s, est. speed input: 93.09 toks/s, output: 1489.37 toks/s]
Processed prompts:  70%|██████▉   | 357/512 [01:01<00:15,  9.86it/s, est. speed input: 93.36 toks/s, output: 1493.75 toks/s]
Processed prompts:  70%|███████   | 359/512 [01:01<00:13, 11.62it/s, est. speed input: 93.72 toks/s, output: 1499.44 toks/s]
Processed prompts:  71%|███████   | 362/512 [01:01<00:10, 13.73it/s, est. speed input: 94.25 toks/s, output: 1507.99 toks/s]
Processed prompts:  71%|███████   | 364/512 [01:02<00:19,  7.46it/s, est. speed input: 93.88 toks/s, output: 1502.14 toks/s]
Processed prompts:  71%|███████▏  | 366/512 [01:03<00:39,  3.67it/s, est. speed input: 92.56 toks/s, output: 1481.02 toks/s]
Processed prompts:  72%|███████▏  | 367/512 [01:03<00:42,  3.45it/s, est. speed input: 92.27 toks/s, output: 1476.32 toks/s]
Processed prompts:  72%|███████▏  | 368/512 [01:03<00:40,  3.58it/s, est. speed input: 92.18 toks/s, output: 1474.85 toks/s]
Processed prompts:  72%|███████▏  | 369/512 [01:04<00:36,  3.94it/s, est. speed input: 92.20 toks/s, output: 1475.12 toks/s]
Processed prompts:  72%|███████▏  | 370/512 [01:04<00:37,  3.79it/s, est. speed input: 92.02 toks/s, output: 1472.31 toks/s]
Processed prompts:  72%|███████▏  | 371/512 [01:04<00:35,  3.98it/s, est. speed input: 91.96 toks/s, output: 1471.41 toks/s]
Processed prompts:  73%|███████▎  | 372/512 [01:04<00:37,  3.77it/s, est. speed input: 91.78 toks/s, output: 1468.46 toks/s]
Processed prompts:  73%|███████▎  | 373/512 [01:05<00:33,  4.19it/s, est. speed input: 91.79 toks/s, output: 1468.60 toks/s]
Processed prompts:  73%|███████▎  | 374/512 [01:05<00:30,  4.57it/s, est. speed input: 91.80 toks/s, output: 1468.73 toks/s]
Processed prompts:  73%|███████▎  | 375/512 [01:05<00:27,  5.04it/s, est. speed input: 91.83 toks/s, output: 1469.30 toks/s]
Processed prompts:  73%|███████▎  | 376/512 [01:05<00:23,  5.67it/s, est. speed input: 91.90 toks/s, output: 1470.47 toks/s]
Processed prompts:  74%|███████▎  | 377/512 [01:05<00:23,  5.83it/s, est. speed input: 91.92 toks/s, output: 1470.79 toks/s]
Processed prompts:  74%|███████▍  | 378/512 [01:05<00:22,  5.85it/s, est. speed input: 91.93 toks/s, output: 1470.90 toks/s]
Processed prompts:  74%|███████▍  | 379/512 [01:05<00:21,  6.10it/s, est. speed input: 91.97 toks/s, output: 1471.49 toks/s]
Processed prompts:  74%|███████▍  | 380/512 [01:06<00:19,  6.89it/s, est. speed input: 92.07 toks/s, output: 1473.11 toks/s]
Processed prompts:  75%|███████▍  | 382/512 [01:06<00:13,  9.73it/s, est. speed input: 92.41 toks/s, output: 1478.50 toks/s]
Processed prompts:  75%|███████▌  | 384/512 [01:06<00:11, 10.87it/s, est. speed input: 92.68 toks/s, output: 1482.85 toks/s]
Processed prompts:  75%|███████▌  | 386/512 [01:06<00:11, 11.33it/s, est. speed input: 92.93 toks/s, output: 1486.91 toks/s]
Processed prompts:  76%|███████▌  | 389/512 [01:06<00:09, 13.23it/s, est. speed input: 93.40 toks/s, output: 1494.47 toks/s]
Processed prompts:  76%|███████▋  | 391/512 [01:06<00:08, 14.35it/s, est. speed input: 93.73 toks/s, output: 1499.65 toks/s]
Processed prompts:  77%|███████▋  | 393/512 [01:06<00:08, 13.23it/s, est. speed input: 93.96 toks/s, output: 1503.29 toks/s]
Processed prompts:  77%|███████▋  | 396/512 [01:07<00:11,  9.80it/s, est. speed input: 94.07 toks/s, output: 1505.06 toks/s]
Processed prompts:  78%|███████▊  | 398/512 [01:09<00:34,  3.30it/s, est. speed input: 92.22 toks/s, output: 1475.52 toks/s]
Processed prompts:  78%|███████▊  | 399/512 [01:09<00:35,  3.18it/s, est. speed input: 91.96 toks/s, output: 1471.29 toks/s]
Processed prompts:  78%|███████▊  | 400/512 [01:09<00:32,  3.43it/s, est. speed input: 91.93 toks/s, output: 1470.88 toks/s]
Processed prompts:  78%|███████▊  | 401/512 [01:09<00:32,  3.41it/s, est. speed input: 91.76 toks/s, output: 1468.23 toks/s]
Processed prompts:  79%|███████▊  | 402/512 [01:10<00:31,  3.51it/s, est. speed input: 91.66 toks/s, output: 1466.50 toks/s]
Processed prompts:  79%|███████▊  | 403/512 [01:10<00:31,  3.41it/s, est. speed input: 91.47 toks/s, output: 1463.49 toks/s]
Processed prompts:  79%|███████▉  | 404/512 [01:10<00:29,  3.61it/s, est. speed input: 91.40 toks/s, output: 1462.32 toks/s]
Processed prompts:  79%|███████▉  | 406/512 [01:10<00:22,  4.78it/s, est. speed input: 91.54 toks/s, output: 1464.66 toks/s]
Processed prompts:  79%|███████▉  | 407/512 [01:11<00:21,  4.81it/s, est. speed input: 91.50 toks/s, output: 1464.05 toks/s]
Processed prompts:  80%|███████▉  | 409/512 [01:11<00:15,  6.68it/s, est. speed input: 91.79 toks/s, output: 1468.66 toks/s]
Processed prompts:  80%|████████  | 411/512 [01:11<00:12,  7.89it/s, est. speed input: 92.02 toks/s, output: 1472.27 toks/s]
Processed prompts:  81%|████████  | 413/512 [01:11<00:10,  9.37it/s, est. speed input: 92.29 toks/s, output: 1476.61 toks/s]
Processed prompts:  81%|████████  | 415/512 [01:11<00:08, 10.78it/s, est. speed input: 92.57 toks/s, output: 1481.11 toks/s]
Processed prompts:  82%|████████▏ | 418/512 [01:11<00:06, 14.30it/s, est. speed input: 93.09 toks/s, output: 1489.48 toks/s]
Processed prompts:  82%|████████▏ | 420/512 [01:12<00:06, 13.64it/s, est. speed input: 93.32 toks/s, output: 1493.20 toks/s]
Processed prompts:  82%|████████▏ | 422/512 [01:12<00:07, 12.59it/s, est. speed input: 93.52 toks/s, output: 1496.37 toks/s]
Processed prompts:  83%|████████▎ | 424/512 [01:12<00:06, 12.98it/s, est. speed input: 93.78 toks/s, output: 1500.51 toks/s]
Processed prompts:  83%|████████▎ | 426/512 [01:12<00:06, 13.14it/s, est. speed input: 94.03 toks/s, output: 1504.52 toks/s]
Processed prompts:  84%|████████▎ | 428/512 [01:14<00:26,  3.19it/s, est. speed input: 92.24 toks/s, output: 1475.76 toks/s]
Processed prompts:  84%|████████▍ | 430/512 [01:14<00:25,  3.19it/s, est. speed input: 91.89 toks/s, output: 1470.25 toks/s]
Processed prompts:  84%|████████▍ | 431/512 [01:15<00:23,  3.47it/s, est. speed input: 91.90 toks/s, output: 1470.36 toks/s]
Processed prompts:  84%|████████▍ | 432/512 [01:15<00:20,  3.87it/s, est. speed input: 91.94 toks/s, output: 1471.07 toks/s]
Processed prompts:  85%|████████▍ | 434/512 [01:15<00:17,  4.46it/s, est. speed input: 91.96 toks/s, output: 1471.43 toks/s]
Processed prompts:  85%|████████▍ | 435/512 [01:15<00:16,  4.53it/s, est. speed input: 91.92 toks/s, output: 1470.79 toks/s]
Processed prompts:  85%|████████▌ | 436/512 [01:15<00:15,  5.04it/s, est. speed input: 91.99 toks/s, output: 1471.76 toks/s]
Processed prompts:  85%|████████▌ | 437/512 [01:15<00:13,  5.72it/s, est. speed input: 92.07 toks/s, output: 1473.16 toks/s]
Processed prompts:  86%|████████▌ | 438/512 [01:16<00:12,  5.78it/s, est. speed input: 92.08 toks/s, output: 1473.27 toks/s]
Processed prompts:  86%|████████▌ | 439/512 [01:16<00:11,  6.50it/s, est. speed input: 92.17 toks/s, output: 1474.66 toks/s]
Processed prompts:  86%|████████▌ | 440/512 [01:16<00:10,  7.19it/s, est. speed input: 92.25 toks/s, output: 1476.07 toks/s]
Processed prompts:  86%|████████▋ | 442/512 [01:16<00:08,  8.31it/s, est. speed input: 92.44 toks/s, output: 1479.03 toks/s]
Processed prompts:  87%|████████▋ | 443/512 [01:16<00:08,  7.91it/s, est. speed input: 92.47 toks/s, output: 1479.55 toks/s]
Processed prompts:  87%|████████▋ | 445/512 [01:16<00:08,  8.37it/s, est. speed input: 92.63 toks/s, output: 1482.01 toks/s]
Processed prompts:  87%|████████▋ | 447/512 [01:17<00:07,  8.99it/s, est. speed input: 92.81 toks/s, output: 1484.91 toks/s]
Processed prompts:  88%|████████▊ | 449/512 [01:17<00:06,  9.33it/s, est. speed input: 92.98 toks/s, output: 1487.71 toks/s]
Processed prompts:  88%|████████▊ | 452/512 [01:17<00:04, 12.44it/s, est. speed input: 93.44 toks/s, output: 1495.10 toks/s]
Processed prompts:  89%|████████▉ | 456/512 [01:17<00:05, 10.59it/s, est. speed input: 93.73 toks/s, output: 1499.66 toks/s]
Processed prompts:  89%|████████▉ | 458/512 [01:18<00:09,  5.57it/s, est. speed input: 93.07 toks/s, output: 1489.19 toks/s]
Processed prompts:  90%|█████████ | 463/512 [01:18<00:05,  9.02it/s, est. speed input: 93.90 toks/s, output: 1502.43 toks/s]
Processed prompts:  91%|█████████ | 465/512 [01:19<00:06,  7.32it/s, est. speed input: 93.75 toks/s, output: 1500.04 toks/s]
Processed prompts:  91%|█████████ | 467/512 [01:19<00:06,  7.21it/s, est. speed input: 93.81 toks/s, output: 1500.99 toks/s]
Processed prompts:  92%|█████████▏| 469/512 [01:19<00:05,  8.08it/s, est. speed input: 94.03 toks/s, output: 1504.46 toks/s]
Processed prompts:  92%|█████████▏| 471/512 [01:20<00:05,  7.96it/s, est. speed input: 94.12 toks/s, output: 1505.93 toks/s]
Processed prompts:  92%|█████████▏| 473/512 [01:20<00:05,  7.15it/s, est. speed input: 94.10 toks/s, output: 1505.64 toks/s]
Processed prompts:  93%|█████████▎| 474/512 [01:20<00:05,  7.49it/s, est. speed input: 94.18 toks/s, output: 1506.92 toks/s]
Processed prompts:  93%|█████████▎| 475/512 [01:20<00:04,  7.84it/s, est. speed input: 94.26 toks/s, output: 1508.19 toks/s]
Processed prompts:  93%|█████████▎| 476/512 [01:20<00:04,  7.68it/s, est. speed input: 94.30 toks/s, output: 1508.75 toks/s]
Processed prompts:  93%|█████████▎| 477/512 [01:20<00:04,  7.73it/s, est. speed input: 94.35 toks/s, output: 1509.56 toks/s]
Processed prompts:  94%|█████████▎| 479/512 [01:21<00:03,  8.95it/s, est. speed input: 94.54 toks/s, output: 1512.70 toks/s]
Processed prompts:  94%|█████████▍| 481/512 [01:21<00:03, 10.05it/s, est. speed input: 94.76 toks/s, output: 1516.09 toks/s]
Processed prompts:  94%|█████████▍| 483/512 [01:21<00:02, 10.88it/s, est. speed input: 94.97 toks/s, output: 1519.51 toks/s]
Processed prompts:  95%|█████████▍| 485/512 [01:21<00:02, 11.99it/s, est. speed input: 95.21 toks/s, output: 1523.32 toks/s]
Processed prompts:  95%|█████████▌| 488/512 [01:21<00:01, 15.62it/s, est. speed input: 95.67 toks/s, output: 1530.70 toks/s]
Processed prompts:  96%|█████████▌| 490/512 [01:22<00:03,  6.66it/s, est. speed input: 95.20 toks/s, output: 1523.18 toks/s]
Processed prompts:  96%|█████████▌| 492/512 [01:23<00:05,  3.87it/s, est. speed input: 94.38 toks/s, output: 1510.09 toks/s]
Processed prompts:  96%|█████████▋| 493/512 [01:23<00:04,  3.86it/s, est. speed input: 94.27 toks/s, output: 1508.39 toks/s]
Processed prompts:  96%|█████████▋| 494/512 [01:23<00:04,  4.11it/s, est. speed input: 94.27 toks/s, output: 1508.27 toks/s]
Processed prompts:  97%|█████████▋| 495/512 [01:24<00:04,  4.20it/s, est. speed input: 94.21 toks/s, output: 1507.36 toks/s]
Processed prompts:  97%|█████████▋| 496/512 [01:24<00:03,  4.58it/s, est. speed input: 94.23 toks/s, output: 1507.63 toks/s]
Processed prompts:  97%|█████████▋| 497/512 [01:24<00:03,  4.97it/s, est. speed input: 94.25 toks/s, output: 1507.95 toks/s]
Processed prompts:  97%|█████████▋| 498/512 [01:24<00:02,  5.32it/s, est. speed input: 94.27 toks/s, output: 1508.28 toks/s]
Processed prompts:  98%|█████████▊| 500/512 [01:24<00:02,  6.00it/s, est. speed input: 94.34 toks/s, output: 1509.39 toks/s]
Processed prompts:  98%|█████████▊| 501/512 [01:24<00:01,  6.36it/s, est. speed input: 94.38 toks/s, output: 1510.15 toks/s]
Processed prompts:  98%|█████████▊| 502/512 [01:25<00:01,  6.03it/s, est. speed input: 94.36 toks/s, output: 1509.76 toks/s]
Processed prompts:  98%|█████████▊| 503/512 [01:25<00:01,  6.66it/s, est. speed input: 94.43 toks/s, output: 1510.88 toks/s]
Processed prompts:  98%|█████████▊| 504/512 [01:25<00:01,  6.92it/s, est. speed input: 94.47 toks/s, output: 1511.58 toks/s]
Processed prompts:  99%|█████████▊| 505/512 [01:25<00:00,  7.47it/s, est. speed input: 94.54 toks/s, output: 1512.70 toks/s]
Processed prompts:  99%|█████████▉| 508/512 [01:25<00:00, 11.01it/s, est. speed input: 94.92 toks/s, output: 1518.74 toks/s]
Processed prompts: 100%|█████████▉| 510/512 [01:25<00:00, 12.38it/s, est. speed input: 95.16 toks/s, output: 1522.53 toks/s]
Processed prompts: 100%|██████████| 512/512 [01:25<00:00, 12.38it/s, est. speed input: 95.44 toks/s, output: 1527.04 toks/s]
Processed prompts: 100%|██████████| 512/512 [01:25<00:00,  5.96it/s, est. speed input: 95.44 toks/s, output: 1527.04 toks/s]
[rank0]:[W126 11:49:09.025430154 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

