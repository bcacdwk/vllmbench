======================================================================
SlideSparse vLLM Throughput Benchmark Log
Created: 2026-01-28 08:24:42
======================================================================

原始命令:
  /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model bitnet1.58-2b-int8 --backend cublaslt,cusparselt --stage prefill --sparsity 2_4,2_6,2_8,2_10 --M 512,1024,2048,4096,8192,16384,32768

命令行参数:
  --model: bitnet1.58-2b-int8
  --backend: cublaslt,cusparselt
  --sparsity: 2_4,2_6,2_8,2_10
  --stage: prefill
  --M: 512,1024,2048,4096,8192,16384,32768
  --N: None
  --inner-32: False
  --eager: False
  --gpu-id: 3
  --gpu-mem: 0.8
  --dry-run: False
  --list-models: False

硬件信息:
  GPU: H100
  Compute Capability: cc90
  VRAM: 79.2 GB
  CUDA: 12.9
  Python: py312

Backend 环境变量 (初始状态):
  DISABLE_SLIDESPARSE: 未设置
  USE_CUBLASLT: 未设置
  USE_CUSPARSELT: 未设置
  SPARSITY: 未设置
  INNER_DTYPE_32: 未设置

======================================================================


============================================================
  BitNet-2B-INT8 | cuBLASLt | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints/BitNet-2B-INT8
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_INT8_py312_cu129_x86_64/cublaslt

============================================================
[1/7] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 08:24:51 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3298266) WARNING 01-28 08:25:07 [backends.py:609] Failed to read file <frozen os>
Throughput: 29.13 requests/s, 14945.21 total tokens/s, 29.13 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-28 08:24:51] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:24:51] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:24:51] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:24:51] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:24:51] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:24:51] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:24:51] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:24:51] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:24:51] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:24:51] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:24:51] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:24:51] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:24:51] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:24:51] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 08:24:58] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:24:58] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:24:58] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:24:58] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:24:58] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:24:58] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:24:58] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:24:58] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:24:58] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:24:58] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:24:58] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:24:58] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:24:58] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:24:58] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3298266) [2026-01-28 08:24:59] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=3298266) [2026-01-28 08:24:59] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3298266) [2026-01-28 08:24:59] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=3298266) [2026-01-28 08:24:59] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3298266) [2026-01-28 08:24:59] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=3298266) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3298266) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.05it/s]
(EngineCore_DP0 pid=3298266) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.05it/s]
(EngineCore_DP0 pid=3298266) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=3298266) 2026-01-28 08:25:21,481 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3298266) 2026-01-28 08:25:21,518 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3298266) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  2.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.16it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.13it/s]
(EngineCore_DP0 pid=3298266) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 13.55it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  41%|████▏     | 53/128 [00:00<00:00, 529.80it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 701.47it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:25,  4.92it/s, est. speed input: 2518.90 toks/s, output: 4.92 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:07, 17.48it/s, est. speed input: 7762.47 toks/s, output: 15.16 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:05, 23.17it/s, est. speed input: 10099.13 toks/s, output: 19.72 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:04, 26.12it/s, est. speed input: 11388.20 toks/s, output: 22.24 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:03, 28.05it/s, est. speed input: 12252.99 toks/s, output: 23.93 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:03, 29.22it/s, est. speed input: 12847.46 toks/s, output: 25.09 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:00<00:03, 30.01it/s, est. speed input: 13291.63 toks/s, output: 25.96 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:01<00:03, 30.56it/s, est. speed input: 13635.18 toks/s, output: 26.63 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:01<00:03, 30.90it/s, est. speed input: 13902.73 toks/s, output: 27.15 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:01<00:02, 31.17it/s, est. speed input: 14125.04 toks/s, output: 27.59 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:01<00:02, 31.37it/s, est. speed input: 14310.99 toks/s, output: 27.95 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:01<00:02, 31.47it/s, est. speed input: 14463.35 toks/s, output: 28.25 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:01<00:02, 31.55it/s, est. speed input: 14594.45 toks/s, output: 28.50 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:01<00:02, 31.64it/s, est. speed input: 14711.10 toks/s, output: 28.73 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:01<00:02, 31.69it/s, est. speed input: 14811.19 toks/s, output: 28.93 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:02<00:02, 31.76it/s, est. speed input: 14902.96 toks/s, output: 29.11 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:02<00:01, 31.75it/s, est. speed input: 14978.96 toks/s, output: 29.26 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:02<00:01, 31.76it/s, est. speed input: 15048.57 toks/s, output: 29.39 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:02<00:01, 31.77it/s, est. speed input: 15111.27 toks/s, output: 29.51 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:02<00:01, 31.70it/s, est. speed input: 15161.83 toks/s, output: 29.61 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:02<00:01, 31.64it/s, est. speed input: 15206.67 toks/s, output: 29.70 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:02<00:01, 31.70it/s, est. speed input: 15254.86 toks/s, output: 29.79 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:02<00:01, 31.66it/s, est. speed input: 15293.30 toks/s, output: 29.87 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:03<00:01, 31.65it/s, est. speed input: 15330.41 toks/s, output: 29.94 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:03<00:00, 31.67it/s, est. speed input: 15365.29 toks/s, output: 30.01 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:03<00:00, 31.65it/s, est. speed input: 15396.51 toks/s, output: 30.07 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:03<00:00, 31.65it/s, est. speed input: 15425.57 toks/s, output: 30.13 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:03<00:00, 31.69it/s, est. speed input: 15455.00 toks/s, output: 30.19 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:03<00:00, 31.69it/s, est. speed input: 15481.22 toks/s, output: 30.24 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:03<00:00, 31.70it/s, est. speed input: 15505.97 toks/s, output: 30.28 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:03<00:00, 31.72it/s, est. speed input: 15530.09 toks/s, output: 30.33 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:04<00:00, 31.74it/s, est. speed input: 15552.56 toks/s, output: 30.38 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:04<00:00, 31.74it/s, est. speed input: 15569.64 toks/s, output: 30.41 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:04<00:00, 30.41it/s, est. speed input: 15569.64 toks/s, output: 30.41 toks/s]
[rank0]:[W128 08:25:28.542752202 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 48.0s

测试结果:
  Requests/s:   29.13
  Tokens/s:     14945.21
  Total Reqs:   128
  Elapsed:      4.39s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     14916.08

============================================================
[2/7] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 08:25:37 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3299646) WARNING 01-28 08:25:52 [backends.py:609] Failed to read file <frozen os>
Throughput: 29.82 requests/s, 30565.49 total tokens/s, 29.82 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-28 08:25:37] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:25:37] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:25:37] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:25:37] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:25:37] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:25:37] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:25:37] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:25:37] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:25:37] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:25:37] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:25:37] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:25:37] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:25:37] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:25:37] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 08:25:44] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:25:44] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:25:44] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:25:44] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:25:44] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:25:44] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:25:44] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:25:44] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:25:44] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:25:44] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:25:44] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:25:44] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:25:44] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:25:44] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3299646) [2026-01-28 08:25:45] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=3299646) [2026-01-28 08:25:46] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3299646) [2026-01-28 08:25:46] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=3299646) [2026-01-28 08:25:46] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3299646) [2026-01-28 08:25:46] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=3299646) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3299646) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.96it/s]
(EngineCore_DP0 pid=3299646) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.96it/s]
(EngineCore_DP0 pid=3299646) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=3299646) 2026-01-28 08:26:04,579 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3299646) 2026-01-28 08:26:04,610 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3299646) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  4.35it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  6.25it/s]
(EngineCore_DP0 pid=3299646) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 13.99it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  22%|██▏       | 28/128 [00:00<00:00, 278.64it/s]
Adding requests:  62%|██████▎   | 80/128 [00:00<00:00, 417.78it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 429.64it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   6%|▋         | 8/128 [00:00<00:01, 65.48it/s, est. speed input: 67060.80 toks/s, output: 65.48 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:00<00:02, 40.96it/s, est. speed input: 44625.76 toks/s, output: 43.58 toks/s]
Processed prompts:  16%|█▌        | 20/128 [00:00<00:02, 37.03it/s, est. speed input: 40788.67 toks/s, output: 39.83 toks/s]
Processed prompts:  19%|█▉        | 24/128 [00:00<00:02, 35.18it/s, est. speed input: 39039.67 toks/s, output: 38.12 toks/s]
Processed prompts:  22%|██▏       | 28/128 [00:00<00:02, 34.16it/s, est. speed input: 37988.77 toks/s, output: 37.10 toks/s]
Processed prompts:  25%|██▌       | 32/128 [00:00<00:02, 33.44it/s, est. speed input: 37221.66 toks/s, output: 36.35 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:01<00:02, 32.96it/s, est. speed input: 36648.83 toks/s, output: 35.79 toks/s]
Processed prompts:  31%|███▏      | 40/128 [00:01<00:02, 32.45it/s, est. speed input: 36134.28 toks/s, output: 35.28 toks/s]
Processed prompts:  34%|███▍      | 44/128 [00:01<00:02, 31.95it/s, est. speed input: 35665.99 toks/s, output: 34.83 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:01<00:02, 31.62it/s, est. speed input: 35287.80 toks/s, output: 34.46 toks/s]
Processed prompts:  41%|████      | 52/128 [00:01<00:02, 31.37it/s, est. speed input: 34967.27 toks/s, output: 34.15 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:01<00:02, 31.21it/s, est. speed input: 34701.47 toks/s, output: 33.89 toks/s]
Processed prompts:  47%|████▋     | 60/128 [00:01<00:02, 31.04it/s, est. speed input: 34458.41 toks/s, output: 33.65 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:01<00:02, 30.94it/s, est. speed input: 34252.98 toks/s, output: 33.45 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:02<00:01, 30.88it/s, est. speed input: 34077.65 toks/s, output: 33.28 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:02<00:01, 30.86it/s, est. speed input: 33926.96 toks/s, output: 33.13 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:02<00:01, 30.84it/s, est. speed input: 33790.95 toks/s, output: 33.00 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:02<00:01, 30.77it/s, est. speed input: 33660.36 toks/s, output: 32.87 toks/s]
Processed prompts:  66%|██████▌   | 84/128 [00:02<00:01, 30.74it/s, est. speed input: 33545.66 toks/s, output: 32.76 toks/s]
Processed prompts:  69%|██████▉   | 88/128 [00:02<00:01, 30.77it/s, est. speed input: 33450.90 toks/s, output: 32.67 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:02<00:01, 30.81it/s, est. speed input: 33367.40 toks/s, output: 32.58 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:02<00:01, 30.81it/s, est. speed input: 33288.00 toks/s, output: 32.51 toks/s]
Processed prompts:  78%|███████▊  | 100/128 [00:03<00:00, 30.82it/s, est. speed input: 33216.33 toks/s, output: 32.44 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:03<00:00, 30.83it/s, est. speed input: 33151.12 toks/s, output: 32.37 toks/s]
Processed prompts:  84%|████████▍ | 108/128 [00:03<00:00, 30.84it/s, est. speed input: 33091.44 toks/s, output: 32.32 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:03<00:00, 30.84it/s, est. speed input: 33035.24 toks/s, output: 32.26 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:03<00:00, 30.82it/s, est. speed input: 32980.04 toks/s, output: 32.21 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:03<00:00, 30.75it/s, est. speed input: 32921.54 toks/s, output: 32.15 toks/s]
Processed prompts:  97%|█████████▋| 124/128 [00:03<00:00, 30.76it/s, est. speed input: 32874.28 toks/s, output: 32.10 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 30.77it/s, est. speed input: 32830.37 toks/s, output: 32.06 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 30.77it/s, est. speed input: 32830.37 toks/s, output: 32.06 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 32.06it/s, est. speed input: 32830.37 toks/s, output: 32.06 toks/s]
[rank0]:[W128 08:26:11.113949906 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 42.4s

测试结果:
  Requests/s:   29.82
  Tokens/s:     30565.49
  Total Reqs:   128
  Elapsed:      4.29s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     30535.67

============================================================
[3/7] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 08:26:20 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3300910) WARNING 01-28 08:26:35 [backends.py:609] Failed to read file <frozen os>
Throughput: 55.32 requests/s, 56698.91 total tokens/s, 55.32 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-28 08:26:20] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:26:20] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:26:20] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:26:20] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:26:20] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:26:20] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:26:20] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:26:20] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:26:20] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:26:20] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:26:20] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:26:20] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:26:20] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:26:20] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 08:26:27] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:26:27] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:26:27] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:26:27] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:26:27] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:26:27] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:26:27] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:26:27] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:26:27] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:26:27] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:26:27] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:26:27] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:26:27] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:26:27] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3300910) [2026-01-28 08:26:28] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=3300910) [2026-01-28 08:26:29] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3300910) [2026-01-28 08:26:29] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=3300910) [2026-01-28 08:26:29] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3300910) [2026-01-28 08:26:29] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=3300910) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3300910) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.06it/s]
(EngineCore_DP0 pid=3300910) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.06it/s]
(EngineCore_DP0 pid=3300910) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=3300910) 2026-01-28 08:26:47,381 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3300910) 2026-01-28 08:26:47,412 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3300910) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:00<00:00, 14.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 13.30it/s]
(EngineCore_DP0 pid=3300910) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00,  7.32it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00,  7.32it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  10%|█         | 26/256 [00:00<00:00, 259.71it/s]
Adding requests:  31%|███       | 79/256 [00:00<00:00, 416.47it/s]
Adding requests:  51%|█████     | 130/256 [00:00<00:00, 456.76it/s]
Adding requests:  70%|███████   | 180/256 [00:00<00:00, 471.96it/s]
Adding requests:  91%|█████████ | 232/256 [00:00<00:00, 488.61it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 465.05it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   5%|▌         | 14/256 [00:00<00:01, 124.44it/s, est. speed input: 127444.67 toks/s, output: 124.44 toks/s]
Processed prompts:  11%|█         | 27/256 [00:00<00:02, 82.86it/s, est. speed input: 89505.49 toks/s, output: 87.40 toks/s]   
Processed prompts:  14%|█▍        | 37/256 [00:00<00:02, 73.47it/s, est. speed input: 80673.57 toks/s, output: 78.78 toks/s]
Processed prompts:  18%|█▊        | 45/256 [00:00<00:03, 69.61it/s, est. speed input: 77023.81 toks/s, output: 75.22 toks/s]
Processed prompts:  21%|██        | 53/256 [00:00<00:03, 67.22it/s, est. speed input: 74690.50 toks/s, output: 72.94 toks/s]
Processed prompts:  23%|██▎       | 60/256 [00:00<00:03, 63.11it/s, est. speed input: 71828.12 toks/s, output: 70.14 toks/s]
Processed prompts:  27%|██▋       | 68/256 [00:00<00:02, 62.99it/s, est. speed input: 70841.49 toks/s, output: 69.18 toks/s]
Processed prompts:  30%|██▉       | 76/256 [00:01<00:02, 62.75it/s, est. speed input: 70016.58 toks/s, output: 68.37 toks/s]
Processed prompts:  33%|███▎      | 84/256 [00:01<00:02, 62.59it/s, est. speed input: 69364.30 toks/s, output: 67.74 toks/s]
Processed prompts:  36%|███▌      | 92/256 [00:01<00:02, 62.50it/s, est. speed input: 68840.70 toks/s, output: 67.23 toks/s]
Processed prompts:  39%|███▉      | 100/256 [00:01<00:02, 62.43it/s, est. speed input: 68403.99 toks/s, output: 66.80 toks/s]
Processed prompts:  42%|████▏     | 108/256 [00:01<00:02, 62.37it/s, est. speed input: 68035.15 toks/s, output: 66.44 toks/s]
Processed prompts:  45%|████▌     | 116/256 [00:01<00:02, 62.33it/s, est. speed input: 67719.79 toks/s, output: 66.13 toks/s]
Processed prompts:  48%|████▊     | 124/256 [00:01<00:02, 61.76it/s, est. speed input: 67313.66 toks/s, output: 65.73 toks/s]
Processed prompts:  52%|█████▏    | 132/256 [00:02<00:02, 61.39it/s, est. speed input: 66963.55 toks/s, output: 65.39 toks/s]
Processed prompts:  55%|█████▍    | 140/256 [00:02<00:01, 61.01it/s, est. speed input: 66632.91 toks/s, output: 65.07 toks/s]
Processed prompts:  58%|█████▊    | 148/256 [00:02<00:01, 60.88it/s, est. speed input: 66367.33 toks/s, output: 64.81 toks/s]
Processed prompts:  61%|██████    | 156/256 [00:02<00:01, 60.70it/s, est. speed input: 66111.21 toks/s, output: 64.56 toks/s]
Processed prompts:  64%|██████▍   | 164/256 [00:02<00:01, 60.55it/s, est. speed input: 65880.11 toks/s, output: 64.34 toks/s]
Processed prompts:  67%|██████▋   | 172/256 [00:02<00:01, 60.54it/s, est. speed input: 65686.33 toks/s, output: 64.15 toks/s]
Processed prompts:  70%|███████   | 180/256 [00:02<00:01, 60.37it/s, est. speed input: 65484.41 toks/s, output: 63.95 toks/s]
Processed prompts:  73%|███████▎  | 188/256 [00:02<00:01, 60.33it/s, est. speed input: 65312.87 toks/s, output: 63.78 toks/s]
Processed prompts:  76%|███████▌  | 195/256 [00:03<00:00, 62.67it/s, est. speed input: 65517.53 toks/s, output: 63.98 toks/s]
Processed prompts:  79%|███████▉  | 202/256 [00:03<00:00, 59.56it/s, est. speed input: 65043.43 toks/s, output: 63.52 toks/s]
Processed prompts:  82%|████████▏ | 210/256 [00:03<00:00, 59.77it/s, est. speed input: 64907.43 toks/s, output: 63.39 toks/s]
Processed prompts:  85%|████████▌ | 218/256 [00:03<00:00, 59.86it/s, est. speed input: 64776.45 toks/s, output: 63.26 toks/s]
Processed prompts:  88%|████████▊ | 226/256 [00:03<00:00, 60.02it/s, est. speed input: 64666.71 toks/s, output: 63.15 toks/s]
Processed prompts:  91%|█████████▏| 234/256 [00:03<00:00, 60.13it/s, est. speed input: 64565.72 toks/s, output: 63.05 toks/s]
Processed prompts:  95%|█████████▍| 242/256 [00:03<00:00, 60.25it/s, est. speed input: 64476.96 toks/s, output: 62.97 toks/s]
Processed prompts:  98%|█████████▊| 250/256 [00:03<00:00, 60.19it/s, est. speed input: 64377.41 toks/s, output: 62.87 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:04<00:00, 60.19it/s, est. speed input: 64322.93 toks/s, output: 62.81 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:04<00:00, 62.81it/s, est. speed input: 64322.93 toks/s, output: 62.81 toks/s]
[rank0]:[W128 08:26:54.380535875 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 43.3s

测试结果:
  Requests/s:   55.32
  Tokens/s:     56698.91
  Total Reqs:   256
  Elapsed:      4.63s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     56643.60

============================================================
[4/7] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 08:27:05 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3302012) WARNING 01-28 08:27:19 [backends.py:609] Failed to read file <frozen os>
Throughput: 77.36 requests/s, 79292.47 total tokens/s, 77.36 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-28 08:27:04] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:27:05] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:27:05] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:27:05] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:27:05] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:27:05] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:27:05] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:27:05] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:27:05] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:27:05] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:27:05] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:27:05] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:27:05] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:27:05] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 08:27:11] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:27:11] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:27:11] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:27:11] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:27:11] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:27:11] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:27:11] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:27:11] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:27:11] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:27:11] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:27:11] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:27:11] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:27:11] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:27:11] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3302012) [2026-01-28 08:27:13] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=3302012) [2026-01-28 08:27:13] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3302012) [2026-01-28 08:27:13] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=3302012) [2026-01-28 08:27:13] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3302012) [2026-01-28 08:27:13] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=3302012) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3302012) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.06it/s]
(EngineCore_DP0 pid=3302012) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.06it/s]
(EngineCore_DP0 pid=3302012) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=3302012) 2026-01-28 08:27:31,295 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3302012) 2026-01-28 08:27:31,327 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3302012) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00, 13.85it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 13.34it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 13.41it/s]
(EngineCore_DP0 pid=3302012) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00, 13.65it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 13.95it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   5%|▌         | 28/512 [00:00<00:01, 274.87it/s]
Adding requests:  16%|█▌        | 81/512 [00:00<00:01, 419.50it/s]
Adding requests:  26%|██▌       | 132/512 [00:00<00:00, 458.78it/s]
Adding requests:  35%|███▌      | 181/512 [00:00<00:00, 470.39it/s]
Adding requests:  46%|████▌     | 233/512 [00:00<00:00, 487.41it/s]
Adding requests:  55%|█████▌    | 284/512 [00:00<00:00, 494.31it/s]
Adding requests:  65%|██████▌   | 334/512 [00:00<00:00, 494.81it/s]
Adding requests:  75%|███████▌  | 385/512 [00:00<00:00, 499.04it/s]
Adding requests:  85%|████████▌ | 437/512 [00:00<00:00, 504.73it/s]
Adding requests:  96%|█████████▌| 489/512 [00:01<00:00, 504.60it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 484.77it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   8%|▊         | 42/512 [00:00<00:01, 291.77it/s, est. speed input: 298867.46 toks/s, output: 291.80 toks/s]
Processed prompts:  14%|█▍        | 72/512 [00:00<00:03, 139.64it/s, est. speed input: 157356.11 toks/s, output: 153.66 toks/s]
Processed prompts:  18%|█▊        | 90/512 [00:00<00:03, 112.58it/s, est. speed input: 131663.19 toks/s, output: 128.57 toks/s]
Processed prompts:  20%|██        | 104/512 [00:00<00:03, 109.11it/s, est. speed input: 126764.09 toks/s, output: 123.79 toks/s]
Processed prompts:  23%|██▎       | 116/512 [00:00<00:03, 102.43it/s, est. speed input: 121138.87 toks/s, output: 118.30 toks/s]
Processed prompts:  25%|██▍       | 127/512 [00:01<00:04, 95.57it/s, est. speed input: 116045.18 toks/s, output: 113.32 toks/s] 
Processed prompts:  27%|██▋       | 138/512 [00:01<00:04, 90.76it/s, est. speed input: 112158.39 toks/s, output: 109.53 toks/s]
Processed prompts:  29%|██▉       | 150/512 [00:01<00:04, 89.45it/s, est. speed input: 109790.30 toks/s, output: 107.21 toks/s]
Processed prompts:  32%|███▏      | 162/512 [00:01<00:03, 88.53it/s, est. speed input: 107856.61 toks/s, output: 105.33 toks/s]
Processed prompts:  34%|███▍      | 174/512 [00:01<00:03, 87.87it/s, est. speed input: 106241.85 toks/s, output: 103.75 toks/s]
Processed prompts:  36%|███▋      | 186/512 [00:01<00:03, 87.38it/s, est. speed input: 104867.28 toks/s, output: 102.41 toks/s]
Processed prompts:  39%|███▊      | 198/512 [00:01<00:03, 86.86it/s, est. speed input: 103636.91 toks/s, output: 101.21 toks/s]
Processed prompts:  41%|████      | 210/512 [00:02<00:03, 86.50it/s, est. speed input: 102571.98 toks/s, output: 100.17 toks/s]
Processed prompts:  43%|████▎     | 222/512 [00:02<00:03, 86.28it/s, est. speed input: 101649.64 toks/s, output: 99.27 toks/s] 
Processed prompts:  46%|████▌     | 234/512 [00:02<00:03, 86.21it/s, est. speed input: 100856.75 toks/s, output: 98.49 toks/s]
Processed prompts:  48%|████▊     | 246/512 [00:02<00:03, 86.15it/s, est. speed input: 100145.37 toks/s, output: 97.80 toks/s]
Processed prompts:  50%|█████     | 258/512 [00:02<00:02, 86.11it/s, est. speed input: 99512.12 toks/s, output: 97.18 toks/s] 
Processed prompts:  53%|█████▎    | 270/512 [00:02<00:02, 86.12it/s, est. speed input: 98948.77 toks/s, output: 96.63 toks/s]
Processed prompts:  55%|█████▌    | 282/512 [00:02<00:02, 86.07it/s, est. speed input: 98428.70 toks/s, output: 96.12 toks/s]
Processed prompts:  57%|█████▋    | 294/512 [00:03<00:02, 85.98it/s, est. speed input: 97945.90 toks/s, output: 95.65 toks/s]
Processed prompts:  60%|█████▉    | 306/512 [00:03<00:02, 85.94it/s, est. speed input: 97509.41 toks/s, output: 95.22 toks/s]
Processed prompts:  62%|██████▏   | 318/512 [00:03<00:02, 85.90it/s, est. speed input: 97106.97 toks/s, output: 94.83 toks/s]
Processed prompts:  64%|██████▍   | 330/512 [00:03<00:02, 85.90it/s, est. speed input: 96740.73 toks/s, output: 94.47 toks/s]
Processed prompts:  67%|██████▋   | 342/512 [00:03<00:01, 87.45it/s, est. speed input: 96623.31 toks/s, output: 94.36 toks/s]
Processed prompts:  69%|██████▉   | 354/512 [00:03<00:01, 87.04it/s, est. speed input: 96311.11 toks/s, output: 94.05 toks/s]
Processed prompts:  71%|███████▏  | 366/512 [00:03<00:01, 86.74it/s, est. speed input: 96017.53 toks/s, output: 93.77 toks/s]
Processed prompts:  74%|███████▍  | 378/512 [00:04<00:01, 86.48it/s, est. speed input: 95738.04 toks/s, output: 93.49 toks/s]
Processed prompts:  76%|███████▌  | 390/512 [00:04<00:01, 86.38it/s, est. speed input: 95487.07 toks/s, output: 93.25 toks/s]
Processed prompts:  79%|███████▊  | 402/512 [00:04<00:01, 86.19it/s, est. speed input: 95239.05 toks/s, output: 93.01 toks/s]
Processed prompts:  81%|████████  | 414/512 [00:04<00:01, 85.98it/s, est. speed input: 94996.96 toks/s, output: 92.77 toks/s]
Processed prompts:  83%|████████▎ | 426/512 [00:04<00:01, 85.94it/s, est. speed input: 94781.62 toks/s, output: 92.56 toks/s]
Processed prompts:  86%|████████▌ | 438/512 [00:04<00:00, 85.91it/s, est. speed input: 94578.53 toks/s, output: 92.36 toks/s]
Processed prompts:  88%|████████▊ | 450/512 [00:04<00:00, 87.64it/s, est. speed input: 94567.42 toks/s, output: 92.35 toks/s]
Processed prompts:  90%|█████████ | 462/512 [00:05<00:00, 87.22it/s, est. speed input: 94395.26 toks/s, output: 92.18 toks/s]
Processed prompts:  93%|█████████▎| 474/512 [00:05<00:00, 86.87it/s, est. speed input: 94224.50 toks/s, output: 92.02 toks/s]
Processed prompts:  95%|█████████▍| 486/512 [00:05<00:00, 86.62it/s, est. speed input: 94063.35 toks/s, output: 91.86 toks/s]
Processed prompts:  97%|█████████▋| 498/512 [00:05<00:00, 86.35it/s, est. speed input: 93901.38 toks/s, output: 91.70 toks/s]
Processed prompts: 100%|█████████▉| 510/512 [00:05<00:00, 88.17it/s, est. speed input: 93926.09 toks/s, output: 91.72 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:05<00:00, 88.17it/s, est. speed input: 94289.67 toks/s, output: 92.08 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:05<00:00, 92.08it/s, est. speed input: 94289.67 toks/s, output: 92.08 toks/s]
[rank0]:[W128 08:27:40.285804059 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 46.1s

测试结果:
  Requests/s:   77.36
  Tokens/s:     79292.47
  Total Reqs:   512
  Elapsed:      6.62s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     79215.11

============================================================
[5/7] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 08:27:53 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3303181) WARNING 01-28 08:28:07 [backends.py:609] Failed to read file <frozen os>
Throughput: 86.58 requests/s, 88749.47 total tokens/s, 86.58 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-28 08:27:52] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:27:53] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:27:53] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:27:53] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:27:53] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:27:53] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:27:53] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:27:53] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:27:53] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:27:53] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:27:53] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:27:53] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:27:53] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:27:53] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 08:27:59] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:27:59] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:27:59] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:27:59] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:27:59] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:27:59] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:27:59] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:27:59] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:27:59] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:27:59] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:27:59] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:27:59] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:27:59] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:27:59] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3303181) [2026-01-28 08:28:00] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=3303181) [2026-01-28 08:28:00] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3303181) [2026-01-28 08:28:00] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=3303181) [2026-01-28 08:28:00] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3303181) [2026-01-28 08:28:00] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=3303181) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3303181) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.04it/s]
(EngineCore_DP0 pid=3303181) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.04it/s]
(EngineCore_DP0 pid=3303181) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=3303181) 2026-01-28 08:28:18,618 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3303181) 2026-01-28 08:28:18,650 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3303181) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 1/5 [00:00<00:01,  2.37it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00,  4.24it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00,  7.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00,  6.65it/s]
(EngineCore_DP0 pid=3303181) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00, 14.16it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 14.65it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 14.57it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   5%|▍         | 48/1024 [00:00<00:02, 475.34it/s]
Adding requests:  10%|▉         | 98/1024 [00:00<00:01, 489.65it/s]
Adding requests:  14%|█▍        | 148/1024 [00:00<00:01, 492.89it/s]
Adding requests:  19%|█▉        | 198/1024 [00:00<00:01, 490.76it/s]
Adding requests:  24%|██▍       | 250/1024 [00:00<00:01, 498.91it/s]
Adding requests:  29%|██▉       | 300/1024 [00:00<00:01, 495.60it/s]
Adding requests:  34%|███▍      | 351/1024 [00:00<00:01, 498.83it/s]
Adding requests:  39%|███▉      | 402/1024 [00:00<00:01, 502.13it/s]
Adding requests:  44%|████▍     | 453/1024 [00:00<00:01, 496.67it/s]
Adding requests:  49%|████▉     | 503/1024 [00:01<00:01, 497.04it/s]
Adding requests:  54%|█████▍    | 553/1024 [00:01<00:00, 491.25it/s]
Adding requests:  59%|█████▉    | 604/1024 [00:01<00:00, 494.98it/s]
Adding requests:  64%|██████▍   | 657/1024 [00:01<00:00, 503.25it/s]
Adding requests:  69%|██████▉   | 709/1024 [00:01<00:00, 508.11it/s]
Adding requests:  74%|███████▍  | 760/1024 [00:01<00:00, 505.09it/s]
Adding requests:  79%|███████▉  | 811/1024 [00:01<00:00, 496.28it/s]
Adding requests:  84%|████████▍ | 861/1024 [00:01<00:00, 495.84it/s]
Adding requests:  89%|████████▉ | 913/1024 [00:01<00:00, 500.60it/s]
Adding requests:  94%|█████████▍| 965/1024 [00:01<00:00, 505.04it/s]
Adding requests:  99%|█████████▉| 1017/1024 [00:02<00:00, 507.64it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 499.60it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  17%|█▋        | 178/1024 [00:00<00:00, 1063.97it/s, est. speed input: 1090001.52 toks/s, output: 1064.25 toks/s]
Processed prompts:  28%|██▊       | 285/1024 [00:01<00:04, 176.64it/s, est. speed input: 214380.83 toks/s, output: 209.35 toks/s]   
Processed prompts:  33%|███▎      | 335/1024 [00:01<00:04, 144.67it/s, est. speed input: 179622.38 toks/s, output: 175.41 toks/s]
Processed prompts:  36%|███▌      | 366/1024 [00:02<00:05, 129.31it/s, est. speed input: 165003.54 toks/s, output: 161.14 toks/s]
Processed prompts:  38%|███▊      | 389/1024 [00:02<00:05, 119.20it/s, est. speed input: 156408.85 toks/s, output: 152.74 toks/s]
Processed prompts:  40%|███▉      | 407/1024 [00:02<00:05, 115.59it/s, est. speed input: 152674.56 toks/s, output: 149.10 toks/s]
Processed prompts:  41%|████      | 422/1024 [00:02<00:05, 108.85it/s, est. speed input: 148327.99 toks/s, output: 144.85 toks/s]
Processed prompts:  42%|████▏     | 435/1024 [00:03<00:05, 100.38it/s, est. speed input: 143813.97 toks/s, output: 140.44 toks/s]
Processed prompts:  44%|████▍     | 450/1024 [00:03<00:05, 96.93it/s, est. speed input: 140798.83 toks/s, output: 137.50 toks/s] 
Processed prompts:  46%|████▌     | 466/1024 [00:03<00:05, 94.37it/s, est. speed input: 138047.20 toks/s, output: 134.81 toks/s]
Processed prompts:  47%|████▋     | 482/1024 [00:03<00:05, 92.52it/s, est. speed input: 135611.01 toks/s, output: 132.43 toks/s]
Processed prompts:  49%|████▊     | 498/1024 [00:03<00:05, 91.04it/s, est. speed input: 133386.20 toks/s, output: 130.26 toks/s]
Processed prompts:  50%|█████     | 514/1024 [00:04<00:05, 89.93it/s, est. speed input: 131363.50 toks/s, output: 128.28 toks/s]
Processed prompts:  52%|█████▏    | 530/1024 [00:04<00:05, 88.97it/s, est. speed input: 129483.11 toks/s, output: 126.45 toks/s]
Processed prompts:  53%|█████▎    | 546/1024 [00:04<00:05, 88.34it/s, est. speed input: 127773.06 toks/s, output: 124.78 toks/s]
Processed prompts:  55%|█████▍    | 562/1024 [00:04<00:05, 88.07it/s, est. speed input: 126237.65 toks/s, output: 123.28 toks/s]
Processed prompts:  56%|█████▋    | 578/1024 [00:04<00:05, 87.83it/s, est. speed input: 124812.12 toks/s, output: 121.89 toks/s]
Processed prompts:  58%|█████▊    | 594/1024 [00:04<00:04, 87.65it/s, est. speed input: 123489.30 toks/s, output: 120.59 toks/s]
Processed prompts:  60%|█████▉    | 610/1024 [00:05<00:04, 87.44it/s, est. speed input: 122249.17 toks/s, output: 119.38 toks/s]
Processed prompts:  61%|██████    | 626/1024 [00:05<00:04, 87.25it/s, est. speed input: 121087.25 toks/s, output: 118.25 toks/s]
Processed prompts:  63%|██████▎   | 642/1024 [00:05<00:04, 87.18it/s, est. speed input: 120013.44 toks/s, output: 117.20 toks/s]
Processed prompts:  64%|██████▍   | 658/1024 [00:05<00:04, 87.14it/s, est. speed input: 119011.27 toks/s, output: 116.22 toks/s]
Processed prompts:  66%|██████▌   | 674/1024 [00:05<00:04, 87.15it/s, est. speed input: 118077.30 toks/s, output: 115.31 toks/s]
Processed prompts:  67%|██████▋   | 690/1024 [00:06<00:03, 87.06it/s, est. speed input: 117186.77 toks/s, output: 114.44 toks/s]
Processed prompts:  69%|██████▉   | 706/1024 [00:06<00:03, 87.03it/s, est. speed input: 116353.84 toks/s, output: 113.63 toks/s]
Processed prompts:  71%|███████   | 722/1024 [00:06<00:03, 86.96it/s, est. speed input: 115562.04 toks/s, output: 112.85 toks/s]
Processed prompts:  72%|███████▏  | 738/1024 [00:06<00:03, 86.94it/s, est. speed input: 114818.58 toks/s, output: 112.13 toks/s]
Processed prompts:  74%|███████▎  | 754/1024 [00:06<00:03, 87.03it/s, est. speed input: 114127.87 toks/s, output: 111.45 toks/s]
Processed prompts:  75%|███████▌  | 770/1024 [00:06<00:02, 86.97it/s, est. speed input: 113458.80 toks/s, output: 110.80 toks/s]
Processed prompts:  77%|███████▋  | 786/1024 [00:07<00:02, 86.99it/s, est. speed input: 112831.42 toks/s, output: 110.19 toks/s]
Processed prompts:  78%|███████▊  | 802/1024 [00:07<00:02, 86.94it/s, est. speed input: 112228.91 toks/s, output: 109.60 toks/s]
Processed prompts:  80%|███████▉  | 818/1024 [00:07<00:02, 86.92it/s, est. speed input: 111658.02 toks/s, output: 109.04 toks/s]
Processed prompts:  81%|████████▏ | 834/1024 [00:07<00:02, 86.94it/s, est. speed input: 111116.98 toks/s, output: 108.51 toks/s]
Processed prompts:  83%|████████▎ | 850/1024 [00:07<00:02, 86.87it/s, est. speed input: 110593.82 toks/s, output: 108.00 toks/s]
Processed prompts:  85%|████████▍ | 866/1024 [00:08<00:01, 86.83it/s, est. speed input: 110094.94 toks/s, output: 107.51 toks/s]
Processed prompts:  86%|████████▌ | 882/1024 [00:08<00:01, 86.89it/s, est. speed input: 109626.60 toks/s, output: 107.06 toks/s]
Processed prompts:  88%|████████▊ | 898/1024 [00:08<00:01, 86.87it/s, est. speed input: 109173.42 toks/s, output: 106.61 toks/s]
Processed prompts:  89%|████████▉ | 914/1024 [00:08<00:01, 86.85it/s, est. speed input: 108738.92 toks/s, output: 106.19 toks/s]
Processed prompts:  91%|█████████ | 930/1024 [00:08<00:01, 86.83it/s, est. speed input: 108322.29 toks/s, output: 105.78 toks/s]
Processed prompts:  92%|█████████▏| 946/1024 [00:08<00:00, 88.19it/s, est. speed input: 108038.09 toks/s, output: 105.51 toks/s]
Processed prompts:  94%|█████████▍| 962/1024 [00:09<00:00, 87.80it/s, est. speed input: 107654.47 toks/s, output: 105.13 toks/s]
Processed prompts:  96%|█████████▌| 978/1024 [00:09<00:00, 87.49it/s, est. speed input: 107282.95 toks/s, output: 104.77 toks/s]
Processed prompts:  97%|█████████▋| 994/1024 [00:09<00:00, 88.57it/s, est. speed input: 107027.41 toks/s, output: 104.52 toks/s]
Processed prompts:  99%|█████████▊| 1010/1024 [00:09<00:00, 88.14it/s, est. speed input: 106690.25 toks/s, output: 104.19 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:09<00:00, 88.14it/s, est. speed input: 107270.84 toks/s, output: 104.76 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:09<00:00, 104.75it/s, est. speed input: 107270.84 toks/s, output: 104.76 toks/s]
[rank0]:[W128 08:28:33.658550496 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 53.2s

测试结果:
  Requests/s:   86.58
  Tokens/s:     88749.47
  Total Reqs:   1024
  Elapsed:      11.83s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     88662.88

============================================================
[6/7] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 08:28:50 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3304479) WARNING 01-28 08:29:05 [backends.py:609] Failed to read file <frozen os>
Throughput: 89.92 requests/s, 92167.77 total tokens/s, 89.92 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-28 08:28:50] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:28:50] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:28:50] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:28:50] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:28:50] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:28:50] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:28:50] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:28:50] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:28:50] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:28:50] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:28:50] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:28:50] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:28:50] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:28:50] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 08:28:57] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:28:57] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:28:57] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:28:57] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:28:57] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:28:57] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:28:57] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:28:57] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:28:57] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:28:57] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:28:57] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:28:57] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:28:57] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:28:57] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3304479) [2026-01-28 08:28:58] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=3304479) [2026-01-28 08:28:59] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3304479) [2026-01-28 08:28:59] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=3304479) [2026-01-28 08:28:59] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3304479) [2026-01-28 08:28:59] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=3304479) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3304479) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.93it/s]
(EngineCore_DP0 pid=3304479) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.93it/s]
(EngineCore_DP0 pid=3304479) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=3304479) 2026-01-28 08:29:17,058 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3304479) 2026-01-28 08:29:17,091 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3304479) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00, 13.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00, 14.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:00<00:00, 14.91it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 14.14it/s]
(EngineCore_DP0 pid=3304479) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 1/5 [00:00<00:00,  5.80it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 3/5 [00:00<00:00,  9.33it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 11.77it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 10.65it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 31/2048 [00:00<00:06, 307.45it/s]
Adding requests:   4%|▍         | 84/2048 [00:00<00:04, 434.29it/s]
Adding requests:   7%|▋         | 135/2048 [00:00<00:04, 466.52it/s]
Adding requests:   9%|▉         | 185/2048 [00:00<00:03, 477.35it/s]
Adding requests:  12%|█▏        | 237/2048 [00:00<00:03, 492.34it/s]
Adding requests:  14%|█▍        | 287/2048 [00:00<00:03, 492.76it/s]
Adding requests:  16%|█▋        | 337/2048 [00:00<00:03, 469.02it/s]
Adding requests:  19%|█▉        | 387/2048 [00:00<00:03, 477.69it/s]
Adding requests:  21%|██        | 435/2048 [00:00<00:03, 477.58it/s]
Adding requests:  24%|██▎       | 485/2048 [00:01<00:03, 482.90it/s]
Adding requests:  26%|██▌       | 534/2048 [00:01<00:03, 473.34it/s]
Adding requests:  29%|██▊       | 588/2048 [00:01<00:02, 490.18it/s]
Adding requests:  31%|███▏      | 641/2048 [00:01<00:02, 499.69it/s]
Adding requests:  34%|███▍      | 695/2048 [00:01<00:02, 507.99it/s]
Adding requests:  36%|███▋      | 746/2048 [00:01<00:02, 507.48it/s]
Adding requests:  39%|███▉      | 797/2048 [00:01<00:02, 491.58it/s]
Adding requests:  41%|████▏     | 847/2048 [00:01<00:02, 480.99it/s]
Adding requests:  44%|████▍     | 900/2048 [00:01<00:02, 494.63it/s]
Adding requests:  46%|████▋     | 952/2048 [00:01<00:02, 499.47it/s]
Adding requests:  49%|████▉     | 1004/2048 [00:02<00:02, 505.28it/s]
Adding requests:  52%|█████▏    | 1056/2048 [00:02<00:01, 507.76it/s]
Adding requests:  54%|█████▍    | 1107/2048 [00:02<00:01, 507.15it/s]
Adding requests:  57%|█████▋    | 1159/2048 [00:02<00:01, 509.10it/s]
Adding requests:  59%|█████▉    | 1213/2048 [00:02<00:01, 517.96it/s]
Adding requests:  62%|██████▏   | 1265/2048 [00:02<00:01, 509.43it/s]
Adding requests:  64%|██████▍   | 1317/2048 [00:02<00:01, 510.60it/s]
Adding requests:  67%|██████▋   | 1370/2048 [00:02<00:01, 514.23it/s]
Adding requests:  69%|██████▉   | 1422/2048 [00:02<00:01, 515.20it/s]
Adding requests:  72%|███████▏  | 1475/2048 [00:02<00:01, 516.64it/s]
Adding requests:  75%|███████▍  | 1528/2048 [00:03<00:01, 519.79it/s]
Adding requests:  77%|███████▋  | 1581/2048 [00:03<00:00, 522.74it/s]
Adding requests:  80%|███████▉  | 1635/2048 [00:03<00:00, 526.77it/s]
Adding requests:  82%|████████▏ | 1688/2048 [00:03<00:00, 521.43it/s]
Adding requests:  85%|████████▌ | 1741/2048 [00:03<00:00, 523.46it/s]
Adding requests:  88%|████████▊ | 1794/2048 [00:03<00:00, 519.66it/s]
Adding requests:  90%|█████████ | 1847/2048 [00:03<00:00, 521.32it/s]
Adding requests:  93%|█████████▎| 1900/2048 [00:03<00:00, 516.62it/s]
Adding requests:  95%|█████████▌| 1952/2048 [00:03<00:00, 516.56it/s]
Adding requests:  98%|█████████▊| 2004/2048 [00:03<00:00, 509.35it/s]
Adding requests: 100%|██████████| 2048/2048 [00:04<00:00, 501.57it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  17%|█▋        | 354/2048 [00:00<00:00, 2984.60it/s, est. speed input: 3056830.18 toks/s, output: 2984.78 toks/s]
Processed prompts:  32%|███▏      | 653/2048 [00:03<00:08, 167.44it/s, est. speed input: 202550.80 toks/s, output: 197.80 toks/s]   
Processed prompts:  38%|███▊      | 782/2048 [00:04<00:09, 138.15it/s, est. speed input: 169667.96 toks/s, output: 165.69 toks/s]
Processed prompts:  42%|████▏     | 856/2048 [00:05<00:09, 124.18it/s, est. speed input: 156324.75 toks/s, output: 152.66 toks/s]
Processed prompts:  44%|████▍     | 905/2048 [00:06<00:09, 118.38it/s, est. speed input: 150919.22 toks/s, output: 147.38 toks/s]
Processed prompts:  46%|████▌     | 940/2048 [00:06<00:09, 115.95it/s, est. speed input: 148406.12 toks/s, output: 144.93 toks/s]
Processed prompts:  47%|████▋     | 966/2048 [00:06<00:09, 108.51it/s, est. speed input: 144575.89 toks/s, output: 141.19 toks/s]
Processed prompts:  48%|████▊     | 986/2048 [00:07<00:09, 109.60it/s, est. speed input: 144029.93 toks/s, output: 140.65 toks/s]
Processed prompts:  49%|████▉     | 1004/2048 [00:07<00:09, 108.41it/s, est. speed input: 143019.46 toks/s, output: 139.67 toks/s]
Processed prompts:  50%|████▉     | 1020/2048 [00:07<00:09, 105.36it/s, est. speed input: 141783.41 toks/s, output: 138.46 toks/s]
Processed prompts:  50%|█████     | 1034/2048 [00:07<00:10, 100.32it/s, est. speed input: 140340.60 toks/s, output: 137.05 toks/s]
Processed prompts:  51%|█████     | 1046/2048 [00:07<00:10, 93.35it/s, est. speed input: 138701.47 toks/s, output: 135.45 toks/s] 
Processed prompts:  52%|█████▏    | 1058/2048 [00:07<00:11, 87.33it/s, est. speed input: 137137.39 toks/s, output: 133.92 toks/s]
Processed prompts:  52%|█████▏    | 1074/2048 [00:08<00:11, 88.03it/s, est. speed input: 136151.32 toks/s, output: 132.96 toks/s]
Processed prompts:  53%|█████▎    | 1090/2048 [00:08<00:10, 88.55it/s, est. speed input: 135205.11 toks/s, output: 132.04 toks/s]
Processed prompts:  54%|█████▍    | 1106/2048 [00:08<00:10, 88.93it/s, est. speed input: 134294.92 toks/s, output: 131.15 toks/s]
Processed prompts:  55%|█████▍    | 1122/2048 [00:08<00:10, 89.28it/s, est. speed input: 133431.00 toks/s, output: 130.30 toks/s]
Processed prompts:  56%|█████▌    | 1138/2048 [00:08<00:10, 89.47it/s, est. speed input: 132593.84 toks/s, output: 129.49 toks/s]
Processed prompts:  56%|█████▋    | 1154/2048 [00:08<00:09, 90.96it/s, est. speed input: 131924.88 toks/s, output: 128.83 toks/s]
Processed prompts:  57%|█████▋    | 1170/2048 [00:09<00:09, 90.66it/s, est. speed input: 131149.34 toks/s, output: 128.08 toks/s]
Processed prompts:  58%|█████▊    | 1186/2048 [00:09<00:09, 90.44it/s, est. speed input: 130403.22 toks/s, output: 127.35 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [00:09<00:09, 90.33it/s, est. speed input: 129688.73 toks/s, output: 126.65 toks/s]
Processed prompts:  59%|█████▉    | 1218/2048 [00:09<00:09, 90.25it/s, est. speed input: 129000.39 toks/s, output: 125.98 toks/s]
Processed prompts:  60%|██████    | 1234/2048 [00:09<00:09, 90.16it/s, est. speed input: 128333.82 toks/s, output: 125.33 toks/s]
Processed prompts:  61%|██████    | 1250/2048 [00:10<00:08, 89.96it/s, est. speed input: 127679.66 toks/s, output: 124.69 toks/s]
Processed prompts:  62%|██████▏   | 1266/2048 [00:10<00:08, 91.38it/s, est. speed input: 127174.31 toks/s, output: 124.19 toks/s]
Processed prompts:  63%|██████▎   | 1282/2048 [00:10<00:08, 91.01it/s, est. speed input: 126577.80 toks/s, output: 123.61 toks/s]
Processed prompts:  63%|██████▎   | 1298/2048 [00:10<00:08, 90.75it/s, est. speed input: 126000.99 toks/s, output: 123.05 toks/s]
Processed prompts:  64%|██████▍   | 1314/2048 [00:10<00:08, 90.41it/s, est. speed input: 125431.88 toks/s, output: 122.49 toks/s]
Processed prompts:  65%|██████▍   | 1330/2048 [00:10<00:07, 90.21it/s, est. speed input: 124883.61 toks/s, output: 121.96 toks/s]
Processed prompts:  66%|██████▌   | 1346/2048 [00:11<00:07, 89.88it/s, est. speed input: 124338.93 toks/s, output: 121.42 toks/s]
Processed prompts:  67%|██████▋   | 1362/2048 [00:11<00:07, 89.79it/s, est. speed input: 123821.51 toks/s, output: 120.92 toks/s]
Processed prompts:  67%|██████▋   | 1378/2048 [00:11<00:07, 89.84it/s, est. speed input: 123328.96 toks/s, output: 120.44 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [00:11<00:07, 89.93it/s, est. speed input: 122854.69 toks/s, output: 119.97 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [00:11<00:07, 89.96it/s, est. speed input: 122392.71 toks/s, output: 119.52 toks/s]
Processed prompts:  70%|██████▉   | 1426/2048 [00:11<00:06, 89.98it/s, est. speed input: 121944.52 toks/s, output: 119.09 toks/s]
Processed prompts:  70%|███████   | 1442/2048 [00:12<00:06, 89.96it/s, est. speed input: 121507.14 toks/s, output: 118.66 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [00:12<00:06, 89.95it/s, est. speed input: 121082.77 toks/s, output: 118.24 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [00:12<00:06, 89.92it/s, est. speed input: 120668.44 toks/s, output: 117.84 toks/s]
Processed prompts:  73%|███████▎  | 1490/2048 [00:12<00:06, 89.88it/s, est. speed input: 120265.25 toks/s, output: 117.45 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [00:12<00:06, 89.80it/s, est. speed input: 119869.34 toks/s, output: 117.06 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [00:13<00:05, 89.80it/s, est. speed input: 119488.21 toks/s, output: 116.69 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [00:13<00:05, 89.75it/s, est. speed input: 119114.04 toks/s, output: 116.32 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [00:13<00:05, 89.88it/s, est. speed input: 118759.58 toks/s, output: 115.98 toks/s]
Processed prompts:  77%|███████▋  | 1570/2048 [00:13<00:05, 90.03it/s, est. speed input: 118417.83 toks/s, output: 115.64 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [00:13<00:05, 91.59it/s, est. speed input: 118165.45 toks/s, output: 115.40 toks/s]
Processed prompts:  78%|███████▊  | 1602/2048 [00:13<00:04, 91.06it/s, est. speed input: 117830.67 toks/s, output: 115.07 toks/s]
Processed prompts:  79%|███████▉  | 1618/2048 [00:14<00:04, 90.73it/s, est. speed input: 117506.83 toks/s, output: 114.75 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [00:14<00:04, 90.66it/s, est. speed input: 117199.12 toks/s, output: 114.45 toks/s]
Processed prompts:  81%|████████  | 1650/2048 [00:14<00:04, 90.63it/s, est. speed input: 116900.12 toks/s, output: 114.16 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [00:14<00:04, 90.50it/s, est. speed input: 116602.56 toks/s, output: 113.87 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [00:14<00:04, 90.38it/s, est. speed input: 116310.51 toks/s, output: 113.58 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [00:14<00:03, 90.18it/s, est. speed input: 116019.80 toks/s, output: 113.30 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [00:15<00:03, 90.04it/s, est. speed input: 115735.75 toks/s, output: 113.02 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [00:15<00:03, 90.16it/s, est. speed input: 115468.91 toks/s, output: 112.76 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [00:15<00:03, 90.20it/s, est. speed input: 115206.22 toks/s, output: 112.51 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [00:15<00:03, 90.20it/s, est. speed input: 114948.38 toks/s, output: 112.25 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [00:15<00:02, 90.19it/s, est. speed input: 114695.27 toks/s, output: 112.01 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [00:16<00:02, 90.16it/s, est. speed input: 114446.84 toks/s, output: 111.76 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [00:16<00:02, 90.05it/s, est. speed input: 114199.92 toks/s, output: 111.52 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [00:16<00:02, 90.05it/s, est. speed input: 113962.01 toks/s, output: 111.29 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [00:16<00:02, 90.06it/s, est. speed input: 113729.21 toks/s, output: 111.06 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [00:16<00:02, 90.01it/s, est. speed input: 113499.30 toks/s, output: 110.84 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [00:16<00:01, 91.45it/s, est. speed input: 113337.78 toks/s, output: 110.68 toks/s]
Processed prompts:  92%|█████████▏| 1890/2048 [00:17<00:01, 90.99it/s, est. speed input: 113116.92 toks/s, output: 110.47 toks/s]
Processed prompts:  93%|█████████▎| 1906/2048 [00:17<00:01, 90.70it/s, est. speed input: 112901.51 toks/s, output: 110.26 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [00:17<00:01, 90.46it/s, est. speed input: 112689.24 toks/s, output: 110.05 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [00:17<00:01, 90.30it/s, est. speed input: 112481.73 toks/s, output: 109.85 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [00:17<00:01, 91.63it/s, est. speed input: 112336.66 toks/s, output: 109.70 toks/s]
Processed prompts:  96%|█████████▌| 1970/2048 [00:17<00:00, 91.09it/s, est. speed input: 112135.43 toks/s, output: 109.51 toks/s]
Processed prompts:  97%|█████████▋| 1986/2048 [00:18<00:00, 90.69it/s, est. speed input: 111937.23 toks/s, output: 109.31 toks/s]
Processed prompts:  98%|█████████▊| 2002/2048 [00:18<00:00, 90.49it/s, est. speed input: 111745.90 toks/s, output: 109.13 toks/s]
Processed prompts:  99%|█████████▊| 2018/2048 [00:18<00:00, 90.33it/s, est. speed input: 111557.54 toks/s, output: 108.94 toks/s]
Processed prompts:  99%|█████████▉| 2034/2048 [00:18<00:00, 92.05it/s, est. speed input: 111443.02 toks/s, output: 108.83 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:18<00:00, 92.05it/s, est. speed input: 112206.45 toks/s, output: 109.58 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:18<00:00, 109.58it/s, est. speed input: 112206.45 toks/s, output: 109.58 toks/s]
[rank0]:[W128 08:29:42.718659059 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 69.1s

测试结果:
  Requests/s:   89.92
  Tokens/s:     92167.77
  Total Reqs:   2048
  Elapsed:      22.78s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     92077.85

============================================================
[7/7] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 08:30:08 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3306026) WARNING 01-28 08:30:22 [backends.py:609] Failed to read file <frozen os>
Throughput: 91.82 requests/s, 94114.11 total tokens/s, 91.82 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-28 08:30:08] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:30:08] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:30:08] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:30:08] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:30:08] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:30:08] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:30:08] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:30:08] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:30:08] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:30:08] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:30:08] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:30:08] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:30:08] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:30:08] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 08:30:14] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:30:15] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:30:15] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:30:15] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:30:15] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:30:15] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:30:15] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:30:15] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:30:15] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:30:15] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:30:15] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:30:15] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:30:15] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:30:15] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3306026) [2026-01-28 08:30:16] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=3306026) [2026-01-28 08:30:16] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3306026) [2026-01-28 08:30:16] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=3306026) [2026-01-28 08:30:16] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3306026) [2026-01-28 08:30:16] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=3306026) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3306026) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.02it/s]
(EngineCore_DP0 pid=3306026) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.02it/s]
(EngineCore_DP0 pid=3306026) 
(EngineCore_DP0 pid=3306026) [rank0]:W0128 08:30:28.941000 3306026 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3306026) [rank0]:W0128 08:30:29.778000 3306026 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=3306026) [rank0]:W0128 08:30:31.522000 3306026 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3306026) [rank0]:W0128 08:30:31.652000 3306026 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3306026) 2026-01-28 08:30:36,695 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3306026) 2026-01-28 08:30:36,745 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3306026) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:00, 13.14it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:00<00:00, 14.12it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 6/11 [00:00<00:00,  9.02it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:00<00:00, 10.37it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 10/11 [00:00<00:00, 11.67it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 11.29it/s]
(EngineCore_DP0 pid=3306026) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00, 13.93it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00, 14.50it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00, 14.74it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 14.66it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 32/4096 [00:00<00:12, 317.32it/s]
Adding requests:   2%|▏         | 84/4096 [00:00<00:09, 433.75it/s]
Adding requests:   3%|▎         | 135/4096 [00:00<00:08, 466.30it/s]
Adding requests:   5%|▍         | 185/4096 [00:00<00:08, 477.66it/s]
Adding requests:   6%|▌         | 233/4096 [00:00<00:08, 463.09it/s]
Adding requests:   7%|▋         | 280/4096 [00:00<00:08, 444.84it/s]
Adding requests:   8%|▊         | 329/4096 [00:00<00:08, 457.04it/s]
Adding requests:   9%|▉         | 380/4096 [00:00<00:07, 472.73it/s]
Adding requests:  11%|█         | 432/4096 [00:00<00:07, 485.45it/s]
Adding requests:  12%|█▏        | 483/4096 [00:01<00:07, 492.45it/s]
Adding requests:  13%|█▎        | 533/4096 [00:01<00:07, 484.57it/s]
Adding requests:  14%|█▍        | 586/4096 [00:01<00:07, 497.30it/s]
Adding requests:  16%|█▌        | 638/4096 [00:01<00:06, 502.44it/s]
Adding requests:  17%|█▋        | 692/4096 [00:01<00:06, 511.21it/s]
Adding requests:  18%|█▊        | 744/4096 [00:01<00:06, 510.82it/s]
Adding requests:  19%|█▉        | 796/4096 [00:01<00:06, 507.68it/s]
Adding requests:  21%|██        | 847/4096 [00:01<00:06, 499.07it/s]
Adding requests:  22%|██▏       | 900/4096 [00:01<00:06, 506.58it/s]
Adding requests:  23%|██▎       | 951/4096 [00:01<00:06, 507.40it/s]
Adding requests:  24%|██▍       | 1003/4096 [00:02<00:06, 509.21it/s]
Adding requests:  26%|██▌       | 1055/4096 [00:02<00:05, 509.41it/s]
Adding requests:  27%|██▋       | 1106/4096 [00:02<00:05, 498.59it/s]
Adding requests:  28%|██▊       | 1158/4096 [00:02<00:05, 504.37it/s]
Adding requests:  30%|██▉       | 1212/4096 [00:02<00:05, 513.74it/s]
Adding requests:  31%|███       | 1264/4096 [00:02<00:05, 510.30it/s]
Adding requests:  32%|███▏      | 1316/4096 [00:02<00:05, 489.74it/s]
Adding requests:  33%|███▎      | 1369/4096 [00:02<00:05, 500.26it/s]
Adding requests:  35%|███▍      | 1421/4096 [00:02<00:05, 505.38it/s]
Adding requests:  36%|███▌      | 1473/4096 [00:02<00:05, 507.70it/s]
Adding requests:  37%|███▋      | 1526/4096 [00:03<00:05, 513.12it/s]
Adding requests:  39%|███▊      | 1578/4096 [00:03<00:04, 514.08it/s]
Adding requests:  40%|███▉      | 1632/4096 [00:03<00:04, 519.29it/s]
Adding requests:  41%|████      | 1684/4096 [00:03<00:04, 512.51it/s]
Adding requests:  42%|████▏     | 1736/4096 [00:03<00:04, 514.38it/s]
Adding requests:  44%|████▎     | 1788/4096 [00:03<00:04, 511.16it/s]
Adding requests:  45%|████▍     | 1840/4096 [00:03<00:04, 513.07it/s]
Adding requests:  46%|████▌     | 1892/4096 [00:03<00:04, 508.82it/s]
Adding requests:  47%|████▋     | 1943/4096 [00:03<00:04, 507.16it/s]
Adding requests:  49%|████▊     | 1994/4096 [00:04<00:04, 505.28it/s]
Adding requests:  50%|████▉     | 2045/4096 [00:04<00:04, 504.00it/s]
Adding requests:  51%|█████     | 2096/4096 [00:04<00:04, 492.31it/s]
Adding requests:  52%|█████▏    | 2146/4096 [00:04<00:04, 482.98it/s]
Adding requests:  54%|█████▎    | 2195/4096 [00:04<00:03, 478.20it/s]
Adding requests:  55%|█████▍    | 2247/4096 [00:04<00:03, 488.70it/s]
Adding requests:  56%|█████▌    | 2296/4096 [00:04<00:03, 486.69it/s]
Adding requests:  57%|█████▋    | 2348/4096 [00:04<00:03, 495.98it/s]
Adding requests:  59%|█████▊    | 2400/4096 [00:04<00:03, 502.70it/s]
Adding requests:  60%|█████▉    | 2452/4096 [00:04<00:03, 505.77it/s]
Adding requests:  61%|██████    | 2504/4096 [00:05<00:03, 508.56it/s]
Adding requests:  62%|██████▏   | 2558/4096 [00:05<00:02, 515.47it/s]
Adding requests:  64%|██████▎   | 2610/4096 [00:05<00:02, 516.14it/s]
Adding requests:  65%|██████▌   | 2664/4096 [00:05<00:02, 520.26it/s]
Adding requests:  66%|██████▋   | 2717/4096 [00:05<00:02, 513.80it/s]
Adding requests:  68%|██████▊   | 2770/4096 [00:05<00:02, 516.33it/s]
Adding requests:  69%|██████▉   | 2822/4096 [00:05<00:02, 509.02it/s]
Adding requests:  70%|███████   | 2874/4096 [00:05<00:02, 511.82it/s]
Adding requests:  71%|███████▏  | 2926/4096 [00:05<00:02, 513.31it/s]
Adding requests:  73%|███████▎  | 2978/4096 [00:05<00:02, 513.24it/s]
Adding requests:  74%|███████▍  | 3030/4096 [00:06<00:02, 512.74it/s]
Adding requests:  75%|███████▌  | 3082/4096 [00:06<00:01, 511.07it/s]
Adding requests:  77%|███████▋  | 3135/4096 [00:06<00:01, 514.91it/s]
Adding requests:  78%|███████▊  | 3187/4096 [00:06<00:01, 513.56it/s]
Adding requests:  79%|███████▉  | 3239/4096 [00:06<00:01, 513.88it/s]
Adding requests:  80%|████████  | 3291/4096 [00:06<00:01, 515.44it/s]
Adding requests:  82%|████████▏ | 3343/4096 [00:06<00:01, 515.87it/s]
Adding requests:  83%|████████▎ | 3395/4096 [00:06<00:01, 516.62it/s]
Adding requests:  84%|████████▍ | 3447/4096 [00:06<00:01, 517.60it/s]
Adding requests:  85%|████████▌ | 3499/4096 [00:06<00:01, 514.19it/s]
Adding requests:  87%|████████▋ | 3552/4096 [00:07<00:01, 516.07it/s]
Adding requests:  88%|████████▊ | 3604/4096 [00:07<00:00, 514.11it/s]
Adding requests:  89%|████████▉ | 3656/4096 [00:07<00:00, 494.18it/s]
Adding requests:  91%|█████████ | 3709/4096 [00:07<00:00, 503.36it/s]
Adding requests:  92%|█████████▏| 3761/4096 [00:07<00:00, 507.10it/s]
Adding requests:  93%|█████████▎| 3814/4096 [00:07<00:00, 512.71it/s]
Adding requests:  94%|█████████▍| 3867/4096 [00:07<00:00, 517.10it/s]
Adding requests:  96%|█████████▌| 3919/4096 [00:07<00:00, 516.39it/s]
Adding requests:  97%|█████████▋| 3971/4096 [00:07<00:00, 515.40it/s]
Adding requests:  98%|█████████▊| 4023/4096 [00:07<00:00, 516.32it/s]
Adding requests:  99%|█████████▉| 4075/4096 [00:08<00:00, 508.45it/s]
Adding requests: 100%|██████████| 4096/4096 [00:08<00:00, 503.34it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  17%|█▋        | 716/4096 [00:00<00:00, 5673.00it/s, est. speed input: 5810216.61 toks/s, output: 5673.32 toks/s]
Processed prompts:  31%|███▏      | 1284/4096 [00:06<00:15, 178.50it/s, est. speed input: 218124.17 toks/s, output: 213.01 toks/s]  
Processed prompts:  37%|███▋      | 1524/4096 [00:08<00:18, 141.65it/s, est. speed input: 177078.52 toks/s, output: 172.93 toks/s]
Processed prompts:  40%|████      | 1658/4096 [00:10<00:18, 131.77it/s, est. speed input: 166517.97 toks/s, output: 162.62 toks/s]
Processed prompts:  43%|████▎     | 1743/4096 [00:11<00:19, 122.12it/s, est. speed input: 158811.29 toks/s, output: 155.09 toks/s]
Processed prompts:  44%|████▍     | 1801/4096 [00:11<00:18, 125.85it/s, est. speed input: 159166.12 toks/s, output: 155.44 toks/s]
Processed prompts:  45%|████▌     | 1845/4096 [00:12<00:19, 113.78it/s, est. speed input: 153828.79 toks/s, output: 150.22 toks/s]
Processed prompts:  46%|████▌     | 1877/4096 [00:12<00:19, 111.52it/s, est. speed input: 152306.56 toks/s, output: 148.74 toks/s]
Processed prompts:  46%|████▋     | 1902/4096 [00:12<00:20, 105.53it/s, est. speed input: 150163.99 toks/s, output: 146.64 toks/s]
Processed prompts:  47%|████▋     | 1932/4096 [00:13<00:21, 102.70it/s, est. speed input: 148665.09 toks/s, output: 145.18 toks/s]
Processed prompts:  48%|████▊     | 1964/4096 [00:13<00:21, 100.61it/s, est. speed input: 147288.94 toks/s, output: 143.84 toks/s]
Processed prompts:  49%|████▊     | 1996/4096 [00:14<00:21, 98.57it/s, est. speed input: 145949.57 toks/s, output: 142.53 toks/s] 
Processed prompts:  50%|████▉     | 2028/4096 [00:14<00:21, 96.93it/s, est. speed input: 144687.75 toks/s, output: 141.30 toks/s]
Processed prompts:  50%|█████     | 2060/4096 [00:14<00:21, 95.70it/s, est. speed input: 143496.75 toks/s, output: 140.13 toks/s]
Processed prompts:  51%|█████     | 2092/4096 [00:15<00:21, 94.57it/s, est. speed input: 142339.93 toks/s, output: 139.00 toks/s]
Processed prompts:  52%|█████▏    | 2124/4096 [00:15<00:21, 93.90it/s, est. speed input: 141258.03 toks/s, output: 137.95 toks/s]
Processed prompts:  53%|█████▎    | 2156/4096 [00:15<00:20, 93.43it/s, est. speed input: 140226.20 toks/s, output: 136.94 toks/s]
Processed prompts:  53%|█████▎    | 2188/4096 [00:16<00:20, 92.79it/s, est. speed input: 139205.96 toks/s, output: 135.94 toks/s]
Processed prompts:  54%|█████▍    | 2220/4096 [00:16<00:19, 94.04it/s, est. speed input: 138412.85 toks/s, output: 135.17 toks/s]
Processed prompts:  55%|█████▍    | 2252/4096 [00:16<00:19, 93.38it/s, est. speed input: 137490.32 toks/s, output: 134.27 toks/s]
Processed prompts:  56%|█████▌    | 2284/4096 [00:17<00:19, 93.60it/s, est. speed input: 136673.77 toks/s, output: 133.47 toks/s]
Processed prompts:  57%|█████▋    | 2316/4096 [00:17<00:18, 93.93it/s, est. speed input: 135906.00 toks/s, output: 132.72 toks/s]
Processed prompts:  57%|█████▋    | 2348/4096 [00:17<00:18, 93.34it/s, est. speed input: 135089.84 toks/s, output: 131.92 toks/s]
Processed prompts:  58%|█████▊    | 2380/4096 [00:18<00:18, 94.37it/s, est. speed input: 134436.52 toks/s, output: 131.29 toks/s]
Processed prompts:  59%|█████▉    | 2412/4096 [00:18<00:17, 93.61it/s, est. speed input: 133675.73 toks/s, output: 130.54 toks/s]
Processed prompts:  60%|█████▉    | 2444/4096 [00:18<00:17, 93.01it/s, est. speed input: 132936.31 toks/s, output: 129.82 toks/s]
Processed prompts:  60%|██████    | 2476/4096 [00:19<00:17, 93.37it/s, est. speed input: 132290.74 toks/s, output: 129.19 toks/s]
Processed prompts:  61%|██████    | 2508/4096 [00:19<00:17, 92.84it/s, est. speed input: 131602.01 toks/s, output: 128.52 toks/s]
Processed prompts:  62%|██████▏   | 2540/4096 [00:19<00:16, 92.43it/s, est. speed input: 130934.74 toks/s, output: 127.87 toks/s]
Processed prompts:  63%|██████▎   | 2572/4096 [00:20<00:16, 92.98it/s, est. speed input: 130357.29 toks/s, output: 127.30 toks/s]
Processed prompts:  64%|██████▎   | 2604/4096 [00:20<00:16, 92.57it/s, est. speed input: 129736.05 toks/s, output: 126.70 toks/s]
Processed prompts:  64%|██████▍   | 2636/4096 [00:20<00:15, 92.30it/s, est. speed input: 129137.69 toks/s, output: 126.11 toks/s]
Processed prompts:  65%|██████▌   | 2668/4096 [00:21<00:15, 92.17it/s, est. speed input: 128562.34 toks/s, output: 125.55 toks/s]
Processed prompts:  66%|██████▌   | 2700/4096 [00:21<00:15, 92.07it/s, est. speed input: 128005.71 toks/s, output: 125.01 toks/s]
Processed prompts:  67%|██████▋   | 2732/4096 [00:21<00:14, 91.76it/s, est. speed input: 127448.95 toks/s, output: 124.46 toks/s]
Processed prompts:  67%|██████▋   | 2764/4096 [00:22<00:14, 91.80it/s, est. speed input: 126927.84 toks/s, output: 123.95 toks/s]
Processed prompts:  68%|██████▊   | 2796/4096 [00:22<00:14, 91.73it/s, est. speed input: 126416.40 toks/s, output: 123.45 toks/s]
Processed prompts:  69%|██████▉   | 2828/4096 [00:22<00:13, 91.64it/s, est. speed input: 125917.58 toks/s, output: 122.97 toks/s]
Processed prompts:  70%|██████▉   | 2860/4096 [00:23<00:13, 91.66it/s, est. speed input: 125439.03 toks/s, output: 122.50 toks/s]
Processed prompts:  71%|███████   | 2892/4096 [00:23<00:13, 91.63it/s, est. speed input: 124972.02 toks/s, output: 122.04 toks/s]
Processed prompts:  71%|███████▏  | 2924/4096 [00:24<00:12, 91.58it/s, est. speed input: 124516.20 toks/s, output: 121.60 toks/s]
Processed prompts:  72%|███████▏  | 2956/4096 [00:24<00:12, 91.57it/s, est. speed input: 124075.39 toks/s, output: 121.17 toks/s]
Processed prompts:  73%|███████▎  | 2988/4096 [00:24<00:12, 91.58it/s, est. speed input: 123647.84 toks/s, output: 120.75 toks/s]
Processed prompts:  74%|███████▎  | 3020/4096 [00:25<00:11, 91.60it/s, est. speed input: 123233.37 toks/s, output: 120.34 toks/s]
Processed prompts:  75%|███████▍  | 3052/4096 [00:25<00:11, 91.63it/s, est. speed input: 122830.73 toks/s, output: 119.95 toks/s]
Processed prompts:  75%|███████▌  | 3084/4096 [00:25<00:11, 91.59it/s, est. speed input: 122436.02 toks/s, output: 119.57 toks/s]
Processed prompts:  76%|███████▌  | 3116/4096 [00:26<00:10, 92.37it/s, est. speed input: 122098.70 toks/s, output: 119.24 toks/s]
Processed prompts:  77%|███████▋  | 3148/4096 [00:26<00:10, 92.14it/s, est. speed input: 121725.56 toks/s, output: 118.87 toks/s]
Processed prompts:  78%|███████▊  | 3180/4096 [00:26<00:09, 91.83it/s, est. speed input: 121353.84 toks/s, output: 118.51 toks/s]
Processed prompts:  78%|███████▊  | 3212/4096 [00:27<00:09, 91.77it/s, est. speed input: 120999.97 toks/s, output: 118.16 toks/s]
Processed prompts:  79%|███████▉  | 3244/4096 [00:27<00:09, 91.68it/s, est. speed input: 120652.80 toks/s, output: 117.82 toks/s]
Processed prompts:  80%|███████▉  | 3276/4096 [00:27<00:08, 91.61it/s, est. speed input: 120313.89 toks/s, output: 117.49 toks/s]
Processed prompts:  81%|████████  | 3308/4096 [00:28<00:08, 91.60it/s, est. speed input: 119985.39 toks/s, output: 117.17 toks/s]
Processed prompts:  82%|████████▏ | 3340/4096 [00:28<00:08, 91.69it/s, est. speed input: 119670.03 toks/s, output: 116.87 toks/s]
Processed prompts:  82%|████████▏ | 3372/4096 [00:28<00:07, 91.57it/s, est. speed input: 119352.48 toks/s, output: 116.56 toks/s]
Processed prompts:  83%|████████▎ | 3404/4096 [00:29<00:07, 91.71it/s, est. speed input: 119054.43 toks/s, output: 116.26 toks/s]
Processed prompts:  84%|████████▍ | 3436/4096 [00:29<00:07, 91.62it/s, est. speed input: 118753.50 toks/s, output: 115.97 toks/s]
Processed prompts:  85%|████████▍ | 3468/4096 [00:29<00:06, 92.25it/s, est. speed input: 118494.82 toks/s, output: 115.72 toks/s]
Processed prompts:  85%|████████▌ | 3500/4096 [00:30<00:06, 92.89it/s, est. speed input: 118250.91 toks/s, output: 115.48 toks/s]
Processed prompts:  86%|████████▌ | 3532/4096 [00:30<00:06, 92.54it/s, est. speed input: 117974.25 toks/s, output: 115.21 toks/s]
Processed prompts:  87%|████████▋ | 3564/4096 [00:31<00:05, 92.15it/s, est. speed input: 117696.70 toks/s, output: 114.94 toks/s]
Processed prompts:  88%|████████▊ | 3596/4096 [00:31<00:05, 92.06it/s, est. speed input: 117434.06 toks/s, output: 114.68 toks/s]
Processed prompts:  89%|████████▊ | 3628/4096 [00:31<00:05, 91.94it/s, est. speed input: 117174.39 toks/s, output: 114.43 toks/s]
Processed prompts:  89%|████████▉ | 3660/4096 [00:32<00:04, 91.71it/s, est. speed input: 116913.90 toks/s, output: 114.17 toks/s]
Processed prompts:  90%|█████████ | 3692/4096 [00:32<00:04, 91.71it/s, est. speed input: 116666.06 toks/s, output: 113.93 toks/s]
Processed prompts:  91%|█████████ | 3724/4096 [00:32<00:04, 92.39it/s, est. speed input: 116453.94 toks/s, output: 113.72 toks/s]
Processed prompts:  92%|█████████▏| 3756/4096 [00:33<00:03, 91.97it/s, est. speed input: 116206.97 toks/s, output: 113.48 toks/s]
Processed prompts:  92%|█████████▏| 3788/4096 [00:33<00:03, 91.86it/s, est. speed input: 115973.09 toks/s, output: 113.25 toks/s]
Processed prompts:  93%|█████████▎| 3820/4096 [00:33<00:03, 91.75it/s, est. speed input: 115742.26 toks/s, output: 113.03 toks/s]
Processed prompts:  94%|█████████▍| 3852/4096 [00:34<00:02, 91.63it/s, est. speed input: 115514.83 toks/s, output: 112.81 toks/s]
Processed prompts:  95%|█████████▍| 3884/4096 [00:34<00:02, 91.66it/s, est. speed input: 115296.17 toks/s, output: 112.59 toks/s]
Processed prompts:  96%|█████████▌| 3916/4096 [00:34<00:01, 91.51it/s, est. speed input: 115075.12 toks/s, output: 112.38 toks/s]
Processed prompts:  96%|█████████▋| 3948/4096 [00:35<00:01, 91.39it/s, est. speed input: 114858.05 toks/s, output: 112.17 toks/s]
Processed prompts:  97%|█████████▋| 3980/4096 [00:35<00:01, 91.40it/s, est. speed input: 114648.91 toks/s, output: 111.96 toks/s]
Processed prompts:  98%|█████████▊| 4012/4096 [00:35<00:00, 92.11it/s, est. speed input: 114472.19 toks/s, output: 111.79 toks/s]
Processed prompts:  99%|█████████▊| 4044/4096 [00:36<00:00, 92.00it/s, est. speed input: 114274.77 toks/s, output: 111.60 toks/s]
Processed prompts: 100%|█████████▉| 4076/4096 [00:36<00:00, 102.38it/s, est. speed input: 114451.63 toks/s, output: 111.77 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:36<00:00, 102.38it/s, est. speed input: 115011.01 toks/s, output: 112.32 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:36<00:00, 112.31it/s, est. speed input: 115011.01 toks/s, output: 112.32 toks/s]
[rank0]:[W128 08:31:24.899474141 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 102.5s

测试结果:
  Requests/s:   91.82
  Tokens/s:     94114.11
  Total Reqs:   4096
  Elapsed:      44.61s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     94022.29


------------------------------------------------------------
  生成 CSV: BitNet-2B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_INT8_py312_cu129_x86_64/cublaslt/BitNet-2B-INT8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,29.1330,14945.2099,4.3936
1024,1024,1,128,128,29.8200,30565.4943,4.2924
2048,1024,2,256,128,55.3160,56698.9129,4.6280
4096,1024,4,512,128,77.3585,79292.4677,6.6185
8192,1024,8,1024,128,86.5848,88749.4698,11.8265
16384,1024,16,2048,128,89.9198,92167.7654,22.7759
32768,1024,32,4096,128,91.8186,94114.1074,44.6097

------------------------------------------------------------

[INFO] 完成: 7 成功, 0 失败

============================================================
  BitNet-2B-INT8 | cuSPARSELt (2_4) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_4
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_INT8_py312_cu129_x86_64/cusparselt/2_4

============================================================
[1/7] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 08:31:33 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3307824) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3307824) WARNING 01-28 08:31:48 [backends.py:609] Failed to read file <frozen os>
Throughput: 31.96 requests/s, 16394.44 total tokens/s, 31.96 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-28 08:31:33] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:31:33] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:31:33] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:31:33] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:31:33] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:31:33] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:31:33] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:31:33] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:31:33] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:31:33] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:31:33] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:31:33] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:31:33] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:31:33] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 08:31:40] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:31:40] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:31:40] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:31:40] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:31:40] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:31:40] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:31:40] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:31:40] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:31:40] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:31:40] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:31:40] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:31:40] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:31:40] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:31:40] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3307824) [2026-01-28 08:31:41] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3307824) [2026-01-28 08:31:41] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3307824) [2026-01-28 08:31:41] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3307824) [2026-01-28 08:31:41] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3307824) [2026-01-28 08:31:41] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3307824) [2026-01-28 08:31:41] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3307824) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3307824) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.13it/s]
(EngineCore_DP0 pid=3307824) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.12it/s]
(EngineCore_DP0 pid=3307824) 
(EngineCore_DP0 pid=3307824) [2026-01-28 08:31:42] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3307824) [2026-01-28 08:31:42] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6144000 bytes
(EngineCore_DP0 pid=3307824) [2026-01-28 08:31:42] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3307824) [2026-01-28 08:31:42] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4096000 bytes
(EngineCore_DP0 pid=3307824) [2026-01-28 08:31:42] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3307824) [2026-01-28 08:31:42] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 22118400 bytes
(EngineCore_DP0 pid=3307824) [2026-01-28 08:31:42] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3307824) [2026-01-28 08:31:42] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11059200 bytes
(EngineCore_DP0 pid=3307824) 2026-01-28 08:32:00,437 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3307824) 2026-01-28 08:32:00,471 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3307824) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  4.34it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.60it/s]
(EngineCore_DP0 pid=3307824) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 15.71it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  41%|████▏     | 53/128 [00:00<00:00, 529.89it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 701.90it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:28,  4.52it/s, est. speed input: 2315.17 toks/s, output: 4.52 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:06, 17.83it/s, est. speed input: 7760.78 toks/s, output: 15.16 toks/s]
Processed prompts:   6%|▋         | 8/128 [00:00<00:05, 21.91it/s, est. speed input: 9469.60 toks/s, output: 18.49 toks/s]
Processed prompts:   9%|▉         | 12/128 [00:00<00:04, 26.91it/s, est. speed input: 11298.49 toks/s, output: 22.07 toks/s]
Processed prompts:  12%|█▎        | 16/128 [00:00<00:03, 29.95it/s, est. speed input: 12503.26 toks/s, output: 24.42 toks/s]
Processed prompts:  16%|█▌        | 20/128 [00:00<00:03, 31.81it/s, est. speed input: 13343.26 toks/s, output: 26.06 toks/s]
Processed prompts:  19%|█▉        | 24/128 [00:00<00:03, 33.07it/s, est. speed input: 13976.68 toks/s, output: 27.30 toks/s]
Processed prompts:  22%|██▏       | 28/128 [00:00<00:02, 33.87it/s, est. speed input: 14459.67 toks/s, output: 28.24 toks/s]
Processed prompts:  25%|██▌       | 32/128 [00:01<00:02, 34.44it/s, est. speed input: 14847.71 toks/s, output: 29.00 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:01<00:02, 34.66it/s, est. speed input: 15142.02 toks/s, output: 29.57 toks/s]
Processed prompts:  31%|███▏      | 40/128 [00:01<00:02, 34.84it/s, est. speed input: 15389.85 toks/s, output: 30.06 toks/s]
Processed prompts:  34%|███▍      | 44/128 [00:01<00:02, 35.04it/s, est. speed input: 15607.12 toks/s, output: 30.48 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:01<00:02, 35.17it/s, est. speed input: 15792.43 toks/s, output: 30.84 toks/s]
Processed prompts:  41%|████      | 52/128 [00:01<00:02, 35.17it/s, est. speed input: 15943.69 toks/s, output: 31.14 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:01<00:02, 35.24it/s, est. speed input: 16081.24 toks/s, output: 31.41 toks/s]
Processed prompts:  47%|████▋     | 60/128 [00:01<00:01, 35.27it/s, est. speed input: 16201.90 toks/s, output: 31.64 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:02<00:01, 35.26it/s, est. speed input: 16305.64 toks/s, output: 31.85 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:02<00:01, 35.23it/s, est. speed input: 16396.50 toks/s, output: 32.02 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:02<00:01, 35.27it/s, est. speed input: 16483.07 toks/s, output: 32.19 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:02<00:01, 35.26it/s, est. speed input: 16558.01 toks/s, output: 32.34 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:02<00:01, 35.28it/s, est. speed input: 16628.24 toks/s, output: 32.48 toks/s]
Processed prompts:  66%|██████▌   | 84/128 [00:02<00:01, 35.27it/s, est. speed input: 16690.85 toks/s, output: 32.60 toks/s]
Processed prompts:  69%|██████▉   | 88/128 [00:02<00:01, 35.29it/s, est. speed input: 16749.83 toks/s, output: 32.71 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:02<00:01, 35.32it/s, est. speed input: 16805.16 toks/s, output: 32.82 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:02<00:00, 35.33it/s, est. speed input: 16855.13 toks/s, output: 32.92 toks/s]
Processed prompts:  78%|███████▊  | 100/128 [00:03<00:00, 35.33it/s, est. speed input: 16901.31 toks/s, output: 33.01 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:03<00:00, 35.35it/s, est. speed input: 16945.28 toks/s, output: 33.10 toks/s]
Processed prompts:  84%|████████▍ | 108/128 [00:03<00:00, 35.35it/s, est. speed input: 16985.61 toks/s, output: 33.17 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:03<00:00, 35.33it/s, est. speed input: 17021.58 toks/s, output: 33.25 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:03<00:00, 35.31it/s, est. speed input: 17055.31 toks/s, output: 33.31 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:03<00:00, 35.32it/s, est. speed input: 17088.20 toks/s, output: 33.38 toks/s]
Processed prompts:  97%|█████████▋| 124/128 [00:03<00:00, 35.35it/s, est. speed input: 17120.12 toks/s, output: 33.44 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 35.40it/s, est. speed input: 17151.36 toks/s, output: 33.50 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 35.40it/s, est. speed input: 17151.36 toks/s, output: 33.50 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 33.50it/s, est. speed input: 17151.36 toks/s, output: 33.50 toks/s]
[rank0]:[W128 08:32:06.910943964 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 41.7s

测试结果:
  Requests/s:   31.96
  Tokens/s:     16394.44
  Total Reqs:   128
  Elapsed:      4.01s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     16362.48

============================================================
[2/7] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 08:32:16 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3309002) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3309002) WARNING 01-28 08:32:31 [backends.py:609] Failed to read file <frozen os>
Throughput: 32.84 requests/s, 33663.08 total tokens/s, 32.84 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-28 08:32:15] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:32:16] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:32:16] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:32:16] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:32:16] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:32:16] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:32:16] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:32:16] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:32:16] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:32:16] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:32:16] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:32:16] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:32:16] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:32:16] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 08:32:22] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:32:23] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:32:23] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:32:23] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:32:23] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:32:23] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:32:23] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:32:23] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:32:23] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:32:23] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:32:23] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:32:23] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:32:23] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:32:23] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3309002) [2026-01-28 08:32:24] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3309002) [2026-01-28 08:32:24] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3309002) [2026-01-28 08:32:24] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3309002) [2026-01-28 08:32:24] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3309002) [2026-01-28 08:32:24] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3309002) [2026-01-28 08:32:24] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3309002) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3309002) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.07it/s]
(EngineCore_DP0 pid=3309002) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.07it/s]
(EngineCore_DP0 pid=3309002) 
(EngineCore_DP0 pid=3309002) [2026-01-28 08:32:25] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3309002) [2026-01-28 08:32:25] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6144000 bytes
(EngineCore_DP0 pid=3309002) [2026-01-28 08:32:25] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3309002) [2026-01-28 08:32:25] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4096000 bytes
(EngineCore_DP0 pid=3309002) [2026-01-28 08:32:25] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3309002) [2026-01-28 08:32:25] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 22118400 bytes
(EngineCore_DP0 pid=3309002) [2026-01-28 08:32:25] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3309002) [2026-01-28 08:32:25] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11059200 bytes
(EngineCore_DP0 pid=3309002) 2026-01-28 08:32:43,424 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3309002) 2026-01-28 08:32:43,479 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3309002) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 12.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 12.82it/s]
(EngineCore_DP0 pid=3309002) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 15.79it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  39%|███▉      | 50/128 [00:00<00:00, 498.32it/s]
Adding requests:  79%|███████▉  | 101/128 [00:00<00:00, 504.13it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 504.63it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 3/128 [00:00<00:04, 26.84it/s, est. speed input: 27485.86 toks/s, output: 26.84 toks/s]
Processed prompts:   5%|▌         | 7/128 [00:00<00:03, 32.33it/s, est. speed input: 32267.51 toks/s, output: 31.51 toks/s]
Processed prompts:   9%|▊         | 11/128 [00:00<00:03, 34.03it/s, est. speed input: 33816.05 toks/s, output: 33.02 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:00<00:03, 34.56it/s, est. speed input: 34422.98 toks/s, output: 33.61 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:00<00:03, 34.77it/s, est. speed input: 34737.09 toks/s, output: 33.92 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:00<00:03, 33.91it/s, est. speed input: 34451.67 toks/s, output: 33.64 toks/s]
Processed prompts:  21%|██        | 27/128 [00:00<00:02, 34.45it/s, est. speed input: 34737.52 toks/s, output: 33.92 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:00<00:02, 34.82it/s, est. speed input: 34953.58 toks/s, output: 34.13 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:01<00:02, 35.07it/s, est. speed input: 35122.95 toks/s, output: 34.30 toks/s]
Processed prompts:  30%|███       | 39/128 [00:01<00:02, 35.24it/s, est. speed input: 35256.85 toks/s, output: 34.43 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:01<00:02, 35.32it/s, est. speed input: 35355.77 toks/s, output: 34.53 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:01<00:02, 35.60it/s, est. speed input: 35499.65 toks/s, output: 34.67 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:01<00:02, 35.93it/s, est. speed input: 35654.89 toks/s, output: 34.82 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:01<00:02, 36.13it/s, est. speed input: 35782.26 toks/s, output: 34.94 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:01<00:01, 36.30it/s, est. speed input: 35899.36 toks/s, output: 35.06 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:01<00:01, 36.45it/s, est. speed input: 36006.71 toks/s, output: 35.16 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:01<00:01, 36.53it/s, est. speed input: 36098.25 toks/s, output: 35.25 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:02<00:01, 35.95it/s, est. speed input: 36064.59 toks/s, output: 35.22 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:02<00:01, 35.64it/s, est. speed input: 36049.30 toks/s, output: 35.20 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:02<00:01, 35.45it/s, est. speed input: 36039.52 toks/s, output: 35.19 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:02<00:01, 35.34it/s, est. speed input: 36034.15 toks/s, output: 35.19 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:02<00:01, 35.18it/s, est. speed input: 36015.24 toks/s, output: 35.17 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:02<00:01, 35.07it/s, est. speed input: 35998.87 toks/s, output: 35.15 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:02<00:00, 35.02it/s, est. speed input: 35988.32 toks/s, output: 35.14 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:02<00:00, 35.01it/s, est. speed input: 35981.77 toks/s, output: 35.14 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:02<00:00, 35.00it/s, est. speed input: 35975.93 toks/s, output: 35.13 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:03<00:00, 35.06it/s, est. speed input: 35978.19 toks/s, output: 35.13 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:03<00:00, 35.13it/s, est. speed input: 35983.83 toks/s, output: 35.14 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:03<00:00, 35.16it/s, est. speed input: 35986.58 toks/s, output: 35.14 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:03<00:00, 35.13it/s, est. speed input: 35984.67 toks/s, output: 35.14 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:03<00:00, 35.15it/s, est. speed input: 35985.98 toks/s, output: 35.14 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:03<00:00, 35.14it/s, est. speed input: 35985.10 toks/s, output: 35.14 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 35.14it/s, est. speed input: 35985.90 toks/s, output: 35.14 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 35.14it/s, est. speed input: 35985.90 toks/s, output: 35.14 toks/s]
[rank0]:[W128 08:32:49.485321886 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 42.5s

测试结果:
  Requests/s:   32.84
  Tokens/s:     33663.08
  Total Reqs:   128
  Elapsed:      3.90s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     33630.24

============================================================
[3/7] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 08:32:59 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3310122) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3310122) WARNING 01-28 08:33:13 [backends.py:609] Failed to read file <frozen os>
Throughput: 65.25 requests/s, 66879.27 total tokens/s, 65.25 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-28 08:32:58] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:32:58] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:32:58] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:32:58] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:32:58] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:32:58] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:32:58] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:32:58] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:32:58] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:32:58] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:32:58] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:32:58] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:32:59] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:32:59] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 08:33:05] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:33:05] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:33:05] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:33:05] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:33:05] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:33:05] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:33:05] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:33:05] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:33:05] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:33:05] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:33:05] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:33:05] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:33:05] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:33:05] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3310122) [2026-01-28 08:33:06] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3310122) [2026-01-28 08:33:06] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3310122) [2026-01-28 08:33:06] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3310122) [2026-01-28 08:33:06] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3310122) [2026-01-28 08:33:06] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3310122) [2026-01-28 08:33:06] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3310122) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3310122) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.12it/s]
(EngineCore_DP0 pid=3310122) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.12it/s]
(EngineCore_DP0 pid=3310122) 
(EngineCore_DP0 pid=3310122) [2026-01-28 08:33:07] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3310122) [2026-01-28 08:33:07] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6144000 bytes
(EngineCore_DP0 pid=3310122) [2026-01-28 08:33:07] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3310122) [2026-01-28 08:33:07] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4096000 bytes
(EngineCore_DP0 pid=3310122) [2026-01-28 08:33:07] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3310122) [2026-01-28 08:33:07] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 22118400 bytes
(EngineCore_DP0 pid=3310122) [2026-01-28 08:33:07] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3310122) [2026-01-28 08:33:07] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11059200 bytes
(EngineCore_DP0 pid=3310122) 2026-01-28 08:33:24,736 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3310122) 2026-01-28 08:33:24,762 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3310122) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 1/3 [00:00<00:00,  8.03it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 12.17it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 11.57it/s]
(EngineCore_DP0 pid=3310122) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 16.99it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 16.97it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  15%|█▍        | 38/256 [00:00<00:00, 372.65it/s]
Adding requests:  35%|███▍      | 89/256 [00:00<00:00, 449.29it/s]
Adding requests:  54%|█████▍    | 138/256 [00:00<00:00, 465.90it/s]
Adding requests:  73%|███████▎  | 186/256 [00:00<00:00, 470.02it/s]
Adding requests:  91%|█████████▏| 234/256 [00:00<00:00, 472.61it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 464.55it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   9%|▊         | 22/256 [00:00<00:01, 204.55it/s, est. speed input: 209494.34 toks/s, output: 204.56 toks/s]
Processed prompts:  17%|█▋        | 43/256 [00:00<00:02, 103.78it/s, est. speed input: 114971.81 toks/s, output: 112.27 toks/s]
Processed prompts:  22%|██▏       | 56/256 [00:00<00:02, 87.81it/s, est. speed input: 99617.45 toks/s, output: 97.28 toks/s]   
Processed prompts:  26%|██▌       | 67/256 [00:00<00:02, 85.45it/s, est. speed input: 96237.30 toks/s, output: 93.98 toks/s]
Processed prompts:  30%|███       | 77/256 [00:00<00:02, 81.61it/s, est. speed input: 92755.94 toks/s, output: 90.58 toks/s]
Processed prompts:  34%|███▎      | 86/256 [00:00<00:02, 76.79it/s, est. speed input: 89233.50 toks/s, output: 87.14 toks/s]
Processed prompts:  37%|███▋      | 94/256 [00:01<00:02, 75.77it/s, est. speed input: 87766.53 toks/s, output: 85.71 toks/s]
Processed prompts:  40%|███▉      | 102/256 [00:01<00:02, 74.92it/s, est. speed input: 86541.50 toks/s, output: 84.51 toks/s]
Processed prompts:  43%|████▎     | 110/256 [00:01<00:01, 74.29it/s, est. speed input: 85527.01 toks/s, output: 83.52 toks/s]
Processed prompts:  46%|████▌     | 118/256 [00:01<00:01, 73.90it/s, est. speed input: 84693.47 toks/s, output: 82.71 toks/s]
Processed prompts:  49%|████▉     | 126/256 [00:01<00:01, 73.54it/s, est. speed input: 83955.93 toks/s, output: 81.99 toks/s]
Processed prompts:  52%|█████▏    | 134/256 [00:01<00:01, 73.36it/s, est. speed input: 83337.44 toks/s, output: 81.38 toks/s]
Processed prompts:  55%|█████▌    | 142/256 [00:01<00:01, 73.14it/s, est. speed input: 82772.50 toks/s, output: 80.83 toks/s]
Processed prompts:  59%|█████▊    | 150/256 [00:01<00:01, 72.91it/s, est. speed input: 82260.38 toks/s, output: 80.33 toks/s]
Processed prompts:  62%|██████▏   | 158/256 [00:01<00:01, 72.92it/s, est. speed input: 81840.18 toks/s, output: 79.92 toks/s]
Processed prompts:  65%|██████▍   | 166/256 [00:02<00:01, 72.72it/s, est. speed input: 81424.12 toks/s, output: 79.51 toks/s]
Processed prompts:  68%|██████▊   | 174/256 [00:02<00:01, 72.50it/s, est. speed input: 81033.33 toks/s, output: 79.13 toks/s]
Processed prompts:  71%|███████   | 182/256 [00:02<00:01, 72.48it/s, est. speed input: 80704.61 toks/s, output: 78.81 toks/s]
Processed prompts:  74%|███████▍  | 190/256 [00:02<00:00, 72.46it/s, est. speed input: 80405.35 toks/s, output: 78.52 toks/s]
Processed prompts:  77%|███████▋  | 198/256 [00:02<00:00, 72.39it/s, est. speed input: 80124.69 toks/s, output: 78.25 toks/s]
Processed prompts:  80%|████████  | 206/256 [00:02<00:00, 69.03it/s, est. speed input: 79332.82 toks/s, output: 77.47 toks/s]
Processed prompts:  84%|████████▎ | 214/256 [00:02<00:00, 69.44it/s, est. speed input: 79037.74 toks/s, output: 77.18 toks/s]
Processed prompts:  87%|████████▋ | 222/256 [00:02<00:00, 69.71it/s, est. speed input: 78762.23 toks/s, output: 76.92 toks/s]
Processed prompts:  90%|████████▉ | 230/256 [00:03<00:00, 69.88it/s, est. speed input: 78504.56 toks/s, output: 76.66 toks/s]
Processed prompts:  93%|█████████▎| 238/256 [00:03<00:00, 69.98it/s, est. speed input: 78262.64 toks/s, output: 76.43 toks/s]
Processed prompts:  96%|█████████▌| 246/256 [00:03<00:00, 70.01it/s, est. speed input: 78032.84 toks/s, output: 76.20 toks/s]
Processed prompts:  99%|█████████▉| 254/256 [00:03<00:00, 69.98it/s, est. speed input: 77812.72 toks/s, output: 75.99 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 69.98it/s, est. speed input: 77770.83 toks/s, output: 75.95 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 75.94it/s, est. speed input: 77770.83 toks/s, output: 75.95 toks/s]
[rank0]:[W128 08:33:30.855445890 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 41.4s

测试结果:
  Requests/s:   65.25
  Tokens/s:     66879.27
  Total Reqs:   256
  Elapsed:      3.92s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     66814.02

============================================================
[4/7] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 08:33:41 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3311243) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3311243) WARNING 01-28 08:33:56 [backends.py:609] Failed to read file <frozen os>
Throughput: 93.19 requests/s, 95519.49 total tokens/s, 93.19 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-28 08:33:41] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:33:41] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:33:41] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:33:41] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:33:41] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:33:41] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:33:41] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:33:41] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:33:41] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:33:41] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:33:41] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:33:41] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:33:41] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:33:41] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 08:33:48] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:33:48] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:33:48] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:33:48] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:33:48] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:33:48] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:33:48] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:33:48] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:33:48] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:33:48] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:33:48] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:33:48] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:33:48] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:33:48] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3311243) [2026-01-28 08:33:49] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3311243) [2026-01-28 08:33:49] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3311243) [2026-01-28 08:33:49] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3311243) [2026-01-28 08:33:49] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3311243) [2026-01-28 08:33:49] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3311243) [2026-01-28 08:33:49] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3311243) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3311243) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.11it/s]
(EngineCore_DP0 pid=3311243) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.11it/s]
(EngineCore_DP0 pid=3311243) 
(EngineCore_DP0 pid=3311243) [2026-01-28 08:33:50] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3311243) [2026-01-28 08:33:50] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6144000 bytes
(EngineCore_DP0 pid=3311243) [2026-01-28 08:33:50] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3311243) [2026-01-28 08:33:50] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4096000 bytes
(EngineCore_DP0 pid=3311243) [2026-01-28 08:33:50] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3311243) [2026-01-28 08:33:50] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 22118400 bytes
(EngineCore_DP0 pid=3311243) [2026-01-28 08:33:50] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3311243) [2026-01-28 08:33:50] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11059200 bytes
(EngineCore_DP0 pid=3311243) 2026-01-28 08:34:07,813 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3311243) 2026-01-28 08:34:07,840 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3311243) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00, 13.01it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 10.36it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 10.68it/s]
(EngineCore_DP0 pid=3311243) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00, 15.94it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 16.34it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   4%|▍         | 21/512 [00:00<00:02, 208.29it/s]
Adding requests:  13%|█▎        | 68/512 [00:00<00:01, 357.22it/s]
Adding requests:  23%|██▎       | 118/512 [00:00<00:00, 419.39it/s]
Adding requests:  33%|███▎      | 167/512 [00:00<00:00, 442.57it/s]
Adding requests:  42%|████▏     | 215/512 [00:00<00:00, 453.72it/s]
Adding requests:  52%|█████▏    | 267/512 [00:00<00:00, 472.78it/s]
Adding requests:  62%|██████▏   | 315/512 [00:00<00:00, 472.90it/s]
Adding requests:  72%|███████▏  | 367/512 [00:00<00:00, 485.62it/s]
Adding requests:  82%|████████▏ | 418/512 [00:00<00:00, 492.93it/s]
Adding requests:  92%|█████████▏| 469/512 [00:01<00:00, 497.08it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 465.18it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  13%|█▎        | 66/512 [00:00<00:00, 507.33it/s, est. speed input: 519589.64 toks/s, output: 507.35 toks/s]
Processed prompts:  23%|██▎       | 117/512 [00:00<00:02, 175.96it/s, est. speed input: 202578.59 toks/s, output: 197.83 toks/s]
Processed prompts:  28%|██▊       | 145/512 [00:00<00:02, 146.44it/s, est. speed input: 172540.83 toks/s, output: 168.49 toks/s]
Processed prompts:  32%|███▏      | 165/512 [00:01<00:02, 133.77it/s, est. speed input: 160372.44 toks/s, output: 156.61 toks/s]
Processed prompts:  35%|███▌      | 181/512 [00:01<00:02, 126.09it/s, est. speed input: 153396.14 toks/s, output: 149.80 toks/s]
Processed prompts:  38%|███▊      | 195/512 [00:01<00:02, 116.56it/s, est. speed input: 146462.13 toks/s, output: 143.03 toks/s]
Processed prompts:  41%|████      | 208/512 [00:01<00:02, 115.81it/s, est. speed input: 144085.93 toks/s, output: 140.71 toks/s]
Processed prompts:  43%|████▎     | 220/512 [00:01<00:02, 112.91it/s, est. speed input: 141336.85 toks/s, output: 138.02 toks/s]
Processed prompts:  45%|████▌     | 232/512 [00:01<00:02, 110.63it/s, est. speed input: 138980.56 toks/s, output: 135.72 toks/s]
Processed prompts:  48%|████▊     | 244/512 [00:01<00:02, 108.79it/s, est. speed input: 136905.62 toks/s, output: 133.70 toks/s]
Processed prompts:  50%|████▉     | 255/512 [00:01<00:02, 105.22it/s, est. speed input: 134608.97 toks/s, output: 131.45 toks/s]
Processed prompts:  52%|█████▏    | 266/512 [00:02<00:02, 102.48it/s, est. speed input: 132548.51 toks/s, output: 129.44 toks/s]
Processed prompts:  54%|█████▍    | 278/512 [00:02<00:02, 102.93it/s, est. speed input: 131167.81 toks/s, output: 128.09 toks/s]
Processed prompts:  57%|█████▋    | 290/512 [00:02<00:02, 103.08it/s, est. speed input: 129887.28 toks/s, output: 126.84 toks/s]
Processed prompts:  59%|█████▉    | 302/512 [00:02<00:02, 103.30it/s, est. speed input: 128752.69 toks/s, output: 125.73 toks/s]
Processed prompts:  61%|██████▏   | 314/512 [00:02<00:01, 103.48it/s, est. speed input: 127727.52 toks/s, output: 124.73 toks/s]
Processed prompts:  64%|██████▎   | 326/512 [00:02<00:01, 103.58it/s, est. speed input: 126786.99 toks/s, output: 123.81 toks/s]
Processed prompts:  66%|██████▌   | 338/512 [00:02<00:01, 103.31it/s, est. speed input: 125867.30 toks/s, output: 122.92 toks/s]
Processed prompts:  68%|██████▊   | 350/512 [00:02<00:01, 104.72it/s, est. speed input: 125281.67 toks/s, output: 122.34 toks/s]
Processed prompts:  71%|███████   | 362/512 [00:02<00:01, 104.58it/s, est. speed input: 124565.05 toks/s, output: 121.64 toks/s]
Processed prompts:  73%|███████▎  | 374/512 [00:03<00:01, 104.31it/s, est. speed input: 123876.87 toks/s, output: 120.97 toks/s]
Processed prompts:  75%|███████▌  | 386/512 [00:03<00:01, 104.16it/s, est. speed input: 123242.90 toks/s, output: 120.35 toks/s]
Processed prompts:  78%|███████▊  | 398/512 [00:03<00:01, 104.07it/s, est. speed input: 122655.46 toks/s, output: 119.78 toks/s]
Processed prompts:  80%|████████  | 410/512 [00:03<00:00, 104.09it/s, est. speed input: 122118.54 toks/s, output: 119.26 toks/s]
Processed prompts:  82%|████████▏ | 422/512 [00:03<00:00, 104.07it/s, est. speed input: 121611.69 toks/s, output: 118.76 toks/s]
Processed prompts:  85%|████████▍ | 434/512 [00:03<00:00, 103.99it/s, est. speed input: 121129.17 toks/s, output: 118.29 toks/s]
Processed prompts:  87%|████████▋ | 446/512 [00:03<00:00, 103.82it/s, est. speed input: 120662.56 toks/s, output: 117.83 toks/s]
Processed prompts:  89%|████████▉ | 458/512 [00:03<00:00, 105.08it/s, est. speed input: 120380.52 toks/s, output: 117.56 toks/s]
Processed prompts:  92%|█████████▏| 470/512 [00:04<00:00, 104.74it/s, est. speed input: 119978.71 toks/s, output: 117.17 toks/s]
Processed prompts:  94%|█████████▍| 482/512 [00:04<00:00, 104.41it/s, est. speed input: 119590.98 toks/s, output: 116.79 toks/s]
Processed prompts:  96%|█████████▋| 494/512 [00:04<00:00, 104.37it/s, est. speed input: 119242.83 toks/s, output: 116.45 toks/s]
Processed prompts:  99%|█████████▉| 506/512 [00:04<00:00, 104.09it/s, est. speed input: 118888.15 toks/s, output: 116.10 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:04<00:00, 104.09it/s, est. speed input: 119388.72 toks/s, output: 116.59 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:04<00:00, 116.58it/s, est. speed input: 119388.72 toks/s, output: 116.59 toks/s]
[rank0]:[W128 08:34:15.715793862 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 44.8s

测试结果:
  Requests/s:   93.19
  Tokens/s:     95519.49
  Total Reqs:   512
  Elapsed:      5.49s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     95426.30

============================================================
[5/7] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 08:34:28 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3312388) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3312388) WARNING 01-28 08:34:43 [backends.py:609] Failed to read file <frozen os>
Throughput: 102.56 requests/s, 105119.94 total tokens/s, 102.56 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-28 08:34:28] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:34:28] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:34:28] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:34:28] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:34:28] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:34:28] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:34:28] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:34:28] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:34:28] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:34:28] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:34:28] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:34:28] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:34:28] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:34:28] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 08:34:35] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:34:35] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:34:35] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:34:35] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:34:35] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:34:35] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:34:35] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:34:35] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:34:35] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:34:35] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:34:35] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:34:35] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:34:35] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:34:35] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3312388) [2026-01-28 08:34:36] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3312388) [2026-01-28 08:34:36] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3312388) [2026-01-28 08:34:36] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3312388) [2026-01-28 08:34:36] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3312388) [2026-01-28 08:34:36] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3312388) [2026-01-28 08:34:36] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3312388) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3312388) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.08it/s]
(EngineCore_DP0 pid=3312388) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.08it/s]
(EngineCore_DP0 pid=3312388) 
(EngineCore_DP0 pid=3312388) [2026-01-28 08:34:36] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3312388) [2026-01-28 08:34:36] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6144000 bytes
(EngineCore_DP0 pid=3312388) [2026-01-28 08:34:36] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3312388) [2026-01-28 08:34:36] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4096000 bytes
(EngineCore_DP0 pid=3312388) [2026-01-28 08:34:37] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3312388) [2026-01-28 08:34:37] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 22118400 bytes
(EngineCore_DP0 pid=3312388) [2026-01-28 08:34:37] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3312388) [2026-01-28 08:34:37] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11059200 bytes
(EngineCore_DP0 pid=3312388) 2026-01-28 08:34:53,990 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3312388) 2026-01-28 08:34:54,017 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3312388) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 1/5 [00:00<00:00,  4.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 3/5 [00:00<00:00,  9.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 11.63it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 10.18it/s]
(EngineCore_DP0 pid=3312388) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00,  9.87it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 12.99it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 12.40it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   3%|▎         | 32/1024 [00:00<00:03, 318.83it/s]
Adding requests:   8%|▊         | 84/1024 [00:00<00:02, 435.14it/s]
Adding requests:  13%|█▎        | 135/1024 [00:00<00:01, 468.84it/s]
Adding requests:  18%|█▊        | 185/1024 [00:00<00:01, 478.22it/s]
Adding requests:  23%|██▎       | 237/1024 [00:00<00:01, 491.83it/s]
Adding requests:  28%|██▊       | 287/1024 [00:00<00:01, 494.30it/s]
Adding requests:  33%|███▎      | 337/1024 [00:00<00:01, 494.60it/s]
Adding requests:  38%|███▊      | 389/1024 [00:00<00:01, 501.04it/s]
Adding requests:  43%|████▎     | 440/1024 [00:00<00:01, 501.58it/s]
Adding requests:  48%|████▊     | 491/1024 [00:01<00:01, 502.12it/s]
Adding requests:  53%|█████▎    | 542/1024 [00:01<00:00, 490.01it/s]
Adding requests:  58%|█████▊    | 596/1024 [00:01<00:00, 500.63it/s]
Adding requests:  63%|██████▎   | 647/1024 [00:01<00:00, 499.39it/s]
Adding requests:  68%|██████▊   | 700/1024 [00:01<00:00, 508.32it/s]
Adding requests:  73%|███████▎  | 751/1024 [00:01<00:00, 507.22it/s]
Adding requests:  78%|███████▊  | 802/1024 [00:01<00:00, 504.92it/s]
Adding requests:  83%|████████▎ | 853/1024 [00:01<00:00, 498.96it/s]
Adding requests:  88%|████████▊ | 906/1024 [00:01<00:00, 507.61it/s]
Adding requests:  93%|█████████▎| 957/1024 [00:01<00:00, 506.71it/s]
Adding requests:  99%|█████████▊| 1009/1024 [00:02<00:00, 509.14it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 496.42it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  17%|█▋        | 178/1024 [00:00<00:00, 1162.53it/s, est. speed input: 1190597.65 toks/s, output: 1162.58 toks/s]
Processed prompts:  29%|██▉       | 295/1024 [00:01<00:03, 210.92it/s, est. speed input: 253557.74 toks/s, output: 247.61 toks/s]   
Processed prompts:  34%|███▍      | 350/1024 [00:01<00:03, 169.77it/s, est. speed input: 209625.92 toks/s, output: 204.71 toks/s]
Processed prompts:  38%|███▊      | 385/1024 [00:02<00:04, 157.19it/s, est. speed input: 196500.49 toks/s, output: 191.89 toks/s]
Processed prompts:  40%|████      | 411/1024 [00:02<00:04, 139.44it/s, est. speed input: 182860.69 toks/s, output: 178.57 toks/s]
Processed prompts:  42%|████▏     | 431/1024 [00:02<00:04, 138.81it/s, est. speed input: 180191.80 toks/s, output: 175.97 toks/s]
Processed prompts:  44%|████▍     | 449/1024 [00:02<00:04, 135.27it/s, est. speed input: 176850.62 toks/s, output: 172.71 toks/s]
Processed prompts:  45%|████▌     | 465/1024 [00:02<00:04, 130.73it/s, est. speed input: 173651.95 toks/s, output: 169.58 toks/s]
Processed prompts:  47%|████▋     | 480/1024 [00:02<00:04, 124.10it/s, est. speed input: 170098.75 toks/s, output: 166.11 toks/s]
Processed prompts:  48%|████▊     | 493/1024 [00:03<00:04, 115.12it/s, est. speed input: 166177.60 toks/s, output: 162.28 toks/s]
Processed prompts:  49%|████▉     | 506/1024 [00:03<00:04, 108.15it/s, est. speed input: 162672.17 toks/s, output: 158.86 toks/s]
Processed prompts:  51%|█████     | 522/1024 [00:03<00:04, 108.15it/s, est. speed input: 160365.68 toks/s, output: 156.61 toks/s]
Processed prompts:  53%|█████▎    | 538/1024 [00:03<00:04, 108.01it/s, est. speed input: 158225.39 toks/s, output: 154.52 toks/s]
Processed prompts:  54%|█████▍    | 554/1024 [00:03<00:04, 107.91it/s, est. speed input: 156262.56 toks/s, output: 152.60 toks/s]
Processed prompts:  56%|█████▌    | 570/1024 [00:03<00:04, 107.82it/s, est. speed input: 154448.94 toks/s, output: 150.83 toks/s]
Processed prompts:  57%|█████▋    | 586/1024 [00:03<00:04, 107.91it/s, est. speed input: 152800.69 toks/s, output: 149.22 toks/s]
Processed prompts:  59%|█████▉    | 602/1024 [00:04<00:03, 108.02it/s, est. speed input: 151281.64 toks/s, output: 147.73 toks/s]
Processed prompts:  60%|██████    | 618/1024 [00:04<00:03, 108.03it/s, est. speed input: 149856.52 toks/s, output: 146.34 toks/s]
Processed prompts:  62%|██████▏   | 634/1024 [00:04<00:03, 108.12it/s, est. speed input: 148540.89 toks/s, output: 145.06 toks/s]
Processed prompts:  63%|██████▎   | 650/1024 [00:04<00:03, 107.77it/s, est. speed input: 147249.86 toks/s, output: 143.80 toks/s]
Processed prompts:  65%|██████▌   | 666/1024 [00:04<00:03, 107.74it/s, est. speed input: 146071.88 toks/s, output: 142.65 toks/s]
Processed prompts:  67%|██████▋   | 682/1024 [00:04<00:03, 107.33it/s, est. speed input: 144913.32 toks/s, output: 141.52 toks/s]
Processed prompts:  68%|██████▊   | 698/1024 [00:04<00:03, 107.60it/s, est. speed input: 143898.95 toks/s, output: 140.53 toks/s]
Processed prompts:  70%|██████▉   | 714/1024 [00:05<00:02, 107.80it/s, est. speed input: 142945.06 toks/s, output: 139.59 toks/s]
Processed prompts:  71%|███████▏  | 730/1024 [00:05<00:02, 107.91it/s, est. speed input: 142040.13 toks/s, output: 138.71 toks/s]
Processed prompts:  73%|███████▎  | 746/1024 [00:05<00:02, 107.95it/s, est. speed input: 141179.83 toks/s, output: 137.87 toks/s]
Processed prompts:  74%|███████▍  | 762/1024 [00:05<00:02, 107.65it/s, est. speed input: 140328.67 toks/s, output: 137.04 toks/s]
Processed prompts:  76%|███████▌  | 778/1024 [00:05<00:02, 107.59it/s, est. speed input: 139537.91 toks/s, output: 136.27 toks/s]
Processed prompts:  78%|███████▊  | 794/1024 [00:05<00:02, 107.63it/s, est. speed input: 138798.02 toks/s, output: 135.54 toks/s]
Processed prompts:  79%|███████▉  | 810/1024 [00:06<00:01, 107.61it/s, est. speed input: 138088.42 toks/s, output: 134.85 toks/s]
Processed prompts:  81%|████████  | 826/1024 [00:06<00:01, 107.77it/s, est. speed input: 137430.71 toks/s, output: 134.21 toks/s]
Processed prompts:  82%|████████▏ | 842/1024 [00:06<00:01, 107.84it/s, est. speed input: 136799.61 toks/s, output: 133.59 toks/s]
Processed prompts:  84%|████████▍ | 858/1024 [00:06<00:01, 107.52it/s, est. speed input: 136162.11 toks/s, output: 132.97 toks/s]
Processed prompts:  85%|████████▌ | 874/1024 [00:06<00:01, 107.15it/s, est. speed input: 135539.10 toks/s, output: 132.36 toks/s]
Processed prompts:  87%|████████▋ | 890/1024 [00:06<00:01, 107.19it/s, est. speed input: 134972.20 toks/s, output: 131.81 toks/s]
Processed prompts:  88%|████████▊ | 906/1024 [00:06<00:01, 107.18it/s, est. speed input: 134426.06 toks/s, output: 131.27 toks/s]
Processed prompts:  90%|█████████ | 922/1024 [00:07<00:00, 107.39it/s, est. speed input: 133922.64 toks/s, output: 130.78 toks/s]
Processed prompts:  92%|█████████▏| 938/1024 [00:07<00:00, 108.99it/s, est. speed input: 133561.60 toks/s, output: 130.43 toks/s]
Processed prompts:  93%|█████████▎| 954/1024 [00:07<00:00, 108.58it/s, est. speed input: 133089.13 toks/s, output: 129.97 toks/s]
Processed prompts:  95%|█████████▍| 970/1024 [00:07<00:00, 108.33it/s, est. speed input: 132637.59 toks/s, output: 129.53 toks/s]
Processed prompts:  96%|█████████▋| 986/1024 [00:07<00:00, 109.61it/s, est. speed input: 132317.06 toks/s, output: 129.22 toks/s]
Processed prompts:  98%|█████████▊| 1002/1024 [00:07<00:00, 108.91it/s, est. speed input: 131887.49 toks/s, output: 128.80 toks/s]
Processed prompts:  99%|█████████▉| 1018/1024 [00:07<00:00, 110.50it/s, est. speed input: 131627.21 toks/s, output: 128.54 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:07<00:00, 110.50it/s, est. speed input: 132396.84 toks/s, output: 129.29 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:07<00:00, 129.29it/s, est. speed input: 132396.84 toks/s, output: 129.29 toks/s]
[rank0]:[W128 08:35:06.680058605 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 50.9s

测试结果:
  Requests/s:   102.56
  Tokens/s:     105119.94
  Total Reqs:   1024
  Elapsed:      9.98s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     105017.39

============================================================
[6/7] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 08:35:24 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3313748) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3313748) WARNING 01-28 08:35:38 [backends.py:609] Failed to read file <frozen os>
Throughput: 112.28 requests/s, 115086.79 total tokens/s, 112.28 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-28 08:35:23] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:35:23] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:35:23] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:35:23] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:35:23] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:35:23] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:35:23] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:35:23] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:35:23] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:35:23] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:35:23] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:35:23] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:35:23] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:35:23] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 08:35:30] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:35:30] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:35:30] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:35:30] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:35:30] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:35:30] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:35:30] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:35:30] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:35:30] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:35:30] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:35:30] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:35:30] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:35:30] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:35:30] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3313748) [2026-01-28 08:35:31] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3313748) [2026-01-28 08:35:31] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3313748) [2026-01-28 08:35:31] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3313748) [2026-01-28 08:35:31] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3313748) [2026-01-28 08:35:31] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3313748) [2026-01-28 08:35:31] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3313748) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3313748) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.14it/s]
(EngineCore_DP0 pid=3313748) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.14it/s]
(EngineCore_DP0 pid=3313748) 
(EngineCore_DP0 pid=3313748) [2026-01-28 08:35:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3313748) [2026-01-28 08:35:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6144000 bytes
(EngineCore_DP0 pid=3313748) [2026-01-28 08:35:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3313748) [2026-01-28 08:35:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4096000 bytes
(EngineCore_DP0 pid=3313748) [2026-01-28 08:35:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3313748) [2026-01-28 08:35:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 22118400 bytes
(EngineCore_DP0 pid=3313748) [2026-01-28 08:35:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3313748) [2026-01-28 08:35:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11059200 bytes
(EngineCore_DP0 pid=3313748) 2026-01-28 08:35:50,116 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3313748) 2026-01-28 08:35:50,144 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3313748) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00, 12.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00, 12.72it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:00<00:00, 13.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 13.19it/s]
(EngineCore_DP0 pid=3313748) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00, 16.87it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00, 17.77it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 17.75it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 47/2048 [00:00<00:04, 463.83it/s]
Adding requests:   5%|▍         | 97/2048 [00:00<00:04, 482.62it/s]
Adding requests:   7%|▋         | 147/2048 [00:00<00:03, 488.23it/s]
Adding requests:  10%|▉         | 196/2048 [00:00<00:03, 487.31it/s]
Adding requests:  12%|█▏        | 248/2048 [00:00<00:03, 496.50it/s]
Adding requests:  15%|█▍        | 298/2048 [00:00<00:03, 490.40it/s]
Adding requests:  17%|█▋        | 348/2048 [00:00<00:03, 492.54it/s]
Adding requests:  20%|█▉        | 400/2048 [00:00<00:03, 500.37it/s]
Adding requests:  22%|██▏       | 451/2048 [00:00<00:03, 501.17it/s]
Adding requests:  25%|██▍       | 502/2048 [00:01<00:03, 501.10it/s]
Adding requests:  27%|██▋       | 553/2048 [00:01<00:03, 496.61it/s]
Adding requests:  30%|██▉       | 605/2048 [00:01<00:02, 501.09it/s]
Adding requests:  32%|███▏      | 658/2048 [00:01<00:02, 508.88it/s]
Adding requests:  35%|███▍      | 712/2048 [00:01<00:02, 515.58it/s]
Adding requests:  37%|███▋      | 764/2048 [00:01<00:02, 504.72it/s]
Adding requests:  40%|███▉      | 815/2048 [00:01<00:02, 499.88it/s]
Adding requests:  42%|████▏     | 866/2048 [00:01<00:02, 502.24it/s]
Adding requests:  45%|████▍     | 919/2048 [00:01<00:02, 509.32it/s]
Adding requests:  47%|████▋     | 971/2048 [00:01<00:02, 510.20it/s]
Adding requests:  50%|█████     | 1024/2048 [00:02<00:01, 514.88it/s]
Adding requests:  53%|█████▎    | 1076/2048 [00:02<00:01, 510.59it/s]
Adding requests:  55%|█████▌    | 1128/2048 [00:02<00:01, 474.95it/s]
Adding requests:  58%|█████▊    | 1181/2048 [00:02<00:01, 489.28it/s]
Adding requests:  60%|██████    | 1234/2048 [00:02<00:01, 500.17it/s]
Adding requests:  63%|██████▎   | 1285/2048 [00:02<00:01, 499.36it/s]
Adding requests:  65%|██████▌   | 1337/2048 [00:02<00:01, 504.93it/s]
Adding requests:  68%|██████▊   | 1389/2048 [00:02<00:01, 507.82it/s]
Adding requests:  70%|███████   | 1440/2048 [00:02<00:01, 506.43it/s]
Adding requests:  73%|███████▎  | 1491/2048 [00:02<00:01, 506.06it/s]
Adding requests:  75%|███████▌  | 1542/2048 [00:03<00:01, 505.12it/s]
Adding requests:  78%|███████▊  | 1595/2048 [00:03<00:00, 511.40it/s]
Adding requests:  80%|████████  | 1647/2048 [00:03<00:00, 513.39it/s]
Adding requests:  83%|████████▎ | 1699/2048 [00:03<00:00, 509.10it/s]
Adding requests:  85%|████████▌ | 1751/2048 [00:03<00:00, 510.07it/s]
Adding requests:  88%|████████▊ | 1803/2048 [00:03<00:00, 506.87it/s]
Adding requests:  91%|█████████ | 1854/2048 [00:03<00:00, 507.07it/s]
Adding requests:  93%|█████████▎| 1905/2048 [00:03<00:00, 491.90it/s]
Adding requests:  96%|█████████▌| 1956/2048 [00:03<00:00, 494.85it/s]
Adding requests:  98%|█████████▊| 2008/2048 [00:04<00:00, 499.50it/s]
Adding requests: 100%|██████████| 2048/2048 [00:04<00:00, 501.55it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  22%|██▏       | 450/2048 [00:00<00:00, 3090.15it/s, est. speed input: 3165009.63 toks/s, output: 3090.35 toks/s]
Processed prompts:  37%|███▋      | 760/2048 [00:02<00:05, 223.51it/s, est. speed input: 274031.19 toks/s, output: 267.61 toks/s]   
Processed prompts:  44%|████▎     | 895/2048 [00:03<00:06, 185.12it/s, est. speed input: 230555.08 toks/s, output: 225.15 toks/s]
Processed prompts:  48%|████▊     | 974/2048 [00:04<00:06, 167.40it/s, est. speed input: 213289.74 toks/s, output: 208.29 toks/s]
Processed prompts:  50%|█████     | 1026/2048 [00:05<00:06, 151.13it/s, est. speed input: 200647.78 toks/s, output: 195.94 toks/s]
Processed prompts:  52%|█████▏    | 1063/2048 [00:05<00:06, 148.29it/s, est. speed input: 197188.73 toks/s, output: 192.57 toks/s]
Processed prompts:  53%|█████▎    | 1092/2048 [00:05<00:06, 140.82it/s, est. speed input: 192682.56 toks/s, output: 188.17 toks/s]
Processed prompts:  54%|█████▍    | 1115/2048 [00:05<00:06, 143.02it/s, est. speed input: 192040.94 toks/s, output: 187.54 toks/s]
Processed prompts:  55%|█████▌    | 1136/2048 [00:06<00:06, 143.55it/s, est. speed input: 191069.74 toks/s, output: 186.59 toks/s]
Processed prompts:  56%|█████▋    | 1155/2048 [00:06<00:07, 123.64it/s, est. speed input: 185808.78 toks/s, output: 181.45 toks/s]
Processed prompts:  57%|█████▋    | 1171/2048 [00:06<00:07, 121.70it/s, est. speed input: 184240.55 toks/s, output: 179.92 toks/s]
Processed prompts:  58%|█████▊    | 1186/2048 [00:06<00:07, 118.63it/s, est. speed input: 182609.47 toks/s, output: 178.33 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [00:06<00:07, 117.43it/s, est. speed input: 181213.83 toks/s, output: 176.97 toks/s]
Processed prompts:  59%|█████▉    | 1218/2048 [00:06<00:07, 116.40it/s, est. speed input: 179876.56 toks/s, output: 175.66 toks/s]
Processed prompts:  60%|██████    | 1234/2048 [00:07<00:07, 115.40it/s, est. speed input: 178573.00 toks/s, output: 174.39 toks/s]
Processed prompts:  61%|██████    | 1250/2048 [00:07<00:06, 114.67it/s, est. speed input: 177328.71 toks/s, output: 173.17 toks/s]
Processed prompts:  62%|██████▏   | 1266/2048 [00:07<00:06, 115.88it/s, est. speed input: 176321.63 toks/s, output: 172.19 toks/s]
Processed prompts:  63%|██████▎   | 1282/2048 [00:07<00:06, 114.85it/s, est. speed input: 175154.87 toks/s, output: 171.05 toks/s]
Processed prompts:  63%|██████▎   | 1298/2048 [00:07<00:06, 114.29it/s, est. speed input: 174050.08 toks/s, output: 169.97 toks/s]
Processed prompts:  64%|██████▍   | 1314/2048 [00:07<00:06, 113.93it/s, est. speed input: 172989.01 toks/s, output: 168.93 toks/s]
Processed prompts:  65%|██████▍   | 1330/2048 [00:07<00:06, 113.67it/s, est. speed input: 171966.69 toks/s, output: 167.94 toks/s]
Processed prompts:  66%|██████▌   | 1346/2048 [00:08<00:06, 112.86it/s, est. speed input: 170923.54 toks/s, output: 166.92 toks/s]
Processed prompts:  67%|██████▋   | 1362/2048 [00:08<00:06, 112.90it/s, est. speed input: 169970.91 toks/s, output: 165.99 toks/s]
Processed prompts:  67%|██████▋   | 1378/2048 [00:08<00:05, 112.63it/s, est. speed input: 169024.24 toks/s, output: 165.06 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [00:08<00:05, 112.44it/s, est. speed input: 168110.09 toks/s, output: 164.17 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [00:08<00:05, 112.43it/s, est. speed input: 167235.97 toks/s, output: 163.32 toks/s]
Processed prompts:  70%|██████▉   | 1426/2048 [00:08<00:05, 112.45it/s, est. speed input: 166392.71 toks/s, output: 162.49 toks/s]
Processed prompts:  70%|███████   | 1442/2048 [00:08<00:05, 112.63it/s, est. speed input: 165589.21 toks/s, output: 161.71 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [00:09<00:05, 112.65it/s, est. speed input: 164803.03 toks/s, output: 160.94 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [00:09<00:05, 112.74it/s, est. speed input: 164045.81 toks/s, output: 160.20 toks/s]
Processed prompts:  73%|███████▎  | 1490/2048 [00:09<00:04, 112.69it/s, est. speed input: 163304.07 toks/s, output: 159.48 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [00:09<00:04, 112.49it/s, est. speed input: 162572.53 toks/s, output: 158.76 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [00:09<00:04, 112.46it/s, est. speed input: 161870.16 toks/s, output: 158.08 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [00:09<00:04, 112.39it/s, est. speed input: 161185.00 toks/s, output: 157.41 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [00:09<00:04, 112.49it/s, est. speed input: 160529.76 toks/s, output: 156.77 toks/s]
Processed prompts:  77%|███████▋  | 1570/2048 [00:10<00:04, 112.61it/s, est. speed input: 159895.85 toks/s, output: 156.15 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [00:10<00:04, 114.42it/s, est. speed input: 159392.24 toks/s, output: 155.66 toks/s]
Processed prompts:  78%|███████▊  | 1602/2048 [00:10<00:03, 113.74it/s, est. speed input: 158778.17 toks/s, output: 155.06 toks/s]
Processed prompts:  79%|███████▉  | 1618/2048 [00:10<00:03, 113.32it/s, est. speed input: 158183.23 toks/s, output: 154.48 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [00:10<00:03, 113.04it/s, est. speed input: 157605.15 toks/s, output: 153.91 toks/s]
Processed prompts:  81%|████████  | 1650/2048 [00:10<00:03, 112.96it/s, est. speed input: 157049.74 toks/s, output: 153.37 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [00:10<00:03, 112.89it/s, est. speed input: 156507.26 toks/s, output: 152.84 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [00:11<00:03, 112.71it/s, est. speed input: 155971.81 toks/s, output: 152.32 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [00:11<00:03, 112.59it/s, est. speed input: 155450.27 toks/s, output: 151.81 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [00:11<00:02, 112.41it/s, est. speed input: 154935.84 toks/s, output: 151.30 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [00:11<00:02, 112.29it/s, est. speed input: 154434.99 toks/s, output: 150.82 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [00:11<00:02, 112.19it/s, est. speed input: 153945.56 toks/s, output: 150.34 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [00:11<00:02, 112.26it/s, est. speed input: 153475.05 toks/s, output: 149.88 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [00:11<00:02, 112.25it/s, est. speed input: 153013.25 toks/s, output: 149.43 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [00:12<00:02, 112.45it/s, est. speed input: 152573.99 toks/s, output: 149.00 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [00:12<00:02, 112.60it/s, est. speed input: 152144.36 toks/s, output: 148.58 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [00:12<00:01, 112.54it/s, est. speed input: 151716.93 toks/s, output: 148.16 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [00:12<00:01, 112.31it/s, est. speed input: 151288.77 toks/s, output: 147.74 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [00:12<00:01, 111.05it/s, est. speed input: 150814.52 toks/s, output: 147.28 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [00:12<00:01, 114.07it/s, est. speed input: 150545.19 toks/s, output: 147.02 toks/s]
Processed prompts:  92%|█████████▏| 1890/2048 [00:12<00:01, 113.48it/s, est. speed input: 150149.56 toks/s, output: 146.63 toks/s]
Processed prompts:  93%|█████████▎| 1906/2048 [00:13<00:01, 113.22it/s, est. speed input: 149769.71 toks/s, output: 146.26 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [00:13<00:01, 113.00it/s, est. speed input: 149396.68 toks/s, output: 145.89 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [00:13<00:00, 112.78it/s, est. speed input: 149028.21 toks/s, output: 145.53 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [00:13<00:00, 114.54it/s, est. speed input: 148755.09 toks/s, output: 145.27 toks/s]
Processed prompts:  96%|█████████▌| 1970/2048 [00:13<00:00, 113.75it/s, est. speed input: 148396.41 toks/s, output: 144.92 toks/s]
Processed prompts:  97%|█████████▋| 1986/2048 [00:13<00:00, 113.22it/s, est. speed input: 148046.06 toks/s, output: 144.58 toks/s]
Processed prompts:  98%|█████████▊| 2002/2048 [00:13<00:00, 112.94it/s, est. speed input: 147706.10 toks/s, output: 144.24 toks/s]
Processed prompts:  99%|█████████▊| 2018/2048 [00:14<00:00, 112.76it/s, est. speed input: 147374.59 toks/s, output: 143.92 toks/s]
Processed prompts:  99%|█████████▉| 2034/2048 [00:14<00:00, 115.08it/s, est. speed input: 147154.15 toks/s, output: 143.70 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:14<00:00, 115.08it/s, est. speed input: 148161.03 toks/s, output: 144.69 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:14<00:00, 144.69it/s, est. speed input: 148161.03 toks/s, output: 144.69 toks/s]
[rank0]:[W128 08:36:11.065358260 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 64.5s

测试结果:
  Requests/s:   112.28
  Tokens/s:     115086.79
  Total Reqs:   2048
  Elapsed:      18.24s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     114974.51

============================================================
[7/7] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 08:36:36 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3315242) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3315242) WARNING 01-28 08:36:51 [backends.py:609] Failed to read file <frozen os>
Throughput: 114.58 requests/s, 117446.83 total tokens/s, 114.58 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-28 08:36:36] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:36:36] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:36:36] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:36:36] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:36:36] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:36:36] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:36:36] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:36:36] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:36:36] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:36:36] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:36:36] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:36:36] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:36:36] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:36:36] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 08:36:43] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:36:43] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:36:43] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:36:43] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:36:43] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:36:43] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:36:43] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:36:43] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:36:43] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:36:43] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:36:43] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:36:43] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:36:43] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:36:43] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3315242) [2026-01-28 08:36:44] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3315242) [2026-01-28 08:36:44] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3315242) [2026-01-28 08:36:44] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3315242) [2026-01-28 08:36:44] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3315242) [2026-01-28 08:36:44] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3315242) [2026-01-28 08:36:44] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3315242) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3315242) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.88it/s]
(EngineCore_DP0 pid=3315242) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.88it/s]
(EngineCore_DP0 pid=3315242) 
(EngineCore_DP0 pid=3315242) [2026-01-28 08:36:45] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3315242) [2026-01-28 08:36:45] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6144000 bytes
(EngineCore_DP0 pid=3315242) [2026-01-28 08:36:45] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3315242) [2026-01-28 08:36:45] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4096000 bytes
(EngineCore_DP0 pid=3315242) [2026-01-28 08:36:45] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3315242) [2026-01-28 08:36:45] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 22118400 bytes
(EngineCore_DP0 pid=3315242) [2026-01-28 08:36:45] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3315242) [2026-01-28 08:36:45] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11059200 bytes
(EngineCore_DP0 pid=3315242) [rank0]:W0128 08:36:57.645000 3315242 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3315242) [rank0]:W0128 08:36:57.726000 3315242 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3315242) [rank0]:W0128 08:36:58.726000 3315242 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3315242) [rank0]:W0128 08:36:58.854000 3315242 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3315242) 2026-01-28 08:37:03,023 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3315242) 2026-01-28 08:37:03,052 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3315242) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▉         | 1/11 [00:00<00:02,  3.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 3/11 [00:00<00:00,  8.96it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 5/11 [00:00<00:00, 11.88it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▎   | 7/11 [00:00<00:00, 13.74it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 9/11 [00:00<00:00, 14.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 12.68it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 11.90it/s]
(EngineCore_DP0 pid=3315242) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00, 13.39it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00, 15.87it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00, 13.84it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 14.12it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 42/4096 [00:00<00:09, 413.80it/s]
Adding requests:   2%|▏         | 92/4096 [00:00<00:08, 460.04it/s]
Adding requests:   3%|▎         | 142/4096 [00:00<00:08, 476.27it/s]
Adding requests:   5%|▍         | 191/4096 [00:00<00:08, 481.04it/s]
Adding requests:   6%|▌         | 242/4096 [00:00<00:07, 490.21it/s]
Adding requests:   7%|▋         | 292/4096 [00:00<00:07, 490.87it/s]
Adding requests:   8%|▊         | 342/4096 [00:00<00:07, 489.23it/s]
Adding requests:  10%|▉         | 394/4096 [00:00<00:07, 498.58it/s]
Adding requests:  11%|█         | 444/4096 [00:00<00:07, 497.34it/s]
Adding requests:  12%|█▏        | 495/4096 [00:01<00:07, 499.58it/s]
Adding requests:  13%|█▎        | 545/4096 [00:01<00:07, 490.56it/s]
Adding requests:  15%|█▍        | 596/4096 [00:01<00:07, 496.11it/s]
Adding requests:  16%|█▌        | 648/4096 [00:01<00:06, 502.65it/s]
Adding requests:  17%|█▋        | 701/4096 [00:01<00:06, 508.88it/s]
Adding requests:  18%|█▊        | 752/4096 [00:01<00:06, 506.57it/s]
Adding requests:  20%|█▉        | 803/4096 [00:01<00:06, 498.62it/s]
Adding requests:  21%|██        | 853/4096 [00:01<00:06, 494.87it/s]
Adding requests:  22%|██▏       | 906/4096 [00:01<00:06, 503.70it/s]
Adding requests:  23%|██▎       | 958/4096 [00:01<00:06, 506.11it/s]
Adding requests:  25%|██▍       | 1009/4096 [00:02<00:06, 506.66it/s]
Adding requests:  26%|██▌       | 1061/4096 [00:02<00:05, 508.18it/s]
Adding requests:  27%|██▋       | 1112/4096 [00:02<00:06, 495.71it/s]
Adding requests:  28%|██▊       | 1164/4096 [00:02<00:05, 502.07it/s]
Adding requests:  30%|██▉       | 1217/4096 [00:02<00:05, 509.58it/s]
Adding requests:  31%|███       | 1269/4096 [00:02<00:05, 502.24it/s]
Adding requests:  32%|███▏      | 1321/4096 [00:02<00:05, 506.71it/s]
Adding requests:  34%|███▎      | 1374/4096 [00:02<00:05, 510.84it/s]
Adding requests:  35%|███▍      | 1426/4096 [00:02<00:05, 512.58it/s]
Adding requests:  36%|███▌      | 1478/4096 [00:02<00:05, 513.58it/s]
Adding requests:  37%|███▋      | 1531/4096 [00:03<00:04, 517.23it/s]
Adding requests:  39%|███▊      | 1583/4096 [00:03<00:04, 517.20it/s]
Adding requests:  40%|███▉      | 1637/4096 [00:03<00:04, 523.24it/s]
Adding requests:  41%|████▏     | 1690/4096 [00:03<00:04, 512.81it/s]
Adding requests:  43%|████▎     | 1743/4096 [00:03<00:04, 515.97it/s]
Adding requests:  44%|████▍     | 1795/4096 [00:03<00:04, 515.19it/s]
Adding requests:  45%|████▌     | 1847/4096 [00:03<00:04, 515.94it/s]
Adding requests:  46%|████▋     | 1899/4096 [00:03<00:04, 514.00it/s]
Adding requests:  48%|████▊     | 1951/4096 [00:03<00:04, 511.91it/s]
Adding requests:  49%|████▉     | 2003/4096 [00:03<00:04, 513.64it/s]
Adding requests:  50%|█████     | 2055/4096 [00:04<00:03, 512.74it/s]
Adding requests:  51%|█████▏    | 2108/4096 [00:04<00:03, 515.51it/s]
Adding requests:  53%|█████▎    | 2160/4096 [00:04<00:03, 505.71it/s]
Adding requests:  54%|█████▍    | 2211/4096 [00:04<00:03, 504.78it/s]
Adding requests:  55%|█████▌    | 2262/4096 [00:04<00:03, 499.49it/s]
Adding requests:  57%|█████▋    | 2315/4096 [00:04<00:03, 505.64it/s]
Adding requests:  58%|█████▊    | 2366/4096 [00:04<00:03, 502.91it/s]
Adding requests:  59%|█████▉    | 2418/4096 [00:04<00:03, 506.28it/s]
Adding requests:  60%|██████    | 2470/4096 [00:04<00:03, 508.28it/s]
Adding requests:  62%|██████▏   | 2521/4096 [00:04<00:03, 507.61it/s]
Adding requests:  63%|██████▎   | 2574/4096 [00:05<00:02, 512.22it/s]
Adding requests:  64%|██████▍   | 2626/4096 [00:05<00:02, 510.56it/s]
Adding requests:  65%|██████▌   | 2678/4096 [00:05<00:02, 511.98it/s]
Adding requests:  67%|██████▋   | 2730/4096 [00:05<00:02, 509.58it/s]
Adding requests:  68%|██████▊   | 2782/4096 [00:05<00:02, 509.62it/s]
Adding requests:  69%|██████▉   | 2833/4096 [00:05<00:02, 506.32it/s]
Adding requests:  70%|███████   | 2885/4096 [00:05<00:02, 509.09it/s]
Adding requests:  72%|███████▏  | 2936/4096 [00:05<00:02, 504.27it/s]
Adding requests:  73%|███████▎  | 2988/4096 [00:05<00:02, 506.15it/s]
Adding requests:  74%|███████▍  | 3039/4096 [00:06<00:02, 504.71it/s]
Adding requests:  75%|███████▌  | 3090/4096 [00:06<00:01, 505.70it/s]
Adding requests:  77%|███████▋  | 3141/4096 [00:06<00:01, 506.20it/s]
Adding requests:  78%|███████▊  | 3192/4096 [00:06<00:01, 505.16it/s]
Adding requests:  79%|███████▉  | 3244/4096 [00:06<00:01, 508.20it/s]
Adding requests:  80%|████████  | 3296/4096 [00:06<00:01, 510.73it/s]
Adding requests:  82%|████████▏ | 3348/4096 [00:06<00:01, 505.41it/s]
Adding requests:  83%|████████▎ | 3399/4096 [00:06<00:01, 502.58it/s]
Adding requests:  84%|████████▍ | 3450/4096 [00:06<00:01, 467.45it/s]
Adding requests:  85%|████████▌ | 3501/4096 [00:06<00:01, 477.31it/s]
Adding requests:  87%|████████▋ | 3553/4096 [00:07<00:01, 488.72it/s]
Adding requests:  88%|████████▊ | 3603/4096 [00:07<00:01, 483.19it/s]
Adding requests:  89%|████████▉ | 3652/4096 [00:07<00:01, 436.84it/s]
Adding requests:  90%|█████████ | 3699/4096 [00:07<00:00, 444.30it/s]
Adding requests:  91%|█████████▏| 3745/4096 [00:07<00:00, 447.66it/s]
Adding requests:  93%|█████████▎| 3800/4096 [00:07<00:00, 474.04it/s]
Adding requests:  94%|█████████▍| 3851/4096 [00:07<00:00, 483.73it/s]
Adding requests:  95%|█████████▌| 3904/4096 [00:07<00:00, 494.42it/s]
Adding requests:  97%|█████████▋| 3957/4096 [00:07<00:00, 502.43it/s]
Adding requests:  98%|█████████▊| 4009/4096 [00:08<00:00, 506.94it/s]
Adding requests:  99%|█████████▉| 4060/4096 [00:08<00:00, 504.10it/s]
Adding requests: 100%|██████████| 4096/4096 [00:08<00:00, 500.56it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  23%|██▎       | 930/4096 [00:00<00:01, 2979.50it/s, est. speed input: 3051274.41 toks/s, output: 2979.58 toks/s]
Processed prompts:  30%|██▉       | 1228/4096 [00:02<00:08, 349.30it/s, est. speed input: 447418.08 toks/s, output: 436.93 toks/s]  
Processed prompts:  33%|███▎      | 1358/4096 [00:03<00:10, 263.65it/s, est. speed input: 354615.25 toks/s, output: 346.30 toks/s]
Processed prompts:  35%|███▌      | 1434/4096 [00:04<00:11, 237.03it/s, est. speed input: 327856.17 toks/s, output: 320.17 toks/s]
Processed prompts:  36%|███▋      | 1486/4096 [00:05<00:12, 203.86it/s, est. speed input: 302060.22 toks/s, output: 294.98 toks/s]
Processed prompts:  37%|███▋      | 1523/4096 [00:05<00:13, 193.81it/s, est. speed input: 293340.18 toks/s, output: 286.46 toks/s]
Processed prompts:  38%|███▊      | 1552/4096 [00:05<00:14, 178.66it/s, est. speed input: 283987.39 toks/s, output: 277.33 toks/s]
Processed prompts:  38%|███▊      | 1575/4096 [00:05<00:15, 160.92it/s, est. speed input: 274868.99 toks/s, output: 268.43 toks/s]
Processed prompts:  39%|███▉      | 1602/4096 [00:06<00:16, 146.89it/s, est. speed input: 266858.68 toks/s, output: 260.60 toks/s]
Processed prompts:  40%|███▉      | 1634/4096 [00:06<00:17, 139.17it/s, est. speed input: 260339.53 toks/s, output: 254.24 toks/s]
Processed prompts:  41%|████      | 1666/4096 [00:06<00:18, 132.89it/s, est. speed input: 254362.57 toks/s, output: 248.40 toks/s]
Processed prompts:  41%|████▏     | 1698/4096 [00:06<00:18, 127.88it/s, est. speed input: 248840.77 toks/s, output: 243.01 toks/s]
Processed prompts:  42%|████▏     | 1730/4096 [00:07<00:19, 124.19it/s, est. speed input: 243776.77 toks/s, output: 238.06 toks/s]
Processed prompts:  43%|████▎     | 1762/4096 [00:07<00:19, 121.32it/s, est. speed input: 239054.84 toks/s, output: 233.45 toks/s]
Processed prompts:  44%|████▍     | 1794/4096 [00:07<00:19, 119.21it/s, est. speed input: 234667.82 toks/s, output: 229.17 toks/s]
Processed prompts:  45%|████▍     | 1826/4096 [00:08<00:19, 117.54it/s, est. speed input: 230549.66 toks/s, output: 225.15 toks/s]
Processed prompts:  45%|████▌     | 1858/4096 [00:08<00:19, 117.35it/s, est. speed input: 226928.02 toks/s, output: 221.61 toks/s]
Processed prompts:  46%|████▌     | 1890/4096 [00:08<00:18, 116.56it/s, est. speed input: 223400.89 toks/s, output: 218.16 toks/s]
Processed prompts:  47%|████▋     | 1922/4096 [00:08<00:18, 115.92it/s, est. speed input: 220078.86 toks/s, output: 214.92 toks/s]
Processed prompts:  48%|████▊     | 1954/4096 [00:09<00:18, 116.43it/s, est. speed input: 217139.20 toks/s, output: 212.05 toks/s]
Processed prompts:  48%|████▊     | 1986/4096 [00:09<00:18, 115.78it/s, est. speed input: 214186.07 toks/s, output: 209.17 toks/s]
Processed prompts:  49%|████▉     | 2018/4096 [00:09<00:18, 115.35it/s, est. speed input: 211405.56 toks/s, output: 206.45 toks/s]
Processed prompts:  50%|█████     | 2050/4096 [00:10<00:17, 115.00it/s, est. speed input: 208772.38 toks/s, output: 203.88 toks/s]
Processed prompts:  51%|█████     | 2082/4096 [00:10<00:17, 114.75it/s, est. speed input: 206280.94 toks/s, output: 201.45 toks/s]
Processed prompts:  52%|█████▏    | 2114/4096 [00:10<00:17, 114.54it/s, est. speed input: 203916.10 toks/s, output: 199.14 toks/s]
Processed prompts:  52%|█████▏    | 2146/4096 [00:10<00:17, 114.45it/s, est. speed input: 201681.48 toks/s, output: 196.95 toks/s]
Processed prompts:  53%|█████▎    | 2178/4096 [00:11<00:16, 114.39it/s, est. speed input: 199558.78 toks/s, output: 194.88 toks/s]
Processed prompts:  54%|█████▍    | 2210/4096 [00:11<00:16, 116.22it/s, est. speed input: 197799.73 toks/s, output: 193.16 toks/s]
Processed prompts:  55%|█████▍    | 2242/4096 [00:11<00:16, 115.87it/s, est. speed input: 195901.34 toks/s, output: 191.31 toks/s]
Processed prompts:  56%|█████▌    | 2274/4096 [00:11<00:15, 116.50it/s, est. speed input: 194204.84 toks/s, output: 189.65 toks/s]
Processed prompts:  56%|█████▋    | 2306/4096 [00:12<00:15, 115.86it/s, est. speed input: 192446.67 toks/s, output: 187.94 toks/s]
Processed prompts:  57%|█████▋    | 2338/4096 [00:12<00:15, 116.29it/s, est. speed input: 190873.70 toks/s, output: 186.40 toks/s]
Processed prompts:  58%|█████▊    | 2370/4096 [00:12<00:14, 117.69it/s, est. speed input: 189494.25 toks/s, output: 185.05 toks/s]
Processed prompts:  59%|█████▊    | 2402/4096 [00:13<00:14, 116.85it/s, est. speed input: 187966.88 toks/s, output: 183.56 toks/s]
Processed prompts:  59%|█████▉    | 2434/4096 [00:13<00:14, 116.21it/s, est. speed input: 186496.42 toks/s, output: 182.12 toks/s]
Processed prompts:  60%|██████    | 2466/4096 [00:13<00:14, 115.82it/s, est. speed input: 185091.31 toks/s, output: 180.75 toks/s]
Processed prompts:  61%|██████    | 2498/4096 [00:13<00:13, 116.36it/s, est. speed input: 183827.95 toks/s, output: 179.52 toks/s]
Processed prompts:  62%|██████▏   | 2530/4096 [00:14<00:13, 115.50it/s, est. speed input: 182486.64 toks/s, output: 178.21 toks/s]
Processed prompts:  63%|██████▎   | 2562/4096 [00:14<00:13, 116.22it/s, est. speed input: 181329.48 toks/s, output: 177.08 toks/s]
Processed prompts:  63%|██████▎   | 2594/4096 [00:14<00:12, 115.79it/s, est. speed input: 180123.59 toks/s, output: 175.90 toks/s]
Processed prompts:  64%|██████▍   | 2626/4096 [00:15<00:12, 115.29it/s, est. speed input: 178944.24 toks/s, output: 174.75 toks/s]
Processed prompts:  65%|██████▍   | 2658/4096 [00:15<00:12, 114.99it/s, est. speed input: 177811.71 toks/s, output: 173.64 toks/s]
Processed prompts:  66%|██████▌   | 2690/4096 [00:15<00:12, 114.90it/s, est. speed input: 176730.66 toks/s, output: 172.59 toks/s]
Processed prompts:  66%|██████▋   | 2722/4096 [00:15<00:11, 114.82it/s, est. speed input: 175687.15 toks/s, output: 171.57 toks/s]
Processed prompts:  67%|██████▋   | 2754/4096 [00:16<00:11, 114.72it/s, est. speed input: 174675.00 toks/s, output: 170.58 toks/s]
Processed prompts:  68%|██████▊   | 2786/4096 [00:16<00:11, 114.61it/s, est. speed input: 173693.66 toks/s, output: 169.62 toks/s]
Processed prompts:  69%|██████▉   | 2818/4096 [00:16<00:11, 114.67it/s, est. speed input: 172756.98 toks/s, output: 168.71 toks/s]
Processed prompts:  70%|██████▉   | 2850/4096 [00:16<00:10, 114.75it/s, est. speed input: 171854.39 toks/s, output: 167.83 toks/s]
Processed prompts:  70%|███████   | 2882/4096 [00:17<00:10, 114.76it/s, est. speed input: 170977.05 toks/s, output: 166.97 toks/s]
Processed prompts:  71%|███████   | 2914/4096 [00:17<00:10, 114.79it/s, est. speed input: 170129.48 toks/s, output: 166.14 toks/s]
Processed prompts:  72%|███████▏  | 2946/4096 [00:17<00:10, 114.74it/s, est. speed input: 169302.78 toks/s, output: 165.33 toks/s]
Processed prompts:  73%|███████▎  | 2978/4096 [00:18<00:09, 114.75it/s, est. speed input: 168504.95 toks/s, output: 164.56 toks/s]
Processed prompts:  73%|███████▎  | 3010/4096 [00:18<00:09, 114.66it/s, est. speed input: 167724.48 toks/s, output: 163.79 toks/s]
Processed prompts:  74%|███████▍  | 3042/4096 [00:18<00:09, 114.69it/s, est. speed input: 166974.17 toks/s, output: 163.06 toks/s]
Processed prompts:  75%|███████▌  | 3074/4096 [00:18<00:08, 114.59it/s, est. speed input: 166236.99 toks/s, output: 162.34 toks/s]
Processed prompts:  76%|███████▌  | 3106/4096 [00:19<00:08, 114.61it/s, est. speed input: 165527.51 toks/s, output: 161.65 toks/s]
Processed prompts:  77%|███████▋  | 3138/4096 [00:19<00:08, 115.47it/s, est. speed input: 164896.54 toks/s, output: 161.03 toks/s]
Processed prompts:  77%|███████▋  | 3170/4096 [00:19<00:08, 115.13it/s, est. speed input: 164219.87 toks/s, output: 160.37 toks/s]
Processed prompts:  78%|███████▊  | 3202/4096 [00:20<00:07, 114.91it/s, est. speed input: 163562.73 toks/s, output: 159.73 toks/s]
Processed prompts:  79%|███████▉  | 3234/4096 [00:20<00:07, 114.73it/s, est. speed input: 162922.54 toks/s, output: 159.10 toks/s]
Processed prompts:  80%|███████▉  | 3266/4096 [00:20<00:07, 114.27it/s, est. speed input: 162277.47 toks/s, output: 158.47 toks/s]
Processed prompts:  81%|████████  | 3298/4096 [00:20<00:06, 114.35it/s, est. speed input: 161675.87 toks/s, output: 157.89 toks/s]
Processed prompts:  81%|████████▏ | 3330/4096 [00:21<00:06, 114.32it/s, est. speed input: 161084.90 toks/s, output: 157.31 toks/s]
Processed prompts:  82%|████████▏ | 3362/4096 [00:21<00:06, 114.28it/s, est. speed input: 160507.56 toks/s, output: 156.75 toks/s]
Processed prompts:  83%|████████▎ | 3394/4096 [00:21<00:06, 114.26it/s, est. speed input: 159946.42 toks/s, output: 156.20 toks/s]
Processed prompts:  84%|████████▎ | 3426/4096 [00:22<00:05, 114.22it/s, est. speed input: 159397.51 toks/s, output: 155.66 toks/s]
Processed prompts:  84%|████████▍ | 3458/4096 [00:22<00:05, 114.21it/s, est. speed input: 158863.23 toks/s, output: 155.14 toks/s]
Processed prompts:  85%|████████▌ | 3490/4096 [00:22<00:05, 116.05it/s, est. speed input: 158447.08 toks/s, output: 154.73 toks/s]
Processed prompts:  86%|████████▌ | 3522/4096 [00:22<00:04, 115.62it/s, est. speed input: 157945.02 toks/s, output: 154.24 toks/s]
Processed prompts:  87%|████████▋ | 3554/4096 [00:23<00:04, 115.13it/s, est. speed input: 157444.89 toks/s, output: 153.75 toks/s]
Processed prompts:  88%|████████▊ | 3586/4096 [00:23<00:04, 114.90it/s, est. speed input: 156962.06 toks/s, output: 153.28 toks/s]
Processed prompts:  88%|████████▊ | 3618/4096 [00:23<00:04, 114.79it/s, est. speed input: 156493.80 toks/s, output: 152.83 toks/s]
Processed prompts:  89%|████████▉ | 3650/4096 [00:23<00:03, 114.68it/s, est. speed input: 156034.64 toks/s, output: 152.38 toks/s]
Processed prompts:  90%|████████▉ | 3682/4096 [00:24<00:03, 114.47it/s, est. speed input: 155579.38 toks/s, output: 151.93 toks/s]
Processed prompts:  91%|█████████ | 3714/4096 [00:24<00:03, 115.30it/s, est. speed input: 155184.11 toks/s, output: 151.55 toks/s]
Processed prompts:  91%|█████████▏| 3746/4096 [00:24<00:03, 115.03it/s, est. speed input: 154755.23 toks/s, output: 151.13 toks/s]
Processed prompts:  92%|█████████▏| 3778/4096 [00:25<00:02, 114.87it/s, est. speed input: 154336.76 toks/s, output: 150.72 toks/s]
Processed prompts:  93%|█████████▎| 3810/4096 [00:25<00:02, 114.56it/s, est. speed input: 153917.75 toks/s, output: 150.31 toks/s]
Processed prompts:  94%|█████████▍| 3842/4096 [00:25<00:02, 115.37it/s, est. speed input: 153558.27 toks/s, output: 149.96 toks/s]
Processed prompts:  95%|█████████▍| 3874/4096 [00:25<00:01, 115.20it/s, est. speed input: 153170.85 toks/s, output: 149.58 toks/s]
Processed prompts:  95%|█████████▌| 3906/4096 [00:26<00:01, 115.04it/s, est. speed input: 152789.63 toks/s, output: 149.21 toks/s]
Processed prompts:  96%|█████████▌| 3938/4096 [00:26<00:01, 114.64it/s, est. speed input: 152403.13 toks/s, output: 148.83 toks/s]
Processed prompts:  97%|█████████▋| 3970/4096 [00:26<00:01, 114.57it/s, est. speed input: 152034.46 toks/s, output: 148.47 toks/s]
Processed prompts:  98%|█████████▊| 4002/4096 [00:27<00:00, 114.54it/s, est. speed input: 151673.94 toks/s, output: 148.12 toks/s]
Processed prompts:  98%|█████████▊| 4034/4096 [00:27<00:00, 115.52it/s, est. speed input: 151366.17 toks/s, output: 147.82 toks/s]
Processed prompts:  99%|█████████▉| 4066/4096 [00:27<00:00, 116.34it/s, est. speed input: 151069.40 toks/s, output: 147.53 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:27<00:00, 116.34it/s, est. speed input: 152178.92 toks/s, output: 148.61 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:27<00:00, 148.61it/s, est. speed input: 152178.92 toks/s, output: 148.61 toks/s]
[rank0]:[W128 08:37:42.161518759 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 91.1s

测试结果:
  Requests/s:   114.58
  Tokens/s:     117446.83
  Total Reqs:   4096
  Elapsed:      35.75s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     117332.25


------------------------------------------------------------
  生成 CSV: BitNet-2B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_INT8_py312_cu129_x86_64/cusparselt/2_4/BitNet-2B-INT8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,31.9580,16394.4428,4.0053
1024,1024,1,128,128,32.8420,33663.0845,3.8974
2048,1024,2,256,128,65.2481,66879.2692,3.9235
4096,1024,4,512,128,93.1897,95519.4903,5.4942
8192,1024,8,1024,128,102.5560,105119.9432,9.9848
16384,1024,16,2048,128,112.2798,115086.7882,18.2401
32768,1024,32,4096,128,114.5823,117446.8321,35.7472

------------------------------------------------------------

[INFO] 完成: 7 成功, 0 失败

============================================================
  BitNet-2B-INT8 | cuSPARSELt (2_6) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_6
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_INT8_py312_cu129_x86_64/cusparselt/2_6

============================================================
[1/7] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 08:37:51 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3316751) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3316751) WARNING 01-28 08:38:06 [backends.py:609] Failed to read file <frozen os>
Throughput: 33.51 requests/s, 17192.33 total tokens/s, 33.51 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-28 08:37:50] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:37:51] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:37:51] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:37:51] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:37:51] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:37:51] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:37:51] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:37:51] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:37:51] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:37:51] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:37:51] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:37:51] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:37:51] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:37:51] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 08:37:57] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:37:57] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:37:57] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:37:57] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:37:57] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:37:57] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:37:57] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:37:57] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:37:57] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:37:57] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:37:57] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:37:57] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:37:57] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:37:57] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3316751) [2026-01-28 08:37:58] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3316751) [2026-01-28 08:37:58] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3316751) [2026-01-28 08:37:58] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3316751) [2026-01-28 08:37:58] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3316751) [2026-01-28 08:37:58] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3316751) [2026-01-28 08:37:58] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3316751) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3316751) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.42it/s]
(EngineCore_DP0 pid=3316751) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.42it/s]
(EngineCore_DP0 pid=3316751) 
(EngineCore_DP0 pid=3316751) [2026-01-28 08:37:59] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=3316751) [2026-01-28 08:37:59] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 8232960 bytes
(EngineCore_DP0 pid=3316751) [2026-01-28 08:37:59] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=3316751) [2026-01-28 08:37:59] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5488640 bytes
(EngineCore_DP0 pid=3316751) [2026-01-28 08:37:59] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=3316751) [2026-01-28 08:37:59] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 29638656 bytes
(EngineCore_DP0 pid=3316751) [2026-01-28 08:37:59] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=3316751) [2026-01-28 08:37:59] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 14745600 bytes
(EngineCore_DP0 pid=3316751) 2026-01-28 08:38:17,367 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3316751) 2026-01-28 08:38:17,398 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3316751) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  4.26it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.32it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.43it/s]
(EngineCore_DP0 pid=3316751) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 16.22it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  45%|████▍     | 57/128 [00:00<00:00, 566.09it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 725.39it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:28,  4.41it/s, est. speed input: 2258.08 toks/s, output: 4.41 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:06, 17.60it/s, est. speed input: 7642.74 toks/s, output: 14.93 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:04, 24.78it/s, est. speed input: 10418.86 toks/s, output: 20.35 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:03, 29.08it/s, est. speed input: 12114.91 toks/s, output: 23.66 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:03, 31.80it/s, est. speed input: 13259.37 toks/s, output: 25.90 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:03, 33.47it/s, est. speed input: 14063.74 toks/s, output: 27.47 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:00<00:02, 34.60it/s, est. speed input: 14670.79 toks/s, output: 28.65 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:00<00:02, 35.34it/s, est. speed input: 15141.17 toks/s, output: 29.57 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:01<00:02, 35.89it/s, est. speed input: 15524.51 toks/s, output: 30.32 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:01<00:02, 36.30it/s, est. speed input: 15842.40 toks/s, output: 30.94 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:01<00:02, 36.54it/s, est. speed input: 16102.73 toks/s, output: 31.45 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:01<00:02, 36.69it/s, est. speed input: 16321.88 toks/s, output: 31.88 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:01<00:02, 36.79it/s, est. speed input: 16508.52 toks/s, output: 32.24 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:01<00:02, 36.86it/s, est. speed input: 16671.26 toks/s, output: 32.56 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:01<00:01, 36.99it/s, est. speed input: 16820.86 toks/s, output: 32.85 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:01<00:01, 37.06it/s, est. speed input: 16951.75 toks/s, output: 33.11 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:01<00:01, 37.12it/s, est. speed input: 17068.53 toks/s, output: 33.34 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:02<00:01, 37.07it/s, est. speed input: 17166.32 toks/s, output: 33.53 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:02<00:01, 37.14it/s, est. speed input: 17261.87 toks/s, output: 33.71 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:02<00:01, 37.22it/s, est. speed input: 17350.53 toks/s, output: 33.89 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:02<00:01, 37.29it/s, est. speed input: 17432.70 toks/s, output: 34.05 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:02<00:01, 37.30it/s, est. speed input: 17504.75 toks/s, output: 34.19 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:02<00:01, 37.27it/s, est. speed input: 17568.98 toks/s, output: 34.31 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:02<00:00, 37.25it/s, est. speed input: 17627.93 toks/s, output: 34.43 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:02<00:00, 37.27it/s, est. speed input: 17684.19 toks/s, output: 34.54 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:02<00:00, 37.25it/s, est. speed input: 17734.77 toks/s, output: 34.64 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:03<00:00, 37.29it/s, est. speed input: 17784.56 toks/s, output: 34.74 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:03<00:00, 37.24it/s, est. speed input: 17826.56 toks/s, output: 34.82 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:03<00:00, 37.20it/s, est. speed input: 17865.27 toks/s, output: 34.89 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:03<00:00, 37.23it/s, est. speed input: 17905.12 toks/s, output: 34.97 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:03<00:00, 37.22it/s, est. speed input: 17940.70 toks/s, output: 35.04 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:03<00:00, 37.23it/s, est. speed input: 17974.90 toks/s, output: 35.11 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 37.23it/s, est. speed input: 17999.38 toks/s, output: 35.15 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 35.15it/s, est. speed input: 17999.38 toks/s, output: 35.15 toks/s]
[rank0]:[W128 08:38:23.650026275 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 41.4s

测试结果:
  Requests/s:   33.51
  Tokens/s:     17192.33
  Total Reqs:   128
  Elapsed:      3.82s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     17158.82

============================================================
[2/7] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 08:38:32 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3317879) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3317879) WARNING 01-28 08:38:47 [backends.py:609] Failed to read file <frozen os>
Throughput: 33.95 requests/s, 34794.51 total tokens/s, 33.95 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-28 08:38:32] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:38:32] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:38:32] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:38:32] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:38:32] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:38:32] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:38:32] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:38:32] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:38:32] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:38:32] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:38:32] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:38:32] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:38:32] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:38:32] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 08:38:39] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:38:39] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:38:39] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:38:39] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:38:39] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:38:39] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:38:39] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:38:39] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:38:39] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:38:39] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:38:39] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:38:39] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:38:39] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:38:39] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3317879) [2026-01-28 08:38:40] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3317879) [2026-01-28 08:38:40] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3317879) [2026-01-28 08:38:40] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3317879) [2026-01-28 08:38:40] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3317879) [2026-01-28 08:38:40] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3317879) [2026-01-28 08:38:40] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3317879) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3317879) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.45it/s]
(EngineCore_DP0 pid=3317879) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.45it/s]
(EngineCore_DP0 pid=3317879) 
(EngineCore_DP0 pid=3317879) [2026-01-28 08:38:41] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=3317879) [2026-01-28 08:38:41] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 8232960 bytes
(EngineCore_DP0 pid=3317879) [2026-01-28 08:38:41] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=3317879) [2026-01-28 08:38:41] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5488640 bytes
(EngineCore_DP0 pid=3317879) [2026-01-28 08:38:41] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=3317879) [2026-01-28 08:38:41] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 29638656 bytes
(EngineCore_DP0 pid=3317879) [2026-01-28 08:38:41] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=3317879) [2026-01-28 08:38:41] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 14745600 bytes
(EngineCore_DP0 pid=3317879) 2026-01-28 08:38:58,471 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3317879) 2026-01-28 08:38:58,498 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3317879) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 10.08it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 10.07it/s]
(EngineCore_DP0 pid=3317879) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 11.82it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  23%|██▎       | 29/128 [00:00<00:00, 285.31it/s]
Adding requests:  63%|██████▎   | 81/128 [00:00<00:00, 420.41it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 430.03it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:03, 39.62it/s, est. speed input: 40580.20 toks/s, output: 39.63 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:03, 37.94it/s, est. speed input: 39134.18 toks/s, output: 38.21 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:03, 37.39it/s, est. speed input: 38635.65 toks/s, output: 37.73 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:02, 37.15it/s, est. speed input: 38399.45 toks/s, output: 37.50 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:02, 36.96it/s, est. speed input: 38228.04 toks/s, output: 37.33 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:00<00:02, 36.85it/s, est. speed input: 38109.52 toks/s, output: 37.21 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:00<00:02, 36.79it/s, est. speed input: 38030.04 toks/s, output: 37.14 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:00<00:02, 36.73it/s, est. speed input: 37964.01 toks/s, output: 37.07 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:00<00:02, 36.75it/s, est. speed input: 37933.17 toks/s, output: 37.04 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:01<00:02, 36.77it/s, est. speed input: 37909.75 toks/s, output: 37.02 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:01<00:02, 36.78it/s, est. speed input: 37888.17 toks/s, output: 37.00 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:01<00:02, 36.77it/s, est. speed input: 37866.03 toks/s, output: 36.98 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:01<00:02, 36.68it/s, est. speed input: 37826.79 toks/s, output: 36.94 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:01<00:01, 36.68it/s, est. speed input: 37809.45 toks/s, output: 36.92 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:01<00:01, 36.70it/s, est. speed input: 37797.59 toks/s, output: 36.91 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:01<00:01, 36.73it/s, est. speed input: 37790.39 toks/s, output: 36.90 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:01<00:01, 36.70it/s, est. speed input: 37774.30 toks/s, output: 36.89 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:01<00:01, 36.67it/s, est. speed input: 37757.81 toks/s, output: 36.87 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:02<00:01, 36.70it/s, est. speed input: 37751.35 toks/s, output: 36.87 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:02<00:01, 36.75it/s, est. speed input: 37752.76 toks/s, output: 36.87 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:02<00:01, 36.82it/s, est. speed input: 37757.04 toks/s, output: 36.87 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:02<00:01, 36.87it/s, est. speed input: 37762.44 toks/s, output: 36.88 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:02<00:00, 36.90it/s, est. speed input: 37766.35 toks/s, output: 36.88 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:02<00:00, 36.91it/s, est. speed input: 37768.25 toks/s, output: 36.88 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:02<00:00, 36.93it/s, est. speed input: 37772.20 toks/s, output: 36.89 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:02<00:00, 36.86it/s, est. speed input: 37765.07 toks/s, output: 36.88 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:02<00:00, 36.87it/s, est. speed input: 37765.89 toks/s, output: 36.88 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:03<00:00, 36.79it/s, est. speed input: 37755.67 toks/s, output: 36.87 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:03<00:00, 36.84it/s, est. speed input: 37758.60 toks/s, output: 36.87 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:03<00:00, 36.84it/s, est. speed input: 37757.56 toks/s, output: 36.87 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:03<00:00, 36.86it/s, est. speed input: 37758.93 toks/s, output: 36.87 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 36.86it/s, est. speed input: 37759.48 toks/s, output: 36.87 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 36.87it/s, est. speed input: 37759.48 toks/s, output: 36.87 toks/s]
[rank0]:[W128 08:39:04.372404508 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 40.7s

测试结果:
  Requests/s:   33.95
  Tokens/s:     34794.51
  Total Reqs:   128
  Elapsed:      3.77s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     34760.57

============================================================
[3/7] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 08:39:13 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3318963) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3318963) WARNING 01-28 08:39:28 [backends.py:609] Failed to read file <frozen os>
Throughput: 62.12 requests/s, 63672.50 total tokens/s, 62.12 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-28 08:39:13] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:39:13] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:39:13] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:39:13] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:39:13] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:39:13] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:39:13] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:39:13] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:39:13] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:39:13] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:39:13] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:39:13] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:39:13] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:39:13] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 08:39:20] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:39:20] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:39:20] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:39:20] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:39:20] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:39:20] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:39:20] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:39:20] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:39:20] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:39:20] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:39:20] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:39:20] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:39:20] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:39:20] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3318963) [2026-01-28 08:39:21] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3318963) [2026-01-28 08:39:21] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3318963) [2026-01-28 08:39:21] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3318963) [2026-01-28 08:39:21] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3318963) [2026-01-28 08:39:21] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3318963) [2026-01-28 08:39:21] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3318963) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3318963) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.46it/s]
(EngineCore_DP0 pid=3318963) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.46it/s]
(EngineCore_DP0 pid=3318963) 
(EngineCore_DP0 pid=3318963) [2026-01-28 08:39:22] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=3318963) [2026-01-28 08:39:22] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 8232960 bytes
(EngineCore_DP0 pid=3318963) [2026-01-28 08:39:22] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=3318963) [2026-01-28 08:39:22] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5488640 bytes
(EngineCore_DP0 pid=3318963) [2026-01-28 08:39:22] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=3318963) [2026-01-28 08:39:22] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 29638656 bytes
(EngineCore_DP0 pid=3318963) [2026-01-28 08:39:22] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=3318963) [2026-01-28 08:39:22] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 14745600 bytes
(EngineCore_DP0 pid=3318963) 2026-01-28 08:39:39,797 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3318963) 2026-01-28 08:39:39,834 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3318963) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 1/3 [00:00<00:00,  8.73it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 11.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 11.15it/s]
(EngineCore_DP0 pid=3318963) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 14.38it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 14.37it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  11%|█▏        | 29/256 [00:00<00:00, 286.69it/s]
Adding requests:  32%|███▏      | 81/256 [00:00<00:00, 421.25it/s]
Adding requests:  51%|█████     | 130/256 [00:00<00:00, 452.40it/s]
Adding requests:  70%|██████▉   | 178/256 [00:00<00:00, 462.26it/s]
Adding requests:  89%|████████▉ | 229/256 [00:00<00:00, 478.50it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 459.99it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   6%|▋         | 16/256 [00:00<00:01, 130.48it/s, est. speed input: 133627.37 toks/s, output: 130.48 toks/s]
Processed prompts:  12%|█▏        | 30/256 [00:00<00:02, 88.79it/s, est. speed input: 95833.80 toks/s, output: 93.58 toks/s]   
Processed prompts:  16%|█▌        | 40/256 [00:00<00:02, 81.50it/s, est. speed input: 88844.54 toks/s, output: 86.76 toks/s]
Processed prompts:  19%|█▉        | 49/256 [00:00<00:02, 80.89it/s, est. speed input: 87400.78 toks/s, output: 85.35 toks/s]
Processed prompts:  23%|██▎       | 58/256 [00:00<00:02, 74.52it/s, est. speed input: 82968.22 toks/s, output: 81.02 toks/s]
Processed prompts:  26%|██▌       | 66/256 [00:00<00:02, 73.49it/s, est. speed input: 81574.75 toks/s, output: 79.66 toks/s]
Processed prompts:  29%|██▉       | 74/256 [00:00<00:02, 72.69it/s, est. speed input: 80486.48 toks/s, output: 78.60 toks/s]
Processed prompts:  32%|███▏      | 82/256 [00:01<00:02, 72.03it/s, est. speed input: 79590.66 toks/s, output: 77.72 toks/s]
Processed prompts:  35%|███▌      | 90/256 [00:01<00:02, 71.77it/s, est. speed input: 78941.09 toks/s, output: 77.09 toks/s]
Processed prompts:  38%|███▊      | 98/256 [00:01<00:02, 71.47it/s, est. speed input: 78369.08 toks/s, output: 76.53 toks/s]
Processed prompts:  41%|████▏     | 106/256 [00:01<00:02, 71.39it/s, est. speed input: 77928.56 toks/s, output: 76.10 toks/s]
Processed prompts:  45%|████▍     | 114/256 [00:01<00:01, 71.42it/s, est. speed input: 77578.19 toks/s, output: 75.76 toks/s]
Processed prompts:  48%|████▊     | 122/256 [00:01<00:01, 71.17it/s, est. speed input: 77207.02 toks/s, output: 75.40 toks/s]
Processed prompts:  51%|█████     | 130/256 [00:01<00:01, 70.45it/s, est. speed input: 76755.96 toks/s, output: 74.96 toks/s]
Processed prompts:  54%|█████▍    | 138/256 [00:01<00:01, 69.96it/s, est. speed input: 76362.85 toks/s, output: 74.57 toks/s]
Processed prompts:  57%|█████▋    | 146/256 [00:01<00:01, 69.68it/s, est. speed input: 76027.18 toks/s, output: 74.24 toks/s]
Processed prompts:  60%|██████    | 154/256 [00:02<00:01, 69.39it/s, est. speed input: 75711.47 toks/s, output: 73.94 toks/s]
Processed prompts:  63%|██████▎   | 162/256 [00:02<00:01, 69.12it/s, est. speed input: 75415.70 toks/s, output: 73.65 toks/s]
Processed prompts:  66%|██████▋   | 170/256 [00:02<00:01, 68.97it/s, est. speed input: 75156.46 toks/s, output: 73.39 toks/s]
Processed prompts:  70%|██████▉   | 178/256 [00:02<00:01, 69.02it/s, est. speed input: 74948.84 toks/s, output: 73.19 toks/s]
Processed prompts:  73%|███████▎  | 186/256 [00:02<00:01, 69.04it/s, est. speed input: 74758.18 toks/s, output: 73.01 toks/s]
Processed prompts:  76%|███████▌  | 194/256 [00:02<00:00, 69.02it/s, est. speed input: 74578.39 toks/s, output: 72.83 toks/s]
Processed prompts:  79%|███████▉  | 202/256 [00:02<00:00, 69.01it/s, est. speed input: 74413.48 toks/s, output: 72.67 toks/s]
Processed prompts:  82%|████████▏ | 210/256 [00:02<00:00, 69.00it/s, est. speed input: 74262.76 toks/s, output: 72.52 toks/s]
Processed prompts:  85%|████████▌ | 218/256 [00:03<00:00, 69.02it/s, est. speed input: 74125.87 toks/s, output: 72.39 toks/s]
Processed prompts:  88%|████████▊ | 226/256 [00:03<00:00, 68.97it/s, est. speed input: 73991.22 toks/s, output: 72.26 toks/s]
Processed prompts:  91%|█████████▏| 234/256 [00:03<00:00, 68.87it/s, est. speed input: 73858.32 toks/s, output: 72.13 toks/s]
Processed prompts:  95%|█████████▍| 242/256 [00:03<00:00, 68.87it/s, est. speed input: 73743.22 toks/s, output: 72.01 toks/s]
Processed prompts:  98%|█████████▊| 250/256 [00:03<00:00, 68.96it/s, est. speed input: 73646.73 toks/s, output: 71.92 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 68.96it/s, est. speed input: 73582.61 toks/s, output: 71.86 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 71.85it/s, est. speed input: 73582.61 toks/s, output: 71.86 toks/s]
[rank0]:[W128 08:39:46.255226997 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 42.0s

测试结果:
  Requests/s:   62.12
  Tokens/s:     63672.50
  Total Reqs:   256
  Elapsed:      4.12s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     63610.38

============================================================
[4/7] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 08:39:57 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3320077) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3320077) WARNING 01-28 08:40:12 [backends.py:609] Failed to read file <frozen os>
Throughput: 85.88 requests/s, 88024.83 total tokens/s, 85.88 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-28 08:39:56] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:39:56] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:39:57] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:39:57] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:39:57] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:39:57] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:39:57] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:39:57] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:39:57] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:39:57] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:39:57] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:39:57] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:39:57] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:39:57] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 08:40:03] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:40:04] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:40:04] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:40:04] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:40:04] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:40:04] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:40:04] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:40:04] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:40:04] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:40:04] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:40:04] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:40:04] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:40:04] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:40:04] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3320077) [2026-01-28 08:40:04] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3320077) [2026-01-28 08:40:04] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3320077) [2026-01-28 08:40:04] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3320077) [2026-01-28 08:40:04] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3320077) [2026-01-28 08:40:04] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3320077) [2026-01-28 08:40:04] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3320077) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3320077) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.45it/s]
(EngineCore_DP0 pid=3320077) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.45it/s]
(EngineCore_DP0 pid=3320077) 
(EngineCore_DP0 pid=3320077) [2026-01-28 08:40:06] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=3320077) [2026-01-28 08:40:06] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 8232960 bytes
(EngineCore_DP0 pid=3320077) [2026-01-28 08:40:06] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=3320077) [2026-01-28 08:40:06] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5488640 bytes
(EngineCore_DP0 pid=3320077) [2026-01-28 08:40:06] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=3320077) [2026-01-28 08:40:06] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 29638656 bytes
(EngineCore_DP0 pid=3320077) [2026-01-28 08:40:06] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=3320077) [2026-01-28 08:40:06] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 14745600 bytes
(EngineCore_DP0 pid=3320077) 2026-01-28 08:40:23,037 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3320077) 2026-01-28 08:40:23,064 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3320077) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 1/4 [00:00<00:00,  9.86it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 3/4 [00:00<00:00, 14.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 13.63it/s]
(EngineCore_DP0 pid=3320077) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  33%|███▎      | 1/3 [00:00<00:00,  3.98it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00,  8.30it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00,  7.49it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   6%|▌         | 29/512 [00:00<00:01, 285.43it/s]
Adding requests:  16%|█▌        | 82/512 [00:00<00:01, 425.33it/s]
Adding requests:  26%|██▌       | 133/512 [00:00<00:00, 463.14it/s]
Adding requests:  36%|███▌      | 182/512 [00:00<00:00, 473.10it/s]
Adding requests:  46%|████▌     | 234/512 [00:00<00:00, 486.36it/s]
Adding requests:  56%|█████▌    | 285/512 [00:00<00:00, 492.22it/s]
Adding requests:  65%|██████▌   | 335/512 [00:00<00:00, 490.77it/s]
Adding requests:  76%|███████▌  | 387/512 [00:00<00:00, 497.49it/s]
Adding requests:  86%|████████▌ | 438/512 [00:00<00:00, 500.27it/s]
Adding requests:  96%|█████████▌| 489/512 [00:01<00:00, 499.90it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 483.01it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  14%|█▎        | 70/512 [00:00<00:00, 511.61it/s, est. speed input: 523964.54 toks/s, output: 511.63 toks/s]
Processed prompts:  24%|██▍       | 122/512 [00:00<00:02, 153.92it/s, est. speed input: 179191.24 toks/s, output: 174.98 toks/s]
Processed prompts:  29%|██▉       | 149/512 [00:00<00:02, 135.36it/s, est. speed input: 158991.17 toks/s, output: 155.26 toks/s]
Processed prompts:  33%|███▎      | 169/512 [00:01<00:02, 122.21it/s, est. speed input: 146942.31 toks/s, output: 143.50 toks/s]
Processed prompts:  36%|███▌      | 185/512 [00:01<00:02, 114.52it/s, est. speed input: 140178.02 toks/s, output: 136.89 toks/s]
Processed prompts:  39%|███▉      | 199/512 [00:01<00:02, 105.55it/s, est. speed input: 133666.23 toks/s, output: 130.53 toks/s]
Processed prompts:  41%|████      | 211/512 [00:01<00:02, 102.51it/s, est. speed input: 130548.41 toks/s, output: 127.49 toks/s]
Processed prompts:  43%|████▎     | 222/512 [00:01<00:02, 98.33it/s, est. speed input: 127406.47 toks/s, output: 124.42 toks/s] 
Processed prompts:  46%|████▌     | 234/512 [00:01<00:02, 96.47it/s, est. speed input: 125062.88 toks/s, output: 122.13 toks/s]
Processed prompts:  48%|████▊     | 246/512 [00:02<00:02, 95.08it/s, est. speed input: 123036.82 toks/s, output: 120.15 toks/s]
Processed prompts:  50%|█████     | 258/512 [00:02<00:02, 94.26it/s, est. speed input: 121319.02 toks/s, output: 118.47 toks/s]
Processed prompts:  53%|█████▎    | 270/512 [00:02<00:02, 93.50it/s, est. speed input: 119757.40 toks/s, output: 116.95 toks/s]
Processed prompts:  55%|█████▌    | 282/512 [00:02<00:02, 93.23it/s, est. speed input: 118429.11 toks/s, output: 115.65 toks/s]
Processed prompts:  57%|█████▋    | 294/512 [00:02<00:02, 92.97it/s, est. speed input: 117220.96 toks/s, output: 114.47 toks/s]
Processed prompts:  60%|█████▉    | 306/512 [00:02<00:02, 92.84it/s, est. speed input: 116140.42 toks/s, output: 113.42 toks/s]
Processed prompts:  62%|██████▏   | 318/512 [00:02<00:02, 92.47it/s, est. speed input: 115105.86 toks/s, output: 112.41 toks/s]
Processed prompts:  64%|██████▍   | 330/512 [00:02<00:01, 92.29it/s, est. speed input: 114176.27 toks/s, output: 111.50 toks/s]
Processed prompts:  67%|██████▋   | 342/512 [00:03<00:01, 93.06it/s, est. speed input: 113482.20 toks/s, output: 110.82 toks/s]
Processed prompts:  69%|██████▉   | 354/512 [00:03<00:01, 92.91it/s, est. speed input: 112727.78 toks/s, output: 110.08 toks/s]
Processed prompts:  71%|███████▏  | 366/512 [00:03<00:01, 92.61it/s, est. speed input: 112002.87 toks/s, output: 109.38 toks/s]
Processed prompts:  74%|███████▍  | 378/512 [00:03<00:01, 92.48it/s, est. speed input: 111342.23 toks/s, output: 108.73 toks/s]
Processed prompts:  76%|███████▌  | 390/512 [00:03<00:01, 92.35it/s, est. speed input: 110725.03 toks/s, output: 108.13 toks/s]
Processed prompts:  79%|███████▊  | 402/512 [00:03<00:01, 92.35it/s, est. speed input: 110162.26 toks/s, output: 107.58 toks/s]
Processed prompts:  81%|████████  | 414/512 [00:03<00:01, 92.20it/s, est. speed input: 109618.56 toks/s, output: 107.05 toks/s]
Processed prompts:  83%|████████▎ | 426/512 [00:03<00:00, 92.07it/s, est. speed input: 109106.75 toks/s, output: 106.55 toks/s]
Processed prompts:  86%|████████▌ | 438/512 [00:04<00:00, 91.83it/s, est. speed input: 108608.11 toks/s, output: 106.06 toks/s]
Processed prompts:  88%|████████▊ | 450/512 [00:04<00:00, 93.07it/s, est. speed input: 108309.12 toks/s, output: 105.77 toks/s]
Processed prompts:  90%|█████████ | 462/512 [00:04<00:00, 92.97it/s, est. speed input: 107914.71 toks/s, output: 105.38 toks/s]
Processed prompts:  93%|█████████▎| 474/512 [00:04<00:00, 92.66it/s, est. speed input: 107517.51 toks/s, output: 105.00 toks/s]
Processed prompts:  95%|█████████▍| 486/512 [00:04<00:00, 92.51it/s, est. speed input: 107148.90 toks/s, output: 104.64 toks/s]
Processed prompts:  97%|█████████▋| 498/512 [00:04<00:00, 92.17it/s, est. speed input: 106775.80 toks/s, output: 104.27 toks/s]
Processed prompts: 100%|█████████▉| 510/512 [00:04<00:00, 93.58it/s, est. speed input: 106589.29 toks/s, output: 104.09 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:04<00:00, 93.58it/s, est. speed input: 107000.80 toks/s, output: 104.49 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:04<00:00, 104.49it/s, est. speed input: 107000.80 toks/s, output: 104.49 toks/s]
[rank0]:[W128 08:40:31.535240741 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 45.2s

测试结果:
  Requests/s:   85.88
  Tokens/s:     88024.83
  Total Reqs:   512
  Elapsed:      5.96s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     87938.95

============================================================
[5/7] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 08:40:44 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3321211) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3321211) WARNING 01-28 08:40:59 [backends.py:609] Failed to read file <frozen os>
Throughput: 94.85 requests/s, 97225.84 total tokens/s, 94.85 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-28 08:40:43] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:40:44] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:40:44] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:40:44] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:40:44] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:40:44] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:40:44] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:40:44] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:40:44] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:40:44] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:40:44] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:40:44] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:40:44] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:40:44] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 08:40:50] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:40:50] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:40:50] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:40:50] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:40:50] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:40:50] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:40:50] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:40:50] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:40:50] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:40:50] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:40:50] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:40:50] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:40:50] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:40:50] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3321211) [2026-01-28 08:40:52] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3321211) [2026-01-28 08:40:52] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3321211) [2026-01-28 08:40:52] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3321211) [2026-01-28 08:40:52] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3321211) [2026-01-28 08:40:52] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3321211) [2026-01-28 08:40:52] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3321211) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3321211) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.38it/s]
(EngineCore_DP0 pid=3321211) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.38it/s]
(EngineCore_DP0 pid=3321211) 
(EngineCore_DP0 pid=3321211) [2026-01-28 08:40:52] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=3321211) [2026-01-28 08:40:52] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 8232960 bytes
(EngineCore_DP0 pid=3321211) [2026-01-28 08:40:52] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=3321211) [2026-01-28 08:40:52] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5488640 bytes
(EngineCore_DP0 pid=3321211) [2026-01-28 08:40:52] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=3321211) [2026-01-28 08:40:52] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 29638656 bytes
(EngineCore_DP0 pid=3321211) [2026-01-28 08:40:52] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=3321211) [2026-01-28 08:40:52] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 14745600 bytes
(EngineCore_DP0 pid=3321211) 2026-01-28 08:41:09,904 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3321211) 2026-01-28 08:41:09,931 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3321211) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 1/5 [00:00<00:01,  2.59it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00,  4.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00,  8.42it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00,  7.31it/s]
(EngineCore_DP0 pid=3321211) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00, 17.09it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 17.85it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 17.72it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   3%|▎         | 28/1024 [00:00<00:03, 279.70it/s]
Adding requests:   8%|▊         | 80/1024 [00:00<00:02, 416.99it/s]
Adding requests:  13%|█▎        | 131/1024 [00:00<00:01, 456.59it/s]
Adding requests:  18%|█▊        | 180/1024 [00:00<00:01, 466.75it/s]
Adding requests:  23%|██▎       | 232/1024 [00:00<00:01, 482.15it/s]
Adding requests:  28%|██▊       | 283/1024 [00:00<00:01, 488.85it/s]
Adding requests:  33%|███▎      | 333/1024 [00:00<00:01, 489.79it/s]
Adding requests:  38%|███▊      | 384/1024 [00:00<00:01, 494.35it/s]
Adding requests:  42%|████▏     | 435/1024 [00:00<00:01, 496.96it/s]
Adding requests:  47%|████▋     | 486/1024 [00:01<00:01, 499.78it/s]
Adding requests:  52%|█████▏    | 536/1024 [00:01<00:01, 483.68it/s]
Adding requests:  58%|█████▊    | 589/1024 [00:01<00:00, 496.02it/s]
Adding requests:  62%|██████▎   | 640/1024 [00:01<00:00, 500.08it/s]
Adding requests:  68%|██████▊   | 693/1024 [00:01<00:00, 507.55it/s]
Adding requests:  73%|███████▎  | 744/1024 [00:01<00:00, 504.61it/s]
Adding requests:  78%|███████▊  | 795/1024 [00:01<00:00, 503.13it/s]
Adding requests:  83%|████████▎ | 846/1024 [00:01<00:00, 495.23it/s]
Adding requests:  88%|████████▊ | 899/1024 [00:01<00:00, 505.11it/s]
Adding requests:  93%|█████████▎| 950/1024 [00:01<00:00, 505.65it/s]
Adding requests:  98%|█████████▊| 1002/1024 [00:02<00:00, 508.04it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 492.00it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  19%|█▉        | 194/1024 [00:00<00:00, 1845.72it/s, est. speed input: 1890463.45 toks/s, output: 1845.85 toks/s]
Processed prompts:  37%|███▋      | 379/1024 [00:02<00:04, 161.12it/s, est. speed input: 191877.55 toks/s, output: 187.38 toks/s]   
Processed prompts:  45%|████▌     | 461/1024 [00:02<00:04, 137.76it/s, est. speed input: 165391.06 toks/s, output: 161.51 toks/s]
Processed prompts:  50%|████▉     | 510/1024 [00:03<00:04, 127.83it/s, est. speed input: 155496.76 toks/s, output: 151.85 toks/s]
Processed prompts:  53%|█████▎    | 544/1024 [00:03<00:03, 122.91it/s, est. speed input: 150846.76 toks/s, output: 147.31 toks/s]
Processed prompts:  56%|█████▌    | 570/1024 [00:04<00:04, 113.26it/s, est. speed input: 144888.73 toks/s, output: 141.49 toks/s]
Processed prompts:  58%|█████▊    | 590/1024 [00:04<00:03, 113.98it/s, est. speed input: 143949.58 toks/s, output: 140.58 toks/s]
Processed prompts:  59%|█████▉    | 607/1024 [00:04<00:03, 111.87it/s, est. speed input: 142374.46 toks/s, output: 139.04 toks/s]
Processed prompts:  61%|██████    | 622/1024 [00:04<00:03, 107.67it/s, est. speed input: 140476.15 toks/s, output: 137.18 toks/s]
Processed prompts:  62%|██████▏   | 635/1024 [00:04<00:03, 101.52it/s, est. speed input: 138326.40 toks/s, output: 135.08 toks/s]
Processed prompts:  63%|██████▎   | 650/1024 [00:04<00:03, 98.67it/s, est. speed input: 136702.04 toks/s, output: 133.50 toks/s] 
Processed prompts:  65%|██████▌   | 666/1024 [00:05<00:03, 97.80it/s, est. speed input: 135390.44 toks/s, output: 132.22 toks/s]
Processed prompts:  67%|██████▋   | 682/1024 [00:05<00:03, 97.11it/s, est. speed input: 134164.97 toks/s, output: 131.02 toks/s]
Processed prompts:  68%|██████▊   | 698/1024 [00:05<00:03, 96.43it/s, est. speed input: 132992.16 toks/s, output: 129.87 toks/s]
Processed prompts:  70%|██████▉   | 714/1024 [00:05<00:03, 96.15it/s, est. speed input: 131924.80 toks/s, output: 128.83 toks/s]
Processed prompts:  71%|███████▏  | 730/1024 [00:05<00:03, 96.05it/s, est. speed input: 130935.51 toks/s, output: 127.87 toks/s]
Processed prompts:  73%|███████▎  | 746/1024 [00:05<00:02, 95.81it/s, est. speed input: 129979.00 toks/s, output: 126.93 toks/s]
Processed prompts:  74%|███████▍  | 762/1024 [00:06<00:02, 95.71it/s, est. speed input: 129086.18 toks/s, output: 126.06 toks/s]
Processed prompts:  76%|███████▌  | 778/1024 [00:06<00:02, 95.60it/s, est. speed input: 128235.52 toks/s, output: 125.23 toks/s]
Processed prompts:  78%|███████▊  | 794/1024 [00:06<00:02, 95.38it/s, est. speed input: 127414.30 toks/s, output: 124.43 toks/s]
Processed prompts:  79%|███████▉  | 810/1024 [00:06<00:02, 95.43it/s, est. speed input: 126658.23 toks/s, output: 123.69 toks/s]
Processed prompts:  81%|████████  | 826/1024 [00:06<00:02, 95.32it/s, est. speed input: 125923.02 toks/s, output: 122.97 toks/s]
Processed prompts:  82%|████████▏ | 842/1024 [00:06<00:01, 95.21it/s, est. speed input: 125221.53 toks/s, output: 122.29 toks/s]
Processed prompts:  84%|████████▍ | 858/1024 [00:07<00:01, 95.25it/s, est. speed input: 124564.83 toks/s, output: 121.64 toks/s]
Processed prompts:  85%|████████▌ | 874/1024 [00:07<00:01, 95.17it/s, est. speed input: 123927.54 toks/s, output: 121.02 toks/s]
Processed prompts:  87%|████████▋ | 890/1024 [00:07<00:01, 95.20it/s, est. speed input: 123328.62 toks/s, output: 120.44 toks/s]
Processed prompts:  88%|████████▊ | 906/1024 [00:07<00:01, 95.08it/s, est. speed input: 122742.65 toks/s, output: 119.87 toks/s]
Processed prompts:  90%|█████████ | 922/1024 [00:07<00:01, 95.25it/s, est. speed input: 122205.89 toks/s, output: 119.34 toks/s]
Processed prompts:  92%|█████████▏| 938/1024 [00:07<00:00, 96.50it/s, est. speed input: 121792.79 toks/s, output: 118.94 toks/s]
Processed prompts:  93%|█████████▎| 954/1024 [00:08<00:00, 96.16it/s, est. speed input: 121289.55 toks/s, output: 118.45 toks/s]
Processed prompts:  95%|█████████▍| 970/1024 [00:08<00:00, 95.81it/s, est. speed input: 120797.87 toks/s, output: 117.97 toks/s]
Processed prompts:  96%|█████████▋| 986/1024 [00:08<00:00, 97.03it/s, est. speed input: 120446.69 toks/s, output: 117.62 toks/s]
Processed prompts:  98%|█████████▊| 1002/1024 [00:08<00:00, 96.41it/s, est. speed input: 119990.19 toks/s, output: 117.18 toks/s]
Processed prompts:  99%|█████████▉| 1018/1024 [00:08<00:00, 97.37it/s, est. speed input: 119660.15 toks/s, output: 116.86 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:08<00:00, 97.37it/s, est. speed input: 120360.09 toks/s, output: 117.54 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:08<00:00, 117.54it/s, est. speed input: 120360.09 toks/s, output: 117.54 toks/s]
[rank0]:[W128 08:41:23.470293093 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 52.0s

测试结果:
  Requests/s:   94.85
  Tokens/s:     97225.84
  Total Reqs:   1024
  Elapsed:      10.80s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     97130.98

============================================================
[6/7] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 08:41:40 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3322491) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3322491) WARNING 01-28 08:41:55 [backends.py:609] Failed to read file <frozen os>
Throughput: 98.07 requests/s, 100521.36 total tokens/s, 98.07 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-28 08:41:40] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:41:40] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:41:40] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:41:40] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:41:40] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:41:40] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:41:40] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:41:40] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:41:40] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:41:40] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:41:40] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:41:40] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:41:40] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:41:40] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 08:41:46] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:41:47] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:41:47] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:41:47] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:41:47] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:41:47] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:41:47] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:41:47] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:41:47] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:41:47] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:41:47] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:41:47] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:41:47] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:41:47] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3322491) [2026-01-28 08:41:48] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3322491) [2026-01-28 08:41:48] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3322491) [2026-01-28 08:41:48] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3322491) [2026-01-28 08:41:48] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3322491) [2026-01-28 08:41:48] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3322491) [2026-01-28 08:41:48] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3322491) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3322491) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.48it/s]
(EngineCore_DP0 pid=3322491) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.48it/s]
(EngineCore_DP0 pid=3322491) 
(EngineCore_DP0 pid=3322491) [2026-01-28 08:41:49] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=3322491) [2026-01-28 08:41:49] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 8232960 bytes
(EngineCore_DP0 pid=3322491) [2026-01-28 08:41:49] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=3322491) [2026-01-28 08:41:49] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5488640 bytes
(EngineCore_DP0 pid=3322491) [2026-01-28 08:41:49] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=3322491) [2026-01-28 08:41:49] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 29638656 bytes
(EngineCore_DP0 pid=3322491) [2026-01-28 08:41:49] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=3322491) [2026-01-28 08:41:49] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 14745600 bytes
(EngineCore_DP0 pid=3322491) 2026-01-28 08:42:06,060 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3322491) 2026-01-28 08:42:06,087 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3322491) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 1/7 [00:00<00:00,  9.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 3/7 [00:00<00:00, 13.96it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 5/7 [00:00<00:00, 15.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 15.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 14.80it/s]
(EngineCore_DP0 pid=3322491) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00,  7.05it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 3/5 [00:00<00:00,  7.79it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 10.86it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00,  9.68it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 33/2048 [00:00<00:06, 324.31it/s]
Adding requests:   4%|▍         | 85/2048 [00:00<00:04, 436.57it/s]
Adding requests:   7%|▋         | 135/2048 [00:00<00:04, 462.76it/s]
Adding requests:   9%|▉         | 184/2048 [00:00<00:03, 472.56it/s]
Adding requests:  11%|█▏        | 235/2048 [00:00<00:03, 485.71it/s]
Adding requests:  14%|█▍        | 285/2048 [00:00<00:03, 489.82it/s]
Adding requests:  16%|█▋        | 334/2048 [00:00<00:03, 485.65it/s]
Adding requests:  19%|█▉        | 385/2048 [00:00<00:03, 491.28it/s]
Adding requests:  21%|██▏       | 436/2048 [00:00<00:03, 497.02it/s]
Adding requests:  24%|██▍       | 487/2048 [00:01<00:03, 500.12it/s]
Adding requests:  26%|██▋       | 538/2048 [00:01<00:03, 489.68it/s]
Adding requests:  29%|██▉       | 590/2048 [00:01<00:02, 498.25it/s]
Adding requests:  31%|███▏      | 641/2048 [00:01<00:02, 500.08it/s]
Adding requests:  34%|███▍      | 694/2048 [00:01<00:02, 507.24it/s]
Adding requests:  36%|███▋      | 745/2048 [00:01<00:02, 496.36it/s]
Adding requests:  39%|███▉      | 795/2048 [00:01<00:02, 495.85it/s]
Adding requests:  41%|████▏     | 845/2048 [00:01<00:02, 490.16it/s]
Adding requests:  44%|████▍     | 899/2048 [00:01<00:02, 502.15it/s]
Adding requests:  46%|████▋     | 951/2048 [00:01<00:02, 505.23it/s]
Adding requests:  49%|████▉     | 1003/2048 [00:02<00:02, 509.57it/s]
Adding requests:  52%|█████▏    | 1055/2048 [00:02<00:01, 510.62it/s]
Adding requests:  54%|█████▍    | 1107/2048 [00:02<00:01, 508.91it/s]
Adding requests:  57%|█████▋    | 1159/2048 [00:02<00:01, 510.09it/s]
Adding requests:  59%|█████▉    | 1213/2048 [00:02<00:01, 518.51it/s]
Adding requests:  62%|██████▏   | 1265/2048 [00:02<00:01, 510.92it/s]
Adding requests:  64%|██████▍   | 1317/2048 [00:02<00:01, 510.22it/s]
Adding requests:  67%|██████▋   | 1369/2048 [00:02<00:01, 512.36it/s]
Adding requests:  69%|██████▉   | 1421/2048 [00:02<00:01, 511.84it/s]
Adding requests:  72%|███████▏  | 1473/2048 [00:02<00:01, 512.73it/s]
Adding requests:  75%|███████▍  | 1526/2048 [00:03<00:01, 517.15it/s]
Adding requests:  77%|███████▋  | 1579/2048 [00:03<00:00, 518.93it/s]
Adding requests:  80%|███████▉  | 1633/2048 [00:03<00:00, 523.03it/s]
Adding requests:  82%|████████▏ | 1686/2048 [00:03<00:00, 518.62it/s]
Adding requests:  85%|████████▍ | 1739/2048 [00:03<00:00, 520.75it/s]
Adding requests:  88%|████████▊ | 1792/2048 [00:03<00:00, 517.68it/s]
Adding requests:  90%|█████████ | 1845/2048 [00:03<00:00, 519.60it/s]
Adding requests:  93%|█████████▎| 1897/2048 [00:03<00:00, 503.20it/s]
Adding requests:  95%|█████████▌| 1948/2048 [00:03<00:00, 504.70it/s]
Adding requests:  98%|█████████▊| 2000/2048 [00:03<00:00, 508.40it/s]
Adding requests: 100%|██████████| 2048/2048 [00:04<00:00, 502.83it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  20%|█▉        | 402/2048 [00:00<00:01, 1598.22it/s, est. speed input: 1636869.84 toks/s, output: 1598.30 toks/s]
Processed prompts:  27%|██▋       | 562/2048 [00:01<00:06, 246.64it/s, est. speed input: 308562.51 toks/s, output: 301.32 toks/s]   
Processed prompts:  31%|███       | 634/2048 [00:02<00:07, 199.68it/s, est. speed input: 257598.81 toks/s, output: 251.56 toks/s]
Processed prompts:  33%|███▎      | 678/2048 [00:03<00:08, 170.57it/s, est. speed input: 230949.63 toks/s, output: 225.54 toks/s]
Processed prompts:  35%|███▍      | 709/2048 [00:03<00:08, 154.95it/s, est. speed input: 217712.85 toks/s, output: 212.61 toks/s]
Processed prompts:  36%|███▌      | 732/2048 [00:03<00:08, 153.39it/s, est. speed input: 214398.68 toks/s, output: 209.37 toks/s]
Processed prompts:  37%|███▋      | 752/2048 [00:03<00:08, 148.87it/s, est. speed input: 210523.18 toks/s, output: 205.59 toks/s]
Processed prompts:  38%|███▊      | 770/2048 [00:03<00:10, 120.84it/s, est. speed input: 198167.76 toks/s, output: 193.52 toks/s]
Processed prompts:  38%|███▊      | 786/2048 [00:04<00:10, 116.70it/s, est. speed input: 194299.74 toks/s, output: 189.74 toks/s]
Processed prompts:  39%|███▉      | 802/2048 [00:04<00:11, 112.80it/s, est. speed input: 190714.50 toks/s, output: 186.24 toks/s]
Processed prompts:  40%|███▉      | 818/2048 [00:04<00:11, 109.41it/s, est. speed input: 187401.05 toks/s, output: 183.01 toks/s]
Processed prompts:  41%|████      | 834/2048 [00:04<00:11, 106.76it/s, est. speed input: 184365.91 toks/s, output: 180.04 toks/s]
Processed prompts:  42%|████▏     | 850/2048 [00:04<00:11, 104.86it/s, est. speed input: 181589.29 toks/s, output: 177.33 toks/s]
Processed prompts:  42%|████▏     | 866/2048 [00:04<00:11, 103.24it/s, est. speed input: 178962.66 toks/s, output: 174.77 toks/s]
Processed prompts:  43%|████▎     | 882/2048 [00:05<00:11, 101.93it/s, est. speed input: 176485.85 toks/s, output: 172.35 toks/s]
Processed prompts:  44%|████▍     | 898/2048 [00:05<00:11, 100.87it/s, est. speed input: 174142.49 toks/s, output: 170.06 toks/s]
Processed prompts:  45%|████▍     | 914/2048 [00:05<00:11, 100.02it/s, est. speed input: 171928.18 toks/s, output: 167.90 toks/s]
Processed prompts:  45%|████▌     | 930/2048 [00:05<00:11, 100.60it/s, est. speed input: 170038.97 toks/s, output: 166.05 toks/s]
Processed prompts:  46%|████▌     | 946/2048 [00:05<00:11, 100.12it/s, est. speed input: 168112.13 toks/s, output: 164.17 toks/s]
Processed prompts:  47%|████▋     | 962/2048 [00:05<00:10, 99.53it/s, est. speed input: 166252.19 toks/s, output: 162.35 toks/s] 
Processed prompts:  48%|████▊     | 978/2048 [00:06<00:10, 100.43it/s, est. speed input: 164684.97 toks/s, output: 160.82 toks/s]
Processed prompts:  49%|████▊     | 994/2048 [00:06<00:10, 99.61it/s, est. speed input: 162989.92 toks/s, output: 159.17 toks/s] 
Processed prompts:  49%|████▉     | 1010/2048 [00:06<00:10, 99.11it/s, est. speed input: 161392.13 toks/s, output: 157.61 toks/s]
Processed prompts:  50%|█████     | 1026/2048 [00:06<00:10, 98.80it/s, est. speed input: 159879.24 toks/s, output: 156.13 toks/s]
Processed prompts:  51%|█████     | 1042/2048 [00:06<00:10, 98.63it/s, est. speed input: 158445.57 toks/s, output: 154.73 toks/s]
Processed prompts:  52%|█████▏    | 1058/2048 [00:06<00:10, 98.63it/s, est. speed input: 157093.60 toks/s, output: 153.41 toks/s]
Processed prompts:  52%|█████▏    | 1074/2048 [00:07<00:09, 98.61it/s, est. speed input: 155802.41 toks/s, output: 152.15 toks/s]
Processed prompts:  53%|█████▎    | 1090/2048 [00:07<00:09, 98.55it/s, est. speed input: 154563.52 toks/s, output: 150.94 toks/s]
Processed prompts:  54%|█████▍    | 1106/2048 [00:07<00:09, 98.29it/s, est. speed input: 153353.55 toks/s, output: 149.76 toks/s]
Processed prompts:  55%|█████▍    | 1122/2048 [00:07<00:09, 98.11it/s, est. speed input: 152197.38 toks/s, output: 148.63 toks/s]
Processed prompts:  56%|█████▌    | 1138/2048 [00:07<00:09, 98.12it/s, est. speed input: 151104.03 toks/s, output: 147.56 toks/s]
Processed prompts:  56%|█████▋    | 1154/2048 [00:07<00:08, 99.51it/s, est. speed input: 150200.39 toks/s, output: 146.68 toks/s]
Processed prompts:  57%|█████▋    | 1170/2048 [00:08<00:08, 99.19it/s, est. speed input: 149200.73 toks/s, output: 145.70 toks/s]
Processed prompts:  58%|█████▊    | 1186/2048 [00:08<00:08, 98.95it/s, est. speed input: 148239.00 toks/s, output: 144.76 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [00:08<00:08, 98.60it/s, est. speed input: 147297.39 toks/s, output: 143.84 toks/s]
Processed prompts:  59%|█████▉    | 1218/2048 [00:08<00:08, 98.62it/s, est. speed input: 146416.45 toks/s, output: 142.98 toks/s]
Processed prompts:  60%|██████    | 1234/2048 [00:08<00:08, 98.48it/s, est. speed input: 145554.53 toks/s, output: 142.14 toks/s]
Processed prompts:  61%|██████    | 1250/2048 [00:08<00:08, 98.26it/s, est. speed input: 144713.82 toks/s, output: 141.32 toks/s]
Processed prompts:  62%|██████▏   | 1266/2048 [00:09<00:07, 99.64it/s, est. speed input: 144036.84 toks/s, output: 140.66 toks/s]
Processed prompts:  63%|██████▎   | 1282/2048 [00:09<00:07, 99.21it/s, est. speed input: 143263.69 toks/s, output: 139.91 toks/s]
Processed prompts:  63%|██████▎   | 1298/2048 [00:09<00:07, 98.78it/s, est. speed input: 142507.29 toks/s, output: 139.17 toks/s]
Processed prompts:  64%|██████▍   | 1314/2048 [00:09<00:07, 98.40it/s, est. speed input: 141770.66 toks/s, output: 138.45 toks/s]
Processed prompts:  65%|██████▍   | 1330/2048 [00:09<00:07, 98.26it/s, est. speed input: 141068.72 toks/s, output: 137.76 toks/s]
Processed prompts:  66%|██████▌   | 1346/2048 [00:09<00:07, 98.17it/s, est. speed input: 140390.50 toks/s, output: 137.10 toks/s]
Processed prompts:  67%|██████▋   | 1362/2048 [00:09<00:06, 98.29it/s, est. speed input: 139748.39 toks/s, output: 136.47 toks/s]
Processed prompts:  67%|██████▋   | 1378/2048 [00:10<00:06, 98.23it/s, est. speed input: 139116.86 toks/s, output: 135.86 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [00:10<00:06, 98.12it/s, est. speed input: 138499.82 toks/s, output: 135.25 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [00:10<00:06, 97.91it/s, est. speed input: 137892.29 toks/s, output: 134.66 toks/s]
Processed prompts:  70%|██████▉   | 1426/2048 [00:10<00:06, 98.04it/s, est. speed input: 137322.88 toks/s, output: 134.10 toks/s]
Processed prompts:  70%|███████   | 1442/2048 [00:10<00:06, 98.02it/s, est. speed input: 136763.56 toks/s, output: 133.56 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [00:10<00:06, 98.15it/s, est. speed input: 136230.49 toks/s, output: 133.04 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [00:11<00:05, 98.35it/s, est. speed input: 135720.04 toks/s, output: 132.54 toks/s]
Processed prompts:  73%|███████▎  | 1490/2048 [00:11<00:05, 98.30it/s, est. speed input: 135212.31 toks/s, output: 132.04 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [00:11<00:05, 97.79it/s, est. speed input: 134687.61 toks/s, output: 131.53 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [00:11<00:05, 97.93it/s, est. speed input: 134209.74 toks/s, output: 131.06 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [00:11<00:05, 97.82it/s, est. speed input: 133732.06 toks/s, output: 130.60 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [00:11<00:05, 98.08it/s, est. speed input: 133288.56 toks/s, output: 130.16 toks/s]
Processed prompts:  77%|███████▋  | 1570/2048 [00:12<00:04, 98.37it/s, est. speed input: 132862.98 toks/s, output: 129.75 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [00:12<00:04, 99.57it/s, est. speed input: 132507.49 toks/s, output: 129.40 toks/s]
Processed prompts:  78%|███████▊  | 1602/2048 [00:12<00:04, 99.34it/s, est. speed input: 132098.93 toks/s, output: 129.00 toks/s]
Processed prompts:  79%|███████▉  | 1618/2048 [00:12<00:04, 98.99it/s, est. speed input: 131690.48 toks/s, output: 128.60 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [00:12<00:04, 98.62it/s, est. speed input: 131284.84 toks/s, output: 128.21 toks/s]
Processed prompts:  81%|████████  | 1650/2048 [00:12<00:04, 98.28it/s, est. speed input: 130884.93 toks/s, output: 127.82 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [00:13<00:03, 98.23it/s, est. speed input: 130505.36 toks/s, output: 127.45 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [00:13<00:03, 98.14it/s, est. speed input: 130132.19 toks/s, output: 127.08 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [00:13<00:03, 98.04it/s, est. speed input: 129766.27 toks/s, output: 126.72 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [00:13<00:03, 98.17it/s, est. speed input: 129419.91 toks/s, output: 126.39 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [00:13<00:03, 97.87it/s, est. speed input: 129061.12 toks/s, output: 126.04 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [00:13<00:03, 97.94it/s, est. speed input: 128725.25 toks/s, output: 125.71 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [00:14<00:02, 97.85it/s, est. speed input: 128389.81 toks/s, output: 125.38 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [00:14<00:02, 97.80it/s, est. speed input: 128063.36 toks/s, output: 125.06 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [00:14<00:02, 97.94it/s, est. speed input: 127752.81 toks/s, output: 124.76 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [00:14<00:02, 98.19it/s, est. speed input: 127456.48 toks/s, output: 124.47 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [00:14<00:02, 98.06it/s, est. speed input: 127151.77 toks/s, output: 124.17 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [00:14<00:02, 97.86it/s, est. speed input: 126849.07 toks/s, output: 123.88 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [00:15<00:01, 97.89it/s, est. speed input: 126560.42 toks/s, output: 123.59 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [00:15<00:01, 99.09it/s, est. speed input: 126332.38 toks/s, output: 123.37 toks/s]
Processed prompts:  92%|█████████▏| 1890/2048 [00:15<00:01, 98.74it/s, est. speed input: 126055.24 toks/s, output: 123.10 toks/s]
Processed prompts:  93%|█████████▎| 1906/2048 [00:15<00:01, 98.52it/s, est. speed input: 125785.06 toks/s, output: 122.84 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [00:15<00:01, 98.52it/s, est. speed input: 125526.80 toks/s, output: 122.58 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [00:15<00:01, 98.22it/s, est. speed input: 125261.06 toks/s, output: 122.32 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [00:16<00:00, 99.25it/s, est. speed input: 125054.02 toks/s, output: 122.12 toks/s]
Processed prompts:  96%|█████████▌| 1970/2048 [00:16<00:00, 98.89it/s, est. speed input: 124805.31 toks/s, output: 121.88 toks/s]
Processed prompts:  97%|█████████▋| 1986/2048 [00:16<00:00, 98.46it/s, est. speed input: 124553.95 toks/s, output: 121.63 toks/s]
Processed prompts:  98%|█████████▊| 2002/2048 [00:16<00:00, 98.31it/s, est. speed input: 124313.72 toks/s, output: 121.40 toks/s]
Processed prompts:  99%|█████████▊| 2018/2048 [00:16<00:00, 98.11it/s, est. speed input: 124074.64 toks/s, output: 121.17 toks/s]
Processed prompts:  99%|█████████▉| 2034/2048 [00:16<00:00, 100.05it/s, est. speed input: 123922.94 toks/s, output: 121.02 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:16<00:00, 100.05it/s, est. speed input: 124772.87 toks/s, output: 121.85 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:16<00:00, 121.85it/s, est. speed input: 124772.87 toks/s, output: 121.85 toks/s]
[rank0]:[W128 08:42:29.859644004 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 66.5s

测试结果:
  Requests/s:   98.07
  Tokens/s:     100521.36
  Total Reqs:   2048
  Elapsed:      20.88s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     100423.29

============================================================
[7/7] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 08:42:55 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3323979) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3323979) WARNING 01-28 08:43:10 [backends.py:609] Failed to read file <frozen os>
Throughput: 99.96 requests/s, 102454.71 total tokens/s, 99.96 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-28 08:42:55] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:42:55] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:42:55] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:42:55] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:42:55] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:42:55] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:42:55] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:42:55] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:42:55] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:42:55] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:42:55] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:42:55] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:42:55] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:42:55] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 08:43:02] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:43:02] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:43:02] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:43:02] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:43:02] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:43:02] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:43:02] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:43:02] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:43:02] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:43:02] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:43:02] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:43:02] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:43:02] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:43:02] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3323979) [2026-01-28 08:43:03] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3323979) [2026-01-28 08:43:03] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3323979) [2026-01-28 08:43:03] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3323979) [2026-01-28 08:43:03] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3323979) [2026-01-28 08:43:03] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3323979) [2026-01-28 08:43:03] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3323979) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3323979) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.46it/s]
(EngineCore_DP0 pid=3323979) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.46it/s]
(EngineCore_DP0 pid=3323979) 
(EngineCore_DP0 pid=3323979) [2026-01-28 08:43:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=3323979) [2026-01-28 08:43:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 8232960 bytes
(EngineCore_DP0 pid=3323979) [2026-01-28 08:43:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=3323979) [2026-01-28 08:43:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5488640 bytes
(EngineCore_DP0 pid=3323979) [2026-01-28 08:43:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=3323979) [2026-01-28 08:43:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 29638656 bytes
(EngineCore_DP0 pid=3323979) [2026-01-28 08:43:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=3323979) [2026-01-28 08:43:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 14745600 bytes
(EngineCore_DP0 pid=3323979) [rank0]:W0128 08:43:16.712000 3323979 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3323979) [rank0]:W0128 08:43:16.789000 3323979 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3323979) [rank0]:W0128 08:43:17.882000 3323979 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3323979) [rank0]:W0128 08:43:18.004000 3323979 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3323979) 2026-01-28 08:43:21,918 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3323979) 2026-01-28 08:43:21,947 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3323979) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▉         | 1/11 [00:00<00:02,  4.44it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 3/11 [00:00<00:00,  9.15it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 5/11 [00:00<00:00, 11.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▎   | 7/11 [00:00<00:00, 13.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 9/11 [00:00<00:00, 14.66it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 14.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 12.79it/s]
(EngineCore_DP0 pid=3323979) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00, 16.01it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00, 16.90it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00, 11.15it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 11.25it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 36/4096 [00:00<00:11, 354.15it/s]
Adding requests:   2%|▏         | 88/4096 [00:00<00:08, 445.95it/s]
Adding requests:   3%|▎         | 138/4096 [00:00<00:08, 469.61it/s]
Adding requests:   5%|▍         | 188/4096 [00:00<00:08, 476.09it/s]
Adding requests:   6%|▌         | 239/4096 [00:00<00:07, 488.17it/s]
Adding requests:   7%|▋         | 289/4096 [00:00<00:07, 491.24it/s]
Adding requests:   8%|▊         | 339/4096 [00:00<00:07, 490.54it/s]
Adding requests:  10%|▉         | 391/4096 [00:00<00:07, 498.87it/s]
Adding requests:  11%|█         | 441/4096 [00:00<00:07, 497.35it/s]
Adding requests:  12%|█▏        | 492/4096 [00:01<00:07, 500.44it/s]
Adding requests:  13%|█▎        | 543/4096 [00:01<00:07, 492.05it/s]
Adding requests:  15%|█▍        | 596/4096 [00:01<00:07, 499.60it/s]
Adding requests:  16%|█▌        | 648/4096 [00:01<00:06, 504.05it/s]
Adding requests:  17%|█▋        | 701/4096 [00:01<00:06, 510.87it/s]
Adding requests:  18%|█▊        | 753/4096 [00:01<00:06, 508.77it/s]
Adding requests:  20%|█▉        | 804/4096 [00:01<00:06, 502.89it/s]
Adding requests:  21%|██        | 855/4096 [00:01<00:06, 498.62it/s]
Adding requests:  22%|██▏       | 907/4096 [00:01<00:06, 504.57it/s]
Adding requests:  23%|██▎       | 959/4096 [00:01<00:06, 508.48it/s]
Adding requests:  25%|██▍       | 1011/4096 [00:02<00:06, 510.57it/s]
Adding requests:  26%|██▌       | 1063/4096 [00:02<00:05, 509.32it/s]
Adding requests:  27%|██▋       | 1114/4096 [00:02<00:05, 502.33it/s]
Adding requests:  28%|██▊       | 1167/4096 [00:02<00:05, 509.07it/s]
Adding requests:  30%|██▉       | 1218/4096 [00:02<00:05, 504.79it/s]
Adding requests:  31%|███       | 1269/4096 [00:02<00:05, 500.92it/s]
Adding requests:  32%|███▏      | 1321/4096 [00:02<00:05, 504.68it/s]
Adding requests:  34%|███▎      | 1374/4096 [00:02<00:05, 509.56it/s]
Adding requests:  35%|███▍      | 1426/4096 [00:02<00:05, 512.37it/s]
Adding requests:  36%|███▌      | 1478/4096 [00:02<00:05, 512.23it/s]
Adding requests:  37%|███▋      | 1531/4096 [00:03<00:04, 517.25it/s]
Adding requests:  39%|███▊      | 1583/4096 [00:03<00:04, 515.61it/s]
Adding requests:  40%|███▉      | 1637/4096 [00:03<00:04, 520.42it/s]
Adding requests:  41%|████▏     | 1690/4096 [00:03<00:04, 512.99it/s]
Adding requests:  43%|████▎     | 1743/4096 [00:03<00:04, 515.77it/s]
Adding requests:  44%|████▍     | 1795/4096 [00:03<00:04, 512.32it/s]
Adding requests:  45%|████▌     | 1847/4096 [00:03<00:04, 513.38it/s]
Adding requests:  46%|████▋     | 1899/4096 [00:03<00:04, 510.75it/s]
Adding requests:  48%|████▊     | 1951/4096 [00:03<00:04, 508.56it/s]
Adding requests:  49%|████▉     | 2004/4096 [00:03<00:04, 513.05it/s]
Adding requests:  50%|█████     | 2056/4096 [00:04<00:03, 513.05it/s]
Adding requests:  51%|█████▏    | 2109/4096 [00:04<00:03, 517.84it/s]
Adding requests:  53%|█████▎    | 2161/4096 [00:04<00:03, 507.59it/s]
Adding requests:  54%|█████▍    | 2212/4096 [00:04<00:03, 506.16it/s]
Adding requests:  55%|█████▌    | 2264/4096 [00:04<00:03, 510.13it/s]
Adding requests:  57%|█████▋    | 2317/4096 [00:04<00:03, 514.79it/s]
Adding requests:  58%|█████▊    | 2369/4096 [00:04<00:03, 511.90it/s]
Adding requests:  59%|█████▉    | 2421/4096 [00:04<00:03, 500.30it/s]
Adding requests:  60%|██████    | 2472/4096 [00:04<00:03, 502.80it/s]
Adding requests:  62%|██████▏   | 2523/4096 [00:05<00:03, 501.63it/s]
Adding requests:  63%|██████▎   | 2576/4096 [00:05<00:02, 509.40it/s]
Adding requests:  64%|██████▍   | 2627/4096 [00:05<00:02, 507.96it/s]
Adding requests:  65%|██████▌   | 2679/4096 [00:05<00:02, 509.78it/s]
Adding requests:  67%|██████▋   | 2730/4096 [00:05<00:02, 507.54it/s]
Adding requests:  68%|██████▊   | 2781/4096 [00:05<00:02, 507.97it/s]
Adding requests:  69%|██████▉   | 2832/4096 [00:05<00:02, 504.89it/s]
Adding requests:  70%|███████   | 2885/4096 [00:05<00:02, 510.03it/s]
Adding requests:  72%|███████▏  | 2937/4096 [00:05<00:02, 506.31it/s]
Adding requests:  73%|███████▎  | 2990/4096 [00:05<00:02, 511.83it/s]
Adding requests:  74%|███████▍  | 3042/4096 [00:06<00:02, 510.99it/s]
Adding requests:  76%|███████▌  | 3094/4096 [00:06<00:01, 507.68it/s]
Adding requests:  77%|███████▋  | 3145/4096 [00:06<00:01, 505.11it/s]
Adding requests:  78%|███████▊  | 3196/4096 [00:06<00:01, 505.83it/s]
Adding requests:  79%|███████▉  | 3249/4096 [00:06<00:01, 511.67it/s]
Adding requests:  81%|████████  | 3301/4096 [00:06<00:01, 510.39it/s]
Adding requests:  82%|████████▏ | 3353/4096 [00:06<00:01, 512.74it/s]
Adding requests:  83%|████████▎ | 3405/4096 [00:06<00:01, 510.86it/s]
Adding requests:  84%|████████▍ | 3457/4096 [00:06<00:01, 507.96it/s]
Adding requests:  86%|████████▌ | 3508/4096 [00:06<00:01, 502.87it/s]
Adding requests:  87%|████████▋ | 3559/4096 [00:07<00:01, 501.24it/s]
Adding requests:  88%|████████▊ | 3610/4096 [00:07<00:00, 495.71it/s]
Adding requests:  89%|████████▉ | 3660/4096 [00:07<00:00, 492.12it/s]
Adding requests:  91%|█████████ | 3712/4096 [00:07<00:00, 498.42it/s]
Adding requests:  92%|█████████▏| 3762/4096 [00:07<00:00, 482.53it/s]
Adding requests:  93%|█████████▎| 3814/4096 [00:07<00:00, 491.51it/s]
Adding requests:  94%|█████████▍| 3867/4096 [00:07<00:00, 501.82it/s]
Adding requests:  96%|█████████▌| 3919/4096 [00:07<00:00, 505.52it/s]
Adding requests:  97%|█████████▋| 3971/4096 [00:07<00:00, 507.85it/s]
Adding requests:  98%|█████████▊| 4023/4096 [00:07<00:00, 510.08it/s]
Adding requests:  99%|█████████▉| 4075/4096 [00:08<00:00, 503.06it/s]
Adding requests: 100%|██████████| 4096/4096 [00:08<00:00, 504.56it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  19%|█▉        | 796/4096 [00:00<00:00, 3406.18it/s, est. speed input: 3488262.72 toks/s, output: 3406.28 toks/s]
Processed prompts:  28%|██▊       | 1137/4096 [00:03<00:11, 268.59it/s, est. speed input: 341016.53 toks/s, output: 333.02 toks/s]  
Processed prompts:  31%|███▏      | 1284/4096 [00:04<00:14, 197.66it/s, est. speed input: 263075.91 toks/s, output: 256.91 toks/s]
Processed prompts:  33%|███▎      | 1368/4096 [00:05<00:14, 184.92it/s, est. speed input: 248310.32 toks/s, output: 242.49 toks/s]
Processed prompts:  35%|███▍      | 1424/4096 [00:06<00:16, 164.20it/s, est. speed input: 232121.84 toks/s, output: 226.68 toks/s]
Processed prompts:  36%|███▌      | 1463/4096 [00:06<00:16, 158.55it/s, est. speed input: 226879.08 toks/s, output: 221.56 toks/s]
Processed prompts:  36%|███▋      | 1493/4096 [00:06<00:17, 148.15it/s, est. speed input: 220765.62 toks/s, output: 215.59 toks/s]
Processed prompts:  37%|███▋      | 1517/4096 [00:07<00:19, 134.59it/s, est. speed input: 214375.26 toks/s, output: 209.35 toks/s]
Processed prompts:  38%|███▊      | 1536/4096 [00:07<00:21, 118.88it/s, est. speed input: 207867.03 toks/s, output: 202.99 toks/s]
Processed prompts:  38%|███▊      | 1564/4096 [00:07<00:22, 111.70it/s, est. speed input: 203073.27 toks/s, output: 198.31 toks/s]
Processed prompts:  39%|███▉      | 1596/4096 [00:08<00:22, 109.70it/s, est. speed input: 199403.40 toks/s, output: 194.73 toks/s]
Processed prompts:  40%|███▉      | 1628/4096 [00:08<00:23, 107.01it/s, est. speed input: 195719.71 toks/s, output: 191.13 toks/s]
Processed prompts:  41%|████      | 1660/4096 [00:08<00:23, 104.90it/s, est. speed input: 192290.24 toks/s, output: 187.78 toks/s]
Processed prompts:  41%|████▏     | 1692/4096 [00:09<00:23, 103.31it/s, est. speed input: 189099.93 toks/s, output: 184.67 toks/s]
Processed prompts:  42%|████▏     | 1724/4096 [00:09<00:23, 102.20it/s, est. speed input: 186137.84 toks/s, output: 181.77 toks/s]
Processed prompts:  43%|████▎     | 1756/4096 [00:09<00:23, 101.38it/s, est. speed input: 183368.38 toks/s, output: 179.07 toks/s]
Processed prompts:  44%|████▎     | 1788/4096 [00:10<00:22, 101.01it/s, est. speed input: 180815.05 toks/s, output: 176.58 toks/s]
Processed prompts:  44%|████▍     | 1820/4096 [00:10<00:22, 100.57it/s, est. speed input: 178385.96 toks/s, output: 174.20 toks/s]
Processed prompts:  45%|████▌     | 1852/4096 [00:10<00:22, 101.23it/s, est. speed input: 176274.12 toks/s, output: 172.14 toks/s]
Processed prompts:  46%|████▌     | 1884/4096 [00:11<00:21, 100.63it/s, est. speed input: 174100.35 toks/s, output: 170.02 toks/s]
Processed prompts:  47%|████▋     | 1916/4096 [00:11<00:21, 100.12it/s, est. speed input: 172036.35 toks/s, output: 168.00 toks/s]
Processed prompts:  48%|████▊     | 1948/4096 [00:11<00:21, 100.83it/s, est. speed input: 170250.90 toks/s, output: 166.26 toks/s]
Processed prompts:  48%|████▊     | 1980/4096 [00:12<00:21, 100.57it/s, est. speed input: 168445.11 toks/s, output: 164.50 toks/s]
Processed prompts:  49%|████▉     | 2012/4096 [00:12<00:20, 100.42it/s, est. speed input: 166736.95 toks/s, output: 162.83 toks/s]
Processed prompts:  50%|████▉     | 2044/4096 [00:12<00:20, 100.23it/s, est. speed input: 165104.65 toks/s, output: 161.23 toks/s]
Processed prompts:  51%|█████     | 2076/4096 [00:12<00:20, 100.19it/s, est. speed input: 163564.44 toks/s, output: 159.73 toks/s]
Processed prompts:  51%|█████▏    | 2108/4096 [00:13<00:19, 100.37it/s, est. speed input: 162126.03 toks/s, output: 158.33 toks/s]
Processed prompts:  52%|█████▏    | 2140/4096 [00:13<00:19, 100.03it/s, est. speed input: 160695.31 toks/s, output: 156.93 toks/s]
Processed prompts:  53%|█████▎    | 2172/4096 [00:13<00:19, 100.08it/s, est. speed input: 159365.71 toks/s, output: 155.63 toks/s]
Processed prompts:  54%|█████▍    | 2204/4096 [00:14<00:18, 99.96it/s, est. speed input: 158077.35 toks/s, output: 154.37 toks/s] 
Processed prompts:  55%|█████▍    | 2236/4096 [00:14<00:18, 101.38it/s, est. speed input: 157016.08 toks/s, output: 153.34 toks/s]
Processed prompts:  55%|█████▌    | 2268/4096 [00:14<00:18, 101.05it/s, est. speed input: 155853.59 toks/s, output: 152.20 toks/s]
Processed prompts:  56%|█████▌    | 2300/4096 [00:15<00:17, 101.53it/s, est. speed input: 154814.12 toks/s, output: 151.19 toks/s]
Processed prompts:  57%|█████▋    | 2332/4096 [00:15<00:17, 101.90it/s, est. speed input: 153820.02 toks/s, output: 150.21 toks/s]
Processed prompts:  58%|█████▊    | 2364/4096 [00:15<00:16, 102.99it/s, est. speed input: 152945.96 toks/s, output: 149.36 toks/s]
Processed prompts:  58%|█████▊    | 2396/4096 [00:16<00:16, 102.14it/s, est. speed input: 151950.70 toks/s, output: 148.39 toks/s]
Processed prompts:  59%|█████▉    | 2428/4096 [00:16<00:16, 101.49it/s, est. speed input: 150987.77 toks/s, output: 147.45 toks/s]
Processed prompts:  60%|██████    | 2460/4096 [00:16<00:16, 101.03it/s, est. speed input: 150060.88 toks/s, output: 146.54 toks/s]
Processed prompts:  61%|██████    | 2492/4096 [00:17<00:15, 101.64it/s, est. speed input: 149252.91 toks/s, output: 145.75 toks/s]
Processed prompts:  62%|██████▏   | 2524/4096 [00:17<00:15, 101.00it/s, est. speed input: 148379.53 toks/s, output: 144.90 toks/s]
Processed prompts:  62%|██████▏   | 2556/4096 [00:17<00:15, 100.47it/s, est. speed input: 147530.19 toks/s, output: 144.07 toks/s]
Processed prompts:  63%|██████▎   | 2588/4096 [00:18<00:14, 101.00it/s, est. speed input: 146788.15 toks/s, output: 143.35 toks/s]
Processed prompts:  64%|██████▍   | 2620/4096 [00:18<00:14, 100.52it/s, est. speed input: 146000.17 toks/s, output: 142.58 toks/s]
Processed prompts:  65%|██████▍   | 2652/4096 [00:18<00:14, 100.34it/s, est. speed input: 145252.05 toks/s, output: 141.85 toks/s]
Processed prompts:  66%|██████▌   | 2684/4096 [00:19<00:14, 100.16it/s, est. speed input: 144524.76 toks/s, output: 141.14 toks/s]
Processed prompts:  66%|██████▋   | 2716/4096 [00:19<00:13, 100.17it/s, est. speed input: 143832.52 toks/s, output: 140.46 toks/s]
Processed prompts:  67%|██████▋   | 2748/4096 [00:19<00:13, 100.04it/s, est. speed input: 143151.54 toks/s, output: 139.80 toks/s]
Processed prompts:  68%|██████▊   | 2780/4096 [00:19<00:13, 99.84it/s, est. speed input: 142484.27 toks/s, output: 139.14 toks/s] 
Processed prompts:  69%|██████▊   | 2812/4096 [00:20<00:12, 99.67it/s, est. speed input: 141836.40 toks/s, output: 138.51 toks/s]
Processed prompts:  69%|██████▉   | 2844/4096 [00:20<00:12, 99.62it/s, est. speed input: 141213.06 toks/s, output: 137.90 toks/s]
Processed prompts:  70%|███████   | 2876/4096 [00:20<00:12, 99.66it/s, est. speed input: 140615.16 toks/s, output: 137.32 toks/s]
Processed prompts:  71%|███████   | 2908/4096 [00:21<00:11, 99.77it/s, est. speed input: 140040.14 toks/s, output: 136.76 toks/s]
Processed prompts:  72%|███████▏  | 2940/4096 [00:21<00:11, 99.67it/s, est. speed input: 139470.40 toks/s, output: 136.20 toks/s]
Processed prompts:  73%|███████▎  | 2972/4096 [00:21<00:11, 99.88it/s, est. speed input: 138936.20 toks/s, output: 135.68 toks/s]
Processed prompts:  73%|███████▎  | 3004/4096 [00:22<00:10, 99.76it/s, est. speed input: 138399.94 toks/s, output: 135.16 toks/s]
Processed prompts:  74%|███████▍  | 3036/4096 [00:22<00:10, 99.70it/s, est. speed input: 137880.47 toks/s, output: 134.65 toks/s]
Processed prompts:  75%|███████▍  | 3068/4096 [00:22<00:10, 99.67it/s, est. speed input: 137376.29 toks/s, output: 134.16 toks/s]
Processed prompts:  76%|███████▌  | 3100/4096 [00:23<00:09, 99.69it/s, est. speed input: 136888.45 toks/s, output: 133.68 toks/s]
Processed prompts:  76%|███████▋  | 3132/4096 [00:23<00:09, 100.45it/s, est. speed input: 136460.40 toks/s, output: 133.26 toks/s]
Processed prompts:  77%|███████▋  | 3164/4096 [00:23<00:09, 100.06it/s, est. speed input: 135987.47 toks/s, output: 132.80 toks/s]
Processed prompts:  78%|███████▊  | 3196/4096 [00:24<00:09, 99.80it/s, est. speed input: 135527.95 toks/s, output: 132.35 toks/s] 
Processed prompts:  79%|███████▉  | 3228/4096 [00:24<00:08, 99.75it/s, est. speed input: 135088.03 toks/s, output: 131.92 toks/s]
Processed prompts:  80%|███████▉  | 3260/4096 [00:24<00:08, 99.02it/s, est. speed input: 134618.64 toks/s, output: 131.46 toks/s]
Processed prompts:  80%|████████  | 3292/4096 [00:25<00:08, 99.50it/s, est. speed input: 134219.37 toks/s, output: 131.07 toks/s]
Processed prompts:  81%|████████  | 3324/4096 [00:25<00:07, 99.48it/s, est. speed input: 133809.59 toks/s, output: 130.67 toks/s]
Processed prompts:  82%|████████▏ | 3356/4096 [00:25<00:07, 99.42it/s, est. speed input: 133406.84 toks/s, output: 130.28 toks/s]
Processed prompts:  83%|████████▎ | 3388/4096 [00:26<00:07, 99.46it/s, est. speed input: 133019.34 toks/s, output: 129.90 toks/s]
Processed prompts:  83%|████████▎ | 3420/4096 [00:26<00:06, 99.33it/s, est. speed input: 132632.53 toks/s, output: 129.52 toks/s]
Processed prompts:  84%|████████▍ | 3452/4096 [00:26<00:06, 99.44it/s, est. speed input: 132265.49 toks/s, output: 129.17 toks/s]
Processed prompts:  85%|████████▌ | 3484/4096 [00:27<00:06, 100.95it/s, est. speed input: 131981.89 toks/s, output: 128.89 toks/s]
Processed prompts:  86%|████████▌ | 3516/4096 [00:27<00:05, 100.49it/s, est. speed input: 131627.04 toks/s, output: 128.54 toks/s]
Processed prompts:  87%|████████▋ | 3548/4096 [00:27<00:05, 100.20it/s, est. speed input: 131281.66 toks/s, output: 128.20 toks/s]
Processed prompts:  87%|████████▋ | 3580/4096 [00:27<00:05, 100.01it/s, est. speed input: 130945.02 toks/s, output: 127.88 toks/s]
Processed prompts:  88%|████████▊ | 3612/4096 [00:28<00:04, 99.83it/s, est. speed input: 130613.78 toks/s, output: 127.55 toks/s] 
Processed prompts:  89%|████████▉ | 3644/4096 [00:28<00:04, 99.61it/s, est. speed input: 130285.43 toks/s, output: 127.23 toks/s]
Processed prompts:  90%|████████▉ | 3676/4096 [00:28<00:04, 99.52it/s, est. speed input: 129967.03 toks/s, output: 126.92 toks/s]
Processed prompts:  91%|█████████ | 3708/4096 [00:29<00:03, 99.46it/s, est. speed input: 129656.05 toks/s, output: 126.62 toks/s]
Processed prompts:  91%|█████████▏| 3740/4096 [00:29<00:03, 100.25it/s, est. speed input: 129390.70 toks/s, output: 126.36 toks/s]
Processed prompts:  92%|█████████▏| 3772/4096 [00:29<00:03, 99.90it/s, est. speed input: 129089.55 toks/s, output: 126.06 toks/s] 
Processed prompts:  93%|█████████▎| 3804/4096 [00:30<00:02, 99.77it/s, est. speed input: 128799.97 toks/s, output: 125.78 toks/s]
Processed prompts:  94%|█████████▎| 3836/4096 [00:30<00:02, 100.50it/s, est. speed input: 128552.95 toks/s, output: 125.54 toks/s]
Processed prompts:  94%|█████████▍| 3868/4096 [00:30<00:02, 100.13it/s, est. speed input: 128272.33 toks/s, output: 125.27 toks/s]
Processed prompts:  95%|█████████▌| 3900/4096 [00:31<00:01, 99.96it/s, est. speed input: 128001.26 toks/s, output: 125.00 toks/s] 
Processed prompts:  96%|█████████▌| 3932/4096 [00:31<00:01, 99.77it/s, est. speed input: 127732.51 toks/s, output: 124.74 toks/s]
Processed prompts:  97%|█████████▋| 3964/4096 [00:31<00:01, 99.56it/s, est. speed input: 127466.07 toks/s, output: 124.48 toks/s]
Processed prompts:  98%|█████████▊| 3996/4096 [00:32<00:01, 99.42it/s, est. speed input: 127205.26 toks/s, output: 124.22 toks/s]
Processed prompts:  98%|█████████▊| 4028/4096 [00:32<00:00, 100.44it/s, est. speed input: 126996.22 toks/s, output: 124.02 toks/s]
Processed prompts:  99%|█████████▉| 4060/4096 [00:32<00:00, 100.21it/s, est. speed input: 126752.28 toks/s, output: 123.78 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:32<00:00, 100.21it/s, est. speed input: 127653.86 toks/s, output: 124.66 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:32<00:00, 124.66it/s, est. speed input: 127653.86 toks/s, output: 124.66 toks/s]
[rank0]:[W128 08:44:06.374930994 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 96.5s

测试结果:
  Requests/s:   99.96
  Tokens/s:     102454.71
  Total Reqs:   4096
  Elapsed:      40.98s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     102354.76


------------------------------------------------------------
  生成 CSV: BitNet-2B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_INT8_py312_cu129_x86_64/cusparselt/2_6/BitNet-2B-INT8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,33.5133,17192.3328,3.8194
1024,1024,1,128,128,33.9459,34794.5118,3.7707
2048,1024,2,256,128,62.1195,63672.5017,4.1211
4096,1024,4,512,128,85.8779,88024.8302,5.9620
8192,1024,8,1024,128,94.8545,97225.8388,10.7955
16384,1024,16,2048,128,98.0696,100521.3640,20.8831
32768,1024,32,4096,128,99.9558,102454.7150,40.9781

------------------------------------------------------------

[INFO] 完成: 7 成功, 0 失败

============================================================
  BitNet-2B-INT8 | cuSPARSELt (2_8) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_8
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_INT8_py312_cu129_x86_64/cusparselt/2_8

============================================================
[1/7] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 08:44:15 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3325541) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3325541) WARNING 01-28 08:44:30 [backends.py:609] Failed to read file <frozen os>
Throughput: 33.62 requests/s, 17249.40 total tokens/s, 33.62 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-28 08:44:14] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:44:15] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:44:15] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:44:15] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:44:15] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:44:15] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:44:15] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:44:15] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:44:15] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:44:15] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:44:15] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:44:15] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:44:15] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:44:15] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 08:44:21] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:44:22] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:44:22] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:44:22] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:44:22] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:44:22] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:44:22] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:44:22] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:44:22] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:44:22] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:44:22] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:44:22] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:44:22] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:44:22] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3325541) [2026-01-28 08:44:23] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3325541) [2026-01-28 08:44:23] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3325541) [2026-01-28 08:44:23] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3325541) [2026-01-28 08:44:23] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3325541) [2026-01-28 08:44:23] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3325541) [2026-01-28 08:44:23] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3325541) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3325541) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.30it/s]
(EngineCore_DP0 pid=3325541) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.30it/s]
(EngineCore_DP0 pid=3325541) 
(EngineCore_DP0 pid=3325541) [2026-01-28 08:44:24] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=3325541) [2026-01-28 08:44:24] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9216000 bytes
(EngineCore_DP0 pid=3325541) [2026-01-28 08:44:24] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=3325541) [2026-01-28 08:44:24] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6144000 bytes
(EngineCore_DP0 pid=3325541) [2026-01-28 08:44:24] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=3325541) [2026-01-28 08:44:24] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 33177600 bytes
(EngineCore_DP0 pid=3325541) [2026-01-28 08:44:24] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=3325541) [2026-01-28 08:44:24] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16588800 bytes
(EngineCore_DP0 pid=3325541) 2026-01-28 08:44:41,505 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3325541) 2026-01-28 08:44:41,531 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3325541) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  4.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.37it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.48it/s]
(EngineCore_DP0 pid=3325541) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 16.39it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  41%|████▏     | 53/128 [00:00<00:00, 529.20it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 702.30it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:30,  4.21it/s, est. speed input: 2156.55 toks/s, output: 4.21 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:07, 17.14it/s, est. speed input: 7410.08 toks/s, output: 14.47 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:04, 24.38it/s, est. speed input: 10184.70 toks/s, output: 19.89 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:03, 28.80it/s, est. speed input: 11901.00 toks/s, output: 23.24 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:03, 31.59it/s, est. speed input: 13063.26 toks/s, output: 25.51 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:03, 33.42it/s, est. speed input: 13901.12 toks/s, output: 27.15 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:00<00:02, 34.61it/s, est. speed input: 14531.13 toks/s, output: 28.38 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:00<00:02, 35.48it/s, est. speed input: 15032.49 toks/s, output: 29.36 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:01<00:02, 36.05it/s, est. speed input: 15432.33 toks/s, output: 30.14 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:01<00:02, 36.45it/s, est. speed input: 15762.39 toks/s, output: 30.79 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:01<00:02, 36.75it/s, est. speed input: 16039.65 toks/s, output: 31.33 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:01<00:02, 36.95it/s, est. speed input: 16275.10 toks/s, output: 31.79 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:01<00:02, 37.03it/s, est. speed input: 16471.78 toks/s, output: 32.17 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:01<00:02, 37.13it/s, est. speed input: 16646.09 toks/s, output: 32.51 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:01<00:01, 37.29it/s, est. speed input: 16807.47 toks/s, output: 32.83 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:01<00:01, 37.37it/s, est. speed input: 16947.56 toks/s, output: 33.10 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:01<00:01, 37.44it/s, est. speed input: 17073.42 toks/s, output: 33.35 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:02<00:01, 37.50it/s, est. speed input: 17187.27 toks/s, output: 33.57 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:02<00:01, 37.55it/s, est. speed input: 17290.44 toks/s, output: 33.77 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:02<00:01, 37.53it/s, est. speed input: 17379.72 toks/s, output: 33.94 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:02<00:01, 37.54it/s, est. speed input: 17462.75 toks/s, output: 34.11 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:02<00:01, 37.63it/s, est. speed input: 17544.21 toks/s, output: 34.27 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:02<00:01, 37.71it/s, est. speed input: 17620.17 toks/s, output: 34.41 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:02<00:00, 37.77it/s, est. speed input: 17690.25 toks/s, output: 34.55 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:02<00:00, 37.77it/s, est. speed input: 17752.63 toks/s, output: 34.67 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:02<00:00, 37.72it/s, est. speed input: 17807.41 toks/s, output: 34.78 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:03<00:00, 37.68it/s, est. speed input: 17858.33 toks/s, output: 34.88 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:03<00:00, 37.66it/s, est. speed input: 17906.20 toks/s, output: 34.97 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:03<00:00, 37.61it/s, est. speed input: 17948.84 toks/s, output: 35.06 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:03<00:00, 37.58it/s, est. speed input: 17988.84 toks/s, output: 35.13 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:03<00:00, 37.59it/s, est. speed input: 18028.10 toks/s, output: 35.21 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:03<00:00, 37.57it/s, est. speed input: 18063.85 toks/s, output: 35.28 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 37.57it/s, est. speed input: 18090.35 toks/s, output: 35.33 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 35.33it/s, est. speed input: 18090.35 toks/s, output: 35.33 toks/s]
[rank0]:[W128 08:44:47.792064083 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 41.3s

测试结果:
  Requests/s:   33.62
  Tokens/s:     17249.40
  Total Reqs:   128
  Elapsed:      3.81s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     17215.78

============================================================
[2/7] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 08:44:56 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3326680) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3326680) WARNING 01-28 08:45:11 [backends.py:609] Failed to read file <frozen os>
Throughput: 32.25 requests/s, 33056.66 total tokens/s, 32.25 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-28 08:44:56] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:44:56] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:44:56] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:44:56] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:44:56] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:44:56] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:44:56] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:44:56] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:44:56] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:44:56] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:44:56] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:44:56] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:44:56] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:44:56] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 08:45:03] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:45:03] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:45:03] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:45:03] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:45:03] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:45:03] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:45:03] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:45:03] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:45:03] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:45:03] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:45:03] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:45:03] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:45:03] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:45:03] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3326680) [2026-01-28 08:45:04] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3326680) [2026-01-28 08:45:04] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3326680) [2026-01-28 08:45:04] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3326680) [2026-01-28 08:45:04] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3326680) [2026-01-28 08:45:04] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3326680) [2026-01-28 08:45:04] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3326680) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3326680) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.26it/s]
(EngineCore_DP0 pid=3326680) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.26it/s]
(EngineCore_DP0 pid=3326680) 
(EngineCore_DP0 pid=3326680) [2026-01-28 08:45:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=3326680) [2026-01-28 08:45:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9216000 bytes
(EngineCore_DP0 pid=3326680) [2026-01-28 08:45:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=3326680) [2026-01-28 08:45:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6144000 bytes
(EngineCore_DP0 pid=3326680) [2026-01-28 08:45:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=3326680) [2026-01-28 08:45:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 33177600 bytes
(EngineCore_DP0 pid=3326680) [2026-01-28 08:45:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=3326680) [2026-01-28 08:45:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16588800 bytes
(EngineCore_DP0 pid=3326680) 2026-01-28 08:45:22,646 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3326680) 2026-01-28 08:45:22,672 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3326680) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  9.42it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 11.77it/s]
(EngineCore_DP0 pid=3326680) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 15.93it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  29%|██▉       | 37/128 [00:00<00:00, 367.18it/s]
Adding requests:  69%|██████▉   | 88/128 [00:00<00:00, 448.48it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 450.84it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   3%|▎         | 4/128 [00:00<00:03, 35.09it/s, est. speed input: 35943.63 toks/s, output: 35.09 toks/s]
Processed prompts:   6%|▋         | 8/128 [00:00<00:03, 34.95it/s, est. speed input: 35819.34 toks/s, output: 34.98 toks/s]
Processed prompts:   9%|▉         | 12/128 [00:00<00:03, 34.85it/s, est. speed input: 35733.45 toks/s, output: 34.89 toks/s]
Processed prompts:  12%|█▎        | 16/128 [00:00<00:03, 34.84it/s, est. speed input: 35715.42 toks/s, output: 34.88 toks/s]
Processed prompts:  16%|█▌        | 20/128 [00:00<00:03, 34.95it/s, est. speed input: 35771.14 toks/s, output: 34.93 toks/s]
Processed prompts:  19%|█▉        | 24/128 [00:00<00:02, 34.96it/s, est. speed input: 35779.72 toks/s, output: 34.94 toks/s]
Processed prompts:  22%|██▏       | 28/128 [00:00<00:02, 35.01it/s, est. speed input: 35802.19 toks/s, output: 34.96 toks/s]
Processed prompts:  25%|██▌       | 32/128 [00:00<00:02, 35.00it/s, est. speed input: 35803.95 toks/s, output: 34.96 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:01<00:02, 35.05it/s, est. speed input: 35827.36 toks/s, output: 34.99 toks/s]
Processed prompts:  31%|███▏      | 40/128 [00:01<00:02, 34.89it/s, est. speed input: 35779.37 toks/s, output: 34.94 toks/s]
Processed prompts:  34%|███▍      | 44/128 [00:01<00:02, 34.99it/s, est. speed input: 35804.41 toks/s, output: 34.96 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:01<00:02, 35.00it/s, est. speed input: 35810.63 toks/s, output: 34.97 toks/s]
Processed prompts:  41%|████      | 52/128 [00:01<00:02, 34.95it/s, est. speed input: 35800.00 toks/s, output: 34.96 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:01<00:02, 34.82it/s, est. speed input: 35768.36 toks/s, output: 34.93 toks/s]
Processed prompts:  47%|████▋     | 60/128 [00:01<00:01, 34.91it/s, est. speed input: 35781.04 toks/s, output: 34.94 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:01<00:01, 35.01it/s, est. speed input: 35799.94 toks/s, output: 34.96 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:01<00:01, 33.13it/s, est. speed input: 35409.99 toks/s, output: 34.58 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:02<00:01, 33.33it/s, est. speed input: 35365.07 toks/s, output: 34.54 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:02<00:01, 33.96it/s, est. speed input: 35416.91 toks/s, output: 34.59 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:02<00:01, 34.30it/s, est. speed input: 35443.69 toks/s, output: 34.61 toks/s]
Processed prompts:  66%|██████▌   | 84/128 [00:02<00:01, 34.52it/s, est. speed input: 35464.70 toks/s, output: 34.63 toks/s]
Processed prompts:  69%|██████▉   | 88/128 [00:02<00:01, 34.68it/s, est. speed input: 35485.00 toks/s, output: 34.65 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:02<00:01, 34.83it/s, est. speed input: 35507.01 toks/s, output: 34.67 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:02<00:00, 34.84it/s, est. speed input: 35515.65 toks/s, output: 34.68 toks/s]
Processed prompts:  78%|███████▊  | 100/128 [00:02<00:00, 34.91it/s, est. speed input: 35531.05 toks/s, output: 34.70 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:02<00:00, 34.95it/s, est. speed input: 35544.48 toks/s, output: 34.71 toks/s]
Processed prompts:  84%|████████▍ | 108/128 [00:03<00:00, 34.96it/s, est. speed input: 35555.46 toks/s, output: 34.72 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:03<00:00, 34.92it/s, est. speed input: 35558.70 toks/s, output: 34.72 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:03<00:00, 34.91it/s, est. speed input: 35564.48 toks/s, output: 34.73 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:03<00:00, 34.93it/s, est. speed input: 35572.57 toks/s, output: 34.74 toks/s]
Processed prompts:  97%|█████████▋| 124/128 [00:03<00:00, 34.95it/s, est. speed input: 35581.03 toks/s, output: 34.75 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 34.94it/s, est. speed input: 35586.17 toks/s, output: 34.75 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 34.94it/s, est. speed input: 35586.17 toks/s, output: 34.75 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 34.75it/s, est. speed input: 35586.17 toks/s, output: 34.75 toks/s]
[rank0]:[W128 08:45:28.672038362 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 40.9s

测试结果:
  Requests/s:   32.25
  Tokens/s:     33056.66
  Total Reqs:   128
  Elapsed:      3.97s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     33024.41

============================================================
[3/7] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 08:45:38 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3327766) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3327766) WARNING 01-28 08:45:54 [backends.py:609] Failed to read file <frozen os>
Throughput: 63.05 requests/s, 64626.50 total tokens/s, 63.05 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-28 08:45:37] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:45:38] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:45:38] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:45:38] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:45:38] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:45:38] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:45:38] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:45:38] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:45:38] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:45:38] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:45:38] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:45:38] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:45:38] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:45:38] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 08:45:45] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:45:45] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:45:45] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:45:45] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:45:45] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:45:45] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:45:45] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:45:45] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:45:45] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:45:45] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:45:45] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:45:45] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:45:45] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:45:45] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3327766) [2026-01-28 08:45:46] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3327766) [2026-01-28 08:45:46] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3327766) [2026-01-28 08:45:46] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3327766) [2026-01-28 08:45:46] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3327766) [2026-01-28 08:45:46] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3327766) [2026-01-28 08:45:46] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3327766) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3327766) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.20it/s]
(EngineCore_DP0 pid=3327766) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.20it/s]
(EngineCore_DP0 pid=3327766) 
(EngineCore_DP0 pid=3327766) [2026-01-28 08:45:47] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=3327766) [2026-01-28 08:45:47] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9216000 bytes
(EngineCore_DP0 pid=3327766) [2026-01-28 08:45:47] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=3327766) [2026-01-28 08:45:47] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6144000 bytes
(EngineCore_DP0 pid=3327766) [2026-01-28 08:45:47] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=3327766) [2026-01-28 08:45:47] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 33177600 bytes
(EngineCore_DP0 pid=3327766) [2026-01-28 08:45:47] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=3327766) [2026-01-28 08:45:47] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16588800 bytes
(EngineCore_DP0 pid=3327766) 2026-01-28 08:46:04,766 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3327766) 2026-01-28 08:46:04,793 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3327766) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:00<00:00, 13.17it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 12.96it/s]
(EngineCore_DP0 pid=3327766) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 16.63it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 16.61it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  11%|█▏        | 29/256 [00:00<00:00, 287.46it/s]
Adding requests:  32%|███▏      | 82/256 [00:00<00:00, 424.34it/s]
Adding requests:  52%|█████▏    | 132/256 [00:00<00:00, 457.98it/s]
Adding requests:  71%|███████   | 181/256 [00:00<00:00, 470.51it/s]
Adding requests:  91%|█████████ | 233/256 [00:00<00:00, 484.44it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 464.79it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   6%|▋         | 16/256 [00:00<00:01, 132.53it/s, est. speed input: 135733.06 toks/s, output: 132.54 toks/s]
Processed prompts:  12%|█▏        | 30/256 [00:00<00:02, 89.30it/s, est. speed input: 96490.46 toks/s, output: 94.22 toks/s]   
Processed prompts:  16%|█▌        | 40/256 [00:00<00:02, 81.67it/s, est. speed input: 89182.23 toks/s, output: 87.09 toks/s]
Processed prompts:  19%|█▉        | 49/256 [00:00<00:02, 81.11it/s, est. speed input: 87738.09 toks/s, output: 85.68 toks/s]
Processed prompts:  23%|██▎       | 58/256 [00:00<00:02, 74.87it/s, est. speed input: 83363.74 toks/s, output: 81.41 toks/s]
Processed prompts:  26%|██▌       | 66/256 [00:00<00:02, 73.78it/s, est. speed input: 81930.44 toks/s, output: 80.01 toks/s]
Processed prompts:  29%|██▉       | 74/256 [00:00<00:02, 72.94it/s, est. speed input: 80814.87 toks/s, output: 78.92 toks/s]
Processed prompts:  32%|███▏      | 82/256 [00:01<00:02, 72.44it/s, est. speed input: 79977.10 toks/s, output: 78.10 toks/s]
Processed prompts:  35%|███▌      | 90/256 [00:01<00:02, 71.90it/s, est. speed input: 79230.21 toks/s, output: 77.37 toks/s]
Processed prompts:  38%|███▊      | 98/256 [00:01<00:02, 71.62it/s, est. speed input: 78649.84 toks/s, output: 76.80 toks/s]
Processed prompts:  41%|████▏     | 106/256 [00:01<00:02, 71.43it/s, est. speed input: 78167.28 toks/s, output: 76.33 toks/s]
Processed prompts:  45%|████▍     | 114/256 [00:01<00:01, 71.14it/s, est. speed input: 77712.05 toks/s, output: 75.89 toks/s]
Processed prompts:  48%|████▊     | 122/256 [00:01<00:01, 70.96it/s, est. speed input: 77327.16 toks/s, output: 75.51 toks/s]
Processed prompts:  51%|█████     | 130/256 [00:01<00:01, 70.88it/s, est. speed input: 77005.22 toks/s, output: 75.20 toks/s]
Processed prompts:  54%|█████▍    | 138/256 [00:01<00:01, 70.79it/s, est. speed input: 76712.88 toks/s, output: 74.91 toks/s]
Processed prompts:  57%|█████▋    | 146/256 [00:01<00:01, 70.65it/s, est. speed input: 76440.94 toks/s, output: 74.65 toks/s]
Processed prompts:  60%|██████    | 154/256 [00:02<00:01, 70.62it/s, est. speed input: 76209.73 toks/s, output: 74.42 toks/s]
Processed prompts:  63%|██████▎   | 162/256 [00:02<00:01, 70.59it/s, est. speed input: 76001.34 toks/s, output: 74.22 toks/s]
Processed prompts:  66%|██████▋   | 170/256 [00:02<00:01, 70.57it/s, est. speed input: 75815.14 toks/s, output: 74.04 toks/s]
Processed prompts:  70%|██████▉   | 178/256 [00:02<00:01, 70.65it/s, est. speed input: 75661.56 toks/s, output: 73.89 toks/s]
Processed prompts:  73%|███████▎  | 186/256 [00:02<00:00, 70.73it/s, est. speed input: 75525.11 toks/s, output: 73.75 toks/s]
Processed prompts:  76%|███████▌  | 194/256 [00:02<00:00, 70.87it/s, est. speed input: 75413.69 toks/s, output: 73.65 toks/s]
Processed prompts:  79%|███████▉  | 202/256 [00:02<00:00, 70.92it/s, est. speed input: 75303.82 toks/s, output: 73.54 toks/s]
Processed prompts:  82%|████████▏ | 210/256 [00:02<00:00, 70.96it/s, est. speed input: 75202.71 toks/s, output: 73.44 toks/s]
Processed prompts:  85%|████████▌ | 218/256 [00:02<00:00, 70.93it/s, est. speed input: 75102.61 toks/s, output: 73.34 toks/s]
Processed prompts:  88%|████████▊ | 226/256 [00:03<00:00, 70.97it/s, est. speed input: 75017.91 toks/s, output: 73.26 toks/s]
Processed prompts:  91%|█████████▏| 234/256 [00:03<00:00, 70.99it/s, est. speed input: 74937.97 toks/s, output: 73.18 toks/s]
Processed prompts:  95%|█████████▍| 242/256 [00:03<00:00, 70.92it/s, est. speed input: 74853.12 toks/s, output: 73.10 toks/s]
Processed prompts:  98%|█████████▊| 250/256 [00:03<00:00, 70.83it/s, est. speed input: 74769.15 toks/s, output: 73.02 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 70.83it/s, est. speed input: 74735.50 toks/s, output: 72.98 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 72.98it/s, est. speed input: 74735.50 toks/s, output: 72.98 toks/s]
[rank0]:[W128 08:46:11.009727938 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 42.5s

测试结果:
  Requests/s:   63.05
  Tokens/s:     64626.50
  Total Reqs:   256
  Elapsed:      4.06s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     64563.45

============================================================
[4/7] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 08:46:21 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3328998) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3328998) WARNING 01-28 08:46:36 [backends.py:609] Failed to read file <frozen os>
Throughput: 84.56 requests/s, 86675.44 total tokens/s, 84.56 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-28 08:46:21] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:46:21] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:46:21] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:46:21] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:46:21] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:46:21] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:46:21] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:46:21] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:46:21] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:46:21] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:46:21] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:46:21] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:46:21] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:46:21] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 08:46:28] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:46:28] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:46:28] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:46:28] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:46:28] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:46:28] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:46:28] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:46:28] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:46:28] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:46:28] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:46:28] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:46:28] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:46:28] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:46:28] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3328998) [2026-01-28 08:46:29] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3328998) [2026-01-28 08:46:29] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3328998) [2026-01-28 08:46:29] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3328998) [2026-01-28 08:46:29] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3328998) [2026-01-28 08:46:29] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3328998) [2026-01-28 08:46:29] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3328998) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3328998) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.27it/s]
(EngineCore_DP0 pid=3328998) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.27it/s]
(EngineCore_DP0 pid=3328998) 
(EngineCore_DP0 pid=3328998) [2026-01-28 08:46:30] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=3328998) [2026-01-28 08:46:30] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9216000 bytes
(EngineCore_DP0 pid=3328998) [2026-01-28 08:46:30] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=3328998) [2026-01-28 08:46:30] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6144000 bytes
(EngineCore_DP0 pid=3328998) [2026-01-28 08:46:30] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=3328998) [2026-01-28 08:46:30] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 33177600 bytes
(EngineCore_DP0 pid=3328998) [2026-01-28 08:46:30] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=3328998) [2026-01-28 08:46:30] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16588800 bytes
(EngineCore_DP0 pid=3328998) 2026-01-28 08:46:47,678 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3328998) 2026-01-28 08:46:47,705 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3328998) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 1/4 [00:00<00:00,  6.11it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00,  6.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00,  9.16it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00,  8.46it/s]
(EngineCore_DP0 pid=3328998) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00, 16.44it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 16.87it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   6%|▌         | 30/512 [00:00<00:01, 298.09it/s]
Adding requests:  16%|█▌        | 80/512 [00:00<00:01, 413.60it/s]
Adding requests:  25%|██▌       | 129/512 [00:00<00:00, 447.77it/s]
Adding requests:  35%|███▍      | 177/512 [00:00<00:00, 459.70it/s]
Adding requests:  44%|████▍     | 227/512 [00:00<00:00, 472.80it/s]
Adding requests:  54%|█████▍    | 277/512 [00:00<00:00, 481.75it/s]
Adding requests:  64%|██████▍   | 327/512 [00:00<00:00, 486.43it/s]
Adding requests:  74%|███████▍  | 379/512 [00:00<00:00, 495.70it/s]
Adding requests:  84%|████████▍ | 431/512 [00:00<00:00, 500.36it/s]
Adding requests:  94%|█████████▍| 482/512 [00:01<00:00, 500.98it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 478.55it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  15%|█▌        | 78/512 [00:00<00:00, 687.20it/s, est. speed input: 703810.47 toks/s, output: 687.23 toks/s]
Processed prompts:  29%|██▊       | 147/512 [00:00<00:02, 145.43it/s, est. speed input: 170299.91 toks/s, output: 166.31 toks/s]
Processed prompts:  35%|███▌      | 181/512 [00:01<00:02, 126.14it/s, est. speed input: 148936.55 toks/s, output: 145.44 toks/s]
Processed prompts:  40%|███▉      | 204/512 [00:01<00:02, 114.01it/s, est. speed input: 137674.15 toks/s, output: 134.45 toks/s]
Processed prompts:  43%|████▎     | 221/512 [00:01<00:02, 109.56it/s, est. speed input: 133178.76 toks/s, output: 130.06 toks/s]
Processed prompts:  46%|████▌     | 236/512 [00:01<00:02, 103.28it/s, est. speed input: 128521.46 toks/s, output: 125.51 toks/s]
Processed prompts:  49%|████▊     | 249/512 [00:02<00:02, 101.92it/s, est. speed input: 126535.73 toks/s, output: 123.57 toks/s]
Processed prompts:  51%|█████     | 261/512 [00:02<00:02, 98.97it/s, est. speed input: 124289.43 toks/s, output: 121.38 toks/s] 
Processed prompts:  53%|█████▎    | 272/512 [00:02<00:02, 94.66it/s, est. speed input: 121844.71 toks/s, output: 118.99 toks/s]
Processed prompts:  55%|█████▌    | 282/512 [00:02<00:02, 89.25it/s, est. speed input: 119242.95 toks/s, output: 116.45 toks/s]
Processed prompts:  57%|█████▋    | 294/512 [00:02<00:02, 89.01it/s, est. speed input: 117714.74 toks/s, output: 114.95 toks/s]
Processed prompts:  60%|█████▉    | 306/512 [00:02<00:02, 88.70it/s, est. speed input: 116308.96 toks/s, output: 113.58 toks/s]
Processed prompts:  62%|██████▏   | 318/512 [00:02<00:02, 88.61it/s, est. speed input: 115071.76 toks/s, output: 112.37 toks/s]
Processed prompts:  64%|██████▍   | 330/512 [00:02<00:02, 88.61it/s, est. speed input: 113961.34 toks/s, output: 111.29 toks/s]
Processed prompts:  67%|██████▋   | 342/512 [00:03<00:01, 89.63it/s, est. speed input: 113139.31 toks/s, output: 110.49 toks/s]
Processed prompts:  69%|██████▉   | 354/512 [00:03<00:01, 89.58it/s, est. speed input: 112246.12 toks/s, output: 109.61 toks/s]
Processed prompts:  71%|███████▏  | 366/512 [00:03<00:01, 89.21it/s, est. speed input: 111365.41 toks/s, output: 108.75 toks/s]
Processed prompts:  74%|███████▍  | 378/512 [00:03<00:01, 88.86it/s, est. speed input: 110539.37 toks/s, output: 107.95 toks/s]
Processed prompts:  76%|███████▌  | 390/512 [00:03<00:01, 88.56it/s, est. speed input: 109767.39 toks/s, output: 107.19 toks/s]
Processed prompts:  79%|███████▊  | 402/512 [00:03<00:01, 88.48it/s, est. speed input: 109070.78 toks/s, output: 106.51 toks/s]
Processed prompts:  81%|████████  | 414/512 [00:03<00:01, 88.54it/s, est. speed input: 108437.80 toks/s, output: 105.90 toks/s]
Processed prompts:  83%|████████▎ | 426/512 [00:04<00:00, 88.45it/s, est. speed input: 107829.83 toks/s, output: 105.30 toks/s]
Processed prompts:  86%|████████▌ | 438/512 [00:04<00:00, 88.47it/s, est. speed input: 107272.80 toks/s, output: 104.76 toks/s]
Processed prompts:  88%|████████▊ | 450/512 [00:04<00:00, 89.83it/s, est. speed input: 106918.86 toks/s, output: 104.41 toks/s]
Processed prompts:  90%|█████████ | 462/512 [00:04<00:00, 89.53it/s, est. speed input: 106433.35 toks/s, output: 103.94 toks/s]
Processed prompts:  93%|█████████▎| 474/512 [00:04<00:00, 89.08it/s, est. speed input: 105948.90 toks/s, output: 103.47 toks/s]
Processed prompts:  95%|█████████▍| 486/512 [00:04<00:00, 88.74it/s, est. speed input: 105490.22 toks/s, output: 103.02 toks/s]
Processed prompts:  97%|█████████▋| 498/512 [00:04<00:00, 88.60it/s, est. speed input: 105067.79 toks/s, output: 102.60 toks/s]
Processed prompts: 100%|█████████▉| 510/512 [00:04<00:00, 89.86it/s, est. speed input: 104811.23 toks/s, output: 102.35 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:04<00:00, 89.86it/s, est. speed input: 105216.53 toks/s, output: 102.75 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:04<00:00, 102.75it/s, est. speed input: 105216.53 toks/s, output: 102.75 toks/s]
[rank0]:[W128 08:46:56.235676084 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 45.1s

测试结果:
  Requests/s:   84.56
  Tokens/s:     86675.44
  Total Reqs:   512
  Elapsed:      6.05s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     86590.88

============================================================
[5/7] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 08:47:08 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3330187) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3330187) WARNING 01-28 08:47:24 [backends.py:609] Failed to read file <frozen os>
Throughput: 91.05 requests/s, 93326.39 total tokens/s, 91.05 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-28 08:47:08] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:47:08] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:47:08] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:47:08] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:47:08] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:47:08] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:47:08] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:47:08] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:47:08] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:47:08] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:47:08] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:47:08] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:47:08] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:47:08] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 08:47:15] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:47:15] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:47:15] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:47:15] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:47:15] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:47:15] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:47:15] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:47:15] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:47:15] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:47:15] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:47:15] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:47:15] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:47:15] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:47:15] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3330187) [2026-01-28 08:47:16] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3330187) [2026-01-28 08:47:16] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3330187) [2026-01-28 08:47:16] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3330187) [2026-01-28 08:47:16] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3330187) [2026-01-28 08:47:16] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3330187) [2026-01-28 08:47:16] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3330187) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3330187) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.26it/s]
(EngineCore_DP0 pid=3330187) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.26it/s]
(EngineCore_DP0 pid=3330187) 
(EngineCore_DP0 pid=3330187) [2026-01-28 08:47:17] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=3330187) [2026-01-28 08:47:17] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9216000 bytes
(EngineCore_DP0 pid=3330187) [2026-01-28 08:47:17] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=3330187) [2026-01-28 08:47:17] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6144000 bytes
(EngineCore_DP0 pid=3330187) [2026-01-28 08:47:17] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=3330187) [2026-01-28 08:47:17] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 33177600 bytes
(EngineCore_DP0 pid=3330187) [2026-01-28 08:47:17] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=3330187) [2026-01-28 08:47:17] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16588800 bytes
(EngineCore_DP0 pid=3330187) 2026-01-28 08:47:35,020 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3330187) 2026-01-28 08:47:35,050 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3330187) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 1/5 [00:00<00:00,  4.13it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 3/5 [00:00<00:00,  9.33it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 11.54it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 10.07it/s]
(EngineCore_DP0 pid=3330187) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00, 16.84it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 17.55it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 17.42it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   3%|▎         | 29/1024 [00:00<00:03, 289.69it/s]
Adding requests:   8%|▊         | 81/1024 [00:00<00:02, 423.19it/s]
Adding requests:  13%|█▎        | 132/1024 [00:00<00:01, 459.40it/s]
Adding requests:  18%|█▊        | 180/1024 [00:00<00:01, 467.43it/s]
Adding requests:  23%|██▎       | 232/1024 [00:00<00:01, 483.49it/s]
Adding requests:  28%|██▊       | 282/1024 [00:00<00:01, 488.51it/s]
Adding requests:  32%|███▏      | 331/1024 [00:00<00:01, 488.57it/s]
Adding requests:  37%|███▋      | 383/1024 [00:00<00:01, 493.90it/s]
Adding requests:  42%|████▏     | 434/1024 [00:00<00:01, 498.81it/s]
Adding requests:  47%|████▋     | 484/1024 [00:01<00:01, 496.11it/s]
Adding requests:  52%|█████▏    | 534/1024 [00:01<00:01, 482.28it/s]
Adding requests:  57%|█████▋    | 587/1024 [00:01<00:00, 495.30it/s]
Adding requests:  62%|██████▏   | 638/1024 [00:01<00:00, 498.25it/s]
Adding requests:  67%|██████▋   | 691/1024 [00:01<00:00, 507.05it/s]
Adding requests:  72%|███████▏  | 742/1024 [00:01<00:00, 507.33it/s]
Adding requests:  77%|███████▋  | 793/1024 [00:01<00:00, 507.30it/s]
Adding requests:  82%|████████▏ | 844/1024 [00:01<00:00, 498.58it/s]
Adding requests:  88%|████████▊ | 898/1024 [00:01<00:00, 508.58it/s]
Adding requests:  93%|█████████▎| 949/1024 [00:01<00:00, 508.66it/s]
Adding requests:  98%|█████████▊| 1002/1024 [00:02<00:00, 512.91it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 493.81it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  18%|█▊        | 186/1024 [00:00<00:00, 1555.94it/s, est. speed input: 1593711.68 toks/s, output: 1556.05 toks/s]
Processed prompts:  33%|███▎      | 342/1024 [00:01<00:04, 165.10it/s, est. speed input: 197929.48 toks/s, output: 193.29 toks/s]   
Processed prompts:  40%|████      | 412/1024 [00:02<00:04, 135.19it/s, est. speed input: 165080.45 toks/s, output: 161.21 toks/s]
Processed prompts:  44%|████▍     | 454/1024 [00:02<00:04, 126.42it/s, est. speed input: 155750.50 toks/s, output: 152.10 toks/s]
Processed prompts:  47%|████▋     | 484/1024 [00:03<00:04, 117.71it/s, est. speed input: 148630.32 toks/s, output: 145.15 toks/s]
Processed prompts:  49%|████▉     | 506/1024 [00:03<00:04, 111.48it/s, est. speed input: 144105.05 toks/s, output: 140.73 toks/s]
Processed prompts:  51%|█████     | 524/1024 [00:03<00:04, 110.20it/s, est. speed input: 142312.93 toks/s, output: 138.98 toks/s]
Processed prompts:  53%|█████▎    | 539/1024 [00:03<00:04, 105.90it/s, est. speed input: 139910.37 toks/s, output: 136.63 toks/s]
Processed prompts:  54%|█████▍    | 554/1024 [00:04<00:04, 101.78it/s, est. speed input: 137687.79 toks/s, output: 134.46 toks/s]
Processed prompts:  56%|█████▌    | 570/1024 [00:04<00:04, 99.43it/s, est. speed input: 135885.75 toks/s, output: 132.70 toks/s] 
Processed prompts:  57%|█████▋    | 586/1024 [00:04<00:04, 97.60it/s, est. speed input: 134256.67 toks/s, output: 131.11 toks/s]
Processed prompts:  59%|█████▉    | 602/1024 [00:04<00:04, 96.08it/s, est. speed input: 132738.85 toks/s, output: 129.63 toks/s]
Processed prompts:  60%|██████    | 618/1024 [00:04<00:04, 94.90it/s, est. speed input: 131332.52 toks/s, output: 128.25 toks/s]
Processed prompts:  62%|██████▏   | 634/1024 [00:04<00:04, 93.81it/s, est. speed input: 129988.30 toks/s, output: 126.94 toks/s]
Processed prompts:  63%|██████▎   | 650/1024 [00:05<00:04, 93.06it/s, est. speed input: 128745.96 toks/s, output: 125.73 toks/s]
Processed prompts:  65%|██████▌   | 666/1024 [00:05<00:03, 92.52it/s, est. speed input: 127584.92 toks/s, output: 124.59 toks/s]
Processed prompts:  67%|██████▋   | 682/1024 [00:05<00:03, 92.24it/s, est. speed input: 126513.46 toks/s, output: 123.55 toks/s]
Processed prompts:  68%|██████▊   | 698/1024 [00:05<00:03, 91.96it/s, est. speed input: 125497.83 toks/s, output: 122.56 toks/s]
Processed prompts:  70%|██████▉   | 714/1024 [00:05<00:03, 91.89it/s, est. speed input: 124558.67 toks/s, output: 121.64 toks/s]
Processed prompts:  71%|███████▏  | 730/1024 [00:06<00:03, 91.70it/s, est. speed input: 123656.25 toks/s, output: 120.76 toks/s]
Processed prompts:  73%|███████▎  | 746/1024 [00:06<00:03, 91.61it/s, est. speed input: 122809.92 toks/s, output: 119.93 toks/s]
Processed prompts:  74%|███████▍  | 762/1024 [00:06<00:02, 91.40it/s, est. speed input: 121992.27 toks/s, output: 119.13 toks/s]
Processed prompts:  76%|███████▌  | 778/1024 [00:06<00:02, 91.53it/s, est. speed input: 121251.34 toks/s, output: 118.41 toks/s]
Processed prompts:  78%|███████▊  | 794/1024 [00:06<00:02, 91.46it/s, est. speed input: 120530.02 toks/s, output: 117.70 toks/s]
Processed prompts:  79%|███████▉  | 810/1024 [00:06<00:02, 91.56it/s, est. speed input: 119862.08 toks/s, output: 117.05 toks/s]
Processed prompts:  81%|████████  | 826/1024 [00:07<00:02, 91.28it/s, est. speed input: 119188.44 toks/s, output: 116.39 toks/s]
Processed prompts:  82%|████████▏ | 842/1024 [00:07<00:01, 91.22it/s, est. speed input: 118562.87 toks/s, output: 115.78 toks/s]
Processed prompts:  84%|████████▍ | 858/1024 [00:07<00:01, 91.24it/s, est. speed input: 117972.18 toks/s, output: 115.21 toks/s]
Processed prompts:  85%|████████▌ | 874/1024 [00:07<00:01, 91.40it/s, est. speed input: 117424.54 toks/s, output: 114.67 toks/s]
Processed prompts:  87%|████████▋ | 890/1024 [00:07<00:01, 91.36it/s, est. speed input: 116884.71 toks/s, output: 114.14 toks/s]
Processed prompts:  88%|████████▊ | 906/1024 [00:07<00:01, 91.24it/s, est. speed input: 116360.77 toks/s, output: 113.63 toks/s]
Processed prompts:  90%|█████████ | 922/1024 [00:08<00:01, 91.22it/s, est. speed input: 115865.38 toks/s, output: 113.15 toks/s]
Processed prompts:  92%|█████████▏| 938/1024 [00:08<00:00, 92.60it/s, est. speed input: 115513.53 toks/s, output: 112.81 toks/s]
Processed prompts:  93%|█████████▎| 954/1024 [00:08<00:00, 92.27it/s, est. speed input: 115063.86 toks/s, output: 112.37 toks/s]
Processed prompts:  95%|█████████▍| 970/1024 [00:08<00:00, 92.08it/s, est. speed input: 114636.56 toks/s, output: 111.95 toks/s]
Processed prompts:  96%|█████████▋| 986/1024 [00:08<00:00, 93.35it/s, est. speed input: 114338.46 toks/s, output: 111.66 toks/s]
Processed prompts:  98%|█████████▊| 1002/1024 [00:09<00:00, 92.64it/s, est. speed input: 113925.72 toks/s, output: 111.26 toks/s]
Processed prompts:  99%|█████████▉| 1018/1024 [00:09<00:00, 94.06it/s, est. speed input: 113675.08 toks/s, output: 111.01 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:09<00:00, 94.06it/s, est. speed input: 114340.47 toks/s, output: 111.66 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:09<00:00, 111.66it/s, est. speed input: 114340.47 toks/s, output: 111.66 toks/s]
[rank0]:[W128 08:47:48.881189283 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 52.6s

测试结果:
  Requests/s:   91.05
  Tokens/s:     93326.39
  Total Reqs:   1024
  Elapsed:      11.25s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     93235.34

============================================================
[6/7] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 08:48:05 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3331603) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3331603) WARNING 01-28 08:48:21 [backends.py:609] Failed to read file <frozen os>
Throughput: 94.77 requests/s, 97143.72 total tokens/s, 94.77 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-28 08:48:05] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:48:05] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:48:05] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:48:05] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:48:05] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:48:05] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:48:05] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:48:05] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:48:05] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:48:05] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:48:05] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:48:05] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:48:05] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:48:05] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 08:48:12] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:48:12] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:48:12] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:48:12] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:48:12] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:48:12] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:48:12] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:48:12] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:48:12] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:48:12] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:48:12] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:48:12] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:48:12] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:48:12] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3331603) [2026-01-28 08:48:13] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3331603) [2026-01-28 08:48:13] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3331603) [2026-01-28 08:48:13] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3331603) [2026-01-28 08:48:13] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3331603) [2026-01-28 08:48:13] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3331603) [2026-01-28 08:48:13] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3331603) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3331603) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.27it/s]
(EngineCore_DP0 pid=3331603) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.27it/s]
(EngineCore_DP0 pid=3331603) 
(EngineCore_DP0 pid=3331603) [2026-01-28 08:48:14] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=3331603) [2026-01-28 08:48:14] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9216000 bytes
(EngineCore_DP0 pid=3331603) [2026-01-28 08:48:14] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=3331603) [2026-01-28 08:48:14] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6144000 bytes
(EngineCore_DP0 pid=3331603) [2026-01-28 08:48:14] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=3331603) [2026-01-28 08:48:14] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 33177600 bytes
(EngineCore_DP0 pid=3331603) [2026-01-28 08:48:14] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=3331603) [2026-01-28 08:48:14] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16588800 bytes
(EngineCore_DP0 pid=3331603) 2026-01-28 08:48:31,917 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3331603) 2026-01-28 08:48:31,946 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3331603) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00, 12.54it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00, 14.91it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:00<00:00, 11.37it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 10.87it/s]
(EngineCore_DP0 pid=3331603) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00, 13.22it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00, 15.45it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 15.48it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 33/2048 [00:00<00:06, 328.19it/s]
Adding requests:   4%|▍         | 84/2048 [00:00<00:04, 432.70it/s]
Adding requests:   7%|▋         | 134/2048 [00:00<00:04, 460.57it/s]
Adding requests:   9%|▉         | 183/2048 [00:00<00:03, 471.79it/s]
Adding requests:  11%|█▏        | 235/2048 [00:00<00:03, 487.21it/s]
Adding requests:  14%|█▍        | 286/2048 [00:00<00:03, 494.23it/s]
Adding requests:  16%|█▋        | 336/2048 [00:00<00:03, 493.89it/s]
Adding requests:  19%|█▉        | 388/2048 [00:00<00:03, 500.63it/s]
Adding requests:  21%|██▏       | 439/2048 [00:00<00:03, 503.11it/s]
Adding requests:  24%|██▍       | 490/2048 [00:01<00:03, 503.50it/s]
Adding requests:  26%|██▋       | 541/2048 [00:01<00:03, 495.59it/s]
Adding requests:  29%|██▉       | 594/2048 [00:01<00:02, 504.73it/s]
Adding requests:  32%|███▏      | 646/2048 [00:01<00:02, 506.80it/s]
Adding requests:  34%|███▍      | 700/2048 [00:01<00:02, 514.97it/s]
Adding requests:  37%|███▋      | 752/2048 [00:01<00:02, 506.19it/s]
Adding requests:  39%|███▉      | 803/2048 [00:01<00:02, 502.86it/s]
Adding requests:  42%|████▏     | 854/2048 [00:01<00:02, 470.35it/s]
Adding requests:  44%|████▍     | 904/2048 [00:01<00:02, 476.78it/s]
Adding requests:  47%|████▋     | 953/2048 [00:01<00:02, 473.60it/s]
Adding requests:  49%|████▉     | 1002/2048 [00:02<00:02, 476.76it/s]
Adding requests:  51%|█████▏    | 1052/2048 [00:02<00:02, 481.37it/s]
Adding requests:  54%|█████▍    | 1101/2048 [00:02<00:01, 482.06it/s]
Adding requests:  56%|█████▌    | 1151/2048 [00:02<00:01, 487.31it/s]
Adding requests:  59%|█████▉    | 1206/2048 [00:02<00:01, 503.97it/s]
Adding requests:  61%|██████▏   | 1257/2048 [00:02<00:01, 476.53it/s]
Adding requests:  64%|██████▍   | 1308/2048 [00:02<00:01, 485.07it/s]
Adding requests:  66%|██████▋   | 1360/2048 [00:02<00:01, 494.28it/s]
Adding requests:  69%|██████▉   | 1414/2048 [00:02<00:01, 505.08it/s]
Adding requests:  72%|███████▏  | 1466/2048 [00:02<00:01, 509.29it/s]
Adding requests:  74%|███████▍  | 1519/2048 [00:03<00:01, 513.58it/s]
Adding requests:  77%|███████▋  | 1572/2048 [00:03<00:00, 517.14it/s]
Adding requests:  79%|███████▉  | 1626/2048 [00:03<00:00, 522.44it/s]
Adding requests:  82%|████████▏ | 1679/2048 [00:03<00:00, 518.77it/s]
Adding requests:  85%|████████▍ | 1731/2048 [00:03<00:00, 518.39it/s]
Adding requests:  87%|████████▋ | 1783/2048 [00:03<00:00, 512.84it/s]
Adding requests:  90%|████████▉ | 1836/2048 [00:03<00:00, 516.50it/s]
Adding requests:  92%|█████████▏| 1888/2048 [00:03<00:00, 509.18it/s]
Adding requests:  95%|█████████▍| 1940/2048 [00:03<00:00, 509.41it/s]
Adding requests:  97%|█████████▋| 1992/2048 [00:04<00:00, 511.22it/s]
Adding requests: 100%|█████████▉| 2046/2048 [00:04<00:00, 517.52it/s]
Adding requests: 100%|██████████| 2048/2048 [00:04<00:00, 497.74it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  19%|█▉        | 386/2048 [00:00<00:00, 2154.40it/s, est. speed input: 2206410.82 toks/s, output: 2154.49 toks/s]
Processed prompts:  29%|██▉       | 602/2048 [00:02<00:06, 210.87it/s, est. speed input: 261272.83 toks/s, output: 255.15 toks/s]   
Processed prompts:  34%|███▍      | 697/2048 [00:03<00:08, 165.94it/s, est. speed input: 211861.19 toks/s, output: 206.89 toks/s]
Processed prompts:  37%|███▋      | 753/2048 [00:03<00:08, 154.15it/s, est. speed input: 199108.34 toks/s, output: 194.44 toks/s]
Processed prompts:  39%|███▊      | 792/2048 [00:04<00:09, 136.00it/s, est. speed input: 185244.38 toks/s, output: 180.90 toks/s]
Processed prompts:  40%|████      | 820/2048 [00:04<00:09, 126.39it/s, est. speed input: 178138.30 toks/s, output: 173.96 toks/s]
Processed prompts:  41%|████      | 841/2048 [00:04<00:09, 126.20it/s, est. speed input: 176399.41 toks/s, output: 172.26 toks/s]
Processed prompts:  42%|████▏     | 860/2048 [00:05<00:09, 124.25it/s, est. speed input: 174354.48 toks/s, output: 170.27 toks/s]
Processed prompts:  43%|████▎     | 876/2048 [00:05<00:09, 119.26it/s, est. speed input: 171851.27 toks/s, output: 167.82 toks/s]
Processed prompts:  43%|████▎     | 890/2048 [00:05<00:10, 112.19it/s, est. speed input: 169126.07 toks/s, output: 165.16 toks/s]
Processed prompts:  44%|████▍     | 903/2048 [00:05<00:10, 104.70it/s, est. speed input: 166418.28 toks/s, output: 162.52 toks/s]
Processed prompts:  45%|████▍     | 914/2048 [00:05<00:11, 95.24it/s, est. speed input: 163463.29 toks/s, output: 159.63 toks/s] 
Processed prompts:  45%|████▌     | 930/2048 [00:05<00:11, 96.37it/s, est. speed input: 161798.74 toks/s, output: 158.00 toks/s]
Processed prompts:  46%|████▌     | 946/2048 [00:06<00:11, 95.90it/s, est. speed input: 159984.52 toks/s, output: 156.23 toks/s]
Processed prompts:  47%|████▋     | 962/2048 [00:06<00:11, 95.60it/s, est. speed input: 158280.25 toks/s, output: 154.57 toks/s]
Processed prompts:  48%|████▊     | 978/2048 [00:06<00:11, 96.80it/s, est. speed input: 156879.65 toks/s, output: 153.20 toks/s]
Processed prompts:  49%|████▊     | 994/2048 [00:06<00:10, 96.43it/s, est. speed input: 155369.60 toks/s, output: 151.73 toks/s]
Processed prompts:  49%|████▉     | 1010/2048 [00:06<00:10, 95.90it/s, est. speed input: 153898.33 toks/s, output: 150.29 toks/s]
Processed prompts:  50%|█████     | 1026/2048 [00:06<00:10, 95.57it/s, est. speed input: 152505.08 toks/s, output: 148.93 toks/s]
Processed prompts:  51%|█████     | 1042/2048 [00:07<00:10, 95.33it/s, est. speed input: 151179.09 toks/s, output: 147.64 toks/s]
Processed prompts:  52%|█████▏    | 1058/2048 [00:07<00:10, 95.03it/s, est. speed input: 149897.25 toks/s, output: 146.38 toks/s]
Processed prompts:  52%|█████▏    | 1074/2048 [00:07<00:10, 95.02it/s, est. speed input: 148699.02 toks/s, output: 145.21 toks/s]
Processed prompts:  53%|█████▎    | 1090/2048 [00:07<00:10, 94.90it/s, est. speed input: 147541.32 toks/s, output: 144.08 toks/s]
Processed prompts:  54%|█████▍    | 1106/2048 [00:07<00:09, 94.81it/s, est. speed input: 146432.50 toks/s, output: 143.00 toks/s]
Processed prompts:  55%|█████▍    | 1122/2048 [00:07<00:09, 94.88it/s, est. speed input: 145386.88 toks/s, output: 141.98 toks/s]
Processed prompts:  56%|█████▌    | 1138/2048 [00:08<00:09, 94.75it/s, est. speed input: 144365.11 toks/s, output: 140.98 toks/s]
Processed prompts:  56%|█████▋    | 1154/2048 [00:08<00:09, 96.45it/s, est. speed input: 143567.60 toks/s, output: 140.20 toks/s]
Processed prompts:  57%|█████▋    | 1170/2048 [00:08<00:09, 96.09it/s, est. speed input: 142647.16 toks/s, output: 139.30 toks/s]
Processed prompts:  58%|█████▊    | 1186/2048 [00:08<00:09, 95.65it/s, est. speed input: 141745.48 toks/s, output: 138.42 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [00:08<00:08, 95.41it/s, est. speed input: 140883.39 toks/s, output: 137.58 toks/s]
Processed prompts:  59%|█████▉    | 1218/2048 [00:08<00:08, 94.99it/s, est. speed input: 140031.81 toks/s, output: 136.75 toks/s]
Processed prompts:  60%|██████    | 1234/2048 [00:09<00:08, 94.87it/s, est. speed input: 139226.66 toks/s, output: 135.96 toks/s]
Processed prompts:  61%|██████    | 1250/2048 [00:09<00:08, 94.80it/s, est. speed input: 138452.47 toks/s, output: 135.21 toks/s]
Processed prompts:  62%|██████▏   | 1266/2048 [00:09<00:08, 96.49it/s, est. speed input: 137854.96 toks/s, output: 134.62 toks/s]
Processed prompts:  63%|██████▎   | 1282/2048 [00:09<00:07, 95.82it/s, est. speed input: 137123.56 toks/s, output: 133.91 toks/s]
Processed prompts:  63%|██████▎   | 1298/2048 [00:09<00:07, 95.58it/s, est. speed input: 136435.29 toks/s, output: 133.24 toks/s]
Processed prompts:  64%|██████▍   | 1314/2048 [00:09<00:07, 95.17it/s, est. speed input: 135750.89 toks/s, output: 132.57 toks/s]
Processed prompts:  65%|██████▍   | 1330/2048 [00:10<00:07, 95.12it/s, est. speed input: 135108.03 toks/s, output: 131.94 toks/s]
Processed prompts:  66%|██████▌   | 1346/2048 [00:10<00:07, 94.91it/s, est. speed input: 134472.87 toks/s, output: 131.32 toks/s]
Processed prompts:  67%|██████▋   | 1362/2048 [00:10<00:07, 95.02it/s, est. speed input: 133877.83 toks/s, output: 130.74 toks/s]
Processed prompts:  67%|██████▋   | 1378/2048 [00:10<00:07, 94.80it/s, est. speed input: 133279.94 toks/s, output: 130.16 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [00:10<00:06, 94.85it/s, est. speed input: 132715.56 toks/s, output: 129.60 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [00:10<00:06, 94.61it/s, est. speed input: 132148.87 toks/s, output: 129.05 toks/s]
Processed prompts:  70%|██████▉   | 1426/2048 [00:11<00:06, 94.56it/s, est. speed input: 131607.16 toks/s, output: 128.52 toks/s]
Processed prompts:  70%|███████   | 1442/2048 [00:11<00:06, 94.54it/s, est. speed input: 131083.26 toks/s, output: 128.01 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [00:11<00:06, 94.57it/s, est. speed input: 130578.19 toks/s, output: 127.52 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [00:11<00:06, 94.47it/s, est. speed input: 130079.38 toks/s, output: 127.03 toks/s]
Processed prompts:  73%|███████▎  | 1490/2048 [00:11<00:05, 94.49it/s, est. speed input: 129600.86 toks/s, output: 126.56 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [00:11<00:05, 94.49it/s, est. speed input: 129135.19 toks/s, output: 126.11 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [00:12<00:05, 94.50it/s, est. speed input: 128682.98 toks/s, output: 125.67 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [00:12<00:05, 94.64it/s, est. speed input: 128251.86 toks/s, output: 125.25 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [00:12<00:05, 94.59it/s, est. speed input: 127823.13 toks/s, output: 124.83 toks/s]
Processed prompts:  77%|███████▋  | 1570/2048 [00:12<00:05, 94.76it/s, est. speed input: 127418.05 toks/s, output: 124.43 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [00:12<00:04, 96.12it/s, est. speed input: 127096.08 toks/s, output: 124.12 toks/s]
Processed prompts:  78%|███████▊  | 1602/2048 [00:12<00:04, 95.50it/s, est. speed input: 126692.37 toks/s, output: 123.72 toks/s]
Processed prompts:  79%|███████▉  | 1618/2048 [00:13<00:04, 95.15it/s, est. speed input: 126303.11 toks/s, output: 123.34 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [00:13<00:04, 95.08it/s, est. speed input: 125933.70 toks/s, output: 122.98 toks/s]
Processed prompts:  81%|████████  | 1650/2048 [00:13<00:04, 94.89it/s, est. speed input: 125566.47 toks/s, output: 122.62 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [00:13<00:04, 94.70it/s, est. speed input: 125204.08 toks/s, output: 122.27 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [00:13<00:03, 94.68it/s, est. speed input: 124857.08 toks/s, output: 121.93 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [00:13<00:03, 94.58it/s, est. speed input: 124514.29 toks/s, output: 121.60 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [00:14<00:03, 94.43it/s, est. speed input: 124175.17 toks/s, output: 121.26 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [00:14<00:03, 94.50it/s, est. speed input: 123853.46 toks/s, output: 120.95 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [00:14<00:03, 94.50it/s, est. speed input: 123536.69 toks/s, output: 120.64 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [00:14<00:03, 94.47it/s, est. speed input: 123225.68 toks/s, output: 120.34 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [00:14<00:02, 94.49it/s, est. speed input: 122923.67 toks/s, output: 120.04 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [00:14<00:02, 94.65it/s, est. speed input: 122635.70 toks/s, output: 119.76 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [00:15<00:02, 94.47it/s, est. speed input: 122340.39 toks/s, output: 119.47 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [00:15<00:02, 94.65it/s, est. speed input: 122065.51 toks/s, output: 119.20 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [00:15<00:02, 94.37it/s, est. speed input: 121777.92 toks/s, output: 118.92 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [00:15<00:02, 94.45it/s, est. speed input: 121509.25 toks/s, output: 118.66 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [00:15<00:01, 95.94it/s, est. speed input: 121311.34 toks/s, output: 118.47 toks/s]
Processed prompts:  92%|█████████▏| 1890/2048 [00:15<00:01, 95.34it/s, est. speed input: 121044.33 toks/s, output: 118.21 toks/s]
Processed prompts:  93%|█████████▎| 1906/2048 [00:16<00:01, 95.08it/s, est. speed input: 120789.28 toks/s, output: 117.96 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [00:16<00:01, 94.87it/s, est. speed input: 120538.93 toks/s, output: 117.71 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [00:16<00:01, 94.66it/s, est. speed input: 120290.67 toks/s, output: 117.47 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [00:16<00:00, 96.09it/s, est. speed input: 120114.32 toks/s, output: 117.30 toks/s]
Processed prompts:  96%|█████████▌| 1970/2048 [00:16<00:00, 95.60it/s, est. speed input: 119878.89 toks/s, output: 117.07 toks/s]
Processed prompts:  97%|█████████▋| 1986/2048 [00:16<00:00, 95.21it/s, est. speed input: 119646.45 toks/s, output: 116.84 toks/s]
Processed prompts:  98%|█████████▊| 2002/2048 [00:17<00:00, 95.07it/s, est. speed input: 119423.80 toks/s, output: 116.62 toks/s]
Processed prompts:  99%|█████████▊| 2018/2048 [00:17<00:00, 95.13it/s, est. speed input: 119211.66 toks/s, output: 116.42 toks/s]
Processed prompts:  99%|█████████▉| 2034/2048 [00:17<00:00, 96.90it/s, est. speed input: 119072.14 toks/s, output: 116.28 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:17<00:00, 96.90it/s, est. speed input: 119889.03 toks/s, output: 117.08 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:17<00:00, 117.08it/s, est. speed input: 119889.03 toks/s, output: 117.08 toks/s]
[rank0]:[W128 08:48:56.414928538 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 67.6s

测试结果:
  Requests/s:   94.77
  Tokens/s:     97143.72
  Total Reqs:   2048
  Elapsed:      21.61s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     97048.95

============================================================
[7/7] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 08:49:22 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3333125) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3333125) WARNING 01-28 08:49:37 [backends.py:609] Failed to read file <frozen os>
Throughput: 96.82 requests/s, 99237.19 total tokens/s, 96.82 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-28 08:49:21] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:49:22] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:49:22] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:49:22] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:49:22] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:49:22] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:49:22] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:49:22] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:49:22] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:49:22] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:49:22] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:49:22] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:49:22] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:49:22] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 08:49:28] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:49:29] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:49:29] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:49:29] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:49:29] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:49:29] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:49:29] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:49:29] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:49:29] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:49:29] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:49:29] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:49:29] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:49:29] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:49:29] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3333125) [2026-01-28 08:49:30] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3333125) [2026-01-28 08:49:30] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3333125) [2026-01-28 08:49:30] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3333125) [2026-01-28 08:49:30] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3333125) [2026-01-28 08:49:30] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3333125) [2026-01-28 08:49:30] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3333125) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3333125) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.27it/s]
(EngineCore_DP0 pid=3333125) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.27it/s]
(EngineCore_DP0 pid=3333125) 
(EngineCore_DP0 pid=3333125) [2026-01-28 08:49:31] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=3333125) [2026-01-28 08:49:31] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9216000 bytes
(EngineCore_DP0 pid=3333125) [2026-01-28 08:49:31] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=3333125) [2026-01-28 08:49:31] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6144000 bytes
(EngineCore_DP0 pid=3333125) [2026-01-28 08:49:31] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=3333125) [2026-01-28 08:49:31] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 33177600 bytes
(EngineCore_DP0 pid=3333125) [2026-01-28 08:49:31] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=3333125) [2026-01-28 08:49:31] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16588800 bytes
(EngineCore_DP0 pid=3333125) [rank0]:W0128 08:49:43.897000 3333125 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3333125) [rank0]:W0128 08:49:43.979000 3333125 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3333125) [rank0]:W0128 08:49:45.156000 3333125 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3333125) [rank0]:W0128 08:49:45.319000 3333125 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3333125) 2026-01-28 08:49:49,206 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3333125) 2026-01-28 08:49:49,241 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3333125) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▉         | 1/11 [00:00<00:02,  4.68it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 3/11 [00:00<00:00,  9.35it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 5/11 [00:00<00:00, 12.08it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▎   | 7/11 [00:00<00:00, 13.90it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 9/11 [00:00<00:00, 15.09it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 14.99it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 13.16it/s]
(EngineCore_DP0 pid=3333125) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00, 16.69it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00, 17.49it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00, 17.73it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 17.65it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 49/4096 [00:00<00:08, 488.92it/s]
Adding requests:   2%|▏         | 99/4096 [00:00<00:08, 492.00it/s]
Adding requests:   4%|▎         | 149/4096 [00:00<00:07, 493.50it/s]
Adding requests:   5%|▍         | 199/4096 [00:00<00:07, 495.36it/s]
Adding requests:   6%|▌         | 251/4096 [00:00<00:07, 504.10it/s]
Adding requests:   7%|▋         | 302/4096 [00:00<00:07, 499.73it/s]
Adding requests:   9%|▊         | 354/4096 [00:00<00:07, 505.54it/s]
Adding requests:  10%|▉         | 405/4096 [00:00<00:07, 506.40it/s]
Adding requests:  11%|█         | 456/4096 [00:00<00:07, 507.10it/s]
Adding requests:  12%|█▏        | 507/4096 [00:01<00:07, 504.48it/s]
Adding requests:  14%|█▎        | 558/4096 [00:01<00:07, 500.89it/s]
Adding requests:  15%|█▍        | 609/4096 [00:01<00:06, 502.48it/s]
Adding requests:  16%|█▌        | 662/4096 [00:01<00:06, 509.28it/s]
Adding requests:  17%|█▋        | 716/4096 [00:01<00:06, 515.34it/s]
Adding requests:  19%|█▉        | 768/4096 [00:01<00:06, 513.35it/s]
Adding requests:  20%|██        | 820/4096 [00:01<00:06, 502.59it/s]
Adding requests:  21%|██▏       | 871/4096 [00:01<00:06, 502.74it/s]
Adding requests:  23%|██▎       | 924/4096 [00:01<00:06, 508.16it/s]
Adding requests:  24%|██▍       | 976/4096 [00:01<00:06, 510.13it/s]
Adding requests:  25%|██▌       | 1028/4096 [00:02<00:06, 507.63it/s]
Adding requests:  26%|██▋       | 1079/4096 [00:02<00:06, 498.28it/s]
Adding requests:  28%|██▊       | 1129/4096 [00:02<00:05, 498.20it/s]
Adding requests:  29%|██▉       | 1182/4096 [00:02<00:05, 506.64it/s]
Adding requests:  30%|███       | 1234/4096 [00:02<00:05, 508.91it/s]
Adding requests:  31%|███▏      | 1285/4096 [00:02<00:05, 498.47it/s]
Adding requests:  33%|███▎      | 1337/4096 [00:02<00:05, 501.64it/s]
Adding requests:  34%|███▍      | 1389/4096 [00:02<00:05, 505.76it/s]
Adding requests:  35%|███▌      | 1441/4096 [00:02<00:05, 507.30it/s]
Adding requests:  36%|███▋      | 1494/4096 [00:02<00:05, 511.67it/s]
Adding requests:  38%|███▊      | 1546/4096 [00:03<00:04, 512.60it/s]
Adding requests:  39%|███▉      | 1599/4096 [00:03<00:04, 516.59it/s]
Adding requests:  40%|████      | 1652/4096 [00:03<00:04, 518.36it/s]
Adding requests:  42%|████▏     | 1704/4096 [00:03<00:04, 508.32it/s]
Adding requests:  43%|████▎     | 1755/4096 [00:03<00:04, 508.76it/s]
Adding requests:  44%|████▍     | 1807/4096 [00:03<00:04, 510.48it/s]
Adding requests:  45%|████▌     | 1859/4096 [00:03<00:04, 511.84it/s]
Adding requests:  47%|████▋     | 1911/4096 [00:03<00:04, 508.63it/s]
Adding requests:  48%|████▊     | 1962/4096 [00:03<00:04, 508.55it/s]
Adding requests:  49%|████▉     | 2014/4096 [00:03<00:04, 510.16it/s]
Adding requests:  50%|█████     | 2066/4096 [00:04<00:03, 512.26it/s]
Adding requests:  52%|█████▏    | 2118/4096 [00:04<00:03, 512.90it/s]
Adding requests:  53%|█████▎    | 2170/4096 [00:04<00:03, 504.75it/s]
Adding requests:  54%|█████▍    | 2221/4096 [00:04<00:03, 505.07it/s]
Adding requests:  55%|█████▌    | 2272/4096 [00:04<00:03, 496.28it/s]
Adding requests:  57%|█████▋    | 2323/4096 [00:04<00:03, 499.96it/s]
Adding requests:  58%|█████▊    | 2374/4096 [00:04<00:03, 500.96it/s]
Adding requests:  59%|█████▉    | 2425/4096 [00:04<00:03, 503.04it/s]
Adding requests:  60%|██████    | 2476/4096 [00:04<00:03, 503.33it/s]
Adding requests:  62%|██████▏   | 2527/4096 [00:04<00:03, 502.38it/s]
Adding requests:  63%|██████▎   | 2580/4096 [00:05<00:02, 509.90it/s]
Adding requests:  64%|██████▍   | 2632/4096 [00:05<00:02, 506.37it/s]
Adding requests:  66%|██████▌   | 2684/4096 [00:05<00:02, 508.08it/s]
Adding requests:  67%|██████▋   | 2735/4096 [00:05<00:02, 503.51it/s]
Adding requests:  68%|██████▊   | 2786/4096 [00:05<00:02, 504.75it/s]
Adding requests:  69%|██████▉   | 2837/4096 [00:05<00:02, 501.31it/s]
Adding requests:  71%|███████   | 2890/4096 [00:05<00:02, 508.31it/s]
Adding requests:  72%|███████▏  | 2941/4096 [00:05<00:02, 500.90it/s]
Adding requests:  73%|███████▎  | 2993/4096 [00:05<00:02, 504.26it/s]
Adding requests:  74%|███████▍  | 3044/4096 [00:06<00:02, 501.38it/s]
Adding requests:  76%|███████▌  | 3095/4096 [00:06<00:01, 500.80it/s]
Adding requests:  77%|███████▋  | 3146/4096 [00:06<00:01, 500.62it/s]
Adding requests:  78%|███████▊  | 3197/4096 [00:06<00:01, 502.38it/s]
Adding requests:  79%|███████▉  | 3250/4096 [00:06<00:01, 509.37it/s]
Adding requests:  81%|████████  | 3301/4096 [00:06<00:01, 508.59it/s]
Adding requests:  82%|████████▏ | 3352/4096 [00:06<00:01, 506.34it/s]
Adding requests:  83%|████████▎ | 3403/4096 [00:06<00:01, 505.98it/s]
Adding requests:  84%|████████▍ | 3454/4096 [00:06<00:01, 506.06it/s]
Adding requests:  86%|████████▌ | 3505/4096 [00:06<00:01, 501.83it/s]
Adding requests:  87%|████████▋ | 3556/4096 [00:07<00:01, 503.94it/s]
Adding requests:  88%|████████▊ | 3607/4096 [00:07<00:00, 492.10it/s]
Adding requests:  89%|████████▉ | 3657/4096 [00:07<00:00, 493.24it/s]
Adding requests:  91%|█████████ | 3709/4096 [00:07<00:00, 500.48it/s]
Adding requests:  92%|█████████▏| 3760/4096 [00:07<00:00, 502.14it/s]
Adding requests:  93%|█████████▎| 3813/4096 [00:07<00:00, 508.17it/s]
Adding requests:  94%|█████████▍| 3866/4096 [00:07<00:00, 512.81it/s]
Adding requests:  96%|█████████▌| 3918/4096 [00:07<00:00, 512.49it/s]
Adding requests:  97%|█████████▋| 3970/4096 [00:07<00:00, 507.60it/s]
Adding requests:  98%|█████████▊| 4022/4096 [00:07<00:00, 509.99it/s]
Adding requests:  99%|█████████▉| 4074/4096 [00:08<00:00, 503.38it/s]
Adding requests: 100%|██████████| 4096/4096 [00:08<00:00, 505.42it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  19%|█▉        | 770/4096 [00:00<00:01, 2660.44it/s, est. speed input: 2724512.37 toks/s, output: 2660.50 toks/s]
Processed prompts:  25%|██▌       | 1037/4096 [00:02<00:10, 285.35it/s, est. speed input: 364728.66 toks/s, output: 356.18 toks/s]  
Processed prompts:  28%|██▊       | 1153/4096 [00:03<00:12, 229.05it/s, est. speed input: 302436.99 toks/s, output: 295.35 toks/s]
Processed prompts:  30%|██▉       | 1222/4096 [00:04<00:16, 178.04it/s, est. speed input: 256050.68 toks/s, output: 250.05 toks/s]
Processed prompts:  31%|███       | 1266/4096 [00:05<00:16, 172.62it/s, est. speed input: 248866.98 toks/s, output: 243.03 toks/s]
Processed prompts:  32%|███▏      | 1300/4096 [00:05<00:17, 161.21it/s, est. speed input: 240193.01 toks/s, output: 234.56 toks/s]
Processed prompts:  32%|███▏      | 1326/4096 [00:05<00:18, 145.86it/s, est. speed input: 231229.22 toks/s, output: 225.81 toks/s]
Processed prompts:  33%|███▎      | 1346/4096 [00:06<00:21, 127.90it/s, est. speed input: 222212.65 toks/s, output: 217.00 toks/s]
Processed prompts:  34%|███▎      | 1378/4096 [00:06<00:22, 120.62it/s, est. speed input: 215950.94 toks/s, output: 210.89 toks/s]
Processed prompts:  34%|███▍      | 1410/4096 [00:06<00:23, 114.76it/s, est. speed input: 210342.94 toks/s, output: 205.41 toks/s]
Processed prompts:  35%|███▌      | 1442/4096 [00:07<00:24, 110.00it/s, est. speed input: 205212.97 toks/s, output: 200.40 toks/s]
Processed prompts:  36%|███▌      | 1474/4096 [00:07<00:24, 106.37it/s, est. speed input: 200540.49 toks/s, output: 195.84 toks/s]
Processed prompts:  37%|███▋      | 1506/4096 [00:07<00:24, 103.78it/s, est. speed input: 196299.87 toks/s, output: 191.70 toks/s]
Processed prompts:  38%|███▊      | 1538/4096 [00:08<00:25, 101.68it/s, est. speed input: 192346.72 toks/s, output: 187.84 toks/s]
Processed prompts:  38%|███▊      | 1570/4096 [00:08<00:24, 101.05it/s, est. speed input: 188925.60 toks/s, output: 184.50 toks/s]
Processed prompts:  39%|███▉      | 1602/4096 [00:08<00:24, 99.84it/s, est. speed input: 185577.93 toks/s, output: 181.23 toks/s] 
Processed prompts:  40%|███▉      | 1634/4096 [00:09<00:24, 98.85it/s, est. speed input: 182439.63 toks/s, output: 178.16 toks/s]
Processed prompts:  41%|████      | 1666/4096 [00:09<00:24, 98.08it/s, est. speed input: 179506.58 toks/s, output: 175.30 toks/s]
Processed prompts:  41%|████▏     | 1698/4096 [00:09<00:24, 97.58it/s, est. speed input: 176781.16 toks/s, output: 172.64 toks/s]
Processed prompts:  42%|████▏     | 1730/4096 [00:10<00:24, 97.36it/s, est. speed input: 174257.66 toks/s, output: 170.17 toks/s]
Processed prompts:  43%|████▎     | 1762/4096 [00:10<00:24, 97.01it/s, est. speed input: 171857.34 toks/s, output: 167.83 toks/s]
Processed prompts:  44%|████▍     | 1794/4096 [00:10<00:23, 96.88it/s, est. speed input: 169624.32 toks/s, output: 165.65 toks/s]
Processed prompts:  45%|████▍     | 1826/4096 [00:11<00:23, 96.80it/s, est. speed input: 167526.74 toks/s, output: 163.60 toks/s]
Processed prompts:  45%|████▌     | 1858/4096 [00:11<00:22, 97.58it/s, est. speed input: 165686.53 toks/s, output: 161.80 toks/s]
Processed prompts:  46%|████▌     | 1890/4096 [00:11<00:22, 97.35it/s, est. speed input: 163824.32 toks/s, output: 159.98 toks/s]
Processed prompts:  47%|████▋     | 1922/4096 [00:12<00:22, 97.26it/s, est. speed input: 162075.15 toks/s, output: 158.28 toks/s]
Processed prompts:  48%|████▊     | 1954/4096 [00:12<00:21, 98.06it/s, est. speed input: 160542.20 toks/s, output: 156.78 toks/s]
Processed prompts:  48%|████▊     | 1986/4096 [00:12<00:21, 97.66it/s, est. speed input: 158952.28 toks/s, output: 155.23 toks/s]
Processed prompts:  49%|████▉     | 2018/4096 [00:13<00:21, 97.47it/s, est. speed input: 157455.03 toks/s, output: 153.76 toks/s]
Processed prompts:  50%|█████     | 2050/4096 [00:13<00:21, 96.78it/s, est. speed input: 155956.77 toks/s, output: 152.30 toks/s]
Processed prompts:  51%|█████     | 2082/4096 [00:13<00:20, 97.11it/s, est. speed input: 154635.88 toks/s, output: 151.01 toks/s]
Processed prompts:  52%|█████▏    | 2114/4096 [00:14<00:20, 97.04it/s, est. speed input: 153339.86 toks/s, output: 149.75 toks/s]
Processed prompts:  52%|█████▏    | 2146/4096 [00:14<00:20, 96.94it/s, est. speed input: 152095.71 toks/s, output: 148.53 toks/s]
Processed prompts:  53%|█████▎    | 2178/4096 [00:14<00:19, 96.90it/s, est. speed input: 150910.30 toks/s, output: 147.37 toks/s]
Processed prompts:  54%|█████▍    | 2210/4096 [00:15<00:19, 98.54it/s, est. speed input: 149962.46 toks/s, output: 146.45 toks/s]
Processed prompts:  55%|█████▍    | 2242/4096 [00:15<00:18, 97.88it/s, est. speed input: 148858.64 toks/s, output: 145.37 toks/s]
Processed prompts:  56%|█████▌    | 2274/4096 [00:15<00:18, 98.59it/s, est. speed input: 147922.79 toks/s, output: 144.46 toks/s]
Processed prompts:  56%|█████▋    | 2306/4096 [00:16<00:18, 98.03it/s, est. speed input: 146917.27 toks/s, output: 143.47 toks/s]
Processed prompts:  57%|█████▋    | 2338/4096 [00:16<00:17, 98.47it/s, est. speed input: 146034.31 toks/s, output: 142.61 toks/s]
Processed prompts:  58%|█████▊    | 2370/4096 [00:16<00:17, 99.98it/s, est. speed input: 145298.41 toks/s, output: 141.89 toks/s]
Processed prompts:  59%|█████▊    | 2402/4096 [00:17<00:17, 98.87it/s, est. speed input: 144389.42 toks/s, output: 141.01 toks/s]
Processed prompts:  59%|█████▉    | 2434/4096 [00:17<00:16, 98.18it/s, est. speed input: 143522.13 toks/s, output: 140.16 toks/s]
Processed prompts:  60%|██████    | 2466/4096 [00:17<00:16, 97.82it/s, est. speed input: 142698.14 toks/s, output: 139.35 toks/s]
Processed prompts:  61%|██████    | 2498/4096 [00:18<00:16, 98.34it/s, est. speed input: 141971.69 toks/s, output: 138.64 toks/s]
Processed prompts:  62%|██████▏   | 2530/4096 [00:18<00:16, 97.84it/s, est. speed input: 141196.81 toks/s, output: 137.89 toks/s]
Processed prompts:  63%|██████▎   | 2562/4096 [00:18<00:15, 98.22it/s, est. speed input: 140509.95 toks/s, output: 137.22 toks/s]
Processed prompts:  63%|██████▎   | 2594/4096 [00:19<00:15, 97.76it/s, est. speed input: 139787.68 toks/s, output: 136.51 toks/s]
Processed prompts:  64%|██████▍   | 2626/4096 [00:19<00:15, 97.46it/s, est. speed input: 139091.71 toks/s, output: 135.83 toks/s]
Processed prompts:  65%|██████▍   | 2658/4096 [00:19<00:14, 97.12it/s, est. speed input: 138408.47 toks/s, output: 135.16 toks/s]
Processed prompts:  66%|██████▌   | 2690/4096 [00:19<00:14, 97.04it/s, est. speed input: 137759.96 toks/s, output: 134.53 toks/s]
Processed prompts:  66%|██████▋   | 2722/4096 [00:20<00:14, 97.02it/s, est. speed input: 137136.36 toks/s, output: 133.92 toks/s]
Processed prompts:  67%|██████▋   | 2754/4096 [00:20<00:13, 96.92it/s, est. speed input: 136525.16 toks/s, output: 133.32 toks/s]
Processed prompts:  68%|██████▊   | 2786/4096 [00:20<00:13, 96.82it/s, est. speed input: 135930.83 toks/s, output: 132.74 toks/s]
Processed prompts:  69%|██████▉   | 2818/4096 [00:21<00:13, 96.71it/s, est. speed input: 135352.27 toks/s, output: 132.18 toks/s]
Processed prompts:  70%|██████▉   | 2850/4096 [00:21<00:12, 96.66it/s, est. speed input: 134793.72 toks/s, output: 131.63 toks/s]
Processed prompts:  70%|███████   | 2882/4096 [00:21<00:12, 96.60it/s, est. speed input: 134250.15 toks/s, output: 131.10 toks/s]
Processed prompts:  71%|███████   | 2914/4096 [00:22<00:12, 96.59it/s, est. speed input: 133724.83 toks/s, output: 130.59 toks/s]
Processed prompts:  72%|███████▏  | 2946/4096 [00:22<00:11, 96.57it/s, est. speed input: 133214.59 toks/s, output: 130.09 toks/s]
Processed prompts:  73%|███████▎  | 2978/4096 [00:22<00:11, 96.55it/s, est. speed input: 132717.96 toks/s, output: 129.61 toks/s]
Processed prompts:  73%|███████▎  | 3010/4096 [00:23<00:11, 96.65it/s, est. speed input: 132242.85 toks/s, output: 129.14 toks/s]
Processed prompts:  74%|███████▍  | 3042/4096 [00:23<00:10, 96.53it/s, est. speed input: 131769.41 toks/s, output: 128.68 toks/s]
Processed prompts:  75%|███████▌  | 3074/4096 [00:23<00:10, 96.44it/s, est. speed input: 131308.34 toks/s, output: 128.23 toks/s]
Processed prompts:  76%|███████▌  | 3106/4096 [00:24<00:10, 96.49it/s, est. speed input: 130866.87 toks/s, output: 127.80 toks/s]
Processed prompts:  77%|███████▋  | 3138/4096 [00:24<00:09, 97.29it/s, est. speed input: 130483.71 toks/s, output: 127.43 toks/s]
Processed prompts:  77%|███████▋  | 3170/4096 [00:24<00:09, 96.93it/s, est. speed input: 130055.76 toks/s, output: 127.01 toks/s]
Processed prompts:  78%|███████▊  | 3202/4096 [00:25<00:09, 96.84it/s, est. speed input: 129648.44 toks/s, output: 126.61 toks/s]
Processed prompts:  79%|███████▉  | 3234/4096 [00:25<00:08, 96.58it/s, est. speed input: 129240.19 toks/s, output: 126.21 toks/s]
Processed prompts:  80%|███████▉  | 3266/4096 [00:25<00:08, 96.38it/s, est. speed input: 128841.45 toks/s, output: 125.82 toks/s]
Processed prompts:  81%|████████  | 3298/4096 [00:26<00:08, 96.51it/s, est. speed input: 128468.27 toks/s, output: 125.46 toks/s]
Processed prompts:  81%|████████▏ | 3330/4096 [00:26<00:07, 96.45it/s, est. speed input: 128095.74 toks/s, output: 125.09 toks/s]
Processed prompts:  82%|████████▏ | 3362/4096 [00:26<00:07, 96.50it/s, est. speed input: 127737.57 toks/s, output: 124.74 toks/s]
Processed prompts:  83%|████████▎ | 3394/4096 [00:27<00:07, 96.57it/s, est. speed input: 127389.26 toks/s, output: 124.40 toks/s]
Processed prompts:  84%|████████▎ | 3426/4096 [00:27<00:06, 96.54it/s, est. speed input: 127045.97 toks/s, output: 124.07 toks/s]
Processed prompts:  84%|████████▍ | 3458/4096 [00:27<00:06, 96.63it/s, est. speed input: 126715.95 toks/s, output: 123.75 toks/s]
Processed prompts:  85%|████████▌ | 3490/4096 [00:28<00:06, 98.23it/s, est. speed input: 126471.26 toks/s, output: 123.51 toks/s]
Processed prompts:  86%|████████▌ | 3522/4096 [00:28<00:05, 97.76it/s, est. speed input: 126153.49 toks/s, output: 123.20 toks/s]
Processed prompts:  87%|████████▋ | 3554/4096 [00:28<00:05, 97.22it/s, est. speed input: 125832.00 toks/s, output: 122.88 toks/s]
Processed prompts:  88%|████████▊ | 3586/4096 [00:29<00:05, 97.01it/s, est. speed input: 125526.18 toks/s, output: 122.58 toks/s]
Processed prompts:  88%|████████▊ | 3618/4096 [00:29<00:04, 96.83it/s, est. speed input: 125225.66 toks/s, output: 122.29 toks/s]
Processed prompts:  89%|████████▉ | 3650/4096 [00:29<00:04, 96.28it/s, est. speed input: 124911.15 toks/s, output: 121.98 toks/s]
Processed prompts:  90%|████████▉ | 3682/4096 [00:30<00:04, 96.37it/s, est. speed input: 124626.32 toks/s, output: 121.71 toks/s]
Processed prompts:  91%|█████████ | 3714/4096 [00:30<00:03, 97.39it/s, est. speed input: 124391.55 toks/s, output: 121.48 toks/s]
Processed prompts:  91%|█████████▏| 3746/4096 [00:30<00:03, 97.00it/s, est. speed input: 124111.77 toks/s, output: 121.20 toks/s]
Processed prompts:  92%|█████████▏| 3778/4096 [00:31<00:03, 96.73it/s, est. speed input: 123837.70 toks/s, output: 120.94 toks/s]
Processed prompts:  93%|█████████▎| 3810/4096 [00:31<00:02, 96.70it/s, est. speed input: 123576.54 toks/s, output: 120.68 toks/s]
Processed prompts:  94%|█████████▍| 3842/4096 [00:31<00:02, 97.32it/s, est. speed input: 123349.23 toks/s, output: 120.46 toks/s]
Processed prompts:  95%|█████████▍| 3874/4096 [00:32<00:02, 97.13it/s, est. speed input: 123099.06 toks/s, output: 120.21 toks/s]
Processed prompts:  95%|█████████▌| 3906/4096 [00:32<00:01, 96.98it/s, est. speed input: 122853.54 toks/s, output: 119.97 toks/s]
Processed prompts:  96%|█████████▌| 3938/4096 [00:32<00:01, 96.59it/s, est. speed input: 122600.91 toks/s, output: 119.73 toks/s]
Processed prompts:  97%|█████████▋| 3970/4096 [00:33<00:01, 96.50it/s, est. speed input: 122360.81 toks/s, output: 119.49 toks/s]
Processed prompts:  98%|█████████▊| 4002/4096 [00:33<00:00, 96.44it/s, est. speed input: 122125.40 toks/s, output: 119.26 toks/s]
Processed prompts:  98%|█████████▊| 4034/4096 [00:33<00:00, 97.08it/s, est. speed input: 121922.94 toks/s, output: 119.07 toks/s]
Processed prompts:  99%|█████████▉| 4066/4096 [00:34<00:00, 98.05it/s, est. speed input: 121744.57 toks/s, output: 118.89 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:34<00:00, 98.05it/s, est. speed input: 122639.58 toks/s, output: 119.77 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:34<00:00, 119.76it/s, est. speed input: 122639.58 toks/s, output: 119.77 toks/s]
[rank0]:[W128 08:50:34.713495388 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 98.5s

测试结果:
  Requests/s:   96.82
  Tokens/s:     99237.19
  Total Reqs:   4096
  Elapsed:      42.31s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     99140.37


------------------------------------------------------------
  生成 CSV: BitNet-2B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_INT8_py312_cu129_x86_64/cusparselt/2_8/BitNet-2B-INT8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,33.6246,17249.4005,3.8067
1024,1024,1,128,128,32.2504,33056.6619,3.9689
2048,1024,2,256,128,63.0502,64626.4968,4.0603
4096,1024,4,512,128,84.5614,86675.4389,6.0548
8192,1024,8,1024,128,91.0501,93326.3930,11.2466
16384,1024,16,2048,128,94.7744,97143.7220,21.6092
32768,1024,32,4096,128,96.8168,99237.1900,42.3067

------------------------------------------------------------

[INFO] 完成: 7 成功, 0 失败

============================================================
  BitNet-2B-INT8 | cuSPARSELt (2_10) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-INT8-SlideSparse-2_10
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_INT8_py312_cu129_x86_64/cusparselt/2_10

============================================================
[1/7] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 08:50:43 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3334698) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3334698) WARNING 01-28 08:50:59 [backends.py:609] Failed to read file <frozen os>
Throughput: 31.85 requests/s, 16337.85 total tokens/s, 31.85 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-28 08:50:43] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:50:43] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:50:43] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:50:43] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:50:43] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:50:43] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:50:43] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:50:43] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:50:43] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:50:43] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:50:43] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:50:43] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:50:43] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:50:43] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 08:50:50] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:50:51] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:50:51] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:50:51] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:50:51] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:50:51] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:50:51] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:50:51] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:50:51] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:50:51] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:50:51] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:50:51] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:50:51] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:50:51] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3334698) [2026-01-28 08:50:52] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3334698) [2026-01-28 08:50:52] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3334698) [2026-01-28 08:50:52] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3334698) [2026-01-28 08:50:52] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3334698) [2026-01-28 08:50:52] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3334698) [2026-01-28 08:50:52] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3334698) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3334698) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.02it/s]
(EngineCore_DP0 pid=3334698) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.02it/s]
(EngineCore_DP0 pid=3334698) 
(EngineCore_DP0 pid=3334698) [2026-01-28 08:50:53] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=3334698) [2026-01-28 08:50:53] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9830400 bytes
(EngineCore_DP0 pid=3334698) [2026-01-28 08:50:53] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=3334698) [2026-01-28 08:50:53] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6553600 bytes
(EngineCore_DP0 pid=3334698) [2026-01-28 08:50:53] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=3334698) [2026-01-28 08:50:53] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 35389440 bytes
(EngineCore_DP0 pid=3334698) [2026-01-28 08:50:53] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=3334698) [2026-01-28 08:50:53] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 17715200 bytes
(EngineCore_DP0 pid=3334698) 2026-01-28 08:51:10,633 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3334698) 2026-01-28 08:51:10,660 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3334698) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  2.87it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  2.82it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  2.82it/s]
(EngineCore_DP0 pid=3334698) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 16.07it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  42%|████▏     | 54/128 [00:00<00:00, 536.70it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 709.73it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:33,  3.81it/s, est. speed input: 1949.44 toks/s, output: 3.81 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:07, 16.03it/s, est. speed input: 6882.06 toks/s, output: 13.44 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:05, 23.22it/s, est. speed input: 9587.40 toks/s, output: 18.72 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:04, 27.62it/s, est. speed input: 11271.82 toks/s, output: 22.01 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:03, 30.56it/s, est. speed input: 12447.98 toks/s, output: 24.31 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:03, 32.51it/s, est. speed input: 13307.40 toks/s, output: 25.99 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:00<00:03, 33.90it/s, est. speed input: 13972.54 toks/s, output: 27.29 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:01<00:02, 34.74it/s, est. speed input: 14482.07 toks/s, output: 28.28 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:01<00:02, 35.31it/s, est. speed input: 14892.18 toks/s, output: 29.09 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:01<00:02, 35.78it/s, est. speed input: 15240.18 toks/s, output: 29.77 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:01<00:02, 36.15it/s, est. speed input: 15536.04 toks/s, output: 30.34 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:01<00:02, 36.23it/s, est. speed input: 15769.71 toks/s, output: 30.80 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:01<00:02, 36.00it/s, est. speed input: 15941.37 toks/s, output: 31.14 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:01<00:02, 34.56it/s, est. speed input: 15961.49 toks/s, output: 31.17 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:01<00:02, 33.67it/s, est. speed input: 15982.12 toks/s, output: 31.21 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:01<00:02, 33.06it/s, est. speed input: 15999.22 toks/s, output: 31.25 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:02<00:01, 32.73it/s, est. speed input: 16022.47 toks/s, output: 31.29 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:02<00:01, 33.47it/s, est. speed input: 16128.92 toks/s, output: 31.50 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:02<00:01, 34.14it/s, est. speed input: 16235.93 toks/s, output: 31.71 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:02<00:01, 34.64it/s, est. speed input: 16333.88 toks/s, output: 31.90 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:02<00:01, 34.99it/s, est. speed input: 16423.01 toks/s, output: 32.08 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:02<00:01, 35.32it/s, est. speed input: 16509.77 toks/s, output: 32.25 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:02<00:01, 35.54it/s, est. speed input: 16588.88 toks/s, output: 32.40 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:02<00:00, 35.77it/s, est. speed input: 16666.34 toks/s, output: 32.55 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:02<00:00, 35.90it/s, est. speed input: 16735.94 toks/s, output: 32.69 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:03<00:00, 35.56it/s, est. speed input: 16776.21 toks/s, output: 32.77 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:03<00:00, 35.72it/s, est. speed input: 16835.07 toks/s, output: 32.88 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:03<00:00, 35.83it/s, est. speed input: 16890.31 toks/s, output: 32.99 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:03<00:00, 35.81it/s, est. speed input: 16936.81 toks/s, output: 33.08 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:03<00:00, 35.86it/s, est. speed input: 16983.83 toks/s, output: 33.17 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:03<00:00, 35.76it/s, est. speed input: 17021.19 toks/s, output: 33.24 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:03<00:00, 35.65it/s, est. speed input: 17054.38 toks/s, output: 33.31 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 35.65it/s, est. speed input: 17080.42 toks/s, output: 33.36 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 33.36it/s, est. speed input: 17080.42 toks/s, output: 33.36 toks/s]
[rank0]:[W128 08:51:17.251531929 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 42.2s

测试结果:
  Requests/s:   31.85
  Tokens/s:     16337.85
  Total Reqs:   128
  Elapsed:      4.02s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     16306.00

============================================================
[2/7] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 08:51:26 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3335831) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3335831) WARNING 01-28 08:51:41 [backends.py:609] Failed to read file <frozen os>
Throughput: 32.76 requests/s, 33578.68 total tokens/s, 32.76 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-28 08:51:25] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:51:26] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:51:26] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:51:26] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:51:26] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:51:26] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:51:26] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:51:26] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:51:26] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:51:26] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:51:26] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:51:26] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:51:26] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:51:26] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 08:51:32] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:51:33] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:51:33] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:51:33] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:51:33] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:51:33] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:51:33] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:51:33] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:51:33] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:51:33] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:51:33] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:51:33] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:51:33] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:51:33] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3335831) [2026-01-28 08:51:33] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3335831) [2026-01-28 08:51:33] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3335831) [2026-01-28 08:51:33] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3335831) [2026-01-28 08:51:33] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3335831) [2026-01-28 08:51:33] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3335831) [2026-01-28 08:51:33] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3335831) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3335831) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.06it/s]
(EngineCore_DP0 pid=3335831) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.06it/s]
(EngineCore_DP0 pid=3335831) 
(EngineCore_DP0 pid=3335831) [2026-01-28 08:51:34] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=3335831) [2026-01-28 08:51:34] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9830400 bytes
(EngineCore_DP0 pid=3335831) [2026-01-28 08:51:34] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=3335831) [2026-01-28 08:51:34] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6553600 bytes
(EngineCore_DP0 pid=3335831) [2026-01-28 08:51:34] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=3335831) [2026-01-28 08:51:34] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 35389440 bytes
(EngineCore_DP0 pid=3335831) [2026-01-28 08:51:34] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=3335831) [2026-01-28 08:51:34] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 17715200 bytes
(EngineCore_DP0 pid=3335831) 2026-01-28 08:51:52,344 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3335831) 2026-01-28 08:51:52,383 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3335831) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  9.97it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 12.15it/s]
(EngineCore_DP0 pid=3335831) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 15.81it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  20%|██        | 26/128 [00:00<00:00, 258.35it/s]
Adding requests:  60%|██████    | 77/128 [00:00<00:00, 402.20it/s]
Adding requests:  99%|█████████▉| 127/128 [00:00<00:00, 445.92it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 419.87it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:02, 45.10it/s, est. speed input: 46198.94 toks/s, output: 45.11 toks/s]
Processed prompts:   8%|▊         | 10/128 [00:00<00:03, 38.63it/s, est. speed input: 40434.56 toks/s, output: 39.48 toks/s]
Processed prompts:  11%|█         | 14/128 [00:00<00:03, 37.19it/s, est. speed input: 39090.95 toks/s, output: 38.17 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:00<00:03, 36.14it/s, est. speed input: 38171.00 toks/s, output: 37.27 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:00<00:02, 35.80it/s, est. speed input: 37763.19 toks/s, output: 36.88 toks/s]
Processed prompts:  20%|██        | 26/128 [00:00<00:02, 35.61it/s, est. speed input: 37488.24 toks/s, output: 36.61 toks/s]
Processed prompts:  23%|██▎       | 30/128 [00:00<00:02, 35.56it/s, est. speed input: 37325.58 toks/s, output: 36.45 toks/s]
Processed prompts:  27%|██▋       | 34/128 [00:00<00:02, 35.43it/s, est. speed input: 37165.19 toks/s, output: 36.29 toks/s]
Processed prompts:  30%|██▉       | 38/128 [00:01<00:02, 35.33it/s, est. speed input: 37034.25 toks/s, output: 36.17 toks/s]
Processed prompts:  33%|███▎      | 42/128 [00:01<00:02, 35.29it/s, est. speed input: 36935.20 toks/s, output: 36.07 toks/s]
Processed prompts:  36%|███▌      | 46/128 [00:01<00:02, 35.27it/s, est. speed input: 36860.08 toks/s, output: 36.00 toks/s]
Processed prompts:  39%|███▉      | 50/128 [00:01<00:02, 35.23it/s, est. speed input: 36788.38 toks/s, output: 35.93 toks/s]
Processed prompts:  42%|████▏     | 54/128 [00:01<00:02, 35.27it/s, est. speed input: 36745.19 toks/s, output: 35.88 toks/s]
Processed prompts:  45%|████▌     | 58/128 [00:01<00:01, 35.23it/s, est. speed input: 36690.21 toks/s, output: 35.83 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:01<00:01, 35.25it/s, est. speed input: 36654.56 toks/s, output: 35.79 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:01<00:01, 35.23it/s, est. speed input: 36617.06 toks/s, output: 35.76 toks/s]
Processed prompts:  55%|█████▍    | 70/128 [00:01<00:01, 35.27it/s, est. speed input: 36592.38 toks/s, output: 35.73 toks/s]
Processed prompts:  58%|█████▊    | 74/128 [00:02<00:01, 35.29it/s, est. speed input: 36571.17 toks/s, output: 35.71 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:02<00:01, 35.30it/s, est. speed input: 36549.32 toks/s, output: 35.69 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:02<00:01, 35.37it/s, est. speed input: 36542.12 toks/s, output: 35.69 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:02<00:01, 35.36it/s, est. speed input: 36525.43 toks/s, output: 35.67 toks/s]
Processed prompts:  70%|███████   | 90/128 [00:02<00:01, 35.34it/s, est. speed input: 36508.33 toks/s, output: 35.65 toks/s]
Processed prompts:  73%|███████▎  | 94/128 [00:02<00:00, 35.39it/s, est. speed input: 36501.51 toks/s, output: 35.65 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:02<00:00, 35.41it/s, est. speed input: 36492.88 toks/s, output: 35.64 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:02<00:00, 35.35it/s, est. speed input: 36475.67 toks/s, output: 35.62 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:02<00:00, 35.36it/s, est. speed input: 36466.42 toks/s, output: 35.61 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:03<00:00, 35.43it/s, est. speed input: 36465.28 toks/s, output: 35.61 toks/s]
Processed prompts:  89%|████████▉ | 114/128 [00:03<00:00, 35.39it/s, est. speed input: 36454.18 toks/s, output: 35.60 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:03<00:00, 35.30it/s, est. speed input: 36436.61 toks/s, output: 35.58 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:03<00:00, 35.30it/s, est. speed input: 36426.75 toks/s, output: 35.57 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:03<00:00, 35.28it/s, est. speed input: 36415.14 toks/s, output: 35.56 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 35.28it/s, est. speed input: 36403.56 toks/s, output: 35.55 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 35.55it/s, est. speed input: 36403.56 toks/s, output: 35.55 toks/s]
[rank0]:[W128 08:51:58.319142240 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 41.1s

测试结果:
  Requests/s:   32.76
  Tokens/s:     33578.68
  Total Reqs:   128
  Elapsed:      3.91s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     33545.92

============================================================
[3/7] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 08:52:08 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3336925) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3336925) WARNING 01-28 08:52:22 [backends.py:609] Failed to read file <frozen os>
Throughput: 64.14 requests/s, 65745.67 total tokens/s, 64.14 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-28 08:52:07] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:52:08] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:52:08] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:52:08] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:52:08] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:52:08] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:52:08] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:52:08] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:52:08] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:52:08] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:52:08] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:52:08] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:52:08] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:52:08] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 08:52:14] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:52:14] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:52:14] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:52:14] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:52:14] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:52:14] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:52:14] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:52:14] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:52:14] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:52:14] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:52:14] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:52:14] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:52:14] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:52:14] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3336925) [2026-01-28 08:52:15] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3336925) [2026-01-28 08:52:15] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3336925) [2026-01-28 08:52:15] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3336925) [2026-01-28 08:52:15] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3336925) [2026-01-28 08:52:15] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3336925) [2026-01-28 08:52:15] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3336925) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3336925) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.04it/s]
(EngineCore_DP0 pid=3336925) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.03it/s]
(EngineCore_DP0 pid=3336925) 
(EngineCore_DP0 pid=3336925) [2026-01-28 08:52:16] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=3336925) [2026-01-28 08:52:16] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9830400 bytes
(EngineCore_DP0 pid=3336925) [2026-01-28 08:52:16] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=3336925) [2026-01-28 08:52:16] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6553600 bytes
(EngineCore_DP0 pid=3336925) [2026-01-28 08:52:16] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=3336925) [2026-01-28 08:52:16] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 35389440 bytes
(EngineCore_DP0 pid=3336925) [2026-01-28 08:52:16] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=3336925) [2026-01-28 08:52:16] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 17715200 bytes
(EngineCore_DP0 pid=3336925) 2026-01-28 08:52:33,883 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3336925) 2026-01-28 08:52:33,916 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3336925) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 1/3 [00:00<00:00,  6.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00,  9.82it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00,  9.31it/s]
(EngineCore_DP0 pid=3336925) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 1/2 [00:00<00:00,  7.81it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00,  9.51it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  10%|▉         | 25/256 [00:00<00:00, 243.91it/s]
Adding requests:  28%|██▊       | 72/256 [00:00<00:00, 374.33it/s]
Adding requests:  48%|████▊     | 122/256 [00:00<00:00, 429.43it/s]
Adding requests:  67%|██████▋   | 171/256 [00:00<00:00, 452.74it/s]
Adding requests:  86%|████████▋ | 221/256 [00:00<00:00, 468.84it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 446.77it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   9%|▊         | 22/256 [00:00<00:01, 213.62it/s, est. speed input: 218787.99 toks/s, output: 213.63 toks/s]
Processed prompts:  17%|█▋        | 44/256 [00:00<00:02, 97.07it/s, est. speed input: 108268.69 toks/s, output: 105.73 toks/s] 
Processed prompts:  22%|██▏       | 57/256 [00:00<00:02, 89.03it/s, est. speed input: 99516.00 toks/s, output: 97.18 toks/s]  
Processed prompts:  27%|██▋       | 68/256 [00:00<00:02, 80.37it/s, est. speed input: 92039.92 toks/s, output: 89.88 toks/s]
Processed prompts:  30%|███       | 77/256 [00:00<00:02, 80.32it/s, est. speed input: 90749.32 toks/s, output: 88.62 toks/s]
Processed prompts:  34%|███▎      | 86/256 [00:01<00:02, 75.12it/s, est. speed input: 87167.74 toks/s, output: 85.12 toks/s]
Processed prompts:  37%|███▋      | 94/256 [00:01<00:02, 73.92it/s, est. speed input: 85660.66 toks/s, output: 83.65 toks/s]
Processed prompts:  40%|███▉      | 102/256 [00:01<00:02, 73.05it/s, est. speed input: 84451.03 toks/s, output: 82.47 toks/s]
Processed prompts:  43%|████▎     | 110/256 [00:01<00:02, 72.44it/s, est. speed input: 83457.06 toks/s, output: 81.50 toks/s]
Processed prompts:  46%|████▌     | 118/256 [00:01<00:01, 72.01it/s, est. speed input: 82623.29 toks/s, output: 80.69 toks/s]
Processed prompts:  49%|████▉     | 126/256 [00:01<00:01, 71.55it/s, est. speed input: 81866.26 toks/s, output: 79.94 toks/s]
Processed prompts:  52%|█████▏    | 134/256 [00:01<00:01, 71.45it/s, est. speed input: 81270.88 toks/s, output: 79.36 toks/s]
Processed prompts:  55%|█████▌    | 142/256 [00:01<00:01, 71.32it/s, est. speed input: 80736.42 toks/s, output: 78.84 toks/s]
Processed prompts:  59%|█████▊    | 150/256 [00:01<00:01, 71.07it/s, est. speed input: 80228.19 toks/s, output: 78.35 toks/s]
Processed prompts:  62%|██████▏   | 158/256 [00:02<00:01, 70.99it/s, est. speed input: 79796.12 toks/s, output: 77.92 toks/s]
Processed prompts:  65%|██████▍   | 166/256 [00:02<00:01, 70.92it/s, est. speed input: 79409.09 toks/s, output: 77.55 toks/s]
Processed prompts:  68%|██████▊   | 174/256 [00:02<00:01, 70.67it/s, est. speed input: 79022.18 toks/s, output: 77.17 toks/s]
Processed prompts:  71%|███████   | 182/256 [00:02<00:01, 70.70it/s, est. speed input: 78710.05 toks/s, output: 76.86 toks/s]
Processed prompts:  74%|███████▍  | 190/256 [00:02<00:00, 70.79it/s, est. speed input: 78436.57 toks/s, output: 76.60 toks/s]
Processed prompts:  77%|███████▋  | 198/256 [00:02<00:00, 70.66it/s, est. speed input: 78157.01 toks/s, output: 76.32 toks/s]
Processed prompts:  80%|████████  | 206/256 [00:02<00:00, 70.50it/s, est. speed input: 77889.45 toks/s, output: 76.06 toks/s]
Processed prompts:  84%|████████▎ | 214/256 [00:02<00:00, 70.37it/s, est. speed input: 77641.62 toks/s, output: 75.82 toks/s]
Processed prompts:  87%|████████▋ | 222/256 [00:02<00:00, 70.50it/s, est. speed input: 77443.02 toks/s, output: 75.63 toks/s]
Processed prompts:  90%|████████▉ | 230/256 [00:03<00:00, 70.51it/s, est. speed input: 77249.78 toks/s, output: 75.44 toks/s]
Processed prompts:  93%|█████████▎| 238/256 [00:03<00:00, 70.69it/s, est. speed input: 77091.18 toks/s, output: 75.28 toks/s]
Processed prompts:  96%|█████████▌| 246/256 [00:03<00:00, 70.46it/s, est. speed input: 76900.65 toks/s, output: 75.10 toks/s]
Processed prompts:  99%|█████████▉| 254/256 [00:03<00:00, 70.60it/s, est. speed input: 76758.50 toks/s, output: 74.96 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 70.60it/s, est. speed input: 76732.84 toks/s, output: 74.93 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 74.93it/s, est. speed input: 76732.84 toks/s, output: 74.93 toks/s]
[rank0]:[W128 08:52:40.252019400 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 42.0s

测试结果:
  Requests/s:   64.14
  Tokens/s:     65745.67
  Total Reqs:   256
  Elapsed:      3.99s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     65681.53

============================================================
[4/7] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 08:52:51 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3338043) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3338043) WARNING 01-28 08:53:06 [backends.py:609] Failed to read file <frozen os>
Throughput: 81.14 requests/s, 83164.06 total tokens/s, 81.14 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-28 08:52:50] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:52:51] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:52:51] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:52:51] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:52:51] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:52:51] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:52:51] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:52:51] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:52:51] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:52:51] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:52:51] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:52:51] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:52:51] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:52:51] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 08:52:57] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:52:57] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:52:57] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:52:57] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:52:57] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:52:57] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:52:57] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:52:57] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:52:57] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:52:57] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:52:57] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:52:57] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:52:57] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:52:57] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3338043) [2026-01-28 08:52:59] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3338043) [2026-01-28 08:52:59] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3338043) [2026-01-28 08:52:59] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3338043) [2026-01-28 08:52:59] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3338043) [2026-01-28 08:52:59] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3338043) [2026-01-28 08:52:59] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3338043) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3338043) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.04it/s]
(EngineCore_DP0 pid=3338043) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.04it/s]
(EngineCore_DP0 pid=3338043) 
(EngineCore_DP0 pid=3338043) [2026-01-28 08:53:00] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=3338043) [2026-01-28 08:53:00] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9830400 bytes
(EngineCore_DP0 pid=3338043) [2026-01-28 08:53:00] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=3338043) [2026-01-28 08:53:00] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6553600 bytes
(EngineCore_DP0 pid=3338043) [2026-01-28 08:53:00] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=3338043) [2026-01-28 08:53:00] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 35389440 bytes
(EngineCore_DP0 pid=3338043) [2026-01-28 08:53:00] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=3338043) [2026-01-28 08:53:00] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 17715200 bytes
(EngineCore_DP0 pid=3338043) 2026-01-28 08:53:17,764 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3338043) 2026-01-28 08:53:17,795 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3338043) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 1/4 [00:00<00:00,  9.97it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 3/4 [00:00<00:00, 14.68it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 13.72it/s]
(EngineCore_DP0 pid=3338043) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  33%|███▎      | 1/3 [00:00<00:00,  8.08it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 12.25it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 11.64it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   6%|▌         | 30/512 [00:00<00:01, 296.52it/s]
Adding requests:  16%|█▌        | 81/512 [00:00<00:01, 420.41it/s]
Adding requests:  26%|██▌       | 131/512 [00:00<00:00, 452.23it/s]
Adding requests:  35%|███▍      | 179/512 [00:00<00:00, 461.36it/s]
Adding requests:  45%|████▍     | 230/512 [00:00<00:00, 476.53it/s]
Adding requests:  55%|█████▍    | 280/512 [00:00<00:00, 483.49it/s]
Adding requests:  64%|██████▍   | 330/512 [00:00<00:00, 487.41it/s]
Adding requests:  74%|███████▍  | 381/512 [00:00<00:00, 494.33it/s]
Adding requests:  85%|████████▍ | 433/512 [00:00<00:00, 499.69it/s]
Adding requests:  94%|█████████▍| 483/512 [00:01<00:00, 499.37it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 478.44it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  14%|█▎        | 70/512 [00:00<00:00, 543.32it/s, est. speed input: 556441.76 toks/s, output: 543.34 toks/s]
Processed prompts:  24%|██▍       | 125/512 [00:00<00:02, 151.35it/s, est. speed input: 176365.75 toks/s, output: 172.23 toks/s]
Processed prompts:  30%|██▉       | 153/512 [00:01<00:02, 124.14it/s, est. speed input: 148258.90 toks/s, output: 144.78 toks/s]
Processed prompts:  34%|███▎      | 172/512 [00:01<00:03, 112.02it/s, est. speed input: 136760.17 toks/s, output: 133.55 toks/s]
Processed prompts:  37%|███▋      | 187/512 [00:01<00:03, 104.40it/s, est. speed input: 130055.71 toks/s, output: 127.01 toks/s]
Processed prompts:  39%|███▉      | 200/512 [00:01<00:03, 102.27it/s, est. speed input: 127176.50 toks/s, output: 124.19 toks/s]
Processed prompts:  41%|████▏     | 212/512 [00:01<00:03, 98.64it/s, est. speed input: 124121.85 toks/s, output: 121.21 toks/s] 
Processed prompts:  44%|████▎     | 223/512 [00:01<00:03, 93.34it/s, est. speed input: 120776.81 toks/s, output: 117.95 toks/s]
Processed prompts:  46%|████▌     | 234/512 [00:02<00:03, 89.24it/s, est. speed input: 117922.63 toks/s, output: 115.16 toks/s]
Processed prompts:  48%|████▊     | 246/512 [00:02<00:03, 88.28it/s, est. speed input: 115980.48 toks/s, output: 113.26 toks/s]
Processed prompts:  50%|█████     | 258/512 [00:02<00:02, 87.54it/s, est. speed input: 114268.22 toks/s, output: 111.59 toks/s]
Processed prompts:  53%|█████▎    | 270/512 [00:02<00:02, 87.33it/s, est. speed input: 112834.15 toks/s, output: 110.19 toks/s]
Processed prompts:  55%|█████▌    | 282/512 [00:02<00:02, 87.34it/s, est. speed input: 111592.88 toks/s, output: 108.98 toks/s]
Processed prompts:  57%|█████▋    | 294/512 [00:02<00:02, 87.14it/s, est. speed input: 110431.81 toks/s, output: 107.84 toks/s]
Processed prompts:  60%|█████▉    | 306/512 [00:02<00:02, 86.52it/s, est. speed input: 109283.90 toks/s, output: 106.72 toks/s]
Processed prompts:  62%|██████▏   | 318/512 [00:03<00:02, 86.29it/s, est. speed input: 108283.24 toks/s, output: 105.74 toks/s]
Processed prompts:  64%|██████▍   | 330/512 [00:03<00:02, 83.13it/s, est. speed input: 106799.21 toks/s, output: 104.29 toks/s]
Processed prompts:  67%|██████▋   | 342/512 [00:03<00:01, 85.05it/s, est. speed input: 106203.55 toks/s, output: 103.71 toks/s]
Processed prompts:  69%|██████▉   | 354/512 [00:03<00:01, 85.65it/s, est. speed input: 105522.04 toks/s, output: 103.05 toks/s]
Processed prompts:  71%|███████▏  | 366/512 [00:03<00:01, 86.00it/s, est. speed input: 104878.99 toks/s, output: 102.42 toks/s]
Processed prompts:  74%|███████▍  | 378/512 [00:03<00:01, 86.20it/s, est. speed input: 104277.70 toks/s, output: 101.83 toks/s]
Processed prompts:  76%|███████▌  | 390/512 [00:03<00:01, 86.07it/s, est. speed input: 103680.49 toks/s, output: 101.25 toks/s]
Processed prompts:  79%|███████▊  | 402/512 [00:03<00:01, 85.87it/s, est. speed input: 103108.49 toks/s, output: 100.69 toks/s]
Processed prompts:  81%|████████  | 414/512 [00:04<00:01, 85.46it/s, est. speed input: 102540.85 toks/s, output: 100.14 toks/s]
Processed prompts:  83%|████████▎ | 426/512 [00:04<00:01, 85.47it/s, est. speed input: 102047.52 toks/s, output: 99.66 toks/s] 
Processed prompts:  86%|████████▌ | 438/512 [00:04<00:00, 85.77it/s, est. speed input: 101623.19 toks/s, output: 99.24 toks/s]
Processed prompts:  88%|████████▊ | 450/512 [00:04<00:00, 87.15it/s, est. speed input: 101364.25 toks/s, output: 98.99 toks/s]
Processed prompts:  90%|█████████ | 462/512 [00:04<00:00, 87.12it/s, est. speed input: 101003.98 toks/s, output: 98.64 toks/s]
Processed prompts:  93%|█████████▎| 474/512 [00:04<00:00, 86.84it/s, est. speed input: 100636.63 toks/s, output: 98.28 toks/s]
Processed prompts:  95%|█████████▍| 486/512 [00:04<00:00, 86.43it/s, est. speed input: 100265.70 toks/s, output: 97.92 toks/s]
Processed prompts:  97%|█████████▋| 498/512 [00:05<00:00, 85.97it/s, est. speed input: 99897.33 toks/s, output: 97.56 toks/s] 
Processed prompts: 100%|█████████▉| 510/512 [00:05<00:00, 87.13it/s, est. speed input: 99698.76 toks/s, output: 97.36 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:05<00:00, 87.13it/s, est. speed input: 100084.72 toks/s, output: 97.74 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:05<00:00, 97.74it/s, est. speed input: 100084.72 toks/s, output: 97.74 toks/s]
[rank0]:[W128 08:53:26.472315907 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 46.2s

测试结果:
  Requests/s:   81.14
  Tokens/s:     83164.06
  Total Reqs:   512
  Elapsed:      6.31s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     83082.93

============================================================
[5/7] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 08:53:39 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3339208) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3339208) WARNING 01-28 08:53:54 [backends.py:609] Failed to read file <frozen os>
Throughput: 88.40 requests/s, 90608.08 total tokens/s, 88.40 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-28 08:53:38] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:53:39] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:53:39] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:53:39] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:53:39] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:53:39] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:53:39] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:53:39] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:53:39] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:53:39] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:53:39] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:53:39] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:53:39] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:53:39] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 08:53:45] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:53:46] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:53:46] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:53:46] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:53:46] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:53:46] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:53:46] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:53:46] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:53:46] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:53:46] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:53:46] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:53:46] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:53:46] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:53:46] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3339208) [2026-01-28 08:53:47] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3339208) [2026-01-28 08:53:47] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3339208) [2026-01-28 08:53:47] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3339208) [2026-01-28 08:53:47] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3339208) [2026-01-28 08:53:47] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3339208) [2026-01-28 08:53:47] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3339208) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3339208) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.02it/s]
(EngineCore_DP0 pid=3339208) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.02it/s]
(EngineCore_DP0 pid=3339208) 
(EngineCore_DP0 pid=3339208) [2026-01-28 08:53:48] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=3339208) [2026-01-28 08:53:48] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9830400 bytes
(EngineCore_DP0 pid=3339208) [2026-01-28 08:53:48] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=3339208) [2026-01-28 08:53:48] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6553600 bytes
(EngineCore_DP0 pid=3339208) [2026-01-28 08:53:48] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=3339208) [2026-01-28 08:53:48] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 35389440 bytes
(EngineCore_DP0 pid=3339208) [2026-01-28 08:53:48] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=3339208) [2026-01-28 08:53:48] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 17715200 bytes
(EngineCore_DP0 pid=3339208) 2026-01-28 08:54:05,634 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3339208) 2026-01-28 08:54:05,662 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3339208) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 1/5 [00:00<00:01,  3.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 3/5 [00:00<00:00,  8.26it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 10.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00,  9.03it/s]
(EngineCore_DP0 pid=3339208) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00, 16.32it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 17.03it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 16.91it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   3%|▎         | 32/1024 [00:00<00:03, 317.10it/s]
Adding requests:   8%|▊         | 83/1024 [00:00<00:02, 427.07it/s]
Adding requests:  13%|█▎        | 133/1024 [00:00<00:01, 457.28it/s]
Adding requests:  18%|█▊        | 181/1024 [00:00<00:01, 464.76it/s]
Adding requests:  23%|██▎       | 231/1024 [00:00<00:01, 477.21it/s]
Adding requests:  27%|██▋       | 280/1024 [00:00<00:01, 480.93it/s]
Adding requests:  32%|███▏      | 329/1024 [00:00<00:01, 482.21it/s]
Adding requests:  37%|███▋      | 380/1024 [00:00<00:01, 490.50it/s]
Adding requests:  42%|████▏     | 430/1024 [00:00<00:01, 492.79it/s]
Adding requests:  47%|████▋     | 480/1024 [00:01<00:01, 491.24it/s]
Adding requests:  52%|█████▏    | 530/1024 [00:01<00:01, 475.34it/s]
Adding requests:  57%|█████▋    | 580/1024 [00:01<00:00, 481.16it/s]
Adding requests:  62%|██████▏   | 631/1024 [00:01<00:00, 489.34it/s]
Adding requests:  67%|██████▋   | 683/1024 [00:01<00:00, 498.27it/s]
Adding requests:  72%|███████▏  | 735/1024 [00:01<00:00, 503.78it/s]
Adding requests:  77%|███████▋  | 786/1024 [00:01<00:00, 501.44it/s]
Adding requests:  82%|████████▏ | 837/1024 [00:01<00:00, 492.55it/s]
Adding requests:  87%|████████▋ | 890/1024 [00:01<00:00, 502.65it/s]
Adding requests:  92%|█████████▏| 941/1024 [00:01<00:00, 503.82it/s]
Adding requests:  97%|█████████▋| 993/1024 [00:02<00:00, 507.51it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 488.61it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  18%|█▊        | 186/1024 [00:00<00:00, 1232.30it/s, est. speed input: 1262279.56 toks/s, output: 1232.40 toks/s]
Processed prompts:  30%|███       | 310/1024 [00:01<00:04, 174.77it/s, est. speed input: 211660.68 toks/s, output: 206.70 toks/s]   
Processed prompts:  36%|███▌      | 367/1024 [00:02<00:04, 142.96it/s, est. speed input: 176854.26 toks/s, output: 172.71 toks/s]
Processed prompts:  39%|███▉      | 402/1024 [00:02<00:04, 124.83it/s, est. speed input: 160027.93 toks/s, output: 156.27 toks/s]
Processed prompts:  42%|████▏     | 427/1024 [00:02<00:05, 118.60it/s, est. speed input: 153854.32 toks/s, output: 150.25 toks/s]
Processed prompts:  44%|████▎     | 446/1024 [00:03<00:04, 116.44it/s, est. speed input: 151069.93 toks/s, output: 147.53 toks/s]
Processed prompts:  45%|████▌     | 462/1024 [00:03<00:05, 111.93it/s, est. speed input: 147903.01 toks/s, output: 144.44 toks/s]
Processed prompts:  46%|████▋     | 476/1024 [00:03<00:05, 104.86it/s, est. speed input: 144271.47 toks/s, output: 140.89 toks/s]
Processed prompts:  48%|████▊     | 490/1024 [00:03<00:05, 98.85it/s, est. speed input: 141060.37 toks/s, output: 137.75 toks/s] 
Processed prompts:  49%|████▉     | 506/1024 [00:03<00:05, 96.48it/s, est. speed input: 138675.31 toks/s, output: 135.42 toks/s]
Processed prompts:  51%|█████     | 522/1024 [00:03<00:05, 94.46it/s, est. speed input: 136473.36 toks/s, output: 133.27 toks/s]
Processed prompts:  53%|█████▎    | 538/1024 [00:04<00:05, 92.63it/s, est. speed input: 134400.89 toks/s, output: 131.25 toks/s]
Processed prompts:  54%|█████▍    | 554/1024 [00:04<00:05, 91.44it/s, est. speed input: 132543.96 toks/s, output: 129.44 toks/s]
Processed prompts:  56%|█████▌    | 570/1024 [00:04<00:05, 90.76it/s, est. speed input: 130877.22 toks/s, output: 127.81 toks/s]
Processed prompts:  57%|█████▋    | 586/1024 [00:04<00:04, 90.29it/s, est. speed input: 129345.70 toks/s, output: 126.31 toks/s]
Processed prompts:  59%|█████▉    | 602/1024 [00:04<00:04, 89.93it/s, est. speed input: 127922.13 toks/s, output: 124.92 toks/s]
Processed prompts:  60%|██████    | 618/1024 [00:04<00:04, 89.49it/s, est. speed input: 126571.49 toks/s, output: 123.60 toks/s]
Processed prompts:  62%|██████▏   | 634/1024 [00:05<00:04, 89.13it/s, est. speed input: 125305.27 toks/s, output: 122.37 toks/s]
Processed prompts:  63%|██████▎   | 650/1024 [00:05<00:04, 89.02it/s, est. speed input: 124148.54 toks/s, output: 121.24 toks/s]
Processed prompts:  65%|██████▌   | 666/1024 [00:05<00:04, 89.05it/s, est. speed input: 123082.49 toks/s, output: 120.20 toks/s]
Processed prompts:  67%|██████▋   | 682/1024 [00:05<00:03, 88.98it/s, est. speed input: 122071.50 toks/s, output: 119.21 toks/s]
Processed prompts:  68%|██████▊   | 698/1024 [00:05<00:03, 88.92it/s, est. speed input: 121119.25 toks/s, output: 118.28 toks/s]
Processed prompts:  70%|██████▉   | 714/1024 [00:06<00:03, 88.83it/s, est. speed input: 120217.89 toks/s, output: 117.40 toks/s]
Processed prompts:  71%|███████▏  | 730/1024 [00:06<00:03, 88.81it/s, est. speed input: 119373.59 toks/s, output: 116.58 toks/s]
Processed prompts:  73%|███████▎  | 746/1024 [00:06<00:03, 88.85it/s, est. speed input: 118583.95 toks/s, output: 115.80 toks/s]
Processed prompts:  74%|███████▍  | 762/1024 [00:06<00:02, 88.86it/s, est. speed input: 117834.33 toks/s, output: 115.07 toks/s]
Processed prompts:  76%|███████▌  | 778/1024 [00:06<00:02, 88.86it/s, est. speed input: 117123.99 toks/s, output: 114.38 toks/s]
Processed prompts:  78%|███████▊  | 794/1024 [00:06<00:02, 88.85it/s, est. speed input: 116449.13 toks/s, output: 113.72 toks/s]
Processed prompts:  79%|███████▉  | 810/1024 [00:07<00:02, 88.84it/s, est. speed input: 115807.11 toks/s, output: 113.09 toks/s]
Processed prompts:  81%|████████  | 826/1024 [00:07<00:02, 88.81it/s, est. speed input: 115195.55 toks/s, output: 112.50 toks/s]
Processed prompts:  82%|████████▏ | 842/1024 [00:07<00:02, 88.70it/s, est. speed input: 114603.27 toks/s, output: 111.92 toks/s]
Processed prompts:  84%|████████▍ | 858/1024 [00:07<00:01, 88.60it/s, est. speed input: 114036.63 toks/s, output: 111.36 toks/s]
Processed prompts:  85%|████████▌ | 874/1024 [00:07<00:01, 88.66it/s, est. speed input: 113508.25 toks/s, output: 110.85 toks/s]
Processed prompts:  87%|████████▋ | 890/1024 [00:08<00:01, 88.84it/s, est. speed input: 113017.05 toks/s, output: 110.37 toks/s]
Processed prompts:  88%|████████▊ | 906/1024 [00:08<00:01, 88.88it/s, est. speed input: 112539.66 toks/s, output: 109.90 toks/s]
Processed prompts:  90%|█████████ | 922/1024 [00:08<00:01, 88.87it/s, est. speed input: 112077.75 toks/s, output: 109.45 toks/s]
Processed prompts:  92%|█████████▏| 938/1024 [00:08<00:00, 89.56it/s, est. speed input: 111697.22 toks/s, output: 109.08 toks/s]
Processed prompts:  93%|█████████▎| 954/1024 [00:08<00:00, 89.29it/s, est. speed input: 111267.36 toks/s, output: 108.66 toks/s]
Processed prompts:  95%|█████████▍| 970/1024 [00:08<00:00, 89.02it/s, est. speed input: 110848.04 toks/s, output: 108.25 toks/s]
Processed prompts:  96%|█████████▋| 986/1024 [00:09<00:00, 89.80it/s, est. speed input: 110524.34 toks/s, output: 107.93 toks/s]
Processed prompts:  98%|█████████▊| 1002/1024 [00:09<00:00, 89.56it/s, est. speed input: 110149.65 toks/s, output: 107.57 toks/s]
Processed prompts:  99%|█████████▉| 1018/1024 [00:09<00:00, 90.76it/s, est. speed input: 109893.63 toks/s, output: 107.32 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:09<00:00, 90.76it/s, est. speed input: 110537.29 toks/s, output: 107.95 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:09<00:00, 107.94it/s, est. speed input: 110537.29 toks/s, output: 107.95 toks/s]
[rank0]:[W128 08:54:19.891363108 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 53.5s

测试结果:
  Requests/s:   88.40
  Tokens/s:     90608.08
  Total Reqs:   1024
  Elapsed:      11.58s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     90519.68

============================================================
[6/7] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 08:54:37 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3340517) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3340517) WARNING 01-28 08:54:52 [backends.py:609] Failed to read file <frozen os>
Throughput: 92.39 requests/s, 94697.67 total tokens/s, 92.39 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-28 08:54:36] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:54:37] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:54:37] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:54:37] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:54:37] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:54:37] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:54:37] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:54:37] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:54:37] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:54:37] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:54:37] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:54:37] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:54:37] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:54:37] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 08:54:43] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:54:44] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:54:44] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:54:44] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:54:44] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:54:44] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:54:44] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:54:44] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:54:44] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:54:44] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:54:44] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:54:44] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:54:44] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:54:44] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3340517) [2026-01-28 08:54:45] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3340517) [2026-01-28 08:54:45] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3340517) [2026-01-28 08:54:45] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3340517) [2026-01-28 08:54:45] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3340517) [2026-01-28 08:54:45] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3340517) [2026-01-28 08:54:45] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3340517) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3340517) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.07it/s]
(EngineCore_DP0 pid=3340517) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.07it/s]
(EngineCore_DP0 pid=3340517) 
(EngineCore_DP0 pid=3340517) [2026-01-28 08:54:46] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=3340517) [2026-01-28 08:54:46] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9830400 bytes
(EngineCore_DP0 pid=3340517) [2026-01-28 08:54:46] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=3340517) [2026-01-28 08:54:46] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6553600 bytes
(EngineCore_DP0 pid=3340517) [2026-01-28 08:54:46] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=3340517) [2026-01-28 08:54:46] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 35389440 bytes
(EngineCore_DP0 pid=3340517) [2026-01-28 08:54:46] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=3340517) [2026-01-28 08:54:46] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 17715200 bytes
(EngineCore_DP0 pid=3340517) 2026-01-28 08:55:03,450 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3340517) 2026-01-28 08:55:03,478 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3340517) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 1/7 [00:00<00:00,  9.71it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 3/7 [00:00<00:00, 13.90it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 5/7 [00:00<00:00, 15.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 14.94it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 14.56it/s]
(EngineCore_DP0 pid=3340517) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00,  8.30it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00,  9.94it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 10.60it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 34/2048 [00:00<00:06, 334.00it/s]
Adding requests:   4%|▍         | 85/2048 [00:00<00:04, 434.79it/s]
Adding requests:   7%|▋         | 134/2048 [00:00<00:04, 459.57it/s]
Adding requests:   9%|▉         | 182/2048 [00:00<00:04, 465.60it/s]
Adding requests:  11%|█▏        | 233/2048 [00:00<00:03, 478.78it/s]
Adding requests:  14%|█▍        | 283/2048 [00:00<00:03, 482.60it/s]
Adding requests:  16%|█▌        | 332/2048 [00:00<00:03, 483.38it/s]
Adding requests:  19%|█▊        | 382/2048 [00:00<00:03, 488.41it/s]
Adding requests:  21%|██        | 433/2048 [00:00<00:03, 492.94it/s]
Adding requests:  24%|██▎       | 483/2048 [00:01<00:03, 490.84it/s]
Adding requests:  26%|██▌       | 533/2048 [00:01<00:03, 479.59it/s]
Adding requests:  29%|██▊       | 584/2048 [00:01<00:03, 487.65it/s]
Adding requests:  31%|███       | 635/2048 [00:01<00:02, 492.50it/s]
Adding requests:  34%|███▎      | 687/2048 [00:01<00:02, 497.59it/s]
Adding requests:  36%|███▌      | 737/2048 [00:01<00:02, 496.17it/s]
Adding requests:  38%|███▊      | 787/2048 [00:01<00:02, 491.78it/s]
Adding requests:  41%|████      | 837/2048 [00:01<00:02, 477.61it/s]
Adding requests:  43%|████▎     | 889/2048 [00:01<00:02, 487.79it/s]
Adding requests:  46%|████▌     | 939/2048 [00:01<00:02, 490.01it/s]
Adding requests:  48%|████▊     | 990/2048 [00:02<00:02, 493.79it/s]
Adding requests:  51%|█████     | 1041/2048 [00:02<00:02, 495.24it/s]
Adding requests:  53%|█████▎    | 1091/2048 [00:02<00:01, 492.15it/s]
Adding requests:  56%|█████▌    | 1141/2048 [00:02<00:01, 485.07it/s]
Adding requests:  58%|█████▊    | 1196/2048 [00:02<00:01, 503.09it/s]
Adding requests:  61%|██████    | 1248/2048 [00:02<00:01, 507.21it/s]
Adding requests:  63%|██████▎   | 1299/2048 [00:02<00:01, 507.49it/s]
Adding requests:  66%|██████▌   | 1353/2048 [00:02<00:01, 514.21it/s]
Adding requests:  69%|██████▊   | 1407/2048 [00:02<00:01, 519.76it/s]
Adding requests:  71%|███████   | 1459/2048 [00:02<00:01, 516.95it/s]
Adding requests:  74%|███████▍  | 1512/2048 [00:03<00:01, 519.98it/s]
Adding requests:  76%|███████▋  | 1565/2048 [00:03<00:00, 519.90it/s]
Adding requests:  79%|███████▉  | 1618/2048 [00:03<00:00, 521.71it/s]
Adding requests:  82%|████████▏ | 1671/2048 [00:03<00:00, 518.74it/s]
Adding requests:  84%|████████▍ | 1724/2048 [00:03<00:00, 520.95it/s]
Adding requests:  87%|████████▋ | 1777/2048 [00:03<00:00, 515.23it/s]
Adding requests:  89%|████████▉ | 1829/2048 [00:03<00:00, 514.16it/s]
Adding requests:  92%|█████████▏| 1881/2048 [00:03<00:00, 514.22it/s]
Adding requests:  94%|█████████▍| 1933/2048 [00:03<00:00, 449.59it/s]
Adding requests:  97%|█████████▋| 1983/2048 [00:04<00:00, 462.32it/s]
Adding requests:  99%|█████████▉| 2036/2048 [00:04<00:00, 478.48it/s]
Adding requests: 100%|██████████| 2048/2048 [00:04<00:00, 492.54it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  18%|█▊        | 370/2048 [00:00<00:00, 3646.05it/s, est. speed input: 3735684.80 toks/s, output: 3647.12 toks/s]
Processed prompts:  36%|███▌      | 735/2048 [00:03<00:08, 161.85it/s, est. speed input: 193686.77 toks/s, output: 189.15 toks/s]   
Processed prompts:  44%|████▎     | 891/2048 [00:05<00:08, 134.01it/s, est. speed input: 162612.89 toks/s, output: 158.80 toks/s]
Processed prompts:  48%|████▊     | 980/2048 [00:06<00:08, 122.43it/s, est. speed input: 151349.48 toks/s, output: 147.80 toks/s]
Processed prompts:  51%|█████     | 1038/2048 [00:07<00:08, 120.84it/s, est. speed input: 148692.82 toks/s, output: 145.21 toks/s]
Processed prompts:  53%|█████▎    | 1079/2048 [00:07<00:08, 113.35it/s, est. speed input: 144106.75 toks/s, output: 140.73 toks/s]
Processed prompts:  54%|█████▍    | 1109/2048 [00:08<00:08, 109.51it/s, est. speed input: 141733.79 toks/s, output: 138.41 toks/s]
Processed prompts:  55%|█████▌    | 1132/2048 [00:08<00:08, 111.76it/s, est. speed input: 141626.39 toks/s, output: 138.31 toks/s]
Processed prompts:  56%|█████▋    | 1152/2048 [00:08<00:07, 112.25it/s, est. speed input: 141152.28 toks/s, output: 137.84 toks/s]
Processed prompts:  57%|█████▋    | 1170/2048 [00:08<00:09, 97.51it/s, est. speed input: 137775.43 toks/s, output: 134.55 toks/s] 
Processed prompts:  58%|█████▊    | 1186/2048 [00:08<00:08, 96.72it/s, est. speed input: 136931.62 toks/s, output: 133.72 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [00:09<00:08, 96.04it/s, est. speed input: 136135.70 toks/s, output: 132.94 toks/s]
Processed prompts:  59%|█████▉    | 1218/2048 [00:09<00:08, 95.33it/s, est. speed input: 135360.07 toks/s, output: 132.19 toks/s]
Processed prompts:  60%|██████    | 1234/2048 [00:09<00:08, 94.58it/s, est. speed input: 134599.63 toks/s, output: 131.44 toks/s]
Processed prompts:  61%|██████    | 1250/2048 [00:09<00:08, 94.11it/s, est. speed input: 133881.45 toks/s, output: 130.74 toks/s]
Processed prompts:  62%|██████▏   | 1266/2048 [00:09<00:08, 94.81it/s, est. speed input: 133293.08 toks/s, output: 130.17 toks/s]
Processed prompts:  63%|██████▎   | 1282/2048 [00:09<00:08, 94.24it/s, est. speed input: 132624.34 toks/s, output: 129.52 toks/s]
Processed prompts:  63%|██████▎   | 1298/2048 [00:10<00:07, 93.78it/s, est. speed input: 131976.27 toks/s, output: 128.88 toks/s]
Processed prompts:  64%|██████▍   | 1314/2048 [00:10<00:07, 93.63it/s, est. speed input: 131365.27 toks/s, output: 128.29 toks/s]
Processed prompts:  65%|██████▍   | 1330/2048 [00:10<00:07, 93.16it/s, est. speed input: 130744.70 toks/s, output: 127.68 toks/s]
Processed prompts:  66%|██████▌   | 1346/2048 [00:10<00:07, 92.59it/s, est. speed input: 130126.68 toks/s, output: 127.08 toks/s]
Processed prompts:  67%|██████▋   | 1362/2048 [00:10<00:07, 92.61it/s, est. speed input: 129561.47 toks/s, output: 126.52 toks/s]
Processed prompts:  67%|██████▋   | 1378/2048 [00:10<00:07, 92.57it/s, est. speed input: 129009.86 toks/s, output: 125.99 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [00:11<00:07, 92.54it/s, est. speed input: 128475.41 toks/s, output: 125.46 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [00:11<00:06, 92.72it/s, est. speed input: 127971.23 toks/s, output: 124.97 toks/s]
Processed prompts:  70%|██████▉   | 1426/2048 [00:11<00:06, 92.34it/s, est. speed input: 127447.63 toks/s, output: 124.46 toks/s]
Processed prompts:  70%|███████   | 1442/2048 [00:11<00:06, 91.77it/s, est. speed input: 126917.92 toks/s, output: 123.94 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [00:11<00:06, 91.94it/s, est. speed input: 126443.12 toks/s, output: 123.48 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [00:11<00:06, 92.22it/s, est. speed input: 125992.89 toks/s, output: 123.04 toks/s]
Processed prompts:  73%|███████▎  | 1490/2048 [00:12<00:06, 92.28it/s, est. speed input: 125546.63 toks/s, output: 122.60 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [00:12<00:05, 92.41it/s, est. speed input: 125117.40 toks/s, output: 122.18 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [00:12<00:05, 92.26it/s, est. speed input: 124686.07 toks/s, output: 121.76 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [00:12<00:05, 92.43it/s, est. speed input: 124282.71 toks/s, output: 121.37 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [00:12<00:05, 92.42it/s, est. speed input: 123883.15 toks/s, output: 120.98 toks/s]
Processed prompts:  77%|███████▋  | 1570/2048 [00:13<00:05, 92.44it/s, est. speed input: 123495.05 toks/s, output: 120.60 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [00:13<00:04, 93.62it/s, est. speed input: 123184.81 toks/s, output: 120.30 toks/s]
Processed prompts:  78%|███████▊  | 1602/2048 [00:13<00:04, 93.43it/s, est. speed input: 122824.20 toks/s, output: 119.95 toks/s]
Processed prompts:  79%|███████▉  | 1618/2048 [00:13<00:04, 93.06it/s, est. speed input: 122459.92 toks/s, output: 119.59 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [00:13<00:04, 92.82it/s, est. speed input: 122105.77 toks/s, output: 119.24 toks/s]
Processed prompts:  81%|████████  | 1650/2048 [00:13<00:04, 92.67it/s, est. speed input: 121761.51 toks/s, output: 118.91 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [00:14<00:04, 92.51it/s, est. speed input: 121422.99 toks/s, output: 118.58 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [00:14<00:03, 92.56it/s, est. speed input: 121100.88 toks/s, output: 118.26 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [00:14<00:03, 92.46it/s, est. speed input: 120779.72 toks/s, output: 117.95 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [00:14<00:03, 92.37it/s, est. speed input: 120464.86 toks/s, output: 117.64 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [00:14<00:03, 92.53it/s, est. speed input: 120169.17 toks/s, output: 117.35 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [00:14<00:03, 92.47it/s, est. speed input: 119871.13 toks/s, output: 117.06 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [00:15<00:03, 92.34it/s, est. speed input: 119576.23 toks/s, output: 116.77 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [00:15<00:02, 92.57it/s, est. speed input: 119303.12 toks/s, output: 116.51 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [00:15<00:02, 92.35it/s, est. speed input: 119018.29 toks/s, output: 116.23 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [00:15<00:02, 92.34it/s, est. speed input: 118746.19 toks/s, output: 115.96 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [00:15<00:02, 92.51it/s, est. speed input: 118488.77 toks/s, output: 115.71 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [00:15<00:02, 92.42it/s, est. speed input: 118226.97 toks/s, output: 115.46 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [00:16<00:02, 91.59it/s, est. speed input: 117935.52 toks/s, output: 115.17 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [00:16<00:01, 93.74it/s, est. speed input: 117772.99 toks/s, output: 115.01 toks/s]
Processed prompts:  92%|█████████▏| 1890/2048 [00:16<00:01, 93.41it/s, est. speed input: 117532.91 toks/s, output: 114.78 toks/s]
Processed prompts:  93%|█████████▎| 1906/2048 [00:16<00:01, 93.00it/s, est. speed input: 117289.82 toks/s, output: 114.54 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [00:16<00:01, 92.64it/s, est. speed input: 117048.65 toks/s, output: 114.31 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [00:16<00:01, 92.45it/s, est. speed input: 116814.94 toks/s, output: 114.08 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [00:17<00:01, 93.78it/s, est. speed input: 116647.09 toks/s, output: 113.91 toks/s]
Processed prompts:  96%|█████████▌| 1970/2048 [00:17<00:00, 93.49it/s, est. speed input: 116432.37 toks/s, output: 113.70 toks/s]
Processed prompts:  97%|█████████▋| 1986/2048 [00:17<00:00, 93.44it/s, est. speed input: 116228.08 toks/s, output: 113.50 toks/s]
Processed prompts:  98%|█████████▊| 2002/2048 [00:17<00:00, 93.11it/s, est. speed input: 116015.40 toks/s, output: 113.30 toks/s]
Processed prompts:  99%|█████████▊| 2018/2048 [00:17<00:00, 92.98it/s, est. speed input: 115811.03 toks/s, output: 113.10 toks/s]
Processed prompts:  99%|█████████▉| 2034/2048 [00:18<00:00, 94.43it/s, est. speed input: 115671.01 toks/s, output: 112.96 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:18<00:00, 94.43it/s, est. speed input: 116463.61 toks/s, output: 113.73 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:18<00:00, 113.73it/s, est. speed input: 116463.61 toks/s, output: 113.73 toks/s]
[rank0]:[W128 08:55:28.533264016 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 68.6s

测试结果:
  Requests/s:   92.39
  Tokens/s:     94697.67
  Total Reqs:   2048
  Elapsed:      22.17s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     94605.28

============================================================
[7/7] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-INT8                                  │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 08:55:54 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3342023) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3342023) WARNING 01-28 08:56:09 [backends.py:609] Failed to read file <frozen os>
Throughput: 93.80 requests/s, 96147.59 total tokens/s, 93.80 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-28 08:55:53] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:55:54] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:55:54] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:55:54] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:55:54] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:55:54] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:55:54] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:55:54] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:55:54] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:55:54] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:55:54] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:55:54] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:55:54] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:55:54] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 08:56:00] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:56:01] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-INT8'
[2026-01-28 08:56:01] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-INT8
[2026-01-28 08:56:01] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:56:01] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:56:01] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:56:01] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:56:01] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-INT8
[2026-01-28 08:56:01] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-INT8'
[2026-01-28 08:56:01] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:56:01] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:56:01] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:56:01] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:56:01] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3342023) [2026-01-28 08:56:02] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3342023) [2026-01-28 08:56:02] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3342023) [2026-01-28 08:56:02] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3342023) [2026-01-28 08:56:02] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3342023) [2026-01-28 08:56:02] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: BitNet-2B-INT8
(EngineCore_DP0 pid=3342023) [2026-01-28 08:56:02] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3342023) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3342023) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.07it/s]
(EngineCore_DP0 pid=3342023) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.07it/s]
(EngineCore_DP0 pid=3342023) 
(EngineCore_DP0 pid=3342023) [2026-01-28 08:56:03] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=3342023) [2026-01-28 08:56:03] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9830400 bytes
(EngineCore_DP0 pid=3342023) [2026-01-28 08:56:03] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=3342023) [2026-01-28 08:56:03] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6553600 bytes
(EngineCore_DP0 pid=3342023) [2026-01-28 08:56:03] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=3342023) [2026-01-28 08:56:03] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 35389440 bytes
(EngineCore_DP0 pid=3342023) [2026-01-28 08:56:03] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=3342023) [2026-01-28 08:56:03] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 17715200 bytes
(EngineCore_DP0 pid=3342023) [rank0]:W0128 08:56:15.544000 3342023 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3342023) [rank0]:W0128 08:56:15.636000 3342023 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3342023) [rank0]:W0128 08:56:16.643000 3342023 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3342023) [rank0]:W0128 08:56:16.772000 3342023 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3342023) 2026-01-28 08:56:20,668 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3342023) 2026-01-28 08:56:20,698 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3342023) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▉         | 1/11 [00:00<00:01,  9.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:01,  8.73it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 3/11 [00:00<00:01,  5.82it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 5/11 [00:00<00:00,  8.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▎   | 7/11 [00:00<00:00, 10.79it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 9/11 [00:00<00:00, 12.55it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00, 13.11it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00, 10.96it/s]
(EngineCore_DP0 pid=3342023) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00, 15.79it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00, 16.56it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00, 16.80it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 16.72it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 35/4096 [00:00<00:11, 346.64it/s]
Adding requests:   2%|▏         | 86/4096 [00:00<00:09, 438.37it/s]
Adding requests:   3%|▎         | 135/4096 [00:00<00:08, 461.21it/s]
Adding requests:   5%|▍         | 185/4096 [00:00<00:08, 474.78it/s]
Adding requests:   6%|▌         | 237/4096 [00:00<00:07, 489.70it/s]
Adding requests:   7%|▋         | 288/4096 [00:00<00:07, 495.13it/s]
Adding requests:   8%|▊         | 339/4096 [00:00<00:07, 496.83it/s]
Adding requests:  10%|▉         | 392/4096 [00:00<00:07, 506.51it/s]
Adding requests:  11%|█         | 443/4096 [00:00<00:07, 506.57it/s]
Adding requests:  12%|█▏        | 495/4096 [00:01<00:07, 508.85it/s]
Adding requests:  13%|█▎        | 546/4096 [00:01<00:07, 500.97it/s]
Adding requests:  15%|█▍        | 599/4096 [00:01<00:06, 508.24it/s]
Adding requests:  16%|█▌        | 652/4096 [00:01<00:06, 513.05it/s]
Adding requests:  17%|█▋        | 706/4096 [00:01<00:06, 519.58it/s]
Adding requests:  19%|█▊        | 758/4096 [00:01<00:06, 518.43it/s]
Adding requests:  20%|█▉        | 810/4096 [00:01<00:07, 455.49it/s]
Adding requests:  21%|██        | 858/4096 [00:01<00:07, 462.04it/s]
Adding requests:  22%|██▏       | 910/4096 [00:01<00:06, 477.93it/s]
Adding requests:  23%|██▎       | 962/4096 [00:01<00:06, 488.92it/s]
Adding requests:  25%|██▍       | 1014/4096 [00:02<00:06, 496.21it/s]
Adding requests:  26%|██▌       | 1065/4096 [00:02<00:06, 500.08it/s]
Adding requests:  27%|██▋       | 1116/4096 [00:02<00:06, 482.24it/s]
Adding requests:  29%|██▊       | 1169/4096 [00:02<00:05, 495.67it/s]
Adding requests:  30%|██▉       | 1222/4096 [00:02<00:05, 505.41it/s]
Adding requests:  31%|███       | 1273/4096 [00:02<00:05, 501.94it/s]
Adding requests:  32%|███▏      | 1326/4096 [00:02<00:05, 506.39it/s]
Adding requests:  34%|███▎      | 1379/4096 [00:02<00:05, 510.43it/s]
Adding requests:  35%|███▍      | 1432/4096 [00:02<00:05, 514.63it/s]
Adding requests:  36%|███▋      | 1485/4096 [00:02<00:05, 516.70it/s]
Adding requests:  38%|███▊      | 1538/4096 [00:03<00:04, 520.35it/s]
Adding requests:  39%|███▉      | 1592/4096 [00:03<00:04, 523.92it/s]
Adding requests:  40%|████      | 1645/4096 [00:03<00:04, 525.11it/s]
Adding requests:  41%|████▏     | 1698/4096 [00:03<00:04, 519.81it/s]
Adding requests:  43%|████▎     | 1751/4096 [00:03<00:04, 520.13it/s]
Adding requests:  44%|████▍     | 1804/4096 [00:03<00:04, 518.46it/s]
Adding requests:  45%|████▌     | 1857/4096 [00:03<00:04, 519.14it/s]
Adding requests:  47%|████▋     | 1909/4096 [00:03<00:04, 514.65it/s]
Adding requests:  48%|████▊     | 1962/4096 [00:03<00:04, 516.68it/s]
Adding requests:  49%|████▉     | 2015/4096 [00:04<00:04, 519.25it/s]
Adding requests:  50%|█████     | 2067/4096 [00:04<00:03, 512.75it/s]
Adding requests:  52%|█████▏    | 2119/4096 [00:04<00:03, 512.83it/s]
Adding requests:  53%|█████▎    | 2171/4096 [00:04<00:03, 506.53it/s]
Adding requests:  54%|█████▍    | 2223/4096 [00:04<00:03, 509.36it/s]
Adding requests:  56%|█████▌    | 2274/4096 [00:04<00:03, 498.11it/s]
Adding requests:  57%|█████▋    | 2325/4096 [00:04<00:03, 501.17it/s]
Adding requests:  58%|█████▊    | 2377/4096 [00:04<00:03, 504.95it/s]
Adding requests:  59%|█████▉    | 2429/4096 [00:04<00:03, 508.88it/s]
Adding requests:  61%|██████    | 2481/4096 [00:04<00:03, 510.46it/s]
Adding requests:  62%|██████▏   | 2533/4096 [00:05<00:03, 510.24it/s]
Adding requests:  63%|██████▎   | 2586/4096 [00:05<00:02, 515.02it/s]
Adding requests:  64%|██████▍   | 2638/4096 [00:05<00:02, 514.73it/s]
Adding requests:  66%|██████▌   | 2691/4096 [00:05<00:02, 516.34it/s]
Adding requests:  67%|██████▋   | 2743/4096 [00:05<00:02, 514.39it/s]
Adding requests:  68%|██████▊   | 2795/4096 [00:05<00:02, 511.52it/s]
Adding requests:  70%|██████▉   | 2848/4096 [00:05<00:02, 514.04it/s]
Adding requests:  71%|███████   | 2900/4096 [00:05<00:02, 515.30it/s]
Adding requests:  72%|███████▏  | 2952/4096 [00:05<00:02, 511.91it/s]
Adding requests:  73%|███████▎  | 3004/4096 [00:05<00:02, 512.74it/s]
Adding requests:  75%|███████▍  | 3056/4096 [00:06<00:02, 514.78it/s]
Adding requests:  76%|███████▌  | 3108/4096 [00:06<00:01, 510.45it/s]
Adding requests:  77%|███████▋  | 3160/4096 [00:06<00:01, 513.01it/s]
Adding requests:  78%|███████▊  | 3212/4096 [00:06<00:01, 512.10it/s]
Adding requests:  80%|███████▉  | 3265/4096 [00:06<00:01, 515.84it/s]
Adding requests:  81%|████████  | 3317/4096 [00:06<00:01, 514.59it/s]
Adding requests:  82%|████████▏ | 3369/4096 [00:06<00:01, 515.76it/s]
Adding requests:  84%|████████▎ | 3421/4096 [00:06<00:01, 516.68it/s]
Adding requests:  85%|████████▍ | 3473/4096 [00:06<00:01, 508.01it/s]
Adding requests:  86%|████████▌ | 3525/4096 [00:06<00:01, 508.77it/s]
Adding requests:  87%|████████▋ | 3576/4096 [00:07<00:01, 508.62it/s]
Adding requests:  89%|████████▊ | 3627/4096 [00:07<00:00, 490.41it/s]
Adding requests:  90%|████████▉ | 3680/4096 [00:07<00:00, 500.78it/s]
Adding requests:  91%|█████████ | 3732/4096 [00:07<00:00, 503.84it/s]
Adding requests:  92%|█████████▏| 3787/4096 [00:07<00:00, 514.81it/s]
Adding requests:  94%|█████████▎| 3839/4096 [00:07<00:00, 515.87it/s]
Adding requests:  95%|█████████▌| 3893/4096 [00:07<00:00, 520.07it/s]
Adding requests:  96%|█████████▋| 3946/4096 [00:07<00:00, 519.48it/s]
Adding requests:  98%|█████████▊| 3998/4096 [00:07<00:00, 516.67it/s]
Adding requests:  99%|█████████▉| 4050/4096 [00:07<00:00, 511.66it/s]
Adding requests: 100%|██████████| 4096/4096 [00:08<00:00, 506.55it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  18%|█▊        | 738/4096 [00:00<00:01, 2713.39it/s, est. speed input: 2778978.20 toks/s, output: 2713.52 toks/s]
Processed prompts:  25%|██▍       | 1010/4096 [00:02<00:11, 272.57it/s, est. speed input: 347665.64 toks/s, output: 339.52 toks/s]  
Processed prompts:  28%|██▊       | 1128/4096 [00:04<00:14, 197.91it/s, est. speed input: 266455.47 toks/s, output: 260.21 toks/s]
Processed prompts:  29%|██▉       | 1196/4096 [00:05<00:16, 176.60it/s, est. speed input: 244649.28 toks/s, output: 238.91 toks/s]
Processed prompts:  30%|███       | 1242/4096 [00:05<00:16, 170.69it/s, est. speed input: 237755.83 toks/s, output: 232.18 toks/s]
Processed prompts:  31%|███       | 1276/4096 [00:05<00:17, 159.66it/s, est. speed input: 229970.25 toks/s, output: 224.58 toks/s]
Processed prompts:  32%|███▏      | 1302/4096 [00:06<00:19, 143.86it/s, est. speed input: 221431.31 toks/s, output: 216.24 toks/s]
Processed prompts:  32%|███▏      | 1322/4096 [00:06<00:22, 125.49it/s, est. speed input: 212744.90 toks/s, output: 207.76 toks/s]
Processed prompts:  33%|███▎      | 1346/4096 [00:06<00:24, 112.64it/s, est. speed input: 205619.84 toks/s, output: 200.80 toks/s]
Processed prompts:  34%|███▎      | 1378/4096 [00:07<00:25, 107.93it/s, est. speed input: 200321.73 toks/s, output: 195.63 toks/s]
Processed prompts:  34%|███▍      | 1410/4096 [00:07<00:25, 104.13it/s, est. speed input: 195481.25 toks/s, output: 190.90 toks/s]
Processed prompts:  35%|███▌      | 1442/4096 [00:07<00:26, 101.41it/s, est. speed input: 191123.96 toks/s, output: 186.64 toks/s]
Processed prompts:  36%|███▌      | 1474/4096 [00:08<00:26, 99.20it/s, est. speed input: 187083.43 toks/s, output: 182.70 toks/s] 
Processed prompts:  37%|███▋      | 1506/4096 [00:08<00:26, 97.58it/s, est. speed input: 183372.45 toks/s, output: 179.07 toks/s]
Processed prompts:  38%|███▊      | 1538/4096 [00:08<00:26, 96.50it/s, est. speed input: 179970.90 toks/s, output: 175.75 toks/s]
Processed prompts:  38%|███▊      | 1570/4096 [00:09<00:26, 96.35it/s, est. speed input: 176974.73 toks/s, output: 172.83 toks/s]
Processed prompts:  39%|███▉      | 1602/4096 [00:09<00:26, 95.36it/s, est. speed input: 173992.25 toks/s, output: 169.91 toks/s]
Processed prompts:  40%|███▉      | 1634/4096 [00:09<00:25, 94.82it/s, est. speed input: 171255.39 toks/s, output: 167.24 toks/s]
Processed prompts:  41%|████      | 1666/4096 [00:10<00:25, 94.52it/s, est. speed input: 168717.82 toks/s, output: 164.76 toks/s]
Processed prompts:  41%|████▏     | 1698/4096 [00:10<00:25, 94.28it/s, est. speed input: 166341.15 toks/s, output: 162.44 toks/s]
Processed prompts:  42%|████▏     | 1730/4096 [00:10<00:25, 94.21it/s, est. speed input: 164133.67 toks/s, output: 160.29 toks/s]
Processed prompts:  43%|████▎     | 1762/4096 [00:11<00:24, 94.01it/s, est. speed input: 162034.60 toks/s, output: 158.24 toks/s]
Processed prompts:  44%|████▍     | 1794/4096 [00:11<00:24, 93.95it/s, est. speed input: 160072.68 toks/s, output: 156.32 toks/s]
Processed prompts:  45%|████▍     | 1826/4096 [00:11<00:24, 94.09it/s, est. speed input: 158253.95 toks/s, output: 154.54 toks/s]
Processed prompts:  45%|████▌     | 1858/4096 [00:12<00:23, 94.71it/s, est. speed input: 156619.03 toks/s, output: 152.95 toks/s]
Processed prompts:  46%|████▌     | 1890/4096 [00:12<00:23, 94.47it/s, est. speed input: 154969.54 toks/s, output: 151.34 toks/s]
Processed prompts:  47%|████▋     | 1922/4096 [00:12<00:23, 94.30it/s, est. speed input: 153407.51 toks/s, output: 149.81 toks/s]
Processed prompts:  48%|████▊     | 1954/4096 [00:13<00:22, 94.88it/s, est. speed input: 152022.75 toks/s, output: 148.46 toks/s]
Processed prompts:  48%|████▊     | 1986/4096 [00:13<00:22, 94.50it/s, est. speed input: 150601.85 toks/s, output: 147.07 toks/s]
Processed prompts:  49%|████▉     | 2018/4096 [00:13<00:22, 94.40it/s, est. speed input: 149271.37 toks/s, output: 145.77 toks/s]
Processed prompts:  50%|█████     | 2050/4096 [00:14<00:21, 94.28it/s, est. speed input: 147999.35 toks/s, output: 144.53 toks/s]
Processed prompts:  51%|█████     | 2082/4096 [00:14<00:21, 94.18it/s, est. speed input: 146783.99 toks/s, output: 143.34 toks/s]
Processed prompts:  52%|█████▏    | 2114/4096 [00:14<00:21, 93.66it/s, est. speed input: 145571.67 toks/s, output: 142.16 toks/s]
Processed prompts:  52%|█████▏    | 2146/4096 [00:15<00:20, 94.02it/s, est. speed input: 144498.38 toks/s, output: 141.11 toks/s]
Processed prompts:  53%|█████▎    | 2178/4096 [00:15<00:20, 93.91it/s, est. speed input: 143430.32 toks/s, output: 140.07 toks/s]
Processed prompts:  54%|█████▍    | 2210/4096 [00:15<00:19, 95.50it/s, est. speed input: 142586.87 toks/s, output: 139.24 toks/s]
Processed prompts:  55%|█████▍    | 2242/4096 [00:16<00:19, 95.09it/s, est. speed input: 141618.10 toks/s, output: 138.30 toks/s]
Processed prompts:  56%|█████▌    | 2274/4096 [00:16<00:19, 95.44it/s, est. speed input: 140753.28 toks/s, output: 137.45 toks/s]
Processed prompts:  56%|█████▋    | 2306/4096 [00:16<00:18, 94.88it/s, est. speed input: 139844.07 toks/s, output: 136.57 toks/s]
Processed prompts:  57%|█████▋    | 2338/4096 [00:17<00:18, 95.16it/s, est. speed input: 139034.52 toks/s, output: 135.78 toks/s]
Processed prompts:  58%|█████▊    | 2370/4096 [00:17<00:17, 96.58it/s, est. speed input: 138367.70 toks/s, output: 135.12 toks/s]
Processed prompts:  59%|█████▊    | 2402/4096 [00:17<00:17, 95.64it/s, est. speed input: 137552.95 toks/s, output: 134.33 toks/s]
Processed prompts:  59%|█████▉    | 2434/4096 [00:18<00:17, 95.12it/s, est. speed input: 136778.78 toks/s, output: 133.57 toks/s]
Processed prompts:  60%|██████    | 2466/4096 [00:18<00:17, 94.73it/s, est. speed input: 136031.01 toks/s, output: 132.84 toks/s]
Processed prompts:  61%|██████    | 2498/4096 [00:18<00:16, 95.13it/s, est. speed input: 135367.70 toks/s, output: 132.19 toks/s]
Processed prompts:  62%|██████▏   | 2530/4096 [00:19<00:16, 94.74it/s, est. speed input: 134671.59 toks/s, output: 131.51 toks/s]
Processed prompts:  63%|██████▎   | 2562/4096 [00:19<00:16, 95.19it/s, est. speed input: 134058.17 toks/s, output: 130.92 toks/s]
Processed prompts:  63%|██████▎   | 2594/4096 [00:19<00:15, 94.70it/s, est. speed input: 133401.31 toks/s, output: 130.27 toks/s]
Processed prompts:  64%|██████▍   | 2626/4096 [00:20<00:15, 94.39it/s, est. speed input: 132769.73 toks/s, output: 129.66 toks/s]
Processed prompts:  65%|██████▍   | 2658/4096 [00:20<00:15, 94.29it/s, est. speed input: 132167.16 toks/s, output: 129.07 toks/s]
Processed prompts:  66%|██████▌   | 2690/4096 [00:20<00:14, 94.10it/s, est. speed input: 131575.69 toks/s, output: 128.49 toks/s]
Processed prompts:  66%|██████▋   | 2722/4096 [00:21<00:14, 94.00it/s, est. speed input: 131005.51 toks/s, output: 127.93 toks/s]
Processed prompts:  67%|██████▋   | 2754/4096 [00:21<00:14, 94.04it/s, est. speed input: 130460.95 toks/s, output: 127.40 toks/s]
Processed prompts:  68%|██████▊   | 2786/4096 [00:21<00:13, 93.96it/s, est. speed input: 129925.61 toks/s, output: 126.88 toks/s]
Processed prompts:  69%|██████▉   | 2818/4096 [00:22<00:13, 93.93it/s, est. speed input: 129408.40 toks/s, output: 126.38 toks/s]
Processed prompts:  70%|██████▉   | 2850/4096 [00:22<00:13, 93.87it/s, est. speed input: 128904.74 toks/s, output: 125.88 toks/s]
Processed prompts:  70%|███████   | 2882/4096 [00:22<00:12, 93.78it/s, est. speed input: 128412.15 toks/s, output: 125.40 toks/s]
Processed prompts:  71%|███████   | 2914/4096 [00:23<00:12, 93.73it/s, est. speed input: 127935.38 toks/s, output: 124.94 toks/s]
Processed prompts:  72%|███████▏  | 2946/4096 [00:23<00:12, 93.62it/s, est. speed input: 127467.19 toks/s, output: 124.48 toks/s]
Processed prompts:  73%|███████▎  | 2978/4096 [00:24<00:11, 93.55it/s, est. speed input: 127012.86 toks/s, output: 124.04 toks/s]
Processed prompts:  73%|███████▎  | 3010/4096 [00:24<00:11, 93.59it/s, est. speed input: 126576.81 toks/s, output: 123.61 toks/s]
Processed prompts:  74%|███████▍  | 3042/4096 [00:24<00:11, 93.57it/s, est. speed input: 126150.11 toks/s, output: 123.19 toks/s]
Processed prompts:  75%|███████▌  | 3074/4096 [00:25<00:10, 93.56it/s, est. speed input: 125734.79 toks/s, output: 122.79 toks/s]
Processed prompts:  76%|███████▌  | 3106/4096 [00:25<00:10, 93.76it/s, est. speed input: 125343.47 toks/s, output: 122.41 toks/s]
Processed prompts:  77%|███████▋  | 3138/4096 [00:25<00:10, 94.36it/s, est. speed input: 124989.11 toks/s, output: 122.06 toks/s]
Processed prompts:  77%|███████▋  | 3170/4096 [00:26<00:09, 93.96it/s, est. speed input: 124596.98 toks/s, output: 121.68 toks/s]
Processed prompts:  78%|███████▊  | 3202/4096 [00:26<00:09, 94.00it/s, est. speed input: 124233.09 toks/s, output: 121.32 toks/s]
Processed prompts:  79%|███████▉  | 3234/4096 [00:26<00:09, 93.86it/s, est. speed input: 123868.93 toks/s, output: 120.97 toks/s]
Processed prompts:  80%|███████▉  | 3266/4096 [00:27<00:08, 93.67it/s, est. speed input: 123508.63 toks/s, output: 120.61 toks/s]
Processed prompts:  81%|████████  | 3298/4096 [00:27<00:08, 93.76it/s, est. speed input: 123170.03 toks/s, output: 120.28 toks/s]
Processed prompts:  81%|████████▏ | 3330/4096 [00:27<00:08, 93.74it/s, est. speed input: 122834.70 toks/s, output: 119.96 toks/s]
Processed prompts:  82%|████████▏ | 3362/4096 [00:28<00:07, 93.66it/s, est. speed input: 122504.33 toks/s, output: 119.63 toks/s]
Processed prompts:  83%|████████▎ | 3394/4096 [00:28<00:07, 93.64it/s, est. speed input: 122183.98 toks/s, output: 119.32 toks/s]
Processed prompts:  84%|████████▎ | 3426/4096 [00:28<00:07, 93.68it/s, est. speed input: 121873.57 toks/s, output: 119.02 toks/s]
Processed prompts:  84%|████████▍ | 3458/4096 [00:29<00:06, 93.60it/s, est. speed input: 121565.52 toks/s, output: 118.72 toks/s]
Processed prompts:  85%|████████▌ | 3490/4096 [00:29<00:06, 95.13it/s, est. speed input: 121342.72 toks/s, output: 118.50 toks/s]
Processed prompts:  86%|████████▌ | 3522/4096 [00:29<00:06, 94.73it/s, est. speed input: 121053.45 toks/s, output: 118.22 toks/s]
Processed prompts:  87%|████████▋ | 3554/4096 [00:30<00:05, 94.39it/s, est. speed input: 120767.09 toks/s, output: 117.94 toks/s]
Processed prompts:  88%|████████▊ | 3586/4096 [00:30<00:05, 94.11it/s, est. speed input: 120485.89 toks/s, output: 117.66 toks/s]
Processed prompts:  88%|████████▊ | 3618/4096 [00:30<00:05, 93.95it/s, est. speed input: 120211.91 toks/s, output: 117.39 toks/s]
Processed prompts:  89%|████████▉ | 3650/4096 [00:31<00:04, 93.89it/s, est. speed input: 119946.66 toks/s, output: 117.14 toks/s]
Processed prompts:  90%|████████▉ | 3682/4096 [00:31<00:04, 93.87it/s, est. speed input: 119688.23 toks/s, output: 116.88 toks/s]
Processed prompts:  91%|█████████ | 3714/4096 [00:31<00:04, 94.37it/s, est. speed input: 119458.59 toks/s, output: 116.66 toks/s]
Processed prompts:  91%|█████████▏| 3746/4096 [00:32<00:03, 94.09it/s, est. speed input: 119205.84 toks/s, output: 116.41 toks/s]
Processed prompts:  92%|█████████▏| 3778/4096 [00:32<00:03, 93.99it/s, est. speed input: 118962.17 toks/s, output: 116.17 toks/s]
Processed prompts:  93%|█████████▎| 3810/4096 [00:32<00:03, 93.81it/s, est. speed input: 118718.91 toks/s, output: 115.94 toks/s]
Processed prompts:  94%|█████████▍| 3842/4096 [00:33<00:02, 94.29it/s, est. speed input: 118506.82 toks/s, output: 115.73 toks/s]
Processed prompts:  95%|█████████▍| 3874/4096 [00:33<00:02, 94.12it/s, est. speed input: 118277.36 toks/s, output: 115.51 toks/s]
Processed prompts:  95%|█████████▌| 3906/4096 [00:33<00:02, 93.94it/s, est. speed input: 118049.91 toks/s, output: 115.28 toks/s]
Processed prompts:  96%|█████████▌| 3938/4096 [00:34<00:01, 93.80it/s, est. speed input: 117826.51 toks/s, output: 115.06 toks/s]
Processed prompts:  97%|█████████▋| 3970/4096 [00:34<00:01, 93.66it/s, est. speed input: 117606.11 toks/s, output: 114.85 toks/s]
Processed prompts:  98%|█████████▊| 4002/4096 [00:34<00:01, 93.57it/s, est. speed input: 117389.99 toks/s, output: 114.64 toks/s]
Processed prompts:  98%|█████████▊| 4034/4096 [00:35<00:00, 94.08it/s, est. speed input: 117201.37 toks/s, output: 114.45 toks/s]
Processed prompts:  99%|█████████▉| 4066/4096 [00:35<00:00, 94.91it/s, est. speed input: 117034.34 toks/s, output: 114.29 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:35<00:00, 94.91it/s, est. speed input: 117894.33 toks/s, output: 115.13 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:35<00:00, 115.13it/s, est. speed input: 117894.33 toks/s, output: 115.13 toks/s]
[rank0]:[W128 08:57:07.773285610 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 99.3s

测试结果:
  Requests/s:   93.80
  Tokens/s:     96147.59
  Total Reqs:   4096
  Elapsed:      43.67s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     96053.78


------------------------------------------------------------
  生成 CSV: BitNet-2B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_INT8_py312_cu129_x86_64/cusparselt/2_10/BitNet-2B-INT8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,31.8477,16337.8504,4.0191
1024,1024,1,128,128,32.7597,33578.6812,3.9072
2048,1024,2,256,128,64.1421,65745.6681,3.9911
4096,1024,4,512,128,81.1357,83164.0610,6.3104
8192,1024,8,1024,128,88.3981,90608.0814,11.5840
16384,1024,16,2048,128,92.3880,94697.6682,22.1674
32768,1024,32,4096,128,93.8025,96147.5872,43.6662

------------------------------------------------------------

[INFO] 完成: 7 成功, 0 失败


============================================================
  Benchmark 完成!
============================================================


总计: 35 成功, 0 失败
============================================================
