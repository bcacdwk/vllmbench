======================================================================
SlideSparse vLLM Throughput Benchmark Log
Created: 2026-01-28 08:57:16
======================================================================

原始命令:
  /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model bitnet1.58-2b-fp8 --backend cublaslt,cusparselt --stage prefill --sparsity 2_4,2_6,2_8,2_10 --M 512,1024,2048,4096,8192,16384,32768

命令行参数:
  --model: bitnet1.58-2b-fp8
  --backend: cublaslt,cusparselt
  --sparsity: 2_4,2_6,2_8,2_10
  --stage: prefill
  --M: 512,1024,2048,4096,8192,16384,32768
  --N: None
  --inner-32: False
  --eager: False
  --gpu-id: 3
  --gpu-mem: 0.8
  --dry-run: False
  --list-models: False

硬件信息:
  GPU: H100
  Compute Capability: cc90
  VRAM: 79.2 GB
  CUDA: 12.9
  Python: py312

Backend 环境变量 (初始状态):
  DISABLE_SLIDESPARSE: 未设置
  USE_CUBLASLT: 未设置
  USE_CUSPARSELT: 未设置
  SPARSITY: 未设置
  INNER_DTYPE_32: 未设置

======================================================================


============================================================
  BitNet-2B-FP8 | cuBLASLt | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints/BitNet-2B-FP8
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_FP8E4M3_py312_cu129_x86_64/cublaslt

============================================================
[1/7] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuBLASLt                                        │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 08:57:23 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3343867) WARNING 01-28 08:57:37 [backends.py:609] Failed to read file <frozen os>
Throughput: 30.38 requests/s, 15586.29 total tokens/s, 30.38 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-28 08:57:22] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:57:23] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 08:57:23] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 08:57:23] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:57:23] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:57:23] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:57:23] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:57:23] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:57:23] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 08:57:23] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:57:23] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:57:23] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:57:23] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:57:23] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 08:57:29] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:57:29] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 08:57:29] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 08:57:29] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:57:29] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:57:29] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:57:29] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:57:29] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:57:29] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 08:57:29] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:57:29] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:57:29] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:57:29] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:57:29] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3343867) [2026-01-28 08:57:30] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=3343867) [2026-01-28 08:57:30] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3343867) [2026-01-28 08:57:30] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=3343867) [2026-01-28 08:57:30] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3343867) [2026-01-28 08:57:30] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=3343867) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3343867) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.83it/s]
(EngineCore_DP0 pid=3343867) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.83it/s]
(EngineCore_DP0 pid=3343867) 
(EngineCore_DP0 pid=3343867) 2026-01-28 08:57:49,062 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3343867) 2026-01-28 08:57:49,090 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3343867) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  2.55it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  2.92it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  2.85it/s]
(EngineCore_DP0 pid=3343867) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 15.21it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  41%|████▏     | 53/128 [00:00<00:00, 528.12it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 702.77it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:17,  7.17it/s, est. speed input: 3674.00 toks/s, output: 7.17 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:05, 20.82it/s, est. speed input: 9569.59 toks/s, output: 18.69 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:04, 26.49it/s, est. speed input: 11977.32 toks/s, output: 23.39 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:03, 29.00it/s, est. speed input: 13153.87 toks/s, output: 25.69 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:03, 29.97it/s, est. speed input: 13764.28 toks/s, output: 26.88 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:03, 30.80it/s, est. speed input: 14228.48 toks/s, output: 27.79 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:00<00:03, 31.45it/s, est. speed input: 14586.07 toks/s, output: 28.49 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:01<00:03, 31.78it/s, est. speed input: 14838.31 toks/s, output: 28.98 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:01<00:02, 32.07it/s, est. speed input: 15045.71 toks/s, output: 29.39 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:01<00:02, 32.26it/s, est. speed input: 15212.28 toks/s, output: 29.71 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:01<00:02, 32.38it/s, est. speed input: 15347.17 toks/s, output: 29.97 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:01<00:02, 32.44it/s, est. speed input: 15456.91 toks/s, output: 30.19 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:01<00:02, 32.47it/s, est. speed input: 15548.73 toks/s, output: 30.37 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:01<00:02, 32.46it/s, est. speed input: 15624.26 toks/s, output: 30.52 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:01<00:02, 32.46it/s, est. speed input: 15690.01 toks/s, output: 30.64 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:01<00:02, 32.49it/s, est. speed input: 15750.51 toks/s, output: 30.76 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:02<00:01, 32.57it/s, est. speed input: 15809.97 toks/s, output: 30.88 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:02<00:01, 32.61it/s, est. speed input: 15861.60 toks/s, output: 30.98 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:02<00:01, 32.68it/s, est. speed input: 15910.63 toks/s, output: 31.08 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:02<00:01, 32.63it/s, est. speed input: 15947.26 toks/s, output: 31.15 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:02<00:01, 32.62it/s, est. speed input: 15982.70 toks/s, output: 31.22 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:02<00:01, 32.66it/s, est. speed input: 16018.20 toks/s, output: 31.29 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:02<00:01, 32.69it/s, est. speed input: 16050.59 toks/s, output: 31.35 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:02<00:01, 32.72it/s, est. speed input: 16080.57 toks/s, output: 31.41 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:03<00:00, 32.68it/s, est. speed input: 16104.74 toks/s, output: 31.45 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:03<00:00, 32.69it/s, est. speed input: 16129.22 toks/s, output: 31.50 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:03<00:00, 32.71it/s, est. speed input: 16153.12 toks/s, output: 31.55 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:03<00:00, 32.76it/s, est. speed input: 16176.99 toks/s, output: 31.60 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:03<00:00, 32.78it/s, est. speed input: 16198.27 toks/s, output: 31.64 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:03<00:00, 32.85it/s, est. speed input: 16221.76 toks/s, output: 31.68 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:03<00:00, 32.84it/s, est. speed input: 16240.10 toks/s, output: 31.72 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:03<00:00, 32.78it/s, est. speed input: 16254.70 toks/s, output: 31.75 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:04<00:00, 32.78it/s, est. speed input: 16266.33 toks/s, output: 31.77 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:04<00:00, 31.77it/s, est. speed input: 16266.33 toks/s, output: 31.77 toks/s]
[rank0]:[W128 08:57:55.885190442 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 41.5s

测试结果:
  Requests/s:   30.38
  Tokens/s:     15586.29
  Total Reqs:   128
  Elapsed:      4.21s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     15555.91

============================================================
[2/7] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuBLASLt                                        │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 08:58:05 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3345027) WARNING 01-28 08:58:20 [backends.py:609] Failed to read file <frozen os>
Throughput: 31.01 requests/s, 31784.72 total tokens/s, 31.01 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-28 08:58:04] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:58:04] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 08:58:04] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 08:58:04] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:58:04] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:58:04] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:58:04] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:58:04] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:58:04] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 08:58:04] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:58:04] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:58:04] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:58:04] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:58:04] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 08:58:11] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:58:11] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 08:58:11] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 08:58:11] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:58:11] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:58:11] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:58:11] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:58:11] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:58:11] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 08:58:11] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:58:11] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:58:11] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:58:11] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:58:11] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3345027) [2026-01-28 08:58:13] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=3345027) [2026-01-28 08:58:13] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3345027) [2026-01-28 08:58:13] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=3345027) [2026-01-28 08:58:13] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3345027) [2026-01-28 08:58:13] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=3345027) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3345027) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.82it/s]
(EngineCore_DP0 pid=3345027) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.81it/s]
(EngineCore_DP0 pid=3345027) 
(EngineCore_DP0 pid=3345027) 2026-01-28 08:58:32,753 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3345027) 2026-01-28 08:58:32,783 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3345027) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 14.09it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 14.08it/s]
(EngineCore_DP0 pid=3345027) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 14.40it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  22%|██▏       | 28/128 [00:00<00:00, 276.13it/s]
Adding requests:  62%|██████▎   | 80/128 [00:00<00:00, 417.88it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 430.00it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   5%|▍         | 6/128 [00:00<00:02, 50.76it/s, est. speed input: 51996.55 toks/s, output: 50.76 toks/s]
Processed prompts:   9%|▉         | 12/128 [00:00<00:02, 38.97it/s, est. speed input: 41350.58 toks/s, output: 40.38 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:03, 36.51it/s, est. speed input: 38992.65 toks/s, output: 38.08 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:03, 35.09it/s, est. speed input: 37734.16 toks/s, output: 36.85 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:00<00:02, 34.37it/s, est. speed input: 37022.29 toks/s, output: 36.15 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:00<00:02, 33.88it/s, est. speed input: 36511.06 toks/s, output: 35.65 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:00<00:02, 33.57it/s, est. speed input: 36144.75 toks/s, output: 35.30 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:01<00:02, 33.38it/s, est. speed input: 35869.98 toks/s, output: 35.03 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:01<00:02, 33.23it/s, est. speed input: 35643.49 toks/s, output: 34.81 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:01<00:02, 33.16it/s, est. speed input: 35468.59 toks/s, output: 34.64 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:01<00:02, 33.06it/s, est. speed input: 35312.37 toks/s, output: 34.48 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:01<00:02, 32.99it/s, est. speed input: 35178.15 toks/s, output: 34.35 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:01<00:02, 32.96it/s, est. speed input: 35068.55 toks/s, output: 34.25 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:01<00:02, 32.95it/s, est. speed input: 34977.14 toks/s, output: 34.16 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:01<00:01, 32.91it/s, est. speed input: 34889.73 toks/s, output: 34.07 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:02<00:01, 32.91it/s, est. speed input: 34818.40 toks/s, output: 34.00 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:02<00:01, 32.91it/s, est. speed input: 34753.97 toks/s, output: 33.94 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:02<00:01, 32.87it/s, est. speed input: 34690.75 toks/s, output: 33.88 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:02<00:01, 32.87it/s, est. speed input: 34638.77 toks/s, output: 33.83 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:02<00:01, 32.86it/s, est. speed input: 34589.64 toks/s, output: 33.78 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:02<00:01, 32.87it/s, est. speed input: 34547.47 toks/s, output: 33.74 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:02<00:01, 32.89it/s, est. speed input: 34510.56 toks/s, output: 33.70 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:02<00:00, 32.88it/s, est. speed input: 34474.71 toks/s, output: 33.67 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:03<00:00, 32.85it/s, est. speed input: 34437.38 toks/s, output: 33.63 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:03<00:00, 32.87it/s, est. speed input: 34409.50 toks/s, output: 33.60 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:03<00:00, 32.90it/s, est. speed input: 34385.04 toks/s, output: 33.58 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:03<00:00, 32.82it/s, est. speed input: 34350.20 toks/s, output: 33.54 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:03<00:00, 32.77it/s, est. speed input: 34318.37 toks/s, output: 33.51 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:03<00:00, 32.75it/s, est. speed input: 34290.09 toks/s, output: 33.49 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:03<00:00, 32.72it/s, est. speed input: 34261.94 toks/s, output: 33.46 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 32.72it/s, est. speed input: 34238.58 toks/s, output: 33.44 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 33.43it/s, est. speed input: 34238.58 toks/s, output: 33.44 toks/s]
[rank0]:[W128 08:58:38.950905545 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 43.1s

测试结果:
  Requests/s:   31.01
  Tokens/s:     31784.72
  Total Reqs:   128
  Elapsed:      4.13s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     31753.72

============================================================
[3/7] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuBLASLt                                        │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 08:58:48 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3346139) WARNING 01-28 08:59:02 [backends.py:609] Failed to read file <frozen os>
Throughput: 61.54 requests/s, 63081.38 total tokens/s, 61.54 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-28 08:58:48] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:58:48] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 08:58:48] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 08:58:48] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:58:48] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:58:48] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:58:48] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:58:48] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:58:48] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 08:58:48] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:58:48] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:58:48] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:58:48] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:58:48] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 08:58:54] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:58:55] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 08:58:55] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 08:58:55] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:58:55] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:58:55] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:58:55] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:58:55] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:58:55] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 08:58:55] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:58:55] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:58:55] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:58:55] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:58:55] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3346139) [2026-01-28 08:58:56] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=3346139) [2026-01-28 08:58:56] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3346139) [2026-01-28 08:58:56] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=3346139) [2026-01-28 08:58:56] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3346139) [2026-01-28 08:58:56] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=3346139) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3346139) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.76it/s]
(EngineCore_DP0 pid=3346139) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.75it/s]
(EngineCore_DP0 pid=3346139) 
(EngineCore_DP0 pid=3346139) 2026-01-28 08:59:14,133 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3346139) 2026-01-28 08:59:14,162 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3346139) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 1/3 [00:00<00:00,  6.00it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00,  9.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00,  8.99it/s]
(EngineCore_DP0 pid=3346139) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 15.87it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 15.86it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  11%|█▏        | 29/256 [00:00<00:00, 287.47it/s]
Adding requests:  32%|███▏      | 82/256 [00:00<00:00, 426.16it/s]
Adding requests:  52%|█████▏    | 132/256 [00:00<00:00, 458.99it/s]
Adding requests:  71%|███████   | 181/256 [00:00<00:00, 470.11it/s]
Adding requests:  91%|█████████ | 233/256 [00:00<00:00, 484.51it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 465.07it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   9%|▊         | 22/256 [00:00<00:01, 173.03it/s, est. speed input: 177210.37 toks/s, output: 173.04 toks/s]
Processed prompts:  16%|█▌        | 40/256 [00:00<00:02, 93.70it/s, est. speed input: 103806.99 toks/s, output: 101.37 toks/s] 
Processed prompts:  20%|██        | 52/256 [00:00<00:02, 82.75it/s, est. speed input: 92992.53 toks/s, output: 90.81 toks/s]  
Processed prompts:  24%|██▍       | 62/256 [00:00<00:02, 77.67it/s, est. speed input: 88065.82 toks/s, output: 86.00 toks/s]
Processed prompts:  28%|██▊       | 71/256 [00:00<00:02, 77.10it/s, est. speed input: 86550.84 toks/s, output: 84.52 toks/s]
Processed prompts:  31%|███       | 79/256 [00:00<00:02, 74.31it/s, est. speed input: 84353.00 toks/s, output: 82.37 toks/s]
Processed prompts:  34%|███▍      | 87/256 [00:01<00:02, 72.25it/s, est. speed input: 82629.42 toks/s, output: 80.69 toks/s]
Processed prompts:  37%|███▋      | 95/256 [00:01<00:02, 70.66it/s, est. speed input: 81209.97 toks/s, output: 79.31 toks/s]
Processed prompts:  40%|████      | 103/256 [00:01<00:02, 69.65it/s, est. speed input: 80090.96 toks/s, output: 78.21 toks/s]
Processed prompts:  43%|████▎     | 110/256 [00:01<00:02, 66.45it/s, est. speed input: 78443.30 toks/s, output: 76.60 toks/s]
Processed prompts:  46%|████▌     | 118/256 [00:01<00:02, 66.65it/s, est. speed input: 77700.94 toks/s, output: 75.88 toks/s]
Processed prompts:  49%|████▉     | 126/256 [00:01<00:01, 66.79it/s, est. speed input: 77060.42 toks/s, output: 75.25 toks/s]
Processed prompts:  52%|█████▏    | 134/256 [00:01<00:01, 66.87it/s, est. speed input: 76502.29 toks/s, output: 74.71 toks/s]
Processed prompts:  55%|█████▌    | 142/256 [00:01<00:01, 66.99it/s, est. speed input: 76029.67 toks/s, output: 74.25 toks/s]
Processed prompts:  59%|█████▊    | 150/256 [00:02<00:01, 67.01it/s, est. speed input: 75596.07 toks/s, output: 73.82 toks/s]
Processed prompts:  62%|██████▏   | 158/256 [00:02<00:01, 67.05it/s, est. speed input: 75217.43 toks/s, output: 73.45 toks/s]
Processed prompts:  65%|██████▍   | 166/256 [00:02<00:01, 67.15it/s, est. speed input: 74892.33 toks/s, output: 73.14 toks/s]
Processed prompts:  68%|██████▊   | 174/256 [00:02<00:01, 67.13it/s, est. speed input: 74582.30 toks/s, output: 72.83 toks/s]
Processed prompts:  71%|███████   | 182/256 [00:02<00:01, 67.12it/s, est. speed input: 74303.07 toks/s, output: 72.56 toks/s]
Processed prompts:  74%|███████▍  | 190/256 [00:02<00:00, 67.14it/s, est. speed input: 74054.06 toks/s, output: 72.32 toks/s]
Processed prompts:  77%|███████▋  | 198/256 [00:02<00:00, 67.13it/s, est. speed input: 73822.51 toks/s, output: 72.09 toks/s]
Processed prompts:  80%|████████  | 206/256 [00:02<00:00, 67.17it/s, est. speed input: 73617.43 toks/s, output: 71.89 toks/s]
Processed prompts:  84%|████████▎ | 214/256 [00:02<00:00, 67.12it/s, est. speed input: 73416.72 toks/s, output: 71.70 toks/s]
Processed prompts:  87%|████████▋ | 222/256 [00:03<00:00, 67.16it/s, est. speed input: 73241.77 toks/s, output: 71.52 toks/s]
Processed prompts:  90%|████████▉ | 230/256 [00:03<00:00, 67.25it/s, est. speed input: 73089.05 toks/s, output: 71.38 toks/s]
Processed prompts:  93%|█████████▎| 238/256 [00:03<00:00, 67.32it/s, est. speed input: 72947.01 toks/s, output: 71.24 toks/s]
Processed prompts:  96%|█████████▌| 246/256 [00:03<00:00, 67.34it/s, est. speed input: 72812.26 toks/s, output: 71.11 toks/s]
Processed prompts:  99%|█████████▉| 254/256 [00:03<00:00, 67.36it/s, est. speed input: 72686.38 toks/s, output: 70.98 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 67.36it/s, est. speed input: 72671.52 toks/s, output: 70.97 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 70.96it/s, est. speed input: 72671.52 toks/s, output: 70.97 toks/s]
[rank0]:[W128 08:59:20.569494514 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 41.5s

测试结果:
  Requests/s:   61.54
  Tokens/s:     63081.38
  Total Reqs:   256
  Elapsed:      4.16s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     63019.83

============================================================
[4/7] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuBLASLt                                        │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 08:59:31 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3347248) WARNING 01-28 08:59:45 [backends.py:609] Failed to read file <frozen os>
Throughput: 85.30 requests/s, 87430.15 total tokens/s, 85.30 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-28 08:59:30] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:59:31] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 08:59:31] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 08:59:31] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:59:31] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:59:31] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:59:31] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:59:31] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:59:31] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 08:59:31] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:59:31] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:59:31] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:59:31] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:59:31] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 08:59:37] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 08:59:37] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 08:59:37] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 08:59:37] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:59:37] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:59:37] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:59:37] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:59:37] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 08:59:37] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 08:59:37] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 08:59:37] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 08:59:37] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 08:59:37] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 08:59:37] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3347248) [2026-01-28 08:59:38] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=3347248) [2026-01-28 08:59:38] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3347248) [2026-01-28 08:59:38] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=3347248) [2026-01-28 08:59:38] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3347248) [2026-01-28 08:59:38] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=3347248) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3347248) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.76it/s]
(EngineCore_DP0 pid=3347248) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.76it/s]
(EngineCore_DP0 pid=3347248) 
(EngineCore_DP0 pid=3347248) 2026-01-28 08:59:56,650 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3347248) 2026-01-28 08:59:56,679 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3347248) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 1/4 [00:00<00:00,  8.91it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 3/4 [00:00<00:00, 12.71it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 12.24it/s]
(EngineCore_DP0 pid=3347248) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00, 15.87it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 16.26it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   6%|▌         | 29/512 [00:00<00:01, 288.72it/s]
Adding requests:  16%|█▌        | 82/512 [00:00<00:01, 425.39it/s]
Adding requests:  26%|██▌       | 133/512 [00:00<00:00, 462.90it/s]
Adding requests:  36%|███▌      | 183/512 [00:00<00:00, 474.80it/s]
Adding requests:  46%|████▌     | 235/512 [00:00<00:00, 489.00it/s]
Adding requests:  56%|█████▌    | 286/512 [00:00<00:00, 494.69it/s]
Adding requests:  66%|██████▌   | 336/512 [00:00<00:00, 491.29it/s]
Adding requests:  76%|███████▌  | 387/512 [00:00<00:00, 494.70it/s]
Adding requests:  86%|████████▌ | 438/512 [00:00<00:00, 496.62it/s]
Adding requests:  96%|█████████▌| 489/512 [00:01<00:00, 496.15it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 481.47it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  13%|█▎        | 66/512 [00:00<00:00, 630.28it/s, est. speed input: 645640.56 toks/s, output: 630.34 toks/s]
Processed prompts:  25%|██▌       | 130/512 [00:00<00:02, 142.94it/s, est. speed input: 165922.48 toks/s, output: 162.03 toks/s]
Processed prompts:  32%|███▏      | 162/512 [00:01<00:02, 122.53it/s, est. speed input: 143826.20 toks/s, output: 140.45 toks/s]
Processed prompts:  36%|███▌      | 184/512 [00:01<00:02, 116.75it/s, est. speed input: 137259.54 toks/s, output: 134.04 toks/s]
Processed prompts:  39%|███▉      | 201/512 [00:01<00:02, 112.34it/s, est. speed input: 133032.92 toks/s, output: 129.91 toks/s]
Processed prompts:  42%|████▏     | 216/512 [00:01<00:02, 105.82it/s, est. speed input: 128450.64 toks/s, output: 125.44 toks/s]
Processed prompts:  45%|████▍     | 229/512 [00:01<00:02, 104.41it/s, est. speed input: 126540.27 toks/s, output: 123.57 toks/s]
Processed prompts:  47%|████▋     | 241/512 [00:01<00:02, 101.29it/s, est. speed input: 124314.08 toks/s, output: 121.40 toks/s]
Processed prompts:  49%|████▉     | 252/512 [00:02<00:02, 96.80it/s, est. speed input: 121872.01 toks/s, output: 119.02 toks/s] 
Processed prompts:  51%|█████     | 262/512 [00:02<00:02, 91.42it/s, est. speed input: 119310.64 toks/s, output: 116.51 toks/s]
Processed prompts:  54%|█████▎    | 274/512 [00:02<00:02, 91.48it/s, est. speed input: 117908.61 toks/s, output: 115.14 toks/s]
Processed prompts:  56%|█████▌    | 286/512 [00:02<00:02, 91.52it/s, est. speed input: 116653.59 toks/s, output: 113.92 toks/s]
Processed prompts:  58%|█████▊    | 298/512 [00:02<00:02, 91.51it/s, est. speed input: 115512.85 toks/s, output: 112.80 toks/s]
Processed prompts:  61%|██████    | 310/512 [00:02<00:02, 91.50it/s, est. speed input: 114478.44 toks/s, output: 111.79 toks/s]
Processed prompts:  63%|██████▎   | 322/512 [00:02<00:02, 91.38it/s, est. speed input: 113517.18 toks/s, output: 110.86 toks/s]
Processed prompts:  65%|██████▌   | 334/512 [00:03<00:01, 91.39it/s, est. speed input: 112657.62 toks/s, output: 110.02 toks/s]
Processed prompts:  68%|██████▊   | 346/512 [00:03<00:01, 92.98it/s, est. speed input: 112133.91 toks/s, output: 109.50 toks/s]
Processed prompts:  70%|██████▉   | 358/512 [00:03<00:01, 92.50it/s, est. speed input: 111393.24 toks/s, output: 108.78 toks/s]
Processed prompts:  72%|███████▏  | 370/512 [00:03<00:01, 92.08it/s, est. speed input: 110696.82 toks/s, output: 108.10 toks/s]
Processed prompts:  75%|███████▍  | 382/512 [00:03<00:01, 91.97it/s, est. speed input: 110078.34 toks/s, output: 107.50 toks/s]
Processed prompts:  77%|███████▋  | 394/512 [00:03<00:01, 91.71it/s, est. speed input: 109479.71 toks/s, output: 106.91 toks/s]
Processed prompts:  79%|███████▉  | 406/512 [00:03<00:01, 91.64it/s, est. speed input: 108935.12 toks/s, output: 106.38 toks/s]
Processed prompts:  82%|████████▏ | 418/512 [00:03<00:01, 91.59it/s, est. speed input: 108428.34 toks/s, output: 105.89 toks/s]
Processed prompts:  84%|████████▍ | 430/512 [00:04<00:00, 91.40it/s, est. speed input: 107934.07 toks/s, output: 105.40 toks/s]
Processed prompts:  86%|████████▋ | 442/512 [00:04<00:00, 91.44it/s, est. speed input: 107491.75 toks/s, output: 104.97 toks/s]
Processed prompts:  89%|████████▊ | 454/512 [00:04<00:00, 93.22it/s, est. speed input: 107279.27 toks/s, output: 104.76 toks/s]
Processed prompts:  91%|█████████ | 466/512 [00:04<00:00, 92.57it/s, est. speed input: 106865.71 toks/s, output: 104.36 toks/s]
Processed prompts:  93%|█████████▎| 478/512 [00:04<00:00, 92.19it/s, est. speed input: 106483.61 toks/s, output: 103.99 toks/s]
Processed prompts:  96%|█████████▌| 490/512 [00:04<00:00, 92.12it/s, est. speed input: 106143.64 toks/s, output: 103.66 toks/s]
Processed prompts:  98%|█████████▊| 502/512 [00:04<00:00, 91.76it/s, est. speed input: 105790.09 toks/s, output: 103.31 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:04<00:00, 91.76it/s, est. speed input: 106192.31 toks/s, output: 103.70 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:04<00:00, 103.70it/s, est. speed input: 106192.31 toks/s, output: 103.70 toks/s]
[rank0]:[W128 09:00:05.033064727 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 44.6s

测试结果:
  Requests/s:   85.30
  Tokens/s:     87430.15
  Total Reqs:   512
  Elapsed:      6.00s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     87344.85

============================================================
[5/7] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuBLASLt                                        │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 09:00:17 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3348388) WARNING 01-28 09:00:32 [backends.py:609] Failed to read file <frozen os>
Throughput: 93.46 requests/s, 95791.63 total tokens/s, 93.46 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-28 09:00:17] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 09:00:17] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:00:17] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:00:17] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:00:17] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:00:17] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:00:17] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:00:17] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:00:17] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:00:17] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:00:17] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:00:17] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:00:17] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:00:17] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 09:00:23] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 09:00:24] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:00:24] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:00:24] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:00:24] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:00:24] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:00:24] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:00:24] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:00:24] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:00:24] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:00:24] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:00:24] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:00:24] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:00:24] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3348388) [2026-01-28 09:00:25] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=3348388) [2026-01-28 09:00:25] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3348388) [2026-01-28 09:00:25] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=3348388) [2026-01-28 09:00:25] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3348388) [2026-01-28 09:00:25] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=3348388) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3348388) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.82it/s]
(EngineCore_DP0 pid=3348388) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.81it/s]
(EngineCore_DP0 pid=3348388) 
(EngineCore_DP0 pid=3348388) 2026-01-28 09:00:43,268 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3348388) 2026-01-28 09:00:43,298 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3348388) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 1/5 [00:00<00:01,  3.22it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00,  5.18it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00,  9.15it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00,  8.04it/s]
(EngineCore_DP0 pid=3348388) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00, 15.87it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 16.55it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 16.44it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   5%|▍         | 50/1024 [00:00<00:01, 489.74it/s]
Adding requests:  10%|▉         | 101/1024 [00:00<00:01, 498.65it/s]
Adding requests:  15%|█▍        | 151/1024 [00:00<00:01, 497.06it/s]
Adding requests:  20%|█▉        | 201/1024 [00:00<00:01, 496.83it/s]
Adding requests:  25%|██▍       | 254/1024 [00:00<00:01, 505.03it/s]
Adding requests:  30%|██▉       | 305/1024 [00:00<00:01, 501.10it/s]
Adding requests:  35%|███▍      | 356/1024 [00:00<00:01, 503.95it/s]
Adding requests:  40%|███▉      | 407/1024 [00:00<00:01, 505.33it/s]
Adding requests:  45%|████▍     | 458/1024 [00:00<00:01, 499.65it/s]
Adding requests:  50%|████▉     | 508/1024 [00:01<00:01, 496.38it/s]
Adding requests:  54%|█████▍    | 558/1024 [00:01<00:00, 496.16it/s]
Adding requests:  59%|█████▉    | 609/1024 [00:01<00:00, 498.75it/s]
Adding requests:  65%|██████▍   | 662/1024 [00:01<00:00, 506.25it/s]
Adding requests:  70%|██████▉   | 716/1024 [00:01<00:00, 513.08it/s]
Adding requests:  75%|███████▌  | 768/1024 [00:01<00:00, 512.18it/s]
Adding requests:  80%|████████  | 820/1024 [00:01<00:00, 503.17it/s]
Adding requests:  85%|████████▌ | 872/1024 [00:01<00:00, 505.84it/s]
Adding requests:  90%|█████████ | 925/1024 [00:01<00:00, 512.09it/s]
Adding requests:  96%|█████████▌| 978/1024 [00:01<00:00, 514.35it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 506.21it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  18%|█▊        | 186/1024 [00:00<00:00, 1031.37it/s, est. speed input: 1056263.78 toks/s, output: 1031.41 toks/s]
Processed prompts:  28%|██▊       | 290/1024 [00:01<00:03, 191.33it/s, est. speed input: 232336.35 toks/s, output: 226.89 toks/s]   
Processed prompts:  33%|███▎      | 339/1024 [00:01<00:04, 157.08it/s, est. speed input: 195360.87 toks/s, output: 190.78 toks/s]
Processed prompts:  36%|███▌      | 370/1024 [00:02<00:04, 140.01it/s, est. speed input: 179226.78 toks/s, output: 175.02 toks/s]
Processed prompts:  38%|███▊      | 393/1024 [00:02<00:04, 139.36it/s, est. speed input: 176254.97 toks/s, output: 172.12 toks/s]
Processed prompts:  40%|████      | 413/1024 [00:02<00:04, 124.06it/s, est. speed input: 166709.41 toks/s, output: 162.80 toks/s]
Processed prompts:  42%|████▏     | 429/1024 [00:02<00:05, 118.39it/s, est. speed input: 162369.01 toks/s, output: 158.56 toks/s]
Processed prompts:  43%|████▎     | 443/1024 [00:02<00:05, 110.69it/s, est. speed input: 157817.12 toks/s, output: 154.12 toks/s]
Processed prompts:  45%|████▍     | 458/1024 [00:03<00:05, 106.72it/s, est. speed input: 154517.85 toks/s, output: 150.89 toks/s]
Processed prompts:  46%|████▋     | 474/1024 [00:03<00:05, 103.68it/s, est. speed input: 151489.70 toks/s, output: 147.94 toks/s]
Processed prompts:  48%|████▊     | 490/1024 [00:03<00:05, 101.21it/s, est. speed input: 148739.60 toks/s, output: 145.25 toks/s]
Processed prompts:  49%|████▉     | 506/1024 [00:03<00:05, 99.44it/s, est. speed input: 146277.99 toks/s, output: 142.85 toks/s] 
Processed prompts:  51%|█████     | 522/1024 [00:03<00:05, 98.10it/s, est. speed input: 144032.48 toks/s, output: 140.66 toks/s]
Processed prompts:  53%|█████▎    | 538/1024 [00:03<00:05, 97.19it/s, est. speed input: 141999.95 toks/s, output: 138.67 toks/s]
Processed prompts:  54%|█████▍    | 554/1024 [00:04<00:04, 96.37it/s, est. speed input: 140100.40 toks/s, output: 136.82 toks/s]
Processed prompts:  56%|█████▌    | 570/1024 [00:04<00:04, 95.96it/s, est. speed input: 138390.03 toks/s, output: 135.15 toks/s]
Processed prompts:  57%|█████▋    | 586/1024 [00:04<00:04, 95.55it/s, est. speed input: 136785.99 toks/s, output: 133.58 toks/s]
Processed prompts:  59%|█████▉    | 602/1024 [00:04<00:04, 95.23it/s, est. speed input: 135298.09 toks/s, output: 132.13 toks/s]
Processed prompts:  60%|██████    | 618/1024 [00:04<00:04, 95.14it/s, est. speed input: 133938.35 toks/s, output: 130.80 toks/s]
Processed prompts:  62%|██████▏   | 634/1024 [00:04<00:04, 95.00it/s, est. speed input: 132660.79 toks/s, output: 129.55 toks/s]
Processed prompts:  63%|██████▎   | 650/1024 [00:05<00:03, 94.98it/s, est. speed input: 131480.40 toks/s, output: 128.40 toks/s]
Processed prompts:  65%|██████▌   | 666/1024 [00:05<00:03, 94.80it/s, est. speed input: 130351.75 toks/s, output: 127.30 toks/s]
Processed prompts:  67%|██████▋   | 682/1024 [00:05<00:03, 94.78it/s, est. speed input: 129308.63 toks/s, output: 126.28 toks/s]
Processed prompts:  68%|██████▊   | 698/1024 [00:05<00:03, 94.70it/s, est. speed input: 128320.74 toks/s, output: 125.31 toks/s]
Processed prompts:  70%|██████▉   | 714/1024 [00:05<00:03, 94.78it/s, est. speed input: 127407.29 toks/s, output: 124.42 toks/s]
Processed prompts:  71%|███████▏  | 730/1024 [00:05<00:03, 94.73it/s, est. speed input: 126533.97 toks/s, output: 123.57 toks/s]
Processed prompts:  73%|███████▎  | 746/1024 [00:06<00:02, 94.67it/s, est. speed input: 125705.43 toks/s, output: 122.76 toks/s]
Processed prompts:  74%|███████▍  | 762/1024 [00:06<00:02, 94.64it/s, est. speed input: 124923.48 toks/s, output: 121.99 toks/s]
Processed prompts:  76%|███████▌  | 778/1024 [00:06<00:02, 94.64it/s, est. speed input: 124185.79 toks/s, output: 121.27 toks/s]
Processed prompts:  78%|███████▊  | 794/1024 [00:06<00:02, 94.71it/s, est. speed input: 123493.07 toks/s, output: 120.60 toks/s]
Processed prompts:  79%|███████▉  | 810/1024 [00:06<00:02, 94.30it/s, est. speed input: 122785.34 toks/s, output: 119.91 toks/s]
Processed prompts:  81%|████████  | 826/1024 [00:06<00:02, 94.18it/s, est. speed input: 122130.42 toks/s, output: 119.27 toks/s]
Processed prompts:  82%|████████▏ | 842/1024 [00:07<00:01, 94.22it/s, est. speed input: 121519.43 toks/s, output: 118.67 toks/s]
Processed prompts:  84%|████████▍ | 858/1024 [00:07<00:01, 94.27it/s, est. speed input: 120938.91 toks/s, output: 118.10 toks/s]
Processed prompts:  85%|████████▌ | 874/1024 [00:07<00:01, 94.26it/s, est. speed input: 120380.87 toks/s, output: 117.56 toks/s]
Processed prompts:  87%|████████▋ | 890/1024 [00:07<00:01, 94.34it/s, est. speed input: 119855.97 toks/s, output: 117.05 toks/s]
Processed prompts:  88%|████████▊ | 906/1024 [00:07<00:01, 94.30it/s, est. speed input: 119345.20 toks/s, output: 116.55 toks/s]
Processed prompts:  90%|█████████ | 922/1024 [00:07<00:01, 94.46it/s, est. speed input: 118872.30 toks/s, output: 116.09 toks/s]
Processed prompts:  92%|█████████▏| 938/1024 [00:08<00:00, 95.92it/s, est. speed input: 118535.29 toks/s, output: 115.76 toks/s]
Processed prompts:  93%|█████████▎| 954/1024 [00:08<00:00, 95.23it/s, est. speed input: 118068.15 toks/s, output: 115.30 toks/s]
Processed prompts:  95%|█████████▍| 970/1024 [00:08<00:00, 94.98it/s, est. speed input: 117638.58 toks/s, output: 114.88 toks/s]
Processed prompts:  96%|█████████▋| 986/1024 [00:08<00:00, 96.35it/s, est. speed input: 117348.84 toks/s, output: 114.60 toks/s]
Processed prompts:  98%|█████████▊| 1002/1024 [00:08<00:00, 95.78it/s, est. speed input: 116951.12 toks/s, output: 114.21 toks/s]
Processed prompts:  99%|█████████▉| 1018/1024 [00:08<00:00, 97.21it/s, est. speed input: 116705.43 toks/s, output: 113.97 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:08<00:00, 97.21it/s, est. speed input: 117388.50 toks/s, output: 114.64 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:08<00:00, 114.63it/s, est. speed input: 117388.50 toks/s, output: 114.64 toks/s]
[rank0]:[W128 09:00:57.191011710 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 52.1s

测试结果:
  Requests/s:   93.46
  Tokens/s:     95791.63
  Total Reqs:   1024
  Elapsed:      10.96s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     95698.17

============================================================
[6/7] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuBLASLt                                        │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 09:01:14 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3349661) WARNING 01-28 09:01:28 [backends.py:609] Failed to read file <frozen os>
Throughput: 97.04 requests/s, 99461.93 total tokens/s, 97.04 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-28 09:01:13] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 09:01:13] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:01:13] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:01:13] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:01:14] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:01:14] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:01:14] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:01:14] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:01:14] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:01:14] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:01:14] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:01:14] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:01:14] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:01:14] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 09:01:20] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 09:01:20] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:01:20] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:01:20] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:01:20] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:01:20] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:01:20] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:01:20] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:01:20] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:01:20] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:01:20] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:01:20] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:01:20] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:01:20] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3349661) [2026-01-28 09:01:21] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=3349661) [2026-01-28 09:01:21] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3349661) [2026-01-28 09:01:21] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=3349661) [2026-01-28 09:01:21] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3349661) [2026-01-28 09:01:21] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=3349661) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3349661) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.83it/s]
(EngineCore_DP0 pid=3349661) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.83it/s]
(EngineCore_DP0 pid=3349661) 
(EngineCore_DP0 pid=3349661) 2026-01-28 09:01:39,529 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3349661) 2026-01-28 09:01:39,559 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3349661) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00, 13.87it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00, 15.34it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:00<00:00, 10.48it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 10.91it/s]
(EngineCore_DP0 pid=3349661) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00, 13.32it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00, 14.17it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 14.09it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 34/2048 [00:00<00:05, 337.52it/s]
Adding requests:   4%|▍         | 86/2048 [00:00<00:04, 442.95it/s]
Adding requests:   7%|▋         | 137/2048 [00:00<00:04, 470.43it/s]
Adding requests:   9%|▉         | 187/2048 [00:00<00:03, 480.39it/s]
Adding requests:  12%|█▏        | 239/2048 [00:00<00:03, 492.31it/s]
Adding requests:  14%|█▍        | 290/2048 [00:00<00:03, 495.49it/s]
Adding requests:  17%|█▋        | 341/2048 [00:00<00:03, 498.67it/s]
Adding requests:  19%|█▉        | 393/2048 [00:00<00:03, 504.77it/s]
Adding requests:  22%|██▏       | 444/2048 [00:00<00:03, 505.97it/s]
Adding requests:  24%|██▍       | 495/2048 [00:01<00:03, 505.97it/s]
Adding requests:  27%|██▋       | 546/2048 [00:01<00:03, 497.85it/s]
Adding requests:  29%|██▉       | 599/2048 [00:01<00:02, 505.79it/s]
Adding requests:  32%|███▏      | 652/2048 [00:01<00:02, 510.29it/s]
Adding requests:  34%|███▍      | 706/2048 [00:01<00:02, 517.38it/s]
Adding requests:  37%|███▋      | 758/2048 [00:01<00:02, 509.10it/s]
Adding requests:  40%|███▉      | 809/2048 [00:01<00:02, 503.33it/s]
Adding requests:  42%|████▏     | 860/2048 [00:01<00:02, 503.37it/s]
Adding requests:  45%|████▍     | 913/2048 [00:01<00:02, 510.14it/s]
Adding requests:  47%|████▋     | 966/2048 [00:01<00:02, 513.22it/s]
Adding requests:  50%|████▉     | 1018/2048 [00:02<00:01, 515.22it/s]
Adding requests:  52%|█████▏    | 1070/2048 [00:02<00:01, 512.67it/s]
Adding requests:  55%|█████▍    | 1122/2048 [00:02<00:01, 509.55it/s]
Adding requests:  57%|█████▋    | 1175/2048 [00:02<00:01, 515.46it/s]
Adding requests:  60%|█████▉    | 1228/2048 [00:02<00:01, 519.66it/s]
Adding requests:  62%|██████▎   | 1280/2048 [00:02<00:01, 513.80it/s]
Adding requests:  65%|██████▌   | 1333/2048 [00:02<00:01, 517.38it/s]
Adding requests:  68%|██████▊   | 1385/2048 [00:02<00:01, 515.95it/s]
Adding requests:  70%|███████   | 1437/2048 [00:02<00:01, 516.35it/s]
Adding requests:  73%|███████▎  | 1491/2048 [00:02<00:01, 521.28it/s]
Adding requests:  75%|███████▌  | 1544/2048 [00:03<00:00, 522.63it/s]
Adding requests:  78%|███████▊  | 1598/2048 [00:03<00:00, 526.27it/s]
Adding requests:  81%|████████  | 1651/2048 [00:03<00:00, 526.57it/s]
Adding requests:  83%|████████▎ | 1704/2048 [00:03<00:00, 521.51it/s]
Adding requests:  86%|████████▌ | 1757/2048 [00:03<00:00, 521.15it/s]
Adding requests:  88%|████████▊ | 1810/2048 [00:03<00:00, 519.42it/s]
Adding requests:  91%|█████████ | 1862/2048 [00:03<00:00, 518.29it/s]
Adding requests:  93%|█████████▎| 1914/2048 [00:03<00:00, 508.52it/s]
Adding requests:  96%|█████████▌| 1966/2048 [00:03<00:00, 510.74it/s]
Adding requests:  99%|█████████▊| 2019/2048 [00:03<00:00, 515.83it/s]
Adding requests: 100%|██████████| 2048/2048 [00:04<00:00, 509.16it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  19%|█▉        | 386/2048 [00:00<00:00, 2014.39it/s, est. speed input: 2063091.96 toks/s, output: 2014.49 toks/s]
Processed prompts:  29%|██▊       | 588/2048 [00:02<00:06, 225.40it/s, est. speed input: 279738.77 toks/s, output: 273.18 toks/s]   
Processed prompts:  33%|███▎      | 678/2048 [00:03<00:07, 171.52it/s, est. speed input: 221258.24 toks/s, output: 216.07 toks/s]
Processed prompts:  36%|███▌      | 731/2048 [00:03<00:08, 157.11it/s, est. speed input: 206061.75 toks/s, output: 201.23 toks/s]
Processed prompts:  38%|███▊      | 768/2048 [00:03<00:08, 149.39it/s, est. speed input: 198583.94 toks/s, output: 193.93 toks/s]
Processed prompts:  39%|███▉      | 796/2048 [00:04<00:09, 136.55it/s, est. speed input: 190058.10 toks/s, output: 185.60 toks/s]
Processed prompts:  40%|███▉      | 817/2048 [00:04<00:09, 135.42it/s, est. speed input: 187860.04 toks/s, output: 183.46 toks/s]
Processed prompts:  41%|████      | 836/2048 [00:04<00:10, 114.87it/s, est. speed input: 178969.37 toks/s, output: 174.77 toks/s]
Processed prompts:  42%|████▏     | 851/2048 [00:04<00:10, 111.24it/s, est. speed input: 176162.82 toks/s, output: 172.03 toks/s]
Processed prompts:  42%|████▏     | 866/2048 [00:05<00:10, 107.74it/s, est. speed input: 173557.47 toks/s, output: 169.49 toks/s]
Processed prompts:  43%|████▎     | 882/2048 [00:05<00:11, 105.51it/s, est. speed input: 171239.95 toks/s, output: 167.23 toks/s]
Processed prompts:  44%|████▍     | 898/2048 [00:05<00:11, 103.44it/s, est. speed input: 169036.55 toks/s, output: 165.07 toks/s]
Processed prompts:  45%|████▍     | 914/2048 [00:05<00:11, 102.05it/s, est. speed input: 167023.93 toks/s, output: 163.11 toks/s]
Processed prompts:  45%|████▌     | 930/2048 [00:05<00:10, 102.28it/s, est. speed input: 165363.18 toks/s, output: 161.49 toks/s]
Processed prompts:  46%|████▌     | 946/2048 [00:05<00:10, 100.86it/s, est. speed input: 163529.80 toks/s, output: 159.70 toks/s]
Processed prompts:  47%|████▋     | 962/2048 [00:06<00:10, 100.01it/s, est. speed input: 161827.20 toks/s, output: 158.03 toks/s]
Processed prompts:  48%|████▊     | 978/2048 [00:06<00:10, 100.98it/s, est. speed input: 160445.39 toks/s, output: 156.68 toks/s]
Processed prompts:  49%|████▊     | 994/2048 [00:06<00:10, 99.73it/s, est. speed input: 158861.86 toks/s, output: 155.14 toks/s] 
Processed prompts:  49%|████▉     | 1010/2048 [00:06<00:10, 99.10it/s, est. speed input: 157391.91 toks/s, output: 153.70 toks/s]
Processed prompts:  50%|█████     | 1026/2048 [00:06<00:10, 98.52it/s, est. speed input: 155976.25 toks/s, output: 152.32 toks/s]
Processed prompts:  51%|█████     | 1042/2048 [00:06<00:10, 97.82it/s, est. speed input: 154591.75 toks/s, output: 150.97 toks/s]
Processed prompts:  52%|█████▏    | 1058/2048 [00:07<00:10, 97.62it/s, est. speed input: 153306.73 toks/s, output: 149.71 toks/s]
Processed prompts:  52%|█████▏    | 1074/2048 [00:07<00:09, 97.67it/s, est. speed input: 152103.77 toks/s, output: 148.54 toks/s]
Processed prompts:  53%|█████▎    | 1090/2048 [00:07<00:09, 97.41it/s, est. speed input: 150919.02 toks/s, output: 147.38 toks/s]
Processed prompts:  54%|█████▍    | 1106/2048 [00:07<00:09, 97.27it/s, est. speed input: 149791.61 toks/s, output: 146.28 toks/s]
Processed prompts:  55%|█████▍    | 1122/2048 [00:07<00:09, 97.18it/s, est. speed input: 148713.46 toks/s, output: 145.23 toks/s]
Processed prompts:  56%|█████▌    | 1138/2048 [00:07<00:09, 97.05it/s, est. speed input: 147674.12 toks/s, output: 144.21 toks/s]
Processed prompts:  56%|█████▋    | 1154/2048 [00:08<00:09, 98.71it/s, est. speed input: 146853.29 toks/s, output: 143.41 toks/s]
Processed prompts:  57%|█████▋    | 1170/2048 [00:08<00:09, 97.54it/s, est. speed input: 145834.19 toks/s, output: 142.42 toks/s]
Processed prompts:  58%|█████▊    | 1186/2048 [00:08<00:08, 97.81it/s, est. speed input: 144961.01 toks/s, output: 141.56 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [00:08<00:08, 97.83it/s, est. speed input: 144104.93 toks/s, output: 140.73 toks/s]
Processed prompts:  59%|█████▉    | 1218/2048 [00:08<00:08, 97.67it/s, est. speed input: 143265.13 toks/s, output: 139.91 toks/s]
Processed prompts:  60%|██████    | 1234/2048 [00:08<00:08, 97.52it/s, est. speed input: 142452.56 toks/s, output: 139.11 toks/s]
Processed prompts:  61%|██████    | 1250/2048 [00:09<00:08, 97.29it/s, est. speed input: 141658.94 toks/s, output: 138.34 toks/s]
Processed prompts:  62%|██████▏   | 1266/2048 [00:09<00:07, 98.71it/s, est. speed input: 141028.34 toks/s, output: 137.72 toks/s]
Processed prompts:  63%|██████▎   | 1282/2048 [00:09<00:07, 98.33it/s, est. speed input: 140305.23 toks/s, output: 137.02 toks/s]
Processed prompts:  63%|██████▎   | 1298/2048 [00:09<00:07, 98.01it/s, est. speed input: 139601.62 toks/s, output: 136.33 toks/s]
Processed prompts:  64%|██████▍   | 1314/2048 [00:09<00:07, 97.64it/s, est. speed input: 138910.23 toks/s, output: 135.65 toks/s]
Processed prompts:  65%|██████▍   | 1330/2048 [00:09<00:07, 97.55it/s, est. speed input: 138255.56 toks/s, output: 135.01 toks/s]
Processed prompts:  66%|██████▌   | 1346/2048 [00:10<00:07, 97.25it/s, est. speed input: 137604.55 toks/s, output: 134.38 toks/s]
Processed prompts:  67%|██████▋   | 1362/2048 [00:10<00:07, 97.17it/s, est. speed input: 136983.70 toks/s, output: 133.77 toks/s]
Processed prompts:  67%|██████▋   | 1378/2048 [00:10<00:06, 97.04it/s, est. speed input: 136377.83 toks/s, output: 133.18 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [00:10<00:06, 97.00it/s, est. speed input: 135794.40 toks/s, output: 132.61 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [00:10<00:06, 96.88it/s, est. speed input: 135222.15 toks/s, output: 132.05 toks/s]
Processed prompts:  70%|██████▉   | 1426/2048 [00:10<00:06, 96.90it/s, est. speed input: 134674.89 toks/s, output: 131.52 toks/s]
Processed prompts:  70%|███████   | 1442/2048 [00:11<00:06, 97.10it/s, est. speed input: 134156.89 toks/s, output: 131.01 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [00:11<00:06, 97.21it/s, est. speed input: 133652.17 toks/s, output: 130.52 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [00:11<00:05, 97.13it/s, est. speed input: 133151.69 toks/s, output: 130.03 toks/s]
Processed prompts:  73%|███████▎  | 1490/2048 [00:11<00:05, 97.07it/s, est. speed input: 132664.87 toks/s, output: 129.56 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [00:11<00:05, 96.84it/s, est. speed input: 132180.16 toks/s, output: 129.08 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [00:11<00:05, 97.02it/s, est. speed input: 131731.16 toks/s, output: 128.64 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [00:11<00:05, 97.27it/s, est. speed input: 131300.78 toks/s, output: 128.22 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [00:12<00:05, 97.01it/s, est. speed input: 130856.18 toks/s, output: 127.79 toks/s]
Processed prompts:  77%|███████▋  | 1570/2048 [00:12<00:04, 96.98it/s, est. speed input: 130432.54 toks/s, output: 127.38 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [00:12<00:04, 98.45it/s, est. speed input: 130106.95 toks/s, output: 127.06 toks/s]
Processed prompts:  78%|███████▊  | 1602/2048 [00:12<00:04, 97.81it/s, est. speed input: 129693.87 toks/s, output: 126.65 toks/s]
Processed prompts:  79%|███████▉  | 1618/2048 [00:12<00:04, 97.71it/s, est. speed input: 129311.09 toks/s, output: 126.28 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [00:12<00:04, 97.48it/s, est. speed input: 128929.48 toks/s, output: 125.91 toks/s]
Processed prompts:  81%|████████  | 1650/2048 [00:13<00:04, 97.25it/s, est. speed input: 128553.10 toks/s, output: 125.54 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [00:13<00:03, 97.12it/s, est. speed input: 128187.63 toks/s, output: 125.18 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [00:13<00:03, 97.00it/s, est. speed input: 127830.21 toks/s, output: 124.83 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [00:13<00:03, 96.71it/s, est. speed input: 127470.04 toks/s, output: 124.48 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [00:13<00:03, 96.83it/s, est. speed input: 127135.46 toks/s, output: 124.16 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [00:13<00:03, 97.09it/s, est. speed input: 126817.88 toks/s, output: 123.85 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [00:14<00:03, 97.02it/s, est. speed input: 126494.94 toks/s, output: 123.53 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [00:14<00:02, 96.82it/s, est. speed input: 126171.61 toks/s, output: 123.21 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [00:14<00:02, 96.73it/s, est. speed input: 125858.73 toks/s, output: 122.91 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [00:14<00:02, 96.77it/s, est. speed input: 125557.30 toks/s, output: 122.61 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [00:14<00:02, 96.90it/s, est. speed input: 125268.10 toks/s, output: 122.33 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [00:14<00:02, 96.90it/s, est. speed input: 124980.72 toks/s, output: 122.05 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [00:15<00:02, 96.82it/s, est. speed input: 124695.69 toks/s, output: 121.77 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [00:15<00:01, 96.81it/s, est. speed input: 124418.92 toks/s, output: 121.50 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [00:15<00:01, 98.32it/s, est. speed input: 124216.82 toks/s, output: 121.31 toks/s]
Processed prompts:  92%|█████████▏| 1890/2048 [00:15<00:01, 97.95it/s, est. speed input: 123955.04 toks/s, output: 121.05 toks/s]
Processed prompts:  93%|█████████▎| 1906/2048 [00:15<00:01, 97.57it/s, est. speed input: 123693.37 toks/s, output: 120.79 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [00:15<00:01, 97.29it/s, est. speed input: 123436.80 toks/s, output: 120.54 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [00:16<00:01, 97.22it/s, est. speed input: 123190.52 toks/s, output: 120.30 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [00:16<00:00, 98.85it/s, est. speed input: 123019.88 toks/s, output: 120.14 toks/s]
Processed prompts:  96%|█████████▌| 1970/2048 [00:16<00:00, 98.31it/s, est. speed input: 122782.82 toks/s, output: 119.90 toks/s]
Processed prompts:  97%|█████████▋| 1986/2048 [00:16<00:00, 97.86it/s, est. speed input: 122547.69 toks/s, output: 119.68 toks/s]
Processed prompts:  98%|█████████▊| 2002/2048 [00:16<00:00, 97.49it/s, est. speed input: 122314.39 toks/s, output: 119.45 toks/s]
Processed prompts:  99%|█████████▊| 2018/2048 [00:16<00:00, 97.17it/s, est. speed input: 122083.54 toks/s, output: 119.22 toks/s]
Processed prompts:  99%|█████████▉| 2034/2048 [00:17<00:00, 99.15it/s, est. speed input: 121944.16 toks/s, output: 119.09 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:17<00:00, 99.15it/s, est. speed input: 122779.23 toks/s, output: 119.90 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:17<00:00, 119.90it/s, est. speed input: 122779.23 toks/s, output: 119.90 toks/s]
[rank0]:[W128 09:02:03.553889641 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 66.4s

测试结果:
  Requests/s:   97.04
  Tokens/s:     99461.93
  Total Reqs:   2048
  Elapsed:      21.11s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     99364.89

============================================================
[7/7] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuBLASLt                                        │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 09:02:28 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3351149) WARNING 01-28 09:02:43 [backends.py:609] Failed to read file <frozen os>
Throughput: 100.00 requests/s, 102501.36 total tokens/s, 100.00 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-28 09:02:28] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 09:02:28] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:02:28] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:02:28] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:02:28] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:02:28] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:02:28] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:02:28] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:02:28] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:02:28] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:02:28] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:02:28] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:02:28] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:02:28] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 09:02:35] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 09:02:35] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:02:35] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:02:35] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:02:35] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:02:35] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:02:35] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:02:35] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:02:35] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:02:35] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:02:35] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:02:35] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:02:35] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:02:35] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3351149) [2026-01-28 09:02:36] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=3351149) [2026-01-28 09:02:36] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3351149) [2026-01-28 09:02:36] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=3351149) [2026-01-28 09:02:36] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3351149) [2026-01-28 09:02:36] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=3351149) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3351149) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.81it/s]
(EngineCore_DP0 pid=3351149) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.81it/s]
(EngineCore_DP0 pid=3351149) 
(EngineCore_DP0 pid=3351149) [rank0]:W0128 09:02:49.434000 3351149 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3351149) [rank0]:W0128 09:02:49.521000 3351149 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3351149) [rank0]:W0128 09:02:50.673000 3351149 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3351149) [rank0]:W0128 09:02:50.801000 3351149 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3351149) 2026-01-28 09:02:54,912 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3351149) 2026-01-28 09:02:54,949 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3351149) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:00, 13.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:00<00:00, 14.91it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 6/11 [00:00<00:00, 15.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:00<00:00, 15.90it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 10/11 [00:00<00:00, 16.02it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 15.24it/s]
(EngineCore_DP0 pid=3351149) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00, 15.47it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00, 16.28it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00, 10.04it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 11.22it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 34/4096 [00:00<00:11, 339.98it/s]
Adding requests:   2%|▏         | 84/4096 [00:00<00:09, 432.49it/s]
Adding requests:   3%|▎         | 134/4096 [00:00<00:08, 460.16it/s]
Adding requests:   4%|▍         | 183/4096 [00:00<00:08, 469.57it/s]
Adding requests:   6%|▌         | 234/4096 [00:00<00:08, 480.81it/s]
Adding requests:   7%|▋         | 284/4096 [00:00<00:07, 486.87it/s]
Adding requests:   8%|▊         | 333/4096 [00:00<00:07, 486.74it/s]
Adding requests:   9%|▉         | 384/4096 [00:00<00:07, 491.37it/s]
Adding requests:  11%|█         | 434/4096 [00:00<00:07, 492.88it/s]
Adding requests:  12%|█▏        | 485/4096 [00:01<00:07, 496.49it/s]
Adding requests:  13%|█▎        | 535/4096 [00:01<00:07, 484.58it/s]
Adding requests:  14%|█▍        | 588/4096 [00:01<00:07, 495.41it/s]
Adding requests:  16%|█▌        | 639/4096 [00:01<00:06, 498.82it/s]
Adding requests:  17%|█▋        | 691/4096 [00:01<00:06, 504.54it/s]
Adding requests:  18%|█▊        | 742/4096 [00:01<00:06, 504.14it/s]
Adding requests:  19%|█▉        | 793/4096 [00:01<00:06, 503.48it/s]
Adding requests:  21%|██        | 844/4096 [00:01<00:06, 493.91it/s]
Adding requests:  22%|██▏       | 897/4096 [00:01<00:06, 502.32it/s]
Adding requests:  23%|██▎       | 948/4096 [00:01<00:06, 504.30it/s]
Adding requests:  24%|██▍       | 999/4096 [00:02<00:06, 505.10it/s]
Adding requests:  26%|██▌       | 1052/4096 [00:02<00:05, 509.45it/s]
Adding requests:  27%|██▋       | 1103/4096 [00:02<00:06, 493.97it/s]
Adding requests:  28%|██▊       | 1153/4096 [00:02<00:05, 494.63it/s]
Adding requests:  29%|██▉       | 1207/4096 [00:02<00:05, 507.45it/s]
Adding requests:  31%|███       | 1258/4096 [00:02<00:05, 503.81it/s]
Adding requests:  32%|███▏      | 1309/4096 [00:02<00:05, 504.98it/s]
Adding requests:  33%|███▎      | 1362/4096 [00:02<00:05, 509.93it/s]
Adding requests:  35%|███▍      | 1415/4096 [00:02<00:05, 513.87it/s]
Adding requests:  36%|███▌      | 1467/4096 [00:02<00:05, 513.02it/s]
Adding requests:  37%|███▋      | 1519/4096 [00:03<00:05, 514.80it/s]
Adding requests:  38%|███▊      | 1571/4096 [00:03<00:04, 514.98it/s]
Adding requests:  40%|███▉      | 1624/4096 [00:03<00:04, 518.87it/s]
Adding requests:  41%|████      | 1676/4096 [00:03<00:04, 512.43it/s]
Adding requests:  42%|████▏     | 1729/4096 [00:03<00:04, 514.74it/s]
Adding requests:  43%|████▎     | 1781/4096 [00:03<00:04, 508.75it/s]
Adding requests:  45%|████▍     | 1834/4096 [00:03<00:04, 512.64it/s]
Adding requests:  46%|████▌     | 1886/4096 [00:03<00:04, 512.49it/s]
Adding requests:  47%|████▋     | 1938/4096 [00:03<00:04, 512.01it/s]
Adding requests:  49%|████▊     | 1990/4096 [00:03<00:04, 510.39it/s]
Adding requests:  50%|████▉     | 2042/4096 [00:04<00:04, 513.01it/s]
Adding requests:  51%|█████     | 2095/4096 [00:04<00:03, 515.97it/s]
Adding requests:  52%|█████▏    | 2147/4096 [00:04<00:03, 501.54it/s]
Adding requests:  54%|█████▎    | 2198/4096 [00:04<00:03, 495.84it/s]
Adding requests:  55%|█████▍    | 2250/4096 [00:04<00:03, 501.86it/s]
Adding requests:  56%|█████▌    | 2301/4096 [00:04<00:03, 490.48it/s]
Adding requests:  57%|█████▋    | 2353/4096 [00:04<00:03, 496.11it/s]
Adding requests:  59%|█████▊    | 2404/4096 [00:04<00:03, 498.02it/s]
Adding requests:  60%|█████▉    | 2455/4096 [00:04<00:03, 501.10it/s]
Adding requests:  61%|██████    | 2506/4096 [00:05<00:03, 501.60it/s]
Adding requests:  62%|██████▏   | 2559/4096 [00:05<00:03, 507.79it/s]
Adding requests:  64%|██████▎   | 2610/4096 [00:05<00:02, 507.02it/s]
Adding requests:  65%|██████▌   | 2663/4096 [00:05<00:02, 512.42it/s]
Adding requests:  66%|██████▋   | 2715/4096 [00:05<00:02, 506.19it/s]
Adding requests:  68%|██████▊   | 2767/4096 [00:05<00:02, 508.09it/s]
Adding requests:  69%|██████▉   | 2818/4096 [00:05<00:02, 503.64it/s]
Adding requests:  70%|███████   | 2870/4096 [00:05<00:02, 506.30it/s]
Adding requests:  71%|███████▏  | 2921/4096 [00:05<00:02, 507.39it/s]
Adding requests:  73%|███████▎  | 2972/4096 [00:05<00:02, 506.07it/s]
Adding requests:  74%|███████▍  | 3024/4096 [00:06<00:02, 507.81it/s]
Adding requests:  75%|███████▌  | 3075/4096 [00:06<00:02, 502.80it/s]
Adding requests:  76%|███████▋  | 3127/4096 [00:06<00:01, 506.87it/s]
Adding requests:  78%|███████▊  | 3178/4096 [00:06<00:01, 505.99it/s]
Adding requests:  79%|███████▉  | 3229/4096 [00:06<00:01, 505.95it/s]
Adding requests:  80%|████████  | 3281/4096 [00:06<00:01, 508.59it/s]
Adding requests:  81%|████████▏ | 3332/4096 [00:06<00:01, 508.19it/s]
Adding requests:  83%|████████▎ | 3384/4096 [00:06<00:01, 511.03it/s]
Adding requests:  84%|████████▍ | 3436/4096 [00:06<00:01, 510.82it/s]
Adding requests:  85%|████████▌ | 3488/4096 [00:06<00:01, 499.67it/s]
Adding requests:  86%|████████▋ | 3539/4096 [00:07<00:01, 501.96it/s]
Adding requests:  88%|████████▊ | 3590/4096 [00:07<00:01, 503.61it/s]
Adding requests:  89%|████████▉ | 3641/4096 [00:07<00:00, 489.50it/s]
Adding requests:  90%|█████████ | 3694/4096 [00:07<00:00, 498.70it/s]
Adding requests:  91%|█████████▏| 3745/4096 [00:07<00:00, 499.76it/s]
Adding requests:  93%|█████████▎| 3800/4096 [00:07<00:00, 511.72it/s]
Adding requests:  94%|█████████▍| 3853/4096 [00:07<00:00, 515.25it/s]
Adding requests:  95%|█████████▌| 3905/4096 [00:07<00:00, 515.05it/s]
Adding requests:  97%|█████████▋| 3957/4096 [00:07<00:00, 514.97it/s]
Adding requests:  98%|█████████▊| 4009/4096 [00:07<00:00, 514.58it/s]
Adding requests:  99%|█████████▉| 4061/4096 [00:08<00:00, 506.10it/s]
Adding requests: 100%|██████████| 4096/4096 [00:08<00:00, 503.06it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  20%|█▉        | 802/4096 [00:00<00:01, 2483.09it/s, est. speed input: 2543021.84 toks/s, output: 2483.18 toks/s]
Processed prompts:  26%|██▌       | 1051/4096 [00:02<00:09, 331.07it/s, est. speed input: 422923.94 toks/s, output: 413.01 toks/s]  
Processed prompts:  28%|██▊       | 1160/4096 [00:03<00:12, 226.72it/s, est. speed input: 311426.80 toks/s, output: 304.13 toks/s]
Processed prompts:  30%|██▉       | 1224/4096 [00:04<00:14, 197.04it/s, est. speed input: 281436.01 toks/s, output: 274.84 toks/s]
Processed prompts:  31%|███       | 1267/4096 [00:04<00:14, 188.80it/s, est. speed input: 272306.24 toks/s, output: 265.92 toks/s]
Processed prompts:  32%|███▏      | 1299/4096 [00:05<00:16, 173.71it/s, est. speed input: 261654.18 toks/s, output: 255.52 toks/s]
Processed prompts:  32%|███▏      | 1324/4096 [00:05<00:17, 154.99it/s, est. speed input: 250909.92 toks/s, output: 245.03 toks/s]
Processed prompts:  33%|███▎      | 1346/4096 [00:05<00:20, 136.15it/s, est. speed input: 240837.77 toks/s, output: 235.19 toks/s]
Processed prompts:  34%|███▎      | 1378/4096 [00:06<00:21, 127.51it/s, est. speed input: 233494.62 toks/s, output: 228.02 toks/s]
Processed prompts:  34%|███▍      | 1410/4096 [00:06<00:22, 120.54it/s, est. speed input: 226909.29 toks/s, output: 221.59 toks/s]
Processed prompts:  35%|███▌      | 1442/4096 [00:06<00:23, 115.10it/s, est. speed input: 220957.60 toks/s, output: 215.78 toks/s]
Processed prompts:  36%|███▌      | 1474/4096 [00:07<00:23, 110.77it/s, est. speed input: 215486.38 toks/s, output: 210.43 toks/s]
Processed prompts:  37%|███▋      | 1506/4096 [00:07<00:24, 107.75it/s, est. speed input: 210549.21 toks/s, output: 205.61 toks/s]
Processed prompts:  38%|███▊      | 1538/4096 [00:07<00:24, 105.58it/s, est. speed input: 206039.52 toks/s, output: 201.21 toks/s]
Processed prompts:  38%|███▊      | 1570/4096 [00:07<00:24, 104.79it/s, est. speed input: 202097.15 toks/s, output: 197.36 toks/s]
Processed prompts:  39%|███▉      | 1602/4096 [00:08<00:24, 103.32it/s, est. speed input: 198226.57 toks/s, output: 193.58 toks/s]
Processed prompts:  40%|███▉      | 1634/4096 [00:08<00:24, 102.38it/s, est. speed input: 194671.15 toks/s, output: 190.11 toks/s]
Processed prompts:  41%|████      | 1666/4096 [00:08<00:23, 101.62it/s, est. speed input: 191346.64 toks/s, output: 186.86 toks/s]
Processed prompts:  41%|████▏     | 1698/4096 [00:09<00:23, 101.03it/s, est. speed input: 188242.53 toks/s, output: 183.83 toks/s]
Processed prompts:  42%|████▏     | 1730/4096 [00:09<00:23, 100.63it/s, est. speed input: 185347.98 toks/s, output: 181.00 toks/s]
Processed prompts:  43%|████▎     | 1762/4096 [00:09<00:23, 100.23it/s, est. speed input: 182620.04 toks/s, output: 178.34 toks/s]
Processed prompts:  44%|████▍     | 1794/4096 [00:10<00:22, 100.11it/s, est. speed input: 180092.83 toks/s, output: 175.87 toks/s]
Processed prompts:  45%|████▍     | 1826/4096 [00:10<00:22, 100.07it/s, est. speed input: 177728.03 toks/s, output: 173.56 toks/s]
Processed prompts:  45%|████▌     | 1858/4096 [00:10<00:22, 100.74it/s, est. speed input: 175623.91 toks/s, output: 171.51 toks/s]
Processed prompts:  46%|████▌     | 1890/4096 [00:11<00:21, 100.40it/s, est. speed input: 173503.62 toks/s, output: 169.44 toks/s]
Processed prompts:  47%|████▋     | 1922/4096 [00:11<00:21, 100.35it/s, est. speed input: 171531.99 toks/s, output: 167.51 toks/s]
Processed prompts:  48%|████▊     | 1954/4096 [00:11<00:21, 101.05it/s, est. speed input: 169778.94 toks/s, output: 165.80 toks/s]
Processed prompts:  48%|████▊     | 1986/4096 [00:12<00:20, 100.69it/s, est. speed input: 167990.42 toks/s, output: 164.05 toks/s]
Processed prompts:  49%|████▉     | 2018/4096 [00:12<00:20, 100.53it/s, est. speed input: 166309.05 toks/s, output: 162.41 toks/s]
Processed prompts:  50%|█████     | 2050/4096 [00:12<00:20, 100.44it/s, est. speed input: 164713.10 toks/s, output: 160.85 toks/s]
Processed prompts:  51%|█████     | 2082/4096 [00:13<00:20, 100.26it/s, est. speed input: 163180.36 toks/s, output: 159.36 toks/s]
Processed prompts:  52%|█████▏    | 2114/4096 [00:13<00:19, 100.18it/s, est. speed input: 161727.43 toks/s, output: 157.94 toks/s]
Processed prompts:  52%|█████▏    | 2146/4096 [00:13<00:19, 100.11it/s, est. speed input: 160340.40 toks/s, output: 156.58 toks/s]
Processed prompts:  53%|█████▎    | 2178/4096 [00:14<00:19, 100.03it/s, est. speed input: 159012.83 toks/s, output: 155.29 toks/s]
Processed prompts:  54%|█████▍    | 2210/4096 [00:14<00:18, 101.62it/s, est. speed input: 157934.99 toks/s, output: 154.23 toks/s]
Processed prompts:  55%|█████▍    | 2242/4096 [00:14<00:18, 101.12it/s, est. speed input: 156720.41 toks/s, output: 153.05 toks/s]
Processed prompts:  56%|█████▌    | 2274/4096 [00:14<00:17, 101.35it/s, est. speed input: 155620.86 toks/s, output: 151.97 toks/s]
Processed prompts:  56%|█████▋    | 2306/4096 [00:15<00:17, 101.03it/s, est. speed input: 154515.96 toks/s, output: 150.89 toks/s]
Processed prompts:  57%|█████▋    | 2338/4096 [00:15<00:17, 101.55it/s, est. speed input: 153532.66 toks/s, output: 149.93 toks/s]
Processed prompts:  58%|█████▊    | 2370/4096 [00:15<00:16, 102.85it/s, est. speed input: 152678.39 toks/s, output: 149.10 toks/s]
Processed prompts:  59%|█████▊    | 2402/4096 [00:16<00:16, 102.12it/s, est. speed input: 151699.87 toks/s, output: 148.14 toks/s]
Processed prompts:  59%|█████▉    | 2434/4096 [00:16<00:16, 101.63it/s, est. speed input: 150760.52 toks/s, output: 147.23 toks/s]
Processed prompts:  60%|██████    | 2466/4096 [00:16<00:16, 101.11it/s, est. speed input: 149840.03 toks/s, output: 146.33 toks/s]
Processed prompts:  61%|██████    | 2498/4096 [00:17<00:15, 101.64it/s, est. speed input: 149034.35 toks/s, output: 145.54 toks/s]
Processed prompts:  62%|██████▏   | 2530/4096 [00:17<00:15, 101.32it/s, est. speed input: 148196.34 toks/s, output: 144.72 toks/s]
Processed prompts:  63%|██████▎   | 2562/4096 [00:17<00:15, 101.72it/s, est. speed input: 147441.45 toks/s, output: 143.99 toks/s]
Processed prompts:  63%|██████▎   | 2594/4096 [00:18<00:14, 101.01it/s, est. speed input: 146630.16 toks/s, output: 143.19 toks/s]
Processed prompts:  64%|██████▍   | 2626/4096 [00:18<00:14, 100.86it/s, est. speed input: 145875.57 toks/s, output: 142.46 toks/s]
Processed prompts:  65%|██████▍   | 2658/4096 [00:18<00:14, 100.60it/s, est. speed input: 145133.51 toks/s, output: 141.73 toks/s]
Processed prompts:  66%|██████▌   | 2690/4096 [00:19<00:14, 100.30it/s, est. speed input: 144407.03 toks/s, output: 141.02 toks/s]
Processed prompts:  66%|██████▋   | 2722/4096 [00:19<00:13, 100.28it/s, est. speed input: 143719.74 toks/s, output: 140.35 toks/s]
Processed prompts:  67%|██████▋   | 2754/4096 [00:19<00:13, 100.28it/s, est. speed input: 143055.10 toks/s, output: 139.70 toks/s]
Processed prompts:  68%|██████▊   | 2786/4096 [00:20<00:13, 100.10it/s, est. speed input: 142398.96 toks/s, output: 139.06 toks/s]
Processed prompts:  69%|██████▉   | 2818/4096 [00:20<00:12, 100.06it/s, est. speed input: 141768.82 toks/s, output: 138.45 toks/s]
Processed prompts:  70%|██████▉   | 2850/4096 [00:20<00:12, 100.03it/s, est. speed input: 141158.54 toks/s, output: 137.85 toks/s]
Processed prompts:  70%|███████   | 2882/4096 [00:20<00:12, 99.94it/s, est. speed input: 140562.34 toks/s, output: 137.27 toks/s] 
Processed prompts:  71%|███████   | 2914/4096 [00:21<00:11, 99.89it/s, est. speed input: 139984.85 toks/s, output: 136.70 toks/s]
Processed prompts:  72%|███████▏  | 2946/4096 [00:21<00:11, 99.93it/s, est. speed input: 139428.96 toks/s, output: 136.16 toks/s]
Processed prompts:  73%|███████▎  | 2978/4096 [00:21<00:11, 100.03it/s, est. speed input: 138894.83 toks/s, output: 135.64 toks/s]
Processed prompts:  73%|███████▎  | 3010/4096 [00:22<00:10, 99.95it/s, est. speed input: 138365.64 toks/s, output: 135.12 toks/s] 
Processed prompts:  74%|███████▍  | 3042/4096 [00:22<00:10, 99.86it/s, est. speed input: 137849.49 toks/s, output: 134.62 toks/s]
Processed prompts:  75%|███████▌  | 3074/4096 [00:22<00:10, 99.86it/s, est. speed input: 137352.28 toks/s, output: 134.13 toks/s]
Processed prompts:  76%|███████▌  | 3106/4096 [00:23<00:09, 99.86it/s, est. speed input: 136867.82 toks/s, output: 133.66 toks/s]
Processed prompts:  77%|███████▋  | 3138/4096 [00:23<00:09, 100.47it/s, est. speed input: 136434.77 toks/s, output: 133.24 toks/s]
Processed prompts:  77%|███████▋  | 3170/4096 [00:23<00:09, 100.26it/s, est. speed input: 135974.50 toks/s, output: 132.79 toks/s]
Processed prompts:  78%|███████▊  | 3202/4096 [00:24<00:08, 100.11it/s, est. speed input: 135526.05 toks/s, output: 132.35 toks/s]
Processed prompts:  79%|███████▉  | 3234/4096 [00:24<00:08, 99.94it/s, est. speed input: 135085.27 toks/s, output: 131.92 toks/s] 
Processed prompts:  80%|███████▉  | 3266/4096 [00:24<00:08, 99.91it/s, est. speed input: 134661.67 toks/s, output: 131.51 toks/s]
Processed prompts:  81%|████████  | 3298/4096 [00:25<00:07, 99.98it/s, est. speed input: 134253.71 toks/s, output: 131.11 toks/s]
Processed prompts:  81%|████████▏ | 3330/4096 [00:25<00:07, 99.83it/s, est. speed input: 133844.97 toks/s, output: 130.71 toks/s]
Processed prompts:  82%|████████▏ | 3362/4096 [00:25<00:07, 99.80it/s, est. speed input: 133450.32 toks/s, output: 130.32 toks/s]
Processed prompts:  83%|████████▎ | 3394/4096 [00:26<00:07, 99.93it/s, est. speed input: 133073.76 toks/s, output: 129.95 toks/s]
Processed prompts:  84%|████████▎ | 3426/4096 [00:26<00:06, 99.78it/s, est. speed input: 132693.15 toks/s, output: 129.58 toks/s]
Processed prompts:  84%|████████▍ | 3458/4096 [00:26<00:06, 99.78it/s, est. speed input: 132327.59 toks/s, output: 129.23 toks/s]
Processed prompts:  85%|████████▌ | 3490/4096 [00:27<00:05, 101.51it/s, est. speed input: 132059.05 toks/s, output: 128.96 toks/s]
Processed prompts:  86%|████████▌ | 3522/4096 [00:27<00:05, 100.79it/s, est. speed input: 131699.15 toks/s, output: 128.61 toks/s]
Processed prompts:  87%|████████▋ | 3554/4096 [00:27<00:05, 100.34it/s, est. speed input: 131350.05 toks/s, output: 128.27 toks/s]
Processed prompts:  88%|████████▊ | 3586/4096 [00:28<00:05, 100.11it/s, est. speed input: 131013.21 toks/s, output: 127.94 toks/s]
Processed prompts:  88%|████████▊ | 3618/4096 [00:28<00:04, 99.90it/s, est. speed input: 130681.60 toks/s, output: 127.62 toks/s] 
Processed prompts:  89%|████████▉ | 3650/4096 [00:28<00:04, 99.61it/s, est. speed input: 130350.20 toks/s, output: 127.29 toks/s]
Processed prompts:  90%|████████▉ | 3682/4096 [00:28<00:04, 99.70it/s, est. speed input: 130040.63 toks/s, output: 126.99 toks/s]
Processed prompts:  91%|█████████ | 3714/4096 [00:29<00:03, 100.71it/s, est. speed input: 129782.04 toks/s, output: 126.74 toks/s]
Processed prompts:  91%|█████████▏| 3746/4096 [00:29<00:03, 100.36it/s, est. speed input: 129479.85 toks/s, output: 126.45 toks/s]
Processed prompts:  92%|█████████▏| 3778/4096 [00:29<00:03, 100.12it/s, est. speed input: 129184.31 toks/s, output: 126.16 toks/s]
Processed prompts:  93%|█████████▎| 3810/4096 [00:30<00:02, 99.90it/s, est. speed input: 128892.77 toks/s, output: 125.87 toks/s] 
Processed prompts:  94%|█████████▍| 3842/4096 [00:30<00:02, 100.37it/s, est. speed input: 128635.45 toks/s, output: 125.62 toks/s]
Processed prompts:  95%|█████████▍| 3874/4096 [00:30<00:02, 100.16it/s, est. speed input: 128359.58 toks/s, output: 125.35 toks/s]
Processed prompts:  95%|█████████▌| 3906/4096 [00:31<00:01, 100.06it/s, est. speed input: 128091.21 toks/s, output: 125.09 toks/s]
Processed prompts:  96%|█████████▌| 3938/4096 [00:31<00:01, 99.69it/s, est. speed input: 127815.38 toks/s, output: 124.82 toks/s] 
Processed prompts:  97%|█████████▋| 3970/4096 [00:31<00:01, 99.63it/s, est. speed input: 127553.39 toks/s, output: 124.56 toks/s]
Processed prompts:  98%|█████████▊| 4002/4096 [00:32<00:00, 99.38it/s, est. speed input: 127288.00 toks/s, output: 124.30 toks/s]
Processed prompts:  98%|█████████▊| 4034/4096 [00:32<00:00, 100.39it/s, est. speed input: 127077.67 toks/s, output: 124.10 toks/s]
Processed prompts:  99%|█████████▉| 4066/4096 [00:32<00:00, 101.56it/s, est. speed input: 126889.01 toks/s, output: 123.91 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:32<00:00, 101.56it/s, est. speed input: 127821.20 toks/s, output: 124.83 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:32<00:00, 124.82it/s, est. speed input: 127821.20 toks/s, output: 124.83 toks/s]
[rank0]:[W128 09:03:39.193495923 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 95.7s

测试结果:
  Requests/s:   100.00
  Tokens/s:     102501.36
  Total Reqs:   4096
  Elapsed:      40.96s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     102401.36


------------------------------------------------------------
  生成 CSV: BitNet-2B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_FP8E4M3_py312_cu129_x86_64/cublaslt/BitNet-2B-FP8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,30.3826,15586.2891,4.2129
1024,1024,1,128,128,31.0095,31784.7250,4.1278
2048,1024,2,256,128,61.5428,63081.3777,4.1597
4096,1024,4,512,128,85.2977,87430.1482,6.0025
8192,1024,8,1024,128,93.4552,95791.6251,10.9571
16384,1024,16,2048,128,97.0360,99461.9291,21.1056
32768,1024,32,4096,128,100.0013,102501.3640,40.9595

------------------------------------------------------------

[INFO] 完成: 7 成功, 0 失败

============================================================
  BitNet-2B-FP8 | cuSPARSELt (2_4) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_4
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_FP8E4M3_py312_cu129_x86_64/cusparselt/2_4

============================================================
[1/7] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 09:03:48 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3352718) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3352718) WARNING 01-28 09:04:03 [backends.py:609] Failed to read file <frozen os>
Throughput: 31.40 requests/s, 16108.37 total tokens/s, 31.40 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-28 09:03:47] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 09:03:48] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:03:48] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:03:48] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:03:48] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:03:48] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:03:48] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:03:48] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:03:48] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:03:48] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:03:48] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:03:48] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:03:48] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:03:48] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 09:03:54] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 09:03:54] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:03:54] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:03:54] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:03:54] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:03:54] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:03:54] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:03:54] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:03:54] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:03:54] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:03:54] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:03:54] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:03:54] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:03:54] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3352718) [2026-01-28 09:03:55] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3352718) [2026-01-28 09:03:55] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3352718) [2026-01-28 09:03:55] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3352718) [2026-01-28 09:03:55] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3352718) [2026-01-28 09:03:55] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3352718) [2026-01-28 09:03:55] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3352718) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3352718) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.90it/s]
(EngineCore_DP0 pid=3352718) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.89it/s]
(EngineCore_DP0 pid=3352718) 
(EngineCore_DP0 pid=3352718) [2026-01-28 09:03:56] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3352718) [2026-01-28 09:03:56] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6144000 bytes
(EngineCore_DP0 pid=3352718) [2026-01-28 09:03:56] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3352718) [2026-01-28 09:03:56] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4096000 bytes
(EngineCore_DP0 pid=3352718) [2026-01-28 09:03:56] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3352718) [2026-01-28 09:03:56] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 22118400 bytes
(EngineCore_DP0 pid=3352718) [2026-01-28 09:03:56] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3352718) [2026-01-28 09:03:56] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 11059200 bytes
(EngineCore_DP0 pid=3352718) 2026-01-28 09:04:14,627 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3352718) 2026-01-28 09:04:14,655 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3352718) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  5.07it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.82it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.97it/s]
(EngineCore_DP0 pid=3352718) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 15.33it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  38%|███▊      | 48/128 [00:00<00:00, 477.11it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 683.56it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:24,  5.10it/s, est. speed input: 2612.35 toks/s, output: 5.10 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:06, 18.42it/s, est. speed input: 8155.06 toks/s, output: 15.93 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:04, 24.68it/s, est. speed input: 10701.31 toks/s, output: 20.90 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:04, 28.25it/s, est. speed input: 12188.48 toks/s, output: 23.80 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:03, 30.37it/s, est. speed input: 13147.01 toks/s, output: 25.68 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:03, 31.66it/s, est. speed input: 13807.94 toks/s, output: 26.97 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:00<00:03, 32.53it/s, est. speed input: 14301.54 toks/s, output: 27.93 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:01<00:02, 33.14it/s, est. speed input: 14686.66 toks/s, output: 28.68 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:01<00:02, 33.60it/s, est. speed input: 14998.77 toks/s, output: 29.29 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:01<00:02, 33.92it/s, est. speed input: 15254.34 toks/s, output: 29.79 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:01<00:02, 34.17it/s, est. speed input: 15469.52 toks/s, output: 30.21 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:01<00:02, 34.39it/s, est. speed input: 15655.90 toks/s, output: 30.58 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:01<00:02, 34.46it/s, est. speed input: 15806.86 toks/s, output: 30.87 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:01<00:02, 34.46it/s, est. speed input: 15931.88 toks/s, output: 31.12 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:01<00:02, 34.41it/s, est. speed input: 16035.98 toks/s, output: 31.32 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:01<00:01, 34.36it/s, est. speed input: 16126.65 toks/s, output: 31.50 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:02<00:01, 34.31it/s, est. speed input: 16204.84 toks/s, output: 31.65 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:02<00:01, 34.27it/s, est. speed input: 16274.69 toks/s, output: 31.79 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:02<00:01, 34.29it/s, est. speed input: 16341.37 toks/s, output: 31.92 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:02<00:01, 34.21it/s, est. speed input: 16394.12 toks/s, output: 32.02 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:02<00:01, 34.23it/s, est. speed input: 16447.57 toks/s, output: 32.12 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:02<00:01, 34.26it/s, est. speed input: 16497.12 toks/s, output: 32.22 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:02<00:01, 34.28it/s, est. speed input: 16543.04 toks/s, output: 32.31 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:02<00:01, 34.32it/s, est. speed input: 16586.68 toks/s, output: 32.40 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:02<00:00, 34.41it/s, est. speed input: 16630.65 toks/s, output: 32.48 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:03<00:00, 34.39it/s, est. speed input: 16666.69 toks/s, output: 32.55 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:03<00:00, 34.40it/s, est. speed input: 16701.01 toks/s, output: 32.62 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:03<00:00, 34.42it/s, est. speed input: 16734.25 toks/s, output: 32.68 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:03<00:00, 34.47it/s, est. speed input: 16766.52 toks/s, output: 32.75 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:03<00:00, 34.46it/s, est. speed input: 16794.68 toks/s, output: 32.80 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:03<00:00, 34.40it/s, est. speed input: 16818.62 toks/s, output: 32.85 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:03<00:00, 34.38it/s, est. speed input: 16842.04 toks/s, output: 32.89 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 34.38it/s, est. speed input: 16859.18 toks/s, output: 32.93 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 32.93it/s, est. speed input: 16859.18 toks/s, output: 32.93 toks/s]
[rank0]:[W128 09:04:21.193668190 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 41.9s

测试结果:
  Requests/s:   31.40
  Tokens/s:     16108.37
  Total Reqs:   128
  Elapsed:      4.08s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     16076.97

============================================================
[2/7] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 09:04:30 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3353867) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3353867) WARNING 01-28 09:04:45 [backends.py:609] Failed to read file <frozen os>
Throughput: 31.81 requests/s, 32606.44 total tokens/s, 31.81 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-28 09:04:29] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 09:04:30] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:04:30] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:04:30] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:04:30] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:04:30] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:04:30] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:04:30] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:04:30] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:04:30] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:04:30] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:04:30] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:04:30] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:04:30] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 09:04:36] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 09:04:37] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:04:37] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:04:37] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:04:37] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:04:37] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:04:37] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:04:37] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:04:37] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:04:37] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:04:37] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:04:37] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:04:37] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:04:37] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3353867) [2026-01-28 09:04:38] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3353867) [2026-01-28 09:04:38] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3353867) [2026-01-28 09:04:38] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3353867) [2026-01-28 09:04:38] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3353867) [2026-01-28 09:04:38] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3353867) [2026-01-28 09:04:38] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3353867) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3353867) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.89it/s]
(EngineCore_DP0 pid=3353867) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.89it/s]
(EngineCore_DP0 pid=3353867) 
(EngineCore_DP0 pid=3353867) [2026-01-28 09:04:39] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3353867) [2026-01-28 09:04:39] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6144000 bytes
(EngineCore_DP0 pid=3353867) [2026-01-28 09:04:39] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3353867) [2026-01-28 09:04:39] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4096000 bytes
(EngineCore_DP0 pid=3353867) [2026-01-28 09:04:39] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3353867) [2026-01-28 09:04:39] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 22118400 bytes
(EngineCore_DP0 pid=3353867) [2026-01-28 09:04:39] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3353867) [2026-01-28 09:04:39] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 11059200 bytes
(EngineCore_DP0 pid=3353867) 2026-01-28 09:04:56,859 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3353867) 2026-01-28 09:04:56,887 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3353867) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  8.19it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  8.18it/s]
(EngineCore_DP0 pid=3353867) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 10.90it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  21%|██        | 27/128 [00:00<00:00, 268.20it/s]
Adding requests:  62%|██████▏   | 79/128 [00:00<00:00, 413.78it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 427.40it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   3%|▎         | 4/128 [00:00<00:03, 36.06it/s, est. speed input: 36928.80 toks/s, output: 36.06 toks/s]
Processed prompts:   6%|▋         | 8/128 [00:00<00:03, 34.97it/s, est. speed input: 35976.98 toks/s, output: 35.13 toks/s]
Processed prompts:   9%|▉         | 12/128 [00:00<00:03, 34.69it/s, est. speed input: 35710.03 toks/s, output: 34.87 toks/s]
Processed prompts:  12%|█▎        | 16/128 [00:00<00:03, 34.45it/s, est. speed input: 35504.14 toks/s, output: 34.67 toks/s]
Processed prompts:  16%|█▌        | 20/128 [00:00<00:03, 34.36it/s, est. speed input: 35407.55 toks/s, output: 34.58 toks/s]
Processed prompts:  19%|█▉        | 24/128 [00:00<00:03, 34.36it/s, est. speed input: 35372.00 toks/s, output: 34.54 toks/s]
Processed prompts:  22%|██▏       | 28/128 [00:00<00:02, 34.28it/s, est. speed input: 35307.05 toks/s, output: 34.48 toks/s]
Processed prompts:  25%|██▌       | 32/128 [00:00<00:02, 34.41it/s, est. speed input: 35335.55 toks/s, output: 34.51 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:01<00:02, 34.46it/s, est. speed input: 35340.70 toks/s, output: 34.51 toks/s]
Processed prompts:  31%|███▏      | 40/128 [00:01<00:02, 34.49it/s, est. speed input: 35347.84 toks/s, output: 34.52 toks/s]
Processed prompts:  34%|███▍      | 44/128 [00:01<00:02, 34.49it/s, est. speed input: 35343.45 toks/s, output: 34.51 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:01<00:02, 34.46it/s, est. speed input: 35333.14 toks/s, output: 34.50 toks/s]
Processed prompts:  41%|████      | 52/128 [00:01<00:02, 34.47it/s, est. speed input: 35332.75 toks/s, output: 34.50 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:01<00:02, 34.44it/s, est. speed input: 35323.47 toks/s, output: 34.49 toks/s]
Processed prompts:  47%|████▋     | 60/128 [00:01<00:01, 34.44it/s, est. speed input: 35318.43 toks/s, output: 34.49 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:01<00:01, 34.37it/s, est. speed input: 35301.02 toks/s, output: 34.47 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:01<00:01, 34.40it/s, est. speed input: 35301.26 toks/s, output: 34.47 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:02<00:01, 34.37it/s, est. speed input: 35290.64 toks/s, output: 34.46 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:02<00:01, 34.26it/s, est. speed input: 35266.23 toks/s, output: 34.44 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:02<00:01, 34.31it/s, est. speed input: 35265.37 toks/s, output: 34.44 toks/s]
Processed prompts:  66%|██████▌   | 84/128 [00:02<00:01, 34.37it/s, est. speed input: 35268.90 toks/s, output: 34.44 toks/s]
Processed prompts:  69%|██████▉   | 88/128 [00:02<00:01, 34.31it/s, est. speed input: 35255.53 toks/s, output: 34.43 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:02<00:01, 34.30it/s, est. speed input: 35249.37 toks/s, output: 34.42 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:02<00:00, 34.34it/s, est. speed input: 35249.68 toks/s, output: 34.42 toks/s]
Processed prompts:  78%|███████▊  | 100/128 [00:02<00:00, 34.35it/s, est. speed input: 35247.38 toks/s, output: 34.42 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:03<00:00, 34.30it/s, est. speed input: 35238.55 toks/s, output: 34.41 toks/s]
Processed prompts:  84%|████████▍ | 108/128 [00:03<00:00, 34.29it/s, est. speed input: 35232.75 toks/s, output: 34.41 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:03<00:00, 34.29it/s, est. speed input: 35229.00 toks/s, output: 34.40 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:03<00:00, 34.29it/s, est. speed input: 35224.43 toks/s, output: 34.40 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:03<00:00, 34.33it/s, est. speed input: 35224.97 toks/s, output: 34.40 toks/s]
Processed prompts:  97%|█████████▋| 124/128 [00:03<00:00, 34.28it/s, est. speed input: 35217.43 toks/s, output: 34.39 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 34.26it/s, est. speed input: 35212.20 toks/s, output: 34.39 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 34.26it/s, est. speed input: 35212.20 toks/s, output: 34.39 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 34.38it/s, est. speed input: 35212.20 toks/s, output: 34.39 toks/s]
[rank0]:[W128 09:05:03.067309671 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 41.9s

测试结果:
  Requests/s:   31.81
  Tokens/s:     32606.44
  Total Reqs:   128
  Elapsed:      4.02s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     32574.63

============================================================
[3/7] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 09:05:12 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3355056) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3355056) WARNING 01-28 09:05:27 [backends.py:609] Failed to read file <frozen os>
Throughput: 60.90 requests/s, 62427.20 total tokens/s, 60.90 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-28 09:05:12] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 09:05:12] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:05:12] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:05:12] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:05:12] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:05:12] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:05:12] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:05:12] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:05:12] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:05:12] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:05:12] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:05:12] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:05:12] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:05:12] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 09:05:19] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 09:05:19] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:05:19] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:05:19] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:05:19] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:05:19] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:05:19] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:05:19] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:05:19] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:05:19] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:05:19] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:05:19] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:05:19] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:05:19] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3355056) [2026-01-28 09:05:20] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3355056) [2026-01-28 09:05:20] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3355056) [2026-01-28 09:05:20] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3355056) [2026-01-28 09:05:20] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3355056) [2026-01-28 09:05:20] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3355056) [2026-01-28 09:05:20] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3355056) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3355056) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.93it/s]
(EngineCore_DP0 pid=3355056) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.93it/s]
(EngineCore_DP0 pid=3355056) 
(EngineCore_DP0 pid=3355056) [2026-01-28 09:05:21] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3355056) [2026-01-28 09:05:21] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6144000 bytes
(EngineCore_DP0 pid=3355056) [2026-01-28 09:05:21] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3355056) [2026-01-28 09:05:21] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4096000 bytes
(EngineCore_DP0 pid=3355056) [2026-01-28 09:05:21] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3355056) [2026-01-28 09:05:21] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 22118400 bytes
(EngineCore_DP0 pid=3355056) [2026-01-28 09:05:21] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3355056) [2026-01-28 09:05:21] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 11059200 bytes
(EngineCore_DP0 pid=3355056) 2026-01-28 09:05:38,394 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3355056) 2026-01-28 09:05:38,422 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3355056) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 1/3 [00:00<00:00,  5.27it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00,  9.17it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00,  8.53it/s]
(EngineCore_DP0 pid=3355056) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 16.20it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 16.18it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  11%|█▏        | 29/256 [00:00<00:00, 286.89it/s]
Adding requests:  32%|███▏      | 81/256 [00:00<00:00, 419.46it/s]
Adding requests:  51%|█████     | 130/256 [00:00<00:00, 450.07it/s]
Adding requests:  70%|██████▉   | 179/256 [00:00<00:00, 464.58it/s]
Adding requests:  90%|████████▉ | 230/256 [00:00<00:00, 479.20it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 459.82it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   6%|▋         | 16/256 [00:00<00:01, 129.34it/s, est. speed input: 132465.15 toks/s, output: 129.35 toks/s]
Processed prompts:  11%|█▏        | 29/256 [00:00<00:02, 93.11it/s, est. speed input: 99988.08 toks/s, output: 97.64 toks/s]   
Processed prompts:  15%|█▌        | 39/256 [00:00<00:02, 82.61it/s, est. speed input: 90521.02 toks/s, output: 88.40 toks/s]
Processed prompts:  19%|█▉        | 48/256 [00:00<00:02, 74.97it/s, est. speed input: 84094.95 toks/s, output: 82.12 toks/s]
Processed prompts:  22%|██▏       | 56/256 [00:00<00:02, 73.39it/s, est. speed input: 82027.92 toks/s, output: 80.10 toks/s]
Processed prompts:  25%|██▌       | 64/256 [00:00<00:02, 72.33it/s, est. speed input: 80560.60 toks/s, output: 78.67 toks/s]
Processed prompts:  28%|██▊       | 72/256 [00:00<00:02, 71.48it/s, est. speed input: 79398.02 toks/s, output: 77.53 toks/s]
Processed prompts:  31%|███▏      | 80/256 [00:01<00:02, 70.82it/s, est. speed input: 78468.17 toks/s, output: 76.63 toks/s]
Processed prompts:  34%|███▍      | 88/256 [00:01<00:02, 70.32it/s, est. speed input: 77704.44 toks/s, output: 75.88 toks/s]
Processed prompts:  38%|███▊      | 96/256 [00:01<00:02, 69.65it/s, est. speed input: 76973.97 toks/s, output: 75.17 toks/s]
Processed prompts:  41%|████      | 104/256 [00:01<00:02, 68.98it/s, est. speed input: 76301.05 toks/s, output: 74.51 toks/s]
Processed prompts:  44%|████▍     | 112/256 [00:01<00:02, 68.49it/s, est. speed input: 75730.27 toks/s, output: 73.95 toks/s]
Processed prompts:  47%|████▋     | 120/256 [00:01<00:01, 68.25it/s, est. speed input: 75264.92 toks/s, output: 73.50 toks/s]
Processed prompts:  50%|█████     | 128/256 [00:01<00:01, 68.09it/s, est. speed input: 74865.70 toks/s, output: 73.11 toks/s]
Processed prompts:  53%|█████▎    | 136/256 [00:01<00:01, 67.96it/s, est. speed input: 74513.54 toks/s, output: 72.77 toks/s]
Processed prompts:  56%|█████▋    | 144/256 [00:01<00:01, 67.88it/s, est. speed input: 74203.49 toks/s, output: 72.46 toks/s]
Processed prompts:  59%|█████▉    | 152/256 [00:02<00:01, 67.80it/s, est. speed input: 73924.42 toks/s, output: 72.19 toks/s]
Processed prompts:  62%|██████▎   | 160/256 [00:02<00:01, 67.89it/s, est. speed input: 73702.80 toks/s, output: 71.97 toks/s]
Processed prompts:  66%|██████▌   | 168/256 [00:02<00:01, 67.93it/s, est. speed input: 73498.69 toks/s, output: 71.78 toks/s]
Processed prompts:  69%|██████▉   | 176/256 [00:02<00:01, 67.86it/s, est. speed input: 73299.01 toks/s, output: 71.58 toks/s]
Processed prompts:  72%|███████▏  | 184/256 [00:02<00:01, 67.90it/s, est. speed input: 73130.80 toks/s, output: 71.42 toks/s]
Processed prompts:  75%|███████▌  | 192/256 [00:02<00:00, 67.88it/s, est. speed input: 72971.24 toks/s, output: 71.26 toks/s]
Processed prompts:  78%|███████▊  | 200/256 [00:02<00:00, 67.85it/s, est. speed input: 72821.09 toks/s, output: 71.11 toks/s]
Processed prompts:  81%|████████▏ | 208/256 [00:02<00:00, 67.82it/s, est. speed input: 72681.48 toks/s, output: 70.98 toks/s]
Processed prompts:  84%|████████▍ | 216/256 [00:03<00:00, 67.56it/s, est. speed input: 72520.67 toks/s, output: 70.82 toks/s]
Processed prompts:  88%|████████▊ | 224/256 [00:03<00:00, 67.52it/s, est. speed input: 72390.26 toks/s, output: 70.69 toks/s]
Processed prompts:  91%|█████████ | 232/256 [00:03<00:00, 67.46it/s, est. speed input: 72265.12 toks/s, output: 70.57 toks/s]
Processed prompts:  94%|█████████▍| 240/256 [00:03<00:00, 67.39it/s, est. speed input: 72145.13 toks/s, output: 70.45 toks/s]
Processed prompts:  97%|█████████▋| 248/256 [00:03<00:00, 67.32it/s, est. speed input: 72031.08 toks/s, output: 70.34 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 67.26it/s, est. speed input: 71922.90 toks/s, output: 70.24 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 67.26it/s, est. speed input: 71922.90 toks/s, output: 70.24 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 70.23it/s, est. speed input: 71922.90 toks/s, output: 70.24 toks/s]
[rank0]:[W128 09:05:44.916185461 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 41.9s

测试结果:
  Requests/s:   60.90
  Tokens/s:     62427.20
  Total Reqs:   256
  Elapsed:      4.20s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     62366.30

============================================================
[4/7] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 09:05:55 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3356148) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3356148) WARNING 01-28 09:06:10 [backends.py:609] Failed to read file <frozen os>
Throughput: 94.03 requests/s, 96384.34 total tokens/s, 94.03 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-28 09:05:55] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 09:05:55] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:05:55] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:05:55] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:05:55] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:05:55] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:05:55] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:05:55] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:05:55] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:05:55] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:05:55] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:05:55] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:05:55] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:05:55] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 09:06:02] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 09:06:02] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:06:02] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:06:02] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:06:02] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:06:02] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:06:02] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:06:02] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:06:02] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:06:02] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:06:02] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:06:02] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:06:02] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:06:02] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3356148) [2026-01-28 09:06:03] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3356148) [2026-01-28 09:06:03] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3356148) [2026-01-28 09:06:03] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3356148) [2026-01-28 09:06:03] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3356148) [2026-01-28 09:06:03] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3356148) [2026-01-28 09:06:03] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3356148) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3356148) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.90it/s]
(EngineCore_DP0 pid=3356148) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.90it/s]
(EngineCore_DP0 pid=3356148) 
(EngineCore_DP0 pid=3356148) [2026-01-28 09:06:04] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3356148) [2026-01-28 09:06:04] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6144000 bytes
(EngineCore_DP0 pid=3356148) [2026-01-28 09:06:04] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3356148) [2026-01-28 09:06:04] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4096000 bytes
(EngineCore_DP0 pid=3356148) [2026-01-28 09:06:04] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3356148) [2026-01-28 09:06:04] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 22118400 bytes
(EngineCore_DP0 pid=3356148) [2026-01-28 09:06:04] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3356148) [2026-01-28 09:06:04] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 11059200 bytes
(EngineCore_DP0 pid=3356148) 2026-01-28 09:06:21,636 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3356148) 2026-01-28 09:06:21,663 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3356148) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 1/4 [00:00<00:00,  9.15it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 3/4 [00:00<00:00, 13.96it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 13.12it/s]
(EngineCore_DP0 pid=3356148) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00, 16.41it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 14.42it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   5%|▌         | 27/512 [00:00<00:01, 267.01it/s]
Adding requests:  15%|█▌        | 79/512 [00:00<00:01, 414.73it/s]
Adding requests:  25%|██▌       | 130/512 [00:00<00:00, 456.80it/s]
Adding requests:  35%|███▍      | 179/512 [00:00<00:00, 469.48it/s]
Adding requests:  45%|████▌     | 231/512 [00:00<00:00, 486.45it/s]
Adding requests:  55%|█████▌    | 282/512 [00:00<00:00, 493.25it/s]
Adding requests:  65%|██████▍   | 332/512 [00:00<00:00, 494.75it/s]
Adding requests:  75%|███████▍  | 383/512 [00:00<00:00, 499.48it/s]
Adding requests:  85%|████████▍ | 434/512 [00:00<00:00, 501.31it/s]
Adding requests:  95%|█████████▍| 485/512 [00:01<00:00, 502.01it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 482.04it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  14%|█▎        | 70/512 [00:00<00:00, 556.69it/s, est. speed input: 570136.33 toks/s, output: 556.71 toks/s]
Processed prompts:  25%|██▍       | 126/512 [00:00<00:02, 166.48it/s, est. speed input: 193035.68 toks/s, output: 188.50 toks/s]
Processed prompts:  30%|███       | 155/512 [00:00<00:02, 143.49it/s, est. speed input: 168626.13 toks/s, output: 164.67 toks/s]
Processed prompts:  34%|███▍      | 176/512 [00:01<00:02, 133.65it/s, est. speed input: 158757.02 toks/s, output: 155.03 toks/s]
Processed prompts:  38%|███▊      | 193/512 [00:01<00:02, 127.80it/s, est. speed input: 153145.62 toks/s, output: 149.55 toks/s]
Processed prompts:  41%|████      | 208/512 [00:01<00:02, 119.78it/s, est. speed input: 147324.15 toks/s, output: 143.87 toks/s]
Processed prompts:  43%|████▎     | 221/512 [00:01<00:02, 118.18it/s, est. speed input: 144932.42 toks/s, output: 141.53 toks/s]
Processed prompts:  46%|████▌     | 234/512 [00:01<00:02, 108.19it/s, est. speed input: 139604.62 toks/s, output: 136.33 toks/s]
Processed prompts:  48%|████▊     | 246/512 [00:01<00:02, 107.06it/s, est. speed input: 137469.64 toks/s, output: 134.25 toks/s]
Processed prompts:  50%|█████     | 258/512 [00:01<00:02, 106.05it/s, est. speed input: 135565.57 toks/s, output: 132.39 toks/s]
Processed prompts:  53%|█████▎    | 270/512 [00:02<00:02, 105.25it/s, est. speed input: 133871.55 toks/s, output: 130.73 toks/s]
Processed prompts:  55%|█████▌    | 282/512 [00:02<00:02, 104.76it/s, est. speed input: 132385.33 toks/s, output: 129.28 toks/s]
Processed prompts:  57%|█████▋    | 294/512 [00:02<00:02, 104.39it/s, est. speed input: 131051.02 toks/s, output: 127.98 toks/s]
Processed prompts:  60%|█████▉    | 306/512 [00:02<00:01, 103.90it/s, est. speed input: 129796.60 toks/s, output: 126.75 toks/s]
Processed prompts:  62%|██████▏   | 318/512 [00:02<00:01, 103.64it/s, est. speed input: 128677.30 toks/s, output: 125.66 toks/s]
Processed prompts:  64%|██████▍   | 330/512 [00:02<00:01, 103.40it/s, est. speed input: 127647.24 toks/s, output: 124.65 toks/s]
Processed prompts:  67%|██████▋   | 342/512 [00:02<00:01, 103.99it/s, est. speed input: 126835.44 toks/s, output: 123.86 toks/s]
Processed prompts:  69%|██████▉   | 354/512 [00:02<00:01, 103.87it/s, est. speed input: 126001.16 toks/s, output: 123.04 toks/s]
Processed prompts:  71%|███████▏  | 366/512 [00:02<00:01, 103.63it/s, est. speed input: 125203.70 toks/s, output: 122.27 toks/s]
Processed prompts:  74%|███████▍  | 378/512 [00:03<00:01, 103.44it/s, est. speed input: 124464.42 toks/s, output: 121.55 toks/s]
Processed prompts:  76%|███████▌  | 390/512 [00:03<00:01, 103.13it/s, est. speed input: 123753.12 toks/s, output: 120.85 toks/s]
Processed prompts:  79%|███████▊  | 402/512 [00:03<00:01, 102.98it/s, est. speed input: 123100.51 toks/s, output: 120.21 toks/s]
Processed prompts:  81%|████████  | 414/512 [00:03<00:00, 103.17it/s, est. speed input: 122531.91 toks/s, output: 119.66 toks/s]
Processed prompts:  83%|████████▎ | 426/512 [00:03<00:00, 103.16it/s, est. speed input: 121981.07 toks/s, output: 119.12 toks/s]
Processed prompts:  86%|████████▌ | 438/512 [00:03<00:00, 103.09it/s, est. speed input: 121457.28 toks/s, output: 118.61 toks/s]
Processed prompts:  88%|████████▊ | 450/512 [00:03<00:00, 104.22it/s, est. speed input: 121105.39 toks/s, output: 118.27 toks/s]
Processed prompts:  90%|█████████ | 462/512 [00:03<00:00, 104.06it/s, est. speed input: 120664.64 toks/s, output: 117.84 toks/s]
Processed prompts:  93%|█████████▎| 474/512 [00:04<00:00, 103.83it/s, est. speed input: 120236.24 toks/s, output: 117.42 toks/s]
Processed prompts:  95%|█████████▍| 486/512 [00:04<00:00, 103.62it/s, est. speed input: 119826.23 toks/s, output: 117.02 toks/s]
Processed prompts:  97%|█████████▋| 498/512 [00:04<00:00, 103.44it/s, est. speed input: 119435.43 toks/s, output: 116.64 toks/s]
Processed prompts: 100%|█████████▉| 510/512 [00:04<00:00, 104.86it/s, est. speed input: 119220.60 toks/s, output: 116.43 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:04<00:00, 104.86it/s, est. speed input: 119679.79 toks/s, output: 116.87 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:04<00:00, 116.87it/s, est. speed input: 119679.79 toks/s, output: 116.87 toks/s]
[rank0]:[W128 09:06:29.441439409 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 44.5s

测试结果:
  Requests/s:   94.03
  Tokens/s:     96384.34
  Total Reqs:   512
  Elapsed:      5.44s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     96290.31

============================================================
[5/7] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 09:06:42 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3357316) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3357316) WARNING 01-28 09:06:57 [backends.py:609] Failed to read file <frozen os>
Throughput: 104.90 requests/s, 107517.88 total tokens/s, 104.90 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-28 09:06:41] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 09:06:42] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:06:42] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:06:42] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:06:42] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:06:42] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:06:42] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:06:42] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:06:42] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:06:42] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:06:42] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:06:42] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:06:42] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:06:42] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 09:06:48] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 09:06:48] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:06:48] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:06:48] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:06:48] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:06:48] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:06:48] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:06:48] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:06:48] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:06:48] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:06:48] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:06:48] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:06:48] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:06:48] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3357316) [2026-01-28 09:06:50] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3357316) [2026-01-28 09:06:50] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3357316) [2026-01-28 09:06:50] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3357316) [2026-01-28 09:06:50] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3357316) [2026-01-28 09:06:50] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3357316) [2026-01-28 09:06:50] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3357316) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3357316) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.90it/s]
(EngineCore_DP0 pid=3357316) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.90it/s]
(EngineCore_DP0 pid=3357316) 
(EngineCore_DP0 pid=3357316) [2026-01-28 09:06:50] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3357316) [2026-01-28 09:06:50] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6144000 bytes
(EngineCore_DP0 pid=3357316) [2026-01-28 09:06:50] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3357316) [2026-01-28 09:06:50] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4096000 bytes
(EngineCore_DP0 pid=3357316) [2026-01-28 09:06:50] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3357316) [2026-01-28 09:06:50] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 22118400 bytes
(EngineCore_DP0 pid=3357316) [2026-01-28 09:06:50] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3357316) [2026-01-28 09:06:50] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 11059200 bytes
(EngineCore_DP0 pid=3357316) 2026-01-28 09:07:08,387 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3357316) 2026-01-28 09:07:08,415 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3357316) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 1/5 [00:00<00:00,  4.91it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 3/5 [00:00<00:00, 10.18it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 12.05it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 10.79it/s]
(EngineCore_DP0 pid=3357316) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00, 15.11it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 12.68it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 12.99it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   3%|▎         | 29/1024 [00:00<00:03, 289.76it/s]
Adding requests:   8%|▊         | 81/1024 [00:00<00:02, 423.99it/s]
Adding requests:  13%|█▎        | 132/1024 [00:00<00:01, 462.15it/s]
Adding requests:  18%|█▊        | 181/1024 [00:00<00:01, 471.05it/s]
Adding requests:  23%|██▎       | 232/1024 [00:00<00:01, 483.28it/s]
Adding requests:  27%|██▋       | 281/1024 [00:00<00:01, 480.46it/s]
Adding requests:  32%|███▏      | 331/1024 [00:00<00:01, 485.29it/s]
Adding requests:  37%|███▋      | 383/1024 [00:00<00:01, 493.32it/s]
Adding requests:  42%|████▏     | 435/1024 [00:00<00:01, 498.99it/s]
Adding requests:  48%|████▊     | 487/1024 [00:01<00:01, 502.40it/s]
Adding requests:  53%|█████▎    | 538/1024 [00:01<00:00, 488.07it/s]
Adding requests:  58%|█████▊    | 592/1024 [00:01<00:00, 502.13it/s]
Adding requests:  63%|██████▎   | 644/1024 [00:01<00:00, 506.33it/s]
Adding requests:  68%|██████▊   | 698/1024 [00:01<00:00, 515.09it/s]
Adding requests:  73%|███████▎  | 750/1024 [00:01<00:00, 513.46it/s]
Adding requests:  78%|███████▊  | 802/1024 [00:01<00:00, 512.65it/s]
Adding requests:  83%|████████▎ | 854/1024 [00:01<00:00, 507.11it/s]
Adding requests:  89%|████████▊ | 907/1024 [00:01<00:00, 513.31it/s]
Adding requests:  94%|█████████▍| 960/1024 [00:01<00:00, 515.58it/s]
Adding requests:  99%|█████████▉| 1013/1024 [00:02<00:00, 518.51it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 497.32it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  20%|█▉        | 202/1024 [00:00<00:00, 1307.29it/s, est. speed input: 1339022.37 toks/s, output: 1307.38 toks/s]
Processed prompts:  33%|███▎      | 333/1024 [00:01<00:03, 210.45it/s, est. speed input: 254331.69 toks/s, output: 248.37 toks/s]   
Processed prompts:  38%|███▊      | 394/1024 [00:01<00:03, 168.34it/s, est. speed input: 209078.57 toks/s, output: 204.17 toks/s]
Processed prompts:  42%|████▏     | 432/1024 [00:02<00:03, 159.51it/s, est. speed input: 198733.89 toks/s, output: 194.08 toks/s]
Processed prompts:  45%|████▍     | 460/1024 [00:02<00:03, 144.78it/s, est. speed input: 187295.33 toks/s, output: 182.90 toks/s]
Processed prompts:  47%|████▋     | 482/1024 [00:02<00:04, 135.28it/s, est. speed input: 180269.14 toks/s, output: 176.04 toks/s]
Processed prompts:  49%|████▉     | 500/1024 [00:02<00:03, 132.85it/s, est. speed input: 177324.33 toks/s, output: 173.17 toks/s]
Processed prompts:  50%|█████     | 516/1024 [00:03<00:03, 128.08it/s, est. speed input: 174045.02 toks/s, output: 169.97 toks/s]
Processed prompts:  52%|█████▏    | 531/1024 [00:03<00:04, 122.32it/s, est. speed input: 170752.59 toks/s, output: 166.75 toks/s]
Processed prompts:  53%|█████▎    | 546/1024 [00:03<00:04, 117.46it/s, est. speed input: 167784.84 toks/s, output: 163.85 toks/s]
Processed prompts:  55%|█████▍    | 562/1024 [00:03<00:04, 114.91it/s, est. speed input: 165304.67 toks/s, output: 161.43 toks/s]
Processed prompts:  56%|█████▋    | 578/1024 [00:03<00:03, 113.06it/s, est. speed input: 163064.70 toks/s, output: 159.24 toks/s]
Processed prompts:  58%|█████▊    | 594/1024 [00:03<00:03, 111.51it/s, est. speed input: 160970.81 toks/s, output: 157.20 toks/s]
Processed prompts:  60%|█████▉    | 610/1024 [00:03<00:03, 110.36it/s, est. speed input: 159037.96 toks/s, output: 155.31 toks/s]
Processed prompts:  61%|██████    | 626/1024 [00:04<00:03, 109.50it/s, est. speed input: 157243.30 toks/s, output: 153.56 toks/s]
Processed prompts:  63%|██████▎   | 642/1024 [00:04<00:03, 108.88it/s, est. speed input: 155576.04 toks/s, output: 151.93 toks/s]
Processed prompts:  64%|██████▍   | 658/1024 [00:04<00:03, 108.71it/s, est. speed input: 154065.68 toks/s, output: 150.45 toks/s]
Processed prompts:  66%|██████▌   | 674/1024 [00:04<00:03, 108.16it/s, est. speed input: 152587.46 toks/s, output: 149.01 toks/s]
Processed prompts:  67%|██████▋   | 690/1024 [00:04<00:03, 108.02it/s, est. speed input: 151241.20 toks/s, output: 147.70 toks/s]
Processed prompts:  69%|██████▉   | 706/1024 [00:04<00:02, 107.80it/s, est. speed input: 149961.74 toks/s, output: 146.45 toks/s]
Processed prompts:  71%|███████   | 722/1024 [00:04<00:02, 107.69it/s, est. speed input: 148764.01 toks/s, output: 145.28 toks/s]
Processed prompts:  72%|███████▏  | 738/1024 [00:05<00:02, 107.67it/s, est. speed input: 147643.85 toks/s, output: 144.18 toks/s]
Processed prompts:  74%|███████▎  | 754/1024 [00:05<00:02, 107.70it/s, est. speed input: 146593.21 toks/s, output: 143.16 toks/s]
Processed prompts:  75%|███████▌  | 770/1024 [00:05<00:02, 107.61it/s, est. speed input: 145586.20 toks/s, output: 142.17 toks/s]
Processed prompts:  77%|███████▋  | 786/1024 [00:05<00:02, 107.62it/s, est. speed input: 144641.01 toks/s, output: 141.25 toks/s]
Processed prompts:  78%|███████▊  | 802/1024 [00:05<00:02, 107.69it/s, est. speed input: 143752.29 toks/s, output: 140.38 toks/s]
Processed prompts:  80%|███████▉  | 818/1024 [00:05<00:01, 107.46it/s, est. speed input: 142878.39 toks/s, output: 139.53 toks/s]
Processed prompts:  81%|████████▏ | 834/1024 [00:06<00:01, 107.37it/s, est. speed input: 142055.44 toks/s, output: 138.73 toks/s]
Processed prompts:  83%|████████▎ | 850/1024 [00:06<00:01, 107.33it/s, est. speed input: 141274.91 toks/s, output: 137.96 toks/s]
Processed prompts:  85%|████████▍ | 866/1024 [00:06<00:01, 107.36it/s, est. speed input: 140536.67 toks/s, output: 137.24 toks/s]
Processed prompts:  86%|████████▌ | 882/1024 [00:06<00:01, 107.46it/s, est. speed input: 139840.79 toks/s, output: 136.56 toks/s]
Processed prompts:  88%|████████▊ | 898/1024 [00:06<00:01, 107.37it/s, est. speed input: 139159.82 toks/s, output: 135.90 toks/s]
Processed prompts:  89%|████████▉ | 914/1024 [00:06<00:01, 107.40it/s, est. speed input: 138519.00 toks/s, output: 135.27 toks/s]
Processed prompts:  91%|█████████ | 930/1024 [00:06<00:00, 107.22it/s, est. speed input: 137885.98 toks/s, output: 134.65 toks/s]
Processed prompts:  92%|█████████▏| 946/1024 [00:07<00:00, 108.98it/s, est. speed input: 137448.49 toks/s, output: 134.23 toks/s]
Processed prompts:  94%|█████████▍| 962/1024 [00:07<00:00, 108.45it/s, est. speed input: 136875.69 toks/s, output: 133.67 toks/s]
Processed prompts:  96%|█████████▌| 978/1024 [00:07<00:00, 108.16it/s, est. speed input: 136332.16 toks/s, output: 133.14 toks/s]
Processed prompts:  97%|█████████▋| 994/1024 [00:07<00:00, 109.89it/s, est. speed input: 135968.55 toks/s, output: 132.78 toks/s]
Processed prompts:  99%|█████████▊| 1010/1024 [00:07<00:00, 109.20it/s, est. speed input: 135466.94 toks/s, output: 132.29 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:07<00:00, 109.20it/s, est. speed input: 136160.62 toks/s, output: 132.97 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:07<00:00, 132.97it/s, est. speed input: 136160.62 toks/s, output: 132.97 toks/s]
[rank0]:[W128 09:07:20.790694833 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 51.3s

测试结果:
  Requests/s:   104.90
  Tokens/s:     107517.88
  Total Reqs:   1024
  Elapsed:      9.76s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     107412.98

============================================================
[6/7] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 09:07:37 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3358582) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3358582) WARNING 01-28 09:07:52 [backends.py:609] Failed to read file <frozen os>
Throughput: 110.56 requests/s, 113326.31 total tokens/s, 110.56 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-28 09:07:37] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 09:07:37] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:07:37] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:07:37] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:07:37] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:07:37] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:07:37] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:07:37] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:07:37] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:07:37] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:07:37] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:07:37] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:07:37] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:07:37] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 09:07:44] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 09:07:44] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:07:44] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:07:44] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:07:44] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:07:44] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:07:44] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:07:44] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:07:44] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:07:44] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:07:44] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:07:44] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:07:44] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:07:44] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3358582) [2026-01-28 09:07:45] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3358582) [2026-01-28 09:07:45] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3358582) [2026-01-28 09:07:45] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3358582) [2026-01-28 09:07:45] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3358582) [2026-01-28 09:07:45] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3358582) [2026-01-28 09:07:45] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3358582) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3358582) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.88it/s]
(EngineCore_DP0 pid=3358582) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.88it/s]
(EngineCore_DP0 pid=3358582) 
(EngineCore_DP0 pid=3358582) [2026-01-28 09:07:46] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3358582) [2026-01-28 09:07:46] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6144000 bytes
(EngineCore_DP0 pid=3358582) [2026-01-28 09:07:46] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3358582) [2026-01-28 09:07:46] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4096000 bytes
(EngineCore_DP0 pid=3358582) [2026-01-28 09:07:46] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3358582) [2026-01-28 09:07:46] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 22118400 bytes
(EngineCore_DP0 pid=3358582) [2026-01-28 09:07:46] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3358582) [2026-01-28 09:07:46] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 11059200 bytes
(EngineCore_DP0 pid=3358582) 2026-01-28 09:08:03,860 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3358582) 2026-01-28 09:08:03,889 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3358582) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00, 12.33it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00, 14.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:00<00:00, 10.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 10.98it/s]
(EngineCore_DP0 pid=3358582) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00, 15.06it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00, 16.26it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 16.25it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 33/2048 [00:00<00:06, 329.12it/s]
Adding requests:   4%|▍         | 84/2048 [00:00<00:04, 433.84it/s]
Adding requests:   6%|▋         | 133/2048 [00:00<00:04, 459.35it/s]
Adding requests:   9%|▉         | 181/2048 [00:00<00:04, 464.79it/s]
Adding requests:  11%|█▏        | 232/2048 [00:00<00:03, 478.26it/s]
Adding requests:  14%|█▎        | 281/2048 [00:00<00:03, 481.79it/s]
Adding requests:  16%|█▌        | 330/2048 [00:00<00:03, 480.72it/s]
Adding requests:  19%|█▊        | 380/2048 [00:00<00:03, 484.94it/s]
Adding requests:  21%|██        | 431/2048 [00:00<00:03, 489.87it/s]
Adding requests:  23%|██▎       | 481/2048 [00:01<00:03, 490.81it/s]
Adding requests:  26%|██▌       | 531/2048 [00:01<00:03, 479.28it/s]
Adding requests:  28%|██▊       | 582/2048 [00:01<00:03, 486.59it/s]
Adding requests:  31%|███       | 635/2048 [00:01<00:02, 496.86it/s]
Adding requests:  34%|███▎      | 688/2048 [00:01<00:02, 504.87it/s]
Adding requests:  36%|███▌      | 740/2048 [00:01<00:02, 507.76it/s]
Adding requests:  39%|███▊      | 791/2048 [00:01<00:02, 505.36it/s]
Adding requests:  41%|████      | 842/2048 [00:01<00:02, 491.59it/s]
Adding requests:  44%|████▍     | 896/2048 [00:01<00:02, 503.86it/s]
Adding requests:  46%|████▋     | 948/2048 [00:01<00:02, 507.47it/s]
Adding requests:  49%|████▉     | 1000/2048 [00:02<00:02, 510.04it/s]
Adding requests:  51%|█████▏    | 1053/2048 [00:02<00:01, 513.65it/s]
Adding requests:  54%|█████▍    | 1105/2048 [00:02<00:01, 510.31it/s]
Adding requests:  56%|█████▋    | 1157/2048 [00:02<00:01, 511.02it/s]
Adding requests:  59%|█████▉    | 1211/2048 [00:02<00:01, 519.55it/s]
Adding requests:  62%|██████▏   | 1263/2048 [00:02<00:01, 514.60it/s]
Adding requests:  64%|██████▍   | 1315/2048 [00:02<00:01, 514.51it/s]
Adding requests:  67%|██████▋   | 1367/2048 [00:02<00:01, 514.72it/s]
Adding requests:  69%|██████▉   | 1419/2048 [00:02<00:01, 514.69it/s]
Adding requests:  72%|███████▏  | 1472/2048 [00:02<00:01, 516.96it/s]
Adding requests:  74%|███████▍  | 1525/2048 [00:03<00:01, 519.10it/s]
Adding requests:  77%|███████▋  | 1578/2048 [00:03<00:00, 520.70it/s]
Adding requests:  80%|███████▉  | 1632/2048 [00:03<00:00, 524.52it/s]
Adding requests:  82%|████████▏ | 1685/2048 [00:03<00:00, 518.78it/s]
Adding requests:  85%|████████▍ | 1738/2048 [00:03<00:00, 519.37it/s]
Adding requests:  87%|████████▋ | 1790/2048 [00:03<00:00, 513.80it/s]
Adding requests:  90%|████████▉ | 1842/2048 [00:03<00:00, 514.69it/s]
Adding requests:  92%|█████████▏| 1894/2048 [00:03<00:00, 513.10it/s]
Adding requests:  95%|█████████▌| 1946/2048 [00:03<00:00, 513.43it/s]
Adding requests:  98%|█████████▊| 1998/2048 [00:03<00:00, 503.03it/s]
Adding requests: 100%|██████████| 2048/2048 [00:04<00:00, 502.24it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  22%|██▏       | 450/2048 [00:00<00:00, 2238.77it/s, est. speed input: 2292844.92 toks/s, output: 2238.88 toks/s]
Processed prompts:  33%|███▎      | 674/2048 [00:02<00:05, 249.63it/s, est. speed input: 310960.28 toks/s, output: 303.67 toks/s]   
Processed prompts:  38%|███▊      | 773/2048 [00:03<00:06, 200.52it/s, est. speed input: 256714.35 toks/s, output: 250.70 toks/s]
Processed prompts:  41%|████      | 832/2048 [00:03<00:06, 187.10it/s, est. speed input: 242173.58 toks/s, output: 236.50 toks/s]
Processed prompts:  43%|████▎     | 873/2048 [00:03<00:07, 166.17it/s, est. speed input: 226334.61 toks/s, output: 221.03 toks/s]
Processed prompts:  44%|████▍     | 903/2048 [00:04<00:07, 155.07it/s, est. speed input: 218169.32 toks/s, output: 213.06 toks/s]
Processed prompts:  45%|████▌     | 927/2048 [00:04<00:07, 156.30it/s, est. speed input: 216577.96 toks/s, output: 211.50 toks/s]
Processed prompts:  46%|████▋     | 948/2048 [00:04<00:08, 136.92it/s, est. speed input: 208184.48 toks/s, output: 203.30 toks/s]
Processed prompts:  47%|████▋     | 965/2048 [00:04<00:08, 134.16it/s, est. speed input: 205586.31 toks/s, output: 200.77 toks/s]
Processed prompts:  48%|████▊     | 981/2048 [00:04<00:08, 131.26it/s, est. speed input: 203210.54 toks/s, output: 198.45 toks/s]
Processed prompts:  49%|████▊     | 996/2048 [00:05<00:08, 125.74it/s, est. speed input: 200468.45 toks/s, output: 195.77 toks/s]
Processed prompts:  49%|████▉     | 1010/2048 [00:05<00:08, 119.18it/s, est. speed input: 197654.60 toks/s, output: 193.02 toks/s]
Processed prompts:  50%|█████     | 1026/2048 [00:05<00:08, 117.09it/s, est. speed input: 195382.03 toks/s, output: 190.80 toks/s]
Processed prompts:  51%|█████     | 1042/2048 [00:05<00:08, 115.43it/s, est. speed input: 193233.27 toks/s, output: 188.70 toks/s]
Processed prompts:  52%|█████▏    | 1058/2048 [00:05<00:08, 114.27it/s, est. speed input: 191213.16 toks/s, output: 186.73 toks/s]
Processed prompts:  52%|█████▏    | 1074/2048 [00:05<00:08, 113.05it/s, est. speed input: 189242.03 toks/s, output: 184.81 toks/s]
Processed prompts:  53%|█████▎    | 1090/2048 [00:05<00:08, 112.40it/s, est. speed input: 187402.30 toks/s, output: 183.01 toks/s]
Processed prompts:  54%|█████▍    | 1106/2048 [00:06<00:08, 111.86it/s, est. speed input: 185642.89 toks/s, output: 181.29 toks/s]
Processed prompts:  55%|█████▍    | 1122/2048 [00:06<00:08, 111.71it/s, est. speed input: 183994.72 toks/s, output: 179.68 toks/s]
Processed prompts:  56%|█████▌    | 1138/2048 [00:06<00:08, 111.17it/s, est. speed input: 182365.34 toks/s, output: 178.09 toks/s]
Processed prompts:  56%|█████▋    | 1154/2048 [00:06<00:07, 112.54it/s, est. speed input: 181021.32 toks/s, output: 176.78 toks/s]
Processed prompts:  57%|█████▋    | 1170/2048 [00:06<00:07, 112.03it/s, est. speed input: 179560.46 toks/s, output: 175.35 toks/s]
Processed prompts:  58%|█████▊    | 1186/2048 [00:06<00:07, 111.77it/s, est. speed input: 178172.04 toks/s, output: 174.00 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [00:06<00:07, 111.33it/s, est. speed input: 176813.37 toks/s, output: 172.67 toks/s]
Processed prompts:  59%|█████▉    | 1218/2048 [00:07<00:07, 111.31it/s, est. speed input: 175540.68 toks/s, output: 171.43 toks/s]
Processed prompts:  60%|██████    | 1234/2048 [00:07<00:07, 110.96it/s, est. speed input: 174283.64 toks/s, output: 170.20 toks/s]
Processed prompts:  61%|██████    | 1250/2048 [00:07<00:07, 110.61it/s, est. speed input: 173064.63 toks/s, output: 169.01 toks/s]
Processed prompts:  62%|██████▏   | 1266/2048 [00:07<00:06, 112.58it/s, est. speed input: 172110.57 toks/s, output: 168.08 toks/s]
Processed prompts:  63%|██████▎   | 1282/2048 [00:07<00:06, 112.04it/s, est. speed input: 171006.85 toks/s, output: 167.00 toks/s]
Processed prompts:  63%|██████▎   | 1298/2048 [00:07<00:06, 111.83it/s, est. speed input: 169959.91 toks/s, output: 165.98 toks/s]
Processed prompts:  64%|██████▍   | 1314/2048 [00:07<00:06, 111.72it/s, est. speed input: 168953.79 toks/s, output: 164.99 toks/s]
Processed prompts:  65%|██████▍   | 1330/2048 [00:08<00:06, 111.44it/s, est. speed input: 167964.73 toks/s, output: 164.03 toks/s]
Processed prompts:  66%|██████▌   | 1346/2048 [00:08<00:06, 110.84it/s, est. speed input: 166975.75 toks/s, output: 163.06 toks/s]
Processed prompts:  67%|██████▋   | 1362/2048 [00:08<00:06, 110.64it/s, est. speed input: 166039.62 toks/s, output: 162.15 toks/s]
Processed prompts:  67%|██████▋   | 1378/2048 [00:08<00:06, 110.93it/s, est. speed input: 165170.77 toks/s, output: 161.30 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [00:08<00:05, 110.93it/s, est. speed input: 164314.93 toks/s, output: 160.46 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [00:08<00:05, 110.85it/s, est. speed input: 163480.17 toks/s, output: 159.65 toks/s]
Processed prompts:  70%|██████▉   | 1426/2048 [00:08<00:05, 110.82it/s, est. speed input: 162674.10 toks/s, output: 158.86 toks/s]
Processed prompts:  70%|███████   | 1442/2048 [00:09<00:05, 110.79it/s, est. speed input: 161893.39 toks/s, output: 158.10 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [00:09<00:05, 110.82it/s, est. speed input: 161140.16 toks/s, output: 157.36 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [00:09<00:05, 110.63it/s, est. speed input: 160394.82 toks/s, output: 156.63 toks/s]
Processed prompts:  73%|███████▎  | 1490/2048 [00:09<00:05, 110.57it/s, est. speed input: 159677.77 toks/s, output: 155.93 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [00:09<00:04, 110.54it/s, est. speed input: 158982.73 toks/s, output: 155.26 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [00:09<00:04, 110.82it/s, est. speed input: 158329.47 toks/s, output: 154.62 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [00:09<00:04, 110.79it/s, est. speed input: 157678.36 toks/s, output: 153.98 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [00:10<00:04, 110.68it/s, est. speed input: 157040.66 toks/s, output: 153.36 toks/s]
Processed prompts:  77%|███████▋  | 1570/2048 [00:10<00:04, 110.59it/s, est. speed input: 156419.91 toks/s, output: 152.75 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [00:10<00:04, 112.42it/s, est. speed input: 155938.31 toks/s, output: 152.28 toks/s]
Processed prompts:  78%|███████▊  | 1602/2048 [00:10<00:03, 111.87it/s, est. speed input: 155353.63 toks/s, output: 151.71 toks/s]
Processed prompts:  79%|███████▉  | 1618/2048 [00:10<00:03, 111.51it/s, est. speed input: 154786.17 toks/s, output: 151.16 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [00:10<00:03, 111.22it/s, est. speed input: 154231.00 toks/s, output: 150.62 toks/s]
Processed prompts:  81%|████████  | 1650/2048 [00:10<00:03, 110.88it/s, est. speed input: 153682.56 toks/s, output: 150.08 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [00:11<00:03, 110.85it/s, est. speed input: 153160.83 toks/s, output: 149.57 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [00:11<00:03, 110.96it/s, est. speed input: 152660.04 toks/s, output: 149.08 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [00:11<00:03, 110.81it/s, est. speed input: 152158.60 toks/s, output: 148.59 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [00:11<00:03, 110.71it/s, est. speed input: 151670.10 toks/s, output: 148.11 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [00:11<00:02, 110.55it/s, est. speed input: 151188.53 toks/s, output: 147.64 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [00:11<00:02, 110.63it/s, est. speed input: 150729.53 toks/s, output: 147.20 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [00:12<00:02, 110.75it/s, est. speed input: 150284.99 toks/s, output: 146.76 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [00:12<00:02, 110.74it/s, est. speed input: 149846.26 toks/s, output: 146.33 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [00:12<00:02, 110.70it/s, est. speed input: 149415.65 toks/s, output: 145.91 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [00:12<00:02, 110.58it/s, est. speed input: 148990.12 toks/s, output: 145.50 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [00:12<00:02, 110.63it/s, est. speed input: 148581.67 toks/s, output: 145.10 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [00:12<00:01, 110.40it/s, est. speed input: 148168.92 toks/s, output: 144.70 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [00:12<00:01, 110.27it/s, est. speed input: 147767.32 toks/s, output: 144.30 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [00:13<00:01, 112.26it/s, est. speed input: 147475.65 toks/s, output: 144.02 toks/s]
Processed prompts:  92%|█████████▏| 1890/2048 [00:13<00:01, 111.77it/s, est. speed input: 147099.92 toks/s, output: 143.65 toks/s]
Processed prompts:  93%|█████████▎| 1906/2048 [00:13<00:01, 111.54it/s, est. speed input: 146737.82 toks/s, output: 143.30 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [00:13<00:01, 111.40it/s, est. speed input: 146384.18 toks/s, output: 142.95 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [00:13<00:00, 110.86it/s, est. speed input: 146017.51 toks/s, output: 142.59 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [00:13<00:00, 112.51it/s, est. speed input: 145751.21 toks/s, output: 142.33 toks/s]
Processed prompts:  96%|█████████▌| 1970/2048 [00:13<00:00, 111.77it/s, est. speed input: 145405.00 toks/s, output: 142.00 toks/s]
Processed prompts:  97%|█████████▋| 1986/2048 [00:14<00:00, 111.34it/s, est. speed input: 145069.78 toks/s, output: 141.67 toks/s]
Processed prompts:  98%|█████████▊| 2002/2048 [00:14<00:00, 111.27it/s, est. speed input: 144751.52 toks/s, output: 141.36 toks/s]
Processed prompts:  99%|█████████▊| 2018/2048 [00:14<00:00, 111.09it/s, est. speed input: 144434.09 toks/s, output: 141.05 toks/s]
Processed prompts:  99%|█████████▉| 2034/2048 [00:14<00:00, 113.08it/s, est. speed input: 144212.49 toks/s, output: 140.83 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:14<00:00, 113.08it/s, est. speed input: 145198.90 toks/s, output: 141.80 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:14<00:00, 141.79it/s, est. speed input: 145198.90 toks/s, output: 141.80 toks/s]
[rank0]:[W128 09:08:25.246146069 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 64.6s

测试结果:
  Requests/s:   110.56
  Tokens/s:     113326.31
  Total Reqs:   2048
  Elapsed:      18.52s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     113215.75

============================================================
[7/7] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 09:08:50 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3360053) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3360053) WARNING 01-28 09:09:05 [backends.py:609] Failed to read file <frozen os>
Throughput: 114.94 requests/s, 117818.51 total tokens/s, 114.94 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-28 09:08:50] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 09:08:50] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:08:50] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:08:50] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:08:50] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:08:50] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:08:50] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:08:50] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:08:50] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:08:50] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:08:50] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:08:50] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:08:50] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:08:50] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 09:08:56] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 09:08:57] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:08:57] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:08:57] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:08:57] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:08:57] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:08:57] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:08:57] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:08:57] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:08:57] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:08:57] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:08:57] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:08:57] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:08:57] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3360053) [2026-01-28 09:08:58] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3360053) [2026-01-28 09:08:58] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3360053) [2026-01-28 09:08:58] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3360053) [2026-01-28 09:08:58] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3360053) [2026-01-28 09:08:58] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3360053) [2026-01-28 09:08:58] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3360053) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3360053) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.94it/s]
(EngineCore_DP0 pid=3360053) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.94it/s]
(EngineCore_DP0 pid=3360053) 
(EngineCore_DP0 pid=3360053) [2026-01-28 09:08:59] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3360053) [2026-01-28 09:08:59] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6144000 bytes
(EngineCore_DP0 pid=3360053) [2026-01-28 09:08:59] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3360053) [2026-01-28 09:08:59] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4096000 bytes
(EngineCore_DP0 pid=3360053) [2026-01-28 09:08:59] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3360053) [2026-01-28 09:08:59] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 22118400 bytes
(EngineCore_DP0 pid=3360053) [2026-01-28 09:08:59] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3360053) [2026-01-28 09:08:59] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 11059200 bytes
(EngineCore_DP0 pid=3360053) [rank0]:W0128 09:09:11.252000 3360053 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3360053) [rank0]:W0128 09:09:11.334000 3360053 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3360053) [rank0]:W0128 09:09:12.368000 3360053 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3360053) [rank0]:W0128 09:09:12.495000 3360053 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3360053) 2026-01-28 09:09:16,970 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3360053) 2026-01-28 09:09:17,043 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3360053) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▉         | 1/11 [00:00<00:01,  9.66it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 3/11 [00:00<00:00, 13.65it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 5/11 [00:00<00:00, 15.10it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▎   | 7/11 [00:00<00:00, 15.86it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 9/11 [00:00<00:00, 16.33it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 11.33it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 12.81it/s]
(EngineCore_DP0 pid=3360053) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 1/7 [00:00<00:00,  6.91it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 3/7 [00:00<00:00, 11.10it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 5/7 [00:00<00:00, 13.69it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 15.15it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 13.63it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 33/4096 [00:00<00:12, 326.78it/s]
Adding requests:   2%|▏         | 84/4096 [00:00<00:09, 431.69it/s]
Adding requests:   3%|▎         | 135/4096 [00:00<00:08, 462.96it/s]
Adding requests:   4%|▍         | 184/4096 [00:00<00:08, 471.67it/s]
Adding requests:   6%|▌         | 235/4096 [00:00<00:07, 484.19it/s]
Adding requests:   7%|▋         | 284/4096 [00:00<00:07, 482.68it/s]
Adding requests:   8%|▊         | 334/4096 [00:00<00:07, 486.54it/s]
Adding requests:   9%|▉         | 386/4096 [00:00<00:07, 494.76it/s]
Adding requests:  11%|█         | 437/4096 [00:00<00:07, 497.04it/s]
Adding requests:  12%|█▏        | 488/4096 [00:01<00:07, 500.16it/s]
Adding requests:  13%|█▎        | 539/4096 [00:01<00:07, 488.64it/s]
Adding requests:  14%|█▍        | 592/4096 [00:01<00:07, 499.62it/s]
Adding requests:  16%|█▌        | 643/4096 [00:01<00:06, 499.76it/s]
Adding requests:  17%|█▋        | 694/4096 [00:01<00:06, 486.63it/s]
Adding requests:  18%|█▊        | 743/4096 [00:01<00:06, 485.36it/s]
Adding requests:  19%|█▉        | 792/4096 [00:01<00:06, 483.92it/s]
Adding requests:  21%|██        | 841/4096 [00:01<00:06, 477.20it/s]
Adding requests:  22%|██▏       | 890/4096 [00:01<00:06, 478.12it/s]
Adding requests:  23%|██▎       | 938/4096 [00:01<00:06, 468.34it/s]
Adding requests:  24%|██▍       | 985/4096 [00:02<00:06, 467.56it/s]
Adding requests:  25%|██▌       | 1035/4096 [00:02<00:06, 476.99it/s]
Adding requests:  26%|██▋       | 1083/4096 [00:02<00:06, 468.67it/s]
Adding requests:  28%|██▊       | 1134/4096 [00:02<00:06, 477.81it/s]
Adding requests:  29%|██▉       | 1188/4096 [00:02<00:05, 495.61it/s]
Adding requests:  30%|███       | 1241/4096 [00:02<00:05, 503.87it/s]
Adding requests:  32%|███▏      | 1292/4096 [00:02<00:05, 503.61it/s]
Adding requests:  33%|███▎      | 1345/4096 [00:02<00:05, 510.25it/s]
Adding requests:  34%|███▍      | 1398/4096 [00:02<00:05, 514.68it/s]
Adding requests:  35%|███▌      | 1450/4096 [00:02<00:05, 513.94it/s]
Adding requests:  37%|███▋      | 1504/4096 [00:03<00:04, 519.43it/s]
Adding requests:  38%|███▊      | 1556/4096 [00:03<00:04, 517.61it/s]
Adding requests:  39%|███▉      | 1610/4096 [00:03<00:04, 524.08it/s]
Adding requests:  41%|████      | 1663/4096 [00:03<00:04, 520.78it/s]
Adding requests:  42%|████▏     | 1716/4096 [00:03<00:04, 521.44it/s]
Adding requests:  43%|████▎     | 1769/4096 [00:03<00:04, 518.36it/s]
Adding requests:  44%|████▍     | 1822/4096 [00:03<00:04, 519.03it/s]
Adding requests:  46%|████▌     | 1874/4096 [00:03<00:04, 515.95it/s]
Adding requests:  47%|████▋     | 1926/4096 [00:03<00:04, 516.73it/s]
Adding requests:  48%|████▊     | 1978/4096 [00:03<00:04, 515.16it/s]
Adding requests:  50%|████▉     | 2032/4096 [00:04<00:03, 519.89it/s]
Adding requests:  51%|█████     | 2085/4096 [00:04<00:03, 520.48it/s]
Adding requests:  52%|█████▏    | 2138/4096 [00:04<00:03, 516.38it/s]
Adding requests:  53%|█████▎    | 2190/4096 [00:04<00:03, 510.47it/s]
Adding requests:  55%|█████▍    | 2243/4096 [00:04<00:03, 515.01it/s]
Adding requests:  56%|█████▌    | 2295/4096 [00:04<00:03, 503.98it/s]
Adding requests:  57%|█████▋    | 2347/4096 [00:04<00:03, 508.01it/s]
Adding requests:  59%|█████▊    | 2399/4096 [00:04<00:03, 509.19it/s]
Adding requests:  60%|█████▉    | 2451/4096 [00:04<00:03, 509.39it/s]
Adding requests:  61%|██████    | 2503/4096 [00:05<00:03, 510.37it/s]
Adding requests:  62%|██████▏   | 2557/4096 [00:05<00:02, 516.07it/s]
Adding requests:  64%|██████▎   | 2609/4096 [00:05<00:02, 514.80it/s]
Adding requests:  65%|██████▍   | 2662/4096 [00:05<00:02, 519.04it/s]
Adding requests:  66%|██████▋   | 2714/4096 [00:05<00:02, 512.26it/s]
Adding requests:  68%|██████▊   | 2767/4096 [00:05<00:02, 514.96it/s]
Adding requests:  69%|██████▉   | 2819/4096 [00:05<00:02, 510.87it/s]
Adding requests:  70%|███████   | 2871/4096 [00:05<00:02, 512.80it/s]
Adding requests:  71%|███████▏  | 2923/4096 [00:05<00:02, 512.78it/s]
Adding requests:  73%|███████▎  | 2975/4096 [00:05<00:02, 514.20it/s]
Adding requests:  74%|███████▍  | 3027/4096 [00:06<00:02, 511.42it/s]
Adding requests:  75%|███████▌  | 3079/4096 [00:06<00:02, 501.92it/s]
Adding requests:  76%|███████▋  | 3130/4096 [00:06<00:01, 503.13it/s]
Adding requests:  78%|███████▊  | 3181/4096 [00:06<00:01, 501.09it/s]
Adding requests:  79%|███████▉  | 3232/4096 [00:06<00:01, 498.47it/s]
Adding requests:  80%|████████  | 3283/4096 [00:06<00:01, 499.65it/s]
Adding requests:  81%|████████▏ | 3333/4096 [00:06<00:01, 497.82it/s]
Adding requests:  83%|████████▎ | 3384/4096 [00:06<00:01, 498.75it/s]
Adding requests:  84%|████████▍ | 3435/4096 [00:06<00:01, 501.07it/s]
Adding requests:  85%|████████▌ | 3486/4096 [00:06<00:01, 489.48it/s]
Adding requests:  86%|████████▋ | 3536/4096 [00:07<00:01, 491.12it/s]
Adding requests:  88%|████████▊ | 3586/4096 [00:07<00:01, 479.86it/s]
Adding requests:  89%|████████▊ | 3635/4096 [00:07<00:00, 481.27it/s]
Adding requests:  90%|████████▉ | 3685/4096 [00:07<00:00, 485.21it/s]
Adding requests:  91%|█████████ | 3735/4096 [00:07<00:00, 486.60it/s]
Adding requests:  92%|█████████▏| 3787/4096 [00:07<00:00, 495.74it/s]
Adding requests:  94%|█████████▎| 3837/4096 [00:07<00:00, 496.91it/s]
Adding requests:  95%|█████████▍| 3888/4096 [00:07<00:00, 499.97it/s]
Adding requests:  96%|█████████▌| 3939/4096 [00:07<00:00, 497.68it/s]
Adding requests:  97%|█████████▋| 3989/4096 [00:07<00:00, 495.19it/s]
Adding requests:  99%|█████████▊| 4039/4096 [00:08<00:00, 494.70it/s]
Adding requests: 100%|█████████▉| 4089/4096 [00:08<00:00, 495.91it/s]
Adding requests: 100%|██████████| 4096/4096 [00:08<00:00, 499.36it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  22%|██▏       | 920/4096 [00:00<00:00, 6416.72it/s, est. speed input: 6571465.96 toks/s, output: 6416.96 toks/s]
Processed prompts:  38%|███▊      | 1562/4096 [00:05<00:11, 227.94it/s, est. speed input: 281362.69 toks/s, output: 274.77 toks/s]  
Processed prompts:  45%|████▍     | 1834/4096 [00:07<00:11, 189.84it/s, est. speed input: 237414.78 toks/s, output: 231.85 toks/s]
Processed prompts:  49%|████▊     | 1987/4096 [00:09<00:12, 170.76it/s, est. speed input: 219040.10 toks/s, output: 213.91 toks/s]
Processed prompts:  51%|█████     | 2084/4096 [00:10<00:12, 161.25it/s, est. speed input: 210745.43 toks/s, output: 205.81 toks/s]
Processed prompts:  53%|█████▎    | 2151/4096 [00:10<00:12, 155.37it/s, est. speed input: 206138.54 toks/s, output: 201.31 toks/s]
Processed prompts:  54%|█████▎    | 2199/4096 [00:10<00:12, 156.99it/s, est. speed input: 205406.79 toks/s, output: 200.59 toks/s]
Processed prompts:  55%|█████▍    | 2237/4096 [00:11<00:13, 139.22it/s, est. speed input: 199076.20 toks/s, output: 194.41 toks/s]
Processed prompts:  55%|█████▌    | 2265/4096 [00:11<00:13, 134.18it/s, est. speed input: 196798.24 toks/s, output: 192.19 toks/s]
Processed prompts:  56%|█████▌    | 2296/4096 [00:12<00:13, 131.17it/s, est. speed input: 195012.72 toks/s, output: 190.44 toks/s]
Processed prompts:  57%|█████▋    | 2328/4096 [00:12<00:13, 128.63it/s, est. speed input: 193347.02 toks/s, output: 188.81 toks/s]
Processed prompts:  58%|█████▊    | 2360/4096 [00:12<00:13, 127.02it/s, est. speed input: 191875.34 toks/s, output: 187.38 toks/s]
Processed prompts:  58%|█████▊    | 2392/4096 [00:12<00:13, 124.15it/s, est. speed input: 190258.86 toks/s, output: 185.80 toks/s]
Processed prompts:  59%|█████▉    | 2424/4096 [00:13<00:13, 121.78it/s, est. speed input: 188711.55 toks/s, output: 184.29 toks/s]
Processed prompts:  60%|█████▉    | 2456/4096 [00:13<00:13, 119.75it/s, est. speed input: 187209.28 toks/s, output: 182.82 toks/s]
Processed prompts:  61%|██████    | 2488/4096 [00:13<00:13, 119.02it/s, est. speed input: 185865.27 toks/s, output: 181.51 toks/s]
Processed prompts:  62%|██████▏   | 2520/4096 [00:13<00:13, 117.93it/s, est. speed input: 184514.38 toks/s, output: 180.19 toks/s]
Processed prompts:  62%|██████▏   | 2552/4096 [00:14<00:13, 116.84it/s, est. speed input: 183184.79 toks/s, output: 178.89 toks/s]
Processed prompts:  63%|██████▎   | 2584/4096 [00:14<00:12, 116.98it/s, est. speed input: 182002.43 toks/s, output: 177.74 toks/s]
Processed prompts:  64%|██████▍   | 2616/4096 [00:14<00:12, 116.27it/s, est. speed input: 180783.12 toks/s, output: 176.55 toks/s]
Processed prompts:  65%|██████▍   | 2648/4096 [00:15<00:12, 116.00it/s, est. speed input: 179632.04 toks/s, output: 175.42 toks/s]
Processed prompts:  65%|██████▌   | 2680/4096 [00:15<00:12, 115.39it/s, est. speed input: 178482.57 toks/s, output: 174.30 toks/s]
Processed prompts:  66%|██████▌   | 2712/4096 [00:15<00:12, 115.03it/s, est. speed input: 177380.25 toks/s, output: 173.22 toks/s]
Processed prompts:  67%|██████▋   | 2744/4096 [00:15<00:11, 115.01it/s, est. speed input: 176338.10 toks/s, output: 172.20 toks/s]
Processed prompts:  68%|██████▊   | 2776/4096 [00:16<00:11, 114.83it/s, est. speed input: 175317.71 toks/s, output: 171.21 toks/s]
Processed prompts:  69%|██████▊   | 2808/4096 [00:16<00:11, 114.73it/s, est. speed input: 174333.26 toks/s, output: 170.25 toks/s]
Processed prompts:  69%|██████▉   | 2840/4096 [00:16<00:10, 114.67it/s, est. speed input: 173383.00 toks/s, output: 169.32 toks/s]
Processed prompts:  70%|███████   | 2872/4096 [00:17<00:10, 114.76it/s, est. speed input: 172474.75 toks/s, output: 168.43 toks/s]
Processed prompts:  71%|███████   | 2904/4096 [00:17<00:10, 114.64it/s, est. speed input: 171580.44 toks/s, output: 167.56 toks/s]
Processed prompts:  72%|███████▏  | 2936/4096 [00:17<00:10, 114.84it/s, est. speed input: 170737.57 toks/s, output: 166.74 toks/s]
Processed prompts:  72%|███████▏  | 2968/4096 [00:17<00:09, 114.72it/s, est. speed input: 169900.21 toks/s, output: 165.92 toks/s]
Processed prompts:  73%|███████▎  | 3000/4096 [00:18<00:09, 114.81it/s, est. speed input: 169102.15 toks/s, output: 165.14 toks/s]
Processed prompts:  74%|███████▍  | 3032/4096 [00:18<00:09, 114.53it/s, est. speed input: 168302.93 toks/s, output: 164.36 toks/s]
Processed prompts:  75%|███████▍  | 3064/4096 [00:18<00:09, 114.56it/s, est. speed input: 167543.77 toks/s, output: 163.62 toks/s]
Processed prompts:  76%|███████▌  | 3096/4096 [00:19<00:08, 114.61it/s, est. speed input: 166809.12 toks/s, output: 162.90 toks/s]
Processed prompts:  76%|███████▋  | 3128/4096 [00:19<00:08, 115.50it/s, est. speed input: 166154.91 toks/s, output: 162.26 toks/s]
Processed prompts:  77%|███████▋  | 3160/4096 [00:19<00:08, 115.30it/s, est. speed input: 165462.71 toks/s, output: 161.58 toks/s]
Processed prompts:  78%|███████▊  | 3192/4096 [00:19<00:07, 115.27it/s, est. speed input: 164797.88 toks/s, output: 160.94 toks/s]
Processed prompts:  79%|███████▊  | 3224/4096 [00:20<00:07, 115.27it/s, est. speed input: 164152.34 toks/s, output: 160.30 toks/s]
Processed prompts:  79%|███████▉  | 3256/4096 [00:20<00:07, 114.83it/s, est. speed input: 163495.76 toks/s, output: 159.66 toks/s]
Processed prompts:  80%|████████  | 3288/4096 [00:20<00:07, 114.74it/s, est. speed input: 162871.05 toks/s, output: 159.05 toks/s]
Processed prompts:  81%|████████  | 3320/4096 [00:20<00:06, 114.73it/s, est. speed input: 162266.52 toks/s, output: 158.46 toks/s]
Processed prompts:  82%|████████▏ | 3352/4096 [00:21<00:06, 114.65it/s, est. speed input: 161673.15 toks/s, output: 157.88 toks/s]
Processed prompts:  83%|████████▎ | 3384/4096 [00:21<00:06, 114.48it/s, est. speed input: 161088.32 toks/s, output: 157.31 toks/s]
Processed prompts:  83%|████████▎ | 3416/4096 [00:21<00:05, 114.62it/s, est. speed input: 160534.34 toks/s, output: 156.77 toks/s]
Processed prompts:  84%|████████▍ | 3448/4096 [00:22<00:05, 114.78it/s, est. speed input: 159997.68 toks/s, output: 156.25 toks/s]
Processed prompts:  85%|████████▍ | 3480/4096 [00:22<00:05, 115.41it/s, est. speed input: 159504.22 toks/s, output: 155.77 toks/s]
Processed prompts:  86%|████████▌ | 3512/4096 [00:22<00:05, 116.23it/s, est. speed input: 159043.06 toks/s, output: 155.32 toks/s]
Processed prompts:  87%|████████▋ | 3544/4096 [00:22<00:04, 115.86it/s, est. speed input: 158541.52 toks/s, output: 154.83 toks/s]
Processed prompts:  87%|████████▋ | 3576/4096 [00:23<00:04, 115.35it/s, est. speed input: 158038.34 toks/s, output: 154.33 toks/s]
Processed prompts:  88%|████████▊ | 3608/4096 [00:23<00:04, 114.90it/s, est. speed input: 157541.80 toks/s, output: 153.85 toks/s]
Processed prompts:  89%|████████▉ | 3640/4096 [00:23<00:03, 114.71it/s, est. speed input: 157063.62 toks/s, output: 153.38 toks/s]
Processed prompts:  90%|████████▉ | 3672/4096 [00:24<00:03, 114.99it/s, est. speed input: 156617.91 toks/s, output: 152.95 toks/s]
Processed prompts:  90%|█████████ | 3704/4096 [00:24<00:03, 114.73it/s, est. speed input: 156158.67 toks/s, output: 152.50 toks/s]
Processed prompts:  91%|█████████ | 3736/4096 [00:24<00:03, 115.32it/s, est. speed input: 155750.06 toks/s, output: 152.10 toks/s]
Processed prompts:  92%|█████████▏| 3768/4096 [00:24<00:02, 115.33it/s, est. speed input: 155330.00 toks/s, output: 151.69 toks/s]
Processed prompts:  93%|█████████▎| 3800/4096 [00:25<00:02, 115.25it/s, est. speed input: 154914.41 toks/s, output: 151.28 toks/s]
Processed prompts:  94%|█████████▎| 3832/4096 [00:25<00:02, 116.09it/s, est. speed input: 154552.05 toks/s, output: 150.93 toks/s]
Processed prompts:  94%|█████████▍| 3864/4096 [00:25<00:02, 115.20it/s, est. speed input: 154126.34 toks/s, output: 150.51 toks/s]
Processed prompts:  95%|█████████▌| 3896/4096 [00:25<00:01, 115.13it/s, est. speed input: 153735.58 toks/s, output: 150.13 toks/s]
Processed prompts:  96%|█████████▌| 3928/4096 [00:26<00:01, 114.78it/s, est. speed input: 153339.52 toks/s, output: 149.75 toks/s]
Processed prompts:  97%|█████████▋| 3960/4096 [00:26<00:01, 114.60it/s, est. speed input: 152954.53 toks/s, output: 149.37 toks/s]
Processed prompts:  97%|█████████▋| 3992/4096 [00:26<00:00, 114.29it/s, est. speed input: 152568.90 toks/s, output: 148.99 toks/s]
Processed prompts:  98%|█████████▊| 4024/4096 [00:27<00:00, 115.22it/s, est. speed input: 152243.73 toks/s, output: 148.68 toks/s]
Processed prompts:  99%|█████████▉| 4056/4096 [00:27<00:00, 115.26it/s, est. speed input: 151897.43 toks/s, output: 148.34 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:27<00:00, 115.26it/s, est. speed input: 152916.75 toks/s, output: 149.33 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:27<00:00, 149.33it/s, est. speed input: 152916.75 toks/s, output: 149.33 toks/s]
[rank0]:[W128 09:09:56.028886513 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 90.9s

测试结果:
  Requests/s:   114.94
  Tokens/s:     117818.51
  Total Reqs:   4096
  Elapsed:      35.63s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     117703.56


------------------------------------------------------------
  生成 CSV: BitNet-2B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_FP8E4M3_py312_cu129_x86_64/cusparselt/2_4/BitNet-2B-FP8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,31.4003,16108.3718,4.0764
1024,1024,1,128,128,31.8112,32606.4372,4.0237
2048,1024,2,256,128,60.9046,62427.2034,4.2033
4096,1024,4,512,128,94.0335,96384.3386,5.4449
8192,1024,8,1024,128,104.8955,107517.8790,9.7621
16384,1024,16,2048,128,110.5623,113326.3125,18.5235
32768,1024,32,4096,128,114.9449,117818.5073,35.6345

------------------------------------------------------------

[INFO] 完成: 7 成功, 0 失败

============================================================
  BitNet-2B-FP8 | cuSPARSELt (2_6) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_6
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6

============================================================
[1/7] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 09:10:05 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3361619) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3361619) WARNING 01-28 09:10:20 [backends.py:609] Failed to read file <frozen os>
Throughput: 32.15 requests/s, 16491.61 total tokens/s, 32.15 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-28 09:10:04] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 09:10:05] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:10:05] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:10:05] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:10:05] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:10:05] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:10:05] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:10:05] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:10:05] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:10:05] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:10:05] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:10:05] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:10:05] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:10:05] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 09:10:11] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 09:10:11] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:10:11] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:10:11] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:10:11] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:10:11] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:10:11] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:10:11] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:10:11] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:10:11] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:10:11] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:10:11] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:10:11] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:10:11] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3361619) [2026-01-28 09:10:12] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3361619) [2026-01-28 09:10:12] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3361619) [2026-01-28 09:10:12] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3361619) [2026-01-28 09:10:12] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3361619) [2026-01-28 09:10:12] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3361619) [2026-01-28 09:10:12] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3361619) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3361619) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.39it/s]
(EngineCore_DP0 pid=3361619) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.39it/s]
(EngineCore_DP0 pid=3361619) 
(EngineCore_DP0 pid=3361619) [2026-01-28 09:10:13] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=3361619) [2026-01-28 09:10:13] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8232960 bytes
(EngineCore_DP0 pid=3361619) [2026-01-28 09:10:13] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=3361619) [2026-01-28 09:10:13] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5488640 bytes
(EngineCore_DP0 pid=3361619) [2026-01-28 09:10:13] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=3361619) [2026-01-28 09:10:13] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 29638656 bytes
(EngineCore_DP0 pid=3361619) [2026-01-28 09:10:13] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=3361619) [2026-01-28 09:10:13] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14745600 bytes
(EngineCore_DP0 pid=3361619) 2026-01-28 09:10:30,878 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3361619) 2026-01-28 09:10:30,906 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3361619) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  4.85it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.88it/s]
(EngineCore_DP0 pid=3361619) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 15.14it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  38%|███▊      | 48/128 [00:00<00:00, 475.61it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 676.74it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:24,  5.26it/s, est. speed input: 2695.32 toks/s, output: 5.26 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:06, 19.16it/s, est. speed input: 8468.13 toks/s, output: 16.54 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:04, 25.77it/s, est. speed input: 11145.82 toks/s, output: 21.77 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:03, 29.46it/s, est. speed input: 12692.42 toks/s, output: 24.79 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:03, 31.64it/s, est. speed input: 13686.10 toks/s, output: 26.73 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:03, 33.03it/s, est. speed input: 14386.63 toks/s, output: 28.10 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:00<00:03, 33.99it/s, est. speed input: 14912.05 toks/s, output: 29.12 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:00<00:02, 34.61it/s, est. speed input: 15315.26 toks/s, output: 29.91 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:01<00:02, 34.79it/s, est. speed input: 15597.70 toks/s, output: 30.46 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:01<00:02, 34.86it/s, est. speed input: 15819.65 toks/s, output: 30.90 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:01<00:02, 34.90it/s, est. speed input: 16003.28 toks/s, output: 31.26 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:01<00:02, 34.97it/s, est. speed input: 16161.56 toks/s, output: 31.56 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:01<00:02, 34.95it/s, est. speed input: 16288.43 toks/s, output: 31.81 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:01<00:02, 34.94it/s, est. speed input: 16398.94 toks/s, output: 32.03 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:01<00:02, 35.01it/s, est. speed input: 16502.53 toks/s, output: 32.23 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:01<00:01, 34.98it/s, est. speed input: 16585.78 toks/s, output: 32.39 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:01<00:01, 34.92it/s, est. speed input: 16656.26 toks/s, output: 32.53 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:02<00:01, 34.96it/s, est. speed input: 16725.81 toks/s, output: 32.67 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:02<00:01, 34.97it/s, est. speed input: 16786.96 toks/s, output: 32.79 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:02<00:01, 35.01it/s, est. speed input: 16844.79 toks/s, output: 32.90 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:02<00:01, 34.76it/s, est. speed input: 16875.90 toks/s, output: 32.96 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:02<00:01, 34.80it/s, est. speed input: 16920.26 toks/s, output: 33.05 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:02<00:01, 34.84it/s, est. speed input: 16961.60 toks/s, output: 33.13 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:02<00:01, 34.85it/s, est. speed input: 16998.42 toks/s, output: 33.20 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:02<00:00, 34.84it/s, est. speed input: 17030.56 toks/s, output: 33.26 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:03<00:00, 34.84it/s, est. speed input: 17061.30 toks/s, output: 33.32 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:03<00:00, 34.63it/s, est. speed input: 17076.89 toks/s, output: 33.35 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:03<00:00, 34.96it/s, est. speed input: 17118.95 toks/s, output: 33.44 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:03<00:00, 35.18it/s, est. speed input: 17157.51 toks/s, output: 33.51 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:03<00:00, 35.36it/s, est. speed input: 17195.23 toks/s, output: 33.58 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:03<00:00, 35.51it/s, est. speed input: 17231.39 toks/s, output: 33.65 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:03<00:00, 35.62it/s, est. speed input: 17265.48 toks/s, output: 33.72 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 35.62it/s, est. speed input: 17288.83 toks/s, output: 33.77 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 33.76it/s, est. speed input: 17288.83 toks/s, output: 33.77 toks/s]
[rank0]:[W128 09:10:37.444354669 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 41.2s

测试结果:
  Requests/s:   32.15
  Tokens/s:     16491.61
  Total Reqs:   128
  Elapsed:      3.98s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     16459.46

============================================================
[2/7] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 09:10:46 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3362799) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3362799) WARNING 01-28 09:11:02 [backends.py:609] Failed to read file <frozen os>
Throughput: 32.17 requests/s, 32973.46 total tokens/s, 32.17 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-28 09:10:46] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 09:10:46] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:10:46] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:10:46] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:10:46] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:10:46] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:10:46] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:10:46] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:10:46] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:10:46] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:10:46] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:10:46] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:10:46] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:10:46] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 09:10:53] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 09:10:53] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:10:53] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:10:53] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:10:53] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:10:53] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:10:53] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:10:53] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:10:53] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:10:53] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:10:53] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:10:53] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:10:53] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:10:53] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3362799) [2026-01-28 09:10:54] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3362799) [2026-01-28 09:10:54] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3362799) [2026-01-28 09:10:54] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3362799) [2026-01-28 09:10:54] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3362799) [2026-01-28 09:10:54] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3362799) [2026-01-28 09:10:54] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3362799) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3362799) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.37it/s]
(EngineCore_DP0 pid=3362799) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.36it/s]
(EngineCore_DP0 pid=3362799) 
(EngineCore_DP0 pid=3362799) [2026-01-28 09:10:56] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=3362799) [2026-01-28 09:10:56] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8232960 bytes
(EngineCore_DP0 pid=3362799) [2026-01-28 09:10:56] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=3362799) [2026-01-28 09:10:56] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5488640 bytes
(EngineCore_DP0 pid=3362799) [2026-01-28 09:10:56] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=3362799) [2026-01-28 09:10:56] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 29638656 bytes
(EngineCore_DP0 pid=3362799) [2026-01-28 09:10:56] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=3362799) [2026-01-28 09:10:56] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14745600 bytes
(EngineCore_DP0 pid=3362799) 2026-01-28 09:11:13,639 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3362799) 2026-01-28 09:11:13,667 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3362799) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  6.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  6.49it/s]
(EngineCore_DP0 pid=3362799) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 10.63it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  21%|██        | 27/128 [00:00<00:00, 265.72it/s]
Adding requests:  62%|██████▏   | 79/128 [00:00<00:00, 411.17it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 423.35it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:02, 49.31it/s, est. speed input: 50506.09 toks/s, output: 49.32 toks/s]
Processed prompts:   8%|▊         | 10/128 [00:00<00:02, 39.52it/s, est. speed input: 41721.56 toks/s, output: 40.74 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:00<00:03, 37.20it/s, est. speed input: 39463.28 toks/s, output: 38.54 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:00<00:02, 36.39it/s, est. speed input: 38619.05 toks/s, output: 37.71 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:00<00:02, 35.82it/s, est. speed input: 38051.00 toks/s, output: 37.16 toks/s]
Processed prompts:  21%|██        | 27/128 [00:00<00:02, 35.38it/s, est. speed input: 37613.48 toks/s, output: 36.73 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:00<00:02, 35.01it/s, est. speed input: 37263.10 toks/s, output: 36.39 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:00<00:02, 34.81it/s, est. speed input: 37015.72 toks/s, output: 36.15 toks/s]
Processed prompts:  30%|███       | 39/128 [00:01<00:02, 34.74it/s, est. speed input: 36842.61 toks/s, output: 35.98 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:01<00:02, 34.63it/s, est. speed input: 36685.44 toks/s, output: 35.82 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:01<00:02, 34.55it/s, est. speed input: 36554.05 toks/s, output: 35.70 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:01<00:02, 34.46it/s, est. speed input: 36433.79 toks/s, output: 35.58 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:01<00:02, 34.42it/s, est. speed input: 36337.51 toks/s, output: 35.49 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:01<00:02, 34.43it/s, est. speed input: 36264.11 toks/s, output: 35.41 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:01<00:01, 34.39it/s, est. speed input: 36187.79 toks/s, output: 35.34 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:01<00:01, 34.41it/s, est. speed input: 36132.88 toks/s, output: 35.29 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:02<00:01, 34.41it/s, est. speed input: 36080.28 toks/s, output: 35.23 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:02<00:01, 34.39it/s, est. speed input: 36031.57 toks/s, output: 35.19 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:02<00:01, 34.37it/s, est. speed input: 35984.38 toks/s, output: 35.14 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:02<00:01, 34.33it/s, est. speed input: 35939.68 toks/s, output: 35.10 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:02<00:01, 34.35it/s, est. speed input: 35905.55 toks/s, output: 35.06 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:02<00:01, 34.35it/s, est. speed input: 35872.26 toks/s, output: 35.03 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:02<00:00, 34.35it/s, est. speed input: 35842.28 toks/s, output: 35.00 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:02<00:00, 34.31it/s, est. speed input: 35810.23 toks/s, output: 34.97 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:02<00:00, 34.32it/s, est. speed input: 35784.86 toks/s, output: 34.95 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:03<00:00, 34.27it/s, est. speed input: 35753.19 toks/s, output: 34.91 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:03<00:00, 34.35it/s, est. speed input: 35739.48 toks/s, output: 34.90 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:03<00:00, 34.35it/s, est. speed input: 35719.18 toks/s, output: 34.88 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:03<00:00, 34.37it/s, est. speed input: 35702.66 toks/s, output: 34.87 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:03<00:00, 34.41it/s, est. speed input: 35690.79 toks/s, output: 34.85 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:03<00:00, 34.36it/s, est. speed input: 35670.92 toks/s, output: 34.83 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 34.36it/s, est. speed input: 35666.44 toks/s, output: 34.83 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 34.83it/s, est. speed input: 35666.44 toks/s, output: 34.83 toks/s]
[rank0]:[W128 09:11:19.872174786 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 42.4s

测试结果:
  Requests/s:   32.17
  Tokens/s:     32973.46
  Total Reqs:   128
  Elapsed:      3.98s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     32941.29

============================================================
[3/7] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 09:11:29 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3363883) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3363883) WARNING 01-28 09:11:44 [backends.py:609] Failed to read file <frozen os>
Throughput: 63.48 requests/s, 65063.41 total tokens/s, 63.48 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-28 09:11:29] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 09:11:29] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:11:29] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:11:29] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:11:29] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:11:29] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:11:29] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:11:29] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:11:29] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:11:29] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:11:29] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:11:29] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:11:29] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:11:29] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 09:11:36] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 09:11:36] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:11:36] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:11:36] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:11:36] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:11:36] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:11:36] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:11:36] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:11:36] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:11:36] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:11:36] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:11:36] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:11:36] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:11:36] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3363883) [2026-01-28 09:11:37] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3363883) [2026-01-28 09:11:37] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3363883) [2026-01-28 09:11:37] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3363883) [2026-01-28 09:11:37] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3363883) [2026-01-28 09:11:37] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3363883) [2026-01-28 09:11:37] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3363883) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3363883) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.34it/s]
(EngineCore_DP0 pid=3363883) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.34it/s]
(EngineCore_DP0 pid=3363883) 
(EngineCore_DP0 pid=3363883) [2026-01-28 09:11:38] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=3363883) [2026-01-28 09:11:38] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8232960 bytes
(EngineCore_DP0 pid=3363883) [2026-01-28 09:11:38] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=3363883) [2026-01-28 09:11:38] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5488640 bytes
(EngineCore_DP0 pid=3363883) [2026-01-28 09:11:38] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=3363883) [2026-01-28 09:11:38] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 29638656 bytes
(EngineCore_DP0 pid=3363883) [2026-01-28 09:11:38] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=3363883) [2026-01-28 09:11:38] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14745600 bytes
(EngineCore_DP0 pid=3363883) 2026-01-28 09:11:55,864 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3363883) 2026-01-28 09:11:55,891 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3363883) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:00<00:00, 13.22it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 13.07it/s]
(EngineCore_DP0 pid=3363883) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 16.60it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 16.58it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  20%|█▉        | 50/256 [00:00<00:00, 496.76it/s]
Adding requests:  39%|███▉      | 101/256 [00:00<00:00, 500.49it/s]
Adding requests:  59%|█████▉    | 152/256 [00:00<00:00, 500.47it/s]
Adding requests:  79%|███████▉  | 203/256 [00:00<00:00, 494.64it/s]
Adding requests: 100%|█████████▉| 255/256 [00:00<00:00, 501.90it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 500.01it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   9%|▉         | 24/256 [00:00<00:01, 225.54it/s, est. speed input: 230996.05 toks/s, output: 225.55 toks/s]
Processed prompts:  18%|█▊        | 47/256 [00:00<00:02, 102.08it/s, est. speed input: 114098.85 toks/s, output: 111.42 toks/s]
Processed prompts:  24%|██▍       | 61/256 [00:00<00:02, 88.38it/s, est. speed input: 100338.52 toks/s, output: 97.98 toks/s]  
Processed prompts:  28%|██▊       | 72/256 [00:00<00:02, 79.55it/s, est. speed input: 92443.66 toks/s, output: 90.27 toks/s] 
Processed prompts:  32%|███▏      | 81/256 [00:00<00:02, 78.65it/s, est. speed input: 90543.23 toks/s, output: 88.42 toks/s]
Processed prompts:  35%|███▌      | 90/256 [00:01<00:02, 73.09it/s, est. speed input: 86583.04 toks/s, output: 84.55 toks/s]
Processed prompts:  38%|███▊      | 98/256 [00:01<00:02, 71.70it/s, est. speed input: 84863.10 toks/s, output: 82.87 toks/s]
Processed prompts:  41%|████▏     | 106/256 [00:01<00:02, 70.64it/s, est. speed input: 83460.43 toks/s, output: 81.50 toks/s]
Processed prompts:  45%|████▍     | 114/256 [00:01<00:02, 69.87it/s, est. speed input: 82301.00 toks/s, output: 80.37 toks/s]
Processed prompts:  48%|████▊     | 122/256 [00:01<00:01, 69.37it/s, est. speed input: 81340.42 toks/s, output: 79.43 toks/s]
Processed prompts:  51%|█████     | 130/256 [00:01<00:01, 68.87it/s, est. speed input: 80477.51 toks/s, output: 78.59 toks/s]
Processed prompts:  54%|█████▍    | 138/256 [00:01<00:01, 68.38it/s, est. speed input: 79697.00 toks/s, output: 77.83 toks/s]
Processed prompts:  57%|█████▋    | 146/256 [00:01<00:01, 68.23it/s, est. speed input: 79060.04 toks/s, output: 77.21 toks/s]
Processed prompts:  60%|██████    | 154/256 [00:02<00:01, 67.97it/s, est. speed input: 78463.79 toks/s, output: 76.62 toks/s]
Processed prompts:  63%|██████▎   | 162/256 [00:02<00:01, 67.80it/s, est. speed input: 77937.10 toks/s, output: 76.11 toks/s]
Processed prompts:  66%|██████▋   | 170/256 [00:02<00:01, 67.63it/s, est. speed input: 77456.03 toks/s, output: 75.64 toks/s]
Processed prompts:  70%|██████▉   | 178/256 [00:02<00:01, 67.49it/s, est. speed input: 77019.31 toks/s, output: 75.21 toks/s]
Processed prompts:  73%|███████▎  | 186/256 [00:02<00:01, 67.48it/s, est. speed input: 76639.86 toks/s, output: 74.84 toks/s]
Processed prompts:  76%|███████▌  | 194/256 [00:02<00:00, 67.44it/s, est. speed input: 76290.59 toks/s, output: 74.50 toks/s]
Processed prompts:  79%|███████▉  | 202/256 [00:02<00:00, 67.49it/s, est. speed input: 75983.37 toks/s, output: 74.20 toks/s]
Processed prompts:  82%|████████▏ | 210/256 [00:02<00:00, 67.52it/s, est. speed input: 75700.35 toks/s, output: 73.93 toks/s]
Processed prompts:  85%|████████▌ | 218/256 [00:02<00:00, 67.49it/s, est. speed input: 75433.85 toks/s, output: 73.67 toks/s]
Processed prompts:  88%|████████▊ | 226/256 [00:03<00:00, 67.60it/s, est. speed input: 75205.98 toks/s, output: 73.44 toks/s]
Processed prompts:  91%|█████████▏| 234/256 [00:03<00:00, 67.62it/s, est. speed input: 74987.25 toks/s, output: 73.23 toks/s]
Processed prompts:  95%|█████████▍| 242/256 [00:03<00:00, 67.68it/s, est. speed input: 74790.49 toks/s, output: 73.04 toks/s]
Processed prompts:  98%|█████████▊| 250/256 [00:03<00:00, 67.71it/s, est. speed input: 74604.50 toks/s, output: 72.86 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 67.71it/s, est. speed input: 74482.11 toks/s, output: 72.74 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 72.73it/s, est. speed input: 74482.11 toks/s, output: 72.74 toks/s]
[rank0]:[W128 09:12:02.070218983 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 42.2s

测试结果:
  Requests/s:   63.48
  Tokens/s:     65063.41
  Total Reqs:   256
  Elapsed:      4.03s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     64999.93

============================================================
[4/7] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 09:12:12 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3364997) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3364997) WARNING 01-28 09:12:27 [backends.py:609] Failed to read file <frozen os>
Throughput: 81.88 requests/s, 83924.19 total tokens/s, 81.88 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-28 09:12:12] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 09:12:12] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:12:12] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:12:12] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:12:12] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:12:12] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:12:12] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:12:12] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:12:12] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:12:12] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:12:12] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:12:12] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:12:12] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:12:12] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 09:12:18] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 09:12:19] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:12:19] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:12:19] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:12:19] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:12:19] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:12:19] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:12:19] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:12:19] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:12:19] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:12:19] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:12:19] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:12:19] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:12:19] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3364997) [2026-01-28 09:12:20] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3364997) [2026-01-28 09:12:20] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3364997) [2026-01-28 09:12:20] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3364997) [2026-01-28 09:12:20] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3364997) [2026-01-28 09:12:20] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3364997) [2026-01-28 09:12:20] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3364997) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3364997) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.41it/s]
(EngineCore_DP0 pid=3364997) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.41it/s]
(EngineCore_DP0 pid=3364997) 
(EngineCore_DP0 pid=3364997) [2026-01-28 09:12:21] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=3364997) [2026-01-28 09:12:21] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8232960 bytes
(EngineCore_DP0 pid=3364997) [2026-01-28 09:12:21] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=3364997) [2026-01-28 09:12:21] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5488640 bytes
(EngineCore_DP0 pid=3364997) [2026-01-28 09:12:21] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=3364997) [2026-01-28 09:12:21] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 29638656 bytes
(EngineCore_DP0 pid=3364997) [2026-01-28 09:12:21] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=3364997) [2026-01-28 09:12:21] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14745600 bytes
(EngineCore_DP0 pid=3364997) 2026-01-28 09:12:38,696 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3364997) 2026-01-28 09:12:38,724 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3364997) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 1/4 [00:00<00:00,  9.84it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 3/4 [00:00<00:00, 12.88it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 12.06it/s]
(EngineCore_DP0 pid=3364997) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00,  7.45it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00,  7.32it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00,  7.34it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   5%|▌         | 28/512 [00:00<00:01, 274.82it/s]
Adding requests:  16%|█▌        | 81/512 [00:00<00:01, 420.25it/s]
Adding requests:  26%|██▌       | 132/512 [00:00<00:00, 460.07it/s]
Adding requests:  35%|███▌      | 181/512 [00:00<00:00, 471.32it/s]
Adding requests:  46%|████▌     | 233/512 [00:00<00:00, 486.69it/s]
Adding requests:  55%|█████▌    | 284/512 [00:00<00:00, 491.20it/s]
Adding requests:  65%|██████▌   | 334/512 [00:00<00:00, 492.27it/s]
Adding requests:  75%|███████▌  | 385/512 [00:00<00:00, 495.08it/s]
Adding requests:  85%|████████▌ | 436/512 [00:00<00:00, 498.79it/s]
Adding requests:  95%|█████████▌| 487/512 [00:01<00:00, 501.04it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 481.79it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  13%|█▎        | 66/512 [00:00<00:00, 607.42it/s, est. speed input: 622105.22 toks/s, output: 607.45 toks/s]
Processed prompts:  25%|██▍       | 127/512 [00:00<00:02, 141.17it/s, est. speed input: 164213.48 toks/s, output: 160.36 toks/s]
Processed prompts:  31%|███       | 158/512 [00:01<00:03, 117.28it/s, est. speed input: 138936.51 toks/s, output: 135.68 toks/s]
Processed prompts:  35%|███▍      | 179/512 [00:01<00:03, 110.49it/s, est. speed input: 131536.55 toks/s, output: 128.45 toks/s]
Processed prompts:  38%|███▊      | 195/512 [00:01<00:03, 105.24it/s, est. speed input: 126729.50 toks/s, output: 123.76 toks/s]
Processed prompts:  41%|████      | 209/512 [00:01<00:02, 104.53it/s, est. speed input: 124914.56 toks/s, output: 121.99 toks/s]
Processed prompts:  43%|████▎     | 222/512 [00:01<00:03, 95.02it/s, est. speed input: 119756.82 toks/s, output: 116.95 toks/s] 
Processed prompts:  46%|████▌     | 234/512 [00:02<00:02, 92.98it/s, est. speed input: 117599.59 toks/s, output: 114.84 toks/s]
Processed prompts:  48%|████▊     | 246/512 [00:02<00:02, 91.33it/s, est. speed input: 115731.48 toks/s, output: 113.02 toks/s]
Processed prompts:  50%|█████     | 258/512 [00:02<00:02, 90.23it/s, est. speed input: 114149.78 toks/s, output: 111.47 toks/s]
Processed prompts:  53%|█████▎    | 270/512 [00:02<00:02, 89.42it/s, est. speed input: 112756.71 toks/s, output: 110.11 toks/s]
Processed prompts:  55%|█████▌    | 282/512 [00:02<00:02, 88.83it/s, est. speed input: 111517.22 toks/s, output: 108.90 toks/s]
Processed prompts:  57%|█████▋    | 294/512 [00:02<00:02, 88.49it/s, est. speed input: 110421.77 toks/s, output: 107.83 toks/s]
Processed prompts:  60%|█████▉    | 306/512 [00:02<00:02, 88.01it/s, est. speed input: 109385.08 toks/s, output: 106.82 toks/s]
Processed prompts:  62%|██████▏   | 318/512 [00:03<00:02, 87.51it/s, est. speed input: 108414.15 toks/s, output: 105.87 toks/s]
Processed prompts:  64%|██████▍   | 330/512 [00:03<00:02, 87.22it/s, est. speed input: 107539.33 toks/s, output: 105.02 toks/s]
Processed prompts:  67%|██████▋   | 342/512 [00:03<00:01, 88.33it/s, est. speed input: 106965.40 toks/s, output: 104.46 toks/s]
Processed prompts:  69%|██████▉   | 354/512 [00:03<00:01, 87.87it/s, est. speed input: 106231.91 toks/s, output: 103.74 toks/s]
Processed prompts:  71%|███████▏  | 366/512 [00:03<00:01, 87.58it/s, est. speed input: 105561.86 toks/s, output: 103.09 toks/s]
Processed prompts:  74%|███████▍  | 378/512 [00:03<00:01, 87.63it/s, est. speed input: 104978.99 toks/s, output: 102.52 toks/s]
Processed prompts:  76%|███████▌  | 390/512 [00:03<00:01, 87.46it/s, est. speed input: 104407.95 toks/s, output: 101.96 toks/s]
Processed prompts:  79%|███████▊  | 402/512 [00:03<00:01, 87.18it/s, est. speed input: 103855.78 toks/s, output: 101.42 toks/s]
Processed prompts:  81%|████████  | 414/512 [00:04<00:01, 87.08it/s, est. speed input: 103352.52 toks/s, output: 100.93 toks/s]
Processed prompts:  83%|████████▎ | 426/512 [00:04<00:00, 86.92it/s, est. speed input: 102870.92 toks/s, output: 100.46 toks/s]
Processed prompts:  86%|████████▌ | 438/512 [00:04<00:00, 86.80it/s, est. speed input: 102420.01 toks/s, output: 100.02 toks/s]
Processed prompts:  88%|████████▊ | 450/512 [00:04<00:00, 88.33it/s, est. speed input: 102185.43 toks/s, output: 99.79 toks/s] 
Processed prompts:  90%|█████████ | 462/512 [00:04<00:00, 88.27it/s, est. speed input: 101834.91 toks/s, output: 99.45 toks/s]
Processed prompts:  93%|█████████▎| 474/512 [00:04<00:00, 87.98it/s, est. speed input: 101478.08 toks/s, output: 99.10 toks/s]
Processed prompts:  95%|█████████▍| 486/512 [00:04<00:00, 87.86it/s, est. speed input: 101149.77 toks/s, output: 98.78 toks/s]
Processed prompts:  97%|█████████▋| 498/512 [00:05<00:00, 87.49it/s, est. speed input: 100809.50 toks/s, output: 98.45 toks/s]
Processed prompts: 100%|█████████▉| 510/512 [00:05<00:00, 88.87it/s, est. speed input: 100650.03 toks/s, output: 98.29 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:05<00:00, 88.87it/s, est. speed input: 101040.97 toks/s, output: 98.67 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:05<00:00, 98.67it/s, est. speed input: 101040.97 toks/s, output: 98.67 toks/s]
[rank0]:[W128 09:12:47.600196698 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 45.7s

测试结果:
  Requests/s:   81.88
  Tokens/s:     83924.19
  Total Reqs:   512
  Elapsed:      6.25s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     83842.32

============================================================
[5/7] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 09:13:00 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3366138) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3366138) WARNING 01-28 09:13:15 [backends.py:609] Failed to read file <frozen os>
Throughput: 92.74 requests/s, 95055.82 total tokens/s, 92.74 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-28 09:13:00] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 09:13:00] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:13:00] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:13:00] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:13:00] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:13:00] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:13:00] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:13:00] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:13:00] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:13:00] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:13:00] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:13:00] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:13:00] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:13:00] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 09:13:06] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 09:13:06] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:13:06] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:13:06] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:13:06] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:13:06] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:13:06] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:13:06] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:13:06] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:13:06] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:13:06] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:13:06] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:13:06] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:13:06] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3366138) [2026-01-28 09:13:08] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3366138) [2026-01-28 09:13:08] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3366138) [2026-01-28 09:13:08] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3366138) [2026-01-28 09:13:08] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3366138) [2026-01-28 09:13:08] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3366138) [2026-01-28 09:13:08] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3366138) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3366138) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.38it/s]
(EngineCore_DP0 pid=3366138) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.38it/s]
(EngineCore_DP0 pid=3366138) 
(EngineCore_DP0 pid=3366138) [2026-01-28 09:13:08] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=3366138) [2026-01-28 09:13:09] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8232960 bytes
(EngineCore_DP0 pid=3366138) [2026-01-28 09:13:09] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=3366138) [2026-01-28 09:13:09] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5488640 bytes
(EngineCore_DP0 pid=3366138) [2026-01-28 09:13:09] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=3366138) [2026-01-28 09:13:09] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 29638656 bytes
(EngineCore_DP0 pid=3366138) [2026-01-28 09:13:09] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=3366138) [2026-01-28 09:13:09] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14745600 bytes
(EngineCore_DP0 pid=3366138) 2026-01-28 09:13:25,841 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3366138) 2026-01-28 09:13:25,869 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3366138) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 1/5 [00:00<00:01,  3.10it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00,  5.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00,  9.06it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00,  7.98it/s]
(EngineCore_DP0 pid=3366138) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00, 16.39it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 16.82it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 16.74it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   3%|▎         | 32/1024 [00:00<00:03, 317.65it/s]
Adding requests:   8%|▊         | 84/1024 [00:00<00:02, 433.29it/s]
Adding requests:  13%|█▎        | 135/1024 [00:00<00:01, 466.70it/s]
Adding requests:  18%|█▊        | 184/1024 [00:00<00:01, 475.39it/s]
Adding requests:  23%|██▎       | 236/1024 [00:00<00:01, 489.08it/s]
Adding requests:  28%|██▊       | 287/1024 [00:00<00:01, 492.51it/s]
Adding requests:  33%|███▎      | 337/1024 [00:00<00:01, 493.92it/s]
Adding requests:  38%|███▊      | 389/1024 [00:00<00:01, 502.08it/s]
Adding requests:  43%|████▎     | 440/1024 [00:00<00:01, 502.87it/s]
Adding requests:  48%|████▊     | 491/1024 [00:01<00:01, 503.93it/s]
Adding requests:  53%|█████▎    | 542/1024 [00:01<00:00, 493.83it/s]
Adding requests:  58%|█████▊    | 595/1024 [00:01<00:00, 504.34it/s]
Adding requests:  63%|██████▎   | 646/1024 [00:01<00:00, 501.84it/s]
Adding requests:  68%|██████▊   | 700/1024 [00:01<00:00, 511.24it/s]
Adding requests:  73%|███████▎  | 752/1024 [00:01<00:00, 510.74it/s]
Adding requests:  79%|███████▊  | 804/1024 [00:01<00:00, 507.89it/s]
Adding requests:  83%|████████▎ | 855/1024 [00:01<00:00, 501.91it/s]
Adding requests:  89%|████████▊ | 908/1024 [00:01<00:00, 508.69it/s]
Adding requests:  94%|█████████▍| 960/1024 [00:01<00:00, 510.65it/s]
Adding requests:  99%|█████████▉| 1012/1024 [00:02<00:00, 513.23it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 497.96it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  19%|█▉        | 194/1024 [00:00<00:00, 1042.76it/s, est. speed input: 1067920.56 toks/s, output: 1042.80 toks/s]
Processed prompts:  29%|██▉       | 299/1024 [00:01<00:03, 193.56it/s, est. speed input: 235542.73 toks/s, output: 230.02 toks/s]   
Processed prompts:  34%|███▍      | 348/1024 [00:01<00:04, 158.05it/s, est. speed input: 197368.90 toks/s, output: 192.74 toks/s]
Processed prompts:  37%|███▋      | 379/1024 [00:02<00:04, 140.34it/s, est. speed input: 180673.97 toks/s, output: 176.44 toks/s]
Processed prompts:  39%|███▉      | 402/1024 [00:02<00:04, 129.05it/s, est. speed input: 171078.44 toks/s, output: 167.07 toks/s]
Processed prompts:  41%|████      | 420/1024 [00:02<00:04, 124.93it/s, est. speed input: 166868.81 toks/s, output: 162.96 toks/s]
Processed prompts:  43%|████▎     | 436/1024 [00:02<00:04, 118.73it/s, est. speed input: 162444.58 toks/s, output: 158.64 toks/s]
Processed prompts:  44%|████▍     | 450/1024 [00:02<00:05, 111.89it/s, est. speed input: 158322.34 toks/s, output: 154.61 toks/s]
Processed prompts:  46%|████▌     | 466/1024 [00:03<00:05, 107.44it/s, est. speed input: 154845.25 toks/s, output: 151.21 toks/s]
Processed prompts:  47%|████▋     | 482/1024 [00:03<00:05, 103.81it/s, est. speed input: 151718.30 toks/s, output: 148.16 toks/s]
Processed prompts:  49%|████▊     | 498/1024 [00:03<00:05, 100.97it/s, est. speed input: 148900.36 toks/s, output: 145.41 toks/s]
Processed prompts:  50%|█████     | 514/1024 [00:03<00:05, 98.82it/s, est. speed input: 146350.81 toks/s, output: 142.92 toks/s] 
Processed prompts:  52%|█████▏    | 530/1024 [00:03<00:05, 97.27it/s, est. speed input: 144040.69 toks/s, output: 140.66 toks/s]
Processed prompts:  53%|█████▎    | 546/1024 [00:03<00:04, 96.12it/s, est. speed input: 141928.56 toks/s, output: 138.60 toks/s]
Processed prompts:  55%|█████▍    | 562/1024 [00:04<00:04, 95.21it/s, est. speed input: 139975.82 toks/s, output: 136.69 toks/s]
Processed prompts:  56%|█████▋    | 578/1024 [00:04<00:04, 94.63it/s, est. speed input: 138192.30 toks/s, output: 134.95 toks/s]
Processed prompts:  58%|█████▊    | 594/1024 [00:04<00:04, 94.36it/s, est. speed input: 136572.95 toks/s, output: 133.37 toks/s]
Processed prompts:  60%|█████▉    | 610/1024 [00:04<00:04, 93.97it/s, est. speed input: 135037.63 toks/s, output: 131.87 toks/s]
Processed prompts:  61%|██████    | 626/1024 [00:04<00:04, 93.75it/s, est. speed input: 133622.87 toks/s, output: 130.49 toks/s]
Processed prompts:  63%|██████▎   | 642/1024 [00:04<00:04, 93.62it/s, est. speed input: 132310.11 toks/s, output: 129.21 toks/s]
Processed prompts:  64%|██████▍   | 658/1024 [00:05<00:03, 93.52it/s, est. speed input: 131081.22 toks/s, output: 128.01 toks/s]
Processed prompts:  66%|██████▌   | 674/1024 [00:05<00:03, 93.36it/s, est. speed input: 129919.63 toks/s, output: 126.87 toks/s]
Processed prompts:  67%|██████▋   | 690/1024 [00:05<00:03, 93.25it/s, est. speed input: 128830.80 toks/s, output: 125.81 toks/s]
Processed prompts:  69%|██████▉   | 706/1024 [00:05<00:03, 93.25it/s, est. speed input: 127820.17 toks/s, output: 124.82 toks/s]
Processed prompts:  71%|███████   | 722/1024 [00:05<00:03, 93.21it/s, est. speed input: 126862.91 toks/s, output: 123.89 toks/s]
Processed prompts:  72%|███████▏  | 738/1024 [00:05<00:03, 93.18it/s, est. speed input: 125960.53 toks/s, output: 123.01 toks/s]
Processed prompts:  74%|███████▎  | 754/1024 [00:06<00:02, 93.27it/s, est. speed input: 125121.18 toks/s, output: 122.19 toks/s]
Processed prompts:  75%|███████▌  | 770/1024 [00:06<00:02, 93.14it/s, est. speed input: 124305.05 toks/s, output: 121.39 toks/s]
Processed prompts:  77%|███████▋  | 786/1024 [00:06<00:02, 93.21it/s, est. speed input: 123549.98 toks/s, output: 120.65 toks/s]
Processed prompts:  78%|███████▊  | 802/1024 [00:06<00:02, 93.16it/s, est. speed input: 122823.35 toks/s, output: 119.94 toks/s]
Processed prompts:  80%|███████▉  | 818/1024 [00:06<00:02, 93.20it/s, est. speed input: 122140.66 toks/s, output: 119.28 toks/s]
Processed prompts:  81%|████████▏ | 834/1024 [00:07<00:02, 93.15it/s, est. speed input: 121482.52 toks/s, output: 118.63 toks/s]
Processed prompts:  83%|████████▎ | 850/1024 [00:07<00:01, 93.10it/s, est. speed input: 120855.68 toks/s, output: 118.02 toks/s]
Processed prompts:  85%|████████▍ | 866/1024 [00:07<00:01, 93.10it/s, est. speed input: 120260.93 toks/s, output: 117.44 toks/s]
Processed prompts:  86%|████████▌ | 882/1024 [00:07<00:01, 90.62it/s, est. speed input: 119444.21 toks/s, output: 116.64 toks/s]
Processed prompts:  88%|████████▊ | 898/1024 [00:07<00:01, 93.49it/s, est. speed input: 119114.43 toks/s, output: 116.32 toks/s]
Processed prompts:  89%|████████▉ | 914/1024 [00:07<00:01, 93.40it/s, est. speed input: 118599.58 toks/s, output: 115.82 toks/s]
Processed prompts:  91%|█████████ | 930/1024 [00:08<00:01, 93.22it/s, est. speed input: 118095.25 toks/s, output: 115.33 toks/s]
Processed prompts:  92%|█████████▏| 946/1024 [00:08<00:00, 94.65it/s, est. speed input: 117747.27 toks/s, output: 114.99 toks/s]
Processed prompts:  94%|█████████▍| 962/1024 [00:08<00:00, 94.15it/s, est. speed input: 117285.62 toks/s, output: 114.54 toks/s]
Processed prompts:  96%|█████████▌| 978/1024 [00:08<00:00, 93.75it/s, est. speed input: 116838.70 toks/s, output: 114.10 toks/s]
Processed prompts:  97%|█████████▋| 994/1024 [00:08<00:00, 95.01it/s, est. speed input: 116532.84 toks/s, output: 113.80 toks/s]
Processed prompts:  99%|█████████▊| 1010/1024 [00:08<00:00, 94.30it/s, est. speed input: 116113.23 toks/s, output: 113.39 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:08<00:00, 94.30it/s, est. speed input: 116723.35 toks/s, output: 113.99 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:08<00:00, 113.98it/s, est. speed input: 116723.35 toks/s, output: 113.99 toks/s]
[rank0]:[W128 09:13:39.593974661 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 51.9s

测试结果:
  Requests/s:   92.74
  Tokens/s:     95055.82
  Total Reqs:   1024
  Elapsed:      11.04s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     94963.09

============================================================
[6/7] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 09:13:56 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3367492) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3367492) WARNING 01-28 09:14:11 [backends.py:609] Failed to read file <frozen os>
Throughput: 95.29 requests/s, 97676.82 total tokens/s, 95.29 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-28 09:13:56] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 09:13:56] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:13:56] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:13:56] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:13:56] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:13:56] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:13:56] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:13:56] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:13:56] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:13:56] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:13:56] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:13:56] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:13:56] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:13:56] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 09:14:03] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 09:14:03] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:14:03] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:14:03] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:14:03] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:14:03] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:14:03] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:14:03] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:14:03] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:14:03] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:14:03] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:14:03] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:14:03] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:14:03] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3367492) [2026-01-28 09:14:04] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3367492) [2026-01-28 09:14:04] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3367492) [2026-01-28 09:14:04] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3367492) [2026-01-28 09:14:04] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3367492) [2026-01-28 09:14:04] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3367492) [2026-01-28 09:14:04] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3367492) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3367492) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.39it/s]
(EngineCore_DP0 pid=3367492) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.39it/s]
(EngineCore_DP0 pid=3367492) 
(EngineCore_DP0 pid=3367492) [2026-01-28 09:14:05] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=3367492) [2026-01-28 09:14:05] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8232960 bytes
(EngineCore_DP0 pid=3367492) [2026-01-28 09:14:05] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=3367492) [2026-01-28 09:14:05] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5488640 bytes
(EngineCore_DP0 pid=3367492) [2026-01-28 09:14:05] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=3367492) [2026-01-28 09:14:05] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 29638656 bytes
(EngineCore_DP0 pid=3367492) [2026-01-28 09:14:05] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=3367492) [2026-01-28 09:14:05] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14745600 bytes
(EngineCore_DP0 pid=3367492) 2026-01-28 09:14:22,568 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3367492) 2026-01-28 09:14:22,597 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3367492) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 1/7 [00:00<00:01,  3.05it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:01,  4.94it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00,  8.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:00<00:00, 11.22it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00,  9.09it/s]
(EngineCore_DP0 pid=3367492) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00, 15.57it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00, 16.27it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 16.24it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 33/2048 [00:00<00:06, 326.26it/s]
Adding requests:   4%|▍         | 84/2048 [00:00<00:04, 430.34it/s]
Adding requests:   7%|▋         | 134/2048 [00:00<00:04, 460.11it/s]
Adding requests:   9%|▉         | 183/2048 [00:00<00:03, 470.14it/s]
Adding requests:  11%|█▏        | 233/2048 [00:00<00:03, 480.54it/s]
Adding requests:  14%|█▍        | 283/2048 [00:00<00:03, 483.84it/s]
Adding requests:  16%|█▌        | 332/2048 [00:00<00:03, 485.60it/s]
Adding requests:  19%|█▊        | 382/2048 [00:00<00:03, 488.18it/s]
Adding requests:  21%|██        | 432/2048 [00:00<00:03, 490.83it/s]
Adding requests:  24%|██▎       | 482/2048 [00:01<00:03, 491.11it/s]
Adding requests:  26%|██▌       | 532/2048 [00:01<00:03, 480.02it/s]
Adding requests:  29%|██▊       | 584/2048 [00:01<00:02, 490.07it/s]
Adding requests:  31%|███       | 635/2048 [00:01<00:02, 494.14it/s]
Adding requests:  34%|███▎      | 687/2048 [00:01<00:02, 500.76it/s]
Adding requests:  36%|███▌      | 738/2048 [00:01<00:02, 499.90it/s]
Adding requests:  39%|███▊      | 789/2048 [00:01<00:02, 493.77it/s]
Adding requests:  41%|████      | 839/2048 [00:01<00:02, 484.04it/s]
Adding requests:  44%|████▎     | 891/2048 [00:01<00:02, 494.12it/s]
Adding requests:  46%|████▌     | 942/2048 [00:01<00:02, 496.02it/s]
Adding requests:  49%|████▊     | 994/2048 [00:02<00:02, 501.65it/s]
Adding requests:  51%|█████     | 1046/2048 [00:02<00:01, 505.54it/s]
Adding requests:  54%|█████▎    | 1097/2048 [00:02<00:01, 501.67it/s]
Adding requests:  56%|█████▌    | 1148/2048 [00:02<00:01, 500.08it/s]
Adding requests:  59%|█████▊    | 1202/2048 [00:02<00:01, 510.81it/s]
Adding requests:  61%|██████    | 1254/2048 [00:02<00:01, 502.73it/s]
Adding requests:  64%|██████▎   | 1305/2048 [00:02<00:01, 500.69it/s]
Adding requests:  66%|██████▌   | 1356/2048 [00:02<00:01, 500.61it/s]
Adding requests:  69%|██████▉   | 1408/2048 [00:02<00:01, 503.40it/s]
Adding requests:  71%|███████   | 1459/2048 [00:02<00:01, 501.38it/s]
Adding requests:  74%|███████▎  | 1510/2048 [00:03<00:01, 502.59it/s]
Adding requests:  76%|███████▌  | 1561/2048 [00:03<00:00, 503.21it/s]
Adding requests:  79%|███████▉  | 1613/2048 [00:03<00:00, 506.47it/s]
Adding requests:  81%|████████▏ | 1664/2048 [00:03<00:00, 504.33it/s]
Adding requests:  84%|████████▎ | 1715/2048 [00:03<00:00, 501.82it/s]
Adding requests:  86%|████████▌ | 1766/2048 [00:03<00:00, 499.04it/s]
Adding requests:  89%|████████▊ | 1817/2048 [00:03<00:00, 500.62it/s]
Adding requests:  91%|█████████ | 1868/2048 [00:03<00:00, 499.72it/s]
Adding requests:  94%|█████████▎| 1918/2048 [00:03<00:00, 493.03it/s]
Adding requests:  96%|█████████▌| 1968/2048 [00:03<00:00, 492.90it/s]
Adding requests:  99%|█████████▊| 2019/2048 [00:04<00:00, 497.63it/s]
Adding requests: 100%|██████████| 2048/2048 [00:04<00:00, 493.92it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  19%|█▉        | 386/2048 [00:00<00:00, 2623.50it/s, est. speed input: 2686890.92 toks/s, output: 2623.63 toks/s]
Processed prompts:  32%|███▏      | 649/2048 [00:02<00:07, 192.62it/s, est. speed input: 236313.24 toks/s, output: 230.77 toks/s]   
Processed prompts:  37%|███▋      | 763/2048 [00:03<00:08, 156.78it/s, est. speed input: 196195.47 toks/s, output: 191.60 toks/s]
Processed prompts:  41%|████      | 830/2048 [00:04<00:08, 143.47it/s, est. speed input: 182730.10 toks/s, output: 178.45 toks/s]
Processed prompts:  43%|████▎     | 875/2048 [00:05<00:08, 132.62it/s, est. speed input: 173869.19 toks/s, output: 169.79 toks/s]
Processed prompts:  44%|████▍     | 907/2048 [00:05<00:09, 126.62it/s, est. speed input: 169211.06 toks/s, output: 165.24 toks/s]
Processed prompts:  45%|████▌     | 931/2048 [00:05<00:09, 117.00it/s, est. speed input: 163984.36 toks/s, output: 160.14 toks/s]
Processed prompts:  46%|████▋     | 950/2048 [00:05<00:09, 116.43it/s, est. speed input: 162595.95 toks/s, output: 158.78 toks/s]
Processed prompts:  47%|████▋     | 967/2048 [00:06<00:09, 114.31it/s, est. speed input: 161007.09 toks/s, output: 157.23 toks/s]
Processed prompts:  48%|████▊     | 982/2048 [00:06<00:09, 111.14it/s, est. speed input: 159407.80 toks/s, output: 155.67 toks/s]
Processed prompts:  49%|████▊     | 995/2048 [00:06<00:10, 104.53it/s, est. speed input: 157346.24 toks/s, output: 153.66 toks/s]
Processed prompts:  49%|████▉     | 1010/2048 [00:06<00:10, 101.20it/s, est. speed input: 155687.50 toks/s, output: 152.04 toks/s]
Processed prompts:  50%|█████     | 1026/2048 [00:06<00:10, 99.86it/s, est. speed input: 154266.48 toks/s, output: 150.65 toks/s] 
Processed prompts:  51%|█████     | 1042/2048 [00:06<00:10, 98.72it/s, est. speed input: 152906.70 toks/s, output: 149.32 toks/s]
Processed prompts:  52%|█████▏    | 1058/2048 [00:07<00:10, 97.74it/s, est. speed input: 151597.64 toks/s, output: 148.04 toks/s]
Processed prompts:  52%|█████▏    | 1074/2048 [00:07<00:10, 97.17it/s, est. speed input: 150371.40 toks/s, output: 146.85 toks/s]
Processed prompts:  53%|█████▎    | 1090/2048 [00:07<00:09, 96.56it/s, est. speed input: 149177.86 toks/s, output: 145.68 toks/s]
Processed prompts:  54%|█████▍    | 1106/2048 [00:07<00:09, 96.41it/s, est. speed input: 148070.02 toks/s, output: 144.60 toks/s]
Processed prompts:  55%|█████▍    | 1122/2048 [00:07<00:09, 96.05it/s, est. speed input: 146982.13 toks/s, output: 143.54 toks/s]
Processed prompts:  56%|█████▌    | 1138/2048 [00:07<00:09, 95.90it/s, est. speed input: 145950.62 toks/s, output: 142.53 toks/s]
Processed prompts:  56%|█████▋    | 1154/2048 [00:08<00:09, 97.21it/s, est. speed input: 145108.95 toks/s, output: 141.71 toks/s]
Processed prompts:  57%|█████▋    | 1170/2048 [00:08<00:09, 96.63it/s, est. speed input: 144148.33 toks/s, output: 140.77 toks/s]
Processed prompts:  58%|█████▊    | 1186/2048 [00:08<00:08, 96.26it/s, est. speed input: 143229.98 toks/s, output: 139.87 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [00:08<00:08, 96.09it/s, est. speed input: 142354.42 toks/s, output: 139.02 toks/s]
Processed prompts:  59%|█████▉    | 1218/2048 [00:08<00:08, 95.91it/s, est. speed input: 141507.59 toks/s, output: 138.19 toks/s]
Processed prompts:  60%|██████    | 1234/2048 [00:08<00:08, 95.64it/s, est. speed input: 140678.72 toks/s, output: 137.38 toks/s]
Processed prompts:  61%|██████    | 1250/2048 [00:09<00:08, 95.59it/s, est. speed input: 139892.31 toks/s, output: 136.61 toks/s]
Processed prompts:  62%|██████▏   | 1266/2048 [00:09<00:08, 97.07it/s, est. speed input: 139265.00 toks/s, output: 136.00 toks/s]
Processed prompts:  63%|██████▎   | 1282/2048 [00:09<00:07, 96.58it/s, est. speed input: 138530.32 toks/s, output: 135.28 toks/s]
Processed prompts:  63%|██████▎   | 1298/2048 [00:09<00:07, 96.30it/s, est. speed input: 137827.33 toks/s, output: 134.60 toks/s]
Processed prompts:  64%|██████▍   | 1314/2048 [00:09<00:07, 96.03it/s, est. speed input: 137140.55 toks/s, output: 133.93 toks/s]
Processed prompts:  65%|██████▍   | 1330/2048 [00:09<00:07, 95.88it/s, est. speed input: 136480.44 toks/s, output: 133.28 toks/s]
Processed prompts:  66%|██████▌   | 1346/2048 [00:10<00:07, 95.41it/s, est. speed input: 135814.25 toks/s, output: 132.63 toks/s]
Processed prompts:  67%|██████▋   | 1362/2048 [00:10<00:07, 95.46it/s, est. speed input: 135198.57 toks/s, output: 132.03 toks/s]
Processed prompts:  67%|██████▋   | 1378/2048 [00:10<00:07, 95.30it/s, est. speed input: 134587.46 toks/s, output: 131.43 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [00:10<00:06, 95.42it/s, est. speed input: 134013.59 toks/s, output: 130.87 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [00:10<00:06, 95.46it/s, est. speed input: 133453.97 toks/s, output: 130.33 toks/s]
Processed prompts:  70%|██████▉   | 1426/2048 [00:10<00:06, 95.36it/s, est. speed input: 132901.90 toks/s, output: 129.79 toks/s]
Processed prompts:  70%|███████   | 1442/2048 [00:11<00:06, 95.22it/s, est. speed input: 132362.05 toks/s, output: 129.26 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [00:11<00:06, 95.20it/s, est. speed input: 131843.63 toks/s, output: 128.75 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [00:11<00:06, 95.20it/s, est. speed input: 131340.41 toks/s, output: 128.26 toks/s]
Processed prompts:  73%|███████▎  | 1490/2048 [00:11<00:05, 95.21it/s, est. speed input: 130853.25 toks/s, output: 127.79 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [00:11<00:05, 95.27it/s, est. speed input: 130383.22 toks/s, output: 127.33 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [00:11<00:05, 95.33it/s, est. speed input: 129927.25 toks/s, output: 126.88 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [00:12<00:05, 95.24it/s, est. speed input: 129476.11 toks/s, output: 126.44 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [00:12<00:05, 95.20it/s, est. speed input: 129038.45 toks/s, output: 126.01 toks/s]
Processed prompts:  77%|███████▋  | 1570/2048 [00:12<00:05, 95.19it/s, est. speed input: 128613.28 toks/s, output: 125.60 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [00:12<00:04, 96.70it/s, est. speed input: 128288.55 toks/s, output: 125.28 toks/s]
Processed prompts:  78%|███████▊  | 1602/2048 [00:12<00:04, 96.20it/s, est. speed input: 127882.30 toks/s, output: 124.88 toks/s]
Processed prompts:  79%|███████▉  | 1618/2048 [00:12<00:04, 95.96it/s, est. speed input: 127492.91 toks/s, output: 124.50 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [00:13<00:04, 95.66it/s, est. speed input: 127105.72 toks/s, output: 124.13 toks/s]
Processed prompts:  81%|████████  | 1650/2048 [00:13<00:04, 95.59it/s, est. speed input: 126736.26 toks/s, output: 123.77 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [00:13<00:04, 95.38it/s, est. speed input: 126366.77 toks/s, output: 123.40 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [00:13<00:03, 95.28it/s, est. speed input: 126009.27 toks/s, output: 123.06 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [00:13<00:03, 95.30it/s, est. speed input: 125665.22 toks/s, output: 122.72 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [00:14<00:03, 95.16it/s, est. speed input: 125321.12 toks/s, output: 122.38 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [00:14<00:03, 95.30it/s, est. speed input: 124997.75 toks/s, output: 122.07 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [00:14<00:03, 95.10it/s, est. speed input: 124666.67 toks/s, output: 121.74 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [00:14<00:03, 95.14it/s, est. speed input: 124352.12 toks/s, output: 121.44 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [00:14<00:02, 95.11it/s, est. speed input: 124042.08 toks/s, output: 121.13 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [00:14<00:02, 95.04it/s, est. speed input: 123736.85 toks/s, output: 120.84 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [00:15<00:02, 95.06it/s, est. speed input: 123441.60 toks/s, output: 120.55 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [00:15<00:02, 95.05it/s, est. speed input: 123151.49 toks/s, output: 120.26 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [00:15<00:02, 95.06it/s, est. speed input: 122868.79 toks/s, output: 119.99 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [00:15<00:01, 95.01it/s, est. speed input: 122589.73 toks/s, output: 119.72 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [00:15<00:01, 96.57it/s, est. speed input: 122388.98 toks/s, output: 119.52 toks/s]
Processed prompts:  92%|█████████▏| 1890/2048 [00:15<00:01, 96.15it/s, est. speed input: 122124.46 toks/s, output: 119.26 toks/s]
Processed prompts:  93%|█████████▎| 1906/2048 [00:16<00:01, 95.77it/s, est. speed input: 121861.80 toks/s, output: 119.01 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [00:16<00:01, 95.53it/s, est. speed input: 121605.83 toks/s, output: 118.76 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [00:16<00:01, 95.51it/s, est. speed input: 121361.20 toks/s, output: 118.52 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [00:16<00:00, 96.93it/s, est. speed input: 121182.36 toks/s, output: 118.34 toks/s]
Processed prompts:  96%|█████████▌| 1970/2048 [00:16<00:00, 96.33it/s, est. speed input: 120940.62 toks/s, output: 118.11 toks/s]
Processed prompts:  97%|█████████▋| 1986/2048 [00:16<00:00, 95.93it/s, est. speed input: 120704.14 toks/s, output: 117.87 toks/s]
Processed prompts:  98%|█████████▊| 2002/2048 [00:17<00:00, 95.71it/s, est. speed input: 120474.79 toks/s, output: 117.65 toks/s]
Processed prompts:  99%|█████████▊| 2018/2048 [00:17<00:00, 95.40it/s, est. speed input: 120243.37 toks/s, output: 117.42 toks/s]
Processed prompts:  99%|█████████▉| 2034/2048 [00:17<00:00, 97.33it/s, est. speed input: 120102.31 toks/s, output: 117.29 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:17<00:00, 97.33it/s, est. speed input: 120925.15 toks/s, output: 118.09 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:17<00:00, 118.09it/s, est. speed input: 120925.15 toks/s, output: 118.09 toks/s]
[rank0]:[W128 09:14:47.082932629 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 67.5s

测试结果:
  Requests/s:   95.29
  Tokens/s:     97676.82
  Total Reqs:   2048
  Elapsed:      21.49s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     97581.52

============================================================
[7/7] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 09:15:12 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3368986) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3368986) WARNING 01-28 09:15:28 [backends.py:609] Failed to read file <frozen os>
Throughput: 98.62 requests/s, 101082.11 total tokens/s, 98.62 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-28 09:15:12] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 09:15:12] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:15:12] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:15:12] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:15:12] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:15:12] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:15:12] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:15:12] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:15:12] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:15:12] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:15:12] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:15:12] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:15:12] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:15:12] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 09:15:19] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 09:15:19] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:15:19] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:15:19] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:15:19] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:15:19] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:15:19] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:15:19] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:15:19] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:15:19] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:15:19] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:15:19] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:15:19] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:15:19] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3368986) [2026-01-28 09:15:20] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3368986) [2026-01-28 09:15:20] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3368986) [2026-01-28 09:15:20] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3368986) [2026-01-28 09:15:20] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3368986) [2026-01-28 09:15:20] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3368986) [2026-01-28 09:15:20] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3368986) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3368986) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.42it/s]
(EngineCore_DP0 pid=3368986) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.42it/s]
(EngineCore_DP0 pid=3368986) 
(EngineCore_DP0 pid=3368986) [2026-01-28 09:15:22] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=3368986) [2026-01-28 09:15:22] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8232960 bytes
(EngineCore_DP0 pid=3368986) [2026-01-28 09:15:22] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=3368986) [2026-01-28 09:15:22] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5488640 bytes
(EngineCore_DP0 pid=3368986) [2026-01-28 09:15:22] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=3368986) [2026-01-28 09:15:22] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 29638656 bytes
(EngineCore_DP0 pid=3368986) [2026-01-28 09:15:22] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=3368986) [2026-01-28 09:15:22] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14745600 bytes
(EngineCore_DP0 pid=3368986) [rank0]:W0128 09:15:34.431000 3368986 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3368986) [rank0]:W0128 09:15:34.511000 3368986 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3368986) [rank0]:W0128 09:15:35.573000 3368986 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3368986) [rank0]:W0128 09:15:35.701000 3368986 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3368986) 2026-01-28 09:15:39,551 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3368986) 2026-01-28 09:15:39,591 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3368986) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▉         | 1/11 [00:00<00:01,  9.80it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:01,  5.29it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 3/11 [00:00<00:01,  6.60it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 5/11 [00:00<00:00,  9.77it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▎   | 7/11 [00:00<00:00, 11.81it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 9/11 [00:00<00:00, 13.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 13.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 11.22it/s]
(EngineCore_DP0 pid=3368986) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00, 15.46it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00, 16.14it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00, 16.22it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 16.17it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 36/4096 [00:00<00:11, 354.37it/s]
Adding requests:   2%|▏         | 88/4096 [00:00<00:08, 445.85it/s]
Adding requests:   3%|▎         | 139/4096 [00:00<00:08, 472.34it/s]
Adding requests:   5%|▍         | 188/4096 [00:00<00:08, 478.92it/s]
Adding requests:   6%|▌         | 240/4096 [00:00<00:07, 492.69it/s]
Adding requests:   7%|▋         | 291/4096 [00:00<00:07, 495.93it/s]
Adding requests:   8%|▊         | 341/4096 [00:00<00:07, 495.67it/s]
Adding requests:  10%|▉         | 393/4096 [00:00<00:07, 502.82it/s]
Adding requests:  11%|█         | 444/4096 [00:00<00:07, 502.76it/s]
Adding requests:  12%|█▏        | 495/4096 [00:01<00:07, 503.49it/s]
Adding requests:  13%|█▎        | 546/4096 [00:01<00:07, 493.94it/s]
Adding requests:  15%|█▍        | 598/4096 [00:01<00:06, 500.72it/s]
Adding requests:  16%|█▌        | 650/4096 [00:01<00:06, 505.67it/s]
Adding requests:  17%|█▋        | 703/4096 [00:01<00:06, 512.47it/s]
Adding requests:  18%|█▊        | 755/4096 [00:01<00:06, 510.03it/s]
Adding requests:  20%|█▉        | 807/4096 [00:01<00:06, 501.83it/s]
Adding requests:  21%|██        | 858/4096 [00:01<00:06, 500.45it/s]
Adding requests:  22%|██▏       | 911/4096 [00:01<00:06, 507.62it/s]
Adding requests:  24%|██▎       | 964/4096 [00:01<00:06, 511.13it/s]
Adding requests:  25%|██▍       | 1017/4096 [00:02<00:05, 514.51it/s]
Adding requests:  26%|██▌       | 1069/4096 [00:02<00:05, 514.90it/s]
Adding requests:  27%|██▋       | 1121/4096 [00:02<00:06, 493.46it/s]
Adding requests:  29%|██▊       | 1175/4096 [00:02<00:05, 504.50it/s]
Adding requests:  30%|███       | 1229/4096 [00:02<00:05, 514.04it/s]
Adding requests:  31%|███▏      | 1281/4096 [00:02<00:05, 510.71it/s]
Adding requests:  33%|███▎      | 1335/4096 [00:02<00:05, 518.39it/s]
Adding requests:  34%|███▍      | 1388/4096 [00:02<00:05, 519.50it/s]
Adding requests:  35%|███▌      | 1441/4096 [00:02<00:05, 519.56it/s]
Adding requests:  36%|███▋      | 1495/4096 [00:02<00:04, 523.91it/s]
Adding requests:  38%|███▊      | 1548/4096 [00:03<00:04, 525.06it/s]
Adding requests:  39%|███▉      | 1603/4096 [00:03<00:04, 531.26it/s]
Adding requests:  40%|████      | 1657/4096 [00:03<00:04, 528.04it/s]
Adding requests:  42%|████▏     | 1710/4096 [00:03<00:04, 524.85it/s]
Adding requests:  43%|████▎     | 1763/4096 [00:03<00:04, 525.00it/s]
Adding requests:  44%|████▍     | 1816/4096 [00:03<00:04, 525.05it/s]
Adding requests:  46%|████▌     | 1869/4096 [00:03<00:04, 519.87it/s]
Adding requests:  47%|████▋     | 1922/4096 [00:03<00:04, 522.16it/s]
Adding requests:  48%|████▊     | 1975/4096 [00:03<00:04, 519.65it/s]
Adding requests:  50%|████▉     | 2029/4096 [00:03<00:03, 523.93it/s]
Adding requests:  51%|█████     | 2082/4096 [00:04<00:03, 525.11it/s]
Adding requests:  52%|█████▏    | 2135/4096 [00:04<00:03, 519.39it/s]
Adding requests:  53%|█████▎    | 2187/4096 [00:04<00:03, 514.18it/s]
Adding requests:  55%|█████▍    | 2241/4096 [00:04<00:03, 520.80it/s]
Adding requests:  56%|█████▌    | 2294/4096 [00:04<00:03, 505.83it/s]
Adding requests:  57%|█████▋    | 2347/4096 [00:04<00:03, 511.05it/s]
Adding requests:  59%|█████▊    | 2399/4096 [00:04<00:03, 513.20it/s]
Adding requests:  60%|█████▉    | 2451/4096 [00:04<00:03, 514.41it/s]
Adding requests:  61%|██████    | 2503/4096 [00:04<00:03, 515.13it/s]
Adding requests:  62%|██████▏   | 2557/4096 [00:05<00:02, 521.92it/s]
Adding requests:  64%|██████▎   | 2610/4096 [00:05<00:02, 522.25it/s]
Adding requests:  65%|██████▌   | 2664/4096 [00:05<00:02, 526.82it/s]
Adding requests:  66%|██████▋   | 2717/4096 [00:05<00:02, 520.38it/s]
Adding requests:  68%|██████▊   | 2770/4096 [00:05<00:02, 522.65it/s]
Adding requests:  69%|██████▉   | 2823/4096 [00:05<00:02, 516.52it/s]
Adding requests:  70%|███████   | 2877/4096 [00:05<00:02, 520.77it/s]
Adding requests:  72%|███████▏  | 2930/4096 [00:05<00:02, 518.48it/s]
Adding requests:  73%|███████▎  | 2982/4096 [00:05<00:02, 517.42it/s]
Adding requests:  74%|███████▍  | 3035/4096 [00:05<00:02, 518.90it/s]
Adding requests:  75%|███████▌  | 3087/4096 [00:06<00:01, 517.98it/s]
Adding requests:  77%|███████▋  | 3139/4096 [00:06<00:01, 517.17it/s]
Adding requests:  78%|███████▊  | 3191/4096 [00:06<00:01, 517.39it/s]
Adding requests:  79%|███████▉  | 3243/4096 [00:06<00:01, 514.90it/s]
Adding requests:  80%|████████  | 3296/4096 [00:06<00:01, 517.41it/s]
Adding requests:  82%|████████▏ | 3349/4096 [00:06<00:01, 519.81it/s]
Adding requests:  83%|████████▎ | 3401/4096 [00:06<00:01, 518.09it/s]
Adding requests:  84%|████████▍ | 3453/4096 [00:06<00:01, 518.26it/s]
Adding requests:  86%|████████▌ | 3505/4096 [00:06<00:01, 513.47it/s]
Adding requests:  87%|████████▋ | 3558/4096 [00:06<00:01, 516.42it/s]
Adding requests:  88%|████████▊ | 3610/4096 [00:07<00:00, 500.46it/s]
Adding requests:  89%|████████▉ | 3661/4096 [00:07<00:00, 501.52it/s]
Adding requests:  91%|█████████ | 3714/4096 [00:07<00:00, 509.65it/s]
Adding requests:  92%|█████████▏| 3768/4096 [00:07<00:00, 515.80it/s]
Adding requests:  93%|█████████▎| 3822/4096 [00:07<00:00, 520.50it/s]
Adding requests:  95%|█████████▍| 3877/4096 [00:07<00:00, 527.52it/s]
Adding requests:  96%|█████████▌| 3930/4096 [00:07<00:00, 524.88it/s]
Adding requests:  97%|█████████▋| 3983/4096 [00:07<00:00, 523.77it/s]
Adding requests:  99%|█████████▊| 4036/4096 [00:07<00:00, 521.11it/s]
Adding requests: 100%|█████████▉| 4089/4096 [00:08<00:00, 439.10it/s]
Adding requests: 100%|██████████| 4096/4096 [00:08<00:00, 509.26it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  19%|█▊        | 763/4096 [00:00<00:00, 6602.12it/s, est. speed input: 6763044.05 toks/s, output: 6602.78 toks/s]
Processed prompts:  35%|███▍      | 1424/4096 [00:06<00:14, 182.93it/s, est. speed input: 222020.25 toks/s, output: 216.82 toks/s]  
Processed prompts:  42%|████▏     | 1702/4096 [00:09<00:16, 148.76it/s, est. speed input: 183705.63 toks/s, output: 179.40 toks/s]
Processed prompts:  45%|████▌     | 1858/4096 [00:11<00:16, 136.44it/s, est. speed input: 171343.73 toks/s, output: 167.33 toks/s]
Processed prompts:  48%|████▊     | 1956/4096 [00:12<00:16, 130.54it/s, est. speed input: 165919.53 toks/s, output: 162.03 toks/s]
Processed prompts:  49%|████▉     | 2023/4096 [00:12<00:16, 126.72it/s, est. speed input: 162827.88 toks/s, output: 159.01 toks/s]
Processed prompts:  51%|█████     | 2071/4096 [00:13<00:15, 128.56it/s, est. speed input: 162521.83 toks/s, output: 158.71 toks/s]
Processed prompts:  51%|█████▏    | 2108/4096 [00:13<00:17, 113.85it/s, est. speed input: 157559.80 toks/s, output: 153.87 toks/s]
Processed prompts:  52%|█████▏    | 2139/4096 [00:14<00:17, 111.45it/s, est. speed input: 156161.30 toks/s, output: 152.50 toks/s]
Processed prompts:  53%|█████▎    | 2171/4096 [00:14<00:17, 109.41it/s, est. speed input: 154898.86 toks/s, output: 151.27 toks/s]
Processed prompts:  54%|█████▍    | 2203/4096 [00:14<00:17, 107.35it/s, est. speed input: 153686.58 toks/s, output: 150.08 toks/s]
Processed prompts:  55%|█████▍    | 2235/4096 [00:14<00:17, 106.66it/s, est. speed input: 152713.01 toks/s, output: 149.13 toks/s]
Processed prompts:  55%|█████▌    | 2267/4096 [00:15<00:17, 104.68it/s, est. speed input: 151597.19 toks/s, output: 148.04 toks/s]
Processed prompts:  56%|█████▌    | 2299/4096 [00:15<00:17, 103.68it/s, est. speed input: 150606.66 toks/s, output: 147.08 toks/s]
Processed prompts:  57%|█████▋    | 2331/4096 [00:15<00:17, 102.92it/s, est. speed input: 149660.38 toks/s, output: 146.15 toks/s]
Processed prompts:  58%|█████▊    | 2363/4096 [00:16<00:16, 103.24it/s, est. speed input: 148847.86 toks/s, output: 145.36 toks/s]
Processed prompts:  58%|█████▊    | 2395/4096 [00:16<00:16, 101.74it/s, est. speed input: 147892.06 toks/s, output: 144.43 toks/s]
Processed prompts:  59%|█████▉    | 2427/4096 [00:16<00:16, 100.77it/s, est. speed input: 146984.29 toks/s, output: 143.54 toks/s]
Processed prompts:  60%|██████    | 2459/4096 [00:17<00:16, 100.04it/s, est. speed input: 146108.46 toks/s, output: 142.68 toks/s]
Processed prompts:  61%|██████    | 2491/4096 [00:17<00:16, 100.23it/s, est. speed input: 145330.13 toks/s, output: 141.92 toks/s]
Processed prompts:  62%|██████▏   | 2523/4096 [00:17<00:15, 99.77it/s, est. speed input: 144526.64 toks/s, output: 141.14 toks/s] 
Processed prompts:  62%|██████▏   | 2555/4096 [00:18<00:15, 99.28it/s, est. speed input: 143737.31 toks/s, output: 140.37 toks/s]
Processed prompts:  63%|██████▎   | 2587/4096 [00:18<00:15, 99.92it/s, est. speed input: 143059.02 toks/s, output: 139.71 toks/s]
Processed prompts:  64%|██████▍   | 2619/4096 [00:18<00:14, 99.58it/s, est. speed input: 142338.45 toks/s, output: 139.00 toks/s]
Processed prompts:  65%|██████▍   | 2651/4096 [00:19<00:14, 99.26it/s, est. speed input: 141635.87 toks/s, output: 138.32 toks/s]
Processed prompts:  66%|██████▌   | 2683/4096 [00:19<00:14, 99.24it/s, est. speed input: 140972.41 toks/s, output: 137.67 toks/s]
Processed prompts:  66%|██████▋   | 2715/4096 [00:19<00:13, 99.11it/s, est. speed input: 140322.23 toks/s, output: 137.03 toks/s]
Processed prompts:  67%|██████▋   | 2747/4096 [00:20<00:13, 99.02it/s, est. speed input: 139692.91 toks/s, output: 136.42 toks/s]
Processed prompts:  68%|██████▊   | 2779/4096 [00:20<00:13, 98.88it/s, est. speed input: 139077.36 toks/s, output: 135.82 toks/s]
Processed prompts:  69%|██████▊   | 2811/4096 [00:20<00:12, 98.92it/s, est. speed input: 138491.75 toks/s, output: 135.25 toks/s]
Processed prompts:  69%|██████▉   | 2843/4096 [00:21<00:12, 98.72it/s, est. speed input: 137906.94 toks/s, output: 134.67 toks/s]
Processed prompts:  70%|███████   | 2875/4096 [00:21<00:12, 98.57it/s, est. speed input: 137339.70 toks/s, output: 134.12 toks/s]
Processed prompts:  71%|███████   | 2907/4096 [00:21<00:12, 98.56it/s, est. speed input: 136796.27 toks/s, output: 133.59 toks/s]
Processed prompts:  72%|███████▏  | 2939/4096 [00:22<00:11, 98.56it/s, est. speed input: 136268.63 toks/s, output: 133.07 toks/s]
Processed prompts:  73%|███████▎  | 2971/4096 [00:22<00:11, 98.65it/s, est. speed input: 135762.36 toks/s, output: 132.58 toks/s]
Processed prompts:  73%|███████▎  | 3003/4096 [00:22<00:11, 98.52it/s, est. speed input: 135257.98 toks/s, output: 132.09 toks/s]
Processed prompts:  74%|███████▍  | 3035/4096 [00:23<00:10, 98.46it/s, est. speed input: 134769.93 toks/s, output: 131.61 toks/s]
Processed prompts:  75%|███████▍  | 3067/4096 [00:23<00:10, 98.50it/s, est. speed input: 134300.57 toks/s, output: 131.15 toks/s]
Processed prompts:  76%|███████▌  | 3099/4096 [00:23<00:10, 98.48it/s, est. speed input: 133841.55 toks/s, output: 130.70 toks/s]
Processed prompts:  76%|███████▋  | 3131/4096 [00:24<00:09, 99.23it/s, est. speed input: 133441.13 toks/s, output: 130.31 toks/s]
Processed prompts:  77%|███████▋  | 3163/4096 [00:24<00:09, 98.98it/s, est. speed input: 133004.32 toks/s, output: 129.89 toks/s]
Processed prompts:  78%|███████▊  | 3195/4096 [00:24<00:09, 98.70it/s, est. speed input: 132573.47 toks/s, output: 129.47 toks/s]
Processed prompts:  79%|███████▉  | 3227/4096 [00:25<00:08, 98.63it/s, est. speed input: 132160.97 toks/s, output: 129.06 toks/s]
Processed prompts:  80%|███████▉  | 3259/4096 [00:25<00:08, 98.56it/s, est. speed input: 131757.78 toks/s, output: 128.67 toks/s]
Processed prompts:  80%|████████  | 3291/4096 [00:25<00:08, 98.63it/s, est. speed input: 131371.52 toks/s, output: 128.29 toks/s]
Processed prompts:  81%|████████  | 3323/4096 [00:25<00:07, 98.15it/s, est. speed input: 130965.08 toks/s, output: 127.90 toks/s]
Processed prompts:  82%|████████▏ | 3355/4096 [00:26<00:07, 98.25it/s, est. speed input: 130593.27 toks/s, output: 127.53 toks/s]
Processed prompts:  83%|████████▎ | 3387/4096 [00:26<00:07, 98.47it/s, est. speed input: 130238.26 toks/s, output: 127.19 toks/s]
Processed prompts:  83%|████████▎ | 3419/4096 [00:26<00:06, 98.26it/s, est. speed input: 129872.77 toks/s, output: 126.83 toks/s]
Processed prompts:  84%|████████▍ | 3451/4096 [00:27<00:06, 98.35it/s, est. speed input: 129528.47 toks/s, output: 126.49 toks/s]
Processed prompts:  85%|████████▌ | 3483/4096 [00:27<00:06, 100.28it/s, est. speed input: 129286.77 toks/s, output: 126.26 toks/s]
Processed prompts:  86%|████████▌ | 3515/4096 [00:27<00:05, 99.72it/s, est. speed input: 128954.60 toks/s, output: 125.93 toks/s] 
Processed prompts:  87%|████████▋ | 3547/4096 [00:28<00:05, 99.16it/s, est. speed input: 128621.99 toks/s, output: 125.61 toks/s]
Processed prompts:  87%|████████▋ | 3579/4096 [00:28<00:05, 99.21it/s, est. speed input: 128318.25 toks/s, output: 125.31 toks/s]
Processed prompts:  88%|████████▊ | 3611/4096 [00:28<00:04, 98.90it/s, est. speed input: 128004.96 toks/s, output: 125.00 toks/s]
Processed prompts:  89%|████████▉ | 3643/4096 [00:29<00:04, 98.68it/s, est. speed input: 127698.13 toks/s, output: 124.71 toks/s]
Processed prompts:  90%|████████▉ | 3675/4096 [00:29<00:04, 98.50it/s, est. speed input: 127397.24 toks/s, output: 124.41 toks/s]
Processed prompts:  91%|█████████ | 3707/4096 [00:29<00:03, 98.48it/s, est. speed input: 127107.48 toks/s, output: 124.13 toks/s]
Processed prompts:  91%|█████████▏| 3739/4096 [00:30<00:03, 99.10it/s, est. speed input: 126853.18 toks/s, output: 123.88 toks/s]
Processed prompts:  92%|█████████▏| 3771/4096 [00:30<00:03, 98.95it/s, est. speed input: 126577.91 toks/s, output: 123.61 toks/s]
Processed prompts:  93%|█████████▎| 3803/4096 [00:30<00:02, 98.70it/s, est. speed input: 126302.01 toks/s, output: 123.34 toks/s]
Processed prompts:  94%|█████████▎| 3835/4096 [00:31<00:02, 99.18it/s, est. speed input: 126060.43 toks/s, output: 123.11 toks/s]
Processed prompts:  94%|█████████▍| 3867/4096 [00:31<00:02, 98.82it/s, est. speed input: 125793.85 toks/s, output: 122.85 toks/s]
Processed prompts:  95%|█████████▌| 3899/4096 [00:31<00:01, 98.72it/s, est. speed input: 125538.74 toks/s, output: 122.60 toks/s]
Processed prompts:  96%|█████████▌| 3931/4096 [00:32<00:01, 98.64it/s, est. speed input: 125288.65 toks/s, output: 122.35 toks/s]
Processed prompts:  97%|█████████▋| 3963/4096 [00:32<00:01, 98.55it/s, est. speed input: 125042.28 toks/s, output: 122.11 toks/s]
Processed prompts:  98%|█████████▊| 3995/4096 [00:32<00:01, 98.34it/s, est. speed input: 124794.24 toks/s, output: 121.87 toks/s]
Processed prompts:  98%|█████████▊| 4027/4096 [00:33<00:00, 99.11it/s, est. speed input: 124589.05 toks/s, output: 121.67 toks/s]
Processed prompts:  99%|█████████▉| 4059/4096 [00:33<00:00, 99.19it/s, est. speed input: 124369.07 toks/s, output: 121.45 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:33<00:00, 99.19it/s, est. speed input: 125247.23 toks/s, output: 122.31 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:33<00:00, 122.31it/s, est. speed input: 125247.23 toks/s, output: 122.31 toks/s]
[rank0]:[W128 09:16:24.507104356 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 97.5s

测试结果:
  Requests/s:   98.62
  Tokens/s:     101082.11
  Total Reqs:   4096
  Elapsed:      41.53s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     100983.50


------------------------------------------------------------
  生成 CSV: BitNet-2B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/BitNet-2B-FP8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,32.1474,16491.6062,3.9817
1024,1024,1,128,128,32.1692,32973.4621,3.9790
2048,1024,2,256,128,63.4765,65063.4065,4.0330
4096,1024,4,512,128,81.8773,83924.1943,6.2533
8192,1024,8,1024,128,92.7374,95055.8240,11.0419
16384,1024,16,2048,128,95.2945,97676.8177,21.4913
32768,1024,32,4096,128,98.6167,101082.1149,41.5345

------------------------------------------------------------

[INFO] 完成: 7 成功, 0 失败

============================================================
  BitNet-2B-FP8 | cuSPARSELt (2_8) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_8
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8

============================================================
[1/7] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 09:16:33 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3370551) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3370551) WARNING 01-28 09:16:48 [backends.py:609] Failed to read file <frozen os>
Throughput: 31.74 requests/s, 16284.15 total tokens/s, 31.74 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-28 09:16:33] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 09:16:33] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:16:33] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:16:33] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:16:33] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:16:33] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:16:33] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:16:33] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:16:33] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:16:33] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:16:33] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:16:33] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:16:33] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:16:33] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 09:16:39] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 09:16:40] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:16:40] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:16:40] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:16:40] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:16:40] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:16:40] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:16:40] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:16:40] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:16:40] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:16:40] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:16:40] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:16:40] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:16:40] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3370551) [2026-01-28 09:16:41] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3370551) [2026-01-28 09:16:41] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3370551) [2026-01-28 09:16:41] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3370551) [2026-01-28 09:16:41] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3370551) [2026-01-28 09:16:41] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3370551) [2026-01-28 09:16:41] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3370551) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3370551) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.17it/s]
(EngineCore_DP0 pid=3370551) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.17it/s]
(EngineCore_DP0 pid=3370551) 
(EngineCore_DP0 pid=3370551) [2026-01-28 09:16:42] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=3370551) [2026-01-28 09:16:42] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9216000 bytes
(EngineCore_DP0 pid=3370551) [2026-01-28 09:16:42] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=3370551) [2026-01-28 09:16:42] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6144000 bytes
(EngineCore_DP0 pid=3370551) [2026-01-28 09:16:42] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=3370551) [2026-01-28 09:16:42] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 33177600 bytes
(EngineCore_DP0 pid=3370551) [2026-01-28 09:16:42] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=3370551) [2026-01-28 09:16:42] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16588800 bytes
(EngineCore_DP0 pid=3370551) 2026-01-28 09:16:59,599 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3370551) 2026-01-28 09:16:59,627 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3370551) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  3.24it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.23it/s]
(EngineCore_DP0 pid=3370551) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 15.43it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  41%|████▏     | 53/128 [00:00<00:00, 524.30it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 693.64it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:17,  7.20it/s, est. speed input: 3688.92 toks/s, output: 7.20 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:05, 22.03it/s, est. speed input: 10039.91 toks/s, output: 19.61 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:04, 27.35it/s, est. speed input: 12381.89 toks/s, output: 24.18 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:03, 29.38it/s, est. speed input: 13433.91 toks/s, output: 26.24 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:03, 30.23it/s, est. speed input: 14002.41 toks/s, output: 27.35 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:03, 30.48it/s, est. speed input: 14319.04 toks/s, output: 27.97 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:00<00:03, 31.36it/s, est. speed input: 14693.11 toks/s, output: 28.70 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:00<00:03, 32.59it/s, est. speed input: 15091.05 toks/s, output: 29.47 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:01<00:02, 33.48it/s, est. speed input: 15411.06 toks/s, output: 30.10 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:01<00:02, 34.08it/s, est. speed input: 15669.27 toks/s, output: 30.60 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:01<00:02, 34.50it/s, est. speed input: 15881.63 toks/s, output: 31.02 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:01<00:02, 34.73it/s, est. speed input: 16053.05 toks/s, output: 31.35 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:01<00:02, 34.62it/s, est. speed input: 16168.58 toks/s, output: 31.58 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:01<00:02, 34.53it/s, est. speed input: 16267.63 toks/s, output: 31.77 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:01<00:02, 34.53it/s, est. speed input: 16359.50 toks/s, output: 31.95 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:01<00:01, 34.54it/s, est. speed input: 16440.48 toks/s, output: 32.11 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:02<00:01, 34.51it/s, est. speed input: 16508.96 toks/s, output: 32.24 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:02<00:01, 34.51it/s, est. speed input: 16572.52 toks/s, output: 32.37 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:02<00:01, 34.51it/s, est. speed input: 16628.63 toks/s, output: 32.48 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:02<00:01, 34.41it/s, est. speed input: 16671.97 toks/s, output: 32.56 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:02<00:01, 34.43it/s, est. speed input: 16718.05 toks/s, output: 32.65 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:02<00:01, 34.48it/s, est. speed input: 16762.22 toks/s, output: 32.74 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:02<00:01, 34.54it/s, est. speed input: 16804.54 toks/s, output: 32.82 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:02<00:01, 34.55it/s, est. speed input: 16841.19 toks/s, output: 32.89 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:02<00:00, 34.48it/s, est. speed input: 16869.92 toks/s, output: 32.95 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:03<00:00, 34.42it/s, est. speed input: 16896.15 toks/s, output: 33.00 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:03<00:00, 34.37it/s, est. speed input: 16919.84 toks/s, output: 33.05 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:03<00:00, 34.36it/s, est. speed input: 16942.88 toks/s, output: 33.09 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:03<00:00, 34.34it/s, est. speed input: 16964.07 toks/s, output: 33.13 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:03<00:00, 34.41it/s, est. speed input: 16988.05 toks/s, output: 33.18 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:03<00:00, 34.44it/s, est. speed input: 17009.85 toks/s, output: 33.22 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:03<00:00, 34.40it/s, est. speed input: 17027.31 toks/s, output: 33.26 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 34.40it/s, est. speed input: 17039.62 toks/s, output: 33.28 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 33.28it/s, est. speed input: 17039.62 toks/s, output: 33.28 toks/s]
[rank0]:[W128 09:17:06.177364600 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 41.5s

测试结果:
  Requests/s:   31.74
  Tokens/s:     16284.15
  Total Reqs:   128
  Elapsed:      4.03s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     16252.41

============================================================
[2/7] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 09:17:15 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3371678) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3371678) WARNING 01-28 09:17:30 [backends.py:609] Failed to read file <frozen os>
Throughput: 31.83 requests/s, 32623.62 total tokens/s, 31.83 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-28 09:17:14] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 09:17:15] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:17:15] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:17:15] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:17:15] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:17:15] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:17:15] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:17:15] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:17:15] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:17:15] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:17:15] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:17:15] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:17:15] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:17:15] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 09:17:21] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 09:17:21] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:17:21] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:17:21] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:17:21] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:17:21] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:17:21] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:17:21] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:17:21] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:17:21] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:17:21] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:17:21] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:17:21] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:17:21] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3371678) [2026-01-28 09:17:22] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3371678) [2026-01-28 09:17:22] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3371678) [2026-01-28 09:17:22] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3371678) [2026-01-28 09:17:22] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3371678) [2026-01-28 09:17:22] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3371678) [2026-01-28 09:17:22] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3371678) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3371678) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.16it/s]
(EngineCore_DP0 pid=3371678) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.16it/s]
(EngineCore_DP0 pid=3371678) 
(EngineCore_DP0 pid=3371678) [2026-01-28 09:17:23] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=3371678) [2026-01-28 09:17:23] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9216000 bytes
(EngineCore_DP0 pid=3371678) [2026-01-28 09:17:23] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=3371678) [2026-01-28 09:17:23] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6144000 bytes
(EngineCore_DP0 pid=3371678) [2026-01-28 09:17:23] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=3371678) [2026-01-28 09:17:23] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 33177600 bytes
(EngineCore_DP0 pid=3371678) [2026-01-28 09:17:23] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=3371678) [2026-01-28 09:17:23] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16588800 bytes
(EngineCore_DP0 pid=3371678) 2026-01-28 09:17:41,389 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3371678) 2026-01-28 09:17:41,445 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3371678) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 12.65it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 12.63it/s]
(EngineCore_DP0 pid=3371678) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 15.37it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  23%|██▎       | 29/128 [00:00<00:00, 285.04it/s]
Adding requests:  63%|██████▎   | 81/128 [00:00<00:00, 420.93it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 432.20it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   3%|▎         | 4/128 [00:00<00:03, 35.55it/s, est. speed input: 36410.83 toks/s, output: 35.55 toks/s]
Processed prompts:   6%|▋         | 8/128 [00:00<00:03, 35.48it/s, est. speed input: 36342.97 toks/s, output: 35.49 toks/s]
Processed prompts:   9%|▉         | 12/128 [00:00<00:03, 35.29it/s, est. speed input: 36202.19 toks/s, output: 35.35 toks/s]
Processed prompts:  12%|█▎        | 16/128 [00:00<00:03, 35.25it/s, est. speed input: 36158.54 toks/s, output: 35.31 toks/s]
Processed prompts:  16%|█▌        | 20/128 [00:00<00:03, 34.88it/s, est. speed input: 35932.88 toks/s, output: 35.09 toks/s]
Processed prompts:  19%|█▉        | 24/128 [00:00<00:02, 34.72it/s, est. speed input: 35815.37 toks/s, output: 34.97 toks/s]
Processed prompts:  22%|██▏       | 28/128 [00:00<00:02, 34.63it/s, est. speed input: 35739.26 toks/s, output: 34.90 toks/s]
Processed prompts:  25%|██▌       | 32/128 [00:00<00:02, 34.61it/s, est. speed input: 35693.88 toks/s, output: 34.86 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:01<00:02, 34.53it/s, est. speed input: 35637.62 toks/s, output: 34.80 toks/s]
Processed prompts:  31%|███▏      | 40/128 [00:01<00:02, 34.46it/s, est. speed input: 35586.69 toks/s, output: 34.75 toks/s]
Processed prompts:  34%|███▍      | 44/128 [00:01<00:02, 34.41it/s, est. speed input: 35543.18 toks/s, output: 34.71 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:01<00:02, 34.37it/s, est. speed input: 35505.20 toks/s, output: 34.67 toks/s]
Processed prompts:  41%|████      | 52/128 [00:01<00:02, 34.30it/s, est. speed input: 35463.95 toks/s, output: 34.63 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:01<00:02, 34.28it/s, est. speed input: 35434.23 toks/s, output: 34.60 toks/s]
Processed prompts:  47%|████▋     | 60/128 [00:01<00:01, 34.24it/s, est. speed input: 35401.98 toks/s, output: 34.57 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:01<00:01, 34.21it/s, est. speed input: 35373.35 toks/s, output: 34.54 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:01<00:01, 34.20it/s, est. speed input: 35352.10 toks/s, output: 34.52 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:02<00:01, 34.24it/s, est. speed input: 35340.01 toks/s, output: 34.51 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:02<00:01, 34.22it/s, est. speed input: 35322.35 toks/s, output: 34.49 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:02<00:01, 34.08it/s, est. speed input: 35284.47 toks/s, output: 34.46 toks/s]
Processed prompts:  66%|██████▌   | 84/128 [00:02<00:01, 34.04it/s, est. speed input: 35259.35 toks/s, output: 34.43 toks/s]
Processed prompts:  69%|██████▉   | 88/128 [00:02<00:01, 34.03it/s, est. speed input: 35239.35 toks/s, output: 34.41 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:02<00:01, 34.12it/s, est. speed input: 35235.59 toks/s, output: 34.41 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:02<00:00, 34.07it/s, est. speed input: 35215.59 toks/s, output: 34.39 toks/s]
Processed prompts:  78%|███████▊  | 100/128 [00:02<00:00, 34.12it/s, est. speed input: 35210.12 toks/s, output: 34.38 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:03<00:00, 34.19it/s, est. speed input: 35208.39 toks/s, output: 34.38 toks/s]
Processed prompts:  84%|████████▍ | 108/128 [00:03<00:00, 34.27it/s, est. speed input: 35210.88 toks/s, output: 34.39 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:03<00:00, 34.23it/s, est. speed input: 35202.42 toks/s, output: 34.38 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:03<00:00, 34.24it/s, est. speed input: 35198.43 toks/s, output: 34.37 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:03<00:00, 34.33it/s, est. speed input: 35204.25 toks/s, output: 34.38 toks/s]
Processed prompts:  97%|█████████▋| 124/128 [00:03<00:00, 34.36it/s, est. speed input: 35205.42 toks/s, output: 34.38 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 34.32it/s, est. speed input: 35200.74 toks/s, output: 34.38 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 34.32it/s, est. speed input: 35200.74 toks/s, output: 34.38 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 34.37it/s, est. speed input: 35200.74 toks/s, output: 34.38 toks/s]
[rank0]:[W128 09:17:47.517147895 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 41.3s

测试结果:
  Requests/s:   31.83
  Tokens/s:     32623.62
  Total Reqs:   128
  Elapsed:      4.02s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     32591.79

============================================================
[3/7] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 09:17:57 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3372786) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3372786) WARNING 01-28 09:18:12 [backends.py:609] Failed to read file <frozen os>
Throughput: 62.00 requests/s, 63548.04 total tokens/s, 62.00 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-28 09:17:56] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 09:17:57] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:17:57] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:17:57] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:17:57] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:17:57] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:17:57] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:17:57] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:17:57] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:17:57] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:17:57] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:17:57] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:17:57] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:17:57] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 09:18:03] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 09:18:04] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:18:04] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:18:04] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:18:04] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:18:04] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:18:04] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:18:04] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:18:04] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:18:04] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:18:04] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:18:04] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:18:04] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:18:04] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3372786) [2026-01-28 09:18:04] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3372786) [2026-01-28 09:18:04] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3372786) [2026-01-28 09:18:04] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3372786) [2026-01-28 09:18:04] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3372786) [2026-01-28 09:18:04] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3372786) [2026-01-28 09:18:04] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3372786) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3372786) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.17it/s]
(EngineCore_DP0 pid=3372786) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.17it/s]
(EngineCore_DP0 pid=3372786) 
(EngineCore_DP0 pid=3372786) [2026-01-28 09:18:06] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=3372786) [2026-01-28 09:18:06] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9216000 bytes
(EngineCore_DP0 pid=3372786) [2026-01-28 09:18:06] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=3372786) [2026-01-28 09:18:06] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6144000 bytes
(EngineCore_DP0 pid=3372786) [2026-01-28 09:18:06] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=3372786) [2026-01-28 09:18:06] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 33177600 bytes
(EngineCore_DP0 pid=3372786) [2026-01-28 09:18:06] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=3372786) [2026-01-28 09:18:06] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16588800 bytes
(EngineCore_DP0 pid=3372786) 2026-01-28 09:18:23,066 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3372786) 2026-01-28 09:18:23,094 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3372786) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:00<00:00, 13.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 13.13it/s]
(EngineCore_DP0 pid=3372786) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 16.50it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 16.48it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  10%|█         | 26/256 [00:00<00:00, 257.62it/s]
Adding requests:  30%|███       | 77/256 [00:00<00:00, 404.64it/s]
Adding requests:  50%|████▉     | 127/256 [00:00<00:00, 445.22it/s]
Adding requests:  68%|██████▊   | 174/256 [00:00<00:00, 453.91it/s]
Adding requests:  88%|████████▊ | 225/256 [00:00<00:00, 470.59it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 452.43it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   5%|▌         | 14/256 [00:00<00:01, 136.46it/s, est. speed input: 139789.01 toks/s, output: 136.48 toks/s]
Processed prompts:  11%|█         | 28/256 [00:00<00:02, 87.29it/s, est. speed input: 94507.72 toks/s, output: 92.28 toks/s]   
Processed prompts:  15%|█▍        | 38/256 [00:00<00:02, 79.89it/s, est. speed input: 87202.33 toks/s, output: 85.15 toks/s]
Processed prompts:  18%|█▊        | 47/256 [00:00<00:02, 79.56it/s, est. speed input: 85888.16 toks/s, output: 83.87 toks/s]
Processed prompts:  22%|██▏       | 56/256 [00:00<00:02, 73.46it/s, est. speed input: 81566.45 toks/s, output: 79.65 toks/s]
Processed prompts:  25%|██▌       | 64/256 [00:00<00:02, 72.55it/s, est. speed input: 80242.64 toks/s, output: 78.36 toks/s]
Processed prompts:  28%|██▊       | 72/256 [00:00<00:02, 71.91it/s, est. speed input: 79243.99 toks/s, output: 77.39 toks/s]
Processed prompts:  31%|███▏      | 80/256 [00:01<00:02, 71.38it/s, est. speed input: 78435.20 toks/s, output: 76.59 toks/s]
Processed prompts:  34%|███▍      | 88/256 [00:01<00:02, 71.06it/s, est. speed input: 77802.65 toks/s, output: 75.98 toks/s]
Processed prompts:  38%|███▊      | 96/256 [00:01<00:02, 70.89it/s, est. speed input: 77299.95 toks/s, output: 75.49 toks/s]
Processed prompts:  41%|████      | 104/256 [00:01<00:02, 70.65it/s, est. speed input: 76844.88 toks/s, output: 75.04 toks/s]
Processed prompts:  44%|████▍     | 112/256 [00:01<00:02, 70.54it/s, est. speed input: 76474.40 toks/s, output: 74.68 toks/s]
Processed prompts:  47%|████▋     | 120/256 [00:01<00:01, 70.22it/s, est. speed input: 76093.69 toks/s, output: 74.31 toks/s]
Processed prompts:  50%|█████     | 128/256 [00:01<00:01, 70.17it/s, est. speed input: 75805.98 toks/s, output: 74.03 toks/s]
Processed prompts:  53%|█████▎    | 136/256 [00:01<00:01, 70.10it/s, est. speed input: 75547.82 toks/s, output: 73.78 toks/s]
Processed prompts:  56%|█████▋    | 144/256 [00:01<00:01, 70.17it/s, est. speed input: 75341.84 toks/s, output: 73.57 toks/s]
Processed prompts:  59%|█████▉    | 152/256 [00:02<00:01, 70.13it/s, est. speed input: 75142.02 toks/s, output: 73.38 toks/s]
Processed prompts:  62%|██████▎   | 160/256 [00:02<00:01, 70.11it/s, est. speed input: 74964.02 toks/s, output: 73.21 toks/s]
Processed prompts:  66%|██████▌   | 168/256 [00:02<00:01, 69.98it/s, est. speed input: 74784.00 toks/s, output: 73.03 toks/s]
Processed prompts:  69%|██████▉   | 176/256 [00:02<00:01, 69.93it/s, est. speed input: 74627.35 toks/s, output: 72.88 toks/s]
Processed prompts:  72%|███████▏  | 184/256 [00:02<00:01, 69.87it/s, est. speed input: 74481.43 toks/s, output: 72.73 toks/s]
Processed prompts:  75%|███████▌  | 192/256 [00:02<00:00, 69.78it/s, est. speed input: 74340.14 toks/s, output: 72.60 toks/s]
Processed prompts:  78%|███████▊  | 200/256 [00:02<00:00, 69.79it/s, est. speed input: 74221.76 toks/s, output: 72.48 toks/s]
Processed prompts:  81%|████████▏ | 208/256 [00:02<00:00, 69.85it/s, est. speed input: 74120.53 toks/s, output: 72.38 toks/s]
Processed prompts:  84%|████████▍ | 216/256 [00:02<00:00, 69.81it/s, est. speed input: 74015.25 toks/s, output: 72.28 toks/s]
Processed prompts:  88%|████████▊ | 224/256 [00:03<00:00, 69.73it/s, est. speed input: 73911.38 toks/s, output: 72.18 toks/s]
Processed prompts:  91%|█████████ | 232/256 [00:03<00:00, 69.73it/s, est. speed input: 73822.81 toks/s, output: 72.09 toks/s]
Processed prompts:  94%|█████████▍| 240/256 [00:03<00:00, 69.79it/s, est. speed input: 73746.10 toks/s, output: 72.02 toks/s]
Processed prompts:  97%|█████████▋| 248/256 [00:03<00:00, 69.80it/s, est. speed input: 73671.90 toks/s, output: 71.94 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 69.89it/s, est. speed input: 73610.46 toks/s, output: 71.88 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 69.89it/s, est. speed input: 73610.46 toks/s, output: 71.88 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 71.88it/s, est. speed input: 73610.46 toks/s, output: 71.88 toks/s]
[rank0]:[W128 09:18:29.353225405 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 42.0s

测试结果:
  Requests/s:   62.00
  Tokens/s:     63548.04
  Total Reqs:   256
  Elapsed:      4.13s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     63486.04

============================================================
[4/7] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 09:18:40 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3373864) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3373864) WARNING 01-28 09:18:55 [backends.py:609] Failed to read file <frozen os>
Throughput: 79.98 requests/s, 81982.04 total tokens/s, 79.98 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-28 09:18:39] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 09:18:40] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:18:40] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:18:40] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:18:40] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:18:40] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:18:40] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:18:40] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:18:40] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:18:40] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:18:40] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:18:40] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:18:40] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:18:40] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 09:18:46] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 09:18:47] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:18:47] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:18:47] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:18:47] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:18:47] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:18:47] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:18:47] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:18:47] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:18:47] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:18:47] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:18:47] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:18:47] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:18:47] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3373864) [2026-01-28 09:18:48] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3373864) [2026-01-28 09:18:48] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3373864) [2026-01-28 09:18:48] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3373864) [2026-01-28 09:18:48] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3373864) [2026-01-28 09:18:48] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3373864) [2026-01-28 09:18:48] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3373864) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3373864) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.15it/s]
(EngineCore_DP0 pid=3373864) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.15it/s]
(EngineCore_DP0 pid=3373864) 
(EngineCore_DP0 pid=3373864) [2026-01-28 09:18:49] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=3373864) [2026-01-28 09:18:49] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9216000 bytes
(EngineCore_DP0 pid=3373864) [2026-01-28 09:18:49] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=3373864) [2026-01-28 09:18:49] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6144000 bytes
(EngineCore_DP0 pid=3373864) [2026-01-28 09:18:49] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=3373864) [2026-01-28 09:18:49] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 33177600 bytes
(EngineCore_DP0 pid=3373864) [2026-01-28 09:18:49] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=3373864) [2026-01-28 09:18:49] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16588800 bytes
(EngineCore_DP0 pid=3373864) 2026-01-28 09:19:06,709 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3373864) 2026-01-28 09:19:06,736 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3373864) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 1/4 [00:00<00:00,  6.11it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 3/4 [00:00<00:00, 10.65it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 10.48it/s]
(EngineCore_DP0 pid=3373864) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00, 10.31it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 10.76it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   6%|▌         | 30/512 [00:00<00:01, 298.92it/s]
Adding requests:  16%|█▌        | 82/512 [00:00<00:01, 425.28it/s]
Adding requests:  26%|██▌       | 133/512 [00:00<00:00, 461.51it/s]
Adding requests:  36%|███▌      | 182/512 [00:00<00:00, 470.56it/s]
Adding requests:  46%|████▌     | 234/512 [00:00<00:00, 484.82it/s]
Adding requests:  55%|█████▌    | 284/512 [00:00<00:00, 489.45it/s]
Adding requests:  65%|██████▌   | 334/512 [00:00<00:00, 491.14it/s]
Adding requests:  75%|███████▌  | 385/512 [00:00<00:00, 494.45it/s]
Adding requests:  85%|████████▌ | 436/512 [00:00<00:00, 498.21it/s]
Adding requests:  95%|█████████▌| 487/512 [00:01<00:00, 498.13it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 481.15it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  11%|█▏        | 58/512 [00:00<00:01, 416.11it/s, est. speed input: 426228.73 toks/s, output: 416.15 toks/s]
Processed prompts:  20%|█▉        | 100/512 [00:00<00:02, 148.48it/s, est. speed input: 171207.68 toks/s, output: 167.19 toks/s]
Processed prompts:  24%|██▍       | 123/512 [00:00<00:03, 121.64it/s, est. speed input: 144253.93 toks/s, output: 140.87 toks/s]
Processed prompts:  27%|██▋       | 140/512 [00:01<00:03, 112.74it/s, est. speed input: 135162.17 toks/s, output: 131.99 toks/s]
Processed prompts:  30%|███       | 154/512 [00:01<00:03, 102.07it/s, est. speed input: 126608.65 toks/s, output: 123.64 toks/s]
Processed prompts:  32%|███▏      | 166/512 [00:01<00:03, 98.52it/s, est. speed input: 122865.34 toks/s, output: 119.98 toks/s] 
Processed prompts:  35%|███▍      | 178/512 [00:01<00:03, 95.57it/s, est. speed input: 119780.69 toks/s, output: 116.97 toks/s]
Processed prompts:  37%|███▋      | 190/512 [00:01<00:03, 93.34it/s, est. speed input: 117237.78 toks/s, output: 114.49 toks/s]
Processed prompts:  39%|███▉      | 202/512 [00:01<00:03, 91.69it/s, est. speed input: 115110.40 toks/s, output: 112.41 toks/s]
Processed prompts:  42%|████▏     | 214/512 [00:01<00:03, 90.20it/s, est. speed input: 113202.07 toks/s, output: 110.55 toks/s]
Processed prompts:  44%|████▍     | 226/512 [00:02<00:03, 89.13it/s, est. speed input: 111553.85 toks/s, output: 108.94 toks/s]
Processed prompts:  46%|████▋     | 238/512 [00:02<00:03, 88.43it/s, est. speed input: 110131.44 toks/s, output: 107.55 toks/s]
Processed prompts:  49%|████▉     | 250/512 [00:02<00:02, 88.00it/s, est. speed input: 108892.78 toks/s, output: 106.34 toks/s]
Processed prompts:  51%|█████     | 262/512 [00:02<00:02, 87.62it/s, est. speed input: 107776.02 toks/s, output: 105.25 toks/s]
Processed prompts:  54%|█████▎    | 274/512 [00:02<00:02, 87.40it/s, est. speed input: 106787.82 toks/s, output: 104.28 toks/s]
Processed prompts:  56%|█████▌    | 286/512 [00:02<00:02, 87.15it/s, est. speed input: 105877.25 toks/s, output: 103.39 toks/s]
Processed prompts:  58%|█████▊    | 298/512 [00:02<00:02, 86.92it/s, est. speed input: 105043.44 toks/s, output: 102.58 toks/s]
Processed prompts:  61%|██████    | 310/512 [00:03<00:02, 86.76it/s, est. speed input: 104286.21 toks/s, output: 101.84 toks/s]
Processed prompts:  63%|██████▎   | 322/512 [00:03<00:02, 86.70it/s, est. speed input: 103605.26 toks/s, output: 101.18 toks/s]
Processed prompts:  65%|██████▌   | 334/512 [00:03<00:02, 86.76it/s, est. speed input: 102997.56 toks/s, output: 100.58 toks/s]
Processed prompts:  68%|██████▊   | 346/512 [00:03<00:01, 87.74it/s, est. speed input: 102583.86 toks/s, output: 100.18 toks/s]
Processed prompts:  70%|██████▉   | 358/512 [00:03<00:01, 87.51it/s, est. speed input: 102066.55 toks/s, output: 99.67 toks/s] 
Processed prompts:  72%|███████▏  | 370/512 [00:03<00:01, 87.12it/s, est. speed input: 101551.03 toks/s, output: 99.17 toks/s]
Processed prompts:  75%|███████▍  | 382/512 [00:03<00:01, 86.87it/s, est. speed input: 101076.79 toks/s, output: 98.71 toks/s]
Processed prompts:  77%|███████▋  | 394/512 [00:04<00:01, 86.71it/s, est. speed input: 100637.99 toks/s, output: 98.28 toks/s]
Processed prompts:  79%|███████▉  | 406/512 [00:04<00:01, 86.66it/s, est. speed input: 100235.63 toks/s, output: 97.89 toks/s]
Processed prompts:  82%|████████▏ | 418/512 [00:04<00:01, 86.63it/s, est. speed input: 99860.26 toks/s, output: 97.52 toks/s] 
Processed prompts:  84%|████████▍ | 430/512 [00:04<00:00, 86.52it/s, est. speed input: 99498.22 toks/s, output: 97.17 toks/s]
Processed prompts:  86%|████████▋ | 442/512 [00:04<00:00, 86.71it/s, est. speed input: 99188.45 toks/s, output: 96.86 toks/s]
Processed prompts:  89%|████████▊ | 454/512 [00:04<00:00, 88.02it/s, est. speed input: 99027.75 toks/s, output: 96.71 toks/s]
Processed prompts:  91%|█████████ | 466/512 [00:04<00:00, 87.48it/s, est. speed input: 98718.99 toks/s, output: 96.40 toks/s]
Processed prompts:  93%|█████████▎| 478/512 [00:04<00:00, 87.16it/s, est. speed input: 98433.02 toks/s, output: 96.13 toks/s]
Processed prompts:  96%|█████████▌| 490/512 [00:05<00:00, 87.00it/s, est. speed input: 98170.34 toks/s, output: 95.87 toks/s]
Processed prompts:  98%|█████████▊| 502/512 [00:05<00:00, 86.93it/s, est. speed input: 97924.53 toks/s, output: 95.63 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:05<00:00, 86.93it/s, est. speed input: 98269.57 toks/s, output: 95.97 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:05<00:00, 95.96it/s, est. speed input: 98269.57 toks/s, output: 95.97 toks/s]
[rank0]:[W128 09:19:15.622775286 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 46.2s

测试结果:
  Requests/s:   79.98
  Tokens/s:     81982.04
  Total Reqs:   512
  Elapsed:      6.40s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     81902.05

============================================================
[5/7] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 09:19:28 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3375057) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3375057) WARNING 01-28 09:19:43 [backends.py:609] Failed to read file <frozen os>
Throughput: 89.69 requests/s, 91935.48 total tokens/s, 89.69 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-28 09:19:28] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 09:19:28] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:19:28] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:19:28] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:19:28] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:19:28] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:19:28] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:19:28] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:19:28] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:19:28] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:19:28] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:19:28] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:19:28] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:19:28] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 09:19:35] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 09:19:35] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:19:35] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:19:35] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:19:35] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:19:35] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:19:35] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:19:35] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:19:35] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:19:35] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:19:35] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:19:35] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:19:35] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:19:35] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3375057) [2026-01-28 09:19:36] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3375057) [2026-01-28 09:19:36] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3375057) [2026-01-28 09:19:36] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3375057) [2026-01-28 09:19:36] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3375057) [2026-01-28 09:19:36] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3375057) [2026-01-28 09:19:36] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3375057) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3375057) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.18it/s]
(EngineCore_DP0 pid=3375057) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.18it/s]
(EngineCore_DP0 pid=3375057) 
(EngineCore_DP0 pid=3375057) [2026-01-28 09:19:37] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=3375057) [2026-01-28 09:19:37] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9216000 bytes
(EngineCore_DP0 pid=3375057) [2026-01-28 09:19:37] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=3375057) [2026-01-28 09:19:37] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6144000 bytes
(EngineCore_DP0 pid=3375057) [2026-01-28 09:19:37] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=3375057) [2026-01-28 09:19:37] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 33177600 bytes
(EngineCore_DP0 pid=3375057) [2026-01-28 09:19:37] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=3375057) [2026-01-28 09:19:37] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16588800 bytes
(EngineCore_DP0 pid=3375057) 2026-01-28 09:19:54,357 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3375057) 2026-01-28 09:19:54,393 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3375057) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 1/5 [00:00<00:00,  4.71it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 3/5 [00:00<00:00,  9.97it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 11.88it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 10.58it/s]
(EngineCore_DP0 pid=3375057) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00, 16.31it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 16.99it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 16.88it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   5%|▍         | 50/1024 [00:00<00:01, 489.87it/s]
Adding requests:  10%|▉         | 100/1024 [00:00<00:01, 494.35it/s]
Adding requests:  15%|█▍        | 150/1024 [00:00<00:01, 495.39it/s]
Adding requests:  20%|█▉        | 200/1024 [00:00<00:01, 491.83it/s]
Adding requests:  25%|██▍       | 253/1024 [00:00<00:01, 502.48it/s]
Adding requests:  30%|██▉       | 304/1024 [00:00<00:01, 497.23it/s]
Adding requests:  35%|███▍      | 355/1024 [00:00<00:01, 500.18it/s]
Adding requests:  40%|███▉      | 406/1024 [00:00<00:01, 500.84it/s]
Adding requests:  45%|████▍     | 457/1024 [00:00<00:01, 500.89it/s]
Adding requests:  50%|████▉     | 508/1024 [00:01<00:01, 497.20it/s]
Adding requests:  54%|█████▍    | 558/1024 [00:01<00:00, 491.44it/s]
Adding requests:  59%|█████▉    | 609/1024 [00:01<00:00, 496.44it/s]
Adding requests:  65%|██████▍   | 662/1024 [00:01<00:00, 503.95it/s]
Adding requests:  70%|██████▉   | 715/1024 [00:01<00:00, 511.43it/s]
Adding requests:  75%|███████▍  | 767/1024 [00:01<00:00, 510.17it/s]
Adding requests:  80%|███████▉  | 819/1024 [00:01<00:00, 502.65it/s]
Adding requests:  85%|████████▍ | 870/1024 [00:01<00:00, 504.06it/s]
Adding requests:  90%|█████████ | 923/1024 [00:01<00:00, 510.53it/s]
Adding requests:  95%|█████████▌| 976/1024 [00:01<00:00, 514.15it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 504.12it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  17%|█▋        | 170/1024 [00:00<00:00, 1666.67it/s, est. speed input: 1707049.75 toks/s, output: 1666.79 toks/s]
Processed prompts:  33%|███▎      | 337/1024 [00:01<00:04, 156.58it/s, est. speed input: 185819.09 toks/s, output: 181.46 toks/s]   
Processed prompts:  40%|████      | 412/1024 [00:02<00:04, 127.90it/s, est. speed input: 154525.16 toks/s, output: 150.90 toks/s]
Processed prompts:  45%|████▍     | 457/1024 [00:03<00:04, 122.67it/s, est. speed input: 148032.52 toks/s, output: 144.56 toks/s]
Processed prompts:  48%|████▊     | 488/1024 [00:03<00:04, 115.61it/s, est. speed input: 142275.07 toks/s, output: 138.94 toks/s]
Processed prompts:  50%|████▉     | 511/1024 [00:03<00:04, 110.48it/s, est. speed input: 138581.32 toks/s, output: 135.33 toks/s]
Processed prompts:  52%|█████▏    | 530/1024 [00:04<00:04, 102.51it/s, est. speed input: 134318.49 toks/s, output: 131.17 toks/s]
Processed prompts:  53%|█████▎    | 546/1024 [00:04<00:04, 100.58it/s, est. speed input: 132587.16 toks/s, output: 129.48 toks/s]
Processed prompts:  55%|█████▍    | 562/1024 [00:04<00:04, 98.82it/s, est. speed input: 131023.25 toks/s, output: 127.95 toks/s] 
Processed prompts:  56%|█████▋    | 578/1024 [00:04<00:04, 97.15it/s, est. speed input: 129566.63 toks/s, output: 126.53 toks/s]
Processed prompts:  58%|█████▊    | 594/1024 [00:04<00:04, 95.78it/s, est. speed input: 128231.65 toks/s, output: 125.23 toks/s]
Processed prompts:  60%|█████▉    | 610/1024 [00:04<00:04, 94.56it/s, est. speed input: 126974.15 toks/s, output: 124.00 toks/s]
Processed prompts:  61%|██████    | 626/1024 [00:05<00:04, 93.64it/s, est. speed input: 125811.25 toks/s, output: 122.86 toks/s]
Processed prompts:  63%|██████▎   | 642/1024 [00:05<00:04, 92.86it/s, est. speed input: 124714.00 toks/s, output: 121.79 toks/s]
Processed prompts:  64%|██████▍   | 658/1024 [00:05<00:03, 92.28it/s, est. speed input: 123685.99 toks/s, output: 120.79 toks/s]
Processed prompts:  66%|██████▌   | 674/1024 [00:05<00:03, 91.81it/s, est. speed input: 122717.03 toks/s, output: 119.84 toks/s]
Processed prompts:  67%|██████▋   | 690/1024 [00:05<00:03, 91.47it/s, est. speed input: 121806.36 toks/s, output: 118.95 toks/s]
Processed prompts:  69%|██████▉   | 706/1024 [00:05<00:03, 91.34it/s, est. speed input: 120965.93 toks/s, output: 118.13 toks/s]
Processed prompts:  71%|███████   | 722/1024 [00:06<00:03, 91.25it/s, est. speed input: 120172.98 toks/s, output: 117.36 toks/s]
Processed prompts:  72%|███████▏  | 738/1024 [00:06<00:03, 91.00it/s, est. speed input: 119401.90 toks/s, output: 116.60 toks/s]
Processed prompts:  74%|███████▎  | 754/1024 [00:06<00:02, 91.04it/s, est. speed input: 118698.02 toks/s, output: 115.92 toks/s]
Processed prompts:  75%|███████▌  | 770/1024 [00:06<00:02, 91.04it/s, est. speed input: 118028.19 toks/s, output: 115.26 toks/s]
Processed prompts:  77%|███████▋  | 786/1024 [00:06<00:02, 91.00it/s, est. speed input: 117387.56 toks/s, output: 114.64 toks/s]
Processed prompts:  78%|███████▊  | 802/1024 [00:07<00:02, 90.85it/s, est. speed input: 116766.69 toks/s, output: 114.03 toks/s]
Processed prompts:  80%|███████▉  | 818/1024 [00:07<00:02, 90.90it/s, est. speed input: 116192.31 toks/s, output: 113.47 toks/s]
Processed prompts:  81%|████████▏ | 834/1024 [00:07<00:02, 90.93it/s, est. speed input: 115643.80 toks/s, output: 112.93 toks/s]
Processed prompts:  83%|████████▎ | 850/1024 [00:07<00:01, 90.88it/s, est. speed input: 115114.53 toks/s, output: 112.42 toks/s]
Processed prompts:  85%|████████▍ | 866/1024 [00:07<00:01, 90.90it/s, est. speed input: 114614.67 toks/s, output: 111.93 toks/s]
Processed prompts:  86%|████████▌ | 882/1024 [00:07<00:01, 90.90it/s, est. speed input: 114135.93 toks/s, output: 111.46 toks/s]
Processed prompts:  88%|████████▊ | 898/1024 [00:08<00:01, 90.77it/s, est. speed input: 113665.97 toks/s, output: 111.00 toks/s]
Processed prompts:  89%|████████▉ | 914/1024 [00:08<00:01, 90.82it/s, est. speed input: 113228.41 toks/s, output: 110.57 toks/s]
Processed prompts:  91%|█████████ | 930/1024 [00:08<00:01, 90.79it/s, est. speed input: 112804.22 toks/s, output: 110.16 toks/s]
Processed prompts:  92%|█████████▏| 946/1024 [00:08<00:00, 92.21it/s, est. speed input: 112516.45 toks/s, output: 109.88 toks/s]
Processed prompts:  94%|█████████▍| 962/1024 [00:08<00:00, 91.82it/s, est. speed input: 112128.03 toks/s, output: 109.50 toks/s]
Processed prompts:  96%|█████████▌| 978/1024 [00:08<00:00, 91.51it/s, est. speed input: 111751.40 toks/s, output: 109.13 toks/s]
Processed prompts:  97%|█████████▋| 994/1024 [00:09<00:00, 92.57it/s, est. speed input: 111486.93 toks/s, output: 108.87 toks/s]
Processed prompts:  99%|█████████▊| 1010/1024 [00:09<00:00, 92.17it/s, est. speed input: 111146.94 toks/s, output: 108.54 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:09<00:00, 92.17it/s, est. speed input: 111741.01 toks/s, output: 109.12 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:09<00:00, 109.12it/s, est. speed input: 111741.01 toks/s, output: 109.12 toks/s]
[rank0]:[W128 09:20:08.379150171 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 52.7s

测试结果:
  Requests/s:   89.69
  Tokens/s:     91935.48
  Total Reqs:   1024
  Elapsed:      11.42s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     91845.78

============================================================
[6/7] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 09:20:25 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3376326) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3376326) WARNING 01-28 09:20:40 [backends.py:609] Failed to read file <frozen os>
Throughput: 93.47 requests/s, 95806.57 total tokens/s, 93.47 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-28 09:20:24] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 09:20:25] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:20:25] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:20:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:20:25] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:20:25] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:20:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:20:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:20:25] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:20:25] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:20:25] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:20:25] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:20:25] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:20:25] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 09:20:31] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 09:20:32] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:20:32] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:20:32] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:20:32] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:20:32] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:20:32] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:20:32] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:20:32] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:20:32] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:20:32] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:20:32] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:20:32] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:20:32] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3376326) [2026-01-28 09:20:33] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3376326) [2026-01-28 09:20:33] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3376326) [2026-01-28 09:20:33] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3376326) [2026-01-28 09:20:33] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3376326) [2026-01-28 09:20:33] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3376326) [2026-01-28 09:20:33] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3376326) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3376326) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.17it/s]
(EngineCore_DP0 pid=3376326) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.17it/s]
(EngineCore_DP0 pid=3376326) 
(EngineCore_DP0 pid=3376326) [2026-01-28 09:20:34] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=3376326) [2026-01-28 09:20:34] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9216000 bytes
(EngineCore_DP0 pid=3376326) [2026-01-28 09:20:34] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=3376326) [2026-01-28 09:20:34] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6144000 bytes
(EngineCore_DP0 pid=3376326) [2026-01-28 09:20:34] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=3376326) [2026-01-28 09:20:34] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 33177600 bytes
(EngineCore_DP0 pid=3376326) [2026-01-28 09:20:34] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=3376326) [2026-01-28 09:20:34] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16588800 bytes
(EngineCore_DP0 pid=3376326) 2026-01-28 09:20:51,702 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3376326) 2026-01-28 09:20:51,771 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3376326) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 1/7 [00:00<00:00,  9.24it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00,  8.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00, 10.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:00<00:00, 13.06it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00,  9.87it/s]
(EngineCore_DP0 pid=3376326) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00, 11.49it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00, 14.44it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 14.49it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 32/2048 [00:00<00:06, 315.95it/s]
Adding requests:   4%|▍         | 84/2048 [00:00<00:04, 431.65it/s]
Adding requests:   7%|▋         | 134/2048 [00:00<00:04, 462.45it/s]
Adding requests:   9%|▉         | 183/2048 [00:00<00:03, 473.22it/s]
Adding requests:  11%|█▏        | 234/2048 [00:00<00:03, 485.38it/s]
Adding requests:  14%|█▍        | 284/2048 [00:00<00:03, 489.77it/s]
Adding requests:  16%|█▋        | 333/2048 [00:00<00:03, 488.97it/s]
Adding requests:  19%|█▊        | 383/2048 [00:00<00:03, 491.45it/s]
Adding requests:  21%|██        | 434/2048 [00:00<00:03, 496.38it/s]
Adding requests:  24%|██▎       | 485/2048 [00:01<00:03, 500.47it/s]
Adding requests:  26%|██▌       | 536/2048 [00:01<00:03, 489.97it/s]
Adding requests:  29%|██▊       | 588/2048 [00:01<00:02, 498.90it/s]
Adding requests:  31%|███▏      | 640/2048 [00:01<00:02, 502.96it/s]
Adding requests:  34%|███▎      | 691/2048 [00:01<00:02, 473.90it/s]
Adding requests:  36%|███▌      | 739/2048 [00:01<00:03, 423.40it/s]
Adding requests:  39%|███▊      | 789/2048 [00:01<00:02, 440.68it/s]
Adding requests:  41%|████      | 837/2048 [00:01<00:02, 450.17it/s]
Adding requests:  43%|████▎     | 890/2048 [00:01<00:02, 472.14it/s]
Adding requests:  46%|████▌     | 942/2048 [00:01<00:02, 483.53it/s]
Adding requests:  49%|████▊     | 994/2048 [00:02<00:02, 493.93it/s]
Adding requests:  51%|█████     | 1046/2048 [00:02<00:02, 500.78it/s]
Adding requests:  54%|█████▎    | 1097/2048 [00:02<00:01, 501.11it/s]
Adding requests:  56%|█████▌    | 1148/2048 [00:02<00:01, 501.07it/s]
Adding requests:  59%|█████▊    | 1203/2048 [00:02<00:01, 514.31it/s]
Adding requests:  61%|██████▏   | 1255/2048 [00:02<00:01, 511.06it/s]
Adding requests:  64%|██████▍   | 1307/2048 [00:02<00:01, 511.62it/s]
Adding requests:  66%|██████▋   | 1359/2048 [00:02<00:01, 512.75it/s]
Adding requests:  69%|██████▉   | 1413/2048 [00:02<00:01, 518.82it/s]
Adding requests:  72%|███████▏  | 1465/2048 [00:02<00:01, 518.04it/s]
Adding requests:  74%|███████▍  | 1518/2048 [00:03<00:01, 518.43it/s]
Adding requests:  77%|███████▋  | 1571/2048 [00:03<00:00, 519.19it/s]
Adding requests:  79%|███████▉  | 1624/2048 [00:03<00:00, 522.36it/s]
Adding requests:  82%|████████▏ | 1677/2048 [00:03<00:00, 520.48it/s]
Adding requests:  84%|████████▍ | 1730/2048 [00:03<00:00, 520.46it/s]
Adding requests:  87%|████████▋ | 1783/2048 [00:03<00:00, 512.97it/s]
Adding requests:  90%|████████▉ | 1835/2048 [00:03<00:00, 514.82it/s]
Adding requests:  92%|█████████▏| 1887/2048 [00:03<00:00, 504.87it/s]
Adding requests:  95%|█████████▍| 1938/2048 [00:03<00:00, 505.10it/s]
Adding requests:  97%|█████████▋| 1990/2048 [00:04<00:00, 506.92it/s]
Adding requests: 100%|█████████▉| 2042/2048 [00:04<00:00, 509.08it/s]
Adding requests: 100%|██████████| 2048/2048 [00:04<00:00, 495.00it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  19%|█▉        | 386/2048 [00:00<00:01, 1473.47it/s, est. speed input: 1508967.55 toks/s, output: 1473.51 toks/s]
Processed prompts:  26%|██▌       | 534/2048 [00:01<00:06, 245.34it/s, est. speed input: 306658.84 toks/s, output: 299.47 toks/s]   
Processed prompts:  29%|██▉       | 601/2048 [00:02<00:07, 191.49it/s, est. speed input: 249300.72 toks/s, output: 243.46 toks/s]
Processed prompts:  31%|███▏      | 642/2048 [00:02<00:08, 160.48it/s, est. speed input: 220745.54 toks/s, output: 215.57 toks/s]
Processed prompts:  33%|███▎      | 670/2048 [00:03<00:08, 160.93it/s, est. speed input: 217903.44 toks/s, output: 212.80 toks/s]
Processed prompts:  34%|███▍      | 694/2048 [00:03<00:09, 138.27it/s, est. speed input: 203730.33 toks/s, output: 198.95 toks/s]
Processed prompts:  35%|███▍      | 713/2048 [00:03<00:09, 134.24it/s, est. speed input: 199580.73 toks/s, output: 194.90 toks/s]
Processed prompts:  36%|███▌      | 729/2048 [00:03<00:10, 127.22it/s, est. speed input: 195053.93 toks/s, output: 190.48 toks/s]
Processed prompts:  36%|███▋      | 743/2048 [00:03<00:11, 117.95it/s, est. speed input: 190296.59 toks/s, output: 185.84 toks/s]
Processed prompts:  37%|███▋      | 756/2048 [00:04<00:11, 108.49it/s, est. speed input: 185690.76 toks/s, output: 181.34 toks/s]
Processed prompts:  38%|███▊      | 770/2048 [00:04<00:12, 101.85it/s, est. speed input: 181636.40 toks/s, output: 177.38 toks/s]
Processed prompts:  38%|███▊      | 786/2048 [00:04<00:12, 99.89it/s, est. speed input: 178431.65 toks/s, output: 174.25 toks/s] 
Processed prompts:  39%|███▉      | 802/2048 [00:04<00:12, 98.29it/s, est. speed input: 175438.32 toks/s, output: 171.33 toks/s]
Processed prompts:  40%|███▉      | 818/2048 [00:04<00:12, 97.13it/s, est. speed input: 172668.37 toks/s, output: 168.62 toks/s]
Processed prompts:  41%|████      | 834/2048 [00:05<00:12, 96.12it/s, est. speed input: 170052.05 toks/s, output: 166.07 toks/s]
Processed prompts:  42%|████▏     | 850/2048 [00:05<00:12, 95.24it/s, est. speed input: 167578.46 toks/s, output: 163.65 toks/s]
Processed prompts:  42%|████▏     | 866/2048 [00:05<00:12, 94.79it/s, est. speed input: 165298.83 toks/s, output: 161.42 toks/s]
Processed prompts:  43%|████▎     | 882/2048 [00:05<00:12, 94.42it/s, est. speed input: 163150.87 toks/s, output: 159.33 toks/s]
Processed prompts:  44%|████▍     | 898/2048 [00:05<00:12, 94.40it/s, est. speed input: 161172.20 toks/s, output: 157.39 toks/s]
Processed prompts:  45%|████▍     | 914/2048 [00:05<00:12, 94.13it/s, est. speed input: 159267.40 toks/s, output: 155.53 toks/s]
Processed prompts:  45%|████▌     | 930/2048 [00:06<00:11, 95.66it/s, est. speed input: 157738.01 toks/s, output: 154.04 toks/s]
Processed prompts:  46%|████▌     | 946/2048 [00:06<00:11, 95.05it/s, est. speed input: 156036.27 toks/s, output: 152.38 toks/s]
Processed prompts:  47%|████▋     | 962/2048 [00:06<00:11, 94.42it/s, est. speed input: 154395.33 toks/s, output: 150.78 toks/s]
Processed prompts:  48%|████▊     | 978/2048 [00:06<00:11, 95.78it/s, est. speed input: 153090.92 toks/s, output: 149.50 toks/s]
Processed prompts:  49%|████▊     | 994/2048 [00:06<00:11, 95.18it/s, est. speed input: 151641.57 toks/s, output: 148.09 toks/s]
Processed prompts:  49%|████▉     | 1010/2048 [00:06<00:10, 94.73it/s, est. speed input: 150259.19 toks/s, output: 146.74 toks/s]
Processed prompts:  50%|█████     | 1026/2048 [00:07<00:10, 94.39it/s, est. speed input: 148941.30 toks/s, output: 145.45 toks/s]
Processed prompts:  51%|█████     | 1042/2048 [00:07<00:10, 94.16it/s, est. speed input: 147686.30 toks/s, output: 144.22 toks/s]
Processed prompts:  52%|█████▏    | 1058/2048 [00:07<00:10, 93.84it/s, est. speed input: 146469.24 toks/s, output: 143.04 toks/s]
Processed prompts:  52%|█████▏    | 1074/2048 [00:07<00:10, 93.75it/s, est. speed input: 145323.81 toks/s, output: 141.92 toks/s]
Processed prompts:  53%|█████▎    | 1090/2048 [00:07<00:10, 93.72it/s, est. speed input: 144232.71 toks/s, output: 140.85 toks/s]
Processed prompts:  54%|█████▍    | 1106/2048 [00:07<00:10, 93.79it/s, est. speed input: 143198.85 toks/s, output: 139.84 toks/s]
Processed prompts:  55%|█████▍    | 1122/2048 [00:08<00:09, 93.76it/s, est. speed input: 142200.00 toks/s, output: 138.87 toks/s]
Processed prompts:  56%|█████▌    | 1138/2048 [00:08<00:09, 93.78it/s, est. speed input: 141245.98 toks/s, output: 137.93 toks/s]
Processed prompts:  56%|█████▋    | 1154/2048 [00:08<00:09, 95.22it/s, est. speed input: 140473.14 toks/s, output: 137.18 toks/s]
Processed prompts:  57%|█████▋    | 1170/2048 [00:08<00:09, 94.71it/s, est. speed input: 139582.54 toks/s, output: 136.31 toks/s]
Processed prompts:  58%|█████▊    | 1186/2048 [00:08<00:09, 94.34it/s, est. speed input: 138725.76 toks/s, output: 135.47 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [00:08<00:08, 94.08it/s, est. speed input: 137900.57 toks/s, output: 134.67 toks/s]
Processed prompts:  59%|█████▉    | 1218/2048 [00:09<00:08, 93.94it/s, est. speed input: 137110.66 toks/s, output: 133.90 toks/s]
Processed prompts:  60%|██████    | 1234/2048 [00:09<00:08, 93.97it/s, est. speed input: 136361.02 toks/s, output: 133.16 toks/s]
Processed prompts:  61%|██████    | 1250/2048 [00:09<00:08, 93.78it/s, est. speed input: 135620.24 toks/s, output: 132.44 toks/s]
Processed prompts:  62%|██████▏   | 1266/2048 [00:09<00:08, 95.34it/s, est. speed input: 135048.34 toks/s, output: 131.88 toks/s]
Processed prompts:  63%|██████▎   | 1282/2048 [00:09<00:08, 94.71it/s, est. speed input: 134354.95 toks/s, output: 131.21 toks/s]
Processed prompts:  63%|██████▎   | 1298/2048 [00:09<00:07, 94.36it/s, est. speed input: 133691.15 toks/s, output: 130.56 toks/s]
Processed prompts:  64%|██████▍   | 1314/2048 [00:10<00:07, 93.99it/s, est. speed input: 133040.33 toks/s, output: 129.92 toks/s]
Processed prompts:  65%|██████▍   | 1330/2048 [00:10<00:07, 93.93it/s, est. speed input: 132426.34 toks/s, output: 129.32 toks/s]
Processed prompts:  66%|██████▌   | 1346/2048 [00:10<00:07, 93.31it/s, est. speed input: 131788.52 toks/s, output: 128.70 toks/s]
Processed prompts:  67%|██████▋   | 1362/2048 [00:10<00:07, 93.51it/s, est. speed input: 131219.24 toks/s, output: 128.14 toks/s]
Processed prompts:  67%|██████▋   | 1378/2048 [00:10<00:07, 93.45it/s, est. speed input: 130652.97 toks/s, output: 127.59 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [00:10<00:06, 93.57it/s, est. speed input: 130115.34 toks/s, output: 127.07 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [00:11<00:06, 93.42it/s, est. speed input: 129578.65 toks/s, output: 126.54 toks/s]
Processed prompts:  70%|██████▉   | 1426/2048 [00:11<00:06, 93.48it/s, est. speed input: 129069.51 toks/s, output: 126.04 toks/s]
Processed prompts:  70%|███████   | 1442/2048 [00:11<00:06, 93.45it/s, est. speed input: 128570.71 toks/s, output: 125.56 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [00:11<00:06, 93.47it/s, est. speed input: 128088.93 toks/s, output: 125.09 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [00:11<00:06, 93.55it/s, est. speed input: 127625.63 toks/s, output: 124.63 toks/s]
Processed prompts:  73%|███████▎  | 1490/2048 [00:11<00:05, 93.47it/s, est. speed input: 127166.61 toks/s, output: 124.19 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [00:12<00:05, 93.60it/s, est. speed input: 126732.37 toks/s, output: 123.76 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [00:12<00:05, 93.50it/s, est. speed input: 126298.55 toks/s, output: 123.34 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [00:12<00:05, 93.67it/s, est. speed input: 125891.08 toks/s, output: 122.94 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [00:12<00:05, 93.60it/s, est. speed input: 125483.05 toks/s, output: 122.54 toks/s]
Processed prompts:  77%|███████▋  | 1570/2048 [00:12<00:05, 93.49it/s, est. speed input: 125082.28 toks/s, output: 122.15 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [00:13<00:04, 94.98it/s, est. speed input: 124782.22 toks/s, output: 121.86 toks/s]
Processed prompts:  78%|███████▊  | 1602/2048 [00:13<00:04, 94.51it/s, est. speed input: 124404.36 toks/s, output: 121.49 toks/s]
Processed prompts:  79%|███████▉  | 1618/2048 [00:13<00:04, 94.13it/s, est. speed input: 124032.72 toks/s, output: 121.13 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [00:13<00:04, 93.95it/s, est. speed input: 123675.62 toks/s, output: 120.78 toks/s]
Processed prompts:  81%|████████  | 1650/2048 [00:13<00:04, 93.78it/s, est. speed input: 123325.07 toks/s, output: 120.43 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [00:13<00:04, 93.79it/s, est. speed input: 122989.65 toks/s, output: 120.11 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [00:14<00:03, 93.61it/s, est. speed input: 122652.84 toks/s, output: 119.78 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [00:14<00:03, 93.44it/s, est. speed input: 122321.44 toks/s, output: 119.45 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [00:14<00:03, 93.42it/s, est. speed input: 122003.27 toks/s, output: 119.14 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [00:14<00:03, 93.44it/s, est. speed input: 121694.15 toks/s, output: 118.84 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [00:14<00:03, 93.39it/s, est. speed input: 121389.54 toks/s, output: 118.54 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [00:14<00:03, 93.44it/s, est. speed input: 121095.90 toks/s, output: 118.26 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [00:15<00:02, 93.52it/s, est. speed input: 120810.88 toks/s, output: 117.98 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [00:15<00:02, 93.40it/s, est. speed input: 120523.83 toks/s, output: 117.70 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [00:15<00:02, 93.52it/s, est. speed input: 120253.16 toks/s, output: 117.43 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [00:15<00:02, 93.60it/s, est. speed input: 119988.19 toks/s, output: 117.18 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [00:15<00:02, 93.63it/s, est. speed input: 119727.59 toks/s, output: 116.92 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [00:15<00:02, 93.80it/s, est. speed input: 119479.33 toks/s, output: 116.68 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [00:16<00:01, 95.15it/s, est. speed input: 119290.78 toks/s, output: 116.49 toks/s]
Processed prompts:  92%|█████████▏| 1890/2048 [00:16<00:01, 94.66it/s, est. speed input: 119043.46 toks/s, output: 116.25 toks/s]
Processed prompts:  93%|█████████▎| 1906/2048 [00:16<00:01, 94.50it/s, est. speed input: 118808.85 toks/s, output: 116.02 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [00:16<00:01, 94.24it/s, est. speed input: 118573.12 toks/s, output: 115.79 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [00:16<00:01, 94.27it/s, est. speed input: 118350.95 toks/s, output: 115.58 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [00:16<00:00, 95.69it/s, est. speed input: 118190.60 toks/s, output: 115.42 toks/s]
Processed prompts:  96%|█████████▌| 1970/2048 [00:17<00:00, 95.00it/s, est. speed input: 117965.08 toks/s, output: 115.20 toks/s]
Processed prompts:  97%|█████████▋| 1986/2048 [00:17<00:00, 94.60it/s, est. speed input: 117747.40 toks/s, output: 114.99 toks/s]
Processed prompts:  98%|█████████▊| 2002/2048 [00:17<00:00, 94.42it/s, est. speed input: 117537.62 toks/s, output: 114.78 toks/s]
Processed prompts:  99%|█████████▊| 2018/2048 [00:17<00:00, 94.21it/s, est. speed input: 117328.56 toks/s, output: 114.58 toks/s]
Processed prompts:  99%|█████████▉| 2034/2048 [00:17<00:00, 96.09it/s, est. speed input: 117202.13 toks/s, output: 114.46 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:17<00:00, 96.09it/s, est. speed input: 118007.16 toks/s, output: 115.24 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:17<00:00, 115.24it/s, est. speed input: 118007.16 toks/s, output: 115.24 toks/s]
[rank0]:[W128 09:21:16.623841831 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 68.3s

测试结果:
  Requests/s:   93.47
  Tokens/s:     95806.57
  Total Reqs:   2048
  Elapsed:      21.91s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     95713.10

============================================================
[7/7] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 09:21:42 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3377868) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3377868) WARNING 01-28 09:21:58 [backends.py:609] Failed to read file <frozen os>
Throughput: 95.66 requests/s, 98055.66 total tokens/s, 95.66 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-28 09:21:41] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 09:21:42] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:21:42] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:21:42] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:21:42] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:21:42] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:21:42] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:21:42] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:21:42] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:21:42] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:21:42] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:21:42] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:21:42] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:21:42] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 09:21:49] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 09:21:49] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:21:49] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:21:49] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:21:49] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:21:49] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:21:49] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:21:49] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:21:49] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:21:49] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:21:49] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:21:49] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:21:49] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:21:49] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3377868) [2026-01-28 09:21:50] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3377868) [2026-01-28 09:21:50] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3377868) [2026-01-28 09:21:50] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3377868) [2026-01-28 09:21:50] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3377868) [2026-01-28 09:21:50] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3377868) [2026-01-28 09:21:50] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3377868) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3377868) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.18it/s]
(EngineCore_DP0 pid=3377868) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.17it/s]
(EngineCore_DP0 pid=3377868) 
(EngineCore_DP0 pid=3377868) [2026-01-28 09:21:51] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=3377868) [2026-01-28 09:21:51] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9216000 bytes
(EngineCore_DP0 pid=3377868) [2026-01-28 09:21:51] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=3377868) [2026-01-28 09:21:51] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6144000 bytes
(EngineCore_DP0 pid=3377868) [2026-01-28 09:21:51] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=3377868) [2026-01-28 09:21:51] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 33177600 bytes
(EngineCore_DP0 pid=3377868) [2026-01-28 09:21:51] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=3377868) [2026-01-28 09:21:51] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16588800 bytes
(EngineCore_DP0 pid=3377868) [rank0]:W0128 09:22:04.049000 3377868 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3377868) [rank0]:W0128 09:22:04.129000 3377868 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3377868) [rank0]:W0128 09:22:05.191000 3377868 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3377868) [rank0]:W0128 09:22:05.323000 3377868 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3377868) 2026-01-28 09:22:09,213 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3377868) 2026-01-28 09:22:09,254 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3377868) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▉         | 1/11 [00:00<00:02,  4.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 3/11 [00:00<00:00,  8.58it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 5/11 [00:00<00:00, 11.26it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▎   | 7/11 [00:00<00:00, 12.92it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 9/11 [00:00<00:00, 13.99it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 13.94it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 12.22it/s]
(EngineCore_DP0 pid=3377868) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00, 15.49it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00, 16.11it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00, 16.39it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 12.61it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 35/4096 [00:00<00:11, 347.91it/s]
Adding requests:   2%|▏         | 87/4096 [00:00<00:08, 447.16it/s]
Adding requests:   3%|▎         | 138/4096 [00:00<00:08, 473.89it/s]
Adding requests:   5%|▍         | 188/4096 [00:00<00:08, 483.05it/s]
Adding requests:   6%|▌         | 240/4096 [00:00<00:07, 495.57it/s]
Adding requests:   7%|▋         | 291/4096 [00:00<00:07, 498.27it/s]
Adding requests:   8%|▊         | 341/4096 [00:00<00:07, 497.84it/s]
Adding requests:  10%|▉         | 393/4096 [00:00<00:07, 503.27it/s]
Adding requests:  11%|█         | 444/4096 [00:00<00:07, 501.06it/s]
Adding requests:  12%|█▏        | 495/4096 [00:01<00:07, 501.63it/s]
Adding requests:  13%|█▎        | 546/4096 [00:01<00:07, 494.41it/s]
Adding requests:  15%|█▍        | 599/4096 [00:01<00:06, 502.10it/s]
Adding requests:  16%|█▌        | 652/4096 [00:01<00:06, 507.74it/s]
Adding requests:  17%|█▋        | 706/4096 [00:01<00:06, 515.01it/s]
Adding requests:  19%|█▊        | 758/4096 [00:01<00:06, 514.63it/s]
Adding requests:  20%|█▉        | 810/4096 [00:01<00:06, 506.97it/s]
Adding requests:  21%|██        | 861/4096 [00:01<00:06, 506.52it/s]
Adding requests:  22%|██▏       | 914/4096 [00:01<00:06, 511.55it/s]
Adding requests:  24%|██▎       | 967/4096 [00:01<00:06, 515.53it/s]
Adding requests:  25%|██▍       | 1019/4096 [00:02<00:05, 514.87it/s]
Adding requests:  26%|██▌       | 1071/4096 [00:02<00:05, 510.16it/s]
Adding requests:  27%|██▋       | 1123/4096 [00:02<00:05, 497.16it/s]
Adding requests:  29%|██▊       | 1176/4096 [00:02<00:05, 504.69it/s]
Adding requests:  30%|███       | 1230/4096 [00:02<00:05, 512.69it/s]
Adding requests:  31%|███▏      | 1282/4096 [00:02<00:05, 507.43it/s]
Adding requests:  33%|███▎      | 1335/4096 [00:02<00:05, 512.47it/s]
Adding requests:  34%|███▍      | 1387/4096 [00:02<00:05, 512.35it/s]
Adding requests:  35%|███▌      | 1439/4096 [00:02<00:05, 506.98it/s]
Adding requests:  36%|███▋      | 1490/4096 [00:02<00:05, 507.09it/s]
Adding requests:  38%|███▊      | 1542/4096 [00:03<00:05, 508.63it/s]
Adding requests:  39%|███▉      | 1596/4096 [00:03<00:04, 515.43it/s]
Adding requests:  40%|████      | 1648/4096 [00:03<00:04, 509.27it/s]
Adding requests:  41%|████▏     | 1699/4096 [00:03<00:04, 506.44it/s]
Adding requests:  43%|████▎     | 1751/4096 [00:03<00:04, 509.80it/s]
Adding requests:  44%|████▍     | 1803/4096 [00:03<00:04, 510.81it/s]
Adding requests:  45%|████▌     | 1855/4096 [00:03<00:04, 513.49it/s]
Adding requests:  47%|████▋     | 1907/4096 [00:03<00:04, 512.40it/s]
Adding requests:  48%|████▊     | 1959/4096 [00:03<00:04, 512.23it/s]
Adding requests:  49%|████▉     | 2012/4096 [00:03<00:04, 516.25it/s]
Adding requests:  50%|█████     | 2064/4096 [00:04<00:03, 517.15it/s]
Adding requests:  52%|█████▏    | 2116/4096 [00:04<00:03, 516.85it/s]
Adding requests:  53%|█████▎    | 2168/4096 [00:04<00:03, 508.20it/s]
Adding requests:  54%|█████▍    | 2219/4096 [00:04<00:03, 507.32it/s]
Adding requests:  55%|█████▌    | 2270/4096 [00:04<00:03, 498.78it/s]
Adding requests:  57%|█████▋    | 2322/4096 [00:04<00:03, 504.41it/s]
Adding requests:  58%|█████▊    | 2373/4096 [00:04<00:03, 505.86it/s]
Adding requests:  59%|█████▉    | 2425/4096 [00:04<00:03, 508.83it/s]
Adding requests:  60%|██████    | 2476/4096 [00:04<00:03, 508.91it/s]
Adding requests:  62%|██████▏   | 2527/4096 [00:04<00:03, 507.67it/s]
Adding requests:  63%|██████▎   | 2581/4096 [00:05<00:02, 514.47it/s]
Adding requests:  64%|██████▍   | 2633/4096 [00:05<00:02, 511.93it/s]
Adding requests:  66%|██████▌   | 2685/4096 [00:05<00:02, 514.28it/s]
Adding requests:  67%|██████▋   | 2737/4096 [00:05<00:02, 509.48it/s]
Adding requests:  68%|██████▊   | 2788/4096 [00:05<00:02, 506.31it/s]
Adding requests:  69%|██████▉   | 2839/4096 [00:05<00:02, 506.27it/s]
Adding requests:  71%|███████   | 2892/4096 [00:05<00:02, 511.11it/s]
Adding requests:  72%|███████▏  | 2944/4096 [00:05<00:02, 507.65it/s]
Adding requests:  73%|███████▎  | 2996/4096 [00:05<00:02, 510.36it/s]
Adding requests:  74%|███████▍  | 3048/4096 [00:06<00:02, 510.90it/s]
Adding requests:  76%|███████▌  | 3100/4096 [00:06<00:01, 511.12it/s]
Adding requests:  77%|███████▋  | 3152/4096 [00:06<00:01, 509.82it/s]
Adding requests:  78%|███████▊  | 3204/4096 [00:06<00:01, 512.13it/s]
Adding requests:  80%|███████▉  | 3258/4096 [00:06<00:01, 515.86it/s]
Adding requests:  81%|████████  | 3311/4096 [00:06<00:01, 516.85it/s]
Adding requests:  82%|████████▏ | 3364/4096 [00:06<00:01, 518.14it/s]
Adding requests:  83%|████████▎ | 3416/4096 [00:06<00:01, 517.43it/s]
Adding requests:  85%|████████▍ | 3468/4096 [00:06<00:01, 509.95it/s]
Adding requests:  86%|████████▌ | 3520/4096 [00:06<00:01, 508.46it/s]
Adding requests:  87%|████████▋ | 3571/4096 [00:07<00:01, 508.33it/s]
Adding requests:  88%|████████▊ | 3622/4096 [00:07<00:00, 493.70it/s]
Adding requests:  90%|████████▉ | 3674/4096 [00:07<00:00, 500.97it/s]
Adding requests:  91%|█████████ | 3726/4096 [00:07<00:00, 503.99it/s]
Adding requests:  92%|█████████▏| 3779/4096 [00:07<00:00, 510.27it/s]
Adding requests:  94%|█████████▎| 3831/4096 [00:07<00:00, 511.97it/s]
Adding requests:  95%|█████████▍| 3884/4096 [00:07<00:00, 515.14it/s]
Adding requests:  96%|█████████▌| 3936/4096 [00:07<00:00, 513.79it/s]
Adding requests:  97%|█████████▋| 3988/4096 [00:07<00:00, 512.49it/s]
Adding requests:  99%|█████████▊| 4040/4096 [00:07<00:00, 513.23it/s]
Adding requests: 100%|█████████▉| 4092/4096 [00:08<00:00, 514.02it/s]
Adding requests: 100%|██████████| 4096/4096 [00:08<00:00, 507.69it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  19%|█▉        | 770/4096 [00:00<00:01, 1773.55it/s, est. speed input: 1816304.17 toks/s, output: 1773.60 toks/s]
Processed prompts:  23%|██▎       | 948/4096 [00:02<00:08, 365.17it/s, est. speed input: 463646.76 toks/s, output: 452.78 toks/s]   
Processed prompts:  25%|██▌       | 1028/4096 [00:03<00:12, 245.73it/s, est. speed input: 340966.40 toks/s, output: 332.97 toks/s]
Processed prompts:  26%|██▌       | 1075/4096 [00:03<00:13, 227.87it/s, est. speed input: 321541.63 toks/s, output: 314.00 toks/s]
Processed prompts:  27%|██▋       | 1109/4096 [00:03<00:14, 203.43it/s, est. speed input: 302160.21 toks/s, output: 295.08 toks/s]
Processed prompts:  28%|██▊       | 1135/4096 [00:04<00:16, 176.39it/s, est. speed input: 284066.76 toks/s, output: 277.41 toks/s]
Processed prompts:  28%|██▊       | 1155/4096 [00:04<00:19, 149.95it/s, est. speed input: 267860.73 toks/s, output: 261.58 toks/s]
Processed prompts:  29%|██▉       | 1186/4096 [00:04<00:21, 135.36it/s, est. speed input: 255652.26 toks/s, output: 249.66 toks/s]
Processed prompts:  30%|██▉       | 1218/4096 [00:05<00:23, 124.92it/s, est. speed input: 245337.18 toks/s, output: 239.59 toks/s]
Processed prompts:  31%|███       | 1250/4096 [00:05<00:24, 117.60it/s, est. speed input: 236574.78 toks/s, output: 231.03 toks/s]
Processed prompts:  31%|███▏      | 1282/4096 [00:05<00:25, 111.44it/s, est. speed input: 228528.19 toks/s, output: 223.17 toks/s]
Processed prompts:  32%|███▏      | 1314/4096 [00:06<00:26, 106.70it/s, est. speed input: 221270.75 toks/s, output: 216.08 toks/s]
Processed prompts:  33%|███▎      | 1346/4096 [00:06<00:26, 103.26it/s, est. speed input: 214759.96 toks/s, output: 209.73 toks/s]
Processed prompts:  34%|███▎      | 1378/4096 [00:06<00:26, 101.17it/s, est. speed input: 209021.61 toks/s, output: 204.12 toks/s]
Processed prompts:  34%|███▍      | 1410/4096 [00:07<00:26, 99.61it/s, est. speed input: 203799.72 toks/s, output: 199.02 toks/s] 
Processed prompts:  35%|███▌      | 1442/4096 [00:07<00:27, 98.28it/s, est. speed input: 198979.19 toks/s, output: 194.31 toks/s]
Processed prompts:  36%|███▌      | 1474/4096 [00:07<00:26, 97.48it/s, est. speed input: 194615.49 toks/s, output: 190.05 toks/s]
Processed prompts:  37%|███▋      | 1506/4096 [00:08<00:26, 96.92it/s, est. speed input: 190614.36 toks/s, output: 186.15 toks/s]
Processed prompts:  38%|███▊      | 1538/4096 [00:08<00:26, 96.57it/s, est. speed input: 186941.53 toks/s, output: 182.56 toks/s]
Processed prompts:  38%|███▊      | 1570/4096 [00:08<00:26, 96.96it/s, est. speed input: 183702.93 toks/s, output: 179.40 toks/s]
Processed prompts:  39%|███▉      | 1602/4096 [00:09<00:25, 96.50it/s, est. speed input: 180531.79 toks/s, output: 176.30 toks/s]
Processed prompts:  40%|███▉      | 1634/4096 [00:09<00:25, 96.16it/s, est. speed input: 177578.88 toks/s, output: 173.42 toks/s]
Processed prompts:  41%|████      | 1666/4096 [00:09<00:25, 95.77it/s, est. speed input: 174800.35 toks/s, output: 170.70 toks/s]
Processed prompts:  41%|████▏     | 1698/4096 [00:10<00:25, 95.70it/s, est. speed input: 172245.72 toks/s, output: 168.21 toks/s]
Processed prompts:  42%|████▏     | 1730/4096 [00:10<00:24, 95.80it/s, est. speed input: 169883.68 toks/s, output: 165.90 toks/s]
Processed prompts:  43%|████▎     | 1762/4096 [00:10<00:24, 95.77it/s, est. speed input: 167649.96 toks/s, output: 163.72 toks/s]
Processed prompts:  44%|████▍     | 1794/4096 [00:11<00:24, 95.57it/s, est. speed input: 165520.85 toks/s, output: 161.64 toks/s]
Processed prompts:  45%|████▍     | 1826/4096 [00:11<00:23, 95.63it/s, est. speed input: 163549.67 toks/s, output: 159.72 toks/s]
Processed prompts:  45%|████▌     | 1858/4096 [00:11<00:23, 96.49it/s, est. speed input: 161820.41 toks/s, output: 158.03 toks/s]
Processed prompts:  46%|████▌     | 1890/4096 [00:12<00:22, 96.27it/s, est. speed input: 160057.31 toks/s, output: 156.31 toks/s]
Processed prompts:  47%|████▋     | 1922/4096 [00:12<00:22, 96.17it/s, est. speed input: 158397.84 toks/s, output: 154.68 toks/s]
Processed prompts:  48%|████▊     | 1954/4096 [00:12<00:22, 96.82it/s, est. speed input: 156927.26 toks/s, output: 153.25 toks/s]
Processed prompts:  48%|████▊     | 1986/4096 [00:13<00:21, 96.59it/s, est. speed input: 155435.41 toks/s, output: 151.79 toks/s]
Processed prompts:  49%|████▉     | 2018/4096 [00:13<00:21, 96.38it/s, est. speed input: 154012.01 toks/s, output: 150.40 toks/s]
Processed prompts:  50%|█████     | 2050/4096 [00:13<00:21, 96.25it/s, est. speed input: 152659.76 toks/s, output: 149.08 toks/s]
Processed prompts:  51%|█████     | 2082/4096 [00:14<00:20, 96.08it/s, est. speed input: 151361.31 toks/s, output: 147.81 toks/s]
Processed prompts:  52%|█████▏    | 2114/4096 [00:14<00:20, 95.92it/s, est. speed input: 150117.85 toks/s, output: 146.60 toks/s]
Processed prompts:  52%|█████▏    | 2146/4096 [00:14<00:20, 95.87it/s, est. speed input: 148939.45 toks/s, output: 145.45 toks/s]
Processed prompts:  53%|█████▎    | 2178/4096 [00:15<00:20, 95.86it/s, est. speed input: 147814.90 toks/s, output: 144.35 toks/s]
Processed prompts:  54%|█████▍    | 2210/4096 [00:15<00:19, 97.36it/s, est. speed input: 146903.77 toks/s, output: 143.46 toks/s]
Processed prompts:  55%|█████▍    | 2242/4096 [00:15<00:19, 96.80it/s, est. speed input: 145858.36 toks/s, output: 142.44 toks/s]
Processed prompts:  56%|█████▌    | 2274/4096 [00:16<00:18, 97.33it/s, est. speed input: 144951.32 toks/s, output: 141.55 toks/s]
Processed prompts:  56%|█████▋    | 2306/4096 [00:16<00:18, 96.72it/s, est. speed input: 143982.20 toks/s, output: 140.61 toks/s]
Processed prompts:  57%|█████▋    | 2338/4096 [00:16<00:18, 97.19it/s, est. speed input: 143139.36 toks/s, output: 139.78 toks/s]
Processed prompts:  58%|█████▊    | 2370/4096 [00:17<00:17, 98.32it/s, est. speed input: 142402.65 toks/s, output: 139.06 toks/s]
Processed prompts:  59%|█████▊    | 2402/4096 [00:17<00:17, 97.33it/s, est. speed input: 141530.95 toks/s, output: 138.21 toks/s]
Processed prompts:  59%|█████▉    | 2434/4096 [00:17<00:17, 96.85it/s, est. speed input: 140710.36 toks/s, output: 137.41 toks/s]
Processed prompts:  60%|██████    | 2466/4096 [00:18<00:16, 96.43it/s, est. speed input: 139912.51 toks/s, output: 136.63 toks/s]
Processed prompts:  61%|██████    | 2498/4096 [00:18<00:16, 97.11it/s, est. speed input: 139227.83 toks/s, output: 135.96 toks/s]
Processed prompts:  62%|██████▏   | 2530/4096 [00:18<00:16, 96.69it/s, est. speed input: 138491.58 toks/s, output: 135.25 toks/s]
Processed prompts:  63%|██████▎   | 2562/4096 [00:19<00:15, 97.10it/s, est. speed input: 137839.27 toks/s, output: 134.61 toks/s]
Processed prompts:  63%|██████▎   | 2594/4096 [00:19<00:15, 96.74it/s, est. speed input: 137156.30 toks/s, output: 133.94 toks/s]
Processed prompts:  64%|██████▍   | 2626/4096 [00:19<00:15, 96.31it/s, est. speed input: 136482.78 toks/s, output: 133.28 toks/s]
Processed prompts:  65%|██████▍   | 2658/4096 [00:20<00:14, 95.93it/s, est. speed input: 135824.89 toks/s, output: 132.64 toks/s]
Processed prompts:  66%|██████▌   | 2690/4096 [00:20<00:14, 95.77it/s, est. speed input: 135197.13 toks/s, output: 132.03 toks/s]
Processed prompts:  66%|██████▋   | 2722/4096 [00:20<00:14, 95.85it/s, est. speed input: 134603.95 toks/s, output: 131.45 toks/s]
Processed prompts:  67%|██████▋   | 2754/4096 [00:21<00:14, 95.85it/s, est. speed input: 134025.94 toks/s, output: 130.88 toks/s]
Processed prompts:  68%|██████▊   | 2786/4096 [00:21<00:13, 95.59it/s, est. speed input: 133446.42 toks/s, output: 130.32 toks/s]
Processed prompts:  69%|██████▉   | 2818/4096 [00:21<00:13, 95.83it/s, est. speed input: 132915.57 toks/s, output: 129.80 toks/s]
Processed prompts:  70%|██████▉   | 2850/4096 [00:22<00:13, 95.82it/s, est. speed input: 132387.61 toks/s, output: 129.28 toks/s]
Processed prompts:  70%|███████   | 2882/4096 [00:22<00:12, 95.53it/s, est. speed input: 131856.38 toks/s, output: 128.77 toks/s]
Processed prompts:  71%|███████   | 2914/4096 [00:22<00:12, 95.35it/s, est. speed input: 131342.37 toks/s, output: 128.26 toks/s]
Processed prompts:  72%|███████▏  | 2946/4096 [00:23<00:12, 95.60it/s, est. speed input: 130868.37 toks/s, output: 127.80 toks/s]
Processed prompts:  73%|███████▎  | 2978/4096 [00:23<00:11, 95.56it/s, est. speed input: 130394.02 toks/s, output: 127.34 toks/s]
Processed prompts:  73%|███████▎  | 3010/4096 [00:23<00:11, 95.34it/s, est. speed input: 129920.47 toks/s, output: 126.88 toks/s]
Processed prompts:  74%|███████▍  | 3042/4096 [00:24<00:11, 95.39it/s, est. speed input: 129472.99 toks/s, output: 126.44 toks/s]
Processed prompts:  75%|███████▌  | 3074/4096 [00:24<00:10, 95.42it/s, est. speed input: 129037.71 toks/s, output: 126.01 toks/s]
Processed prompts:  76%|███████▌  | 3106/4096 [00:24<00:10, 95.43it/s, est. speed input: 128613.61 toks/s, output: 125.60 toks/s]
Processed prompts:  77%|███████▋  | 3138/4096 [00:25<00:09, 96.25it/s, est. speed input: 128248.79 toks/s, output: 125.24 toks/s]
Processed prompts:  77%|███████▋  | 3170/4096 [00:25<00:09, 96.06it/s, est. speed input: 127849.32 toks/s, output: 124.85 toks/s]
Processed prompts:  78%|███████▊  | 3202/4096 [00:25<00:09, 95.92it/s, est. speed input: 127459.54 toks/s, output: 124.47 toks/s]
Processed prompts:  79%|███████▉  | 3234/4096 [00:26<00:09, 95.70it/s, est. speed input: 127072.31 toks/s, output: 124.09 toks/s]
Processed prompts:  80%|███████▉  | 3266/4096 [00:26<00:08, 95.50it/s, est. speed input: 126693.14 toks/s, output: 123.72 toks/s]
Processed prompts:  81%|████████  | 3298/4096 [00:26<00:08, 95.73it/s, est. speed input: 126343.51 toks/s, output: 123.38 toks/s]
Processed prompts:  81%|████████▏ | 3330/4096 [00:27<00:08, 95.60it/s, est. speed input: 125986.83 toks/s, output: 123.03 toks/s]
Processed prompts:  82%|████████▏ | 3362/4096 [00:27<00:07, 95.38it/s, est. speed input: 125631.50 toks/s, output: 122.69 toks/s]
Processed prompts:  83%|████████▎ | 3394/4096 [00:27<00:07, 95.28it/s, est. speed input: 125288.28 toks/s, output: 122.35 toks/s]
Processed prompts:  84%|████████▎ | 3426/4096 [00:28<00:07, 95.26it/s, est. speed input: 124955.57 toks/s, output: 122.03 toks/s]
Processed prompts:  84%|████████▍ | 3458/4096 [00:28<00:06, 95.32it/s, est. speed input: 124634.43 toks/s, output: 121.71 toks/s]
Processed prompts:  85%|████████▌ | 3490/4096 [00:28<00:06, 97.03it/s, est. speed input: 124404.14 toks/s, output: 121.49 toks/s]
Processed prompts:  86%|████████▌ | 3522/4096 [00:29<00:05, 96.49it/s, est. speed input: 124093.45 toks/s, output: 121.18 toks/s]
Processed prompts:  87%|████████▋ | 3554/4096 [00:29<00:05, 95.97it/s, est. speed input: 123782.82 toks/s, output: 120.88 toks/s]
Processed prompts:  88%|████████▊ | 3586/4096 [00:29<00:05, 95.75it/s, est. speed input: 123486.20 toks/s, output: 120.59 toks/s]
Processed prompts:  88%|████████▊ | 3618/4096 [00:30<00:04, 95.72it/s, est. speed input: 123202.13 toks/s, output: 120.31 toks/s]
Processed prompts:  89%|████████▉ | 3650/4096 [00:30<00:04, 95.58it/s, est. speed input: 122918.59 toks/s, output: 120.04 toks/s]
Processed prompts:  90%|████████▉ | 3682/4096 [00:30<00:04, 95.42it/s, est. speed input: 122638.35 toks/s, output: 119.76 toks/s]
Processed prompts:  91%|█████████ | 3714/4096 [00:31<00:03, 96.04it/s, est. speed input: 122397.85 toks/s, output: 119.53 toks/s]
Processed prompts:  91%|█████████▏| 3746/4096 [00:31<00:03, 95.86it/s, est. speed input: 122134.65 toks/s, output: 119.27 toks/s]
Processed prompts:  92%|█████████▏| 3778/4096 [00:31<00:03, 95.61it/s, est. speed input: 121871.33 toks/s, output: 119.01 toks/s]
Processed prompts:  93%|█████████▎| 3810/4096 [00:32<00:02, 95.45it/s, est. speed input: 121613.98 toks/s, output: 118.76 toks/s]
Processed prompts:  94%|█████████▍| 3842/4096 [00:32<00:02, 96.09it/s, est. speed input: 121394.79 toks/s, output: 118.55 toks/s]
Processed prompts:  95%|█████████▍| 3874/4096 [00:32<00:02, 95.81it/s, est. speed input: 121149.18 toks/s, output: 118.31 toks/s]
Processed prompts:  95%|█████████▌| 3906/4096 [00:33<00:01, 95.45it/s, est. speed input: 120901.17 toks/s, output: 118.07 toks/s]
Processed prompts:  96%|█████████▌| 3938/4096 [00:33<00:01, 95.27it/s, est. speed input: 120661.07 toks/s, output: 117.83 toks/s]
Processed prompts:  97%|█████████▋| 3970/4096 [00:33<00:01, 95.22it/s, est. speed input: 120428.94 toks/s, output: 117.61 toks/s]
Processed prompts:  98%|█████████▊| 4002/4096 [00:34<00:00, 95.21it/s, est. speed input: 120202.89 toks/s, output: 117.39 toks/s]
Processed prompts:  98%|█████████▊| 4034/4096 [00:34<00:00, 95.92it/s, est. speed input: 120009.94 toks/s, output: 117.20 toks/s]
Processed prompts:  99%|█████████▉| 4066/4096 [00:34<00:00, 96.77it/s, est. speed input: 119834.34 toks/s, output: 117.03 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:34<00:00, 96.77it/s, est. speed input: 120715.28 toks/s, output: 117.89 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:34<00:00, 117.89it/s, est. speed input: 120715.28 toks/s, output: 117.89 toks/s]
[rank0]:[W128 09:22:55.557560711 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 98.9s

测试结果:
  Requests/s:   95.66
  Tokens/s:     98055.66
  Total Reqs:   4096
  Elapsed:      42.82s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     97960.00


------------------------------------------------------------
  生成 CSV: BitNet-2B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/BitNet-2B-FP8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,31.7430,16284.1538,4.0324
1024,1024,1,128,128,31.8279,32623.6173,4.0216
2048,1024,2,256,128,61.9981,63548.0370,4.1292
4096,1024,4,512,128,79.9825,81982.0365,6.4014
8192,1024,8,1024,128,89.6931,91935.4781,11.4167
16384,1024,16,2048,128,93.4698,95806.5725,21.9108
32768,1024,32,4096,128,95.6641,98055.6600,42.8165

------------------------------------------------------------

[INFO] 完成: 7 成功, 0 失败

============================================================
  BitNet-2B-FP8 | cuSPARSELt (2_10) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_10
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10

============================================================
[1/7] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 09:23:04 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3379462) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3379462) WARNING 01-28 09:23:19 [backends.py:609] Failed to read file <frozen os>
Throughput: 32.43 requests/s, 16638.63 total tokens/s, 32.43 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-28 09:23:04] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 09:23:04] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:23:04] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:23:04] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:23:04] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:23:04] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:23:04] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:23:04] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:23:04] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:23:04] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:23:04] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:23:04] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:23:04] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:23:04] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 09:23:10] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 09:23:11] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:23:11] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:23:11] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:23:11] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:23:11] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:23:11] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:23:11] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:23:11] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:23:11] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:23:11] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:23:11] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:23:11] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:23:11] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3379462) [2026-01-28 09:23:12] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3379462) [2026-01-28 09:23:12] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3379462) [2026-01-28 09:23:12] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3379462) [2026-01-28 09:23:12] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3379462) [2026-01-28 09:23:12] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3379462) [2026-01-28 09:23:12] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3379462) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3379462) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.01it/s]
(EngineCore_DP0 pid=3379462) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.01it/s]
(EngineCore_DP0 pid=3379462) 
(EngineCore_DP0 pid=3379462) [2026-01-28 09:23:13] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=3379462) [2026-01-28 09:23:13] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9830400 bytes
(EngineCore_DP0 pid=3379462) [2026-01-28 09:23:13] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=3379462) [2026-01-28 09:23:13] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6553600 bytes
(EngineCore_DP0 pid=3379462) [2026-01-28 09:23:13] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=3379462) [2026-01-28 09:23:13] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 35389440 bytes
(EngineCore_DP0 pid=3379462) [2026-01-28 09:23:13] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=3379462) [2026-01-28 09:23:13] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 17715200 bytes
(EngineCore_DP0 pid=3379462) 2026-01-28 09:23:30,895 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3379462) 2026-01-28 09:23:30,925 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3379462) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  4.90it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.88it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  4.01it/s]
(EngineCore_DP0 pid=3379462) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 15.48it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  50%|█████     | 64/128 [00:00<00:00, 639.66it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 736.81it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:16,  7.84it/s, est. speed input: 4015.95 toks/s, output: 7.84 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:05, 23.29it/s, est. speed input: 10664.00 toks/s, output: 20.83 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:04, 28.85it/s, est. speed input: 13109.08 toks/s, output: 25.60 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:03, 31.50it/s, est. speed input: 14359.05 toks/s, output: 28.04 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:03, 32.98it/s, est. speed input: 15116.14 toks/s, output: 29.52 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:03, 33.88it/s, est. speed input: 15627.73 toks/s, output: 30.52 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:00<00:02, 34.53it/s, est. speed input: 16011.20 toks/s, output: 31.27 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:00<00:02, 34.92it/s, est. speed input: 16292.50 toks/s, output: 31.82 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:01<00:02, 34.96it/s, est. speed input: 16476.92 toks/s, output: 32.18 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:01<00:02, 34.92it/s, est. speed input: 16613.91 toks/s, output: 32.45 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:01<00:02, 34.95it/s, est. speed input: 16733.24 toks/s, output: 32.68 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:01<00:02, 34.97it/s, est. speed input: 16833.54 toks/s, output: 32.88 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:01<00:02, 34.98it/s, est. speed input: 16917.47 toks/s, output: 33.04 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:01<00:02, 34.98it/s, est. speed input: 16988.33 toks/s, output: 33.18 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:01<00:02, 34.95it/s, est. speed input: 17046.52 toks/s, output: 33.29 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:01<00:01, 34.92it/s, est. speed input: 17096.25 toks/s, output: 33.39 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:01<00:01, 34.92it/s, est. speed input: 17142.20 toks/s, output: 33.48 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:02<00:01, 34.89it/s, est. speed input: 17180.81 toks/s, output: 33.56 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:02<00:01, 34.85it/s, est. speed input: 17213.35 toks/s, output: 33.62 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:02<00:01, 34.82it/s, est. speed input: 17242.48 toks/s, output: 33.68 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:02<00:01, 34.81it/s, est. speed input: 17269.54 toks/s, output: 33.73 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:02<00:01, 34.81it/s, est. speed input: 17295.49 toks/s, output: 33.78 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:02<00:01, 34.86it/s, est. speed input: 17321.95 toks/s, output: 33.83 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:02<00:01, 34.84it/s, est. speed input: 17342.19 toks/s, output: 33.87 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:02<00:00, 34.91it/s, est. speed input: 17366.57 toks/s, output: 33.92 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:02<00:00, 34.91it/s, est. speed input: 17386.17 toks/s, output: 33.96 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:03<00:00, 34.88it/s, est. speed input: 17402.53 toks/s, output: 33.99 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:03<00:00, 34.89it/s, est. speed input: 17419.71 toks/s, output: 34.02 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:03<00:00, 34.93it/s, est. speed input: 17437.11 toks/s, output: 34.06 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:03<00:00, 34.96it/s, est. speed input: 17453.61 toks/s, output: 34.09 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:03<00:00, 34.80it/s, est. speed input: 17459.29 toks/s, output: 34.10 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:03<00:00, 33.75it/s, est. speed input: 17413.85 toks/s, output: 34.01 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 33.75it/s, est. speed input: 17378.24 toks/s, output: 33.94 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 33.94it/s, est. speed input: 17378.24 toks/s, output: 33.94 toks/s]
[rank0]:[W128 09:23:37.207374706 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 41.6s

测试结果:
  Requests/s:   32.43
  Tokens/s:     16638.63
  Total Reqs:   128
  Elapsed:      3.95s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     16606.19

============================================================
[2/7] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 09:23:46 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3380597) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3380597) WARNING 01-28 09:24:01 [backends.py:609] Failed to read file <frozen os>
Throughput: 31.68 requests/s, 32468.59 total tokens/s, 31.68 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-28 09:23:45] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 09:23:46] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:23:46] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:23:46] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:23:46] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:23:46] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:23:46] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:23:46] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:23:46] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:23:46] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:23:46] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:23:46] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:23:46] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:23:46] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 09:23:52] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 09:23:53] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:23:53] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:23:53] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:23:53] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:23:53] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:23:53] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:23:53] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:23:53] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:23:53] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:23:53] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:23:53] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:23:53] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:23:53] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3380597) [2026-01-28 09:23:54] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3380597) [2026-01-28 09:23:54] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3380597) [2026-01-28 09:23:54] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3380597) [2026-01-28 09:23:54] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3380597) [2026-01-28 09:23:54] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3380597) [2026-01-28 09:23:54] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3380597) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3380597) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.10it/s]
(EngineCore_DP0 pid=3380597) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.10it/s]
(EngineCore_DP0 pid=3380597) 
(EngineCore_DP0 pid=3380597) [2026-01-28 09:23:55] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=3380597) [2026-01-28 09:23:55] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9830400 bytes
(EngineCore_DP0 pid=3380597) [2026-01-28 09:23:55] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=3380597) [2026-01-28 09:23:55] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6553600 bytes
(EngineCore_DP0 pid=3380597) [2026-01-28 09:23:55] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=3380597) [2026-01-28 09:23:55] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 35389440 bytes
(EngineCore_DP0 pid=3380597) [2026-01-28 09:23:55] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=3380597) [2026-01-28 09:23:55] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 17715200 bytes
(EngineCore_DP0 pid=3380597) 2026-01-28 09:24:12,548 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3380597) 2026-01-28 09:24:12,576 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3380597) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 12.59it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 12.58it/s]
(EngineCore_DP0 pid=3380597) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 15.22it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  23%|██▎       | 29/128 [00:00<00:00, 285.49it/s]
Adding requests:  63%|██████▎   | 81/128 [00:00<00:00, 421.15it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 431.62it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   5%|▍         | 6/128 [00:00<00:02, 50.72it/s, est. speed input: 51951.69 toks/s, output: 50.72 toks/s]
Processed prompts:   9%|▉         | 12/128 [00:00<00:02, 39.28it/s, est. speed input: 41636.26 toks/s, output: 40.66 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:03, 36.69it/s, est. speed input: 39189.40 toks/s, output: 38.27 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:03, 35.59it/s, est. speed input: 38145.24 toks/s, output: 37.25 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:00<00:02, 34.88it/s, est. speed input: 37456.55 toks/s, output: 36.58 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:00<00:02, 34.39it/s, est. speed input: 36963.02 toks/s, output: 36.10 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:00<00:02, 34.13it/s, est. speed input: 36627.83 toks/s, output: 35.77 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:01<00:02, 33.96it/s, est. speed input: 36372.03 toks/s, output: 35.52 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:01<00:02, 33.93it/s, est. speed input: 36196.88 toks/s, output: 35.35 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:01<00:02, 33.92it/s, est. speed input: 36060.61 toks/s, output: 35.22 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:01<00:02, 33.89it/s, est. speed input: 35937.65 toks/s, output: 35.10 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:01<00:02, 33.82it/s, est. speed input: 35822.98 toks/s, output: 34.98 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:01<00:02, 33.73it/s, est. speed input: 35712.98 toks/s, output: 34.88 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:01<00:01, 33.64it/s, est. speed input: 35613.74 toks/s, output: 34.78 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:01<00:01, 33.65it/s, est. speed input: 35541.92 toks/s, output: 34.71 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:01<00:01, 33.73it/s, est. speed input: 35492.72 toks/s, output: 34.66 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:02<00:01, 33.74it/s, est. speed input: 35442.14 toks/s, output: 34.61 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:02<00:01, 33.74it/s, est. speed input: 35394.67 toks/s, output: 34.56 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:02<00:01, 33.74it/s, est. speed input: 35352.23 toks/s, output: 34.52 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:02<00:01, 33.74it/s, est. speed input: 35313.75 toks/s, output: 34.49 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:02<00:01, 33.78it/s, est. speed input: 35285.19 toks/s, output: 34.46 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:02<00:01, 33.79it/s, est. speed input: 35256.01 toks/s, output: 34.43 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:02<00:00, 33.76it/s, est. speed input: 35224.24 toks/s, output: 34.40 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:02<00:00, 33.74it/s, est. speed input: 35195.23 toks/s, output: 34.37 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:03<00:00, 33.71it/s, est. speed input: 35165.42 toks/s, output: 34.34 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:03<00:00, 33.60it/s, est. speed input: 35127.52 toks/s, output: 34.30 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:03<00:00, 33.64it/s, est. speed input: 35105.91 toks/s, output: 34.28 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:03<00:00, 33.62it/s, est. speed input: 35080.51 toks/s, output: 34.26 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:03<00:00, 33.65it/s, est. speed input: 35062.22 toks/s, output: 34.24 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:03<00:00, 33.62it/s, est. speed input: 35039.21 toks/s, output: 34.22 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 33.62it/s, est. speed input: 35023.05 toks/s, output: 34.20 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 34.20it/s, est. speed input: 35023.05 toks/s, output: 34.20 toks/s]
[rank0]:[W128 09:24:18.612873602 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 41.5s

测试结果:
  Requests/s:   31.68
  Tokens/s:     32468.59
  Total Reqs:   128
  Elapsed:      4.04s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     32436.91

============================================================
[3/7] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 09:24:28 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3381686) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3381686) WARNING 01-28 09:24:43 [backends.py:609] Failed to read file <frozen os>
Throughput: 66.67 requests/s, 68334.85 total tokens/s, 66.67 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-28 09:24:27] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 09:24:28] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:24:28] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:24:28] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:24:28] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:24:28] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:24:28] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:24:28] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:24:28] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:24:28] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:24:28] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:24:28] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:24:28] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:24:28] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 09:24:34] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 09:24:34] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:24:34] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:24:34] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:24:34] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:24:34] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:24:34] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:24:34] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:24:34] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:24:34] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:24:34] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:24:34] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:24:34] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:24:34] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3381686) [2026-01-28 09:24:35] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3381686) [2026-01-28 09:24:35] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3381686) [2026-01-28 09:24:35] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3381686) [2026-01-28 09:24:35] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3381686) [2026-01-28 09:24:35] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3381686) [2026-01-28 09:24:35] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3381686) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3381686) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.09it/s]
(EngineCore_DP0 pid=3381686) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.09it/s]
(EngineCore_DP0 pid=3381686) 
(EngineCore_DP0 pid=3381686) [2026-01-28 09:24:36] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=3381686) [2026-01-28 09:24:36] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9830400 bytes
(EngineCore_DP0 pid=3381686) [2026-01-28 09:24:36] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=3381686) [2026-01-28 09:24:36] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6553600 bytes
(EngineCore_DP0 pid=3381686) [2026-01-28 09:24:36] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=3381686) [2026-01-28 09:24:36] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 35389440 bytes
(EngineCore_DP0 pid=3381686) [2026-01-28 09:24:36] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=3381686) [2026-01-28 09:24:37] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 17715200 bytes
(EngineCore_DP0 pid=3381686) 2026-01-28 09:24:54,245 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3381686) 2026-01-28 09:24:54,273 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3381686) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:00<00:00, 13.10it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 12.92it/s]
(EngineCore_DP0 pid=3381686) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 16.17it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 16.16it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  11%|█▏        | 29/256 [00:00<00:00, 288.87it/s]
Adding requests:  32%|███▏      | 81/256 [00:00<00:00, 424.50it/s]
Adding requests:  51%|█████     | 131/256 [00:00<00:00, 456.14it/s]
Adding requests:  71%|███████   | 181/256 [00:00<00:00, 470.13it/s]
Adding requests:  91%|█████████ | 233/256 [00:00<00:00, 484.56it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 464.71it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  13%|█▎        | 34/256 [00:00<00:00, 264.62it/s, est. speed input: 271006.53 toks/s, output: 264.63 toks/s]
Processed prompts:  24%|██▍       | 61/256 [00:00<00:01, 110.76it/s, est. speed input: 125638.46 toks/s, output: 122.69 toks/s]
Processed prompts:  30%|███       | 77/256 [00:00<00:01, 93.97it/s, est. speed input: 108786.82 toks/s, output: 106.23 toks/s] 
Processed prompts:  35%|███▍      | 89/256 [00:00<00:01, 86.67it/s, est. speed input: 101759.54 toks/s, output: 99.37 toks/s] 
Processed prompts:  39%|███▊      | 99/256 [00:01<00:01, 82.25it/s, est. speed input: 97660.04 toks/s, output: 95.37 toks/s] 
Processed prompts:  42%|████▏     | 108/256 [00:01<00:01, 76.97it/s, est. speed input: 93686.95 toks/s, output: 91.49 toks/s]
Processed prompts:  45%|████▌     | 116/256 [00:01<00:01, 75.39it/s, est. speed input: 91786.08 toks/s, output: 89.63 toks/s]
Processed prompts:  48%|████▊     | 124/256 [00:01<00:01, 74.06it/s, est. speed input: 90178.06 toks/s, output: 88.06 toks/s]
Processed prompts:  52%|█████▏    | 132/256 [00:01<00:01, 73.05it/s, est. speed input: 88816.01 toks/s, output: 86.73 toks/s]
Processed prompts:  55%|█████▍    | 140/256 [00:01<00:01, 72.29it/s, est. speed input: 87645.30 toks/s, output: 85.59 toks/s]
Processed prompts:  58%|█████▊    | 148/256 [00:01<00:01, 71.82it/s, est. speed input: 86651.65 toks/s, output: 84.62 toks/s]
Processed prompts:  61%|██████    | 156/256 [00:01<00:01, 71.35it/s, est. speed input: 85748.64 toks/s, output: 83.74 toks/s]
Processed prompts:  64%|██████▍   | 164/256 [00:01<00:01, 71.08it/s, est. speed input: 84964.71 toks/s, output: 82.97 toks/s]
Processed prompts:  67%|██████▋   | 172/256 [00:02<00:01, 70.77it/s, est. speed input: 84239.87 toks/s, output: 82.26 toks/s]
Processed prompts:  70%|███████   | 180/256 [00:02<00:01, 70.57it/s, est. speed input: 83596.12 toks/s, output: 81.64 toks/s]
Processed prompts:  73%|███████▎  | 188/256 [00:02<00:00, 70.44it/s, est. speed input: 83016.39 toks/s, output: 81.07 toks/s]
Processed prompts:  77%|███████▋  | 196/256 [00:02<00:00, 70.44it/s, est. speed input: 82507.98 toks/s, output: 80.57 toks/s]
Processed prompts:  80%|███████▉  | 204/256 [00:02<00:00, 70.39it/s, est. speed input: 82035.63 toks/s, output: 80.11 toks/s]
Processed prompts:  83%|████████▎ | 212/256 [00:02<00:00, 70.25it/s, est. speed input: 81586.84 toks/s, output: 79.67 toks/s]
Processed prompts:  86%|████████▌ | 220/256 [00:02<00:00, 70.17it/s, est. speed input: 81177.73 toks/s, output: 79.27 toks/s]
Processed prompts:  89%|████████▉ | 228/256 [00:02<00:00, 70.08it/s, est. speed input: 80797.38 toks/s, output: 78.90 toks/s]
Processed prompts:  92%|█████████▏| 236/256 [00:03<00:00, 70.09it/s, est. speed input: 80454.65 toks/s, output: 78.57 toks/s]
Processed prompts:  95%|█████████▌| 244/256 [00:03<00:00, 70.14it/s, est. speed input: 80143.97 toks/s, output: 78.26 toks/s]
Processed prompts:  98%|█████████▊| 252/256 [00:03<00:00, 70.25it/s, est. speed input: 79864.88 toks/s, output: 77.99 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 70.25it/s, est. speed input: 79747.67 toks/s, output: 77.88 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 77.87it/s, est. speed input: 79747.67 toks/s, output: 77.88 toks/s]
[rank0]:[W128 09:25:00.272233552 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 41.5s

测试结果:
  Requests/s:   66.67
  Tokens/s:     68334.85
  Total Reqs:   256
  Elapsed:      3.84s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     68268.18

============================================================
[4/7] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 09:25:11 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3382778) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3382778) WARNING 01-28 09:25:26 [backends.py:609] Failed to read file <frozen os>
Throughput: 78.37 requests/s, 80329.84 total tokens/s, 78.37 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-28 09:25:10] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 09:25:11] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:25:11] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:25:11] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:25:11] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:25:11] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:25:11] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:25:11] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:25:11] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:25:11] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:25:11] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:25:11] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:25:11] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:25:11] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 09:25:17] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 09:25:18] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:25:18] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:25:18] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:25:18] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:25:18] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:25:18] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:25:18] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:25:18] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:25:18] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:25:18] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:25:18] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:25:18] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:25:18] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3382778) [2026-01-28 09:25:19] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3382778) [2026-01-28 09:25:19] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3382778) [2026-01-28 09:25:19] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3382778) [2026-01-28 09:25:19] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3382778) [2026-01-28 09:25:19] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3382778) [2026-01-28 09:25:19] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3382778) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3382778) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.10it/s]
(EngineCore_DP0 pid=3382778) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.10it/s]
(EngineCore_DP0 pid=3382778) 
(EngineCore_DP0 pid=3382778) [2026-01-28 09:25:20] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=3382778) [2026-01-28 09:25:20] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9830400 bytes
(EngineCore_DP0 pid=3382778) [2026-01-28 09:25:20] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=3382778) [2026-01-28 09:25:20] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6553600 bytes
(EngineCore_DP0 pid=3382778) [2026-01-28 09:25:20] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=3382778) [2026-01-28 09:25:20] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 35389440 bytes
(EngineCore_DP0 pid=3382778) [2026-01-28 09:25:20] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=3382778) [2026-01-28 09:25:20] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 17715200 bytes
(EngineCore_DP0 pid=3382778) 2026-01-28 09:25:37,527 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3382778) 2026-01-28 09:25:37,554 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3382778) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00, 13.24it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00,  9.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00,  9.67it/s]
(EngineCore_DP0 pid=3382778) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  33%|███▎      | 1/3 [00:00<00:00,  9.46it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00,  9.43it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 10.67it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   6%|▌         | 29/512 [00:00<00:01, 286.85it/s]
Adding requests:  16%|█▌        | 82/512 [00:00<00:01, 424.87it/s]
Adding requests:  26%|██▌       | 133/512 [00:00<00:00, 462.02it/s]
Adding requests:  36%|███▌      | 183/512 [00:00<00:00, 474.04it/s]
Adding requests:  45%|████▌     | 232/512 [00:00<00:00, 477.20it/s]
Adding requests:  55%|█████▌    | 282/512 [00:00<00:00, 482.87it/s]
Adding requests:  65%|██████▍   | 331/512 [00:00<00:00, 484.67it/s]
Adding requests:  75%|███████▍  | 382/512 [00:00<00:00, 490.82it/s]
Adding requests:  85%|████████▍ | 433/512 [00:00<00:00, 495.06it/s]
Adding requests:  94%|█████████▍| 483/512 [00:01<00:00, 493.80it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 477.31it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  11%|█         | 54/512 [00:00<00:01, 436.13it/s, est. speed input: 446754.70 toks/s, output: 436.17 toks/s]
Processed prompts:  19%|█▉        | 98/512 [00:00<00:03, 136.78it/s, est. speed input: 158002.79 toks/s, output: 154.29 toks/s]
Processed prompts:  24%|██▍       | 122/512 [00:00<00:03, 114.50it/s, est. speed input: 134877.95 toks/s, output: 131.71 toks/s]
Processed prompts:  27%|██▋       | 139/512 [00:01<00:03, 107.95it/s, est. speed input: 127781.78 toks/s, output: 124.79 toks/s]
Processed prompts:  30%|██▉       | 153/512 [00:01<00:03, 106.23it/s, est. speed input: 124997.88 toks/s, output: 122.07 toks/s]
Processed prompts:  32%|███▏      | 166/512 [00:01<00:03, 95.37it/s, est. speed input: 118126.27 toks/s, output: 115.36 toks/s] 
Processed prompts:  35%|███▍      | 178/512 [00:01<00:03, 93.13it/s, est. speed input: 115489.95 toks/s, output: 112.78 toks/s]
Processed prompts:  37%|███▋      | 190/512 [00:01<00:03, 91.13it/s, est. speed input: 113199.22 toks/s, output: 110.54 toks/s]
Processed prompts:  39%|███▉      | 202/512 [00:01<00:03, 89.09it/s, est. speed input: 111082.54 toks/s, output: 108.48 toks/s]
Processed prompts:  42%|████▏     | 214/512 [00:02<00:03, 87.87it/s, est. speed input: 109362.38 toks/s, output: 106.80 toks/s]
Processed prompts:  44%|████▍     | 226/512 [00:02<00:03, 87.22it/s, est. speed input: 107939.91 toks/s, output: 105.41 toks/s]
Processed prompts:  46%|████▋     | 238/512 [00:02<00:03, 86.67it/s, est. speed input: 106673.42 toks/s, output: 104.17 toks/s]
Processed prompts:  49%|████▉     | 250/512 [00:02<00:03, 86.49it/s, est. speed input: 105604.91 toks/s, output: 103.13 toks/s]
Processed prompts:  51%|█████     | 262/512 [00:02<00:02, 86.10it/s, est. speed input: 104596.05 toks/s, output: 102.14 toks/s]
Processed prompts:  54%|█████▎    | 274/512 [00:02<00:02, 86.02it/s, est. speed input: 103731.56 toks/s, output: 101.30 toks/s]
Processed prompts:  56%|█████▌    | 286/512 [00:02<00:02, 85.48it/s, est. speed input: 102856.46 toks/s, output: 100.44 toks/s]
Processed prompts:  58%|█████▊    | 298/512 [00:02<00:02, 85.27it/s, est. speed input: 102097.21 toks/s, output: 99.70 toks/s] 
Processed prompts:  61%|██████    | 310/512 [00:03<00:02, 85.09it/s, est. speed input: 101400.55 toks/s, output: 99.02 toks/s]
Processed prompts:  63%|██████▎   | 322/512 [00:03<00:02, 85.25it/s, est. speed input: 100812.51 toks/s, output: 98.45 toks/s]
Processed prompts:  65%|██████▌   | 334/512 [00:03<00:02, 85.25it/s, est. speed input: 100254.70 toks/s, output: 97.90 toks/s]
Processed prompts:  68%|██████▊   | 346/512 [00:03<00:01, 86.67it/s, est. speed input: 99957.59 toks/s, output: 97.61 toks/s] 
Processed prompts:  70%|██████▉   | 358/512 [00:03<00:01, 86.35it/s, est. speed input: 99490.23 toks/s, output: 97.16 toks/s]
Processed prompts:  72%|███████▏  | 370/512 [00:03<00:01, 85.96it/s, est. speed input: 99033.54 toks/s, output: 96.71 toks/s]
Processed prompts:  75%|███████▍  | 382/512 [00:03<00:01, 85.57it/s, est. speed input: 98593.45 toks/s, output: 96.28 toks/s]
Processed prompts:  77%|███████▋  | 394/512 [00:04<00:01, 85.37it/s, est. speed input: 98193.59 toks/s, output: 95.89 toks/s]
Processed prompts:  79%|███████▉  | 406/512 [00:04<00:01, 85.34it/s, est. speed input: 97831.94 toks/s, output: 95.54 toks/s]
Processed prompts:  82%|████████▏ | 418/512 [00:04<00:01, 85.22it/s, est. speed input: 97483.08 toks/s, output: 95.20 toks/s]
Processed prompts:  84%|████████▍ | 430/512 [00:04<00:00, 85.18it/s, est. speed input: 97160.29 toks/s, output: 94.88 toks/s]
Processed prompts:  86%|████████▋ | 442/512 [00:04<00:00, 85.04it/s, est. speed input: 96845.55 toks/s, output: 94.58 toks/s]
Processed prompts:  89%|████████▊ | 454/512 [00:04<00:00, 86.45it/s, est. speed input: 96713.21 toks/s, output: 94.45 toks/s]
Processed prompts:  91%|█████████ | 466/512 [00:04<00:00, 86.01it/s, est. speed input: 96436.81 toks/s, output: 94.18 toks/s]
Processed prompts:  93%|█████████▎| 478/512 [00:05<00:00, 85.44it/s, est. speed input: 96149.82 toks/s, output: 93.90 toks/s]
Processed prompts:  96%|█████████▌| 490/512 [00:05<00:00, 85.32it/s, est. speed input: 95904.48 toks/s, output: 93.66 toks/s]
Processed prompts:  98%|█████████▊| 502/512 [00:05<00:00, 85.36it/s, est. speed input: 95685.01 toks/s, output: 93.44 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:05<00:00, 85.36it/s, est. speed input: 96051.90 toks/s, output: 93.80 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:05<00:00, 93.80it/s, est. speed input: 96051.90 toks/s, output: 93.80 toks/s]
[rank0]:[W128 09:25:46.572109592 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 46.4s

测试结果:
  Requests/s:   78.37
  Tokens/s:     80329.84
  Total Reqs:   512
  Elapsed:      6.53s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     80251.46

============================================================
[5/7] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 09:25:59 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3383932) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3383932) WARNING 01-28 09:26:15 [backends.py:609] Failed to read file <frozen os>
Throughput: 85.96 requests/s, 88112.61 total tokens/s, 85.96 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-28 09:25:58] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 09:25:59] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:25:59] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:25:59] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:25:59] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:25:59] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:25:59] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:25:59] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:25:59] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:25:59] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:25:59] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:25:59] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:25:59] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:25:59] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 09:26:05] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 09:26:06] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:26:06] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:26:06] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:26:06] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:26:06] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:26:06] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:26:06] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:26:06] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:26:06] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:26:06] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:26:06] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:26:06] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:26:06] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3383932) [2026-01-28 09:26:07] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3383932) [2026-01-28 09:26:07] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3383932) [2026-01-28 09:26:07] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3383932) [2026-01-28 09:26:07] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3383932) [2026-01-28 09:26:07] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3383932) [2026-01-28 09:26:07] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3383932) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3383932) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.09it/s]
(EngineCore_DP0 pid=3383932) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.09it/s]
(EngineCore_DP0 pid=3383932) 
(EngineCore_DP0 pid=3383932) [2026-01-28 09:26:08] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=3383932) [2026-01-28 09:26:08] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9830400 bytes
(EngineCore_DP0 pid=3383932) [2026-01-28 09:26:08] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=3383932) [2026-01-28 09:26:08] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6553600 bytes
(EngineCore_DP0 pid=3383932) [2026-01-28 09:26:08] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=3383932) [2026-01-28 09:26:08] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 35389440 bytes
(EngineCore_DP0 pid=3383932) [2026-01-28 09:26:08] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=3383932) [2026-01-28 09:26:08] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 17715200 bytes
(EngineCore_DP0 pid=3383932) 2026-01-28 09:26:25,921 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3383932) 2026-01-28 09:26:25,949 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3383932) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 1/5 [00:00<00:01,  3.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00,  5.11it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00,  8.85it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00,  7.87it/s]
(EngineCore_DP0 pid=3383932) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00, 15.71it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 16.45it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 16.33it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   3%|▎         | 31/1024 [00:00<00:03, 304.88it/s]
Adding requests:   8%|▊         | 83/1024 [00:00<00:02, 426.90it/s]
Adding requests:  13%|█▎        | 133/1024 [00:00<00:01, 456.24it/s]
Adding requests:  18%|█▊        | 181/1024 [00:00<00:01, 463.21it/s]
Adding requests:  23%|██▎       | 231/1024 [00:00<00:01, 475.64it/s]
Adding requests:  27%|██▋       | 280/1024 [00:00<00:01, 477.72it/s]
Adding requests:  32%|███▏      | 330/1024 [00:00<00:01, 482.45it/s]
Adding requests:  37%|███▋      | 382/1024 [00:00<00:01, 491.41it/s]
Adding requests:  42%|████▏     | 433/1024 [00:00<00:01, 496.33it/s]
Adding requests:  47%|████▋     | 483/1024 [00:01<00:01, 497.29it/s]
Adding requests:  52%|█████▏    | 533/1024 [00:01<00:01, 479.74it/s]
Adding requests:  57%|█████▋    | 584/1024 [00:01<00:00, 488.33it/s]
Adding requests:  62%|██████▏   | 635/1024 [00:01<00:00, 492.12it/s]
Adding requests:  67%|██████▋   | 687/1024 [00:01<00:00, 498.24it/s]
Adding requests:  72%|███████▏  | 739/1024 [00:01<00:00, 502.08it/s]
Adding requests:  77%|███████▋  | 790/1024 [00:01<00:00, 497.37it/s]
Adding requests:  82%|████████▏ | 840/1024 [00:01<00:00, 490.31it/s]
Adding requests:  87%|████████▋ | 893/1024 [00:01<00:00, 500.51it/s]
Adding requests:  92%|█████████▏| 944/1024 [00:01<00:00, 501.43it/s]
Adding requests:  97%|█████████▋| 996/1024 [00:02<00:00, 503.95it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 488.07it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  16%|█▌        | 162/1024 [00:00<00:00, 972.90it/s, est. speed input: 996512.13 toks/s, output: 972.97 toks/s]
Processed prompts:  25%|██▌       | 260/1024 [00:01<00:04, 176.38it/s, est. speed input: 213251.10 toks/s, output: 208.25 toks/s]
Processed prompts:  30%|██▉       | 306/1024 [00:01<00:05, 140.51it/s, est. speed input: 175055.74 toks/s, output: 170.95 toks/s]
Processed prompts:  33%|███▎      | 335/1024 [00:02<00:05, 132.92it/s, est. speed input: 166251.28 toks/s, output: 162.35 toks/s]
Processed prompts:  35%|███▍      | 357/1024 [00:02<00:05, 121.38it/s, est. speed input: 157089.58 toks/s, output: 153.41 toks/s]
Processed prompts:  37%|███▋      | 374/1024 [00:02<00:05, 116.48it/s, est. speed input: 152797.72 toks/s, output: 149.22 toks/s]
Processed prompts:  38%|███▊      | 389/1024 [00:02<00:05, 109.38it/s, est. speed input: 148209.62 toks/s, output: 144.74 toks/s]
Processed prompts:  39%|███▉      | 402/1024 [00:02<00:06, 100.50it/s, est. speed input: 143418.10 toks/s, output: 140.05 toks/s]
Processed prompts:  41%|████      | 418/1024 [00:03<00:06, 97.40it/s, est. speed input: 140246.22 toks/s, output: 136.96 toks/s] 
Processed prompts:  42%|████▏     | 434/1024 [00:03<00:06, 95.08it/s, est. speed input: 137471.91 toks/s, output: 134.25 toks/s]
Processed prompts:  44%|████▍     | 450/1024 [00:03<00:06, 94.64it/s, est. speed input: 135364.46 toks/s, output: 132.19 toks/s]
Processed prompts:  46%|████▌     | 466/1024 [00:03<00:06, 92.93it/s, est. speed input: 133114.44 toks/s, output: 129.99 toks/s]
Processed prompts:  47%|████▋     | 482/1024 [00:03<00:05, 91.43it/s, est. speed input: 131019.86 toks/s, output: 127.95 toks/s]
Processed prompts:  49%|████▊     | 498/1024 [00:03<00:05, 90.40it/s, est. speed input: 129131.05 toks/s, output: 126.10 toks/s]
Processed prompts:  50%|█████     | 514/1024 [00:04<00:05, 89.79it/s, est. speed input: 127433.40 toks/s, output: 124.45 toks/s]
Processed prompts:  52%|█████▏    | 530/1024 [00:04<00:05, 89.51it/s, est. speed input: 125910.74 toks/s, output: 122.96 toks/s]
Processed prompts:  53%|█████▎    | 546/1024 [00:04<00:05, 89.35it/s, est. speed input: 124516.53 toks/s, output: 121.60 toks/s]
Processed prompts:  55%|█████▍    | 562/1024 [00:04<00:05, 88.98it/s, est. speed input: 123184.33 toks/s, output: 120.30 toks/s]
Processed prompts:  56%|█████▋    | 578/1024 [00:04<00:05, 88.68it/s, est. speed input: 121945.35 toks/s, output: 119.09 toks/s]
Processed prompts:  58%|█████▊    | 594/1024 [00:05<00:04, 88.48it/s, est. speed input: 120795.76 toks/s, output: 117.96 toks/s]
Processed prompts:  60%|█████▉    | 610/1024 [00:05<00:04, 88.39it/s, est. speed input: 119734.03 toks/s, output: 116.93 toks/s]
Processed prompts:  61%|██████    | 626/1024 [00:05<00:04, 88.54it/s, est. speed input: 118777.39 toks/s, output: 115.99 toks/s]
Processed prompts:  63%|██████▎   | 642/1024 [00:05<00:04, 88.46it/s, est. speed input: 117854.91 toks/s, output: 115.09 toks/s]
Processed prompts:  64%|██████▍   | 658/1024 [00:05<00:04, 88.42it/s, est. speed input: 116992.99 toks/s, output: 114.25 toks/s]
Processed prompts:  66%|██████▌   | 674/1024 [00:05<00:03, 88.15it/s, est. speed input: 116150.82 toks/s, output: 113.43 toks/s]
Processed prompts:  67%|██████▋   | 690/1024 [00:06<00:03, 88.31it/s, est. speed input: 115404.71 toks/s, output: 112.70 toks/s]
Processed prompts:  69%|██████▉   | 706/1024 [00:06<00:03, 88.38it/s, est. speed input: 114695.13 toks/s, output: 112.01 toks/s]
Processed prompts:  71%|███████   | 722/1024 [00:06<00:03, 88.38it/s, est. speed input: 114019.18 toks/s, output: 111.35 toks/s]
Processed prompts:  72%|███████▏  | 738/1024 [00:06<00:03, 88.34it/s, est. speed input: 113376.12 toks/s, output: 110.72 toks/s]
Processed prompts:  74%|███████▎  | 754/1024 [00:06<00:03, 88.20it/s, est. speed input: 112754.45 toks/s, output: 110.11 toks/s]
Processed prompts:  75%|███████▌  | 770/1024 [00:07<00:02, 88.18it/s, est. speed input: 112172.31 toks/s, output: 109.54 toks/s]
Processed prompts:  77%|███████▋  | 786/1024 [00:07<00:02, 88.44it/s, est. speed input: 111649.45 toks/s, output: 109.03 toks/s]
Processed prompts:  78%|███████▊  | 802/1024 [00:07<00:02, 88.53it/s, est. speed input: 111142.91 toks/s, output: 108.54 toks/s]
Processed prompts:  80%|███████▉  | 818/1024 [00:07<00:02, 88.48it/s, est. speed input: 110648.84 toks/s, output: 108.05 toks/s]
Processed prompts:  81%|████████▏ | 834/1024 [00:07<00:02, 88.39it/s, est. speed input: 110171.75 toks/s, output: 107.59 toks/s]
Processed prompts:  83%|████████▎ | 850/1024 [00:07<00:01, 88.11it/s, est. speed input: 109697.33 toks/s, output: 107.13 toks/s]
Processed prompts:  85%|████████▍ | 866/1024 [00:08<00:01, 88.11it/s, est. speed input: 109261.26 toks/s, output: 106.70 toks/s]
Processed prompts:  86%|████████▌ | 882/1024 [00:08<00:01, 88.18it/s, est. speed input: 108850.76 toks/s, output: 106.30 toks/s]
Processed prompts:  88%|████████▊ | 898/1024 [00:08<00:01, 88.12it/s, est. speed input: 108448.72 toks/s, output: 105.91 toks/s]
Processed prompts:  89%|████████▉ | 914/1024 [00:08<00:01, 88.13it/s, est. speed input: 108067.26 toks/s, output: 105.53 toks/s]
Processed prompts:  91%|█████████ | 930/1024 [00:08<00:01, 88.07it/s, est. speed input: 107696.17 toks/s, output: 105.17 toks/s]
Processed prompts:  92%|█████████▏| 946/1024 [00:09<00:00, 89.45it/s, est. speed input: 107455.42 toks/s, output: 104.94 toks/s]
Processed prompts:  94%|█████████▍| 962/1024 [00:09<00:00, 89.10it/s, est. speed input: 107119.71 toks/s, output: 104.61 toks/s]
Processed prompts:  96%|█████████▌| 978/1024 [00:09<00:00, 88.75it/s, est. speed input: 106788.64 toks/s, output: 104.29 toks/s]
Processed prompts:  97%|█████████▋| 994/1024 [00:09<00:00, 90.12it/s, est. speed input: 106589.95 toks/s, output: 104.09 toks/s]
Processed prompts:  99%|█████████▊| 1010/1024 [00:09<00:00, 89.47it/s, est. speed input: 106281.89 toks/s, output: 103.79 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:09<00:00, 89.47it/s, est. speed input: 106869.06 toks/s, output: 104.36 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:09<00:00, 104.36it/s, est. speed input: 106869.06 toks/s, output: 104.36 toks/s]
[rank0]:[W128 09:26:40.600798699 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 54.0s

测试结果:
  Requests/s:   85.96
  Tokens/s:     88112.61
  Total Reqs:   1024
  Elapsed:      11.91s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     88026.65

============================================================
[6/7] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 09:26:57 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3385231) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3385231) WARNING 01-28 09:27:13 [backends.py:609] Failed to read file <frozen os>
Throughput: 91.08 requests/s, 93361.02 total tokens/s, 91.08 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-28 09:26:57] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 09:26:57] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:26:57] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:26:57] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:26:57] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:26:57] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:26:57] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:26:57] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:26:57] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:26:57] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:26:57] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:26:57] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:26:57] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:26:57] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 09:27:04] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 09:27:04] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:27:04] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:27:04] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:27:04] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:27:04] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:27:04] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:27:04] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:27:04] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:27:04] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:27:04] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:27:04] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:27:04] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:27:04] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3385231) [2026-01-28 09:27:05] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3385231) [2026-01-28 09:27:05] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3385231) [2026-01-28 09:27:05] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3385231) [2026-01-28 09:27:05] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3385231) [2026-01-28 09:27:05] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3385231) [2026-01-28 09:27:05] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3385231) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3385231) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.07it/s]
(EngineCore_DP0 pid=3385231) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.07it/s]
(EngineCore_DP0 pid=3385231) 
(EngineCore_DP0 pid=3385231) [2026-01-28 09:27:07] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=3385231) [2026-01-28 09:27:07] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9830400 bytes
(EngineCore_DP0 pid=3385231) [2026-01-28 09:27:07] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=3385231) [2026-01-28 09:27:07] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6553600 bytes
(EngineCore_DP0 pid=3385231) [2026-01-28 09:27:07] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=3385231) [2026-01-28 09:27:07] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 35389440 bytes
(EngineCore_DP0 pid=3385231) [2026-01-28 09:27:07] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=3385231) [2026-01-28 09:27:07] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 17715200 bytes
(EngineCore_DP0 pid=3385231) 2026-01-28 09:27:24,296 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3385231) 2026-01-28 09:27:24,324 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3385231) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 1/7 [00:00<00:00,  9.10it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 3/7 [00:00<00:00, 12.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 5/7 [00:00<00:00, 10.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 10.18it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 10.38it/s]
(EngineCore_DP0 pid=3385231) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00, 15.56it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00, 16.82it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 16.78it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 34/2048 [00:00<00:06, 333.14it/s]
Adding requests:   4%|▍         | 85/2048 [00:00<00:04, 435.44it/s]
Adding requests:   7%|▋         | 135/2048 [00:00<00:04, 461.64it/s]
Adding requests:   9%|▉         | 184/2048 [00:00<00:03, 470.16it/s]
Adding requests:  11%|█▏        | 235/2048 [00:00<00:03, 483.84it/s]
Adding requests:  14%|█▍        | 285/2048 [00:00<00:03, 488.56it/s]
Adding requests:  16%|█▋        | 334/2048 [00:00<00:03, 488.87it/s]
Adding requests:  19%|█▉        | 385/2048 [00:00<00:03, 492.88it/s]
Adding requests:  21%|██▏       | 436/2048 [00:00<00:03, 496.89it/s]
Adding requests:  24%|██▍       | 487/2048 [00:01<00:03, 498.14it/s]
Adding requests:  26%|██▌       | 537/2048 [00:01<00:03, 487.53it/s]
Adding requests:  29%|██▉       | 590/2048 [00:01<00:02, 498.08it/s]
Adding requests:  31%|███▏      | 642/2048 [00:01<00:02, 501.12it/s]
Adding requests:  34%|███▍      | 695/2048 [00:01<00:02, 507.16it/s]
Adding requests:  36%|███▋      | 746/2048 [00:01<00:02, 499.82it/s]
Adding requests:  39%|███▉      | 797/2048 [00:01<00:02, 500.63it/s]
Adding requests:  41%|████▏     | 848/2048 [00:01<00:02, 494.77it/s]
Adding requests:  44%|████▍     | 901/2048 [00:01<00:02, 504.71it/s]
Adding requests:  46%|████▋     | 952/2048 [00:01<00:02, 504.88it/s]
Adding requests:  49%|████▉     | 1004/2048 [00:02<00:02, 508.14it/s]
Adding requests:  52%|█████▏    | 1056/2048 [00:02<00:01, 509.24it/s]
Adding requests:  54%|█████▍    | 1107/2048 [00:02<00:01, 508.61it/s]
Adding requests:  57%|█████▋    | 1159/2048 [00:02<00:01, 510.30it/s]
Adding requests:  59%|█████▉    | 1213/2048 [00:02<00:01, 518.42it/s]
Adding requests:  62%|██████▏   | 1265/2048 [00:02<00:01, 511.75it/s]
Adding requests:  64%|██████▍   | 1317/2048 [00:02<00:01, 512.36it/s]
Adding requests:  67%|██████▋   | 1369/2048 [00:02<00:01, 514.31it/s]
Adding requests:  69%|██████▉   | 1421/2048 [00:02<00:01, 514.96it/s]
Adding requests:  72%|███████▏  | 1473/2048 [00:02<00:01, 511.45it/s]
Adding requests:  74%|███████▍  | 1525/2048 [00:03<00:01, 513.61it/s]
Adding requests:  77%|███████▋  | 1578/2048 [00:03<00:00, 515.34it/s]
Adding requests:  80%|███████▉  | 1632/2048 [00:03<00:00, 520.87it/s]
Adding requests:  82%|████████▏ | 1685/2048 [00:03<00:00, 515.86it/s]
Adding requests:  85%|████████▍ | 1738/2048 [00:03<00:00, 517.43it/s]
Adding requests:  87%|████████▋ | 1790/2048 [00:03<00:00, 513.55it/s]
Adding requests:  90%|████████▉ | 1842/2048 [00:03<00:00, 515.16it/s]
Adding requests:  92%|█████████▏| 1894/2048 [00:03<00:00, 505.41it/s]
Adding requests:  95%|█████████▍| 1945/2048 [00:03<00:00, 506.70it/s]
Adding requests:  97%|█████████▋| 1996/2048 [00:03<00:00, 507.01it/s]
Adding requests: 100%|██████████| 2048/2048 [00:04<00:00, 502.89it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  18%|█▊        | 370/2048 [00:00<00:01, 1621.16it/s, est. speed input: 1660274.00 toks/s, output: 1621.24 toks/s]
Processed prompts:  26%|██▌       | 533/2048 [00:01<00:06, 222.29it/s, est. speed input: 277497.10 toks/s, output: 270.99 toks/s]   
Processed prompts:  30%|██▉       | 606/2048 [00:02<00:07, 181.52it/s, est. speed input: 232633.94 toks/s, output: 227.18 toks/s]
Processed prompts:  32%|███▏      | 651/2048 [00:03<00:08, 155.73it/s, est. speed input: 208862.30 toks/s, output: 203.97 toks/s]
Processed prompts:  33%|███▎      | 682/2048 [00:03<00:09, 141.98it/s, est. speed input: 197169.85 toks/s, output: 192.55 toks/s]
Processed prompts:  34%|███▍      | 705/2048 [00:03<00:09, 140.58it/s, est. speed input: 194195.52 toks/s, output: 189.64 toks/s]
Processed prompts:  35%|███▌      | 725/2048 [00:04<00:11, 118.45it/s, est. speed input: 182589.37 toks/s, output: 178.31 toks/s]
Processed prompts:  36%|███▌      | 740/2048 [00:04<00:11, 113.06it/s, est. speed input: 178616.85 toks/s, output: 174.43 toks/s]
Processed prompts:  37%|███▋      | 754/2048 [00:04<00:12, 106.96it/s, est. speed input: 174804.51 toks/s, output: 170.71 toks/s]
Processed prompts:  38%|███▊      | 770/2048 [00:04<00:12, 103.92it/s, est. speed input: 171779.89 toks/s, output: 167.75 toks/s]
Processed prompts:  38%|███▊      | 786/2048 [00:04<00:12, 100.97it/s, est. speed input: 168891.76 toks/s, output: 164.93 toks/s]
Processed prompts:  39%|███▉      | 802/2048 [00:04<00:12, 98.61it/s, est. speed input: 166227.30 toks/s, output: 162.33 toks/s] 
Processed prompts:  40%|███▉      | 818/2048 [00:05<00:12, 96.56it/s, est. speed input: 163706.57 toks/s, output: 159.87 toks/s]
Processed prompts:  41%|████      | 834/2048 [00:05<00:12, 95.08it/s, est. speed input: 161371.67 toks/s, output: 157.59 toks/s]
Processed prompts:  42%|████▏     | 850/2048 [00:05<00:12, 94.03it/s, est. speed input: 159195.91 toks/s, output: 155.46 toks/s]
Processed prompts:  42%|████▏     | 866/2048 [00:05<00:12, 93.28it/s, est. speed input: 157161.13 toks/s, output: 153.48 toks/s]
Processed prompts:  43%|████▎     | 882/2048 [00:05<00:12, 92.77it/s, est. speed input: 155254.06 toks/s, output: 151.61 toks/s]
Processed prompts:  44%|████▍     | 898/2048 [00:05<00:12, 92.48it/s, est. speed input: 153470.78 toks/s, output: 149.87 toks/s]
Processed prompts:  45%|████▍     | 914/2048 [00:06<00:12, 91.81it/s, est. speed input: 151715.54 toks/s, output: 148.16 toks/s]
Processed prompts:  45%|████▌     | 930/2048 [00:06<00:12, 92.94it/s, est. speed input: 150301.06 toks/s, output: 146.78 toks/s]
Processed prompts:  46%|████▌     | 946/2048 [00:06<00:11, 92.50it/s, est. speed input: 148778.61 toks/s, output: 145.29 toks/s]
Processed prompts:  47%|████▋     | 962/2048 [00:06<00:11, 92.15it/s, est. speed input: 147330.83 toks/s, output: 143.88 toks/s]
Processed prompts:  48%|████▊     | 978/2048 [00:06<00:11, 93.00it/s, est. speed input: 146104.58 toks/s, output: 142.68 toks/s]
Processed prompts:  49%|████▊     | 994/2048 [00:07<00:11, 92.61it/s, est. speed input: 144808.99 toks/s, output: 141.41 toks/s]
Processed prompts:  49%|████▉     | 1010/2048 [00:07<00:11, 91.94it/s, est. speed input: 143526.01 toks/s, output: 140.16 toks/s]
Processed prompts:  50%|█████     | 1026/2048 [00:07<00:11, 91.60it/s, est. speed input: 142319.37 toks/s, output: 138.98 toks/s]
Processed prompts:  51%|█████     | 1042/2048 [00:07<00:10, 91.61it/s, est. speed input: 141199.48 toks/s, output: 137.89 toks/s]
Processed prompts:  52%|█████▏    | 1058/2048 [00:07<00:10, 91.50it/s, est. speed input: 140116.30 toks/s, output: 136.83 toks/s]
Processed prompts:  52%|█████▏    | 1074/2048 [00:07<00:10, 91.34it/s, est. speed input: 139071.10 toks/s, output: 135.81 toks/s]
Processed prompts:  53%|█████▎    | 1090/2048 [00:08<00:10, 91.36it/s, est. speed input: 138086.38 toks/s, output: 134.85 toks/s]
Processed prompts:  54%|█████▍    | 1106/2048 [00:08<00:10, 91.16it/s, est. speed input: 137120.56 toks/s, output: 133.91 toks/s]
Processed prompts:  55%|█████▍    | 1122/2048 [00:08<00:10, 91.39it/s, est. speed input: 136233.44 toks/s, output: 133.04 toks/s]
Processed prompts:  56%|█████▌    | 1138/2048 [00:08<00:09, 91.30it/s, est. speed input: 135357.38 toks/s, output: 132.18 toks/s]
Processed prompts:  56%|█████▋    | 1154/2048 [00:08<00:09, 92.39it/s, est. speed input: 134628.10 toks/s, output: 131.47 toks/s]
Processed prompts:  57%|█████▋    | 1170/2048 [00:08<00:09, 92.14it/s, est. speed input: 133830.50 toks/s, output: 130.69 toks/s]
Processed prompts:  58%|█████▊    | 1186/2048 [00:09<00:09, 92.04it/s, est. speed input: 133069.63 toks/s, output: 129.95 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [00:09<00:09, 91.68it/s, est. speed input: 132311.58 toks/s, output: 129.21 toks/s]
Processed prompts:  59%|█████▉    | 1218/2048 [00:09<00:09, 91.60it/s, est. speed input: 131596.57 toks/s, output: 128.51 toks/s]
Processed prompts:  60%|██████    | 1234/2048 [00:09<00:08, 91.54it/s, est. speed input: 130907.61 toks/s, output: 127.84 toks/s]
Processed prompts:  61%|██████    | 1250/2048 [00:09<00:08, 91.67it/s, est. speed input: 130257.25 toks/s, output: 127.20 toks/s]
Processed prompts:  62%|██████▏   | 1266/2048 [00:09<00:08, 92.78it/s, est. speed input: 129712.24 toks/s, output: 126.67 toks/s]
Processed prompts:  63%|██████▎   | 1282/2048 [00:10<00:08, 92.37it/s, est. speed input: 129092.07 toks/s, output: 126.07 toks/s]
Processed prompts:  63%|██████▎   | 1298/2048 [00:10<00:08, 91.98it/s, est. speed input: 128483.52 toks/s, output: 125.47 toks/s]
Processed prompts:  64%|██████▍   | 1314/2048 [00:10<00:08, 91.72it/s, est. speed input: 127896.08 toks/s, output: 124.90 toks/s]
Processed prompts:  65%|██████▍   | 1330/2048 [00:10<00:07, 91.37it/s, est. speed input: 127315.47 toks/s, output: 124.33 toks/s]
Processed prompts:  66%|██████▌   | 1346/2048 [00:10<00:07, 90.92it/s, est. speed input: 126738.37 toks/s, output: 123.77 toks/s]
Processed prompts:  67%|██████▋   | 1362/2048 [00:11<00:07, 91.03it/s, est. speed input: 126210.74 toks/s, output: 123.25 toks/s]
Processed prompts:  67%|██████▋   | 1378/2048 [00:11<00:07, 91.06it/s, est. speed input: 125695.90 toks/s, output: 122.75 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [00:11<00:07, 90.97it/s, est. speed input: 125189.90 toks/s, output: 122.26 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [00:11<00:07, 91.14it/s, est. speed input: 124714.72 toks/s, output: 121.79 toks/s]
Processed prompts:  70%|██████▉   | 1426/2048 [00:11<00:06, 91.24it/s, est. speed input: 124252.90 toks/s, output: 121.34 toks/s]
Processed prompts:  70%|███████   | 1442/2048 [00:11<00:06, 91.06it/s, est. speed input: 123787.47 toks/s, output: 120.89 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [00:12<00:06, 90.85it/s, est. speed input: 123330.56 toks/s, output: 120.44 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [00:12<00:06, 90.86it/s, est. speed input: 122896.34 toks/s, output: 120.02 toks/s]
Processed prompts:  73%|███████▎  | 1490/2048 [00:12<00:06, 90.97it/s, est. speed input: 122481.61 toks/s, output: 119.61 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [00:12<00:05, 91.07it/s, est. speed input: 122079.14 toks/s, output: 119.22 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [00:12<00:05, 91.02it/s, est. speed input: 121680.96 toks/s, output: 118.83 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [00:12<00:05, 90.86it/s, est. speed input: 121285.91 toks/s, output: 118.44 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [00:13<00:05, 91.01it/s, est. speed input: 120916.89 toks/s, output: 118.08 toks/s]
Processed prompts:  77%|███████▋  | 1570/2048 [00:13<00:05, 90.92it/s, est. speed input: 120545.79 toks/s, output: 117.72 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [00:13<00:05, 92.30it/s, est. speed input: 120266.71 toks/s, output: 117.45 toks/s]
Processed prompts:  78%|███████▊  | 1602/2048 [00:13<00:04, 91.72it/s, est. speed input: 119908.21 toks/s, output: 117.10 toks/s]
Processed prompts:  79%|███████▉  | 1618/2048 [00:13<00:04, 91.42it/s, est. speed input: 119564.65 toks/s, output: 116.76 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [00:14<00:04, 91.45it/s, est. speed input: 119242.28 toks/s, output: 116.45 toks/s]
Processed prompts:  81%|████████  | 1650/2048 [00:14<00:04, 91.24it/s, est. speed input: 118916.18 toks/s, output: 116.13 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [00:14<00:04, 90.95it/s, est. speed input: 118589.91 toks/s, output: 115.81 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [00:14<00:04, 90.74it/s, est. speed input: 118271.35 toks/s, output: 115.50 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [00:14<00:03, 90.68it/s, est. speed input: 117964.71 toks/s, output: 115.20 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [00:14<00:03, 90.87it/s, est. speed input: 117677.71 toks/s, output: 114.92 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [00:15<00:03, 90.91it/s, est. speed input: 117392.34 toks/s, output: 114.64 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [00:15<00:03, 90.95it/s, est. speed input: 117114.38 toks/s, output: 114.37 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [00:15<00:03, 90.78it/s, est. speed input: 116832.69 toks/s, output: 114.09 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [00:15<00:02, 90.70it/s, est. speed input: 116559.70 toks/s, output: 113.83 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [00:15<00:02, 90.58it/s, est. speed input: 116289.12 toks/s, output: 113.56 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [00:15<00:02, 90.65it/s, est. speed input: 116032.40 toks/s, output: 113.31 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [00:16<00:02, 90.79it/s, est. speed input: 115785.21 toks/s, output: 113.07 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [00:16<00:02, 90.94it/s, est. speed input: 115545.65 toks/s, output: 112.84 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [00:16<00:02, 90.77it/s, est. speed input: 115298.96 toks/s, output: 112.60 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [00:16<00:01, 91.98it/s, est. speed input: 115116.07 toks/s, output: 112.42 toks/s]
Processed prompts:  92%|█████████▏| 1890/2048 [00:16<00:01, 91.79it/s, est. speed input: 114891.69 toks/s, output: 112.20 toks/s]
Processed prompts:  93%|█████████▎| 1906/2048 [00:17<00:01, 91.37it/s, est. speed input: 114659.78 toks/s, output: 111.97 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [00:17<00:01, 91.42it/s, est. speed input: 114447.24 toks/s, output: 111.76 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [00:17<00:01, 91.34it/s, est. speed input: 114233.66 toks/s, output: 111.56 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [00:17<00:01, 92.37it/s, est. speed input: 114069.29 toks/s, output: 111.40 toks/s]
Processed prompts:  96%|█████████▌| 1970/2048 [00:17<00:00, 91.95it/s, est. speed input: 113861.76 toks/s, output: 111.19 toks/s]
Processed prompts:  97%|█████████▋| 1986/2048 [00:17<00:00, 91.63it/s, est. speed input: 113657.32 toks/s, output: 110.99 toks/s]
Processed prompts:  98%|█████████▊| 2002/2048 [00:18<00:00, 91.70it/s, est. speed input: 113468.53 toks/s, output: 110.81 toks/s]
Processed prompts:  99%|█████████▊| 2018/2048 [00:18<00:00, 91.61it/s, est. speed input: 113277.75 toks/s, output: 110.62 toks/s]
Processed prompts:  99%|█████████▉| 2034/2048 [00:18<00:00, 92.81it/s, est. speed input: 113139.24 toks/s, output: 110.49 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:18<00:00, 92.81it/s, est. speed input: 113914.53 toks/s, output: 111.24 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:18<00:00, 111.24it/s, est. speed input: 113914.53 toks/s, output: 111.24 toks/s]
[rank0]:[W128 09:27:49.685835313 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 69.1s

测试结果:
  Requests/s:   91.08
  Tokens/s:     93361.02
  Total Reqs:   2048
  Elapsed:      22.48s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     93269.94

============================================================
[7/7] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 09:28:15 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3386747) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3386747) WARNING 01-28 09:28:30 [backends.py:609] Failed to read file <frozen os>
Throughput: 92.89 requests/s, 95212.79 total tokens/s, 92.89 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-28 09:28:15] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 09:28:15] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:28:15] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:28:15] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:28:15] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:28:15] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:28:15] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:28:15] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:28:15] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:28:15] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:28:15] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:28:15] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:28:15] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:28:15] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 09:28:21] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 09:28:21] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 09:28:21] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 09:28:21] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:28:21] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:28:21] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:28:21] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:28:21] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 09:28:21] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 09:28:21] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 09:28:21] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 09:28:21] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 09:28:21] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 09:28:21] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3386747) [2026-01-28 09:28:23] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3386747) [2026-01-28 09:28:23] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3386747) [2026-01-28 09:28:23] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3386747) [2026-01-28 09:28:23] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3386747) [2026-01-28 09:28:23] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3386747) [2026-01-28 09:28:23] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3386747) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3386747) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.08it/s]
(EngineCore_DP0 pid=3386747) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.08it/s]
(EngineCore_DP0 pid=3386747) 
(EngineCore_DP0 pid=3386747) [2026-01-28 09:28:24] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=3386747) [2026-01-28 09:28:24] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9830400 bytes
(EngineCore_DP0 pid=3386747) [2026-01-28 09:28:24] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=3386747) [2026-01-28 09:28:24] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6553600 bytes
(EngineCore_DP0 pid=3386747) [2026-01-28 09:28:24] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=3386747) [2026-01-28 09:28:24] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 35389440 bytes
(EngineCore_DP0 pid=3386747) [2026-01-28 09:28:24] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=3386747) [2026-01-28 09:28:24] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 17715200 bytes
(EngineCore_DP0 pid=3386747) [rank0]:W0128 09:28:36.437000 3386747 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3386747) [rank0]:W0128 09:28:36.519000 3386747 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3386747) [rank0]:W0128 09:28:37.770000 3386747 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3386747) [rank0]:W0128 09:28:37.927000 3386747 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3386747) 2026-01-28 09:28:41,898 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3386747) 2026-01-28 09:28:41,940 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3386747) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▉         | 1/11 [00:00<00:01,  9.91it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 3/11 [00:00<00:00, 13.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 5/11 [00:00<00:00,  8.67it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▎   | 7/11 [00:00<00:00,  9.04it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 9/11 [00:00<00:00, 10.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00, 11.94it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00, 10.93it/s]
(EngineCore_DP0 pid=3386747) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00, 15.95it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00, 16.69it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00, 16.95it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 16.87it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 33/4096 [00:00<00:12, 324.02it/s]
Adding requests:   2%|▏         | 84/4096 [00:00<00:09, 427.80it/s]
Adding requests:   3%|▎         | 134/4096 [00:00<00:08, 457.37it/s]
Adding requests:   4%|▍         | 182/4096 [00:00<00:08, 464.73it/s]
Adding requests:   6%|▌         | 233/4096 [00:00<00:08, 477.16it/s]
Adding requests:   7%|▋         | 283/4096 [00:00<00:07, 484.20it/s]
Adding requests:   8%|▊         | 332/4096 [00:00<00:07, 482.00it/s]
Adding requests:   9%|▉         | 381/4096 [00:00<00:07, 484.35it/s]
Adding requests:  11%|█         | 431/4096 [00:00<00:07, 488.12it/s]
Adding requests:  12%|█▏        | 481/4096 [00:01<00:07, 490.31it/s]
Adding requests:  13%|█▎        | 531/4096 [00:01<00:07, 480.10it/s]
Adding requests:  14%|█▍        | 584/4096 [00:01<00:07, 492.71it/s]
Adding requests:  16%|█▌        | 635/4096 [00:01<00:06, 496.86it/s]
Adding requests:  17%|█▋        | 687/4096 [00:01<00:06, 502.91it/s]
Adding requests:  18%|█▊        | 739/4096 [00:01<00:06, 506.40it/s]
Adding requests:  19%|█▉        | 790/4096 [00:01<00:06, 499.44it/s]
Adding requests:  21%|██        | 840/4096 [00:01<00:06, 491.08it/s]
Adding requests:  22%|██▏       | 892/4096 [00:01<00:06, 499.01it/s]
Adding requests:  23%|██▎       | 942/4096 [00:01<00:06, 498.29it/s]
Adding requests:  24%|██▍       | 993/4096 [00:02<00:06, 499.30it/s]
Adding requests:  25%|██▌       | 1044/4096 [00:02<00:06, 500.58it/s]
Adding requests:  27%|██▋       | 1095/4096 [00:02<00:06, 488.95it/s]
Adding requests:  28%|██▊       | 1144/4096 [00:02<00:06, 489.05it/s]
Adding requests:  29%|██▉       | 1197/4096 [00:02<00:05, 500.76it/s]
Adding requests:  30%|███       | 1248/4096 [00:02<00:05, 501.76it/s]
Adding requests:  32%|███▏      | 1299/4096 [00:02<00:05, 497.89it/s]
Adding requests:  33%|███▎      | 1351/4096 [00:02<00:05, 501.61it/s]
Adding requests:  34%|███▍      | 1403/4096 [00:02<00:05, 505.49it/s]
Adding requests:  35%|███▌      | 1454/4096 [00:02<00:05, 505.23it/s]
Adding requests:  37%|███▋      | 1506/4096 [00:03<00:05, 508.09it/s]
Adding requests:  38%|███▊      | 1558/4096 [00:03<00:04, 509.40it/s]
Adding requests:  39%|███▉      | 1611/4096 [00:03<00:04, 513.01it/s]
Adding requests:  41%|████      | 1663/4096 [00:03<00:04, 510.26it/s]
Adding requests:  42%|████▏     | 1715/4096 [00:03<00:04, 511.44it/s]
Adding requests:  43%|████▎     | 1767/4096 [00:03<00:04, 509.06it/s]
Adding requests:  44%|████▍     | 1820/4096 [00:03<00:04, 513.77it/s]
Adding requests:  46%|████▌     | 1872/4096 [00:03<00:04, 510.77it/s]
Adding requests:  47%|████▋     | 1924/4096 [00:03<00:04, 512.74it/s]
Adding requests:  48%|████▊     | 1976/4096 [00:03<00:04, 508.74it/s]
Adding requests:  50%|████▉     | 2029/4096 [00:04<00:04, 512.80it/s]
Adding requests:  51%|█████     | 2081/4096 [00:04<00:03, 514.19it/s]
Adding requests:  52%|█████▏    | 2133/4096 [00:04<00:03, 510.76it/s]
Adding requests:  53%|█████▎    | 2185/4096 [00:04<00:03, 502.71it/s]
Adding requests:  55%|█████▍    | 2238/4096 [00:04<00:03, 508.74it/s]
Adding requests:  56%|█████▌    | 2289/4096 [00:04<00:03, 492.86it/s]
Adding requests:  57%|█████▋    | 2340/4096 [00:04<00:03, 496.90it/s]
Adding requests:  58%|█████▊    | 2392/4096 [00:04<00:03, 501.43it/s]
Adding requests:  60%|█████▉    | 2444/4096 [00:04<00:03, 504.44it/s]
Adding requests:  61%|██████    | 2495/4096 [00:05<00:03, 504.53it/s]
Adding requests:  62%|██████▏   | 2547/4096 [00:05<00:03, 507.12it/s]
Adding requests:  63%|██████▎   | 2599/4096 [00:05<00:02, 509.20it/s]
Adding requests:  65%|██████▍   | 2651/4096 [00:05<00:02, 510.43it/s]
Adding requests:  66%|██████▌   | 2703/4096 [00:05<00:02, 508.42it/s]
Adding requests:  67%|██████▋   | 2754/4096 [00:05<00:02, 507.99it/s]
Adding requests:  69%|██████▊   | 2806/4096 [00:05<00:02, 508.74it/s]
Adding requests:  70%|██████▉   | 2858/4096 [00:05<00:02, 510.31it/s]
Adding requests:  71%|███████   | 2911/4096 [00:05<00:02, 514.82it/s]
Adding requests:  72%|███████▏  | 2963/4096 [00:05<00:02, 506.56it/s]
Adding requests:  74%|███████▎  | 3015/4096 [00:06<00:02, 509.45it/s]
Adding requests:  75%|███████▍  | 3066/4096 [00:06<00:02, 507.08it/s]
Adding requests:  76%|███████▌  | 3119/4096 [00:06<00:01, 510.40it/s]
Adding requests:  77%|███████▋  | 3171/4096 [00:06<00:01, 507.05it/s]
Adding requests:  79%|███████▊  | 3223/4096 [00:06<00:01, 509.55it/s]
Adding requests:  80%|███████▉  | 3275/4096 [00:06<00:01, 511.34it/s]
Adding requests:  81%|████████  | 3327/4096 [00:06<00:01, 511.47it/s]
Adding requests:  83%|████████▎ | 3380/4096 [00:06<00:01, 515.21it/s]
Adding requests:  84%|████████▍ | 3432/4096 [00:06<00:01, 514.88it/s]
Adding requests:  85%|████████▌ | 3484/4096 [00:06<00:01, 507.37it/s]
Adding requests:  86%|████████▋ | 3536/4096 [00:07<00:01, 508.21it/s]
Adding requests:  88%|████████▊ | 3588/4096 [00:07<00:00, 509.81it/s]
Adding requests:  89%|████████▉ | 3639/4096 [00:07<00:00, 495.23it/s]
Adding requests:  90%|█████████ | 3691/4096 [00:07<00:00, 501.57it/s]
Adding requests:  91%|█████████▏| 3742/4096 [00:07<00:00, 501.41it/s]
Adding requests:  93%|█████████▎| 3797/4096 [00:07<00:00, 514.68it/s]
Adding requests:  94%|█████████▍| 3850/4096 [00:07<00:00, 518.14it/s]
Adding requests:  95%|█████████▌| 3903/4096 [00:07<00:00, 519.51it/s]
Adding requests:  97%|█████████▋| 3955/4096 [00:07<00:00, 518.23it/s]
Adding requests:  98%|█████████▊| 4007/4096 [00:07<00:00, 516.74it/s]
Adding requests:  99%|█████████▉| 4059/4096 [00:08<00:00, 513.37it/s]
Adding requests: 100%|██████████| 4096/4096 [00:08<00:00, 502.97it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  18%|█▊        | 757/4096 [00:00<00:01, 1777.53it/s, est. speed input: 1820290.62 toks/s, output: 1777.56 toks/s]
Processed prompts:  23%|██▎       | 935/4096 [00:02<00:08, 352.10it/s, est. speed input: 447771.01 toks/s, output: 437.27 toks/s]   
Processed prompts:  25%|██▍       | 1015/4096 [00:03<00:13, 236.69it/s, est. speed input: 328654.17 toks/s, output: 320.95 toks/s]
Processed prompts:  26%|██▌       | 1062/4096 [00:03<00:13, 219.84it/s, est. speed input: 310118.93 toks/s, output: 302.85 toks/s]
Processed prompts:  27%|██▋       | 1096/4096 [00:03<00:15, 196.17it/s, est. speed input: 291291.78 toks/s, output: 284.46 toks/s]
Processed prompts:  27%|██▋       | 1122/4096 [00:04<00:17, 170.00it/s, est. speed input: 273719.03 toks/s, output: 267.30 toks/s]
Processed prompts:  28%|██▊       | 1142/4096 [00:04<00:20, 144.69it/s, est. speed input: 258125.73 toks/s, output: 252.08 toks/s]
Processed prompts:  29%|██▊       | 1173/4096 [00:04<00:22, 130.61it/s, est. speed input: 246302.20 toks/s, output: 240.53 toks/s]
Processed prompts:  29%|██▉       | 1205/4096 [00:05<00:23, 120.66it/s, est. speed input: 236366.37 toks/s, output: 230.82 toks/s]
Processed prompts:  30%|███       | 1237/4096 [00:05<00:25, 113.08it/s, est. speed input: 227654.81 toks/s, output: 222.32 toks/s]
Processed prompts:  31%|███       | 1269/4096 [00:05<00:26, 108.04it/s, est. speed input: 220207.60 toks/s, output: 215.05 toks/s]
Processed prompts:  32%|███▏      | 1301/4096 [00:06<00:26, 103.63it/s, est. speed input: 213292.46 toks/s, output: 208.29 toks/s]
Processed prompts:  33%|███▎      | 1333/4096 [00:06<00:27, 100.22it/s, est. speed input: 207013.23 toks/s, output: 202.16 toks/s]
Processed prompts:  33%|███▎      | 1365/4096 [00:06<00:27, 97.90it/s, est. speed input: 201400.11 toks/s, output: 196.68 toks/s] 
Processed prompts:  34%|███▍      | 1397/4096 [00:07<00:27, 96.43it/s, est. speed input: 196377.21 toks/s, output: 191.77 toks/s]
Processed prompts:  35%|███▍      | 1429/4096 [00:07<00:27, 95.29it/s, est. speed input: 191778.14 toks/s, output: 187.28 toks/s]
Processed prompts:  36%|███▌      | 1461/4096 [00:07<00:27, 94.42it/s, est. speed input: 187555.96 toks/s, output: 183.16 toks/s]
Processed prompts:  36%|███▋      | 1493/4096 [00:08<00:27, 93.94it/s, est. speed input: 183722.84 toks/s, output: 179.42 toks/s]
Processed prompts:  37%|███▋      | 1525/4096 [00:08<00:27, 93.56it/s, est. speed input: 180183.08 toks/s, output: 175.96 toks/s]
Processed prompts:  38%|███▊      | 1557/4096 [00:09<00:27, 93.14it/s, est. speed input: 176878.31 toks/s, output: 172.73 toks/s]
Processed prompts:  39%|███▉      | 1589/4096 [00:09<00:26, 93.99it/s, est. speed input: 174080.67 toks/s, output: 170.00 toks/s]
Processed prompts:  40%|███▉      | 1621/4096 [00:09<00:26, 93.48it/s, est. speed input: 171234.21 toks/s, output: 167.22 toks/s]
Processed prompts:  40%|████      | 1653/4096 [00:10<00:26, 93.09it/s, est. speed input: 168577.39 toks/s, output: 164.63 toks/s]
Processed prompts:  41%|████      | 1685/4096 [00:10<00:25, 92.96it/s, est. speed input: 166126.48 toks/s, output: 162.23 toks/s]
Processed prompts:  42%|████▏     | 1717/4096 [00:10<00:25, 92.83it/s, est. speed input: 163827.01 toks/s, output: 159.99 toks/s]
Processed prompts:  43%|████▎     | 1749/4096 [00:11<00:25, 92.61it/s, est. speed input: 161646.76 toks/s, output: 157.86 toks/s]
Processed prompts:  43%|████▎     | 1781/4096 [00:11<00:24, 92.65it/s, est. speed input: 159632.20 toks/s, output: 155.89 toks/s]
Processed prompts:  44%|████▍     | 1813/4096 [00:11<00:24, 92.50it/s, est. speed input: 157707.61 toks/s, output: 154.01 toks/s]
Processed prompts:  45%|████▌     | 1845/4096 [00:12<00:24, 92.37it/s, est. speed input: 155888.63 toks/s, output: 152.23 toks/s]
Processed prompts:  46%|████▌     | 1877/4096 [00:12<00:23, 93.36it/s, est. speed input: 154336.01 toks/s, output: 150.72 toks/s]
Processed prompts:  47%|████▋     | 1909/4096 [00:12<00:23, 93.18it/s, est. speed input: 152736.65 toks/s, output: 149.16 toks/s]
Processed prompts:  47%|████▋     | 1941/4096 [00:13<00:22, 93.95it/s, est. speed input: 151347.42 toks/s, output: 147.80 toks/s]
Processed prompts:  48%|████▊     | 1973/4096 [00:13<00:22, 93.71it/s, est. speed input: 149920.34 toks/s, output: 146.41 toks/s]
Processed prompts:  49%|████▉     | 2005/4096 [00:13<00:22, 93.41it/s, est. speed input: 148547.36 toks/s, output: 145.07 toks/s]
Processed prompts:  50%|████▉     | 2037/4096 [00:14<00:22, 93.25it/s, est. speed input: 147248.26 toks/s, output: 143.80 toks/s]
Processed prompts:  51%|█████     | 2069/4096 [00:14<00:21, 93.07it/s, est. speed input: 146001.44 toks/s, output: 142.58 toks/s]
Processed prompts:  51%|█████▏    | 2101/4096 [00:14<00:21, 92.97it/s, est. speed input: 144816.42 toks/s, output: 141.42 toks/s]
Processed prompts:  52%|█████▏    | 2133/4096 [00:15<00:21, 93.00it/s, est. speed input: 143695.91 toks/s, output: 140.33 toks/s]
Processed prompts:  53%|█████▎    | 2165/4096 [00:15<00:20, 93.05it/s, est. speed input: 142628.55 toks/s, output: 139.29 toks/s]
Processed prompts:  54%|█████▎    | 2197/4096 [00:15<00:20, 92.90it/s, est. speed input: 141587.18 toks/s, output: 138.27 toks/s]
Processed prompts:  54%|█████▍    | 2229/4096 [00:16<00:19, 94.42it/s, est. speed input: 140761.59 toks/s, output: 137.46 toks/s]
Processed prompts:  55%|█████▌    | 2261/4096 [00:16<00:19, 94.01it/s, est. speed input: 139818.20 toks/s, output: 136.54 toks/s]
Processed prompts:  56%|█████▌    | 2293/4096 [00:16<00:19, 94.57it/s, est. speed input: 138995.64 toks/s, output: 135.74 toks/s]
Processed prompts:  57%|█████▋    | 2325/4096 [00:17<00:18, 94.83it/s, est. speed input: 138193.19 toks/s, output: 134.95 toks/s]
Processed prompts:  58%|█████▊    | 2357/4096 [00:17<00:18, 95.03it/s, est. speed input: 137421.80 toks/s, output: 134.20 toks/s]
Processed prompts:  58%|█████▊    | 2389/4096 [00:17<00:17, 95.16it/s, est. speed input: 136679.60 toks/s, output: 133.48 toks/s]
Processed prompts:  59%|█████▉    | 2421/4096 [00:18<00:17, 94.47it/s, est. speed input: 135894.40 toks/s, output: 132.71 toks/s]
Processed prompts:  60%|█████▉    | 2453/4096 [00:18<00:17, 93.92it/s, est. speed input: 135133.01 toks/s, output: 131.97 toks/s]
Processed prompts:  61%|██████    | 2485/4096 [00:18<00:17, 94.27it/s, est. speed input: 134461.00 toks/s, output: 131.31 toks/s]
Processed prompts:  61%|██████▏   | 2517/4096 [00:19<00:16, 93.63it/s, est. speed input: 133739.46 toks/s, output: 130.60 toks/s]
Processed prompts:  62%|██████▏   | 2549/4096 [00:19<00:16, 93.38it/s, est. speed input: 133058.56 toks/s, output: 129.94 toks/s]
Processed prompts:  63%|██████▎   | 2581/4096 [00:19<00:16, 94.09it/s, est. speed input: 132472.83 toks/s, output: 129.37 toks/s]
Processed prompts:  64%|██████▍   | 2613/4096 [00:20<00:15, 93.53it/s, est. speed input: 131823.89 toks/s, output: 128.73 toks/s]
Processed prompts:  65%|██████▍   | 2645/4096 [00:20<00:15, 93.24it/s, est. speed input: 131203.85 toks/s, output: 128.13 toks/s]
Processed prompts:  65%|██████▌   | 2677/4096 [00:20<00:15, 93.02it/s, est. speed input: 130602.47 toks/s, output: 127.54 toks/s]
Processed prompts:  66%|██████▌   | 2709/4096 [00:21<00:14, 92.80it/s, est. speed input: 130015.77 toks/s, output: 126.97 toks/s]
Processed prompts:  67%|██████▋   | 2741/4096 [00:21<00:14, 92.85it/s, est. speed input: 129463.36 toks/s, output: 126.43 toks/s]
Processed prompts:  68%|██████▊   | 2773/4096 [00:22<00:14, 92.85it/s, est. speed input: 128924.93 toks/s, output: 125.90 toks/s]
Processed prompts:  68%|██████▊   | 2805/4096 [00:22<00:13, 92.53it/s, est. speed input: 128380.59 toks/s, output: 125.37 toks/s]
Processed prompts:  69%|██████▉   | 2837/4096 [00:22<00:13, 92.69it/s, est. speed input: 127879.85 toks/s, output: 124.88 toks/s]
Processed prompts:  70%|███████   | 2869/4096 [00:23<00:13, 92.77it/s, est. speed input: 127391.66 toks/s, output: 124.41 toks/s]
Processed prompts:  71%|███████   | 2901/4096 [00:23<00:12, 92.55it/s, est. speed input: 126899.64 toks/s, output: 123.93 toks/s]
Processed prompts:  72%|███████▏  | 2933/4096 [00:23<00:12, 92.59it/s, est. speed input: 126434.88 toks/s, output: 123.47 toks/s]
Processed prompts:  72%|███████▏  | 2965/4096 [00:24<00:12, 92.71it/s, est. speed input: 125989.22 toks/s, output: 123.04 toks/s]
Processed prompts:  73%|███████▎  | 2997/4096 [00:24<00:11, 92.56it/s, est. speed input: 125541.14 toks/s, output: 122.60 toks/s]
Processed prompts:  74%|███████▍  | 3029/4096 [00:24<00:11, 92.51it/s, est. speed input: 125108.94 toks/s, output: 122.18 toks/s]
Processed prompts:  75%|███████▍  | 3061/4096 [00:25<00:11, 92.52it/s, est. speed input: 124691.67 toks/s, output: 121.77 toks/s]
Processed prompts:  76%|███████▌  | 3093/4096 [00:25<00:10, 92.60it/s, est. speed input: 124289.97 toks/s, output: 121.38 toks/s]
Processed prompts:  76%|███████▋  | 3125/4096 [00:25<00:10, 93.38it/s, est. speed input: 123941.60 toks/s, output: 121.04 toks/s]
Processed prompts:  77%|███████▋  | 3157/4096 [00:26<00:10, 93.08it/s, est. speed input: 123553.55 toks/s, output: 120.66 toks/s]
Processed prompts:  78%|███████▊  | 3189/4096 [00:26<00:09, 92.96it/s, est. speed input: 123180.68 toks/s, output: 120.29 toks/s]
Processed prompts:  79%|███████▊  | 3221/4096 [00:26<00:09, 92.75it/s, est. speed input: 122809.51 toks/s, output: 119.93 toks/s]
Processed prompts:  79%|███████▉  | 3253/4096 [00:27<00:09, 92.65it/s, est. speed input: 122450.79 toks/s, output: 119.58 toks/s]
Processed prompts:  80%|████████  | 3285/4096 [00:27<00:08, 92.61it/s, est. speed input: 122103.25 toks/s, output: 119.24 toks/s]
Processed prompts:  81%|████████  | 3317/4096 [00:27<00:08, 92.51it/s, est. speed input: 121759.89 toks/s, output: 118.91 toks/s]
Processed prompts:  82%|████████▏ | 3349/4096 [00:28<00:08, 92.58it/s, est. speed input: 121432.26 toks/s, output: 118.59 toks/s]
Processed prompts:  83%|████████▎ | 3381/4096 [00:28<00:07, 92.52it/s, est. speed input: 121107.39 toks/s, output: 118.27 toks/s]
Processed prompts:  83%|████████▎ | 3413/4096 [00:28<00:07, 92.38it/s, est. speed input: 120784.89 toks/s, output: 117.95 toks/s]
Processed prompts:  84%|████████▍ | 3445/4096 [00:29<00:07, 92.44it/s, est. speed input: 120477.92 toks/s, output: 117.65 toks/s]
Processed prompts:  85%|████████▍ | 3477/4096 [00:29<00:06, 93.33it/s, est. speed input: 120221.13 toks/s, output: 117.40 toks/s]
Processed prompts:  86%|████████▌ | 3509/4096 [00:29<00:06, 93.93it/s, est. speed input: 119968.30 toks/s, output: 117.16 toks/s]
Processed prompts:  86%|████████▋ | 3541/4096 [00:30<00:05, 93.43it/s, est. speed input: 119676.69 toks/s, output: 116.87 toks/s]
Processed prompts:  87%|████████▋ | 3573/4096 [00:30<00:05, 93.19it/s, est. speed input: 119397.02 toks/s, output: 116.60 toks/s]
Processed prompts:  88%|████████▊ | 3605/4096 [00:30<00:05, 92.94it/s, est. speed input: 119119.24 toks/s, output: 116.33 toks/s]
Processed prompts:  89%|████████▉ | 3637/4096 [00:31<00:04, 92.77it/s, est. speed input: 118848.25 toks/s, output: 116.06 toks/s]
Processed prompts:  90%|████████▉ | 3669/4096 [00:31<00:04, 92.74it/s, est. speed input: 118587.27 toks/s, output: 115.81 toks/s]
Processed prompts:  90%|█████████ | 3701/4096 [00:32<00:04, 92.57it/s, est. speed input: 118324.86 toks/s, output: 115.55 toks/s]
Processed prompts:  91%|█████████ | 3733/4096 [00:32<00:03, 93.13it/s, est. speed input: 118098.78 toks/s, output: 115.33 toks/s]
Processed prompts:  92%|█████████▏| 3765/4096 [00:32<00:03, 92.96it/s, est. speed input: 117852.64 toks/s, output: 115.09 toks/s]
Processed prompts:  93%|█████████▎| 3797/4096 [00:33<00:03, 92.80it/s, est. speed input: 117609.64 toks/s, output: 114.85 toks/s]
Processed prompts:  93%|█████████▎| 3829/4096 [00:33<00:02, 93.40it/s, est. speed input: 117402.41 toks/s, output: 114.65 toks/s]
Processed prompts:  94%|█████████▍| 3861/4096 [00:33<00:02, 93.07it/s, est. speed input: 117167.50 toks/s, output: 114.42 toks/s]
Processed prompts:  95%|█████████▌| 3893/4096 [00:34<00:02, 92.77it/s, est. speed input: 116934.04 toks/s, output: 114.19 toks/s]
Processed prompts:  96%|█████████▌| 3925/4096 [00:34<00:01, 92.57it/s, est. speed input: 116706.07 toks/s, output: 113.97 toks/s]
Processed prompts:  97%|█████████▋| 3957/4096 [00:34<00:01, 92.55it/s, est. speed input: 116487.38 toks/s, output: 113.76 toks/s]
Processed prompts:  97%|█████████▋| 3989/4096 [00:35<00:01, 92.35it/s, est. speed input: 116265.29 toks/s, output: 113.54 toks/s]
Processed prompts:  98%|█████████▊| 4021/4096 [00:35<00:00, 93.13it/s, est. speed input: 116084.98 toks/s, output: 113.36 toks/s]
Processed prompts:  99%|█████████▉| 4053/4096 [00:35<00:00, 93.02it/s, est. speed input: 115882.00 toks/s, output: 113.17 toks/s]
Processed prompts: 100%|█████████▉| 4085/4096 [00:35<00:00, 114.05it/s, est. speed input: 116366.00 toks/s, output: 113.64 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:35<00:00, 114.05it/s, est. speed input: 116677.56 toks/s, output: 113.94 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:35<00:00, 113.94it/s, est. speed input: 116677.56 toks/s, output: 113.94 toks/s]
[rank0]:[W128 09:29:29.445836988 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 99.8s

测试结果:
  Requests/s:   92.89
  Tokens/s:     95212.79
  Total Reqs:   4096
  Elapsed:      44.09s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     95119.90


------------------------------------------------------------
  生成 CSV: BitNet-2B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/BitNet-2B-FP8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,32.4340,16638.6255,3.9465
1024,1024,1,128,128,31.6767,32468.5855,4.0408
2048,1024,2,256,128,66.6681,68334.8476,3.8399
4096,1024,4,512,128,78.3706,80329.8351,6.5331
8192,1024,8,1024,128,85.9635,88112.6138,11.9120
16384,1024,16,2048,128,91.0839,93361.0200,22.4848
32768,1024,32,4096,128,92.8905,95212.7935,44.0949

------------------------------------------------------------

[INFO] 完成: 7 成功, 0 失败


============================================================
  Benchmark 完成!
============================================================


总计: 35 成功, 0 失败
============================================================
