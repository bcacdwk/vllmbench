======================================================================
SlideSparse vLLM Throughput Benchmark Log
Created: 2026-01-26 11:43:25
======================================================================

原始命令:
  /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-fp8 --backend cublaslt,cusparselt --stage prefill --sparsity 2_4,2_6,2_8,2_10 --M 512,1024,2048,4096,8192,16384,32768,65536

命令行参数:
  --model: qwen2.5-7b-fp8
  --backend: cublaslt,cusparselt
  --sparsity: 2_4,2_6,2_8,2_10
  --stage: prefill
  --M: 512,1024,2048,4096,8192,16384,32768,65536
  --N: None
  --inner-32: False
  --eager: False
  --gpu-id: 0
  --gpu-mem: 0.8
  --dry-run: False
  --list-models: False

硬件信息:
  GPU: H100
  Compute Capability: cc90
  VRAM: 79.2 GB
  CUDA: 12.9
  Python: py312

Backend 环境变量 (初始状态):
  DISABLE_SLIDESPARSE: 未设置
  USE_CUBLASLT: 未设置
  USE_CUSPARSELT: 未设置
  SPARSITY: 未设置
  INNER_DTYPE_32: 未设置

======================================================================


============================================================
  Qwen2.5-7B-FP8 | cuBLASLt | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints/Qwen2.5-7B-FP8
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_FP8E4M3_py312_cu129_x86_64/cublaslt

============================================================
[1/8] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuBLASLt                                        │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 11:43:32 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1304689) WARNING 01-26 11:43:48 [backends.py:609] Failed to read file <frozen os>
Throughput: 35.87 requests/s, 18402.22 total tokens/s, 35.87 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-26 11:43:32] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:43:32] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 11:43:32] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 11:43:32] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:43:32] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:43:32] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:43:32] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:43:32] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:43:32] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 11:43:32] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:43:32] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:43:32] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:43:32] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:43:32] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 11:43:39] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:43:40] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 11:43:40] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 11:43:40] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:43:40] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:43:40] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:43:40] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:43:40] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:43:40] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 11:43:40] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:43:40] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:43:40] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:43:40] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:43:40] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1304689) [2026-01-26 11:43:41] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=1304689) [2026-01-26 11:43:41] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1304689) [2026-01-26 11:43:41] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=1304689) [2026-01-26 11:43:41] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=1304689) [2026-01-26 11:43:41] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=1304689) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1304689) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.61it/s]
(EngineCore_DP0 pid=1304689) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  1.93it/s]
(EngineCore_DP0 pid=1304689) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.01it/s]
(EngineCore_DP0 pid=1304689) 
(EngineCore_DP0 pid=1304689) 2026-01-26 11:43:59,540 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1304689) 2026-01-26 11:43:59,564 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1304689) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  4.06it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  2.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  2.68it/s]
(EngineCore_DP0 pid=1304689) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  2.11it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  2.11it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  27%|██▋       | 35/128 [00:00<00:00, 346.86it/s]
Adding requests:  83%|████████▎ | 106/128 [00:00<00:00, 556.49it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 547.25it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 2/128 [00:00<00:08, 15.45it/s, est. speed input: 7910.33 toks/s, output: 15.45 toks/s]
Processed prompts:   5%|▍         | 6/128 [00:00<00:04, 28.04it/s, est. speed input: 13277.55 toks/s, output: 25.93 toks/s]
Processed prompts:   8%|▊         | 10/128 [00:00<00:03, 32.92it/s, est. speed input: 15381.23 toks/s, output: 30.04 toks/s]
Processed prompts:  11%|█         | 14/128 [00:00<00:03, 34.65it/s, est. speed input: 16288.60 toks/s, output: 31.81 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:00<00:03, 36.31it/s, est. speed input: 17019.38 toks/s, output: 33.24 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:00<00:02, 37.26it/s, est. speed input: 17501.48 toks/s, output: 34.18 toks/s]
Processed prompts:  20%|██        | 26/128 [00:00<00:02, 37.95it/s, est. speed input: 17864.91 toks/s, output: 34.89 toks/s]
Processed prompts:  23%|██▎       | 30/128 [00:00<00:02, 38.40it/s, est. speed input: 18141.63 toks/s, output: 35.43 toks/s]
Processed prompts:  27%|██▋       | 34/128 [00:00<00:02, 38.67it/s, est. speed input: 18352.22 toks/s, output: 35.84 toks/s]
Processed prompts:  30%|██▉       | 38/128 [00:01<00:02, 38.93it/s, est. speed input: 18533.69 toks/s, output: 36.20 toks/s]
Processed prompts:  33%|███▎      | 42/128 [00:01<00:02, 39.07it/s, est. speed input: 18677.21 toks/s, output: 36.48 toks/s]
Processed prompts:  36%|███▌      | 46/128 [00:01<00:02, 39.11it/s, est. speed input: 18791.46 toks/s, output: 36.70 toks/s]
Processed prompts:  39%|███▉      | 50/128 [00:01<00:01, 39.18it/s, est. speed input: 18892.06 toks/s, output: 36.90 toks/s]
Processed prompts:  42%|████▏     | 54/128 [00:01<00:01, 39.23it/s, est. speed input: 18979.93 toks/s, output: 37.07 toks/s]
Processed prompts:  45%|████▌     | 58/128 [00:01<00:01, 39.22it/s, est. speed input: 19051.63 toks/s, output: 37.21 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:01<00:01, 39.34it/s, est. speed input: 19126.85 toks/s, output: 37.36 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:01<00:01, 39.39it/s, est. speed input: 19190.05 toks/s, output: 37.48 toks/s]
Processed prompts:  55%|█████▍    | 70/128 [00:01<00:01, 39.37it/s, est. speed input: 19241.61 toks/s, output: 37.58 toks/s]
Processed prompts:  58%|█████▊    | 74/128 [00:01<00:01, 39.40it/s, est. speed input: 19291.14 toks/s, output: 37.68 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:02<00:01, 39.38it/s, est. speed input: 19332.84 toks/s, output: 37.76 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:02<00:01, 39.31it/s, est. speed input: 19366.62 toks/s, output: 37.82 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:02<00:01, 39.36it/s, est. speed input: 19404.57 toks/s, output: 37.90 toks/s]
Processed prompts:  70%|███████   | 90/128 [00:02<00:00, 39.35it/s, est. speed input: 19435.85 toks/s, output: 37.96 toks/s]
Processed prompts:  73%|███████▎  | 94/128 [00:02<00:00, 39.39it/s, est. speed input: 19467.51 toks/s, output: 38.02 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:02<00:00, 39.45it/s, est. speed input: 19499.30 toks/s, output: 38.08 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:02<00:00, 39.52it/s, est. speed input: 19529.85 toks/s, output: 38.14 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:02<00:00, 39.49it/s, est. speed input: 19553.64 toks/s, output: 38.19 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:02<00:00, 39.51it/s, est. speed input: 19578.34 toks/s, output: 38.24 toks/s]
Processed prompts:  89%|████████▉ | 114/128 [00:02<00:00, 39.48it/s, est. speed input: 19598.89 toks/s, output: 38.28 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:03<00:00, 39.46it/s, est. speed input: 19617.84 toks/s, output: 38.32 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:03<00:00, 39.48it/s, est. speed input: 19638.04 toks/s, output: 38.36 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:03<00:00, 39.49it/s, est. speed input: 19655.75 toks/s, output: 38.39 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 39.49it/s, est. speed input: 19665.34 toks/s, output: 38.41 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 38.41it/s, est. speed input: 19665.34 toks/s, output: 38.41 toks/s]
[rank0]:[W126 11:44:06.170362420 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 42.4s

测试结果:
  Requests/s:   35.87
  Tokens/s:     18402.22
  Total Reqs:   128
  Elapsed:      3.57s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     18366.35

============================================================
[2/8] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuBLASLt                                        │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 11:44:16 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1305886) WARNING 01-26 11:44:32 [backends.py:609] Failed to read file <frozen os>
Throughput: 33.01 requests/s, 33835.99 total tokens/s, 33.01 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-26 11:44:15] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:44:16] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 11:44:16] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 11:44:16] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:44:16] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:44:16] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:44:16] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:44:16] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:44:16] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 11:44:16] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:44:16] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:44:16] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:44:16] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:44:16] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 11:44:22] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:44:22] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 11:44:22] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 11:44:22] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:44:22] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:44:22] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:44:22] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:44:22] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:44:22] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 11:44:22] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:44:22] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:44:22] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:44:22] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:44:22] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1305886) [2026-01-26 11:44:24] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=1305886) [2026-01-26 11:44:24] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1305886) [2026-01-26 11:44:24] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=1305886) [2026-01-26 11:44:24] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=1305886) [2026-01-26 11:44:24] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=1305886) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1305886) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.60it/s]
(EngineCore_DP0 pid=1305886) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  1.92it/s]
(EngineCore_DP0 pid=1305886) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.00it/s]
(EngineCore_DP0 pid=1305886) 
(EngineCore_DP0 pid=1305886) 2026-01-26 11:44:42,192 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1305886) 2026-01-26 11:44:42,215 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1305886) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  8.12it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  2.80it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.11it/s]
(EngineCore_DP0 pid=1305886) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  5.43it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  5.43it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  14%|█▍        | 18/128 [00:00<00:00, 172.46it/s]
Adding requests:  42%|████▏     | 54/128 [00:00<00:00, 279.23it/s]
Adding requests:  73%|███████▎  | 93/128 [00:00<00:00, 326.46it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 318.68it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   9%|▉         | 12/128 [00:00<00:01, 96.79it/s, est. speed input: 99131.02 toks/s, output: 96.80 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:00<00:02, 49.06it/s, est. speed input: 54656.78 toks/s, output: 53.37 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:00<00:02, 42.88it/s, est. speed input: 48424.40 toks/s, output: 47.29 toks/s]
Processed prompts:  27%|██▋       | 34/128 [00:00<00:02, 40.44it/s, est. speed input: 46023.23 toks/s, output: 44.94 toks/s]
Processed prompts:  30%|███       | 39/128 [00:00<00:02, 38.61it/s, est. speed input: 44298.85 toks/s, output: 43.26 toks/s]
Processed prompts:  34%|███▍      | 44/128 [00:01<00:02, 37.19it/s, est. speed input: 42969.36 toks/s, output: 41.96 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:01<00:02, 36.58it/s, est. speed input: 42246.30 toks/s, output: 41.26 toks/s]
Processed prompts:  41%|████      | 52/128 [00:01<00:02, 36.09it/s, est. speed input: 41644.23 toks/s, output: 40.67 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:01<00:02, 35.72it/s, est. speed input: 41143.81 toks/s, output: 40.18 toks/s]
Processed prompts:  47%|████▋     | 60/128 [00:01<00:01, 35.44it/s, est. speed input: 40718.30 toks/s, output: 39.76 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:01<00:01, 35.25it/s, est. speed input: 40356.17 toks/s, output: 39.41 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:01<00:01, 35.10it/s, est. speed input: 40039.61 toks/s, output: 39.10 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:01<00:01, 34.93it/s, est. speed input: 39748.23 toks/s, output: 38.82 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:01<00:01, 34.76it/s, est. speed input: 39477.84 toks/s, output: 38.55 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:02<00:01, 34.65it/s, est. speed input: 39240.43 toks/s, output: 38.32 toks/s]
Processed prompts:  66%|██████▌   | 84/128 [00:02<00:01, 34.65it/s, est. speed input: 39043.93 toks/s, output: 38.13 toks/s]
Processed prompts:  69%|██████▉   | 88/128 [00:02<00:01, 34.71it/s, est. speed input: 38876.75 toks/s, output: 37.97 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:02<00:01, 34.64it/s, est. speed input: 38707.11 toks/s, output: 37.80 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:02<00:00, 34.68it/s, est. speed input: 38567.32 toks/s, output: 37.66 toks/s]
Processed prompts:  78%|███████▊  | 100/128 [00:02<00:00, 34.71it/s, est. speed input: 38440.15 toks/s, output: 37.54 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:02<00:00, 34.69it/s, est. speed input: 38316.04 toks/s, output: 37.42 toks/s]
Processed prompts:  84%|████████▍ | 108/128 [00:02<00:00, 34.63it/s, est. speed input: 38196.44 toks/s, output: 37.30 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:03<00:00, 34.62it/s, est. speed input: 38090.37 toks/s, output: 37.20 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:03<00:00, 34.57it/s, est. speed input: 37986.43 toks/s, output: 37.10 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:03<00:00, 34.58it/s, est. speed input: 37894.92 toks/s, output: 37.01 toks/s]
Processed prompts:  97%|█████████▋| 124/128 [00:03<00:00, 34.63it/s, est. speed input: 37816.07 toks/s, output: 36.93 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 34.57it/s, est. speed input: 37730.72 toks/s, output: 36.85 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 34.57it/s, est. speed input: 37730.72 toks/s, output: 36.85 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 36.84it/s, est. speed input: 37730.72 toks/s, output: 36.85 toks/s]
[rank0]:[W126 11:44:48.731166172 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 42.6s

测试结果:
  Requests/s:   33.01
  Tokens/s:     33835.99
  Total Reqs:   128
  Elapsed:      3.88s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     33802.98

============================================================
[3/8] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuBLASLt                                        │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 11:44:59 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1306976) WARNING 01-26 11:45:14 [backends.py:609] Failed to read file <frozen os>
Throughput: 35.47 requests/s, 36353.59 total tokens/s, 35.47 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-26 11:44:58] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:44:59] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 11:44:59] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 11:44:59] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:44:59] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:44:59] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:44:59] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:44:59] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:44:59] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 11:44:59] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:44:59] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:44:59] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:44:59] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:44:59] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 11:45:05] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:45:06] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 11:45:06] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 11:45:06] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:45:06] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:45:06] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:45:06] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:45:06] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:45:06] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 11:45:06] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:45:06] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:45:06] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:45:06] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:45:06] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1306976) [2026-01-26 11:45:07] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=1306976) [2026-01-26 11:45:07] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1306976) [2026-01-26 11:45:07] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=1306976) [2026-01-26 11:45:07] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=1306976) [2026-01-26 11:45:07] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=1306976) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1306976) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.62it/s]
(EngineCore_DP0 pid=1306976) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  1.93it/s]
(EngineCore_DP0 pid=1306976) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.01it/s]
(EngineCore_DP0 pid=1306976) 
(EngineCore_DP0 pid=1306976) 2026-01-26 11:45:25,181 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1306976) 2026-01-26 11:45:25,205 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1306976) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:00<00:00, 15.11it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 14.79it/s]
(EngineCore_DP0 pid=1306976) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 1/2 [00:00<00:00,  5.66it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00,  5.45it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00,  5.47it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   8%|▊         | 21/256 [00:00<00:01, 208.05it/s]
Adding requests:  24%|██▍       | 62/256 [00:00<00:00, 325.33it/s]
Adding requests:  38%|███▊      | 98/256 [00:00<00:00, 340.08it/s]
Adding requests:  53%|█████▎    | 136/256 [00:00<00:00, 355.39it/s]
Adding requests:  69%|██████▉   | 176/256 [00:00<00:00, 368.69it/s]
Adding requests:  84%|████████▍ | 216/256 [00:00<00:00, 377.83it/s]
Adding requests: 100%|█████████▉| 255/256 [00:00<00:00, 379.43it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 360.34it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   7%|▋         | 18/256 [00:00<00:01, 161.19it/s, est. speed input: 165091.57 toks/s, output: 161.20 toks/s]
Processed prompts:  14%|█▎        | 35/256 [00:00<00:03, 58.77it/s, est. speed input: 66726.17 toks/s, output: 65.16 toks/s]   
Processed prompts:  17%|█▋        | 44/256 [00:00<00:04, 47.57it/s, est. speed input: 55675.28 toks/s, output: 54.37 toks/s]
Processed prompts:  20%|█▉        | 51/256 [00:00<00:04, 45.86it/s, est. speed input: 53385.76 toks/s, output: 52.13 toks/s]
Processed prompts:  22%|██▏       | 57/256 [00:01<00:04, 43.45it/s, est. speed input: 51201.80 toks/s, output: 50.00 toks/s]
Processed prompts:  24%|██▍       | 62/256 [00:01<00:04, 40.04it/s, est. speed input: 48849.58 toks/s, output: 47.70 toks/s]
Processed prompts:  26%|██▌       | 67/256 [00:01<00:04, 41.51it/s, est. speed input: 48795.16 toks/s, output: 47.65 toks/s]
Processed prompts:  28%|██▊       | 72/256 [00:01<00:04, 38.32it/s, est. speed input: 47083.05 toks/s, output: 45.98 toks/s]
Processed prompts:  30%|███       | 77/256 [00:01<00:04, 40.18it/s, est. speed input: 47109.56 toks/s, output: 46.01 toks/s]
Processed prompts:  32%|███▏      | 82/256 [00:01<00:04, 36.91it/s, est. speed input: 45680.11 toks/s, output: 44.61 toks/s]
Processed prompts:  34%|███▎      | 86/256 [00:01<00:04, 36.76it/s, est. speed input: 45199.99 toks/s, output: 44.14 toks/s]
Processed prompts:  35%|███▌      | 90/256 [00:02<00:04, 36.70it/s, est. speed input: 44785.10 toks/s, output: 43.73 toks/s]
Processed prompts:  37%|███▋      | 94/256 [00:02<00:04, 36.80it/s, est. speed input: 44445.32 toks/s, output: 43.40 toks/s]
Processed prompts:  38%|███▊      | 98/256 [00:02<00:04, 36.85it/s, est. speed input: 44132.49 toks/s, output: 43.10 toks/s]
Processed prompts:  40%|███▉      | 102/256 [00:02<00:04, 37.05it/s, est. speed input: 43878.40 toks/s, output: 42.85 toks/s]
Processed prompts:  41%|████▏     | 106/256 [00:02<00:04, 37.06it/s, est. speed input: 43622.00 toks/s, output: 42.60 toks/s]
Processed prompts:  43%|████▎     | 110/256 [00:02<00:03, 37.16it/s, est. speed input: 43402.70 toks/s, output: 42.38 toks/s]
Processed prompts:  45%|████▍     | 114/256 [00:02<00:03, 37.19it/s, est. speed input: 43194.51 toks/s, output: 42.18 toks/s]
Processed prompts:  46%|████▌     | 118/256 [00:02<00:03, 36.94it/s, est. speed input: 42960.65 toks/s, output: 41.95 toks/s]
Processed prompts:  48%|████▊     | 122/256 [00:02<00:03, 36.78it/s, est. speed input: 42747.40 toks/s, output: 41.75 toks/s]
Processed prompts:  49%|████▉     | 126/256 [00:03<00:03, 36.71it/s, est. speed input: 42555.28 toks/s, output: 41.56 toks/s]
Processed prompts:  51%|█████     | 130/256 [00:03<00:03, 36.73it/s, est. speed input: 42386.45 toks/s, output: 41.39 toks/s]
Processed prompts:  52%|█████▏    | 134/256 [00:03<00:03, 36.87it/s, est. speed input: 42244.32 toks/s, output: 41.25 toks/s]
Processed prompts:  54%|█████▍    | 138/256 [00:03<00:03, 36.94it/s, est. speed input: 42107.00 toks/s, output: 41.12 toks/s]
Processed prompts:  55%|█████▌    | 142/256 [00:03<00:03, 37.05it/s, est. speed input: 41986.68 toks/s, output: 41.00 toks/s]
Processed prompts:  57%|█████▋    | 146/256 [00:03<00:02, 37.04it/s, est. speed input: 41863.09 toks/s, output: 40.88 toks/s]
Processed prompts:  59%|█████▊    | 150/256 [00:03<00:02, 37.13it/s, est. speed input: 41757.46 toks/s, output: 40.78 toks/s]
Processed prompts:  60%|██████    | 154/256 [00:03<00:02, 36.90it/s, est. speed input: 41626.95 toks/s, output: 40.65 toks/s]
Processed prompts:  62%|██████▏   | 158/256 [00:03<00:02, 36.89it/s, est. speed input: 41518.96 toks/s, output: 40.55 toks/s]
Processed prompts:  63%|██████▎   | 162/256 [00:04<00:02, 36.91it/s, est. speed input: 41419.39 toks/s, output: 40.45 toks/s]
Processed prompts:  65%|██████▍   | 166/256 [00:04<00:02, 36.85it/s, est. speed input: 41317.67 toks/s, output: 40.35 toks/s]
Processed prompts:  66%|██████▋   | 170/256 [00:04<00:02, 36.91it/s, est. speed input: 41231.40 toks/s, output: 40.26 toks/s]
Processed prompts:  68%|██████▊   | 174/256 [00:04<00:02, 36.99it/s, est. speed input: 41152.62 toks/s, output: 40.19 toks/s]
Processed prompts:  70%|██████▉   | 178/256 [00:04<00:02, 37.00it/s, est. speed input: 41074.13 toks/s, output: 40.11 toks/s]
Processed prompts:  71%|███████   | 182/256 [00:04<00:01, 37.02it/s, est. speed input: 41000.31 toks/s, output: 40.04 toks/s]
Processed prompts:  73%|███████▎  | 186/256 [00:04<00:01, 37.04it/s, est. speed input: 40930.06 toks/s, output: 39.97 toks/s]
Processed prompts:  74%|███████▍  | 190/256 [00:04<00:01, 37.05it/s, est. speed input: 40862.68 toks/s, output: 39.90 toks/s]
Processed prompts:  76%|███████▌  | 194/256 [00:04<00:01, 36.97it/s, est. speed input: 40791.55 toks/s, output: 39.84 toks/s]
Processed prompts:  77%|███████▋  | 198/256 [00:04<00:01, 36.95it/s, est. speed input: 40726.15 toks/s, output: 39.77 toks/s]
Processed prompts:  80%|███████▉  | 204/256 [00:05<00:01, 38.70it/s, est. speed input: 40783.61 toks/s, output: 39.83 toks/s]
Processed prompts:  81%|████████▏ | 208/256 [00:05<00:01, 38.24it/s, est. speed input: 40725.34 toks/s, output: 39.77 toks/s]
Processed prompts:  83%|████████▎ | 212/256 [00:05<00:01, 37.87it/s, est. speed input: 40666.87 toks/s, output: 39.71 toks/s]
Processed prompts:  84%|████████▍ | 216/256 [00:05<00:01, 37.66it/s, est. speed input: 40614.80 toks/s, output: 39.66 toks/s]
Processed prompts:  86%|████████▌ | 220/256 [00:05<00:00, 37.52it/s, est. speed input: 40565.17 toks/s, output: 39.61 toks/s]
Processed prompts:  88%|████████▊ | 224/256 [00:05<00:00, 37.42it/s, est. speed input: 40517.86 toks/s, output: 39.57 toks/s]
Processed prompts:  89%|████████▉ | 228/256 [00:05<00:00, 37.21it/s, est. speed input: 40463.06 toks/s, output: 39.51 toks/s]
Processed prompts:  91%|█████████ | 232/256 [00:05<00:00, 37.07it/s, est. speed input: 40410.17 toks/s, output: 39.46 toks/s]
Processed prompts:  92%|█████████▏| 236/256 [00:05<00:00, 36.93it/s, est. speed input: 40356.95 toks/s, output: 39.41 toks/s]
Processed prompts:  94%|█████████▍| 240/256 [00:06<00:00, 36.90it/s, est. speed input: 40309.65 toks/s, output: 39.36 toks/s]
Processed prompts:  95%|█████████▌| 244/256 [00:06<00:00, 37.00it/s, est. speed input: 40272.26 toks/s, output: 39.33 toks/s]
Processed prompts:  97%|█████████▋| 248/256 [00:06<00:00, 36.98it/s, est. speed input: 40229.76 toks/s, output: 39.29 toks/s]
Processed prompts:  98%|█████████▊| 252/256 [00:06<00:00, 36.92it/s, est. speed input: 40186.36 toks/s, output: 39.24 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:06<00:00, 36.92it/s, est. speed input: 40294.62 toks/s, output: 39.35 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:06<00:00, 39.35it/s, est. speed input: 40294.62 toks/s, output: 39.35 toks/s]
[rank0]:[W126 11:45:34.754909238 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 45.7s

测试结果:
  Requests/s:   35.47
  Tokens/s:     36353.59
  Total Reqs:   256
  Elapsed:      7.22s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     36318.12

============================================================
[4/8] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuBLASLt                                        │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 11:45:46 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1308125) WARNING 01-26 11:46:01 [backends.py:609] Failed to read file <frozen os>
Throughput: 38.17 requests/s, 39123.06 total tokens/s, 38.17 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-26 11:45:45] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:45:46] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 11:45:46] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 11:45:46] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:45:46] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:45:46] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:45:46] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:45:46] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:45:46] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 11:45:46] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:45:46] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:45:46] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:45:46] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:45:46] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 11:45:52] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:45:52] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 11:45:52] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 11:45:52] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:45:52] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:45:52] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:45:52] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:45:52] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:45:52] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 11:45:52] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:45:52] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:45:52] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:45:52] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:45:52] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1308125) [2026-01-26 11:45:54] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=1308125) [2026-01-26 11:45:54] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1308125) [2026-01-26 11:45:54] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=1308125) [2026-01-26 11:45:54] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=1308125) [2026-01-26 11:45:54] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=1308125) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1308125) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.62it/s]
(EngineCore_DP0 pid=1308125) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  1.93it/s]
(EngineCore_DP0 pid=1308125) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.00it/s]
(EngineCore_DP0 pid=1308125) 
(EngineCore_DP0 pid=1308125) 2026-01-26 11:46:11,881 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1308125) 2026-01-26 11:46:11,928 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1308125) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00, 17.17it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 16.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 16.87it/s]
(EngineCore_DP0 pid=1308125) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00, 18.79it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 19.16it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   4%|▎         | 19/512 [00:00<00:02, 189.35it/s]
Adding requests:  12%|█▏        | 59/512 [00:00<00:01, 311.41it/s]
Adding requests:  19%|█▉        | 96/512 [00:00<00:01, 335.79it/s]
Adding requests:  26%|██▌       | 134/512 [00:00<00:01, 351.75it/s]
Adding requests:  34%|███▍      | 174/512 [00:00<00:00, 367.60it/s]
Adding requests:  42%|████▏     | 214/512 [00:00<00:00, 378.37it/s]
Adding requests:  49%|████▉     | 253/512 [00:00<00:00, 379.90it/s]
Adding requests:  57%|█████▋    | 293/512 [00:00<00:00, 382.63it/s]
Adding requests:  65%|██████▌   | 335/512 [00:00<00:00, 391.75it/s]
Adding requests:  73%|███████▎  | 375/512 [00:01<00:00, 392.91it/s]
Adding requests:  81%|████████▏ | 417/512 [00:01<00:00, 399.46it/s]
Adding requests:  89%|████████▉ | 457/512 [00:01<00:00, 395.36it/s]
Adding requests:  98%|█████████▊| 500/512 [00:01<00:00, 403.11it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 379.32it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  11%|█         | 54/512 [00:00<00:01, 309.71it/s, est. speed input: 317188.50 toks/s, output: 309.73 toks/s]
Processed prompts:  17%|█▋        | 85/512 [00:00<00:05, 80.31it/s, est. speed input: 95758.76 toks/s, output: 93.51 toks/s]   
Processed prompts:  20%|█▉        | 101/512 [00:01<00:06, 63.17it/s, est. speed input: 78062.63 toks/s, output: 76.23 toks/s]
Processed prompts:  22%|██▏       | 112/512 [00:01<00:07, 54.57it/s, est. speed input: 69993.36 toks/s, output: 68.35 toks/s]
Processed prompts:  23%|██▎       | 120/512 [00:01<00:07, 50.62it/s, est. speed input: 66332.74 toks/s, output: 64.78 toks/s]
Processed prompts:  25%|██▍       | 127/512 [00:02<00:08, 46.42it/s, est. speed input: 63058.16 toks/s, output: 61.58 toks/s]
Processed prompts:  26%|██▌       | 133/512 [00:02<00:07, 48.09it/s, est. speed input: 62870.84 toks/s, output: 61.40 toks/s]
Processed prompts:  27%|██▋       | 139/512 [00:02<00:08, 42.42it/s, est. speed input: 59961.27 toks/s, output: 58.56 toks/s]
Processed prompts:  28%|██▊       | 144/512 [00:02<00:08, 43.38it/s, est. speed input: 59502.67 toks/s, output: 58.11 toks/s]
Processed prompts:  29%|██▉       | 149/512 [00:02<00:08, 44.25it/s, est. speed input: 59070.91 toks/s, output: 57.69 toks/s]
Processed prompts:  30%|███       | 154/512 [00:02<00:09, 36.63it/s, est. speed input: 56412.75 toks/s, output: 55.09 toks/s]
Processed prompts:  31%|███       | 158/512 [00:02<00:09, 36.91it/s, est. speed input: 55775.59 toks/s, output: 54.47 toks/s]
Processed prompts:  32%|███▏      | 162/512 [00:03<00:09, 37.23it/s, est. speed input: 55203.05 toks/s, output: 53.91 toks/s]
Processed prompts:  32%|███▏      | 166/512 [00:03<00:09, 37.55it/s, est. speed input: 54676.27 toks/s, output: 53.39 toks/s]
Processed prompts:  33%|███▎      | 170/512 [00:03<00:09, 37.71it/s, est. speed input: 54167.69 toks/s, output: 52.90 toks/s]
Processed prompts:  34%|███▍      | 174/512 [00:03<00:08, 37.86it/s, est. speed input: 53696.62 toks/s, output: 52.44 toks/s]
Processed prompts:  35%|███▍      | 178/512 [00:03<00:08, 38.00it/s, est. speed input: 53256.74 toks/s, output: 52.01 toks/s]
Processed prompts:  36%|███▌      | 182/512 [00:03<00:08, 38.11it/s, est. speed input: 52844.62 toks/s, output: 51.61 toks/s]
Processed prompts:  36%|███▋      | 186/512 [00:03<00:08, 38.06it/s, est. speed input: 52438.91 toks/s, output: 51.21 toks/s]
Processed prompts:  37%|███▋      | 190/512 [00:03<00:08, 37.93it/s, est. speed input: 52042.06 toks/s, output: 50.82 toks/s]
Processed prompts:  38%|███▊      | 194/512 [00:03<00:08, 37.79it/s, est. speed input: 51662.62 toks/s, output: 50.45 toks/s]
Processed prompts:  39%|███▊      | 198/512 [00:03<00:08, 37.90it/s, est. speed input: 51329.09 toks/s, output: 50.13 toks/s]
Processed prompts:  40%|████      | 206/512 [00:04<00:07, 39.67it/s, est. speed input: 50945.87 toks/s, output: 49.75 toks/s]
Processed prompts:  41%|████      | 210/512 [00:04<00:07, 39.40it/s, est. speed input: 50666.43 toks/s, output: 49.48 toks/s]
Processed prompts:  42%|████▏     | 214/512 [00:04<00:07, 39.16it/s, est. speed input: 50397.15 toks/s, output: 49.22 toks/s]
Processed prompts:  43%|████▎     | 218/512 [00:04<00:07, 38.84it/s, est. speed input: 50126.19 toks/s, output: 48.95 toks/s]
Processed prompts:  43%|████▎     | 222/512 [00:04<00:07, 38.53it/s, est. speed input: 49860.26 toks/s, output: 48.69 toks/s]
Processed prompts:  44%|████▍     | 226/512 [00:04<00:07, 38.34it/s, est. speed input: 49608.82 toks/s, output: 48.45 toks/s]
Processed prompts:  45%|████▍     | 230/512 [00:04<00:07, 38.22it/s, est. speed input: 49371.00 toks/s, output: 48.21 toks/s]
Processed prompts:  46%|████▌     | 234/512 [00:04<00:07, 38.13it/s, est. speed input: 49142.85 toks/s, output: 47.99 toks/s]
Processed prompts:  46%|████▋     | 238/512 [00:04<00:07, 38.06it/s, est. speed input: 48923.72 toks/s, output: 47.78 toks/s]
Processed prompts:  47%|████▋     | 242/512 [00:05<00:07, 38.10it/s, est. speed input: 48721.76 toks/s, output: 47.58 toks/s]
Processed prompts:  48%|████▊     | 246/512 [00:05<00:06, 38.20it/s, est. speed input: 48533.90 toks/s, output: 47.40 toks/s]
Processed prompts:  49%|████▉     | 250/512 [00:05<00:06, 38.22it/s, est. speed input: 48349.34 toks/s, output: 47.22 toks/s]
Processed prompts:  50%|████▉     | 254/512 [00:05<00:06, 38.27it/s, est. speed input: 48174.68 toks/s, output: 47.05 toks/s]
Processed prompts:  50%|█████     | 258/512 [00:05<00:06, 38.25it/s, est. speed input: 48002.37 toks/s, output: 46.88 toks/s]
Processed prompts:  51%|█████     | 262/512 [00:05<00:06, 38.28it/s, est. speed input: 47839.89 toks/s, output: 46.72 toks/s]
Processed prompts:  52%|█████▏    | 266/512 [00:05<00:06, 38.13it/s, est. speed input: 47670.50 toks/s, output: 46.55 toks/s]
Processed prompts:  53%|█████▎    | 270/512 [00:05<00:06, 38.08it/s, est. speed input: 47511.24 toks/s, output: 46.40 toks/s]
Processed prompts:  54%|█████▎    | 274/512 [00:05<00:06, 38.04it/s, est. speed input: 47357.13 toks/s, output: 46.25 toks/s]
Processed prompts:  54%|█████▍    | 278/512 [00:06<00:06, 38.01it/s, est. speed input: 47208.40 toks/s, output: 46.10 toks/s]
Processed prompts:  55%|█████▌    | 282/512 [00:06<00:06, 37.96it/s, est. speed input: 47062.85 toks/s, output: 45.96 toks/s]
Processed prompts:  56%|█████▌    | 286/512 [00:06<00:05, 38.02it/s, est. speed input: 46928.49 toks/s, output: 45.83 toks/s]
Processed prompts:  57%|█████▋    | 290/512 [00:06<00:05, 38.18it/s, est. speed input: 46806.91 toks/s, output: 45.71 toks/s]
Processed prompts:  57%|█████▋    | 294/512 [00:06<00:05, 38.27it/s, est. speed input: 46687.30 toks/s, output: 45.59 toks/s]
Processed prompts:  58%|█████▊    | 298/512 [00:06<00:05, 38.19it/s, est. speed input: 46562.76 toks/s, output: 45.47 toks/s]
Processed prompts:  59%|█████▉    | 302/512 [00:06<00:05, 38.08it/s, est. speed input: 46438.46 toks/s, output: 45.35 toks/s]
Processed prompts:  61%|██████    | 310/512 [00:06<00:05, 39.89it/s, est. speed input: 46349.93 toks/s, output: 45.26 toks/s]
Processed prompts:  61%|██████▏   | 314/512 [00:06<00:05, 39.38it/s, est. speed input: 46235.99 toks/s, output: 45.15 toks/s]
Processed prompts:  62%|██████▏   | 318/512 [00:07<00:04, 39.02it/s, est. speed input: 46127.21 toks/s, output: 45.05 toks/s]
Processed prompts:  63%|██████▎   | 322/512 [00:07<00:04, 38.71it/s, est. speed input: 46019.57 toks/s, output: 44.94 toks/s]
Processed prompts:  64%|██████▎   | 326/512 [00:07<00:04, 38.55it/s, est. speed input: 45919.23 toks/s, output: 44.84 toks/s]
Processed prompts:  64%|██████▍   | 330/512 [00:07<00:04, 38.44it/s, est. speed input: 45822.46 toks/s, output: 44.75 toks/s]
Processed prompts:  65%|██████▌   | 334/512 [00:07<00:04, 38.39it/s, est. speed input: 45729.10 toks/s, output: 44.66 toks/s]
Processed prompts:  66%|██████▌   | 338/512 [00:07<00:04, 38.29it/s, est. speed input: 45635.33 toks/s, output: 44.57 toks/s]
Processed prompts:  67%|██████▋   | 342/512 [00:07<00:04, 38.15it/s, est. speed input: 45540.57 toks/s, output: 44.47 toks/s]
Processed prompts:  68%|██████▊   | 346/512 [00:07<00:04, 38.07it/s, est. speed input: 45448.99 toks/s, output: 44.38 toks/s]
Processed prompts:  68%|██████▊   | 350/512 [00:07<00:04, 38.06it/s, est. speed input: 45362.75 toks/s, output: 44.30 toks/s]
Processed prompts:  69%|██████▉   | 354/512 [00:08<00:04, 38.04it/s, est. speed input: 45277.71 toks/s, output: 44.22 toks/s]
Processed prompts:  70%|██████▉   | 358/512 [00:08<00:04, 37.99it/s, est. speed input: 45193.21 toks/s, output: 44.13 toks/s]
Processed prompts:  71%|███████   | 362/512 [00:08<00:03, 37.96it/s, est. speed input: 45111.16 toks/s, output: 44.05 toks/s]
Processed prompts:  71%|███████▏  | 366/512 [00:08<00:03, 38.02it/s, est. speed input: 45034.90 toks/s, output: 43.98 toks/s]
Processed prompts:  72%|███████▏  | 370/512 [00:08<00:03, 38.10it/s, est. speed input: 44962.84 toks/s, output: 43.91 toks/s]
Processed prompts:  73%|███████▎  | 374/512 [00:08<00:03, 38.09it/s, est. speed input: 44888.86 toks/s, output: 43.84 toks/s]
Processed prompts:  74%|███████▍  | 378/512 [00:08<00:03, 37.96it/s, est. speed input: 44811.31 toks/s, output: 43.76 toks/s]
Processed prompts:  75%|███████▍  | 382/512 [00:08<00:03, 38.00it/s, est. speed input: 44741.68 toks/s, output: 43.69 toks/s]
Processed prompts:  75%|███████▌  | 386/512 [00:08<00:03, 38.16it/s, est. speed input: 44679.88 toks/s, output: 43.63 toks/s]
Processed prompts:  76%|███████▌  | 390/512 [00:08<00:03, 38.16it/s, est. speed input: 44613.84 toks/s, output: 43.57 toks/s]
Processed prompts:  77%|███████▋  | 394/512 [00:09<00:03, 38.03it/s, est. speed input: 44543.92 toks/s, output: 43.50 toks/s]
Processed prompts:  78%|███████▊  | 398/512 [00:09<00:02, 38.03it/s, est. speed input: 44479.71 toks/s, output: 43.44 toks/s]
Processed prompts:  79%|███████▊  | 402/512 [00:09<00:02, 38.16it/s, est. speed input: 44422.58 toks/s, output: 43.38 toks/s]
Processed prompts:  79%|███████▉  | 406/512 [00:09<00:02, 38.11it/s, est. speed input: 44360.76 toks/s, output: 43.32 toks/s]
Processed prompts:  80%|████████  | 410/512 [00:09<00:02, 38.14it/s, est. speed input: 44302.94 toks/s, output: 43.26 toks/s]
Processed prompts:  81%|████████  | 414/512 [00:09<00:02, 38.03it/s, est. speed input: 44240.77 toks/s, output: 43.20 toks/s]
Processed prompts:  82%|████████▏ | 418/512 [00:09<00:02, 38.07it/s, est. speed input: 44184.99 toks/s, output: 43.15 toks/s]
Processed prompts:  82%|████████▏ | 422/512 [00:09<00:02, 38.13it/s, est. speed input: 44131.74 toks/s, output: 43.10 toks/s]
Processed prompts:  83%|████████▎ | 426/512 [00:09<00:02, 38.06it/s, est. speed input: 44074.99 toks/s, output: 43.04 toks/s]
Processed prompts:  84%|████████▍ | 430/512 [00:10<00:02, 37.99it/s, est. speed input: 44018.35 toks/s, output: 42.99 toks/s]
Processed prompts:  86%|████████▌ | 438/512 [00:10<00:01, 39.74it/s, est. speed input: 43999.73 toks/s, output: 42.97 toks/s]
Processed prompts:  86%|████████▋ | 442/512 [00:10<00:01, 39.31it/s, est. speed input: 43948.09 toks/s, output: 42.92 toks/s]
Processed prompts:  87%|████████▋ | 446/512 [00:10<00:01, 39.07it/s, est. speed input: 43901.93 toks/s, output: 42.87 toks/s]
Processed prompts:  88%|████████▊ | 450/512 [00:10<00:01, 38.76it/s, est. speed input: 43851.55 toks/s, output: 42.82 toks/s]
Processed prompts:  89%|████████▊ | 454/512 [00:10<00:01, 38.42it/s, est. speed input: 43797.45 toks/s, output: 42.77 toks/s]
Processed prompts:  89%|████████▉ | 458/512 [00:10<00:01, 38.38it/s, est. speed input: 43752.86 toks/s, output: 42.73 toks/s]
Processed prompts:  90%|█████████ | 462/512 [00:10<00:01, 38.37it/s, est. speed input: 43709.54 toks/s, output: 42.68 toks/s]
Processed prompts:  91%|█████████ | 466/512 [00:10<00:01, 38.30it/s, est. speed input: 43664.75 toks/s, output: 42.64 toks/s]
Processed prompts:  92%|█████████▏| 470/512 [00:11<00:01, 38.07it/s, est. speed input: 43614.12 toks/s, output: 42.59 toks/s]
Processed prompts:  93%|█████████▎| 474/512 [00:11<00:00, 38.05it/s, est. speed input: 43569.82 toks/s, output: 42.55 toks/s]
Processed prompts:  93%|█████████▎| 478/512 [00:11<00:00, 38.06it/s, est. speed input: 43527.16 toks/s, output: 42.51 toks/s]
Processed prompts:  94%|█████████▍| 482/512 [00:11<00:00, 38.16it/s, est. speed input: 43488.37 toks/s, output: 42.47 toks/s]
Processed prompts:  95%|█████████▍| 486/512 [00:11<00:00, 38.10it/s, est. speed input: 43445.95 toks/s, output: 42.43 toks/s]
Processed prompts:  96%|█████████▌| 490/512 [00:11<00:00, 37.94it/s, est. speed input: 43400.14 toks/s, output: 42.38 toks/s]
Processed prompts:  96%|█████████▋| 494/512 [00:11<00:00, 37.97it/s, est. speed input: 43359.92 toks/s, output: 42.34 toks/s]
Processed prompts:  97%|█████████▋| 498/512 [00:11<00:00, 38.01it/s, est. speed input: 43321.15 toks/s, output: 42.31 toks/s]
Processed prompts:  98%|█████████▊| 502/512 [00:11<00:00, 37.95it/s, est. speed input: 43280.07 toks/s, output: 42.27 toks/s]
Processed prompts:  99%|█████████▉| 506/512 [00:11<00:00, 37.90it/s, est. speed input: 43239.72 toks/s, output: 42.23 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:12<00:00, 37.90it/s, est. speed input: 43464.69 toks/s, output: 42.45 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:12<00:00, 42.45it/s, est. speed input: 43464.69 toks/s, output: 42.45 toks/s]
[rank0]:[W126 11:46:27.529285234 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 52.9s

测试结果:
  Requests/s:   38.17
  Tokens/s:     39123.06
  Total Reqs:   512
  Elapsed:      13.41s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     39084.89

============================================================
[5/8] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuBLASLt                                        │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 11:46:41 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1309375) WARNING 01-26 11:46:57 [backends.py:609] Failed to read file <frozen os>
Throughput: 38.59 requests/s, 39555.38 total tokens/s, 38.59 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-26 11:46:41] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:46:41] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 11:46:41] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 11:46:41] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:46:41] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:46:41] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:46:41] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:46:41] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:46:41] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 11:46:41] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:46:41] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:46:41] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:46:41] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:46:41] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 11:46:48] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:46:49] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 11:46:49] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 11:46:49] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:46:49] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:46:49] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:46:49] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:46:49] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:46:49] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 11:46:49] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:46:49] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:46:49] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:46:49] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:46:49] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1309375) [2026-01-26 11:46:50] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=1309375) [2026-01-26 11:46:50] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1309375) [2026-01-26 11:46:50] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=1309375) [2026-01-26 11:46:50] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=1309375) [2026-01-26 11:46:50] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=1309375) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1309375) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.61it/s]
(EngineCore_DP0 pid=1309375) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.91it/s]
(EngineCore_DP0 pid=1309375) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.99it/s]
(EngineCore_DP0 pid=1309375) 
(EngineCore_DP0 pid=1309375) 2026-01-26 11:47:08,481 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1309375) 2026-01-26 11:47:08,506 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1309375) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 1/5 [00:00<00:00,  6.22it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00,  6.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 3/5 [00:00<00:00,  6.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00,  6.79it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00,  6.67it/s]
(EngineCore_DP0 pid=1309375) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 1/4 [00:00<00:01,  2.26it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00,  3.51it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00,  7.26it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00,  5.58it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 22/1024 [00:00<00:04, 216.72it/s]
Adding requests:   6%|▌         | 62/1024 [00:00<00:02, 323.01it/s]
Adding requests:  10%|▉         | 98/1024 [00:00<00:02, 336.81it/s]
Adding requests:  13%|█▎        | 136/1024 [00:00<00:02, 350.71it/s]
Adding requests:  17%|█▋        | 175/1024 [00:00<00:02, 364.17it/s]
Adding requests:  21%|██        | 216/1024 [00:00<00:02, 376.10it/s]
Adding requests:  25%|██▍       | 254/1024 [00:00<00:02, 376.97it/s]
Adding requests:  29%|██▊       | 293/1024 [00:00<00:01, 380.93it/s]
Adding requests:  33%|███▎      | 334/1024 [00:00<00:01, 389.06it/s]
Adding requests:  37%|███▋      | 374/1024 [00:01<00:01, 392.21it/s]
Adding requests:  41%|████      | 415/1024 [00:01<00:01, 396.80it/s]
Adding requests:  44%|████▍     | 455/1024 [00:01<00:01, 394.45it/s]
Adding requests:  49%|████▊     | 498/1024 [00:01<00:01, 402.06it/s]
Adding requests:  53%|█████▎    | 539/1024 [00:01<00:01, 401.78it/s]
Adding requests:  57%|█████▋    | 580/1024 [00:01<00:01, 398.85it/s]
Adding requests:  61%|██████    | 620/1024 [00:01<00:01, 391.07it/s]
Adding requests:  64%|██████▍   | 660/1024 [00:01<00:00, 386.24it/s]
Adding requests:  68%|██████▊   | 700/1024 [00:01<00:00, 388.39it/s]
Adding requests:  72%|███████▏  | 739/1024 [00:01<00:00, 383.40it/s]
Adding requests:  76%|███████▌  | 778/1024 [00:02<00:00, 383.59it/s]
Adding requests:  80%|███████▉  | 817/1024 [00:02<00:00, 382.23it/s]
Adding requests:  84%|████████▍ | 858/1024 [00:02<00:00, 389.56it/s]
Adding requests:  88%|████████▊ | 898/1024 [00:02<00:00, 389.68it/s]
Adding requests:  92%|█████████▏| 937/1024 [00:02<00:00, 380.99it/s]
Adding requests:  95%|█████████▌| 976/1024 [00:02<00:00, 380.94it/s]
Adding requests:  99%|█████████▉| 1015/1024 [00:02<00:00, 373.22it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 380.18it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  10%|▉         | 98/1024 [00:00<00:02, 443.53it/s, est. speed input: 454220.70 toks/s, output: 443.54 toks/s]
Processed prompts:  14%|█▍        | 143/1024 [00:01<00:09, 95.97it/s, est. speed input: 117149.51 toks/s, output: 114.40 toks/s]
Processed prompts:  16%|█▌        | 164/1024 [00:01<00:12, 69.31it/s, est. speed input: 89725.86 toks/s, output: 87.62 toks/s]  
Processed prompts:  17%|█▋        | 177/1024 [00:02<00:12, 68.34it/s, est. speed input: 87290.00 toks/s, output: 85.24 toks/s]
Processed prompts:  18%|█▊        | 188/1024 [00:02<00:15, 54.75it/s, est. speed input: 77341.12 toks/s, output: 75.53 toks/s]
Processed prompts:  19%|█▉        | 196/1024 [00:02<00:15, 51.78it/s, est. speed input: 74485.81 toks/s, output: 72.74 toks/s]
Processed prompts:  20%|█▉        | 203/1024 [00:02<00:16, 49.05it/s, est. speed input: 72183.06 toks/s, output: 70.49 toks/s]
Processed prompts:  21%|██        | 210/1024 [00:03<00:17, 45.44it/s, est. speed input: 69667.90 toks/s, output: 68.03 toks/s]
Processed prompts:  21%|██▏       | 218/1024 [00:03<00:18, 43.90it/s, est. speed input: 67847.96 toks/s, output: 66.26 toks/s]
Processed prompts:  22%|██▏       | 226/1024 [00:03<00:18, 42.42it/s, est. speed input: 66144.35 toks/s, output: 64.59 toks/s]
Processed prompts:  23%|██▎       | 234/1024 [00:03<00:19, 41.43it/s, est. speed input: 64676.00 toks/s, output: 63.16 toks/s]
Processed prompts:  24%|██▎       | 242/1024 [00:03<00:19, 40.74it/s, est. speed input: 63376.31 toks/s, output: 61.89 toks/s]
Processed prompts:  24%|██▍       | 250/1024 [00:04<00:19, 40.20it/s, est. speed input: 62196.03 toks/s, output: 60.74 toks/s]
Processed prompts:  25%|██▌       | 258/1024 [00:04<00:19, 39.86it/s, est. speed input: 61141.84 toks/s, output: 59.71 toks/s]
Processed prompts:  26%|██▌       | 266/1024 [00:04<00:19, 39.55it/s, est. speed input: 60166.69 toks/s, output: 58.76 toks/s]
Processed prompts:  27%|██▋       | 274/1024 [00:04<00:19, 39.25it/s, est. speed input: 59259.89 toks/s, output: 57.87 toks/s]
Processed prompts:  28%|██▊       | 282/1024 [00:04<00:18, 39.11it/s, est. speed input: 58443.79 toks/s, output: 57.07 toks/s]
Processed prompts:  28%|██▊       | 290/1024 [00:05<00:18, 38.97it/s, est. speed input: 57684.39 toks/s, output: 56.33 toks/s]
Processed prompts:  29%|██▉       | 298/1024 [00:05<00:18, 38.99it/s, est. speed input: 57006.14 toks/s, output: 55.67 toks/s]
Processed prompts:  30%|██▉       | 306/1024 [00:05<00:17, 40.25it/s, est. speed input: 56594.95 toks/s, output: 55.27 toks/s]
Processed prompts:  31%|███       | 314/1024 [00:05<00:17, 39.86it/s, est. speed input: 55998.20 toks/s, output: 54.69 toks/s]
Processed prompts:  31%|███▏      | 322/1024 [00:05<00:17, 39.55it/s, est. speed input: 55435.86 toks/s, output: 54.14 toks/s]
Processed prompts:  32%|███▏      | 330/1024 [00:06<00:17, 39.29it/s, est. speed input: 54905.16 toks/s, output: 53.62 toks/s]
Processed prompts:  33%|███▎      | 338/1024 [00:06<00:17, 39.14it/s, est. speed input: 54412.65 toks/s, output: 53.14 toks/s]
Processed prompts:  34%|███▍      | 346/1024 [00:06<00:17, 39.03it/s, est. speed input: 53950.71 toks/s, output: 52.69 toks/s]
Processed prompts:  35%|███▍      | 354/1024 [00:06<00:17, 39.04it/s, est. speed input: 53529.25 toks/s, output: 52.27 toks/s]
Processed prompts:  35%|███▌      | 362/1024 [00:06<00:16, 38.96it/s, est. speed input: 53120.03 toks/s, output: 51.87 toks/s]
Processed prompts:  36%|███▌      | 370/1024 [00:07<00:16, 38.95it/s, est. speed input: 52740.57 toks/s, output: 51.50 toks/s]
Processed prompts:  37%|███▋      | 378/1024 [00:07<00:16, 38.86it/s, est. speed input: 52372.11 toks/s, output: 51.14 toks/s]
Processed prompts:  38%|███▊      | 386/1024 [00:07<00:16, 38.87it/s, est. speed input: 52032.36 toks/s, output: 50.81 toks/s]
Processed prompts:  38%|███▊      | 394/1024 [00:07<00:16, 38.95it/s, est. speed input: 51719.24 toks/s, output: 50.51 toks/s]
Processed prompts:  39%|███▉      | 402/1024 [00:08<00:16, 38.81it/s, est. speed input: 51399.68 toks/s, output: 50.19 toks/s]
Processed prompts:  40%|████      | 410/1024 [00:08<00:15, 38.75it/s, est. speed input: 51100.58 toks/s, output: 49.90 toks/s]
Processed prompts:  41%|████      | 418/1024 [00:08<00:15, 38.60it/s, est. speed input: 50804.45 toks/s, output: 49.61 toks/s]
Processed prompts:  42%|████▏     | 426/1024 [00:08<00:15, 38.69it/s, est. speed input: 50543.43 toks/s, output: 49.36 toks/s]
Processed prompts:  42%|████▏     | 434/1024 [00:08<00:14, 40.02it/s, est. speed input: 50418.65 toks/s, output: 49.24 toks/s]
Processed prompts:  43%|████▎     | 442/1024 [00:09<00:14, 39.63it/s, est. speed input: 50172.54 toks/s, output: 49.00 toks/s]
Processed prompts:  44%|████▍     | 450/1024 [00:09<00:14, 39.24it/s, est. speed input: 49926.58 toks/s, output: 48.76 toks/s]
Processed prompts:  45%|████▍     | 458/1024 [00:09<00:14, 39.03it/s, est. speed input: 49696.85 toks/s, output: 48.53 toks/s]
Processed prompts:  46%|████▌     | 466/1024 [00:09<00:14, 39.01it/s, est. speed input: 49488.17 toks/s, output: 48.33 toks/s]
Processed prompts:  46%|████▋     | 474/1024 [00:09<00:14, 39.00it/s, est. speed input: 49288.77 toks/s, output: 48.13 toks/s]
Processed prompts:  47%|████▋     | 482/1024 [00:10<00:13, 38.93it/s, est. speed input: 49091.80 toks/s, output: 47.94 toks/s]
Processed prompts:  48%|████▊     | 490/1024 [00:10<00:13, 38.83it/s, est. speed input: 48898.22 toks/s, output: 47.75 toks/s]
Processed prompts:  49%|████▊     | 498/1024 [00:10<00:13, 38.69it/s, est. speed input: 48706.78 toks/s, output: 47.57 toks/s]
Processed prompts:  49%|████▉     | 506/1024 [00:10<00:13, 38.66it/s, est. speed input: 48528.56 toks/s, output: 47.39 toks/s]
Processed prompts:  50%|█████     | 514/1024 [00:10<00:13, 38.65it/s, est. speed input: 48357.54 toks/s, output: 47.22 toks/s]
Processed prompts:  51%|█████     | 522/1024 [00:11<00:12, 38.66it/s, est. speed input: 48194.73 toks/s, output: 47.07 toks/s]
Processed prompts:  52%|█████▏    | 530/1024 [00:11<00:12, 38.74it/s, est. speed input: 48042.79 toks/s, output: 46.92 toks/s]
Processed prompts:  53%|█████▎    | 538/1024 [00:11<00:12, 38.74it/s, est. speed input: 47892.68 toks/s, output: 46.77 toks/s]
Processed prompts:  53%|█████▎    | 546/1024 [00:11<00:12, 38.83it/s, est. speed input: 47753.89 toks/s, output: 46.63 toks/s]
Processed prompts:  54%|█████▍    | 554/1024 [00:11<00:12, 38.81it/s, est. speed input: 47614.31 toks/s, output: 46.50 toks/s]
Processed prompts:  55%|█████▍    | 562/1024 [00:12<00:11, 38.67it/s, est. speed input: 47471.01 toks/s, output: 46.36 toks/s]
Processed prompts:  56%|█████▌    | 570/1024 [00:12<00:11, 38.74it/s, est. speed input: 47343.92 toks/s, output: 46.23 toks/s]
Processed prompts:  56%|█████▋    | 578/1024 [00:12<00:11, 38.63it/s, est. speed input: 47209.63 toks/s, output: 46.10 toks/s]
Processed prompts:  57%|█████▋    | 586/1024 [00:12<00:11, 38.67it/s, est. speed input: 47088.05 toks/s, output: 45.98 toks/s]
Processed prompts:  58%|█████▊    | 594/1024 [00:12<00:11, 38.78it/s, est. speed input: 46975.40 toks/s, output: 45.87 toks/s]
Processed prompts:  59%|█████▉    | 602/1024 [00:13<00:10, 38.67it/s, est. speed input: 46854.63 toks/s, output: 45.76 toks/s]
Processed prompts:  60%|█████▉    | 610/1024 [00:13<00:10, 38.70it/s, est. speed input: 46744.16 toks/s, output: 45.65 toks/s]
Processed prompts:  60%|██████    | 618/1024 [00:13<00:10, 38.79it/s, est. speed input: 46641.48 toks/s, output: 45.55 toks/s]
Processed prompts:  61%|██████    | 626/1024 [00:13<00:10, 38.83it/s, est. speed input: 46539.78 toks/s, output: 45.45 toks/s]
Processed prompts:  62%|██████▏   | 634/1024 [00:13<00:10, 38.73it/s, est. speed input: 46434.17 toks/s, output: 45.35 toks/s]
Processed prompts:  63%|██████▎   | 642/1024 [00:14<00:09, 38.73it/s, est. speed input: 46335.29 toks/s, output: 45.25 toks/s]
Processed prompts:  63%|██████▎   | 650/1024 [00:14<00:09, 38.67it/s, est. speed input: 46236.16 toks/s, output: 45.15 toks/s]
Processed prompts:  64%|██████▍   | 658/1024 [00:14<00:09, 38.71it/s, est. speed input: 46144.65 toks/s, output: 45.06 toks/s]
Processed prompts:  65%|██████▌   | 666/1024 [00:14<00:09, 38.74it/s, est. speed input: 46055.40 toks/s, output: 44.98 toks/s]
Processed prompts:  66%|██████▌   | 674/1024 [00:15<00:09, 38.70it/s, est. speed input: 45965.16 toks/s, output: 44.89 toks/s]
Processed prompts:  67%|██████▋   | 682/1024 [00:15<00:08, 38.53it/s, est. speed input: 45870.02 toks/s, output: 44.79 toks/s]
Processed prompts:  67%|██████▋   | 690/1024 [00:15<00:08, 38.67it/s, est. speed input: 45790.96 toks/s, output: 44.72 toks/s]
Processed prompts:  68%|██████▊   | 698/1024 [00:15<00:08, 38.71it/s, est. speed input: 45711.32 toks/s, output: 44.64 toks/s]
Processed prompts:  69%|██████▉   | 706/1024 [00:15<00:08, 38.66it/s, est. speed input: 45629.59 toks/s, output: 44.56 toks/s]
Processed prompts:  70%|██████▉   | 714/1024 [00:16<00:08, 38.67it/s, est. speed input: 45552.17 toks/s, output: 44.48 toks/s]
Processed prompts:  71%|███████   | 722/1024 [00:16<00:07, 38.61it/s, est. speed input: 45473.28 toks/s, output: 44.41 toks/s]
Processed prompts:  71%|███████▏  | 730/1024 [00:16<00:07, 38.61it/s, est. speed input: 45398.81 toks/s, output: 44.33 toks/s]
Processed prompts:  72%|███████▏  | 738/1024 [00:16<00:07, 38.70it/s, est. speed input: 45330.06 toks/s, output: 44.27 toks/s]
Processed prompts:  73%|███████▎  | 746/1024 [00:16<00:07, 38.73it/s, est. speed input: 45261.74 toks/s, output: 44.20 toks/s]
Processed prompts:  74%|███████▎  | 754/1024 [00:17<00:06, 38.65it/s, est. speed input: 45190.15 toks/s, output: 44.13 toks/s]
Processed prompts:  74%|███████▍  | 762/1024 [00:17<00:06, 38.65it/s, est. speed input: 45123.20 toks/s, output: 44.07 toks/s]
Processed prompts:  75%|███████▌  | 770/1024 [00:17<00:06, 38.49it/s, est. speed input: 45050.15 toks/s, output: 43.99 toks/s]
Processed prompts:  76%|███████▌  | 778/1024 [00:17<00:06, 38.54it/s, est. speed input: 44986.02 toks/s, output: 43.93 toks/s]
Processed prompts:  77%|███████▋  | 786/1024 [00:17<00:05, 39.93it/s, est. speed input: 44982.59 toks/s, output: 43.93 toks/s]
Processed prompts:  78%|███████▊  | 794/1024 [00:18<00:05, 39.64it/s, est. speed input: 44925.14 toks/s, output: 43.87 toks/s]
Processed prompts:  78%|███████▊  | 802/1024 [00:18<00:05, 39.40it/s, est. speed input: 44867.40 toks/s, output: 43.82 toks/s]
Processed prompts:  79%|███████▉  | 810/1024 [00:18<00:05, 39.21it/s, est. speed input: 44809.88 toks/s, output: 43.76 toks/s]
Processed prompts:  80%|███████▉  | 818/1024 [00:18<00:05, 39.08it/s, est. speed input: 44753.63 toks/s, output: 43.70 toks/s]
Processed prompts:  81%|████████  | 826/1024 [00:18<00:05, 39.07it/s, est. speed input: 44701.91 toks/s, output: 43.65 toks/s]
Processed prompts:  81%|████████▏ | 834/1024 [00:19<00:04, 39.08it/s, est. speed input: 44652.23 toks/s, output: 43.61 toks/s]
Processed prompts:  82%|████████▏ | 842/1024 [00:19<00:04, 39.04it/s, est. speed input: 44601.38 toks/s, output: 43.56 toks/s]
Processed prompts:  83%|████████▎ | 850/1024 [00:19<00:04, 38.88it/s, est. speed input: 44546.57 toks/s, output: 43.50 toks/s]
Processed prompts:  84%|████████▍ | 858/1024 [00:19<00:04, 38.83it/s, est. speed input: 44495.22 toks/s, output: 43.45 toks/s]
Processed prompts:  85%|████████▍ | 866/1024 [00:19<00:04, 38.93it/s, est. speed input: 44450.10 toks/s, output: 43.41 toks/s]
Processed prompts:  85%|████████▌ | 874/1024 [00:20<00:03, 38.89it/s, est. speed input: 44402.09 toks/s, output: 43.36 toks/s]
Processed prompts:  86%|████████▌ | 882/1024 [00:20<00:03, 38.91it/s, est. speed input: 44356.42 toks/s, output: 43.32 toks/s]
Processed prompts:  87%|████████▋ | 890/1024 [00:20<00:03, 38.90it/s, est. speed input: 44310.94 toks/s, output: 43.27 toks/s]
Processed prompts:  88%|████████▊ | 898/1024 [00:20<00:03, 38.81it/s, est. speed input: 44263.42 toks/s, output: 43.23 toks/s]
Processed prompts:  88%|████████▊ | 906/1024 [00:20<00:03, 38.82it/s, est. speed input: 44219.39 toks/s, output: 43.18 toks/s]
Processed prompts:  89%|████████▉ | 914/1024 [00:21<00:02, 38.88it/s, est. speed input: 44177.88 toks/s, output: 43.14 toks/s]
Processed prompts:  90%|█████████ | 922/1024 [00:21<00:02, 38.96it/s, est. speed input: 44139.03 toks/s, output: 43.10 toks/s]
Processed prompts:  91%|█████████ | 930/1024 [00:21<00:02, 38.92it/s, est. speed input: 44097.06 toks/s, output: 43.06 toks/s]
Processed prompts:  92%|█████████▏| 938/1024 [00:21<00:02, 38.82it/s, est. speed input: 44053.50 toks/s, output: 43.02 toks/s]
Processed prompts:  92%|█████████▏| 946/1024 [00:22<00:02, 38.89it/s, est. speed input: 44015.78 toks/s, output: 42.98 toks/s]
Processed prompts:  93%|█████████▎| 954/1024 [00:22<00:01, 38.94it/s, est. speed input: 43978.59 toks/s, output: 42.95 toks/s]
Processed prompts:  94%|█████████▍| 962/1024 [00:22<00:01, 38.88it/s, est. speed input: 43939.08 toks/s, output: 42.91 toks/s]
Processed prompts:  95%|█████████▍| 970/1024 [00:22<00:01, 38.91it/s, est. speed input: 43902.44 toks/s, output: 42.87 toks/s]
Processed prompts:  96%|█████████▌| 978/1024 [00:22<00:01, 38.83it/s, est. speed input: 43863.22 toks/s, output: 42.84 toks/s]
Processed prompts:  96%|█████████▋| 986/1024 [00:23<00:00, 38.86it/s, est. speed input: 43827.54 toks/s, output: 42.80 toks/s]
Processed prompts:  97%|█████████▋| 994/1024 [00:23<00:00, 38.89it/s, est. speed input: 43792.92 toks/s, output: 42.77 toks/s]
Processed prompts:  98%|█████████▊| 1002/1024 [00:23<00:00, 38.73it/s, est. speed input: 43752.77 toks/s, output: 42.73 toks/s]
Processed prompts:  99%|█████████▊| 1010/1024 [00:23<00:00, 38.69it/s, est. speed input: 43715.83 toks/s, output: 42.69 toks/s]
Processed prompts:  99%|█████████▉| 1018/1024 [00:23<00:00, 40.21it/s, est. speed input: 43727.98 toks/s, output: 42.70 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:23<00:00, 40.21it/s, est. speed input: 43985.00 toks/s, output: 42.95 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:23<00:00, 42.95it/s, est. speed input: 43985.00 toks/s, output: 42.95 toks/s]
[rank0]:[W126 11:47:38.320049946 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 70.7s

测试结果:
  Requests/s:   38.59
  Tokens/s:     39555.38
  Total Reqs:   1024
  Elapsed:      26.53s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     39516.79

============================================================
[6/8] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuBLASLt                                        │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 11:47:58 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1310903) WARNING 01-26 11:48:14 [backends.py:609] Failed to read file <frozen os>
Throughput: 38.79 requests/s, 39756.31 total tokens/s, 38.79 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-26 11:47:57] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:47:58] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 11:47:58] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 11:47:58] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:47:58] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:47:58] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:47:58] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:47:58] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:47:58] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 11:47:58] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:47:58] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:47:58] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:47:58] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:47:58] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 11:48:04] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:48:04] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 11:48:04] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 11:48:04] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:48:04] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:48:04] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:48:04] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:48:04] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:48:04] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 11:48:04] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:48:04] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:48:04] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:48:04] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:48:04] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1310903) [2026-01-26 11:48:06] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=1310903) [2026-01-26 11:48:06] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1310903) [2026-01-26 11:48:06] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=1310903) [2026-01-26 11:48:06] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=1310903) [2026-01-26 11:48:06] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=1310903) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1310903) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.57it/s]
(EngineCore_DP0 pid=1310903) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.92it/s]
(EngineCore_DP0 pid=1310903) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.99it/s]
(EngineCore_DP0 pid=1310903) 
(EngineCore_DP0 pid=1310903) [rank0]:W0126 11:48:19.872000 1310903 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1310903) [rank0]:W0126 11:48:19.951000 1310903 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1310903) [rank0]:W0126 11:48:21.299000 1310903 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1310903) [rank0]:W0126 11:48:21.426000 1310903 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1310903) 2026-01-26 11:48:25,858 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1310903) 2026-01-26 11:48:25,911 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1310903) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00, 15.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00, 17.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:00<00:00, 18.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 17.14it/s]
(EngineCore_DP0 pid=1310903) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00, 18.33it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00,  6.68it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00,  7.45it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   1%|          | 24/2048 [00:00<00:08, 239.77it/s]
Adding requests:   3%|▎         | 64/2048 [00:00<00:06, 329.57it/s]
Adding requests:   5%|▍         | 99/2048 [00:00<00:05, 338.61it/s]
Adding requests:   7%|▋         | 137/2048 [00:00<00:05, 352.86it/s]
Adding requests:   9%|▊         | 176/2048 [00:00<00:05, 364.73it/s]
Adding requests:  11%|█         | 216/2048 [00:00<00:04, 376.41it/s]
Adding requests:  12%|█▏        | 255/2048 [00:00<00:04, 377.43it/s]
Adding requests:  14%|█▍        | 294/2048 [00:00<00:04, 380.79it/s]
Adding requests:  16%|█▋        | 335/2048 [00:00<00:04, 389.14it/s]
Adding requests:  18%|█▊        | 375/2048 [00:01<00:04, 389.73it/s]
Adding requests:  20%|██        | 417/2048 [00:01<00:04, 396.57it/s]
Adding requests:  22%|██▏       | 457/2048 [00:01<00:04, 392.26it/s]
Adding requests:  24%|██▍       | 499/2048 [00:01<00:03, 399.75it/s]
Adding requests:  26%|██▋       | 541/2048 [00:01<00:03, 404.98it/s]
Adding requests:  28%|██▊       | 582/2048 [00:01<00:03, 400.10it/s]
Adding requests:  30%|███       | 623/2048 [00:01<00:03, 392.87it/s]
Adding requests:  32%|███▏      | 663/2048 [00:01<00:03, 384.53it/s]
Adding requests:  34%|███▍      | 703/2048 [00:01<00:03, 387.28it/s]
Adding requests:  36%|███▌      | 742/2048 [00:01<00:03, 382.12it/s]
Adding requests:  38%|███▊      | 781/2048 [00:02<00:03, 375.14it/s]
Adding requests:  40%|████      | 820/2048 [00:02<00:03, 376.18it/s]
Adding requests:  42%|████▏     | 861/2048 [00:02<00:03, 384.96it/s]
Adding requests:  44%|████▍     | 901/2048 [00:02<00:02, 386.73it/s]
Adding requests:  46%|████▌     | 940/2048 [00:02<00:02, 382.87it/s]
Adding requests:  48%|████▊     | 979/2048 [00:02<00:02, 382.07it/s]
Adding requests:  50%|████▉     | 1018/2048 [00:02<00:02, 377.96it/s]
Adding requests:  52%|█████▏    | 1056/2048 [00:02<00:02, 375.44it/s]
Adding requests:  53%|█████▎    | 1094/2048 [00:02<00:02, 374.60it/s]
Adding requests:  55%|█████▌    | 1135/2048 [00:02<00:02, 382.48it/s]
Adding requests:  57%|█████▋    | 1174/2048 [00:03<00:02, 379.21it/s]
Adding requests:  59%|█████▉    | 1213/2048 [00:03<00:02, 381.52it/s]
Adding requests:  61%|██████    | 1252/2048 [00:03<00:02, 383.94it/s]
Adding requests:  63%|██████▎   | 1291/2048 [00:03<00:02, 376.75it/s]
Adding requests:  65%|██████▍   | 1330/2048 [00:03<00:01, 380.59it/s]
Adding requests:  67%|██████▋   | 1369/2048 [00:03<00:01, 383.28it/s]
Adding requests:  69%|██████▉   | 1408/2048 [00:03<00:01, 381.58it/s]
Adding requests:  71%|███████   | 1447/2048 [00:03<00:01, 380.30it/s]
Adding requests:  73%|███████▎  | 1488/2048 [00:03<00:01, 385.84it/s]
Adding requests:  75%|███████▍  | 1527/2048 [00:04<00:01, 386.68it/s]
Adding requests:  76%|███████▋  | 1566/2048 [00:04<00:01, 381.09it/s]
Adding requests:  78%|███████▊  | 1605/2048 [00:04<00:01, 377.71it/s]
Adding requests:  80%|████████  | 1643/2048 [00:04<00:01, 370.34it/s]
Adding requests:  82%|████████▏ | 1681/2048 [00:04<00:00, 369.71it/s]
Adding requests:  84%|████████▍ | 1720/2048 [00:04<00:00, 374.26it/s]
Adding requests:  86%|████████▌ | 1759/2048 [00:04<00:00, 377.01it/s]
Adding requests:  88%|████████▊ | 1798/2048 [00:04<00:00, 378.41it/s]
Adding requests:  90%|████████▉ | 1836/2048 [00:04<00:00, 377.89it/s]
Adding requests:  92%|█████████▏| 1876/2048 [00:04<00:00, 382.09it/s]
Adding requests:  94%|█████████▎| 1915/2048 [00:05<00:00, 380.03it/s]
Adding requests:  95%|█████████▌| 1955/2048 [00:05<00:00, 385.49it/s]
Adding requests:  97%|█████████▋| 1994/2048 [00:05<00:00, 380.31it/s]
Adding requests:  99%|█████████▉| 2033/2048 [00:05<00:00, 373.20it/s]
Adding requests: 100%|██████████| 2048/2048 [00:05<00:00, 379.59it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   9%|▊         | 178/2048 [00:00<00:02, 867.10it/s, est. speed input: 888007.35 toks/s, output: 867.13 toks/s]
Processed prompts:  13%|█▎        | 265/2048 [00:02<00:18, 98.28it/s, est. speed input: 122528.93 toks/s, output: 119.66 toks/s] 
Processed prompts:  15%|█▍        | 304/2048 [00:03<00:21, 80.63it/s, est. speed input: 102799.36 toks/s, output: 100.39 toks/s]
Processed prompts:  16%|█▌        | 327/2048 [00:03<00:27, 63.73it/s, est. speed input: 87656.79 toks/s, output: 85.60 toks/s]  
Processed prompts:  17%|█▋        | 343/2048 [00:04<00:28, 58.97it/s, est. speed input: 83076.62 toks/s, output: 81.13 toks/s]
Processed prompts:  17%|█▋        | 355/2048 [00:04<00:32, 52.55it/s, est. speed input: 78419.98 toks/s, output: 76.58 toks/s]
Processed prompts:  18%|█▊        | 370/2048 [00:05<00:34, 48.83it/s, est. speed input: 75132.41 toks/s, output: 73.37 toks/s]
Processed prompts:  19%|█▉        | 386/2048 [00:05<00:35, 46.41it/s, est. speed input: 72521.01 toks/s, output: 70.82 toks/s]
Processed prompts:  20%|█▉        | 402/2048 [00:05<00:36, 44.55it/s, est. speed input: 70285.61 toks/s, output: 68.64 toks/s]
Processed prompts:  20%|██        | 418/2048 [00:06<00:37, 43.07it/s, est. speed input: 68321.11 toks/s, output: 66.72 toks/s]
Processed prompts:  21%|██        | 434/2048 [00:06<00:37, 42.78it/s, est. speed input: 66867.91 toks/s, output: 65.30 toks/s]
Processed prompts:  22%|██▏       | 450/2048 [00:07<00:38, 41.83it/s, est. speed input: 65351.03 toks/s, output: 63.82 toks/s]
Processed prompts:  23%|██▎       | 466/2048 [00:07<00:38, 41.04it/s, est. speed input: 63966.01 toks/s, output: 62.47 toks/s]
Processed prompts:  24%|██▎       | 482/2048 [00:07<00:38, 40.55it/s, est. speed input: 62745.16 toks/s, output: 61.27 toks/s]
Processed prompts:  24%|██▍       | 498/2048 [00:08<00:38, 40.15it/s, est. speed input: 61629.13 toks/s, output: 60.18 toks/s]
Processed prompts:  25%|██▌       | 514/2048 [00:08<00:38, 39.83it/s, est. speed input: 60611.46 toks/s, output: 59.19 toks/s]
Processed prompts:  26%|██▌       | 530/2048 [00:09<00:38, 39.63it/s, est. speed input: 59690.58 toks/s, output: 58.29 toks/s]
Processed prompts:  27%|██▋       | 546/2048 [00:09<00:38, 39.52it/s, est. speed input: 58853.16 toks/s, output: 57.47 toks/s]
Processed prompts:  27%|██▋       | 562/2048 [00:09<00:37, 39.43it/s, est. speed input: 58083.31 toks/s, output: 56.72 toks/s]
Processed prompts:  28%|██▊       | 578/2048 [00:10<00:37, 39.34it/s, est. speed input: 57369.25 toks/s, output: 56.02 toks/s]
Processed prompts:  29%|██▉       | 594/2048 [00:10<00:37, 39.22it/s, est. speed input: 56700.46 toks/s, output: 55.37 toks/s]
Processed prompts:  30%|██▉       | 610/2048 [00:11<00:36, 39.15it/s, est. speed input: 56081.17 toks/s, output: 54.77 toks/s]
Processed prompts:  31%|███       | 626/2048 [00:11<00:36, 39.19it/s, est. speed input: 55521.65 toks/s, output: 54.22 toks/s]
Processed prompts:  31%|███▏      | 642/2048 [00:11<00:35, 39.22it/s, est. speed input: 55001.29 toks/s, output: 53.71 toks/s]
Processed prompts:  32%|███▏      | 658/2048 [00:12<00:35, 39.07it/s, est. speed input: 54488.04 toks/s, output: 53.21 toks/s]
Processed prompts:  33%|███▎      | 674/2048 [00:12<00:35, 39.14it/s, est. speed input: 54034.46 toks/s, output: 52.77 toks/s]
Processed prompts:  34%|███▎      | 690/2048 [00:13<00:34, 39.17it/s, est. speed input: 53606.45 toks/s, output: 52.35 toks/s]
Processed prompts:  34%|███▍      | 706/2048 [00:13<00:34, 39.04it/s, est. speed input: 53182.74 toks/s, output: 51.94 toks/s]
Processed prompts:  35%|███▌      | 722/2048 [00:14<00:33, 39.10it/s, est. speed input: 52804.68 toks/s, output: 51.57 toks/s]
Processed prompts:  36%|███▌      | 738/2048 [00:14<00:33, 39.15it/s, est. speed input: 52448.01 toks/s, output: 51.22 toks/s]
Processed prompts:  37%|███▋      | 754/2048 [00:14<00:33, 39.15it/s, est. speed input: 52107.81 toks/s, output: 50.89 toks/s]
Processed prompts:  38%|███▊      | 770/2048 [00:15<00:32, 39.18it/s, est. speed input: 51788.84 toks/s, output: 50.57 toks/s]
Processed prompts:  38%|███▊      | 786/2048 [00:15<00:31, 39.84it/s, est. speed input: 51557.92 toks/s, output: 50.35 toks/s]
Processed prompts:  39%|███▉      | 802/2048 [00:16<00:31, 39.68it/s, est. speed input: 51271.35 toks/s, output: 50.07 toks/s]
Processed prompts:  40%|███▉      | 818/2048 [00:16<00:31, 39.49it/s, est. speed input: 50989.77 toks/s, output: 49.79 toks/s]
Processed prompts:  41%|████      | 834/2048 [00:16<00:30, 39.38it/s, est. speed input: 50724.76 toks/s, output: 49.54 toks/s]
Processed prompts:  42%|████▏     | 850/2048 [00:17<00:30, 39.33it/s, est. speed input: 50474.03 toks/s, output: 49.29 toks/s]
Processed prompts:  42%|████▏     | 866/2048 [00:17<00:30, 39.26it/s, est. speed input: 50232.04 toks/s, output: 49.05 toks/s]
Processed prompts:  43%|████▎     | 882/2048 [00:18<00:29, 39.20it/s, est. speed input: 50000.13 toks/s, output: 48.83 toks/s]
Processed prompts:  44%|████▍     | 898/2048 [00:18<00:29, 39.15it/s, est. speed input: 49777.53 toks/s, output: 48.61 toks/s]
Processed prompts:  45%|████▍     | 914/2048 [00:18<00:29, 39.05it/s, est. speed input: 49558.34 toks/s, output: 48.40 toks/s]
Processed prompts:  45%|████▌     | 930/2048 [00:19<00:28, 39.08it/s, est. speed input: 49357.91 toks/s, output: 48.20 toks/s]
Processed prompts:  46%|████▌     | 946/2048 [00:19<00:28, 39.10it/s, est. speed input: 49165.22 toks/s, output: 48.01 toks/s]
Processed prompts:  47%|████▋     | 962/2048 [00:20<00:27, 39.06it/s, est. speed input: 48976.26 toks/s, output: 47.83 toks/s]
Processed prompts:  48%|████▊     | 978/2048 [00:20<00:27, 39.00it/s, est. speed input: 48792.50 toks/s, output: 47.65 toks/s]
Processed prompts:  49%|████▊     | 994/2048 [00:20<00:26, 39.04it/s, est. speed input: 48621.86 toks/s, output: 47.48 toks/s]
Processed prompts:  49%|████▉     | 1010/2048 [00:21<00:26, 39.01it/s, est. speed input: 48453.80 toks/s, output: 47.32 toks/s]
Processed prompts:  50%|█████     | 1026/2048 [00:21<00:26, 39.01it/s, est. speed input: 48293.56 toks/s, output: 47.16 toks/s]
Processed prompts:  51%|█████     | 1042/2048 [00:22<00:25, 39.02it/s, est. speed input: 48139.50 toks/s, output: 47.01 toks/s]
Processed prompts:  52%|█████▏    | 1058/2048 [00:22<00:25, 39.02it/s, est. speed input: 47990.76 toks/s, output: 46.87 toks/s]
Processed prompts:  52%|█████▏    | 1074/2048 [00:22<00:24, 38.99it/s, est. speed input: 47845.10 toks/s, output: 46.72 toks/s]
Processed prompts:  53%|█████▎    | 1090/2048 [00:23<00:24, 38.97it/s, est. speed input: 47704.77 toks/s, output: 46.59 toks/s]
Processed prompts:  54%|█████▍    | 1106/2048 [00:23<00:24, 39.00it/s, est. speed input: 47572.64 toks/s, output: 46.46 toks/s]
Processed prompts:  55%|█████▍    | 1122/2048 [00:24<00:23, 39.06it/s, est. speed input: 47447.17 toks/s, output: 46.34 toks/s]
Processed prompts:  56%|█████▌    | 1138/2048 [00:24<00:23, 39.03it/s, est. speed input: 47321.37 toks/s, output: 46.21 toks/s]
Processed prompts:  56%|█████▋    | 1154/2048 [00:25<00:22, 39.06it/s, est. speed input: 47203.15 toks/s, output: 46.10 toks/s]
Processed prompts:  57%|█████▋    | 1170/2048 [00:25<00:22, 38.96it/s, est. speed input: 47080.32 toks/s, output: 45.98 toks/s]
Processed prompts:  58%|█████▊    | 1186/2048 [00:25<00:22, 39.02it/s, est. speed input: 46969.94 toks/s, output: 45.87 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [00:26<00:21, 39.67it/s, est. speed input: 46900.33 toks/s, output: 45.80 toks/s]
Processed prompts:  59%|█████▉    | 1218/2048 [00:26<00:21, 39.47it/s, est. speed input: 46793.76 toks/s, output: 45.70 toks/s]
Processed prompts:  60%|██████    | 1234/2048 [00:27<00:20, 39.94it/s, est. speed input: 46725.68 toks/s, output: 45.63 toks/s]
Processed prompts:  61%|██████    | 1250/2048 [00:27<00:20, 39.62it/s, est. speed input: 46622.22 toks/s, output: 45.53 toks/s]
Processed prompts:  62%|██████▏   | 1266/2048 [00:27<00:19, 39.40it/s, est. speed input: 46521.71 toks/s, output: 45.43 toks/s]
Processed prompts:  63%|██████▎   | 1282/2048 [00:28<00:19, 39.29it/s, est. speed input: 46426.66 toks/s, output: 45.34 toks/s]
Processed prompts:  63%|██████▎   | 1298/2048 [00:28<00:19, 39.18it/s, est. speed input: 46332.68 toks/s, output: 45.25 toks/s]
Processed prompts:  64%|██████▍   | 1314/2048 [00:29<00:18, 39.15it/s, est. speed input: 46244.11 toks/s, output: 45.16 toks/s]
Processed prompts:  65%|██████▍   | 1330/2048 [00:29<00:18, 39.74it/s, est. speed input: 46190.13 toks/s, output: 45.11 toks/s]
Processed prompts:  66%|██████▌   | 1346/2048 [00:29<00:17, 39.49it/s, est. speed input: 46103.04 toks/s, output: 45.02 toks/s]
Processed prompts:  67%|██████▋   | 1362/2048 [00:30<00:17, 39.29it/s, est. speed input: 46017.11 toks/s, output: 44.94 toks/s]
Processed prompts:  67%|██████▋   | 1378/2048 [00:30<00:17, 39.16it/s, est. speed input: 45933.64 toks/s, output: 44.86 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [00:31<00:16, 39.10it/s, est. speed input: 45853.73 toks/s, output: 44.78 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [00:31<00:16, 39.06it/s, est. speed input: 45776.62 toks/s, output: 44.70 toks/s]
Processed prompts:  70%|██████▉   | 1426/2048 [00:31<00:15, 39.01it/s, est. speed input: 45699.77 toks/s, output: 44.63 toks/s]
Processed prompts:  70%|███████   | 1442/2048 [00:32<00:15, 38.99it/s, est. speed input: 45626.02 toks/s, output: 44.56 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [00:32<00:14, 39.62it/s, est. speed input: 45584.78 toks/s, output: 44.52 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [00:33<00:14, 39.41it/s, est. speed input: 45513.95 toks/s, output: 44.45 toks/s]
Processed prompts:  73%|███████▎  | 1490/2048 [00:33<00:14, 39.28it/s, est. speed input: 45445.47 toks/s, output: 44.38 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [00:33<00:13, 39.18it/s, est. speed input: 45378.10 toks/s, output: 44.31 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [00:34<00:13, 39.80it/s, est. speed input: 45343.58 toks/s, output: 44.28 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [00:34<00:12, 39.57it/s, est. speed input: 45280.33 toks/s, output: 44.22 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [00:35<00:12, 40.02it/s, est. speed input: 45245.30 toks/s, output: 44.18 toks/s]
Processed prompts:  77%|███████▋  | 1570/2048 [00:35<00:12, 39.70it/s, est. speed input: 45183.78 toks/s, output: 44.12 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [00:35<00:11, 39.41it/s, est. speed input: 45120.39 toks/s, output: 44.06 toks/s]
Processed prompts:  78%|███████▊  | 1602/2048 [00:36<00:11, 39.14it/s, est. speed input: 45055.74 toks/s, output: 44.00 toks/s]
Processed prompts:  79%|███████▉  | 1618/2048 [00:36<00:10, 39.70it/s, est. speed input: 45024.06 toks/s, output: 43.97 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [00:37<00:10, 39.42it/s, est. speed input: 44965.27 toks/s, output: 43.91 toks/s]
Processed prompts:  81%|████████  | 1650/2048 [00:37<00:10, 39.29it/s, est. speed input: 44910.19 toks/s, output: 43.86 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [00:38<00:09, 39.15it/s, est. speed input: 44854.47 toks/s, output: 43.80 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [00:38<00:09, 39.03it/s, est. speed input: 44798.99 toks/s, output: 43.75 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [00:38<00:08, 39.01it/s, est. speed input: 44747.24 toks/s, output: 43.70 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [00:39<00:08, 38.91it/s, est. speed input: 44693.04 toks/s, output: 43.65 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [00:39<00:08, 39.60it/s, est. speed input: 44669.66 toks/s, output: 43.62 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [00:40<00:07, 40.11it/s, est. speed input: 44647.12 toks/s, output: 43.60 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [00:40<00:07, 39.76it/s, est. speed input: 44599.06 toks/s, output: 43.55 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [00:40<00:06, 39.39it/s, est. speed input: 44547.15 toks/s, output: 43.50 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [00:41<00:06, 39.21it/s, est. speed input: 44499.02 toks/s, output: 43.46 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [00:41<00:06, 39.17it/s, est. speed input: 44454.73 toks/s, output: 43.41 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [00:42<00:05, 38.92it/s, est. speed input: 44403.66 toks/s, output: 43.36 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [00:42<00:05, 38.85it/s, est. speed input: 44356.92 toks/s, output: 43.32 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [00:42<00:04, 38.83it/s, est. speed input: 44312.44 toks/s, output: 43.27 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [00:43<00:04, 38.78it/s, est. speed input: 44267.25 toks/s, output: 43.23 toks/s]
Processed prompts:  92%|█████████▏| 1890/2048 [00:43<00:04, 39.42it/s, est. speed input: 44246.87 toks/s, output: 43.21 toks/s]
Processed prompts:  93%|█████████▎| 1906/2048 [00:44<00:03, 39.21it/s, est. speed input: 44203.83 toks/s, output: 43.17 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [00:44<00:03, 39.03it/s, est. speed input: 44160.74 toks/s, output: 43.13 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [00:44<00:02, 38.90it/s, est. speed input: 44117.81 toks/s, output: 43.08 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [00:45<00:02, 38.89it/s, est. speed input: 44078.58 toks/s, output: 43.05 toks/s]
Processed prompts:  96%|█████████▌| 1970/2048 [00:45<00:02, 38.84it/s, est. speed input: 44038.69 toks/s, output: 43.01 toks/s]
Processed prompts:  97%|█████████▋| 1986/2048 [00:46<00:01, 39.51it/s, est. speed input: 44022.88 toks/s, output: 42.99 toks/s]
Processed prompts:  98%|█████████▊| 2002/2048 [00:46<00:01, 39.27it/s, est. speed input: 43984.04 toks/s, output: 42.95 toks/s]
Processed prompts:  99%|█████████▊| 2018/2048 [00:47<00:00, 39.15it/s, est. speed input: 43947.68 toks/s, output: 42.92 toks/s]
Processed prompts:  99%|█████████▉| 2034/2048 [00:47<00:00, 39.90it/s, est. speed input: 43938.20 toks/s, output: 42.91 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:47<00:00, 39.90it/s, est. speed input: 44240.00 toks/s, output: 43.20 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:47<00:00, 43.20it/s, est. speed input: 44240.00 toks/s, output: 43.20 toks/s]
[rank0]:[W126 11:49:21.671691334 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 103.5s

测试结果:
  Requests/s:   38.79
  Tokens/s:     39756.31
  Total Reqs:   2048
  Elapsed:      52.80s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     39717.52

============================================================
[7/8] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuBLASLt                                        │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 11:49:51 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1312901) WARNING 01-26 11:50:07 [backends.py:609] Failed to read file <frozen os>
Throughput: 39.67 requests/s, 40658.46 total tokens/s, 39.67 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-26 11:49:51] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:49:51] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 11:49:51] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 11:49:51] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:49:51] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:49:51] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:49:51] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:49:51] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:49:51] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 11:49:51] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:49:51] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:49:51] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:49:51] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:49:51] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 11:49:58] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:49:59] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 11:49:59] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 11:49:59] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:49:59] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:49:59] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:49:59] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:49:59] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:49:59] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 11:49:59] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:49:59] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:49:59] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:49:59] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:49:59] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1312901) [2026-01-26 11:50:00] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=1312901) [2026-01-26 11:50:00] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1312901) [2026-01-26 11:50:00] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=1312901) [2026-01-26 11:50:00] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=1312901) [2026-01-26 11:50:00] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=1312901) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1312901) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.62it/s]
(EngineCore_DP0 pid=1312901) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.91it/s]
(EngineCore_DP0 pid=1312901) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.99it/s]
(EngineCore_DP0 pid=1312901) 
(EngineCore_DP0 pid=1312901) [rank0]:W0126 11:50:12.975000 1312901 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1312901) [rank0]:W0126 11:50:13.053000 1312901 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1312901) [rank0]:W0126 11:50:14.067000 1312901 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1312901) [rank0]:W0126 11:50:14.195000 1312901 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1312901) 2026-01-26 11:50:18,543 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1312901) 2026-01-26 11:50:18,571 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1312901) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▉         | 1/11 [00:00<00:04,  2.14it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:01<00:05,  1.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:01<00:01,  3.87it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 6/11 [00:01<00:00,  6.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:01<00:00,  8.55it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 10/11 [00:01<00:00, 10.67it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00,  6.71it/s]
(EngineCore_DP0 pid=1312901) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00, 10.12it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00, 14.17it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00, 16.29it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 12.77it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 23/4096 [00:00<00:17, 228.67it/s]
Adding requests:   2%|▏         | 64/4096 [00:00<00:12, 330.10it/s]
Adding requests:   2%|▏         | 100/4096 [00:00<00:11, 341.88it/s]
Adding requests:   3%|▎         | 138/4096 [00:00<00:11, 353.99it/s]
Adding requests:   4%|▍         | 178/4096 [00:00<00:10, 367.17it/s]
Adding requests:   5%|▌         | 220/4096 [00:00<00:10, 382.60it/s]
Adding requests:   6%|▋         | 259/4096 [00:00<00:10, 378.91it/s]
Adding requests:   7%|▋         | 299/4096 [00:00<00:09, 383.89it/s]
Adding requests:   8%|▊         | 339/4096 [00:00<00:09, 388.31it/s]
Adding requests:   9%|▉         | 380/4096 [00:01<00:09, 391.40it/s]
Adding requests:  10%|█         | 422/4096 [00:01<00:09, 398.36it/s]
Adding requests:  11%|█▏        | 462/4096 [00:01<00:09, 394.25it/s]
Adding requests:  12%|█▏        | 504/4096 [00:01<00:08, 401.58it/s]
Adding requests:  13%|█▎        | 546/4096 [00:01<00:08, 406.09it/s]
Adding requests:  14%|█▍        | 587/4096 [00:01<00:08, 401.47it/s]
Adding requests:  15%|█▌        | 628/4096 [00:01<00:08, 396.76it/s]
Adding requests:  16%|█▋        | 668/4096 [00:01<00:08, 386.28it/s]
Adding requests:  17%|█▋        | 708/4096 [00:01<00:08, 390.00it/s]
Adding requests:  18%|█▊        | 748/4096 [00:01<00:08, 383.08it/s]
Adding requests:  19%|█▉        | 787/4096 [00:02<00:08, 384.77it/s]
Adding requests:  20%|██        | 826/4096 [00:02<00:08, 385.73it/s]
Adding requests:  21%|██        | 866/4096 [00:02<00:08, 388.66it/s]
Adding requests:  22%|██▏       | 907/4096 [00:02<00:08, 391.78it/s]
Adding requests:  23%|██▎       | 947/4096 [00:02<00:08, 377.92it/s]
Adding requests:  24%|██▍       | 986/4096 [00:02<00:08, 381.25it/s]
Adding requests:  25%|██▌       | 1025/4096 [00:02<00:08, 372.25it/s]
Adding requests:  26%|██▌       | 1063/4096 [00:02<00:08, 373.41it/s]
Adding requests:  27%|██▋       | 1101/4096 [00:02<00:08, 366.22it/s]
Adding requests:  28%|██▊       | 1141/4096 [00:02<00:07, 373.46it/s]
Adding requests:  29%|██▉       | 1179/4096 [00:03<00:07, 374.69it/s]
Adding requests:  30%|██▉       | 1220/4096 [00:03<00:07, 382.35it/s]
Adding requests:  31%|███       | 1259/4096 [00:03<00:07, 381.16it/s]
Adding requests:  32%|███▏      | 1298/4096 [00:03<00:07, 378.93it/s]
Adding requests:  33%|███▎      | 1337/4096 [00:03<00:07, 380.84it/s]
Adding requests:  34%|███▎      | 1378/4096 [00:03<00:07, 386.73it/s]
Adding requests:  35%|███▍      | 1417/4096 [00:03<00:07, 382.52it/s]
Adding requests:  36%|███▌      | 1457/4096 [00:03<00:06, 387.11it/s]
Adding requests:  37%|███▋      | 1497/4096 [00:03<00:06, 390.81it/s]
Adding requests:  38%|███▊      | 1537/4096 [00:04<00:06, 388.01it/s]
Adding requests:  38%|███▊      | 1576/4096 [00:04<00:06, 383.40it/s]
Adding requests:  39%|███▉      | 1615/4096 [00:04<00:06, 381.09it/s]
Adding requests:  40%|████      | 1654/4096 [00:04<00:06, 373.39it/s]
Adding requests:  41%|████▏     | 1692/4096 [00:04<00:06, 374.43it/s]
Adding requests:  42%|████▏     | 1732/4096 [00:04<00:06, 379.02it/s]
Adding requests:  43%|████▎     | 1773/4096 [00:04<00:06, 385.81it/s]
Adding requests:  44%|████▍     | 1812/4096 [00:04<00:05, 383.21it/s]
Adding requests:  45%|████▌     | 1851/4096 [00:04<00:05, 382.90it/s]
Adding requests:  46%|████▌     | 1891/4096 [00:04<00:05, 387.92it/s]
Adding requests:  47%|████▋     | 1931/4096 [00:05<00:05, 391.07it/s]
Adding requests:  48%|████▊     | 1971/4096 [00:05<00:05, 391.39it/s]
Adding requests:  49%|████▉     | 2011/4096 [00:05<00:05, 388.51it/s]
Adding requests:  50%|█████     | 2050/4096 [00:05<00:05, 382.96it/s]
Adding requests:  51%|█████     | 2089/4096 [00:05<00:05, 371.84it/s]
Adding requests:  52%|█████▏    | 2129/4096 [00:05<00:05, 379.68it/s]
Adding requests:  53%|█████▎    | 2168/4096 [00:05<00:05, 376.38it/s]
Adding requests:  54%|█████▍    | 2206/4096 [00:05<00:05, 371.66it/s]
Adding requests:  55%|█████▍    | 2245/4096 [00:05<00:04, 376.73it/s]
Adding requests:  56%|█████▌    | 2286/4096 [00:05<00:04, 383.91it/s]
Adding requests:  57%|█████▋    | 2325/4096 [00:06<00:04, 377.51it/s]
Adding requests:  58%|█████▊    | 2363/4096 [00:06<00:04, 377.57it/s]
Adding requests:  59%|█████▊    | 2404/4096 [00:06<00:04, 385.61it/s]
Adding requests:  60%|█████▉    | 2444/4096 [00:06<00:04, 389.21it/s]
Adding requests:  61%|██████    | 2483/4096 [00:06<00:04, 389.27it/s]
Adding requests:  62%|██████▏   | 2524/4096 [00:06<00:03, 393.76it/s]
Adding requests:  63%|██████▎   | 2568/4096 [00:06<00:03, 405.64it/s]
Adding requests:  64%|██████▎   | 2609/4096 [00:06<00:03, 403.51it/s]
Adding requests:  65%|██████▍   | 2650/4096 [00:06<00:03, 392.73it/s]
Adding requests:  66%|██████▌   | 2690/4096 [00:07<00:03, 388.97it/s]
Adding requests:  67%|██████▋   | 2729/4096 [00:07<00:03, 385.18it/s]
Adding requests:  68%|██████▊   | 2770/4096 [00:07<00:03, 392.24it/s]
Adding requests:  69%|██████▊   | 2812/4096 [00:07<00:03, 396.69it/s]
Adding requests:  70%|██████▉   | 2853/4096 [00:07<00:03, 397.98it/s]
Adding requests:  71%|███████   | 2893/4096 [00:07<00:03, 396.05it/s]
Adding requests:  72%|███████▏  | 2933/4096 [00:07<00:02, 396.08it/s]
Adding requests:  73%|███████▎  | 2973/4096 [00:07<00:02, 396.72it/s]
Adding requests:  74%|███████▎  | 3015/4096 [00:07<00:02, 400.49it/s]
Adding requests:  75%|███████▍  | 3056/4096 [00:07<00:02, 401.90it/s]
Adding requests:  76%|███████▌  | 3097/4096 [00:08<00:02, 402.52it/s]
Adding requests:  77%|███████▋  | 3138/4096 [00:08<00:02, 404.48it/s]
Adding requests:  78%|███████▊  | 3179/4096 [00:08<00:02, 395.11it/s]
Adding requests:  79%|███████▊  | 3219/4096 [00:08<00:02, 393.35it/s]
Adding requests:  80%|███████▉  | 3259/4096 [00:08<00:02, 392.69it/s]
Adding requests:  81%|████████  | 3299/4096 [00:08<00:02, 377.43it/s]
Adding requests:  81%|████████▏ | 3337/4096 [00:08<00:02, 376.67it/s]
Adding requests:  82%|████████▏ | 3377/4096 [00:08<00:01, 382.23it/s]
Adding requests:  83%|████████▎ | 3416/4096 [00:08<00:01, 381.86it/s]
Adding requests:  84%|████████▍ | 3455/4096 [00:08<00:01, 382.63it/s]
Adding requests:  85%|████████▌ | 3494/4096 [00:09<00:01, 380.30it/s]
Adding requests:  86%|████████▋ | 3536/4096 [00:09<00:01, 391.44it/s]
Adding requests:  87%|████████▋ | 3576/4096 [00:09<00:01, 390.18it/s]
Adding requests:  88%|████████▊ | 3616/4096 [00:09<00:01, 389.81it/s]
Adding requests:  89%|████████▉ | 3656/4096 [00:09<00:01, 386.76it/s]
Adding requests:  90%|█████████ | 3695/4096 [00:09<00:01, 368.87it/s]
Adding requests:  91%|█████████ | 3735/4096 [00:09<00:00, 374.23it/s]
Adding requests:  92%|█████████▏| 3773/4096 [00:09<00:00, 368.01it/s]
Adding requests:  93%|█████████▎| 3810/4096 [00:09<00:00, 358.37it/s]
Adding requests:  94%|█████████▍| 3849/4096 [00:10<00:00, 365.69it/s]
Adding requests:  95%|█████████▍| 3888/4096 [00:10<00:00, 371.21it/s]
Adding requests:  96%|█████████▌| 3926/4096 [00:10<00:00, 368.55it/s]
Adding requests:  97%|█████████▋| 3964/4096 [00:10<00:00, 371.72it/s]
Adding requests:  98%|█████████▊| 4002/4096 [00:10<00:00, 370.81it/s]
Adding requests:  99%|█████████▊| 4041/4096 [00:10<00:00, 374.26it/s]
Adding requests: 100%|█████████▉| 4079/4096 [00:10<00:00, 373.99it/s]
Adding requests: 100%|██████████| 4096/4096 [00:10<00:00, 383.13it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  10%|▉         | 409/4096 [00:00<00:03, 1008.68it/s, est. speed input: 1033007.48 toks/s, output: 1008.71 toks/s]
Processed prompts:  12%|█▏        | 510/4096 [00:02<00:24, 143.75it/s, est. speed input: 185464.08 toks/s, output: 181.12 toks/s]   
Processed prompts:  14%|█▎        | 554/4096 [00:03<00:30, 117.14it/s, est. speed input: 156717.21 toks/s, output: 153.04 toks/s]
Processed prompts:  14%|█▍        | 581/4096 [00:04<00:38, 92.11it/s, est. speed input: 134457.14 toks/s, output: 131.31 toks/s] 
Processed prompts:  15%|█▍        | 601/4096 [00:05<00:48, 71.92it/s, est. speed input: 117651.64 toks/s, output: 114.89 toks/s]
Processed prompts:  15%|█▌        | 633/4096 [00:06<00:55, 62.37it/s, est. speed input: 107459.86 toks/s, output: 104.94 toks/s]
Processed prompts:  16%|█▌        | 665/4096 [00:06<01:01, 55.68it/s, est. speed input: 99668.00 toks/s, output: 97.33 toks/s]  
Processed prompts:  17%|█▋        | 697/4096 [00:07<01:06, 50.95it/s, est. speed input: 93486.52 toks/s, output: 91.29 toks/s]
Processed prompts:  18%|█▊        | 729/4096 [00:08<01:10, 47.62it/s, est. speed input: 88471.28 toks/s, output: 86.40 toks/s]
Processed prompts:  19%|█▊        | 761/4096 [00:09<01:13, 45.63it/s, est. speed input: 84514.34 toks/s, output: 82.53 toks/s]
Processed prompts:  19%|█▉        | 793/4096 [00:10<01:15, 43.88it/s, est. speed input: 81009.70 toks/s, output: 79.11 toks/s]
Processed prompts:  20%|██        | 825/4096 [00:10<01:16, 42.65it/s, est. speed input: 78018.71 toks/s, output: 76.19 toks/s]
Processed prompts:  21%|██        | 857/4096 [00:11<01:17, 41.78it/s, est. speed input: 75439.08 toks/s, output: 73.67 toks/s]
Processed prompts:  22%|██▏       | 889/4096 [00:12<01:17, 41.16it/s, est. speed input: 73187.03 toks/s, output: 71.47 toks/s]
Processed prompts:  22%|██▏       | 921/4096 [00:13<01:18, 40.70it/s, est. speed input: 71199.25 toks/s, output: 69.53 toks/s]
Processed prompts:  23%|██▎       | 953/4096 [00:14<01:17, 40.41it/s, est. speed input: 69450.77 toks/s, output: 67.82 toks/s]
Processed prompts:  24%|██▍       | 985/4096 [00:14<01:17, 40.19it/s, est. speed input: 67887.78 toks/s, output: 66.30 toks/s]
Processed prompts:  25%|██▍       | 1017/4096 [00:15<01:16, 40.03it/s, est. speed input: 66480.26 toks/s, output: 64.92 toks/s]
Processed prompts:  26%|██▌       | 1049/4096 [00:16<01:16, 39.91it/s, est. speed input: 65211.44 toks/s, output: 63.68 toks/s]
Processed prompts:  26%|██▋       | 1081/4096 [00:17<01:15, 39.86it/s, est. speed input: 64068.15 toks/s, output: 62.57 toks/s]
Processed prompts:  27%|██▋       | 1113/4096 [00:18<01:14, 39.79it/s, est. speed input: 63020.83 toks/s, output: 61.54 toks/s]
Processed prompts:  28%|██▊       | 1145/4096 [00:18<01:14, 39.75it/s, est. speed input: 62062.77 toks/s, output: 60.61 toks/s]
Processed prompts:  29%|██▊       | 1177/4096 [00:19<01:13, 39.70it/s, est. speed input: 61179.20 toks/s, output: 59.75 toks/s]
Processed prompts:  30%|██▉       | 1209/4096 [00:20<01:11, 40.39it/s, est. speed input: 60507.76 toks/s, output: 59.09 toks/s]
Processed prompts:  30%|███       | 1241/4096 [00:21<01:11, 40.11it/s, est. speed input: 59741.89 toks/s, output: 58.34 toks/s]
Processed prompts:  31%|███       | 1273/4096 [00:22<01:10, 40.02it/s, est. speed input: 59051.76 toks/s, output: 57.67 toks/s]
Processed prompts:  32%|███▏      | 1305/4096 [00:22<01:09, 39.89it/s, est. speed input: 58397.73 toks/s, output: 57.03 toks/s]
Processed prompts:  33%|███▎      | 1337/4096 [00:23<01:08, 40.11it/s, est. speed input: 57839.58 toks/s, output: 56.48 toks/s]
Processed prompts:  33%|███▎      | 1369/4096 [00:24<01:08, 39.97it/s, est. speed input: 57270.02 toks/s, output: 55.93 toks/s]
Processed prompts:  34%|███▍      | 1401/4096 [00:25<01:07, 39.84it/s, est. speed input: 56733.43 toks/s, output: 55.40 toks/s]
Processed prompts:  35%|███▍      | 1433/4096 [00:26<01:06, 40.14it/s, est. speed input: 56285.54 toks/s, output: 54.97 toks/s]
Processed prompts:  36%|███▌      | 1465/4096 [00:26<01:05, 40.05it/s, est. speed input: 55822.26 toks/s, output: 54.51 toks/s]
Processed prompts:  37%|███▋      | 1497/4096 [00:27<01:05, 39.94it/s, est. speed input: 55380.48 toks/s, output: 54.08 toks/s]
Processed prompts:  37%|███▋      | 1529/4096 [00:28<01:03, 40.19it/s, est. speed input: 55004.81 toks/s, output: 53.72 toks/s]
Processed prompts:  38%|███▊      | 1561/4096 [00:29<01:02, 40.35it/s, est. speed input: 54648.04 toks/s, output: 53.37 toks/s]
Processed prompts:  39%|███▉      | 1593/4096 [00:30<01:02, 40.12it/s, est. speed input: 54268.33 toks/s, output: 53.00 toks/s]
Processed prompts:  40%|███▉      | 1625/4096 [00:30<01:01, 40.33it/s, est. speed input: 53951.72 toks/s, output: 52.69 toks/s]
Processed prompts:  40%|████      | 1657/4096 [00:31<01:00, 40.05it/s, est. speed input: 53603.09 toks/s, output: 52.35 toks/s]
Processed prompts:  41%|████      | 1689/4096 [00:32<01:00, 39.92it/s, est. speed input: 53278.55 toks/s, output: 52.03 toks/s]
Processed prompts:  42%|████▏     | 1721/4096 [00:33<00:59, 40.11it/s, est. speed input: 53000.46 toks/s, output: 51.76 toks/s]
Processed prompts:  43%|████▎     | 1753/4096 [00:34<00:58, 40.26it/s, est. speed input: 52735.90 toks/s, output: 51.50 toks/s]
Processed prompts:  44%|████▎     | 1785/4096 [00:34<00:57, 39.98it/s, est. speed input: 52445.50 toks/s, output: 51.22 toks/s]
Processed prompts:  44%|████▍     | 1817/4096 [00:35<00:57, 39.87it/s, est. speed input: 52176.76 toks/s, output: 50.95 toks/s]
Processed prompts:  45%|████▌     | 1849/4096 [00:36<00:56, 39.73it/s, est. speed input: 51913.48 toks/s, output: 50.70 toks/s]
Processed prompts:  46%|████▌     | 1881/4096 [00:37<00:55, 40.02it/s, est. speed input: 51697.25 toks/s, output: 50.49 toks/s]
Processed prompts:  47%|████▋     | 1913/4096 [00:38<00:54, 39.82it/s, est. speed input: 51454.13 toks/s, output: 50.25 toks/s]
Processed prompts:  47%|████▋     | 1945/4096 [00:38<00:54, 39.74it/s, est. speed input: 51226.58 toks/s, output: 50.03 toks/s]
Processed prompts:  48%|████▊     | 1977/4096 [00:39<00:53, 39.97it/s, est. speed input: 51032.43 toks/s, output: 49.84 toks/s]
Processed prompts:  49%|████▉     | 2009/4096 [00:40<00:52, 39.82it/s, est. speed input: 50819.74 toks/s, output: 49.63 toks/s]
Processed prompts:  50%|████▉     | 2041/4096 [00:41<00:51, 40.02it/s, est. speed input: 50641.05 toks/s, output: 49.45 toks/s]
Processed prompts:  51%|█████     | 2073/4096 [00:42<00:50, 39.84it/s, est. speed input: 50442.58 toks/s, output: 49.26 toks/s]
Processed prompts:  51%|█████▏    | 2105/4096 [00:42<00:50, 39.71it/s, est. speed input: 50252.03 toks/s, output: 49.07 toks/s]
Processed prompts:  52%|█████▏    | 2137/4096 [00:43<00:49, 39.65it/s, est. speed input: 50070.78 toks/s, output: 48.90 toks/s]
Processed prompts:  53%|█████▎    | 2169/4096 [00:44<00:48, 39.90it/s, est. speed input: 49918.08 toks/s, output: 48.75 toks/s]
Processed prompts:  54%|█████▎    | 2201/4096 [00:45<00:47, 39.75it/s, est. speed input: 49746.08 toks/s, output: 48.58 toks/s]
Processed prompts:  55%|█████▍    | 2233/4096 [00:46<00:46, 39.65it/s, est. speed input: 49580.90 toks/s, output: 48.42 toks/s]
Processed prompts:  55%|█████▌    | 2265/4096 [00:46<00:46, 39.55it/s, est. speed input: 49419.29 toks/s, output: 48.26 toks/s]
Processed prompts:  56%|█████▌    | 2297/4096 [00:47<00:45, 39.50it/s, est. speed input: 49264.93 toks/s, output: 48.11 toks/s]
Processed prompts:  57%|█████▋    | 2329/4096 [00:48<00:44, 39.42it/s, est. speed input: 49112.39 toks/s, output: 47.96 toks/s]
Processed prompts:  58%|█████▊    | 2361/4096 [00:49<00:44, 39.41it/s, est. speed input: 48967.60 toks/s, output: 47.82 toks/s]
Processed prompts:  58%|█████▊    | 2393/4096 [00:50<00:43, 39.41it/s, est. speed input: 48828.03 toks/s, output: 47.68 toks/s]
Processed prompts:  59%|█████▉    | 2425/4096 [00:51<00:42, 39.35it/s, est. speed input: 48689.74 toks/s, output: 47.55 toks/s]
Processed prompts:  60%|█████▉    | 2457/4096 [00:51<00:41, 39.32it/s, est. speed input: 48555.73 toks/s, output: 47.42 toks/s]
Processed prompts:  61%|██████    | 2489/4096 [00:52<00:40, 39.35it/s, est. speed input: 48429.73 toks/s, output: 47.29 toks/s]
Processed prompts:  62%|██████▏   | 2521/4096 [00:53<00:39, 39.73it/s, est. speed input: 48329.20 toks/s, output: 47.20 toks/s]
Processed prompts:  62%|██████▏   | 2553/4096 [00:54<00:38, 39.59it/s, est. speed input: 48207.39 toks/s, output: 47.08 toks/s]
Processed prompts:  63%|██████▎   | 2585/4096 [00:55<00:37, 39.89it/s, est. speed input: 48112.54 toks/s, output: 46.98 toks/s]
Processed prompts:  64%|██████▍   | 2617/4096 [00:55<00:37, 39.71it/s, est. speed input: 47997.75 toks/s, output: 46.87 toks/s]
Processed prompts:  65%|██████▍   | 2649/4096 [00:56<00:36, 39.56it/s, est. speed input: 47884.91 toks/s, output: 46.76 toks/s]
Processed prompts:  65%|██████▌   | 2681/4096 [00:57<00:35, 39.50it/s, est. speed input: 47777.62 toks/s, output: 46.66 toks/s]
Processed prompts:  66%|██████▌   | 2713/4096 [00:58<00:35, 39.41it/s, est. speed input: 47670.78 toks/s, output: 46.55 toks/s]
Processed prompts:  67%|██████▋   | 2745/4096 [00:59<00:33, 39.75it/s, est. speed input: 47588.69 toks/s, output: 46.47 toks/s]
Processed prompts:  68%|██████▊   | 2777/4096 [00:59<00:33, 39.64it/s, est. speed input: 47490.41 toks/s, output: 46.38 toks/s]
Processed prompts:  69%|██████▊   | 2809/4096 [01:00<00:32, 39.55it/s, est. speed input: 47393.71 toks/s, output: 46.28 toks/s]
Processed prompts:  69%|██████▉   | 2841/4096 [01:01<00:31, 39.46it/s, est. speed input: 47298.08 toks/s, output: 46.19 toks/s]
Processed prompts:  70%|███████   | 2873/4096 [01:02<00:31, 39.44it/s, est. speed input: 47207.77 toks/s, output: 46.10 toks/s]
Processed prompts:  71%|███████   | 2905/4096 [01:03<00:29, 40.50it/s, est. speed input: 47172.95 toks/s, output: 46.07 toks/s]
Processed prompts:  72%|███████▏  | 2937/4096 [01:03<00:28, 40.13it/s, est. speed input: 47084.40 toks/s, output: 45.98 toks/s]
Processed prompts:  72%|███████▏  | 2969/4096 [01:04<00:28, 39.86it/s, est. speed input: 46997.44 toks/s, output: 45.90 toks/s]
Processed prompts:  73%|███████▎  | 3001/4096 [01:05<00:27, 39.72it/s, est. speed input: 46915.15 toks/s, output: 45.82 toks/s]
Processed prompts:  74%|███████▍  | 3033/4096 [01:06<00:26, 39.54it/s, est. speed input: 46830.76 toks/s, output: 45.73 toks/s]
Processed prompts:  75%|███████▍  | 3065/4096 [01:07<00:26, 39.49it/s, est. speed input: 46751.63 toks/s, output: 45.66 toks/s]
Processed prompts:  76%|███████▌  | 3097/4096 [01:07<00:25, 39.40it/s, est. speed input: 46671.98 toks/s, output: 45.58 toks/s]
Processed prompts:  76%|███████▋  | 3129/4096 [01:08<00:24, 39.38it/s, est. speed input: 46596.65 toks/s, output: 45.50 toks/s]
Processed prompts:  77%|███████▋  | 3161/4096 [01:09<00:23, 39.26it/s, est. speed input: 46517.51 toks/s, output: 45.43 toks/s]
Processed prompts:  78%|███████▊  | 3193/4096 [01:10<00:22, 39.27it/s, est. speed input: 46444.81 toks/s, output: 45.36 toks/s]
Processed prompts:  79%|███████▊  | 3225/4096 [01:11<00:22, 39.21it/s, est. speed input: 46370.77 toks/s, output: 45.28 toks/s]
Processed prompts:  80%|███████▉  | 3257/4096 [01:12<00:21, 39.22it/s, est. speed input: 46300.63 toks/s, output: 45.22 toks/s]
Processed prompts:  80%|████████  | 3289/4096 [01:12<00:20, 39.23it/s, est. speed input: 46232.37 toks/s, output: 45.15 toks/s]
Processed prompts:  81%|████████  | 3321/4096 [01:13<00:19, 39.24it/s, est. speed input: 46165.78 toks/s, output: 45.08 toks/s]
Processed prompts:  82%|████████▏ | 3353/4096 [01:14<00:18, 39.17it/s, est. speed input: 46097.33 toks/s, output: 45.02 toks/s]
Processed prompts:  83%|████████▎ | 3385/4096 [01:15<00:18, 39.15it/s, est. speed input: 46031.30 toks/s, output: 44.95 toks/s]
Processed prompts:  83%|████████▎ | 3417/4096 [01:16<00:17, 39.12it/s, est. speed input: 45966.50 toks/s, output: 44.89 toks/s]
Processed prompts:  84%|████████▍ | 3449/4096 [01:16<00:16, 39.06it/s, est. speed input: 45901.03 toks/s, output: 44.83 toks/s]
Processed prompts:  85%|████████▍ | 3481/4096 [01:17<00:15, 39.10it/s, est. speed input: 45840.50 toks/s, output: 44.77 toks/s]
Processed prompts:  86%|████████▌ | 3513/4096 [01:18<00:14, 39.15it/s, est. speed input: 45782.14 toks/s, output: 44.71 toks/s]
Processed prompts:  87%|████████▋ | 3545/4096 [01:19<00:13, 39.49it/s, est. speed input: 45736.84 toks/s, output: 44.66 toks/s]
Processed prompts:  87%|████████▋ | 3577/4096 [01:20<00:13, 39.37it/s, est. speed input: 45678.91 toks/s, output: 44.61 toks/s]
Processed prompts:  88%|████████▊ | 3609/4096 [01:21<00:12, 39.33it/s, est. speed input: 45623.29 toks/s, output: 44.55 toks/s]
Processed prompts:  89%|████████▉ | 3641/4096 [01:21<00:11, 39.23it/s, est. speed input: 45566.50 toks/s, output: 44.50 toks/s]
Processed prompts:  90%|████████▉ | 3673/4096 [01:22<00:10, 39.51it/s, est. speed input: 45523.76 toks/s, output: 44.46 toks/s]
Processed prompts:  90%|█████████ | 3705/4096 [01:23<00:09, 39.37it/s, est. speed input: 45469.48 toks/s, output: 44.40 toks/s]
Processed prompts:  91%|█████████ | 3737/4096 [01:24<00:09, 39.20it/s, est. speed input: 45413.22 toks/s, output: 44.35 toks/s]
Processed prompts:  92%|█████████▏| 3769/4096 [01:25<00:08, 39.12it/s, est. speed input: 45359.90 toks/s, output: 44.30 toks/s]
Processed prompts:  93%|█████████▎| 3801/4096 [01:25<00:07, 39.09it/s, est. speed input: 45308.08 toks/s, output: 44.25 toks/s]
Processed prompts:  94%|█████████▎| 3833/4096 [01:26<00:06, 39.07it/s, est. speed input: 45257.47 toks/s, output: 44.20 toks/s]
Processed prompts:  94%|█████████▍| 3865/4096 [01:27<00:05, 39.07it/s, est. speed input: 45208.48 toks/s, output: 44.15 toks/s]
Processed prompts:  95%|█████████▌| 3897/4096 [01:28<00:05, 39.04it/s, est. speed input: 45159.03 toks/s, output: 44.10 toks/s]
Processed prompts:  96%|█████████▌| 3929/4096 [01:29<00:04, 39.71it/s, est. speed input: 45134.70 toks/s, output: 44.08 toks/s]
Processed prompts:  97%|█████████▋| 3961/4096 [01:29<00:03, 39.47it/s, est. speed input: 45086.62 toks/s, output: 44.03 toks/s]
Processed prompts:  97%|█████████▋| 3993/4096 [01:30<00:02, 39.67it/s, est. speed input: 45051.57 toks/s, output: 44.00 toks/s]
Processed prompts:  98%|█████████▊| 4025/4096 [01:31<00:01, 39.48it/s, est. speed input: 45006.26 toks/s, output: 43.95 toks/s]
Processed prompts:  99%|█████████▉| 4057/4096 [01:32<00:00, 39.68it/s, est. speed input: 44972.56 toks/s, output: 43.92 toks/s]
Processed prompts: 100%|█████████▉| 4089/4096 [01:32<00:00, 51.50it/s, est. speed input: 45234.69 toks/s, output: 44.17 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [01:32<00:00, 51.50it/s, est. speed input: 45311.91 toks/s, output: 44.25 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [01:32<00:00, 44.25it/s, est. speed input: 45311.91 toks/s, output: 44.25 toks/s]
[rank0]:[W126 11:52:06.342974071 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 164.6s

测试结果:
  Requests/s:   39.67
  Tokens/s:     40658.46
  Total Reqs:   4096
  Elapsed:      103.26s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     40618.80

============================================================
[8/8] 测试 M=65536
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuBLASLt                                        │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 65536
│   M_prefill     = 65536 (= 64 x 1024)
│   M_decode      = 64
│   batched_tokens = 65536 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 8192
│   --max-num-seqs           = 64
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 65536
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 11:52:59 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1315848) WARNING 01-26 11:53:15 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]     self.model_runner.profile_run()
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]     hidden_states = self.model(
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]                     ^^^^^^^^^^^
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]     def forward(
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]     raise e
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]   File "<eval_with_key>.58", line 325, in forward
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]     return range_entry.runnable(*args)
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]     return self._compiled_fn(*args)
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]     return compiled_fn(full_args)
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]                             ^^^^^^^
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 724, in inner_fn
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]     outs = compiled_fn(args)
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]            ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]     return self.current_callable(inputs)
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]     out = model(new_inputs)
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]   File "/tmp/torchinductor_root/nr/cnrfq5d4ytdh4lf4u7jz2267pfospqxem37br54eck6wsqzsqwol.py", line 1090, in call
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]     triton_poi_fused_mul_quant_only_fp8_silu_slice_1.run(buf15, buf16, triton_poi_fused_mul_quant_only_fp8_silu_slice_1_xnumel, stream=stream0)
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 1272, in run
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]     self.autotune_to_one_config(*args, **kwargs)
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 1048, in autotune_to_one_config
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]     timings = self.benchmark_all_configs(*args, **kwargs)
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 1023, in benchmark_all_configs
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]     launcher: self.bench(launcher, *args, **kwargs)
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 891, in bench
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]     return benchmarker.benchmark_gpu(kernel_call, rep=40)
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/benchmarking.py", line 39, in wrapper
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]     return fn(self, *args, **kwargs)
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/benchmarking.py", line 247, in benchmark_gpu
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]     torch.cuda.synchronize()
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py", line 1083, in synchronize
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]     return torch._C._cuda_synchronize()
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866] torch.AcceleratorError: CUDA error: an illegal memory access was encountered
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866] Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866] For debugging consider passing CUDA_LAUNCH_BLOCKING=1
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
(EngineCore_DP0 pid=1315848) ERROR 01-26 11:53:23 [core.py:866] 


─── STDERR ───
[2026-01-26 11:52:58] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:52:59] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 11:52:59] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 11:52:59] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:52:59] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:52:59] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:52:59] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:52:59] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:52:59] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 11:52:59] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:52:59] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:52:59] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:52:59] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:52:59] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 11:53:06] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:53:06] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 11:53:06] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 11:53:06] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:53:06] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:53:06] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:53:06] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:53:06] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:53:06] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 11:53:06] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:53:06] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:53:06] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:53:06] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:53:06] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1315848) [2026-01-26 11:53:08] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=1315848) [2026-01-26 11:53:08] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1315848) [2026-01-26 11:53:08] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=1315848) [2026-01-26 11:53:08] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=1315848) [2026-01-26 11:53:08] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=1315848) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1315848) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.62it/s]
(EngineCore_DP0 pid=1315848) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  1.92it/s]
(EngineCore_DP0 pid=1315848) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.00it/s]
(EngineCore_DP0 pid=1315848) 
(EngineCore_DP0 pid=1315848) [rank0]:W0126 11:53:20.937000 1315848 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1315848) [rank0]:W0126 11:53:21.015000 1315848 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1315848) [2026-01-26 11:53:21] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=4608, K=3584), falling back to default heuristic
(EngineCore_DP0 pid=1315848) [rank0]:W0126 11:53:21.942000 1315848 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1315848) [rank0]:W0126 11:53:22.064000 1315848 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1315848) [2026-01-26 11:53:22] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3584, K=3584), falling back to default heuristic
(EngineCore_DP0 pid=1315848) [2026-01-26 11:53:22] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=37888, K=3584), falling back to default heuristic
(EngineCore_DP0 pid=1315848) Process EngineCore_DP0:
(EngineCore_DP0 pid=1315848) Traceback (most recent call last):
(EngineCore_DP0 pid=1315848)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=1315848)     self.run()
(EngineCore_DP0 pid=1315848)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=1315848)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=1315848)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=1315848)     raise e
(EngineCore_DP0 pid=1315848)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1315848)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1315848)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1315848)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1315848)     super().__init__(
(EngineCore_DP0 pid=1315848)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=1315848)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=1315848)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1315848)   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=1315848)     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=1315848)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1315848)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=1315848)     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=1315848)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1315848)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=1315848)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=1315848)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1315848)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=1315848)     return func(*args, **kwargs)
(EngineCore_DP0 pid=1315848)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1315848)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=1315848)     return func(*args, **kwargs)
(EngineCore_DP0 pid=1315848)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1315848)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=1315848)     self.model_runner.profile_run()
(EngineCore_DP0 pid=1315848)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=1315848)     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=1315848)                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1315848)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=1315848)     return func(*args, **kwargs)
(EngineCore_DP0 pid=1315848)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1315848)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=1315848)     outputs = self.model(
(EngineCore_DP0 pid=1315848)               ^^^^^^^^^^^
(EngineCore_DP0 pid=1315848)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=1315848)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=1315848)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1315848)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=1315848)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=1315848)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1315848)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=1315848)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=1315848)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1315848)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=1315848)     hidden_states = self.model(
(EngineCore_DP0 pid=1315848)                     ^^^^^^^^^^^
(EngineCore_DP0 pid=1315848)   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=1315848)     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=1315848)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1315848)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=1315848)     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=1315848)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1315848)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=1315848)     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=1315848)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1315848)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
(EngineCore_DP0 pid=1315848)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=1315848)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1315848)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
(EngineCore_DP0 pid=1315848)     def forward(
(EngineCore_DP0 pid=1315848)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=1315848)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=1315848)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1315848)   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=1315848)     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=1315848)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1315848)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=1315848)     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=1315848)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1315848)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=1315848)     raise e
(EngineCore_DP0 pid=1315848)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=1315848)     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=1315848)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1315848)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=1315848)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=1315848)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1315848)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=1315848)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=1315848)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1315848)   File "<eval_with_key>.58", line 325, in forward
(EngineCore_DP0 pid=1315848)     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
(EngineCore_DP0 pid=1315848)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1315848)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=1315848)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=1315848)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1315848)   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=1315848)     return range_entry.runnable(*args)
(EngineCore_DP0 pid=1315848)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1315848)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=1315848)     return self._compiled_fn(*args)
(EngineCore_DP0 pid=1315848)            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1315848)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=1315848)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=1315848)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1315848)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=1315848)     return compiled_fn(full_args)
(EngineCore_DP0 pid=1315848)            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1315848)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=1315848)     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=1315848)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1315848)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=1315848)     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=1315848)                             ^^^^^^^
(EngineCore_DP0 pid=1315848)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 724, in inner_fn
(EngineCore_DP0 pid=1315848)     outs = compiled_fn(args)
(EngineCore_DP0 pid=1315848)            ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1315848)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=1315848)     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=1315848)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1315848)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=1315848)     return self.current_callable(inputs)
(EngineCore_DP0 pid=1315848)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1315848)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=1315848)     out = model(new_inputs)
(EngineCore_DP0 pid=1315848)           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1315848)   File "/tmp/torchinductor_root/nr/cnrfq5d4ytdh4lf4u7jz2267pfospqxem37br54eck6wsqzsqwol.py", line 1090, in call
(EngineCore_DP0 pid=1315848)     triton_poi_fused_mul_quant_only_fp8_silu_slice_1.run(buf15, buf16, triton_poi_fused_mul_quant_only_fp8_silu_slice_1_xnumel, stream=stream0)
(EngineCore_DP0 pid=1315848)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 1272, in run
(EngineCore_DP0 pid=1315848)     self.autotune_to_one_config(*args, **kwargs)
(EngineCore_DP0 pid=1315848)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 1048, in autotune_to_one_config
(EngineCore_DP0 pid=1315848)     timings = self.benchmark_all_configs(*args, **kwargs)
(EngineCore_DP0 pid=1315848)               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1315848)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 1023, in benchmark_all_configs
(EngineCore_DP0 pid=1315848)     launcher: self.bench(launcher, *args, **kwargs)
(EngineCore_DP0 pid=1315848)               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1315848)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 891, in bench
(EngineCore_DP0 pid=1315848)     return benchmarker.benchmark_gpu(kernel_call, rep=40)
(EngineCore_DP0 pid=1315848)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1315848)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/benchmarking.py", line 39, in wrapper
(EngineCore_DP0 pid=1315848)     return fn(self, *args, **kwargs)
(EngineCore_DP0 pid=1315848)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1315848)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/benchmarking.py", line 247, in benchmark_gpu
(EngineCore_DP0 pid=1315848)     torch.cuda.synchronize()
(EngineCore_DP0 pid=1315848)   File "/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py", line 1083, in synchronize
(EngineCore_DP0 pid=1315848)     return torch._C._cuda_synchronize()
(EngineCore_DP0 pid=1315848)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1315848) torch.AcceleratorError: CUDA error: an illegal memory access was encountered
(EngineCore_DP0 pid=1315848) Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
(EngineCore_DP0 pid=1315848) CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
(EngineCore_DP0 pid=1315848) For debugging consider passing CUDA_LAUNCH_BLOCKING=1
(EngineCore_DP0 pid=1315848) Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
(EngineCore_DP0 pid=1315848) 
[rank0]:[W126 11:53:24.118041676 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[ERROR] 测试失败: M=65536 (exit code: 1)


------------------------------------------------------------
  生成 CSV: Qwen2.5-7B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_FP8E4M3_py312_cu129_x86_64/cublaslt/Qwen2.5-7B-FP8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,35.8718,18402.2192,3.5683
1024,1024,1,128,128,33.0107,33835.9891,3.8775
2048,1024,2,256,128,35.4669,36353.5866,7.2180
4096,1024,4,512,128,38.1688,39123.0555,13.4141
8192,1024,8,1024,128,38.5906,39555.3777,26.5350
16384,1024,16,2048,128,38.7866,39756.3084,52.8017
32768,1024,32,4096,128,39.6668,40658.4634,103.2602
65536,1024,64,8192,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[INFO] 完成: 7 成功, 1 失败

============================================================
  Qwen2.5-7B-FP8 | cuSPARSELt (2_4) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_4
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_FP8E4M3_py312_cu129_x86_64/cusparselt/2_4

============================================================
[1/8] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 11:53:33 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1316821) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1316821) WARNING 01-26 11:53:50 [backends.py:609] Failed to read file <frozen os>
Throughput: 36.72 requests/s, 18836.73 total tokens/s, 36.72 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-26 11:53:33] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:53:33] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 11:53:33] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 11:53:33] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:53:33] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:53:33] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:53:33] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:53:33] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:53:33] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 11:53:33] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:53:33] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:53:33] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:53:33] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:53:33] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 11:53:39] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:53:41] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 11:53:41] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 11:53:41] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:53:41] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:53:41] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:53:41] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:53:41] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:53:41] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 11:53:41] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:53:41] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:53:41] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:53:41] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:53:41] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1316821) [2026-01-26 11:53:42] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1316821) [2026-01-26 11:53:42] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1316821) [2026-01-26 11:53:42] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1316821) [2026-01-26 11:53:42] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=1316821) [2026-01-26 11:53:42] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=1316821) [2026-01-26 11:53:42] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=1316821) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1316821) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.62it/s]
(EngineCore_DP0 pid=1316821) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  1.93it/s]
(EngineCore_DP0 pid=1316821) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.01it/s]
(EngineCore_DP0 pid=1316821) 
(EngineCore_DP0 pid=1316821) [2026-01-26 11:53:44] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 3584] -> 1D uint8
(EngineCore_DP0 pid=1316821) [2026-01-26 11:53:44] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10321920 bytes
(EngineCore_DP0 pid=1316821) [2026-01-26 11:53:44] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 3584] -> 1D uint8
(EngineCore_DP0 pid=1316821) [2026-01-26 11:53:44] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8028160 bytes
(EngineCore_DP0 pid=1316821) [2026-01-26 11:53:44] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 3584] -> 1D uint8
(EngineCore_DP0 pid=1316821) [2026-01-26 11:53:44] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 84869120 bytes
(EngineCore_DP0 pid=1316821) [2026-01-26 11:53:44] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 18944] -> 1D uint8
(EngineCore_DP0 pid=1316821) [2026-01-26 11:53:44] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 42434560 bytes
(EngineCore_DP0 pid=1316821) 2026-01-26 11:54:01,946 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1316821) 2026-01-26 11:54:01,969 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1316821) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  1.35it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:01<00:00,  2.05it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:01<00:00,  1.91it/s]
(EngineCore_DP0 pid=1316821) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 17.69it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  27%|██▋       | 35/128 [00:00<00:00, 349.82it/s]
Adding requests:  81%|████████▏ | 104/128 [00:00<00:00, 547.60it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 541.12it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:16,  7.80it/s, est. speed input: 3996.06 toks/s, output: 7.80 toks/s]
Processed prompts:   5%|▍         | 6/128 [00:00<00:04, 26.72it/s, est. speed input: 12202.15 toks/s, output: 23.83 toks/s]
Processed prompts:   9%|▊         | 11/128 [00:00<00:03, 33.01it/s, est. speed input: 15029.83 toks/s, output: 29.35 toks/s]
Processed prompts:  12%|█▎        | 16/128 [00:00<00:03, 36.10it/s, est. speed input: 16483.69 toks/s, output: 32.19 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:02, 37.78it/s, est. speed input: 17355.03 toks/s, output: 33.89 toks/s]
Processed prompts:  20%|██        | 26/128 [00:00<00:02, 38.80it/s, est. speed input: 17936.78 toks/s, output: 35.03 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:00<00:02, 39.44it/s, est. speed input: 18352.73 toks/s, output: 35.84 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:00<00:02, 39.81it/s, est. speed input: 18657.31 toks/s, output: 36.44 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:01<00:02, 40.09it/s, est. speed input: 18897.80 toks/s, output: 36.91 toks/s]
Processed prompts:  36%|███▌      | 46/128 [00:01<00:02, 40.30it/s, est. speed input: 19094.71 toks/s, output: 37.29 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:01<00:01, 40.36it/s, est. speed input: 19244.06 toks/s, output: 37.59 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:01<00:01, 40.56it/s, est. speed input: 19389.36 toks/s, output: 37.87 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:01<00:01, 40.62it/s, est. speed input: 19501.97 toks/s, output: 38.09 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:01<00:01, 40.67it/s, est. speed input: 19600.04 toks/s, output: 38.28 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:01<00:01, 40.75it/s, est. speed input: 19689.69 toks/s, output: 38.46 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:01<00:01, 40.73it/s, est. speed input: 19761.25 toks/s, output: 38.60 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:02<00:01, 40.77it/s, est. speed input: 19828.90 toks/s, output: 38.73 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:02<00:01, 40.76it/s, est. speed input: 19886.20 toks/s, output: 38.84 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:02<00:00, 40.74it/s, est. speed input: 19936.32 toks/s, output: 38.94 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:02<00:00, 40.75it/s, est. speed input: 19982.74 toks/s, output: 39.03 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:02<00:00, 40.75it/s, est. speed input: 20024.95 toks/s, output: 39.11 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:02<00:00, 40.71it/s, est. speed input: 20059.93 toks/s, output: 39.18 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:02<00:00, 40.68it/s, est. speed input: 20091.64 toks/s, output: 39.24 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:02<00:00, 40.62it/s, est. speed input: 20118.00 toks/s, output: 39.29 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:03<00:00, 40.60it/s, est. speed input: 20143.78 toks/s, output: 39.34 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:03<00:00, 40.61it/s, est. speed input: 20169.14 toks/s, output: 39.39 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 40.61it/s, est. speed input: 20179.36 toks/s, output: 39.41 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 39.41it/s, est. speed input: 20179.36 toks/s, output: 39.41 toks/s]
[rank0]:[W126 11:54:08.387760384 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 43.7s

测试结果:
  Requests/s:   36.72
  Tokens/s:     18836.73
  Total Reqs:   128
  Elapsed:      3.49s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     18800.02

============================================================
[2/8] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 11:54:17 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1318011) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1318011) WARNING 01-26 11:54:34 [backends.py:609] Failed to read file <frozen os>
Throughput: 37.07 requests/s, 37993.50 total tokens/s, 37.07 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-26 11:54:17] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:54:17] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 11:54:17] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 11:54:17] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:54:17] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:54:17] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:54:17] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:54:17] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:54:17] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 11:54:17] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:54:17] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:54:17] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:54:17] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:54:17] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 11:54:24] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:54:25] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 11:54:25] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 11:54:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:54:25] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:54:25] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:54:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:54:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:54:25] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 11:54:25] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:54:25] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:54:25] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:54:25] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:54:25] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1318011) [2026-01-26 11:54:26] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1318011) [2026-01-26 11:54:26] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1318011) [2026-01-26 11:54:26] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1318011) [2026-01-26 11:54:26] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=1318011) [2026-01-26 11:54:26] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=1318011) [2026-01-26 11:54:26] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=1318011) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1318011) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.64it/s]
(EngineCore_DP0 pid=1318011) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  1.93it/s]
(EngineCore_DP0 pid=1318011) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.01it/s]
(EngineCore_DP0 pid=1318011) 
(EngineCore_DP0 pid=1318011) [2026-01-26 11:54:27] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 3584] -> 1D uint8
(EngineCore_DP0 pid=1318011) [2026-01-26 11:54:28] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10321920 bytes
(EngineCore_DP0 pid=1318011) [2026-01-26 11:54:28] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 3584] -> 1D uint8
(EngineCore_DP0 pid=1318011) [2026-01-26 11:54:28] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8028160 bytes
(EngineCore_DP0 pid=1318011) [2026-01-26 11:54:28] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 3584] -> 1D uint8
(EngineCore_DP0 pid=1318011) [2026-01-26 11:54:28] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 84869120 bytes
(EngineCore_DP0 pid=1318011) [2026-01-26 11:54:28] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 18944] -> 1D uint8
(EngineCore_DP0 pid=1318011) [2026-01-26 11:54:28] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 42434560 bytes
(EngineCore_DP0 pid=1318011) 2026-01-26 11:54:45,153 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1318011) 2026-01-26 11:54:45,184 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1318011) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  2.29it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.75it/s]
(EngineCore_DP0 pid=1318011) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.29it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.28it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  16%|█▌        | 20/128 [00:00<00:00, 198.53it/s]
Adding requests:  48%|████▊     | 61/128 [00:00<00:00, 318.33it/s]
Adding requests:  77%|███████▋  | 98/128 [00:00<00:00, 338.03it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 332.15it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   5%|▍         | 6/128 [00:00<00:02, 55.35it/s, est. speed input: 56702.73 toks/s, output: 55.36 toks/s]
Processed prompts:   9%|▉         | 12/128 [00:00<00:02, 45.97it/s, est. speed input: 48307.36 toks/s, output: 47.17 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:02, 43.81it/s, est. speed input: 46276.72 toks/s, output: 45.19 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:00<00:02, 42.77it/s, est. speed input: 45255.33 toks/s, output: 44.19 toks/s]
Processed prompts:  21%|██        | 27/128 [00:00<00:02, 42.23it/s, est. speed input: 44666.40 toks/s, output: 43.62 toks/s]
Processed prompts:  25%|██▌       | 32/128 [00:00<00:02, 41.92it/s, est. speed input: 44281.08 toks/s, output: 43.24 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:00<00:02, 41.70it/s, est. speed input: 43988.00 toks/s, output: 42.96 toks/s]
Processed prompts:  33%|███▎      | 42/128 [00:00<00:02, 41.56it/s, est. speed input: 43776.60 toks/s, output: 42.75 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:01<00:01, 41.46it/s, est. speed input: 43604.92 toks/s, output: 42.58 toks/s]
Processed prompts:  41%|████      | 52/128 [00:01<00:01, 41.38it/s, est. speed input: 43462.90 toks/s, output: 42.44 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:01<00:01, 41.36it/s, est. speed input: 43361.44 toks/s, output: 42.34 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:01<00:01, 41.31it/s, est. speed input: 43262.42 toks/s, output: 42.25 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:01<00:01, 41.27it/s, est. speed input: 43178.55 toks/s, output: 42.17 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:01<00:01, 41.26it/s, est. speed input: 43111.62 toks/s, output: 42.10 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:01<00:01, 41.26it/s, est. speed input: 43054.63 toks/s, output: 42.04 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:01<00:01, 41.25it/s, est. speed input: 43002.16 toks/s, output: 41.99 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:02<00:00, 41.23it/s, est. speed input: 42954.02 toks/s, output: 41.95 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:02<00:00, 41.20it/s, est. speed input: 42907.72 toks/s, output: 41.90 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:02<00:00, 41.20it/s, est. speed input: 42870.37 toks/s, output: 41.87 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:02<00:00, 41.30it/s, est. speed input: 42853.65 toks/s, output: 41.85 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:02<00:00, 41.35it/s, est. speed input: 42834.19 toks/s, output: 41.83 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:02<00:00, 41.30it/s, est. speed input: 42804.63 toks/s, output: 41.80 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:02<00:00, 41.34it/s, est. speed input: 42787.98 toks/s, output: 41.78 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:02<00:00, 41.36it/s, est. speed input: 42771.72 toks/s, output: 41.77 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:03<00:00, 41.33it/s, est. speed input: 42751.64 toks/s, output: 41.75 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 41.33it/s, est. speed input: 42749.78 toks/s, output: 41.75 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 41.74it/s, est. speed input: 42749.78 toks/s, output: 41.75 toks/s]
[rank0]:[W126 11:54:51.161012709 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 42.7s

测试结果:
  Requests/s:   37.07
  Tokens/s:     37993.50
  Total Reqs:   128
  Elapsed:      3.45s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     37956.43

============================================================
[3/8] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 11:55:01 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1319130) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1319130) WARNING 01-26 11:55:17 [backends.py:609] Failed to read file <frozen os>
Throughput: 43.04 requests/s, 44112.61 total tokens/s, 43.04 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-26 11:55:00] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:55:01] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 11:55:01] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 11:55:01] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:55:01] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:55:01] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:55:01] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:55:01] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:55:01] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 11:55:01] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:55:01] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:55:01] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:55:01] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:55:01] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 11:55:08] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:55:08] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 11:55:08] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 11:55:08] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:55:08] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:55:08] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:55:08] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:55:08] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:55:08] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 11:55:08] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:55:08] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:55:08] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:55:08] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:55:08] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1319130) [2026-01-26 11:55:10] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1319130) [2026-01-26 11:55:10] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1319130) [2026-01-26 11:55:10] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1319130) [2026-01-26 11:55:10] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=1319130) [2026-01-26 11:55:10] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=1319130) [2026-01-26 11:55:10] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=1319130) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1319130) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.62it/s]
(EngineCore_DP0 pid=1319130) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.91it/s]
(EngineCore_DP0 pid=1319130) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.99it/s]
(EngineCore_DP0 pid=1319130) 
(EngineCore_DP0 pid=1319130) [2026-01-26 11:55:11] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 3584] -> 1D uint8
(EngineCore_DP0 pid=1319130) [2026-01-26 11:55:11] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10321920 bytes
(EngineCore_DP0 pid=1319130) [2026-01-26 11:55:11] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 3584] -> 1D uint8
(EngineCore_DP0 pid=1319130) [2026-01-26 11:55:11] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8028160 bytes
(EngineCore_DP0 pid=1319130) [2026-01-26 11:55:11] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 3584] -> 1D uint8
(EngineCore_DP0 pid=1319130) [2026-01-26 11:55:11] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 84869120 bytes
(EngineCore_DP0 pid=1319130) [2026-01-26 11:55:11] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 18944] -> 1D uint8
(EngineCore_DP0 pid=1319130) [2026-01-26 11:55:11] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 42434560 bytes
(EngineCore_DP0 pid=1319130) 2026-01-26 11:55:28,001 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1319130) 2026-01-26 11:55:28,024 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1319130) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:00<00:00,  8.66it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00,  6.44it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00,  6.78it/s]
(EngineCore_DP0 pid=1319130) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 1/2 [00:00<00:00,  1.82it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00,  3.36it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00,  2.98it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   7%|▋         | 19/256 [00:00<00:01, 187.31it/s]
Adding requests:  23%|██▎       | 58/256 [00:00<00:00, 302.89it/s]
Adding requests:  37%|███▋      | 95/256 [00:00<00:00, 332.63it/s]
Adding requests:  52%|█████▏    | 133/256 [00:00<00:00, 347.74it/s]
Adding requests:  67%|██████▋   | 172/256 [00:00<00:00, 362.07it/s]
Adding requests:  83%|████████▎ | 212/256 [00:00<00:00, 374.77it/s]
Adding requests:  98%|█████████▊| 251/256 [00:00<00:00, 378.69it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 354.48it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   8%|▊         | 20/256 [00:00<00:01, 141.53it/s, est. speed input: 144947.46 toks/s, output: 141.54 toks/s]
Processed prompts:  14%|█▎        | 35/256 [00:00<00:03, 72.43it/s, est. speed input: 80949.45 toks/s, output: 79.05 toks/s]   
Processed prompts:  17%|█▋        | 44/256 [00:00<00:03, 59.16it/s, est. speed input: 68504.62 toks/s, output: 66.90 toks/s]
Processed prompts:  20%|█▉        | 51/256 [00:00<00:03, 57.83it/s, est. speed input: 66379.02 toks/s, output: 64.82 toks/s]
Processed prompts:  23%|██▎       | 58/256 [00:00<00:03, 51.57it/s, est. speed input: 61680.99 toks/s, output: 60.23 toks/s]
Processed prompts:  25%|██▌       | 64/256 [00:01<00:03, 50.02it/s, est. speed input: 59911.11 toks/s, output: 58.51 toks/s]
Processed prompts:  27%|██▋       | 70/256 [00:01<00:03, 48.96it/s, est. speed input: 58568.83 toks/s, output: 57.19 toks/s]
Processed prompts:  30%|██▉       | 76/256 [00:01<00:03, 48.17it/s, est. speed input: 57487.64 toks/s, output: 56.14 toks/s]
Processed prompts:  32%|███▏      | 82/256 [00:01<00:03, 47.66it/s, est. speed input: 56615.17 toks/s, output: 55.29 toks/s]
Processed prompts:  34%|███▍      | 88/256 [00:01<00:03, 47.37it/s, est. speed input: 55908.72 toks/s, output: 54.60 toks/s]
Processed prompts:  37%|███▋      | 94/256 [00:01<00:03, 47.07it/s, est. speed input: 55282.01 toks/s, output: 53.99 toks/s]
Processed prompts:  39%|███▉      | 100/256 [00:01<00:03, 46.73it/s, est. speed input: 54706.91 toks/s, output: 53.42 toks/s]
Processed prompts:  41%|████▏     | 106/256 [00:02<00:03, 46.42it/s, est. speed input: 54188.37 toks/s, output: 52.92 toks/s]
Processed prompts:  44%|████▍     | 112/256 [00:02<00:03, 46.26it/s, est. speed input: 53746.38 toks/s, output: 52.49 toks/s]
Processed prompts:  46%|████▌     | 118/256 [00:02<00:02, 46.25it/s, est. speed input: 53379.75 toks/s, output: 52.13 toks/s]
Processed prompts:  48%|████▊     | 124/256 [00:02<00:02, 46.24it/s, est. speed input: 53050.39 toks/s, output: 51.81 toks/s]
Processed prompts:  51%|█████     | 130/256 [00:02<00:02, 46.29it/s, est. speed input: 52766.29 toks/s, output: 51.53 toks/s]
Processed prompts:  53%|█████▎    | 136/256 [00:02<00:02, 46.30it/s, est. speed input: 52507.42 toks/s, output: 51.28 toks/s]
Processed prompts:  55%|█████▌    | 142/256 [00:02<00:02, 46.24it/s, est. speed input: 52258.27 toks/s, output: 51.03 toks/s]
Processed prompts:  58%|█████▊    | 148/256 [00:02<00:02, 46.15it/s, est. speed input: 52025.65 toks/s, output: 50.81 toks/s]
Processed prompts:  60%|██████    | 154/256 [00:03<00:02, 46.11it/s, est. speed input: 51815.59 toks/s, output: 50.60 toks/s]
Processed prompts:  62%|██████▎   | 160/256 [00:03<00:02, 46.06it/s, est. speed input: 51619.42 toks/s, output: 50.41 toks/s]
Processed prompts:  65%|██████▍   | 166/256 [00:03<00:01, 46.08it/s, est. speed input: 51446.20 toks/s, output: 50.24 toks/s]
Processed prompts:  67%|██████▋   | 172/256 [00:03<00:01, 46.07it/s, est. speed input: 51283.58 toks/s, output: 50.08 toks/s]
Processed prompts:  70%|██████▉   | 178/256 [00:03<00:01, 46.12it/s, est. speed input: 51140.86 toks/s, output: 49.94 toks/s]
Processed prompts:  72%|███████▏  | 184/256 [00:03<00:01, 46.17it/s, est. speed input: 51008.97 toks/s, output: 49.81 toks/s]
Processed prompts:  74%|███████▍  | 190/256 [00:03<00:01, 46.16it/s, est. speed input: 50881.09 toks/s, output: 49.69 toks/s]
Processed prompts:  77%|███████▋  | 196/256 [00:03<00:01, 46.07it/s, est. speed input: 50751.76 toks/s, output: 49.56 toks/s]
Processed prompts:  79%|███████▉  | 202/256 [00:04<00:01, 47.41it/s, est. speed input: 50789.75 toks/s, output: 49.60 toks/s]
Processed prompts:  81%|████████▏ | 208/256 [00:04<00:01, 47.21it/s, est. speed input: 50700.61 toks/s, output: 49.51 toks/s]
Processed prompts:  84%|████████▎ | 214/256 [00:04<00:00, 46.94it/s, est. speed input: 50603.09 toks/s, output: 49.42 toks/s]
Processed prompts:  86%|████████▌ | 220/256 [00:04<00:00, 46.66it/s, est. speed input: 50501.05 toks/s, output: 49.32 toks/s]
Processed prompts:  88%|████████▊ | 226/256 [00:04<00:00, 46.53it/s, est. speed input: 50411.37 toks/s, output: 49.23 toks/s]
Processed prompts:  91%|█████████ | 232/256 [00:04<00:00, 46.39it/s, est. speed input: 50321.92 toks/s, output: 49.14 toks/s]
Processed prompts:  93%|█████████▎| 238/256 [00:04<00:00, 46.43it/s, est. speed input: 50250.47 toks/s, output: 49.07 toks/s]
Processed prompts:  95%|█████████▌| 244/256 [00:04<00:00, 46.16it/s, est. speed input: 50154.73 toks/s, output: 48.98 toks/s]
Processed prompts:  98%|█████████▊| 250/256 [00:05<00:00, 46.06it/s, est. speed input: 50072.61 toks/s, output: 48.90 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:05<00:00, 48.13it/s, est. speed input: 50178.62 toks/s, output: 49.00 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:05<00:00, 48.13it/s, est. speed input: 50178.62 toks/s, output: 49.00 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:05<00:00, 49.00it/s, est. speed input: 50178.62 toks/s, output: 49.00 toks/s]
[rank0]:[W126 11:55:36.870969711 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 45.7s

测试结果:
  Requests/s:   43.04
  Tokens/s:     44112.61
  Total Reqs:   256
  Elapsed:      5.95s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     44069.58

============================================================
[4/8] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 11:55:48 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1320277) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1320277) WARNING 01-26 11:56:04 [backends.py:609] Failed to read file <frozen os>
Throughput: 47.42 requests/s, 48602.84 total tokens/s, 47.42 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-26 11:55:47] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:55:48] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 11:55:48] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 11:55:48] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:55:48] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:55:48] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:55:48] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:55:48] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:55:48] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 11:55:48] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:55:48] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:55:48] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:55:48] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:55:48] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 11:55:54] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:55:55] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 11:55:55] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 11:55:55] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:55:55] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:55:55] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:55:55] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:55:55] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:55:55] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 11:55:55] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:55:55] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:55:55] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:55:55] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:55:55] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1320277) [2026-01-26 11:55:56] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1320277) [2026-01-26 11:55:56] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1320277) [2026-01-26 11:55:56] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1320277) [2026-01-26 11:55:56] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=1320277) [2026-01-26 11:55:56] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=1320277) [2026-01-26 11:55:56] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=1320277) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1320277) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.63it/s]
(EngineCore_DP0 pid=1320277) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  1.93it/s]
(EngineCore_DP0 pid=1320277) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.01it/s]
(EngineCore_DP0 pid=1320277) 
(EngineCore_DP0 pid=1320277) [2026-01-26 11:55:58] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 3584] -> 1D uint8
(EngineCore_DP0 pid=1320277) [2026-01-26 11:55:58] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10321920 bytes
(EngineCore_DP0 pid=1320277) [2026-01-26 11:55:58] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 3584] -> 1D uint8
(EngineCore_DP0 pid=1320277) [2026-01-26 11:55:58] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8028160 bytes
(EngineCore_DP0 pid=1320277) [2026-01-26 11:55:58] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 3584] -> 1D uint8
(EngineCore_DP0 pid=1320277) [2026-01-26 11:55:58] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 84869120 bytes
(EngineCore_DP0 pid=1320277) [2026-01-26 11:55:58] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 18944] -> 1D uint8
(EngineCore_DP0 pid=1320277) [2026-01-26 11:55:58] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 42434560 bytes
(EngineCore_DP0 pid=1320277) 2026-01-26 11:56:15,372 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1320277) 2026-01-26 11:56:15,397 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1320277) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00,  5.96it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 3/4 [00:00<00:00,  5.82it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:01<00:00,  3.26it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:01<00:00,  3.82it/s]
(EngineCore_DP0 pid=1320277) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00, 17.47it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 18.49it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   4%|▍         | 20/512 [00:00<00:02, 195.71it/s]
Adding requests:  12%|█▏        | 59/512 [00:00<00:01, 308.36it/s]
Adding requests:  19%|█▉        | 96/512 [00:00<00:01, 332.58it/s]
Adding requests:  26%|██▌       | 133/512 [00:00<00:01, 344.06it/s]
Adding requests:  34%|███▎      | 172/512 [00:00<00:00, 359.18it/s]
Adding requests:  42%|████▏     | 213/512 [00:00<00:00, 374.90it/s]
Adding requests:  49%|████▉     | 252/512 [00:00<00:00, 377.47it/s]
Adding requests:  57%|█████▋    | 291/512 [00:00<00:00, 380.16it/s]
Adding requests:  65%|██████▍   | 332/512 [00:00<00:00, 387.89it/s]
Adding requests:  73%|███████▎  | 373/512 [00:01<00:00, 394.28it/s]
Adding requests:  81%|████████  | 414/512 [00:01<00:00, 398.35it/s]
Adding requests:  89%|████████▊ | 454/512 [00:01<00:00, 396.19it/s]
Adding requests:  97%|█████████▋| 497/512 [00:01<00:00, 404.50it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 377.53it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  12%|█▏        | 62/512 [00:00<00:01, 395.77it/s, est. speed input: 405325.60 toks/s, output: 395.78 toks/s]
Processed prompts:  20%|█▉        | 102/512 [00:00<00:04, 88.29it/s, est. speed input: 105338.69 toks/s, output: 102.87 toks/s]
Processed prompts:  24%|██▍       | 122/512 [00:01<00:05, 72.86it/s, est. speed input: 88852.83 toks/s, output: 86.77 toks/s]  
Processed prompts:  26%|██▋       | 135/512 [00:01<00:05, 67.79it/s, est. speed input: 83569.63 toks/s, output: 81.61 toks/s]
Processed prompts:  28%|██▊       | 145/512 [00:01<00:05, 66.05it/s, est. speed input: 81402.34 toks/s, output: 79.49 toks/s]
Processed prompts:  30%|███       | 154/512 [00:02<00:06, 57.26it/s, est. speed input: 76005.65 toks/s, output: 74.22 toks/s]
Processed prompts:  32%|███▏      | 162/512 [00:02<00:06, 55.33it/s, est. speed input: 74044.42 toks/s, output: 72.31 toks/s]
Processed prompts:  33%|███▎      | 170/512 [00:02<00:06, 53.63it/s, est. speed input: 72336.23 toks/s, output: 70.64 toks/s]
Processed prompts:  35%|███▍      | 178/512 [00:02<00:06, 52.35it/s, est. speed input: 70884.94 toks/s, output: 69.22 toks/s]
Processed prompts:  36%|███▋      | 186/512 [00:02<00:06, 51.16it/s, est. speed input: 69551.25 toks/s, output: 67.92 toks/s]
Processed prompts:  38%|███▊      | 194/512 [00:02<00:06, 49.99it/s, est. speed input: 68294.65 toks/s, output: 66.69 toks/s]
Processed prompts:  39%|███▉      | 202/512 [00:03<00:06, 50.50it/s, est. speed input: 67532.93 toks/s, output: 65.95 toks/s]
Processed prompts:  41%|████      | 210/512 [00:03<00:06, 49.90it/s, est. speed input: 66615.72 toks/s, output: 65.05 toks/s]
Processed prompts:  43%|████▎     | 218/512 [00:03<00:05, 49.39it/s, est. speed input: 65768.92 toks/s, output: 64.23 toks/s]
Processed prompts:  44%|████▍     | 226/512 [00:03<00:05, 49.05it/s, est. speed input: 65006.39 toks/s, output: 63.48 toks/s]
Processed prompts:  46%|████▌     | 234/512 [00:03<00:05, 48.60it/s, est. speed input: 64269.82 toks/s, output: 62.76 toks/s]
Processed prompts:  47%|████▋     | 242/512 [00:03<00:05, 48.29it/s, est. speed input: 63598.19 toks/s, output: 62.11 toks/s]
Processed prompts:  49%|████▉     | 250/512 [00:04<00:05, 48.21it/s, est. speed input: 63007.09 toks/s, output: 61.53 toks/s]
Processed prompts:  50%|█████     | 258/512 [00:04<00:05, 48.14it/s, est. speed input: 62458.93 toks/s, output: 60.99 toks/s]
Processed prompts:  52%|█████▏    | 266/512 [00:04<00:05, 48.19it/s, est. speed input: 61970.55 toks/s, output: 60.52 toks/s]
Processed prompts:  54%|█████▎    | 274/512 [00:04<00:04, 48.14it/s, est. speed input: 61503.46 toks/s, output: 60.06 toks/s]
Processed prompts:  55%|█████▌    | 282/512 [00:04<00:04, 48.08it/s, est. speed input: 61064.95 toks/s, output: 59.63 toks/s]
Processed prompts:  57%|█████▋    | 290/512 [00:04<00:04, 47.94it/s, est. speed input: 60642.47 toks/s, output: 59.22 toks/s]
Processed prompts:  58%|█████▊    | 298/512 [00:05<00:04, 47.88it/s, est. speed input: 60253.73 toks/s, output: 58.84 toks/s]
Processed prompts:  60%|█████▉    | 306/512 [00:05<00:04, 49.21it/s, est. speed input: 60067.89 toks/s, output: 58.66 toks/s]
Processed prompts:  61%|██████▏   | 314/512 [00:05<00:04, 49.05it/s, est. speed input: 59756.05 toks/s, output: 58.36 toks/s]
Processed prompts:  63%|██████▎   | 322/512 [00:05<00:03, 48.71it/s, est. speed input: 59434.48 toks/s, output: 58.04 toks/s]
Processed prompts:  64%|██████▍   | 330/512 [00:05<00:03, 48.42it/s, est. speed input: 59126.44 toks/s, output: 57.74 toks/s]
Processed prompts:  66%|██████▌   | 338/512 [00:05<00:03, 48.27it/s, est. speed input: 58840.52 toks/s, output: 57.46 toks/s]
Processed prompts:  68%|██████▊   | 346/512 [00:06<00:03, 48.07it/s, est. speed input: 58560.59 toks/s, output: 57.19 toks/s]
Processed prompts:  69%|██████▉   | 354/512 [00:06<00:03, 47.96it/s, est. speed input: 58298.61 toks/s, output: 56.93 toks/s]
Processed prompts:  71%|███████   | 362/512 [00:06<00:03, 48.01it/s, est. speed input: 58064.06 toks/s, output: 56.70 toks/s]
Processed prompts:  72%|███████▏  | 370/512 [00:06<00:02, 47.96it/s, est. speed input: 57832.41 toks/s, output: 56.48 toks/s]
Processed prompts:  74%|███████▍  | 378/512 [00:06<00:02, 47.89it/s, est. speed input: 57609.18 toks/s, output: 56.26 toks/s]
Processed prompts:  75%|███████▌  | 386/512 [00:06<00:02, 47.89it/s, est. speed input: 57401.22 toks/s, output: 56.06 toks/s]
Processed prompts:  77%|███████▋  | 394/512 [00:07<00:02, 47.85it/s, est. speed input: 57199.50 toks/s, output: 55.86 toks/s]
Processed prompts:  79%|███████▊  | 402/512 [00:07<00:02, 47.82it/s, est. speed input: 57006.62 toks/s, output: 55.67 toks/s]
Processed prompts:  80%|████████  | 410/512 [00:07<00:02, 47.90it/s, est. speed input: 56832.29 toks/s, output: 55.50 toks/s]
Processed prompts:  82%|████████▏ | 418/512 [00:07<00:01, 47.88it/s, est. speed input: 56657.71 toks/s, output: 55.33 toks/s]
Processed prompts:  83%|████████▎ | 426/512 [00:07<00:01, 47.97it/s, est. speed input: 56500.29 toks/s, output: 55.18 toks/s]
Processed prompts:  85%|████████▍ | 434/512 [00:07<00:01, 49.25it/s, est. speed input: 56448.06 toks/s, output: 55.12 toks/s]
Processed prompts:  86%|████████▋ | 442/512 [00:08<00:01, 48.90it/s, est. speed input: 56299.24 toks/s, output: 54.98 toks/s]
Processed prompts:  88%|████████▊ | 450/512 [00:08<00:01, 48.48it/s, est. speed input: 56142.79 toks/s, output: 54.83 toks/s]
Processed prompts:  89%|████████▉ | 458/512 [00:08<00:01, 48.38it/s, est. speed input: 56006.92 toks/s, output: 54.69 toks/s]
Processed prompts:  91%|█████████ | 466/512 [00:08<00:00, 48.23it/s, est. speed input: 55870.80 toks/s, output: 54.56 toks/s]
Processed prompts:  93%|█████████▎| 474/512 [00:08<00:00, 48.15it/s, est. speed input: 55741.30 toks/s, output: 54.43 toks/s]
Processed prompts:  94%|█████████▍| 482/512 [00:08<00:00, 48.05it/s, est. speed input: 55613.75 toks/s, output: 54.31 toks/s]
Processed prompts:  96%|█████████▌| 490/512 [00:09<00:00, 48.07it/s, est. speed input: 55496.77 toks/s, output: 54.20 toks/s]
Processed prompts:  97%|█████████▋| 498/512 [00:09<00:00, 47.93it/s, est. speed input: 55373.50 toks/s, output: 54.08 toks/s]
Processed prompts:  99%|█████████▉| 506/512 [00:09<00:00, 48.03it/s, est. speed input: 55268.63 toks/s, output: 53.97 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:09<00:00, 48.03it/s, est. speed input: 55540.77 toks/s, output: 54.24 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:09<00:00, 54.24it/s, est. speed input: 55540.77 toks/s, output: 54.24 toks/s]
[rank0]:[W126 11:56:29.199636004 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 52.6s

测试结果:
  Requests/s:   47.42
  Tokens/s:     48602.84
  Total Reqs:   512
  Elapsed:      10.80s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     48555.42

============================================================
[5/8] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 11:56:43 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1321542) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1321542) WARNING 01-26 11:56:59 [backends.py:609] Failed to read file <frozen os>
Throughput: 49.08 requests/s, 50307.87 total tokens/s, 49.08 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-26 11:56:43] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:56:43] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 11:56:43] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 11:56:43] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:56:43] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:56:43] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:56:43] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:56:43] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:56:43] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 11:56:43] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:56:43] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:56:43] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:56:43] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:56:43] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 11:56:50] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:56:50] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 11:56:50] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 11:56:50] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:56:50] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:56:50] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:56:50] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:56:50] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:56:50] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 11:56:50] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:56:50] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:56:50] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:56:50] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:56:50] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1321542) [2026-01-26 11:56:52] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1321542) [2026-01-26 11:56:52] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1321542) [2026-01-26 11:56:52] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1321542) [2026-01-26 11:56:52] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=1321542) [2026-01-26 11:56:52] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=1321542) [2026-01-26 11:56:52] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=1321542) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1321542) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.63it/s]
(EngineCore_DP0 pid=1321542) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  1.93it/s]
(EngineCore_DP0 pid=1321542) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.01it/s]
(EngineCore_DP0 pid=1321542) 
(EngineCore_DP0 pid=1321542) [2026-01-26 11:56:53] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 3584] -> 1D uint8
(EngineCore_DP0 pid=1321542) [2026-01-26 11:56:53] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10321920 bytes
(EngineCore_DP0 pid=1321542) [2026-01-26 11:56:53] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 3584] -> 1D uint8
(EngineCore_DP0 pid=1321542) [2026-01-26 11:56:53] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8028160 bytes
(EngineCore_DP0 pid=1321542) [2026-01-26 11:56:53] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 3584] -> 1D uint8
(EngineCore_DP0 pid=1321542) [2026-01-26 11:56:53] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 84869120 bytes
(EngineCore_DP0 pid=1321542) [2026-01-26 11:56:53] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 18944] -> 1D uint8
(EngineCore_DP0 pid=1321542) [2026-01-26 11:56:53] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 42434560 bytes
(EngineCore_DP0 pid=1321542) 2026-01-26 11:57:10,161 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1321542) 2026-01-26 11:57:10,185 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1321542) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 1/5 [00:00<00:01,  3.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:01,  2.92it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 3/5 [00:01<00:00,  2.96it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:01<00:00,  3.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:01<00:00,  4.03it/s]
(EngineCore_DP0 pid=1321542) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00, 19.42it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 20.13it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   3%|▎         | 28/1024 [00:00<00:03, 278.09it/s]
Adding requests:   7%|▋         | 68/1024 [00:00<00:02, 348.78it/s]
Adding requests:  10%|█         | 105/1024 [00:00<00:02, 357.07it/s]
Adding requests:  14%|█▍        | 142/1024 [00:00<00:02, 361.02it/s]
Adding requests:  18%|█▊        | 182/1024 [00:00<00:02, 372.10it/s]
Adding requests:  22%|██▏       | 223/1024 [00:00<00:02, 384.74it/s]
Adding requests:  26%|██▌       | 262/1024 [00:00<00:01, 381.14it/s]
Adding requests:  29%|██▉       | 302/1024 [00:00<00:01, 386.17it/s]
Adding requests:  33%|███▎      | 343/1024 [00:00<00:01, 389.88it/s]
Adding requests:  38%|███▊      | 384/1024 [00:01<00:01, 394.45it/s]
Adding requests:  42%|████▏     | 425/1024 [00:01<00:01, 397.50it/s]
Adding requests:  45%|████▌     | 465/1024 [00:01<00:01, 393.13it/s]
Adding requests:  50%|████▉     | 507/1024 [00:01<00:01, 398.39it/s]
Adding requests:  53%|█████▎    | 547/1024 [00:01<00:01, 396.46it/s]
Adding requests:  57%|█████▋    | 587/1024 [00:01<00:01, 395.07it/s]
Adding requests:  61%|██████    | 627/1024 [00:01<00:01, 394.13it/s]
Adding requests:  65%|██████▌   | 667/1024 [00:01<00:00, 385.34it/s]
Adding requests:  69%|██████▉   | 708/1024 [00:01<00:00, 391.42it/s]
Adding requests:  73%|███████▎  | 748/1024 [00:01<00:00, 385.29it/s]
Adding requests:  77%|███████▋  | 788/1024 [00:02<00:00, 388.51it/s]
Adding requests:  81%|████████  | 828/1024 [00:02<00:00, 390.11it/s]
Adding requests:  85%|████████▍ | 868/1024 [00:02<00:00, 391.86it/s]
Adding requests:  89%|████████▉ | 909/1024 [00:02<00:00, 396.27it/s]
Adding requests:  93%|█████████▎| 949/1024 [00:02<00:00, 387.28it/s]
Adding requests:  97%|█████████▋| 989/1024 [00:02<00:00, 389.63it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 385.74it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  13%|█▎        | 130/1024 [00:00<00:01, 583.13it/s, est. speed input: 597189.97 toks/s, output: 583.15 toks/s]
Processed prompts:  18%|█▊        | 189/1024 [00:01<00:07, 115.84it/s, est. speed input: 142121.67 toks/s, output: 138.79 toks/s]
Processed prompts:  21%|██        | 217/1024 [00:01<00:08, 96.25it/s, est. speed input: 120950.01 toks/s, output: 118.11 toks/s] 
Processed prompts:  23%|██▎       | 235/1024 [00:02<00:10, 76.68it/s, est. speed input: 103783.29 toks/s, output: 101.35 toks/s]
Processed prompts:  24%|██▍       | 248/1024 [00:02<00:10, 77.11it/s, est. speed input: 102329.20 toks/s, output: 99.93 toks/s] 
Processed prompts:  25%|██▌       | 259/1024 [00:02<00:11, 64.70it/s, est. speed input: 94393.76 toks/s, output: 92.18 toks/s] 
Processed prompts:  26%|██▌       | 268/1024 [00:02<00:11, 63.14it/s, est. speed input: 92334.64 toks/s, output: 90.17 toks/s]
Processed prompts:  27%|██▋       | 276/1024 [00:03<00:12, 60.64it/s, est. speed input: 90237.05 toks/s, output: 88.12 toks/s]
Processed prompts:  28%|██▊       | 283/1024 [00:03<00:13, 56.92it/s, est. speed input: 87991.67 toks/s, output: 85.93 toks/s]
Processed prompts:  28%|██▊       | 290/1024 [00:03<00:13, 53.58it/s, est. speed input: 85910.10 toks/s, output: 83.90 toks/s]
Processed prompts:  29%|██▉       | 298/1024 [00:03<00:13, 52.38it/s, est. speed input: 84288.32 toks/s, output: 82.31 toks/s]
Processed prompts:  30%|██▉       | 306/1024 [00:03<00:13, 52.95it/s, est. speed input: 83190.87 toks/s, output: 81.24 toks/s]
Processed prompts:  31%|███       | 314/1024 [00:03<00:13, 51.99it/s, est. speed input: 81853.52 toks/s, output: 79.93 toks/s]
Processed prompts:  31%|███▏      | 322/1024 [00:04<00:13, 51.30it/s, est. speed input: 80626.66 toks/s, output: 78.74 toks/s]
Processed prompts:  32%|███▏      | 330/1024 [00:04<00:13, 50.66it/s, est. speed input: 79463.88 toks/s, output: 77.60 toks/s]
Processed prompts:  33%|███▎      | 338/1024 [00:04<00:13, 50.22it/s, est. speed input: 78391.00 toks/s, output: 76.55 toks/s]
Processed prompts:  34%|███▍      | 346/1024 [00:04<00:13, 49.81it/s, est. speed input: 77375.21 toks/s, output: 75.56 toks/s]
Processed prompts:  35%|███▍      | 354/1024 [00:04<00:13, 49.52it/s, est. speed input: 76429.38 toks/s, output: 74.64 toks/s]
Processed prompts:  35%|███▌      | 362/1024 [00:04<00:13, 49.51it/s, est. speed input: 75580.49 toks/s, output: 73.81 toks/s]
Processed prompts:  36%|███▌      | 370/1024 [00:05<00:13, 49.44it/s, est. speed input: 74775.32 toks/s, output: 73.02 toks/s]
Processed prompts:  37%|███▋      | 378/1024 [00:05<00:13, 49.36it/s, est. speed input: 74016.39 toks/s, output: 72.28 toks/s]
Processed prompts:  38%|███▊      | 386/1024 [00:05<00:12, 49.22it/s, est. speed input: 73289.64 toks/s, output: 71.57 toks/s]
Processed prompts:  38%|███▊      | 394/1024 [00:05<00:12, 49.21it/s, est. speed input: 72618.87 toks/s, output: 70.92 toks/s]
Processed prompts:  39%|███▉      | 402/1024 [00:05<00:12, 49.19it/s, est. speed input: 71984.50 toks/s, output: 70.30 toks/s]
Processed prompts:  40%|████      | 410/1024 [00:05<00:12, 49.22it/s, est. speed input: 71390.43 toks/s, output: 69.72 toks/s]
Processed prompts:  41%|████      | 418/1024 [00:06<00:12, 49.18it/s, est. speed input: 70821.22 toks/s, output: 69.16 toks/s]
Processed prompts:  42%|████▏     | 426/1024 [00:06<00:12, 49.16it/s, est. speed input: 70282.24 toks/s, output: 68.63 toks/s]
Processed prompts:  42%|████▏     | 434/1024 [00:06<00:11, 50.74it/s, est. speed input: 69957.37 toks/s, output: 68.32 toks/s]
Processed prompts:  43%|████▎     | 442/1024 [00:06<00:11, 50.23it/s, est. speed input: 69464.94 toks/s, output: 67.84 toks/s]
Processed prompts:  44%|████▍     | 450/1024 [00:06<00:11, 49.95it/s, est. speed input: 69004.06 toks/s, output: 67.39 toks/s]
Processed prompts:  45%|████▍     | 458/1024 [00:06<00:11, 49.71it/s, est. speed input: 68559.82 toks/s, output: 66.95 toks/s]
Processed prompts:  46%|████▌     | 466/1024 [00:07<00:11, 49.54it/s, est. speed input: 68136.42 toks/s, output: 66.54 toks/s]
Processed prompts:  46%|████▋     | 474/1024 [00:07<00:11, 49.36it/s, est. speed input: 67725.23 toks/s, output: 66.14 toks/s]
Processed prompts:  47%|████▋     | 482/1024 [00:07<00:11, 49.20it/s, est. speed input: 67329.62 toks/s, output: 65.75 toks/s]
Processed prompts:  48%|████▊     | 490/1024 [00:07<00:10, 49.20it/s, est. speed input: 66961.17 toks/s, output: 65.39 toks/s]
Processed prompts:  49%|████▊     | 498/1024 [00:07<00:10, 49.13it/s, est. speed input: 66602.77 toks/s, output: 65.04 toks/s]
Processed prompts:  49%|████▉     | 506/1024 [00:07<00:10, 49.22it/s, est. speed input: 66271.91 toks/s, output: 64.72 toks/s]
Processed prompts:  50%|█████     | 514/1024 [00:07<00:10, 49.25it/s, est. speed input: 65951.54 toks/s, output: 64.41 toks/s]
Processed prompts:  51%|█████     | 522/1024 [00:08<00:10, 49.21it/s, est. speed input: 65638.22 toks/s, output: 64.10 toks/s]
Processed prompts:  52%|█████▏    | 530/1024 [00:08<00:10, 49.14it/s, est. speed input: 65334.00 toks/s, output: 63.80 toks/s]
Processed prompts:  53%|█████▎    | 538/1024 [00:08<00:09, 49.02it/s, est. speed input: 65034.94 toks/s, output: 63.51 toks/s]
Processed prompts:  53%|█████▎    | 546/1024 [00:08<00:09, 48.95it/s, est. speed input: 64748.64 toks/s, output: 63.23 toks/s]
Processed prompts:  54%|█████▍    | 554/1024 [00:08<00:09, 49.05it/s, est. speed input: 64484.81 toks/s, output: 62.97 toks/s]
Processed prompts:  55%|█████▍    | 562/1024 [00:08<00:09, 49.08it/s, est. speed input: 64227.85 toks/s, output: 62.72 toks/s]
Processed prompts:  56%|█████▌    | 570/1024 [00:09<00:09, 49.08it/s, est. speed input: 63978.13 toks/s, output: 62.48 toks/s]
Processed prompts:  56%|█████▋    | 578/1024 [00:09<00:09, 49.01it/s, est. speed input: 63732.27 toks/s, output: 62.24 toks/s]
Processed prompts:  57%|█████▋    | 586/1024 [00:09<00:08, 48.96it/s, est. speed input: 63494.51 toks/s, output: 62.01 toks/s]
Processed prompts:  58%|█████▊    | 594/1024 [00:09<00:08, 48.85it/s, est. speed input: 63259.04 toks/s, output: 61.78 toks/s]
Processed prompts:  59%|█████▉    | 602/1024 [00:09<00:08, 48.96it/s, est. speed input: 63045.89 toks/s, output: 61.57 toks/s]
Processed prompts:  60%|█████▉    | 610/1024 [00:09<00:08, 48.99it/s, est. speed input: 62835.18 toks/s, output: 61.36 toks/s]
Processed prompts:  60%|██████    | 618/1024 [00:10<00:08, 49.02it/s, est. speed input: 62633.14 toks/s, output: 61.16 toks/s]
Processed prompts:  61%|██████    | 626/1024 [00:10<00:08, 48.95it/s, est. speed input: 62430.72 toks/s, output: 60.97 toks/s]
Processed prompts:  62%|██████▏   | 634/1024 [00:10<00:07, 48.96it/s, est. speed input: 62238.20 toks/s, output: 60.78 toks/s]
Processed prompts:  63%|██████▎   | 642/1024 [00:10<00:07, 48.83it/s, est. speed input: 62043.22 toks/s, output: 60.59 toks/s]
Processed prompts:  63%|██████▎   | 650/1024 [00:10<00:07, 48.96it/s, est. speed input: 61867.99 toks/s, output: 60.42 toks/s]
Processed prompts:  64%|██████▍   | 658/1024 [00:10<00:07, 48.98it/s, est. speed input: 61693.94 toks/s, output: 60.25 toks/s]
Processed prompts:  65%|██████▌   | 666/1024 [00:11<00:07, 48.90it/s, est. speed input: 61518.79 toks/s, output: 60.08 toks/s]
Processed prompts:  66%|██████▌   | 674/1024 [00:11<00:07, 48.92it/s, est. speed input: 61353.66 toks/s, output: 59.92 toks/s]
Processed prompts:  67%|██████▋   | 682/1024 [00:11<00:06, 48.96it/s, est. speed input: 61194.37 toks/s, output: 59.76 toks/s]
Processed prompts:  67%|██████▋   | 690/1024 [00:11<00:06, 48.92it/s, est. speed input: 61035.80 toks/s, output: 59.61 toks/s]
Processed prompts:  68%|██████▊   | 698/1024 [00:11<00:06, 49.02it/s, est. speed input: 60889.59 toks/s, output: 59.46 toks/s]
Processed prompts:  69%|██████▉   | 706/1024 [00:11<00:06, 49.04it/s, est. speed input: 60744.03 toks/s, output: 59.32 toks/s]
Processed prompts:  70%|██████▉   | 714/1024 [00:12<00:06, 48.94it/s, est. speed input: 60596.23 toks/s, output: 59.18 toks/s]
Processed prompts:  71%|███████   | 722/1024 [00:12<00:06, 48.87it/s, est. speed input: 60452.07 toks/s, output: 59.04 toks/s]
Processed prompts:  71%|███████▏  | 730/1024 [00:12<00:06, 48.90it/s, est. speed input: 60316.00 toks/s, output: 58.90 toks/s]
Processed prompts:  72%|███████▏  | 738/1024 [00:12<00:05, 48.94it/s, est. speed input: 60184.91 toks/s, output: 58.77 toks/s]
Processed prompts:  73%|███████▎  | 746/1024 [00:12<00:05, 49.07it/s, est. speed input: 60062.07 toks/s, output: 58.65 toks/s]
Processed prompts:  74%|███████▎  | 754/1024 [00:12<00:05, 49.18it/s, est. speed input: 59943.63 toks/s, output: 58.54 toks/s]
Processed prompts:  74%|███████▍  | 762/1024 [00:13<00:05, 49.07it/s, est. speed input: 59818.70 toks/s, output: 58.42 toks/s]
Processed prompts:  75%|███████▌  | 770/1024 [00:13<00:05, 49.13it/s, est. speed input: 59703.30 toks/s, output: 58.30 toks/s]
Processed prompts:  76%|███████▌  | 778/1024 [00:13<00:05, 49.12it/s, est. speed input: 59588.50 toks/s, output: 58.19 toks/s]
Processed prompts:  77%|███████▋  | 786/1024 [00:13<00:04, 50.80it/s, est. speed input: 59555.48 toks/s, output: 58.16 toks/s]
Processed prompts:  78%|███████▊  | 794/1024 [00:13<00:04, 50.37it/s, est. speed input: 59449.25 toks/s, output: 58.06 toks/s]
Processed prompts:  78%|███████▊  | 802/1024 [00:13<00:04, 50.21it/s, est. speed input: 59351.68 toks/s, output: 57.96 toks/s]
Processed prompts:  79%|███████▉  | 810/1024 [00:13<00:04, 50.04it/s, est. speed input: 59253.65 toks/s, output: 57.86 toks/s]
Processed prompts:  80%|███████▉  | 818/1024 [00:14<00:04, 49.81it/s, est. speed input: 59153.00 toks/s, output: 57.77 toks/s]
Processed prompts:  81%|████████  | 826/1024 [00:14<00:03, 49.60it/s, est. speed input: 59052.16 toks/s, output: 57.67 toks/s]
Processed prompts:  81%|████████▏ | 834/1024 [00:14<00:03, 49.38it/s, est. speed input: 58950.60 toks/s, output: 57.57 toks/s]
Processed prompts:  82%|████████▏ | 842/1024 [00:14<00:03, 49.33it/s, est. speed input: 58855.67 toks/s, output: 57.48 toks/s]
Processed prompts:  83%|████████▎ | 850/1024 [00:14<00:03, 49.30it/s, est. speed input: 58762.81 toks/s, output: 57.39 toks/s]
Processed prompts:  84%|████████▍ | 858/1024 [00:14<00:03, 49.29it/s, est. speed input: 58672.94 toks/s, output: 57.30 toks/s]
Processed prompts:  85%|████████▍ | 866/1024 [00:15<00:03, 49.25it/s, est. speed input: 58583.22 toks/s, output: 57.21 toks/s]
Processed prompts:  85%|████████▌ | 874/1024 [00:15<00:03, 49.34it/s, est. speed input: 58500.37 toks/s, output: 57.13 toks/s]
Processed prompts:  86%|████████▌ | 882/1024 [00:15<00:02, 49.30it/s, est. speed input: 58414.99 toks/s, output: 57.05 toks/s]
Processed prompts:  87%|████████▋ | 890/1024 [00:15<00:02, 49.22it/s, est. speed input: 58329.57 toks/s, output: 56.96 toks/s]
Processed prompts:  88%|████████▊ | 898/1024 [00:15<00:02, 49.25it/s, est. speed input: 58249.06 toks/s, output: 56.88 toks/s]
Processed prompts:  88%|████████▊ | 906/1024 [00:15<00:02, 49.23it/s, est. speed input: 58168.73 toks/s, output: 56.81 toks/s]
Processed prompts:  89%|████████▉ | 914/1024 [00:16<00:02, 49.20it/s, est. speed input: 58089.07 toks/s, output: 56.73 toks/s]
Processed prompts:  90%|█████████ | 922/1024 [00:16<00:02, 49.21it/s, est. speed input: 58012.54 toks/s, output: 56.65 toks/s]
Processed prompts:  91%|█████████ | 930/1024 [00:16<00:01, 49.29it/s, est. speed input: 57940.15 toks/s, output: 56.58 toks/s]
Processed prompts:  92%|█████████▏| 938/1024 [00:16<00:01, 49.36it/s, est. speed input: 57869.94 toks/s, output: 56.51 toks/s]
Processed prompts:  92%|█████████▏| 946/1024 [00:16<00:01, 49.33it/s, est. speed input: 57797.96 toks/s, output: 56.44 toks/s]
Processed prompts:  93%|█████████▎| 954/1024 [00:16<00:01, 49.17it/s, est. speed input: 57722.06 toks/s, output: 56.37 toks/s]
Processed prompts:  94%|█████████▍| 962/1024 [00:17<00:01, 49.15it/s, est. speed input: 57651.02 toks/s, output: 56.30 toks/s]
Processed prompts:  95%|█████████▍| 970/1024 [00:17<00:01, 49.12it/s, est. speed input: 57580.99 toks/s, output: 56.23 toks/s]
Processed prompts:  96%|█████████▌| 978/1024 [00:17<00:00, 49.17it/s, est. speed input: 57514.49 toks/s, output: 56.17 toks/s]
Processed prompts:  96%|█████████▋| 986/1024 [00:17<00:00, 49.21it/s, est. speed input: 57449.80 toks/s, output: 56.10 toks/s]
Processed prompts:  97%|█████████▋| 994/1024 [00:17<00:00, 49.37it/s, est. speed input: 57390.57 toks/s, output: 56.05 toks/s]
Processed prompts:  98%|█████████▊| 1002/1024 [00:17<00:00, 49.34it/s, est. speed input: 57327.72 toks/s, output: 55.98 toks/s]
Processed prompts:  99%|█████████▊| 1010/1024 [00:18<00:00, 49.30it/s, est. speed input: 57265.08 toks/s, output: 55.92 toks/s]
Processed prompts:  99%|█████████▉| 1018/1024 [00:18<00:00, 50.81it/s, est. speed input: 57255.33 toks/s, output: 55.91 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:18<00:00, 50.81it/s, est. speed input: 57591.50 toks/s, output: 56.24 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:18<00:00, 56.24it/s, est. speed input: 57591.50 toks/s, output: 56.24 toks/s]
[rank0]:[W126 11:57:34.301973125 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 64.8s

测试结果:
  Requests/s:   49.08
  Tokens/s:     50307.87
  Total Reqs:   1024
  Elapsed:      20.86s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     50258.79

============================================================
[6/8] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 11:57:53 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1322990) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1322990) WARNING 01-26 11:58:11 [backends.py:609] Failed to read file <frozen os>
Throughput: 50.40 requests/s, 51662.66 total tokens/s, 50.40 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-26 11:57:53] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:57:53] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 11:57:53] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 11:57:53] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:57:53] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:57:53] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:57:53] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:57:53] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:57:53] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 11:57:53] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:57:53] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:57:53] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:57:53] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:57:53] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 11:58:00] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:58:00] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 11:58:00] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 11:58:00] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:58:00] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:58:00] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:58:00] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:58:00] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:58:00] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 11:58:00] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:58:00] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:58:00] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:58:00] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:58:00] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1322990) [2026-01-26 11:58:02] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1322990) [2026-01-26 11:58:02] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1322990) [2026-01-26 11:58:02] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1322990) [2026-01-26 11:58:02] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=1322990) [2026-01-26 11:58:02] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=1322990) [2026-01-26 11:58:02] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=1322990) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1322990) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.39it/s]
(EngineCore_DP0 pid=1322990) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.87it/s]
(EngineCore_DP0 pid=1322990) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.93it/s]
(EngineCore_DP0 pid=1322990) 
(EngineCore_DP0 pid=1322990) [2026-01-26 11:58:04] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 3584] -> 1D uint8
(EngineCore_DP0 pid=1322990) [2026-01-26 11:58:04] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10321920 bytes
(EngineCore_DP0 pid=1322990) [2026-01-26 11:58:04] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 3584] -> 1D uint8
(EngineCore_DP0 pid=1322990) [2026-01-26 11:58:04] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8028160 bytes
(EngineCore_DP0 pid=1322990) [2026-01-26 11:58:04] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 3584] -> 1D uint8
(EngineCore_DP0 pid=1322990) [2026-01-26 11:58:04] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 84869120 bytes
(EngineCore_DP0 pid=1322990) [2026-01-26 11:58:04] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 18944] -> 1D uint8
(EngineCore_DP0 pid=1322990) [2026-01-26 11:58:04] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 42434560 bytes
(EngineCore_DP0 pid=1322990) [rank0]:W0126 11:58:16.674000 1322990 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1322990) [rank0]:W0126 11:58:16.753000 1322990 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1322990) [rank0]:W0126 11:58:17.719000 1322990 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1322990) [rank0]:W0126 11:58:17.850000 1322990 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1322990) 2026-01-26 11:58:21,952 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1322990) 2026-01-26 11:58:21,978 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1322990) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 1/7 [00:00<00:04,  1.44it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:02,  2.25it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:01<00:00,  4.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:01<00:00,  7.66it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:01<00:00,  5.29it/s]
(EngineCore_DP0 pid=1322990) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00, 19.29it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 20.12it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 20.00it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   1%|          | 25/2048 [00:00<00:08, 242.74it/s]
Adding requests:   3%|▎         | 65/2048 [00:00<00:05, 331.39it/s]
Adding requests:   5%|▍         | 101/2048 [00:00<00:05, 341.18it/s]
Adding requests:   7%|▋         | 137/2048 [00:00<00:05, 347.81it/s]
Adding requests:   9%|▊         | 176/2048 [00:00<00:05, 361.87it/s]
Adding requests:  11%|█         | 217/2048 [00:00<00:04, 376.89it/s]
Adding requests:  12%|█▎        | 256/2048 [00:00<00:04, 378.75it/s]
Adding requests:  14%|█▍        | 296/2048 [00:00<00:04, 383.80it/s]
Adding requests:  17%|█▋        | 338/2048 [00:00<00:04, 392.11it/s]
Adding requests:  18%|█▊        | 378/2048 [00:01<00:04, 394.25it/s]
Adding requests:  21%|██        | 421/2048 [00:01<00:04, 402.18it/s]
Adding requests:  23%|██▎       | 462/2048 [00:01<00:03, 397.63it/s]
Adding requests:  25%|██▍       | 504/2048 [00:01<00:03, 404.14it/s]
Adding requests:  27%|██▋       | 547/2048 [00:01<00:03, 408.46it/s]
Adding requests:  29%|██▊       | 588/2048 [00:01<00:03, 402.80it/s]
Adding requests:  31%|███       | 629/2048 [00:01<00:03, 398.40it/s]
Adding requests:  33%|███▎      | 669/2048 [00:01<00:03, 387.32it/s]
Adding requests:  35%|███▍      | 710/2048 [00:01<00:03, 393.41it/s]
Adding requests:  37%|███▋      | 750/2048 [00:01<00:03, 381.39it/s]
Adding requests:  39%|███▊      | 789/2048 [00:02<00:03, 379.91it/s]
Adding requests:  40%|████      | 829/2048 [00:02<00:03, 385.13it/s]
Adding requests:  42%|████▏     | 869/2048 [00:02<00:03, 388.85it/s]
Adding requests:  44%|████▍     | 909/2048 [00:02<00:02, 391.92it/s]
Adding requests:  46%|████▋     | 949/2048 [00:02<00:02, 383.54it/s]
Adding requests:  48%|████▊     | 989/2048 [00:02<00:02, 386.32it/s]
Adding requests:  50%|█████     | 1028/2048 [00:02<00:02, 383.87it/s]
Adding requests:  52%|█████▏    | 1067/2048 [00:02<00:02, 381.20it/s]
Adding requests:  54%|█████▍    | 1106/2048 [00:02<00:02, 379.47it/s]
Adding requests:  56%|█████▌    | 1147/2048 [00:02<00:02, 386.12it/s]
Adding requests:  58%|█████▊    | 1186/2048 [00:03<00:02, 384.78it/s]
Adding requests:  60%|█████▉    | 1227/2048 [00:03<00:02, 390.25it/s]
Adding requests:  62%|██████▏   | 1267/2048 [00:03<00:02, 388.09it/s]
Adding requests:  64%|██████▍   | 1306/2048 [00:03<00:01, 384.71it/s]
Adding requests:  66%|██████▌   | 1345/2048 [00:03<00:01, 385.07it/s]
Adding requests:  68%|██████▊   | 1385/2048 [00:03<00:01, 388.41it/s]
Adding requests:  70%|██████▉   | 1424/2048 [00:03<00:01, 384.12it/s]
Adding requests:  71%|███████▏  | 1464/2048 [00:03<00:01, 388.15it/s]
Adding requests:  73%|███████▎  | 1505/2048 [00:03<00:01, 391.99it/s]
Adding requests:  75%|███████▌  | 1545/2048 [00:04<00:01, 390.34it/s]
Adding requests:  77%|███████▋  | 1585/2048 [00:04<00:01, 382.94it/s]
Adding requests:  79%|███████▉  | 1624/2048 [00:04<00:01, 377.27it/s]
Adding requests:  81%|████████  | 1662/2048 [00:04<00:01, 372.40it/s]
Adding requests:  83%|████████▎ | 1701/2048 [00:04<00:00, 376.26it/s]
Adding requests:  85%|████████▌ | 1741/2048 [00:04<00:00, 380.84it/s]
Adding requests:  87%|████████▋ | 1782/2048 [00:04<00:00, 387.78it/s]
Adding requests:  89%|████████▉ | 1821/2048 [00:04<00:00, 383.05it/s]
Adding requests:  91%|█████████ | 1861/2048 [00:04<00:00, 387.30it/s]
Adding requests:  93%|█████████▎| 1900/2048 [00:04<00:00, 386.50it/s]
Adding requests:  95%|█████████▍| 1939/2048 [00:05<00:00, 385.80it/s]
Adding requests:  97%|█████████▋| 1979/2048 [00:05<00:00, 389.42it/s]
Adding requests:  99%|█████████▊| 2018/2048 [00:05<00:00, 379.82it/s]
Adding requests: 100%|██████████| 2048/2048 [00:05<00:00, 383.88it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  13%|█▎        | 269/2048 [00:00<00:02, 742.03it/s, est. speed input: 759887.22 toks/s, output: 742.04 toks/s]
Processed prompts:  17%|█▋        | 344/2048 [00:01<00:09, 174.41it/s, est. speed input: 217657.09 toks/s, output: 212.55 toks/s]
Processed prompts:  19%|█▊        | 379/2048 [00:02<00:12, 129.18it/s, est. speed input: 171770.08 toks/s, output: 167.74 toks/s]
Processed prompts:  20%|█▉        | 401/2048 [00:02<00:17, 96.10it/s, est. speed input: 141727.87 toks/s, output: 138.41 toks/s] 
Processed prompts:  20%|██        | 416/2048 [00:03<00:18, 86.29it/s, est. speed input: 132437.73 toks/s, output: 129.33 toks/s]
Processed prompts:  21%|██        | 429/2048 [00:03<00:20, 77.22it/s, est. speed input: 124935.90 toks/s, output: 122.01 toks/s]
Processed prompts:  22%|██▏       | 445/2048 [00:03<00:22, 70.62it/s, est. speed input: 118804.01 toks/s, output: 116.02 toks/s]
Processed prompts:  23%|██▎       | 461/2048 [00:04<00:24, 65.25it/s, est. speed input: 113579.74 toks/s, output: 110.92 toks/s]
Processed prompts:  23%|██▎       | 477/2048 [00:04<00:25, 61.25it/s, est. speed input: 109169.91 toks/s, output: 106.61 toks/s]
Processed prompts:  24%|██▍       | 493/2048 [00:04<00:26, 58.19it/s, est. speed input: 105332.34 toks/s, output: 102.86 toks/s]
Processed prompts:  25%|██▍       | 509/2048 [00:05<00:27, 55.93it/s, est. speed input: 101973.37 toks/s, output: 99.58 toks/s] 
Processed prompts:  26%|██▌       | 525/2048 [00:05<00:28, 54.15it/s, est. speed input: 98959.48 toks/s, output: 96.64 toks/s] 
Processed prompts:  26%|██▋       | 541/2048 [00:05<00:28, 52.97it/s, est. speed input: 96312.53 toks/s, output: 94.05 toks/s]
Processed prompts:  27%|██▋       | 557/2048 [00:06<00:28, 52.19it/s, est. speed input: 93967.61 toks/s, output: 91.76 toks/s]
Processed prompts:  28%|██▊       | 573/2048 [00:06<00:28, 51.59it/s, est. speed input: 91839.90 toks/s, output: 89.69 toks/s]
Processed prompts:  29%|██▉       | 589/2048 [00:06<00:28, 51.16it/s, est. speed input: 89912.40 toks/s, output: 87.80 toks/s]
Processed prompts:  30%|██▉       | 605/2048 [00:07<00:28, 50.84it/s, est. speed input: 88156.24 toks/s, output: 86.09 toks/s]
Processed prompts:  30%|███       | 621/2048 [00:07<00:28, 50.57it/s, est. speed input: 86541.69 toks/s, output: 84.51 toks/s]
Processed prompts:  31%|███       | 637/2048 [00:07<00:27, 50.54it/s, est. speed input: 85100.24 toks/s, output: 83.11 toks/s]
Processed prompts:  32%|███▏      | 653/2048 [00:07<00:27, 50.48it/s, est. speed input: 83764.40 toks/s, output: 81.80 toks/s]
Processed prompts:  33%|███▎      | 669/2048 [00:08<00:27, 50.40it/s, est. speed input: 82522.72 toks/s, output: 80.59 toks/s]
Processed prompts:  33%|███▎      | 685/2048 [00:08<00:27, 50.41it/s, est. speed input: 81386.80 toks/s, output: 79.48 toks/s]
Processed prompts:  34%|███▍      | 701/2048 [00:08<00:26, 50.38it/s, est. speed input: 80324.02 toks/s, output: 78.44 toks/s]
Processed prompts:  35%|███▌      | 717/2048 [00:09<00:26, 50.34it/s, est. speed input: 79329.53 toks/s, output: 77.47 toks/s]
Processed prompts:  36%|███▌      | 733/2048 [00:09<00:26, 50.40it/s, est. speed input: 78418.77 toks/s, output: 76.58 toks/s]
Processed prompts:  37%|███▋      | 749/2048 [00:09<00:25, 50.46it/s, est. speed input: 77566.99 toks/s, output: 75.75 toks/s]
Processed prompts:  37%|███▋      | 765/2048 [00:10<00:25, 50.44it/s, est. speed input: 76758.87 toks/s, output: 74.96 toks/s]
Processed prompts:  38%|███▊      | 781/2048 [00:10<00:24, 51.28it/s, est. speed input: 76127.65 toks/s, output: 74.34 toks/s]
Processed prompts:  39%|███▉      | 797/2048 [00:10<00:24, 51.04it/s, est. speed input: 75412.51 toks/s, output: 73.64 toks/s]
Processed prompts:  40%|███▉      | 813/2048 [00:11<00:24, 50.84it/s, est. speed input: 74733.72 toks/s, output: 72.98 toks/s]
Processed prompts:  40%|████      | 829/2048 [00:11<00:24, 50.63it/s, est. speed input: 74081.75 toks/s, output: 72.35 toks/s]
Processed prompts:  41%|████▏     | 845/2048 [00:11<00:23, 50.51it/s, est. speed input: 73469.12 toks/s, output: 71.75 toks/s]
Processed prompts:  42%|████▏     | 861/2048 [00:12<00:23, 50.51it/s, est. speed input: 72899.42 toks/s, output: 71.19 toks/s]
Processed prompts:  43%|████▎     | 877/2048 [00:12<00:23, 50.45it/s, est. speed input: 72352.31 toks/s, output: 70.66 toks/s]
Processed prompts:  44%|████▎     | 893/2048 [00:12<00:22, 50.40it/s, est. speed input: 71830.24 toks/s, output: 70.15 toks/s]
Processed prompts:  44%|████▍     | 909/2048 [00:13<00:22, 50.46it/s, est. speed input: 71345.41 toks/s, output: 69.67 toks/s]
Processed prompts:  45%|████▌     | 925/2048 [00:13<00:22, 50.42it/s, est. speed input: 70874.47 toks/s, output: 69.21 toks/s]
Processed prompts:  46%|████▌     | 941/2048 [00:13<00:22, 50.26it/s, est. speed input: 70410.20 toks/s, output: 68.76 toks/s]
Processed prompts:  47%|████▋     | 957/2048 [00:14<00:21, 50.32it/s, est. speed input: 69986.35 toks/s, output: 68.35 toks/s]
Processed prompts:  48%|████▊     | 973/2048 [00:14<00:21, 50.28it/s, est. speed input: 69572.00 toks/s, output: 67.94 toks/s]
Processed prompts:  48%|████▊     | 989/2048 [00:14<00:21, 50.18it/s, est. speed input: 69168.95 toks/s, output: 67.55 toks/s]
Processed prompts:  49%|████▉     | 1005/2048 [00:14<00:20, 50.21it/s, est. speed input: 68793.19 toks/s, output: 67.18 toks/s]
Processed prompts:  50%|████▉     | 1021/2048 [00:15<00:20, 50.19it/s, est. speed input: 68428.22 toks/s, output: 66.82 toks/s]
Processed prompts:  51%|█████     | 1037/2048 [00:15<00:20, 50.10it/s, est. speed input: 68072.06 toks/s, output: 66.48 toks/s]
Processed prompts:  51%|█████▏    | 1053/2048 [00:15<00:19, 50.14it/s, est. speed input: 67739.14 toks/s, output: 66.15 toks/s]
Processed prompts:  52%|█████▏    | 1069/2048 [00:16<00:19, 50.15it/s, est. speed input: 67418.09 toks/s, output: 65.84 toks/s]
Processed prompts:  53%|█████▎    | 1085/2048 [00:16<00:19, 50.15it/s, est. speed input: 67108.33 toks/s, output: 65.54 toks/s]
Processed prompts:  54%|█████▍    | 1101/2048 [00:16<00:18, 50.18it/s, est. speed input: 66813.08 toks/s, output: 65.25 toks/s]
Processed prompts:  55%|█████▍    | 1117/2048 [00:17<00:18, 50.26it/s, est. speed input: 66533.16 toks/s, output: 64.97 toks/s]
Processed prompts:  55%|█████▌    | 1133/2048 [00:17<00:18, 50.25it/s, est. speed input: 66259.00 toks/s, output: 64.71 toks/s]
Processed prompts:  56%|█████▌    | 1149/2048 [00:17<00:17, 50.19it/s, est. speed input: 65989.35 toks/s, output: 64.44 toks/s]
Processed prompts:  57%|█████▋    | 1165/2048 [00:18<00:17, 50.23it/s, est. speed input: 65736.50 toks/s, output: 64.20 toks/s]
Processed prompts:  58%|█████▊    | 1181/2048 [00:18<00:17, 50.16it/s, est. speed input: 65484.77 toks/s, output: 63.95 toks/s]
Processed prompts:  58%|█████▊    | 1197/2048 [00:18<00:16, 51.06it/s, est. speed input: 65310.19 toks/s, output: 63.78 toks/s]
Processed prompts:  59%|█████▉    | 1213/2048 [00:19<00:16, 50.84it/s, est. speed input: 65080.52 toks/s, output: 63.56 toks/s]
Processed prompts:  60%|██████    | 1229/2048 [00:19<00:15, 51.52it/s, est. speed input: 64915.47 toks/s, output: 63.39 toks/s]
Processed prompts:  61%|██████    | 1245/2048 [00:19<00:15, 51.08it/s, est. speed input: 64694.64 toks/s, output: 63.18 toks/s]
Processed prompts:  62%|██████▏   | 1261/2048 [00:20<00:15, 50.82it/s, est. speed input: 64483.56 toks/s, output: 62.97 toks/s]
Processed prompts:  62%|██████▏   | 1277/2048 [00:20<00:15, 50.57it/s, est. speed input: 64274.88 toks/s, output: 62.77 toks/s]
Processed prompts:  63%|██████▎   | 1293/2048 [00:20<00:14, 50.42it/s, est. speed input: 64073.77 toks/s, output: 62.57 toks/s]
Processed prompts:  64%|██████▍   | 1309/2048 [00:20<00:14, 50.32it/s, est. speed input: 63878.95 toks/s, output: 62.38 toks/s]
Processed prompts:  65%|██████▍   | 1325/2048 [00:21<00:14, 51.19it/s, est. speed input: 63748.69 toks/s, output: 62.25 toks/s]
Processed prompts:  65%|██████▌   | 1341/2048 [00:21<00:13, 50.80it/s, est. speed input: 63561.01 toks/s, output: 62.07 toks/s]
Processed prompts:  66%|██████▋   | 1357/2048 [00:21<00:13, 50.53it/s, est. speed input: 63379.27 toks/s, output: 61.89 toks/s]
Processed prompts:  67%|██████▋   | 1373/2048 [00:22<00:13, 50.41it/s, est. speed input: 63206.24 toks/s, output: 61.72 toks/s]
Processed prompts:  68%|██████▊   | 1389/2048 [00:22<00:13, 50.27it/s, est. speed input: 63034.79 toks/s, output: 61.56 toks/s]
Processed prompts:  69%|██████▊   | 1405/2048 [00:22<00:12, 50.15it/s, est. speed input: 62867.47 toks/s, output: 61.39 toks/s]
Processed prompts:  69%|██████▉   | 1421/2048 [00:23<00:12, 50.18it/s, est. speed input: 62710.49 toks/s, output: 61.24 toks/s]
Processed prompts:  70%|███████   | 1437/2048 [00:23<00:12, 50.17it/s, est. speed input: 62556.40 toks/s, output: 61.09 toks/s]
Processed prompts:  71%|███████   | 1453/2048 [00:23<00:11, 50.94it/s, est. speed input: 62448.86 toks/s, output: 60.99 toks/s]
Processed prompts:  72%|███████▏  | 1469/2048 [00:24<00:11, 50.68it/s, est. speed input: 62301.18 toks/s, output: 60.84 toks/s]
Processed prompts:  73%|███████▎  | 1485/2048 [00:24<00:11, 50.51it/s, est. speed input: 62158.08 toks/s, output: 60.70 toks/s]
Processed prompts:  73%|███████▎  | 1501/2048 [00:24<00:10, 50.31it/s, est. speed input: 62014.29 toks/s, output: 60.56 toks/s]
Processed prompts:  74%|███████▍  | 1517/2048 [00:25<00:10, 51.07it/s, est. speed input: 61920.29 toks/s, output: 60.47 toks/s]
Processed prompts:  75%|███████▍  | 1533/2048 [00:25<00:10, 50.82it/s, est. speed input: 61789.09 toks/s, output: 60.34 toks/s]
Processed prompts:  76%|███████▌  | 1549/2048 [00:25<00:09, 50.53it/s, est. speed input: 61655.24 toks/s, output: 60.21 toks/s]
Processed prompts:  76%|███████▋  | 1565/2048 [00:26<00:09, 51.26it/s, est. speed input: 61570.01 toks/s, output: 60.13 toks/s]
Processed prompts:  77%|███████▋  | 1581/2048 [00:26<00:09, 50.92it/s, est. speed input: 61446.32 toks/s, output: 60.01 toks/s]
Processed prompts:  78%|███████▊  | 1597/2048 [00:26<00:08, 50.66it/s, est. speed input: 61324.10 toks/s, output: 59.89 toks/s]
Processed prompts:  79%|███████▉  | 1613/2048 [00:26<00:08, 51.36it/s, est. speed input: 61246.01 toks/s, output: 59.81 toks/s]
Processed prompts:  80%|███████▉  | 1629/2048 [00:27<00:08, 50.98it/s, est. speed input: 61130.09 toks/s, output: 59.70 toks/s]
Processed prompts:  80%|████████  | 1645/2048 [00:27<00:07, 50.67it/s, est. speed input: 61014.52 toks/s, output: 59.58 toks/s]
Processed prompts:  81%|████████  | 1661/2048 [00:27<00:07, 50.49it/s, est. speed input: 60903.23 toks/s, output: 59.48 toks/s]
Processed prompts:  82%|████████▏ | 1677/2048 [00:28<00:07, 50.30it/s, est. speed input: 60791.06 toks/s, output: 59.37 toks/s]
Processed prompts:  83%|████████▎ | 1693/2048 [00:28<00:07, 50.21it/s, est. speed input: 60683.56 toks/s, output: 59.26 toks/s]
Processed prompts:  83%|████████▎ | 1709/2048 [00:28<00:06, 50.14it/s, est. speed input: 60578.54 toks/s, output: 59.16 toks/s]
Processed prompts:  84%|████████▍ | 1725/2048 [00:29<00:06, 50.10it/s, est. speed input: 60475.57 toks/s, output: 59.06 toks/s]
Processed prompts:  85%|████████▌ | 1741/2048 [00:29<00:06, 50.94it/s, est. speed input: 60412.36 toks/s, output: 59.00 toks/s]
Processed prompts:  86%|████████▌ | 1757/2048 [00:29<00:05, 51.62it/s, est. speed input: 60353.39 toks/s, output: 58.94 toks/s]
Processed prompts:  87%|████████▋ | 1773/2048 [00:30<00:05, 51.17it/s, est. speed input: 60258.10 toks/s, output: 58.85 toks/s]
Processed prompts:  87%|████████▋ | 1789/2048 [00:30<00:05, 50.80it/s, est. speed input: 60162.32 toks/s, output: 58.75 toks/s]
Processed prompts:  88%|████████▊ | 1805/2048 [00:30<00:04, 50.57it/s, est. speed input: 60069.76 toks/s, output: 58.66 toks/s]
Processed prompts:  89%|████████▉ | 1821/2048 [00:31<00:04, 50.46it/s, est. speed input: 59980.86 toks/s, output: 58.57 toks/s]
Processed prompts:  90%|████████▉ | 1837/2048 [00:31<00:04, 50.25it/s, est. speed input: 59888.76 toks/s, output: 58.49 toks/s]
Processed prompts:  90%|█████████ | 1853/2048 [00:31<00:03, 50.15it/s, est. speed input: 59799.97 toks/s, output: 58.40 toks/s]
Processed prompts:  91%|█████████▏| 1869/2048 [00:32<00:03, 50.15it/s, est. speed input: 59715.88 toks/s, output: 58.32 toks/s]
Processed prompts:  92%|█████████▏| 1885/2048 [00:32<00:03, 50.99it/s, est. speed input: 59665.90 toks/s, output: 58.27 toks/s]
Processed prompts:  93%|█████████▎| 1901/2048 [00:32<00:02, 50.64it/s, est. speed input: 59580.96 toks/s, output: 58.18 toks/s]
Processed prompts:  94%|█████████▎| 1917/2048 [00:32<00:02, 50.52it/s, est. speed input: 59502.56 toks/s, output: 58.11 toks/s]
Processed prompts:  94%|█████████▍| 1933/2048 [00:33<00:02, 50.35it/s, est. speed input: 59422.30 toks/s, output: 58.03 toks/s]
Processed prompts:  95%|█████████▌| 1949/2048 [00:33<00:01, 50.20it/s, est. speed input: 59342.58 toks/s, output: 57.95 toks/s]
Processed prompts:  96%|█████████▌| 1965/2048 [00:33<00:01, 50.14it/s, est. speed input: 59265.64 toks/s, output: 57.88 toks/s]
Processed prompts:  97%|█████████▋| 1981/2048 [00:34<00:01, 51.07it/s, est. speed input: 59225.37 toks/s, output: 57.84 toks/s]
Processed prompts:  98%|█████████▊| 1997/2048 [00:34<00:01, 50.70it/s, est. speed input: 59149.63 toks/s, output: 57.76 toks/s]
Processed prompts:  98%|█████████▊| 2013/2048 [00:34<00:00, 50.48it/s, est. speed input: 59076.34 toks/s, output: 57.69 toks/s]
Processed prompts:  99%|█████████▉| 2029/2048 [00:35<00:00, 50.43it/s, est. speed input: 59008.27 toks/s, output: 57.63 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:35<00:00, 50.43it/s, est. speed input: 59417.42 toks/s, output: 58.02 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:35<00:00, 58.02it/s, est. speed input: 59417.42 toks/s, output: 58.02 toks/s]
[rank0]:[W126 11:59:06.158888597 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 91.9s

测试结果:
  Requests/s:   50.40
  Tokens/s:     51662.66
  Total Reqs:   2048
  Elapsed:      40.63s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     51612.26

============================================================
[7/8] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 11:59:36 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1324848) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1324848) WARNING 01-26 11:59:54 [backends.py:609] Failed to read file <frozen os>
Throughput: 49.93 requests/s, 51179.11 total tokens/s, 49.93 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-26 11:59:35] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:59:36] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 11:59:36] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 11:59:36] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:59:36] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:59:36] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:59:36] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:59:36] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:59:36] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 11:59:36] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:59:36] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:59:36] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:59:36] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:59:36] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 11:59:43] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:59:43] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 11:59:43] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 11:59:43] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:59:43] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:59:43] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:59:43] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:59:43] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 11:59:43] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 11:59:43] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:59:43] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:59:43] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:59:43] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:59:43] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1324848) [2026-01-26 11:59:45] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1324848) [2026-01-26 11:59:45] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1324848) [2026-01-26 11:59:45] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1324848) [2026-01-26 11:59:45] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=1324848) [2026-01-26 11:59:45] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=1324848) [2026-01-26 11:59:45] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=1324848) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1324848) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.62it/s]
(EngineCore_DP0 pid=1324848) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  1.92it/s]
(EngineCore_DP0 pid=1324848) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  2.00it/s]
(EngineCore_DP0 pid=1324848) 
(EngineCore_DP0 pid=1324848) [2026-01-26 11:59:47] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 3584] -> 1D uint8
(EngineCore_DP0 pid=1324848) [2026-01-26 11:59:48] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10321920 bytes
(EngineCore_DP0 pid=1324848) [2026-01-26 11:59:48] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 3584] -> 1D uint8
(EngineCore_DP0 pid=1324848) [2026-01-26 11:59:48] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8028160 bytes
(EngineCore_DP0 pid=1324848) [2026-01-26 11:59:48] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 3584] -> 1D uint8
(EngineCore_DP0 pid=1324848) [2026-01-26 11:59:48] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 84869120 bytes
(EngineCore_DP0 pid=1324848) [2026-01-26 11:59:48] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 18944] -> 1D uint8
(EngineCore_DP0 pid=1324848) [2026-01-26 11:59:48] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 42434560 bytes
(EngineCore_DP0 pid=1324848) [rank0]:W0126 11:59:59.178000 1324848 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1324848) [rank0]:W0126 11:59:59.255000 1324848 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1324848) [rank0]:W0126 12:00:00.190000 1324848 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1324848) [rank0]:W0126 12:00:00.316000 1324848 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1324848) 2026-01-26 12:00:04,473 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1324848) 2026-01-26 12:00:04,499 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1324848) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▉         | 1/11 [00:00<00:02,  4.84it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:02,  3.80it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 3/11 [00:00<00:01,  4.26it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:00<00:01,  4.07it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 5/11 [00:01<00:01,  3.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▎   | 7/11 [00:01<00:00,  6.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:01<00:00,  6.66it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 10/11 [00:01<00:00,  6.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00,  7.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00,  5.81it/s]
(EngineCore_DP0 pid=1324848) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00, 18.78it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00, 19.46it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00, 14.15it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 15.15it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 25/4096 [00:00<00:16, 245.38it/s]
Adding requests:   2%|▏         | 65/4096 [00:00<00:12, 334.11it/s]
Adding requests:   2%|▏         | 101/4096 [00:00<00:11, 343.83it/s]
Adding requests:   3%|▎         | 139/4096 [00:00<00:11, 356.79it/s]
Adding requests:   4%|▍         | 178/4096 [00:00<00:10, 367.91it/s]
Adding requests:   5%|▌         | 220/4096 [00:00<00:10, 383.13it/s]
Adding requests:   6%|▋         | 259/4096 [00:00<00:10, 379.35it/s]
Adding requests:   7%|▋         | 299/4096 [00:00<00:09, 384.40it/s]
Adding requests:   8%|▊         | 339/4096 [00:00<00:09, 388.55it/s]
Adding requests:   9%|▉         | 379/4096 [00:01<00:09, 391.58it/s]
Adding requests:  10%|█         | 421/4096 [00:01<00:09, 397.13it/s]
Adding requests:  11%|█▏        | 461/4096 [00:01<00:09, 392.32it/s]
Adding requests:  12%|█▏        | 504/4096 [00:01<00:08, 400.10it/s]
Adding requests:  13%|█▎        | 546/4096 [00:01<00:08, 404.45it/s]
Adding requests:  14%|█▍        | 587/4096 [00:01<00:08, 399.98it/s]
Adding requests:  15%|█▌        | 628/4096 [00:01<00:08, 395.85it/s]
Adding requests:  16%|█▋        | 668/4096 [00:01<00:08, 385.45it/s]
Adding requests:  17%|█▋        | 709/4096 [00:01<00:08, 390.16it/s]
Adding requests:  18%|█▊        | 749/4096 [00:01<00:08, 381.90it/s]
Adding requests:  19%|█▉        | 789/4096 [00:02<00:08, 384.55it/s]
Adding requests:  20%|██        | 828/4096 [00:02<00:08, 386.05it/s]
Adding requests:  21%|██        | 868/4096 [00:02<00:08, 388.29it/s]
Adding requests:  22%|██▏       | 908/4096 [00:02<00:08, 391.21it/s]
Adding requests:  23%|██▎       | 948/4096 [00:02<00:08, 382.79it/s]
Adding requests:  24%|██▍       | 988/4096 [00:02<00:08, 384.56it/s]
Adding requests:  25%|██▌       | 1027/4096 [00:02<00:08, 379.18it/s]
Adding requests:  26%|██▌       | 1065/4096 [00:02<00:07, 378.95it/s]
Adding requests:  27%|██▋       | 1103/4096 [00:02<00:08, 368.41it/s]
Adding requests:  28%|██▊       | 1143/4096 [00:02<00:07, 375.17it/s]
Adding requests:  29%|██▉       | 1181/4096 [00:03<00:07, 376.47it/s]
Adding requests:  30%|██▉       | 1221/4096 [00:03<00:07, 382.22it/s]
Adding requests:  31%|███       | 1260/4096 [00:03<00:07, 380.38it/s]
Adding requests:  32%|███▏      | 1299/4096 [00:03<00:07, 377.83it/s]
Adding requests:  33%|███▎      | 1338/4096 [00:03<00:07, 378.90it/s]
Adding requests:  34%|███▎      | 1379/4096 [00:03<00:07, 385.01it/s]
Adding requests:  35%|███▍      | 1418/4096 [00:03<00:07, 381.14it/s]
Adding requests:  36%|███▌      | 1458/4096 [00:03<00:06, 386.24it/s]
Adding requests:  37%|███▋      | 1498/4096 [00:03<00:06, 390.22it/s]
Adding requests:  38%|███▊      | 1538/4096 [00:04<00:06, 389.22it/s]
Adding requests:  39%|███▊      | 1577/4096 [00:04<00:06, 381.75it/s]
Adding requests:  39%|███▉      | 1616/4096 [00:04<00:06, 378.76it/s]
Adding requests:  40%|████      | 1654/4096 [00:04<00:06, 371.69it/s]
Adding requests:  41%|████▏     | 1692/4096 [00:04<00:06, 373.47it/s]
Adding requests:  42%|████▏     | 1732/4096 [00:04<00:06, 378.45it/s]
Adding requests:  43%|████▎     | 1773/4096 [00:04<00:06, 385.18it/s]
Adding requests:  44%|████▍     | 1812/4096 [00:04<00:05, 383.49it/s]
Adding requests:  45%|████▌     | 1851/4096 [00:04<00:05, 383.33it/s]
Adding requests:  46%|████▌     | 1891/4096 [00:04<00:05, 387.32it/s]
Adding requests:  47%|████▋     | 1931/4096 [00:05<00:05, 389.84it/s]
Adding requests:  48%|████▊     | 1970/4096 [00:05<00:05, 389.86it/s]
Adding requests:  49%|████▉     | 2009/4096 [00:05<00:05, 385.79it/s]
Adding requests:  50%|█████     | 2048/4096 [00:05<00:05, 382.04it/s]
Adding requests:  51%|█████     | 2087/4096 [00:05<00:05, 371.85it/s]
Adding requests:  52%|█████▏    | 2128/4096 [00:05<00:05, 379.71it/s]
Adding requests:  53%|█████▎    | 2167/4096 [00:05<00:05, 376.28it/s]
Adding requests:  54%|█████▍    | 2205/4096 [00:05<00:05, 371.40it/s]
Adding requests:  55%|█████▍    | 2244/4096 [00:05<00:04, 376.00it/s]
Adding requests:  56%|█████▌    | 2285/4096 [00:05<00:04, 382.68it/s]
Adding requests:  57%|█████▋    | 2324/4096 [00:06<00:04, 377.06it/s]
Adding requests:  58%|█████▊    | 2364/4096 [00:06<00:04, 382.01it/s]
Adding requests:  59%|█████▊    | 2405/4096 [00:06<00:04, 387.64it/s]
Adding requests:  60%|█████▉    | 2445/4096 [00:06<00:04, 389.60it/s]
Adding requests:  61%|██████    | 2484/4096 [00:06<00:04, 388.65it/s]
Adding requests:  62%|██████▏   | 2525/4096 [00:06<00:04, 391.86it/s]
Adding requests:  63%|██████▎   | 2569/4096 [00:06<00:03, 404.95it/s]
Adding requests:  64%|██████▎   | 2610/4096 [00:06<00:03, 401.27it/s]
Adding requests:  65%|██████▍   | 2651/4096 [00:06<00:03, 390.29it/s]
Adding requests:  66%|██████▌   | 2691/4096 [00:07<00:03, 387.67it/s]
Adding requests:  67%|██████▋   | 2730/4096 [00:07<00:03, 384.00it/s]
Adding requests:  68%|██████▊   | 2771/4096 [00:07<00:03, 389.67it/s]
Adding requests:  69%|██████▊   | 2812/4096 [00:07<00:03, 395.33it/s]
Adding requests:  70%|██████▉   | 2853/4096 [00:07<00:03, 396.61it/s]
Adding requests:  71%|███████   | 2893/4096 [00:07<00:03, 394.79it/s]
Adding requests:  72%|███████▏  | 2933/4096 [00:07<00:02, 395.31it/s]
Adding requests:  73%|███████▎  | 2973/4096 [00:07<00:02, 395.62it/s]
Adding requests:  74%|███████▎  | 3015/4096 [00:07<00:02, 400.21it/s]
Adding requests:  75%|███████▍  | 3056/4096 [00:07<00:02, 400.88it/s]
Adding requests:  76%|███████▌  | 3097/4096 [00:08<00:02, 401.30it/s]
Adding requests:  77%|███████▋  | 3138/4096 [00:08<00:02, 403.43it/s]
Adding requests:  78%|███████▊  | 3179/4096 [00:08<00:02, 394.33it/s]
Adding requests:  79%|███████▊  | 3219/4096 [00:08<00:02, 393.91it/s]
Adding requests:  80%|███████▉  | 3259/4096 [00:08<00:02, 395.63it/s]
Adding requests:  81%|████████  | 3299/4096 [00:08<00:02, 382.02it/s]
Adding requests:  81%|████████▏ | 3338/4096 [00:08<00:01, 383.81it/s]
Adding requests:  82%|████████▏ | 3379/4096 [00:08<00:01, 388.38it/s]
Adding requests:  83%|████████▎ | 3418/4096 [00:08<00:01, 387.42it/s]
Adding requests:  84%|████████▍ | 3459/4096 [00:08<00:01, 390.87it/s]
Adding requests:  85%|████████▌ | 3499/4096 [00:09<00:01, 387.99it/s]
Adding requests:  86%|████████▋ | 3542/4096 [00:09<00:01, 398.30it/s]
Adding requests:  87%|████████▋ | 3582/4096 [00:09<00:01, 396.52it/s]
Adding requests:  88%|████████▊ | 3622/4096 [00:09<00:01, 397.28it/s]
Adding requests:  89%|████████▉ | 3662/4096 [00:09<00:01, 380.84it/s]
Adding requests:  90%|█████████ | 3701/4096 [00:09<00:01, 380.60it/s]
Adding requests:  91%|█████████▏| 3740/4096 [00:09<00:00, 382.83it/s]
Adding requests:  92%|█████████▏| 3779/4096 [00:09<00:00, 372.87it/s]
Adding requests:  93%|█████████▎| 3817/4096 [00:09<00:00, 363.94it/s]
Adding requests:  94%|█████████▍| 3855/4096 [00:10<00:00, 368.46it/s]
Adding requests:  95%|█████████▌| 3893/4096 [00:10<00:00, 369.47it/s]
Adding requests:  96%|█████████▌| 3931/4096 [00:10<00:00, 366.55it/s]
Adding requests:  97%|█████████▋| 3969/4096 [00:10<00:00, 370.07it/s]
Adding requests:  98%|█████████▊| 4007/4096 [00:10<00:00, 370.40it/s]
Adding requests:  99%|█████████▉| 4045/4096 [00:10<00:00, 370.23it/s]
Adding requests: 100%|█████████▉| 4083/4096 [00:10<00:00, 371.21it/s]
Adding requests: 100%|██████████| 4096/4096 [00:10<00:00, 383.72it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  12%|█▏        | 509/4096 [00:00<00:04, 774.49it/s, est. speed input: 793103.83 toks/s, output: 774.50 toks/s]
Processed prompts:  14%|█▍        | 587/4096 [00:01<00:13, 251.87it/s, est. speed input: 312831.13 toks/s, output: 305.50 toks/s]
Processed prompts:  15%|█▌        | 623/4096 [00:02<00:18, 184.51it/s, est. speed input: 249467.59 toks/s, output: 243.62 toks/s]
Processed prompts:  16%|█▌        | 645/4096 [00:03<00:25, 135.18it/s, est. speed input: 207001.81 toks/s, output: 202.15 toks/s]
Processed prompts:  16%|█▋        | 669/4096 [00:03<00:32, 104.11it/s, est. speed input: 179195.26 toks/s, output: 174.99 toks/s]
Processed prompts:  17%|█▋        | 701/4096 [00:04<00:38, 87.22it/s, est. speed input: 161018.97 toks/s, output: 157.24 toks/s] 
Processed prompts:  18%|█▊        | 733/4096 [00:05<00:44, 75.87it/s, est. speed input: 147421.30 toks/s, output: 143.96 toks/s]
Processed prompts:  19%|█▊        | 765/4096 [00:05<00:48, 68.58it/s, est. speed input: 137179.55 toks/s, output: 133.96 toks/s]
Processed prompts:  19%|█▉        | 797/4096 [00:06<00:52, 63.01it/s, est. speed input: 128617.89 toks/s, output: 125.60 toks/s]
Processed prompts:  20%|██        | 829/4096 [00:06<00:55, 59.19it/s, est. speed input: 121625.13 toks/s, output: 118.77 toks/s]
Processed prompts:  21%|██        | 861/4096 [00:07<00:57, 56.49it/s, est. speed input: 115766.05 toks/s, output: 113.05 toks/s]
Processed prompts:  22%|██▏       | 893/4096 [00:08<00:58, 54.68it/s, est. speed input: 110845.47 toks/s, output: 108.25 toks/s]
Processed prompts:  23%|██▎       | 925/4096 [00:08<00:59, 53.38it/s, est. speed input: 106610.40 toks/s, output: 104.11 toks/s]
Processed prompts:  23%|██▎       | 957/4096 [00:09<00:59, 52.44it/s, est. speed input: 102922.46 toks/s, output: 100.51 toks/s]
Processed prompts:  24%|██▍       | 989/4096 [00:10<01:00, 51.76it/s, est. speed input: 99689.63 toks/s, output: 97.35 toks/s]  
Processed prompts:  25%|██▍       | 1021/4096 [00:10<00:59, 51.33it/s, est. speed input: 96850.77 toks/s, output: 94.58 toks/s]
Processed prompts:  26%|██▌       | 1053/4096 [00:11<00:59, 51.03it/s, est. speed input: 94330.72 toks/s, output: 92.12 toks/s]
Processed prompts:  26%|██▋       | 1085/4096 [00:12<00:59, 50.85it/s, est. speed input: 92086.48 toks/s, output: 89.93 toks/s]
Processed prompts:  27%|██▋       | 1117/4096 [00:12<00:58, 50.65it/s, est. speed input: 90044.10 toks/s, output: 87.93 toks/s]
Processed prompts:  28%|██▊       | 1149/4096 [00:13<00:58, 50.54it/s, est. speed input: 88205.34 toks/s, output: 86.14 toks/s]
Processed prompts:  29%|██▉       | 1181/4096 [00:13<00:57, 50.80it/s, est. speed input: 86621.12 toks/s, output: 84.59 toks/s]
Processed prompts:  30%|██▉       | 1213/4096 [00:14<00:56, 50.92it/s, est. speed input: 85155.53 toks/s, output: 83.16 toks/s]
Processed prompts:  30%|███       | 1245/4096 [00:15<00:56, 50.76it/s, est. speed input: 83756.45 toks/s, output: 81.79 toks/s]
Processed prompts:  31%|███       | 1277/4096 [00:15<00:55, 50.60it/s, est. speed input: 82457.73 toks/s, output: 80.52 toks/s]
Processed prompts:  32%|███▏      | 1309/4096 [00:16<00:54, 50.75it/s, est. speed input: 81314.01 toks/s, output: 79.41 toks/s]
Processed prompts:  33%|███▎      | 1341/4096 [00:17<00:54, 50.63it/s, est. speed input: 80208.34 toks/s, output: 78.33 toks/s]
Processed prompts:  34%|███▎      | 1373/4096 [00:17<00:53, 50.47it/s, est. speed input: 79168.90 toks/s, output: 77.31 toks/s]
Processed prompts:  34%|███▍      | 1405/4096 [00:18<00:53, 50.41it/s, est. speed input: 78210.87 toks/s, output: 76.38 toks/s]
Processed prompts:  35%|███▌      | 1437/4096 [00:19<00:52, 50.72it/s, est. speed input: 77375.66 toks/s, output: 75.56 toks/s]
Processed prompts:  36%|███▌      | 1469/4096 [00:19<00:51, 50.56it/s, est. speed input: 76532.70 toks/s, output: 74.74 toks/s]
Processed prompts:  37%|███▋      | 1501/4096 [00:20<00:51, 50.83it/s, est. speed input: 75801.96 toks/s, output: 74.03 toks/s]
Processed prompts:  37%|███▋      | 1533/4096 [00:20<00:50, 50.57it/s, est. speed input: 75048.95 toks/s, output: 73.29 toks/s]
Processed prompts:  38%|███▊      | 1565/4096 [00:21<00:49, 50.80it/s, est. speed input: 74397.76 toks/s, output: 72.65 toks/s]
Processed prompts:  39%|███▉      | 1597/4096 [00:22<00:49, 50.89it/s, est. speed input: 73774.27 toks/s, output: 72.05 toks/s]
Processed prompts:  40%|███▉      | 1629/4096 [00:22<00:48, 50.66it/s, est. speed input: 73145.86 toks/s, output: 71.43 toks/s]
Processed prompts:  41%|████      | 1661/4096 [00:23<00:48, 50.54it/s, est. speed input: 72557.79 toks/s, output: 70.86 toks/s]
Processed prompts:  41%|████▏     | 1693/4096 [00:24<00:47, 50.35it/s, est. speed input: 71986.75 toks/s, output: 70.30 toks/s]
Processed prompts:  42%|████▏     | 1725/4096 [00:24<00:46, 50.62it/s, est. speed input: 71493.98 toks/s, output: 69.82 toks/s]
Processed prompts:  43%|████▎     | 1757/4096 [00:25<00:46, 50.83it/s, est. speed input: 71028.33 toks/s, output: 69.36 toks/s]
Processed prompts:  44%|████▎     | 1789/4096 [00:25<00:45, 50.62it/s, est. speed input: 70544.15 toks/s, output: 68.89 toks/s]
Processed prompts:  44%|████▍     | 1821/4096 [00:26<00:45, 50.52it/s, est. speed input: 70088.46 toks/s, output: 68.45 toks/s]
Processed prompts:  45%|████▌     | 1853/4096 [00:27<00:44, 50.36it/s, est. speed input: 69645.02 toks/s, output: 68.01 toks/s]
Processed prompts:  46%|████▌     | 1885/4096 [00:27<00:43, 50.60it/s, est. speed input: 69257.74 toks/s, output: 67.63 toks/s]
Processed prompts:  47%|████▋     | 1917/4096 [00:28<00:43, 50.48it/s, est. speed input: 68858.85 toks/s, output: 67.24 toks/s]
Processed prompts:  48%|████▊     | 1949/4096 [00:29<00:42, 50.37it/s, est. speed input: 68474.10 toks/s, output: 66.87 toks/s]
Processed prompts:  48%|████▊     | 1981/4096 [00:29<00:41, 50.60it/s, est. speed input: 68135.74 toks/s, output: 66.54 toks/s]
Processed prompts:  49%|████▉     | 2013/4096 [00:30<00:41, 50.46it/s, est. speed input: 67783.04 toks/s, output: 66.19 toks/s]
Processed prompts:  50%|████▉     | 2045/4096 [00:31<00:40, 50.63it/s, est. speed input: 67469.70 toks/s, output: 65.89 toks/s]
Processed prompts:  51%|█████     | 2077/4096 [00:31<00:40, 50.45it/s, est. speed input: 67141.88 toks/s, output: 65.57 toks/s]
Processed prompts:  51%|█████▏    | 2109/4096 [00:32<00:39, 50.32it/s, est. speed input: 66826.74 toks/s, output: 65.26 toks/s]
Processed prompts:  52%|█████▏    | 2141/4096 [00:32<00:38, 50.20it/s, est. speed input: 66521.18 toks/s, output: 64.96 toks/s]
Processed prompts:  53%|█████▎    | 2173/4096 [00:33<00:38, 50.49it/s, est. speed input: 66258.33 toks/s, output: 64.71 toks/s]
Processed prompts:  54%|█████▍    | 2205/4096 [00:34<00:37, 50.26it/s, est. speed input: 65970.05 toks/s, output: 64.42 toks/s]
Processed prompts:  55%|█████▍    | 2237/4096 [00:34<00:37, 50.24it/s, est. speed input: 65703.87 toks/s, output: 64.16 toks/s]
Processed prompts:  55%|█████▌    | 2269/4096 [00:35<00:36, 50.14it/s, est. speed input: 65439.89 toks/s, output: 63.91 toks/s]
Processed prompts:  56%|█████▌    | 2301/4096 [00:36<00:35, 50.08it/s, est. speed input: 65186.37 toks/s, output: 63.66 toks/s]
Processed prompts:  57%|█████▋    | 2333/4096 [00:36<00:35, 50.10it/s, est. speed input: 64946.81 toks/s, output: 63.42 toks/s]
Processed prompts:  58%|█████▊    | 2365/4096 [00:37<00:34, 50.07it/s, est. speed input: 64711.18 toks/s, output: 63.19 toks/s]
Processed prompts:  59%|█████▊    | 2397/4096 [00:38<00:33, 50.04it/s, est. speed input: 64483.22 toks/s, output: 62.97 toks/s]
Processed prompts:  59%|█████▉    | 2429/4096 [00:38<00:33, 50.01it/s, est. speed input: 64262.72 toks/s, output: 62.76 toks/s]
Processed prompts:  60%|██████    | 2461/4096 [00:39<00:32, 50.02it/s, est. speed input: 64051.02 toks/s, output: 62.55 toks/s]
Processed prompts:  61%|██████    | 2493/4096 [00:39<00:32, 49.98it/s, est. speed input: 63843.07 toks/s, output: 62.35 toks/s]
Processed prompts:  62%|██████▏   | 2525/4096 [00:40<00:31, 50.32it/s, est. speed input: 63666.00 toks/s, output: 62.17 toks/s]
Processed prompts:  62%|██████▏   | 2557/4096 [00:41<00:30, 50.22it/s, est. speed input: 63472.66 toks/s, output: 61.98 toks/s]
Processed prompts:  63%|██████▎   | 2589/4096 [00:41<00:29, 50.45it/s, est. speed input: 63303.66 toks/s, output: 61.82 toks/s]
Processed prompts:  64%|██████▍   | 2621/4096 [00:42<00:29, 50.26it/s, est. speed input: 63118.21 toks/s, output: 61.64 toks/s]
Processed prompts:  65%|██████▍   | 2653/4096 [00:43<00:28, 50.46it/s, est. speed input: 62958.82 toks/s, output: 61.48 toks/s]
Processed prompts:  66%|██████▌   | 2685/4096 [00:43<00:28, 50.31it/s, est. speed input: 62786.37 toks/s, output: 61.31 toks/s]
Processed prompts:  66%|██████▋   | 2717/4096 [00:44<00:27, 50.22it/s, est. speed input: 62619.51 toks/s, output: 61.15 toks/s]
Processed prompts:  67%|██████▋   | 2749/4096 [00:45<00:26, 50.44it/s, est. speed input: 62474.23 toks/s, output: 61.01 toks/s]
Processed prompts:  68%|██████▊   | 2781/4096 [00:45<00:26, 50.25it/s, est. speed input: 62312.84 toks/s, output: 60.85 toks/s]
Processed prompts:  69%|██████▊   | 2813/4096 [00:46<00:25, 50.08it/s, est. speed input: 62154.18 toks/s, output: 60.70 toks/s]
Processed prompts:  69%|██████▉   | 2845/4096 [00:46<00:25, 50.00it/s, est. speed input: 62001.66 toks/s, output: 60.55 toks/s]
Processed prompts:  70%|███████   | 2877/4096 [00:47<00:24, 50.28it/s, est. speed input: 61872.00 toks/s, output: 60.42 toks/s]
Processed prompts:  71%|███████   | 2909/4096 [00:48<00:23, 51.21it/s, est. speed input: 61784.45 toks/s, output: 60.34 toks/s]
Processed prompts:  72%|███████▏  | 2941/4096 [00:48<00:22, 50.76it/s, est. speed input: 61641.58 toks/s, output: 60.20 toks/s]
Processed prompts:  73%|███████▎  | 2973/4096 [00:49<00:22, 50.50it/s, est. speed input: 61504.71 toks/s, output: 60.06 toks/s]
Processed prompts:  73%|███████▎  | 3005/4096 [00:50<00:21, 50.29it/s, est. speed input: 61370.38 toks/s, output: 59.93 toks/s]
Processed prompts:  74%|███████▍  | 3037/4096 [00:50<00:21, 50.08it/s, est. speed input: 61236.08 toks/s, output: 59.80 toks/s]
Processed prompts:  75%|███████▍  | 3069/4096 [00:51<00:20, 50.02it/s, est. speed input: 61109.00 toks/s, output: 59.68 toks/s]
Processed prompts:  76%|███████▌  | 3101/4096 [00:52<00:19, 49.98it/s, est. speed input: 60985.74 toks/s, output: 59.56 toks/s]
Processed prompts:  76%|███████▋  | 3133/4096 [00:52<00:19, 49.96it/s, est. speed input: 60865.48 toks/s, output: 59.44 toks/s]
Processed prompts:  77%|███████▋  | 3165/4096 [00:53<00:18, 49.91it/s, est. speed input: 60746.57 toks/s, output: 59.32 toks/s]
Processed prompts:  78%|███████▊  | 3197/4096 [00:53<00:18, 49.88it/s, est. speed input: 60630.59 toks/s, output: 59.21 toks/s]
Processed prompts:  79%|███████▉  | 3229/4096 [00:54<00:17, 49.86it/s, est. speed input: 60517.32 toks/s, output: 59.10 toks/s]
Processed prompts:  80%|███████▉  | 3261/4096 [00:55<00:16, 49.78it/s, est. speed input: 60404.03 toks/s, output: 58.99 toks/s]
Processed prompts:  80%|████████  | 3293/4096 [00:55<00:16, 49.75it/s, est. speed input: 60293.99 toks/s, output: 58.88 toks/s]
Processed prompts:  81%|████████  | 3325/4096 [00:56<00:15, 49.76it/s, est. speed input: 60188.24 toks/s, output: 58.78 toks/s]
Processed prompts:  82%|████████▏ | 3357/4096 [00:57<00:14, 49.80it/s, est. speed input: 60086.42 toks/s, output: 58.68 toks/s]
Processed prompts:  83%|████████▎ | 3389/4096 [00:57<00:14, 49.77it/s, est. speed input: 59984.17 toks/s, output: 58.58 toks/s]
Processed prompts:  84%|████████▎ | 3421/4096 [00:58<00:13, 49.77it/s, est. speed input: 59884.92 toks/s, output: 58.48 toks/s]
Processed prompts:  84%|████████▍ | 3453/4096 [00:59<00:12, 49.77it/s, est. speed input: 59788.13 toks/s, output: 58.39 toks/s]
Processed prompts:  85%|████████▌ | 3485/4096 [00:59<00:12, 49.68it/s, est. speed input: 59689.10 toks/s, output: 58.29 toks/s]
Processed prompts:  86%|████████▌ | 3517/4096 [01:00<00:11, 49.74it/s, est. speed input: 59597.64 toks/s, output: 58.20 toks/s]
Processed prompts:  87%|████████▋ | 3549/4096 [01:01<00:10, 50.06it/s, est. speed input: 59519.99 toks/s, output: 58.12 toks/s]
Processed prompts:  87%|████████▋ | 3581/4096 [01:01<00:10, 49.92it/s, est. speed input: 59428.39 toks/s, output: 58.04 toks/s]
Processed prompts:  88%|████████▊ | 3613/4096 [01:02<00:09, 49.81it/s, est. speed input: 59338.52 toks/s, output: 57.95 toks/s]
Processed prompts:  89%|████████▉ | 3645/4096 [01:02<00:09, 49.79it/s, est. speed input: 59252.93 toks/s, output: 57.86 toks/s]
Processed prompts:  90%|████████▉ | 3677/4096 [01:03<00:08, 50.05it/s, est. speed input: 59179.73 toks/s, output: 57.79 toks/s]
Processed prompts:  91%|█████████ | 3709/4096 [01:04<00:07, 49.93it/s, est. speed input: 59095.91 toks/s, output: 57.71 toks/s]
Processed prompts:  91%|█████████▏| 3741/4096 [01:04<00:07, 49.90it/s, est. speed input: 59016.11 toks/s, output: 57.63 toks/s]
Processed prompts:  92%|█████████▏| 3773/4096 [01:05<00:06, 49.76it/s, est. speed input: 58933.47 toks/s, output: 57.55 toks/s]
Processed prompts:  93%|█████████▎| 3805/4096 [01:06<00:05, 49.72it/s, est. speed input: 58854.18 toks/s, output: 57.47 toks/s]
Processed prompts:  94%|█████████▎| 3837/4096 [01:06<00:05, 49.67it/s, est. speed input: 58775.84 toks/s, output: 57.40 toks/s]
Processed prompts:  94%|█████████▍| 3869/4096 [01:07<00:04, 49.64it/s, est. speed input: 58699.34 toks/s, output: 57.32 toks/s]
Processed prompts:  95%|█████████▌| 3901/4096 [01:08<00:03, 49.69it/s, est. speed input: 58626.55 toks/s, output: 57.25 toks/s]
Processed prompts:  96%|█████████▌| 3933/4096 [01:08<00:03, 50.64it/s, est. speed input: 58588.26 toks/s, output: 57.22 toks/s]
Processed prompts:  97%|█████████▋| 3965/4096 [01:09<00:02, 50.29it/s, est. speed input: 58514.53 toks/s, output: 57.14 toks/s]
Processed prompts:  98%|█████████▊| 3997/4096 [01:10<00:01, 50.45it/s, est. speed input: 58456.43 toks/s, output: 57.09 toks/s]
Processed prompts:  98%|█████████▊| 4029/4096 [01:10<00:01, 50.14it/s, est. speed input: 58384.67 toks/s, output: 57.02 toks/s]
Processed prompts:  99%|█████████▉| 4061/4096 [01:11<00:00, 50.99it/s, est. speed input: 58350.78 toks/s, output: 56.98 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [01:11<00:00, 50.99it/s, est. speed input: 58780.89 toks/s, output: 57.40 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [01:11<00:00, 57.40it/s, est. speed input: 58780.89 toks/s, output: 57.40 toks/s]
[rank0]:[W126 12:01:31.117808260 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 145.1s

测试结果:
  Requests/s:   49.93
  Tokens/s:     51179.11
  Total Reqs:   4096
  Elapsed:      82.03s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     51129.18

============================================================
[8/8] 测试 M=65536
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 65536
│   M_prefill     = 65536 (= 64 x 1024)
│   M_decode      = 64
│   batched_tokens = 65536 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 8192
│   --max-num-seqs           = 64
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 65536
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 12:02:23 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1327520) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1327520) WARNING 01-26 12:02:40 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]     self.model_runner.profile_run()
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]     hidden_states = self.model(
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]                     ^^^^^^^^^^^
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]     def forward(
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]     raise e
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]   File "<eval_with_key>.58", line 325, in forward
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]     return range_entry.runnable(*args)
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]     return self._compiled_fn(*args)
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]     return compiled_fn(full_args)
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]                             ^^^^^^^
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 724, in inner_fn
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]     outs = compiled_fn(args)
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]            ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]     return self.current_callable(inputs)
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]     out = model(new_inputs)
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]   File "/tmp/torchinductor_root/lz/clz3zg4ny6jmlz3bbcln42xg3grk4rvj74xjviyqcxrwehjw5lv2.py", line 1090, in call
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]     triton_poi_fused_mul_quant_slide_fp8_silu_slice_1.run(buf15, buf16, triton_poi_fused_mul_quant_slide_fp8_silu_slice_1_xnumel, stream=stream0)
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 1272, in run
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]     self.autotune_to_one_config(*args, **kwargs)
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 1048, in autotune_to_one_config
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]     timings = self.benchmark_all_configs(*args, **kwargs)
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 1023, in benchmark_all_configs
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]     launcher: self.bench(launcher, *args, **kwargs)
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 891, in bench
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]     return benchmarker.benchmark_gpu(kernel_call, rep=40)
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/benchmarking.py", line 39, in wrapper
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]     return fn(self, *args, **kwargs)
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/benchmarking.py", line 247, in benchmark_gpu
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]     torch.cuda.synchronize()
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py", line 1083, in synchronize
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]     return torch._C._cuda_synchronize()
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866] torch.AcceleratorError: CUDA error: an illegal memory access was encountered
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866] Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866] For debugging consider passing CUDA_LAUNCH_BLOCKING=1
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
(EngineCore_DP0 pid=1327520) ERROR 01-26 12:02:49 [core.py:866] 


─── STDERR ───
[2026-01-26 12:02:22] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 12:02:23] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:02:23] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 12:02:23] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:02:23] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:02:23] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:02:23] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:02:23] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:02:23] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:02:23] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 12:02:23] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 12:02:23] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 12:02:23] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 12:02:23] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 12:02:29] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 12:02:30] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:02:30] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 12:02:30] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:02:30] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:02:30] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:02:30] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:02:30] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:02:30] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:02:30] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 12:02:30] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 12:02:30] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 12:02:30] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 12:02:30] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1327520) [2026-01-26 12:02:32] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1327520) [2026-01-26 12:02:32] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1327520) [2026-01-26 12:02:32] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1327520) [2026-01-26 12:02:32] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=1327520) [2026-01-26 12:02:32] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=1327520) [2026-01-26 12:02:32] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=1327520) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1327520) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.60it/s]
(EngineCore_DP0 pid=1327520) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.91it/s]
(EngineCore_DP0 pid=1327520) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.99it/s]
(EngineCore_DP0 pid=1327520) 
(EngineCore_DP0 pid=1327520) [2026-01-26 12:02:34] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 3584] -> 1D uint8
(EngineCore_DP0 pid=1327520) [2026-01-26 12:02:34] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10321920 bytes
(EngineCore_DP0 pid=1327520) [2026-01-26 12:02:34] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 3584] -> 1D uint8
(EngineCore_DP0 pid=1327520) [2026-01-26 12:02:34] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8028160 bytes
(EngineCore_DP0 pid=1327520) [2026-01-26 12:02:34] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 3584] -> 1D uint8
(EngineCore_DP0 pid=1327520) [2026-01-26 12:02:34] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 84869120 bytes
(EngineCore_DP0 pid=1327520) [2026-01-26 12:02:34] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 18944] -> 1D uint8
(EngineCore_DP0 pid=1327520) [2026-01-26 12:02:34] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 42434560 bytes
(EngineCore_DP0 pid=1327520) [rank0]:W0126 12:02:46.255000 1327520 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1327520) [rank0]:W0126 12:02:46.334000 1327520 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1327520) [rank0]:W0126 12:02:47.942000 1327520 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1327520) [rank0]:W0126 12:02:48.067000 1327520 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1327520) Process EngineCore_DP0:
(EngineCore_DP0 pid=1327520) Traceback (most recent call last):
(EngineCore_DP0 pid=1327520)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=1327520)     self.run()
(EngineCore_DP0 pid=1327520)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=1327520)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=1327520)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=1327520)     raise e
(EngineCore_DP0 pid=1327520)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1327520)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1327520)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1327520)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1327520)     super().__init__(
(EngineCore_DP0 pid=1327520)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=1327520)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=1327520)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1327520)   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=1327520)     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=1327520)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1327520)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=1327520)     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=1327520)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1327520)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=1327520)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=1327520)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1327520)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=1327520)     return func(*args, **kwargs)
(EngineCore_DP0 pid=1327520)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1327520)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=1327520)     return func(*args, **kwargs)
(EngineCore_DP0 pid=1327520)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1327520)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=1327520)     self.model_runner.profile_run()
(EngineCore_DP0 pid=1327520)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=1327520)     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=1327520)                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1327520)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=1327520)     return func(*args, **kwargs)
(EngineCore_DP0 pid=1327520)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1327520)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=1327520)     outputs = self.model(
(EngineCore_DP0 pid=1327520)               ^^^^^^^^^^^
(EngineCore_DP0 pid=1327520)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=1327520)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=1327520)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1327520)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=1327520)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=1327520)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1327520)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=1327520)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=1327520)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1327520)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=1327520)     hidden_states = self.model(
(EngineCore_DP0 pid=1327520)                     ^^^^^^^^^^^
(EngineCore_DP0 pid=1327520)   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=1327520)     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=1327520)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1327520)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=1327520)     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=1327520)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1327520)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=1327520)     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=1327520)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1327520)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
(EngineCore_DP0 pid=1327520)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=1327520)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1327520)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
(EngineCore_DP0 pid=1327520)     def forward(
(EngineCore_DP0 pid=1327520)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=1327520)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=1327520)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1327520)   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=1327520)     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=1327520)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1327520)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=1327520)     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=1327520)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1327520)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=1327520)     raise e
(EngineCore_DP0 pid=1327520)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=1327520)     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=1327520)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1327520)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=1327520)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=1327520)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1327520)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=1327520)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=1327520)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1327520)   File "<eval_with_key>.58", line 325, in forward
(EngineCore_DP0 pid=1327520)     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
(EngineCore_DP0 pid=1327520)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1327520)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=1327520)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=1327520)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1327520)   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=1327520)     return range_entry.runnable(*args)
(EngineCore_DP0 pid=1327520)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1327520)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=1327520)     return self._compiled_fn(*args)
(EngineCore_DP0 pid=1327520)            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1327520)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=1327520)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=1327520)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1327520)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=1327520)     return compiled_fn(full_args)
(EngineCore_DP0 pid=1327520)            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1327520)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=1327520)     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=1327520)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1327520)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=1327520)     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=1327520)                             ^^^^^^^
(EngineCore_DP0 pid=1327520)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 724, in inner_fn
(EngineCore_DP0 pid=1327520)     outs = compiled_fn(args)
(EngineCore_DP0 pid=1327520)            ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1327520)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=1327520)     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=1327520)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1327520)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=1327520)     return self.current_callable(inputs)
(EngineCore_DP0 pid=1327520)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1327520)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=1327520)     out = model(new_inputs)
(EngineCore_DP0 pid=1327520)           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1327520)   File "/tmp/torchinductor_root/lz/clz3zg4ny6jmlz3bbcln42xg3grk4rvj74xjviyqcxrwehjw5lv2.py", line 1090, in call
(EngineCore_DP0 pid=1327520)     triton_poi_fused_mul_quant_slide_fp8_silu_slice_1.run(buf15, buf16, triton_poi_fused_mul_quant_slide_fp8_silu_slice_1_xnumel, stream=stream0)
(EngineCore_DP0 pid=1327520)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 1272, in run
(EngineCore_DP0 pid=1327520)     self.autotune_to_one_config(*args, **kwargs)
(EngineCore_DP0 pid=1327520)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 1048, in autotune_to_one_config
(EngineCore_DP0 pid=1327520)     timings = self.benchmark_all_configs(*args, **kwargs)
(EngineCore_DP0 pid=1327520)               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1327520)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 1023, in benchmark_all_configs
(EngineCore_DP0 pid=1327520)     launcher: self.bench(launcher, *args, **kwargs)
(EngineCore_DP0 pid=1327520)               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1327520)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 891, in bench
(EngineCore_DP0 pid=1327520)     return benchmarker.benchmark_gpu(kernel_call, rep=40)
(EngineCore_DP0 pid=1327520)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1327520)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/benchmarking.py", line 39, in wrapper
(EngineCore_DP0 pid=1327520)     return fn(self, *args, **kwargs)
(EngineCore_DP0 pid=1327520)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1327520)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/benchmarking.py", line 247, in benchmark_gpu
(EngineCore_DP0 pid=1327520)     torch.cuda.synchronize()
(EngineCore_DP0 pid=1327520)   File "/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py", line 1083, in synchronize
(EngineCore_DP0 pid=1327520)     return torch._C._cuda_synchronize()
(EngineCore_DP0 pid=1327520)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1327520) torch.AcceleratorError: CUDA error: an illegal memory access was encountered
(EngineCore_DP0 pid=1327520) Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
(EngineCore_DP0 pid=1327520) CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
(EngineCore_DP0 pid=1327520) For debugging consider passing CUDA_LAUNCH_BLOCKING=1
(EngineCore_DP0 pid=1327520) Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
(EngineCore_DP0 pid=1327520) 
[rank0]:[W126 12:02:50.211973298 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[ERROR] 测试失败: M=65536 (exit code: 1)


------------------------------------------------------------
  生成 CSV: Qwen2.5-7B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_FP8E4M3_py312_cu129_x86_64/cusparselt/2_4/Qwen2.5-7B-FP8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,36.7188,18836.7342,3.4860
1024,1024,1,128,128,37.0668,37993.4980,3.4532
2048,1024,2,256,128,43.0367,44112.6144,5.9484
4096,1024,4,512,128,47.4174,48602.8386,10.7977
8192,1024,8,1024,128,49.0809,50307.8748,20.8635
16384,1024,16,2048,128,50.4026,51662.6625,40.6328
32768,1024,32,4096,128,49.9308,51179.1066,82.0335
65536,1024,64,8192,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[INFO] 完成: 7 成功, 1 失败

============================================================
  Qwen2.5-7B-FP8 | cuSPARSELt (2_6) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_6
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6

============================================================
[1/8] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 12:03:00 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1328550) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1328550) WARNING 01-26 12:03:17 [backends.py:609] Failed to read file <frozen os>
Throughput: 35.14 requests/s, 18028.62 total tokens/s, 35.14 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-26 12:02:59] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 12:03:00] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:03:00] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 12:03:00] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:03:00] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:03:00] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:03:00] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:03:00] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:03:00] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:03:00] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 12:03:00] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 12:03:00] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 12:03:00] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 12:03:00] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 12:03:07] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 12:03:07] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:03:07] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 12:03:07] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:03:07] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:03:07] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:03:07] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:03:07] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:03:07] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:03:07] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 12:03:07] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 12:03:07] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 12:03:07] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 12:03:07] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1328550) [2026-01-26 12:03:09] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1328550) [2026-01-26 12:03:09] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1328550) [2026-01-26 12:03:09] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1328550) [2026-01-26 12:03:09] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=1328550) [2026-01-26 12:03:09] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=1328550) [2026-01-26 12:03:09] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=1328550) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1328550) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.14it/s]
(EngineCore_DP0 pid=1328550) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.54it/s]
(EngineCore_DP0 pid=1328550) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.61it/s]
(EngineCore_DP0 pid=1328550) 
(EngineCore_DP0 pid=1328550) [2026-01-26 12:03:11] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 4800] -> 1D uint8
(EngineCore_DP0 pid=1328550) [2026-01-26 12:03:11] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 13824000 bytes
(EngineCore_DP0 pid=1328550) [2026-01-26 12:03:11] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 4800] -> 1D uint8
(EngineCore_DP0 pid=1328550) [2026-01-26 12:03:11] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10752000 bytes
(EngineCore_DP0 pid=1328550) [2026-01-26 12:03:11] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 4800] -> 1D uint8
(EngineCore_DP0 pid=1328550) [2026-01-26 12:03:11] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 113664000 bytes
(EngineCore_DP0 pid=1328550) [2026-01-26 12:03:11] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 25280] -> 1D uint8
(EngineCore_DP0 pid=1328550) [2026-01-26 12:03:11] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 56627200 bytes
(EngineCore_DP0 pid=1328550) 2026-01-26 12:03:28,232 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1328550) 2026-01-26 12:03:28,255 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1328550) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  4.86it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  2.36it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  2.56it/s]
(EngineCore_DP0 pid=1328550) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.46it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.45it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  31%|███▏      | 40/128 [00:00<00:00, 393.99it/s]
Adding requests:  87%|████████▋ | 111/128 [00:00<00:00, 578.66it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 566.86it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:28,  4.50it/s, est. speed input: 2303.33 toks/s, output: 4.50 toks/s]
Processed prompts:   5%|▍         | 6/128 [00:00<00:05, 20.35it/s, est. speed input: 8858.71 toks/s, output: 17.30 toks/s]
Processed prompts:   8%|▊         | 10/128 [00:00<00:04, 26.94it/s, est. speed input: 11448.21 toks/s, output: 22.36 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:00<00:03, 31.85it/s, est. speed input: 13421.10 toks/s, output: 26.21 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:00<00:03, 34.19it/s, est. speed input: 14447.55 toks/s, output: 28.22 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:00<00:02, 35.87it/s, est. speed input: 15216.98 toks/s, output: 29.72 toks/s]
Processed prompts:  21%|██        | 27/128 [00:00<00:02, 37.04it/s, est. speed input: 15809.05 toks/s, output: 30.88 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:00<00:02, 37.88it/s, est. speed input: 16280.54 toks/s, output: 31.80 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:01<00:02, 38.42it/s, est. speed input: 16659.33 toks/s, output: 32.54 toks/s]
Processed prompts:  30%|███       | 39/128 [00:01<00:02, 38.85it/s, est. speed input: 16979.01 toks/s, output: 33.16 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:01<00:02, 39.11it/s, est. speed input: 17243.63 toks/s, output: 33.68 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:01<00:02, 39.28it/s, est. speed input: 17467.90 toks/s, output: 34.12 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:01<00:01, 39.43it/s, est. speed input: 17665.75 toks/s, output: 34.50 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:01<00:01, 39.51it/s, est. speed input: 17835.07 toks/s, output: 34.83 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:01<00:01, 39.59it/s, est. speed input: 17986.86 toks/s, output: 35.13 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:01<00:01, 39.60it/s, est. speed input: 18116.89 toks/s, output: 35.38 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:01<00:01, 39.66it/s, est. speed input: 18237.87 toks/s, output: 35.62 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:01<00:01, 39.75it/s, est. speed input: 18350.46 toks/s, output: 35.84 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:02<00:01, 39.73it/s, est. speed input: 18445.84 toks/s, output: 36.03 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:02<00:01, 39.74it/s, est. speed input: 18533.51 toks/s, output: 36.20 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:02<00:01, 39.77it/s, est. speed input: 18615.40 toks/s, output: 36.36 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:02<00:01, 39.81it/s, est. speed input: 18692.22 toks/s, output: 36.51 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:02<00:00, 39.80it/s, est. speed input: 18759.69 toks/s, output: 36.64 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:02<00:00, 39.80it/s, est. speed input: 18822.42 toks/s, output: 36.76 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:02<00:00, 39.78it/s, est. speed input: 18879.83 toks/s, output: 36.87 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:02<00:00, 39.78it/s, est. speed input: 18933.40 toks/s, output: 36.98 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:02<00:00, 39.78it/s, est. speed input: 18983.34 toks/s, output: 37.08 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:02<00:00, 39.82it/s, est. speed input: 19031.97 toks/s, output: 37.17 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:03<00:00, 39.79it/s, est. speed input: 19074.42 toks/s, output: 37.25 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:03<00:00, 39.74it/s, est. speed input: 19112.99 toks/s, output: 37.33 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:03<00:00, 39.71it/s, est. speed input: 19149.28 toks/s, output: 37.40 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:03<00:00, 39.72it/s, est. speed input: 19184.79 toks/s, output: 37.47 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 39.72it/s, est. speed input: 19192.71 toks/s, output: 37.49 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 37.48it/s, est. speed input: 19192.71 toks/s, output: 37.49 toks/s]
[rank0]:[W126 12:03:34.643286835 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 44.0s

测试结果:
  Requests/s:   35.14
  Tokens/s:     18028.62
  Total Reqs:   128
  Elapsed:      3.64s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     17993.48

============================================================
[2/8] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 12:03:45 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1329713) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1329713) WARNING 01-26 12:04:02 [backends.py:609] Failed to read file <frozen os>
Throughput: 32.52 requests/s, 33337.23 total tokens/s, 32.52 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-26 12:03:44] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 12:03:45] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:03:45] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 12:03:45] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:03:45] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:03:45] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:03:45] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:03:45] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:03:45] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:03:45] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 12:03:45] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 12:03:45] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 12:03:45] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 12:03:45] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 12:03:51] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 12:03:52] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:03:52] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 12:03:52] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:03:52] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:03:52] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:03:52] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:03:52] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:03:52] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:03:52] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 12:03:52] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 12:03:52] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 12:03:52] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 12:03:52] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1329713) [2026-01-26 12:03:53] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1329713) [2026-01-26 12:03:53] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1329713) [2026-01-26 12:03:53] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1329713) [2026-01-26 12:03:53] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=1329713) [2026-01-26 12:03:53] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=1329713) [2026-01-26 12:03:53] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=1329713) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1329713) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.15it/s]
(EngineCore_DP0 pid=1329713) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.55it/s]
(EngineCore_DP0 pid=1329713) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.62it/s]
(EngineCore_DP0 pid=1329713) 
(EngineCore_DP0 pid=1329713) [2026-01-26 12:03:56] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 4800] -> 1D uint8
(EngineCore_DP0 pid=1329713) [2026-01-26 12:03:56] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 13824000 bytes
(EngineCore_DP0 pid=1329713) [2026-01-26 12:03:56] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 4800] -> 1D uint8
(EngineCore_DP0 pid=1329713) [2026-01-26 12:03:56] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10752000 bytes
(EngineCore_DP0 pid=1329713) [2026-01-26 12:03:56] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 4800] -> 1D uint8
(EngineCore_DP0 pid=1329713) [2026-01-26 12:03:56] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 113664000 bytes
(EngineCore_DP0 pid=1329713) [2026-01-26 12:03:56] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 25280] -> 1D uint8
(EngineCore_DP0 pid=1329713) [2026-01-26 12:03:56] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 56627200 bytes
(EngineCore_DP0 pid=1329713) 2026-01-26 12:04:12,690 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1329713) 2026-01-26 12:04:12,713 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1329713) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  5.70it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  7.55it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  7.19it/s]
(EngineCore_DP0 pid=1329713) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  1.59it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  1.59it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  16%|█▌        | 20/128 [00:00<00:00, 199.46it/s]
Adding requests:  48%|████▊     | 61/128 [00:00<00:00, 319.57it/s]
Adding requests:  77%|███████▋  | 98/128 [00:00<00:00, 339.00it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 333.12it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   5%|▍         | 6/128 [00:00<00:02, 52.59it/s, est. speed input: 53857.08 toks/s, output: 52.59 toks/s]
Processed prompts:   9%|▉         | 12/128 [00:00<00:02, 41.09it/s, est. speed input: 43510.78 toks/s, output: 42.49 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:02, 38.62it/s, est. speed input: 41150.55 toks/s, output: 40.18 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:02, 37.65it/s, est. speed input: 40197.77 toks/s, output: 39.25 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:00<00:02, 36.96it/s, est. speed input: 39537.27 toks/s, output: 38.61 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:00<00:02, 36.52it/s, est. speed input: 39078.16 toks/s, output: 38.16 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:00<00:02, 36.24it/s, est. speed input: 38744.22 toks/s, output: 37.83 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:00<00:02, 36.04it/s, est. speed input: 38479.40 toks/s, output: 37.58 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:01<00:02, 35.83it/s, est. speed input: 38246.68 toks/s, output: 37.35 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:01<00:02, 35.77it/s, est. speed input: 38082.65 toks/s, output: 37.19 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:01<00:02, 35.68it/s, est. speed input: 37933.47 toks/s, output: 37.04 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:01<00:02, 35.62it/s, est. speed input: 37808.34 toks/s, output: 36.92 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:01<00:01, 35.58it/s, est. speed input: 37701.35 toks/s, output: 36.82 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:01<00:01, 35.58it/s, est. speed input: 37615.69 toks/s, output: 36.73 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:01<00:01, 35.53it/s, est. speed input: 37528.36 toks/s, output: 36.65 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:01<00:01, 35.49it/s, est. speed input: 37453.17 toks/s, output: 36.57 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:01<00:01, 35.48it/s, est. speed input: 37388.05 toks/s, output: 36.51 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:02<00:01, 35.51it/s, est. speed input: 37337.94 toks/s, output: 36.46 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:02<00:01, 35.51it/s, est. speed input: 37288.09 toks/s, output: 36.41 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:02<00:01, 35.50it/s, est. speed input: 37241.16 toks/s, output: 36.37 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:02<00:01, 35.51it/s, est. speed input: 37201.75 toks/s, output: 36.33 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:02<00:00, 35.49it/s, est. speed input: 37162.51 toks/s, output: 36.29 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:02<00:00, 35.46it/s, est. speed input: 37122.52 toks/s, output: 36.25 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:02<00:00, 35.45it/s, est. speed input: 37088.19 toks/s, output: 36.22 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:02<00:00, 35.44it/s, est. speed input: 37056.83 toks/s, output: 36.19 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:03<00:00, 35.48it/s, est. speed input: 37033.03 toks/s, output: 36.16 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:03<00:00, 35.46it/s, est. speed input: 37005.23 toks/s, output: 36.14 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:03<00:00, 35.51it/s, est. speed input: 36987.37 toks/s, output: 36.12 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:03<00:00, 35.51it/s, est. speed input: 36966.78 toks/s, output: 36.10 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:03<00:00, 35.48it/s, est. speed input: 36943.54 toks/s, output: 36.08 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 35.48it/s, est. speed input: 36925.87 toks/s, output: 36.06 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 36.06it/s, est. speed input: 36925.87 toks/s, output: 36.06 toks/s]
[rank0]:[W126 12:04:19.395041879 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 44.1s

测试结果:
  Requests/s:   32.52
  Tokens/s:     33337.23
  Total Reqs:   128
  Elapsed:      3.94s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     33304.70

============================================================
[3/8] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 12:04:29 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1330821) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1330821) WARNING 01-26 12:04:45 [backends.py:609] Failed to read file <frozen os>
Throughput: 37.33 requests/s, 38267.89 total tokens/s, 37.33 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-26 12:04:28] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 12:04:29] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:04:29] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 12:04:29] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:04:29] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:04:29] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:04:29] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:04:29] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:04:29] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:04:29] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 12:04:29] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 12:04:29] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 12:04:29] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 12:04:29] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 12:04:36] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 12:04:36] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:04:36] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 12:04:36] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:04:36] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:04:36] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:04:36] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:04:36] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:04:36] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:04:36] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 12:04:36] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 12:04:36] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 12:04:36] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 12:04:36] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1330821) [2026-01-26 12:04:38] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1330821) [2026-01-26 12:04:38] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1330821) [2026-01-26 12:04:38] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1330821) [2026-01-26 12:04:38] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=1330821) [2026-01-26 12:04:38] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=1330821) [2026-01-26 12:04:38] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=1330821) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1330821) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.15it/s]
(EngineCore_DP0 pid=1330821) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.54it/s]
(EngineCore_DP0 pid=1330821) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.61it/s]
(EngineCore_DP0 pid=1330821) 
(EngineCore_DP0 pid=1330821) [2026-01-26 12:04:39] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 4800] -> 1D uint8
(EngineCore_DP0 pid=1330821) [2026-01-26 12:04:39] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 13824000 bytes
(EngineCore_DP0 pid=1330821) [2026-01-26 12:04:39] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 4800] -> 1D uint8
(EngineCore_DP0 pid=1330821) [2026-01-26 12:04:39] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10752000 bytes
(EngineCore_DP0 pid=1330821) [2026-01-26 12:04:39] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 4800] -> 1D uint8
(EngineCore_DP0 pid=1330821) [2026-01-26 12:04:39] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 113664000 bytes
(EngineCore_DP0 pid=1330821) [2026-01-26 12:04:39] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 25280] -> 1D uint8
(EngineCore_DP0 pid=1330821) [2026-01-26 12:04:39] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 56627200 bytes
(EngineCore_DP0 pid=1330821) 2026-01-26 12:04:56,346 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1330821) 2026-01-26 12:04:56,369 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1330821) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 1/3 [00:00<00:00,  8.91it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:00<00:00,  2.79it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00,  3.48it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00,  3.54it/s]
(EngineCore_DP0 pid=1330821) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 18.47it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 18.45it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   7%|▋         | 19/256 [00:00<00:01, 185.69it/s]
Adding requests:  23%|██▎       | 59/256 [00:00<00:00, 308.83it/s]
Adding requests:  38%|███▊      | 96/256 [00:00<00:00, 334.31it/s]
Adding requests:  52%|█████▏    | 134/256 [00:00<00:00, 350.63it/s]
Adding requests:  68%|██████▊   | 174/256 [00:00<00:00, 367.23it/s]
Adding requests:  84%|████████▎ | 214/256 [00:00<00:00, 376.29it/s]
Adding requests:  99%|█████████▉| 253/256 [00:00<00:00, 378.49it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 356.14it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   8%|▊         | 20/256 [00:00<00:01, 183.72it/s, est. speed input: 188167.37 toks/s, output: 183.73 toks/s]
Processed prompts:  15%|█▌        | 39/256 [00:00<00:03, 61.66it/s, est. speed input: 70335.95 toks/s, output: 68.69 toks/s]   
Processed prompts:  20%|█▉        | 50/256 [00:00<00:04, 49.81it/s, est. speed input: 58334.39 toks/s, output: 56.96 toks/s]
Processed prompts:  23%|██▎       | 58/256 [00:01<00:04, 46.40it/s, est. speed input: 54717.51 toks/s, output: 53.43 toks/s]
Processed prompts:  25%|██▌       | 64/256 [00:01<00:04, 44.71it/s, est. speed input: 52947.76 toks/s, output: 51.71 toks/s]
Processed prompts:  27%|██▋       | 70/256 [00:01<00:04, 43.27it/s, est. speed input: 51524.66 toks/s, output: 50.32 toks/s]
Processed prompts:  29%|██▉       | 75/256 [00:01<00:04, 44.39it/s, est. speed input: 51430.57 toks/s, output: 50.22 toks/s]
Processed prompts:  31%|███▏      | 80/256 [00:01<00:04, 40.82it/s, est. speed input: 49723.00 toks/s, output: 48.56 toks/s]
Processed prompts:  33%|███▎      | 85/256 [00:01<00:04, 42.60it/s, est. speed input: 49742.73 toks/s, output: 48.58 toks/s]
Processed prompts:  35%|███▌      | 90/256 [00:01<00:04, 39.17it/s, est. speed input: 48353.37 toks/s, output: 47.22 toks/s]
Processed prompts:  37%|███▋      | 95/256 [00:02<00:03, 41.22it/s, est. speed input: 48385.80 toks/s, output: 47.25 toks/s]
Processed prompts:  39%|███▉      | 100/256 [00:02<00:04, 38.31it/s, est. speed input: 47310.66 toks/s, output: 46.20 toks/s]
Processed prompts:  41%|████      | 104/256 [00:02<00:03, 38.52it/s, est. speed input: 46986.57 toks/s, output: 45.88 toks/s]
Processed prompts:  42%|████▏     | 108/256 [00:02<00:03, 38.61it/s, est. speed input: 46675.23 toks/s, output: 45.58 toks/s]
Processed prompts:  44%|████▍     | 112/256 [00:02<00:03, 38.80it/s, est. speed input: 46411.41 toks/s, output: 45.32 toks/s]
Processed prompts:  45%|████▌     | 116/256 [00:02<00:03, 38.91it/s, est. speed input: 46162.25 toks/s, output: 45.08 toks/s]
Processed prompts:  47%|████▋     | 120/256 [00:02<00:03, 38.92it/s, est. speed input: 45921.35 toks/s, output: 44.84 toks/s]
Processed prompts:  48%|████▊     | 124/256 [00:02<00:03, 38.89it/s, est. speed input: 45692.75 toks/s, output: 44.62 toks/s]
Processed prompts:  50%|█████     | 128/256 [00:02<00:03, 38.86it/s, est. speed input: 45479.24 toks/s, output: 44.41 toks/s]
Processed prompts:  52%|█████▏    | 132/256 [00:02<00:03, 38.57it/s, est. speed input: 45242.25 toks/s, output: 44.18 toks/s]
Processed prompts:  53%|█████▎    | 136/256 [00:03<00:03, 38.63it/s, est. speed input: 45057.57 toks/s, output: 44.00 toks/s]
Processed prompts:  55%|█████▍    | 140/256 [00:03<00:02, 38.83it/s, est. speed input: 44904.86 toks/s, output: 43.85 toks/s]
Processed prompts:  56%|█████▋    | 144/256 [00:03<00:02, 38.80it/s, est. speed input: 44740.06 toks/s, output: 43.69 toks/s]
Processed prompts:  58%|█████▊    | 148/256 [00:03<00:02, 38.92it/s, est. speed input: 44601.81 toks/s, output: 43.56 toks/s]
Processed prompts:  59%|█████▉    | 152/256 [00:03<00:02, 39.00it/s, est. speed input: 44472.05 toks/s, output: 43.43 toks/s]
Processed prompts:  61%|██████    | 156/256 [00:03<00:02, 39.01it/s, est. speed input: 44344.21 toks/s, output: 43.30 toks/s]
Processed prompts:  62%|██████▎   | 160/256 [00:03<00:02, 38.97it/s, est. speed input: 44217.82 toks/s, output: 43.18 toks/s]
Processed prompts:  64%|██████▍   | 164/256 [00:03<00:02, 39.06it/s, est. speed input: 44111.16 toks/s, output: 43.08 toks/s]
Processed prompts:  66%|██████▌   | 168/256 [00:03<00:02, 38.85it/s, est. speed input: 43982.82 toks/s, output: 42.95 toks/s]
Processed prompts:  67%|██████▋   | 172/256 [00:04<00:02, 38.75it/s, est. speed input: 43865.10 toks/s, output: 42.84 toks/s]
Processed prompts:  69%|██████▉   | 176/256 [00:04<00:02, 38.77it/s, est. speed input: 43762.37 toks/s, output: 42.74 toks/s]
Processed prompts:  70%|███████   | 180/256 [00:04<00:01, 38.89it/s, est. speed input: 43674.01 toks/s, output: 42.65 toks/s]
Processed prompts:  72%|███████▏  | 184/256 [00:04<00:01, 38.88it/s, est. speed input: 43581.00 toks/s, output: 42.56 toks/s]
Processed prompts:  73%|███████▎  | 188/256 [00:04<00:01, 38.89it/s, est. speed input: 43494.62 toks/s, output: 42.47 toks/s]
Processed prompts:  75%|███████▌  | 192/256 [00:04<00:01, 38.92it/s, est. speed input: 43414.08 toks/s, output: 42.40 toks/s]
Processed prompts:  77%|███████▋  | 196/256 [00:04<00:01, 38.96it/s, est. speed input: 43338.23 toks/s, output: 42.32 toks/s]
Processed prompts:  78%|███████▊  | 200/256 [00:04<00:01, 39.00it/s, est. speed input: 43266.85 toks/s, output: 42.25 toks/s]
Processed prompts:  80%|████████  | 206/256 [00:04<00:01, 40.59it/s, est. speed input: 43298.37 toks/s, output: 42.28 toks/s]
Processed prompts:  82%|████████▏ | 211/256 [00:04<00:01, 42.76it/s, est. speed input: 43426.10 toks/s, output: 42.41 toks/s]
Processed prompts:  84%|████████▍ | 216/256 [00:05<00:01, 38.88it/s, est. speed input: 43117.95 toks/s, output: 42.11 toks/s]
Processed prompts:  86%|████████▌ | 220/256 [00:05<00:00, 38.89it/s, est. speed input: 43053.79 toks/s, output: 42.04 toks/s]
Processed prompts:  88%|████████▊ | 224/256 [00:05<00:00, 38.89it/s, est. speed input: 42991.73 toks/s, output: 41.98 toks/s]
Processed prompts:  89%|████████▉ | 228/256 [00:05<00:00, 38.89it/s, est. speed input: 42931.95 toks/s, output: 41.93 toks/s]
Processed prompts:  91%|█████████ | 232/256 [00:05<00:00, 38.94it/s, est. speed input: 42877.49 toks/s, output: 41.87 toks/s]
Processed prompts:  92%|█████████▏| 236/256 [00:05<00:00, 38.92it/s, est. speed input: 42821.29 toks/s, output: 41.82 toks/s]
Processed prompts:  94%|█████████▍| 240/256 [00:05<00:00, 38.79it/s, est. speed input: 42759.99 toks/s, output: 41.76 toks/s]
Processed prompts:  95%|█████████▌| 244/256 [00:05<00:00, 38.85it/s, est. speed input: 42709.86 toks/s, output: 41.71 toks/s]
Processed prompts:  97%|█████████▋| 248/256 [00:05<00:00, 38.91it/s, est. speed input: 42663.00 toks/s, output: 41.66 toks/s]
Processed prompts:  98%|█████████▊| 252/256 [00:06<00:00, 38.84it/s, est. speed input: 42611.03 toks/s, output: 41.61 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:06<00:00, 38.84it/s, est. speed input: 42721.21 toks/s, output: 41.72 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:06<00:00, 41.72it/s, est. speed input: 42721.21 toks/s, output: 41.72 toks/s]
[rank0]:[W126 12:05:05.980921243 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 47.0s

测试结果:
  Requests/s:   37.33
  Tokens/s:     38267.89
  Total Reqs:   256
  Elapsed:      6.86s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     38230.55

============================================================
[4/8] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 12:05:18 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1331991) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1331991) WARNING 01-26 12:05:34 [backends.py:609] Failed to read file <frozen os>
Throughput: 40.01 requests/s, 41010.45 total tokens/s, 40.01 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-26 12:05:17] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 12:05:18] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:05:18] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 12:05:18] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:05:18] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:05:18] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:05:18] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:05:18] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:05:18] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:05:18] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 12:05:18] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 12:05:18] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 12:05:18] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 12:05:18] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 12:05:25] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 12:05:25] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:05:25] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 12:05:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:05:25] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:05:25] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:05:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:05:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:05:25] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:05:25] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 12:05:25] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 12:05:25] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 12:05:25] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 12:05:25] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1331991) [2026-01-26 12:05:26] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1331991) [2026-01-26 12:05:26] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1331991) [2026-01-26 12:05:26] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1331991) [2026-01-26 12:05:26] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=1331991) [2026-01-26 12:05:26] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=1331991) [2026-01-26 12:05:26] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=1331991) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1331991) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.15it/s]
(EngineCore_DP0 pid=1331991) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.54it/s]
(EngineCore_DP0 pid=1331991) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.61it/s]
(EngineCore_DP0 pid=1331991) 
(EngineCore_DP0 pid=1331991) [2026-01-26 12:05:28] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 4800] -> 1D uint8
(EngineCore_DP0 pid=1331991) [2026-01-26 12:05:28] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 13824000 bytes
(EngineCore_DP0 pid=1331991) [2026-01-26 12:05:28] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 4800] -> 1D uint8
(EngineCore_DP0 pid=1331991) [2026-01-26 12:05:28] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10752000 bytes
(EngineCore_DP0 pid=1331991) [2026-01-26 12:05:28] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 4800] -> 1D uint8
(EngineCore_DP0 pid=1331991) [2026-01-26 12:05:28] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 113664000 bytes
(EngineCore_DP0 pid=1331991) [2026-01-26 12:05:28] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 25280] -> 1D uint8
(EngineCore_DP0 pid=1331991) [2026-01-26 12:05:28] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 56627200 bytes
(EngineCore_DP0 pid=1331991) 2026-01-26 12:05:46,217 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1331991) 2026-01-26 12:05:46,254 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1331991) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00, 14.24it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 15.63it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 15.39it/s]
(EngineCore_DP0 pid=1331991) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00, 19.16it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 19.57it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   4%|▍         | 21/512 [00:00<00:02, 209.03it/s]
Adding requests:  12%|█▏        | 62/512 [00:00<00:01, 325.77it/s]
Adding requests:  19%|█▉        | 98/512 [00:00<00:01, 340.63it/s]
Adding requests:  27%|██▋       | 136/512 [00:00<00:01, 356.08it/s]
Adding requests:  34%|███▍      | 176/512 [00:00<00:00, 368.99it/s]
Adding requests:  42%|████▏     | 217/512 [00:00<00:00, 381.53it/s]
Adding requests:  50%|█████     | 256/512 [00:00<00:00, 381.40it/s]
Adding requests:  58%|█████▊    | 296/512 [00:00<00:00, 385.59it/s]
Adding requests:  66%|██████▌   | 338/512 [00:00<00:00, 392.88it/s]
Adding requests:  74%|███████▍  | 378/512 [00:01<00:00, 394.33it/s]
Adding requests:  82%|████████▏ | 421/512 [00:01<00:00, 401.89it/s]
Adding requests:  90%|█████████ | 462/512 [00:01<00:00, 397.72it/s]
Adding requests:  99%|█████████▊| 505/512 [00:01<00:00, 404.92it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 382.37it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  11%|█         | 54/512 [00:00<00:01, 379.35it/s, est. speed input: 388533.11 toks/s, output: 379.37 toks/s]
Processed prompts:  18%|█▊        | 92/512 [00:01<00:05, 76.09it/s, est. speed input: 90681.43 toks/s, output: 88.56 toks/s]   
Processed prompts:  22%|██▏       | 111/512 [00:01<00:06, 60.50it/s, est. speed input: 74164.41 toks/s, output: 72.43 toks/s]
Processed prompts:  24%|██▍       | 123/512 [00:01<00:07, 54.97it/s, est. speed input: 68649.60 toks/s, output: 67.04 toks/s]
Processed prompts:  26%|██▌       | 132/512 [00:02<00:07, 52.89it/s, est. speed input: 66382.84 toks/s, output: 64.83 toks/s]
Processed prompts:  27%|██▋       | 140/512 [00:02<00:07, 50.14it/s, est. speed input: 64172.62 toks/s, output: 62.67 toks/s]
Processed prompts:  29%|██▊       | 147/512 [00:02<00:07, 46.61it/s, est. speed input: 61901.55 toks/s, output: 60.45 toks/s]
Processed prompts:  30%|███       | 154/512 [00:02<00:08, 43.63it/s, est. speed input: 59926.29 toks/s, output: 58.52 toks/s]
Processed prompts:  31%|███       | 159/512 [00:02<00:07, 44.57it/s, est. speed input: 59590.40 toks/s, output: 58.19 toks/s]
Processed prompts:  32%|███▏      | 164/512 [00:02<00:07, 45.48it/s, est. speed input: 59275.56 toks/s, output: 57.89 toks/s]
Processed prompts:  33%|███▎      | 169/512 [00:02<00:07, 46.33it/s, est. speed input: 58982.55 toks/s, output: 57.60 toks/s]
Processed prompts:  34%|███▍      | 174/512 [00:03<00:08, 38.46it/s, est. speed input: 56857.57 toks/s, output: 55.52 toks/s]
Processed prompts:  36%|███▌      | 182/512 [00:03<00:08, 39.17it/s, est. speed input: 55946.56 toks/s, output: 54.63 toks/s]
Processed prompts:  37%|███▋      | 190/512 [00:03<00:08, 39.52it/s, est. speed input: 55115.82 toks/s, output: 53.82 toks/s]
Processed prompts:  38%|███▊      | 195/512 [00:03<00:07, 41.50it/s, est. speed input: 55002.79 toks/s, output: 53.71 toks/s]
Processed prompts:  39%|███▉      | 200/512 [00:03<00:07, 43.25it/s, est. speed input: 54888.80 toks/s, output: 53.60 toks/s]
Processed prompts:  40%|████      | 206/512 [00:03<00:07, 39.31it/s, est. speed input: 53859.88 toks/s, output: 52.60 toks/s]
Processed prompts:  41%|████      | 211/512 [00:04<00:07, 41.63it/s, est. speed input: 53790.83 toks/s, output: 52.53 toks/s]
Processed prompts:  42%|████▏     | 216/512 [00:04<00:06, 43.53it/s, est. speed input: 53717.42 toks/s, output: 52.46 toks/s]
Processed prompts:  43%|████▎     | 222/512 [00:04<00:07, 38.19it/s, est. speed input: 52672.38 toks/s, output: 51.44 toks/s]
Processed prompts:  45%|████▍     | 230/512 [00:04<00:07, 38.94it/s, est. speed input: 52167.88 toks/s, output: 50.94 toks/s]
Processed prompts:  46%|████▌     | 235/512 [00:04<00:06, 41.24it/s, est. speed input: 52146.26 toks/s, output: 50.92 toks/s]
Processed prompts:  47%|████▋     | 240/512 [00:04<00:06, 43.21it/s, est. speed input: 52121.81 toks/s, output: 50.90 toks/s]
Processed prompts:  48%|████▊     | 245/512 [00:04<00:05, 44.82it/s, est. speed input: 52096.65 toks/s, output: 50.88 toks/s]
Processed prompts:  49%|████▉     | 250/512 [00:05<00:07, 36.60it/s, est. speed input: 51035.68 toks/s, output: 49.84 toks/s]
Processed prompts:  50%|████▉     | 255/512 [00:05<00:06, 39.62it/s, est. speed input: 51034.82 toks/s, output: 49.84 toks/s]
Processed prompts:  51%|█████     | 262/512 [00:05<00:06, 37.92it/s, est. speed input: 50479.71 toks/s, output: 49.30 toks/s]
Processed prompts:  53%|█████▎    | 270/512 [00:05<00:06, 38.71it/s, est. speed input: 50139.19 toks/s, output: 48.96 toks/s]
Processed prompts:  54%|█████▎    | 275/512 [00:05<00:05, 41.00it/s, est. speed input: 50155.07 toks/s, output: 48.98 toks/s]
Processed prompts:  55%|█████▍    | 280/512 [00:05<00:05, 43.01it/s, est. speed input: 50171.06 toks/s, output: 49.00 toks/s]
Processed prompts:  56%|█████▌    | 285/512 [00:05<00:05, 44.68it/s, est. speed input: 50185.31 toks/s, output: 49.01 toks/s]
Processed prompts:  57%|█████▋    | 290/512 [00:06<00:06, 36.58it/s, est. speed input: 49366.94 toks/s, output: 48.21 toks/s]
Processed prompts:  58%|█████▊    | 295/512 [00:06<00:05, 39.55it/s, est. speed input: 49389.84 toks/s, output: 48.23 toks/s]
Processed prompts:  59%|█████▉    | 302/512 [00:06<00:05, 37.85it/s, est. speed input: 48969.76 toks/s, output: 47.82 toks/s]
Processed prompts:  61%|██████    | 310/512 [00:06<00:05, 40.00it/s, est. speed input: 48867.70 toks/s, output: 47.72 toks/s]
Processed prompts:  62%|██████▏   | 315/512 [00:06<00:04, 42.07it/s, est. speed input: 48899.92 toks/s, output: 47.75 toks/s]
Processed prompts:  62%|██████▎   | 320/512 [00:06<00:04, 43.82it/s, est. speed input: 48928.21 toks/s, output: 47.78 toks/s]
Processed prompts:  63%|██████▎   | 325/512 [00:06<00:04, 45.35it/s, est. speed input: 48961.65 toks/s, output: 47.81 toks/s]
Processed prompts:  64%|██████▍   | 330/512 [00:06<00:04, 36.94it/s, est. speed input: 48296.55 toks/s, output: 47.16 toks/s]
Processed prompts:  65%|██████▌   | 335/512 [00:07<00:04, 39.85it/s, est. speed input: 48331.12 toks/s, output: 47.20 toks/s]
Processed prompts:  67%|██████▋   | 342/512 [00:07<00:04, 37.98it/s, est. speed input: 47993.29 toks/s, output: 46.87 toks/s]
Processed prompts:  68%|██████▊   | 350/512 [00:07<00:04, 38.77it/s, est. speed input: 47811.42 toks/s, output: 46.69 toks/s]
Processed prompts:  69%|██████▉   | 355/512 [00:07<00:03, 41.05it/s, est. speed input: 47853.76 toks/s, output: 46.73 toks/s]
Processed prompts:  70%|███████   | 360/512 [00:07<00:03, 43.02it/s, est. speed input: 47893.75 toks/s, output: 46.77 toks/s]
Processed prompts:  71%|███████▏  | 365/512 [00:07<00:03, 44.72it/s, est. speed input: 47935.48 toks/s, output: 46.81 toks/s]
Processed prompts:  72%|███████▏  | 370/512 [00:07<00:03, 36.60it/s, est. speed input: 47376.08 toks/s, output: 46.27 toks/s]
Processed prompts:  73%|███████▎  | 375/512 [00:08<00:03, 39.63it/s, est. speed input: 47422.43 toks/s, output: 46.31 toks/s]
Processed prompts:  75%|███████▍  | 382/512 [00:08<00:03, 37.87it/s, est. speed input: 47148.25 toks/s, output: 46.04 toks/s]
Processed prompts:  76%|███████▌  | 390/512 [00:08<00:03, 38.68it/s, est. speed input: 47005.55 toks/s, output: 45.90 toks/s]
Processed prompts:  77%|███████▋  | 396/512 [00:08<00:02, 43.04it/s, est. speed input: 47173.28 toks/s, output: 46.07 toks/s]
Processed prompts:  78%|███████▊  | 401/512 [00:08<00:02, 44.55it/s, est. speed input: 47216.20 toks/s, output: 46.11 toks/s]
Processed prompts:  79%|███████▉  | 406/512 [00:08<00:02, 36.83it/s, est. speed input: 46726.41 toks/s, output: 45.63 toks/s]
Processed prompts:  80%|████████  | 411/512 [00:08<00:02, 39.67it/s, est. speed input: 46773.58 toks/s, output: 45.68 toks/s]
Processed prompts:  82%|████████▏ | 418/512 [00:09<00:02, 37.89it/s, est. speed input: 46536.99 toks/s, output: 45.45 toks/s]
Processed prompts:  83%|████████▎ | 426/512 [00:09<00:02, 38.68it/s, est. speed input: 46421.20 toks/s, output: 45.33 toks/s]
Processed prompts:  85%|████████▍ | 434/512 [00:09<00:01, 40.32it/s, est. speed input: 46393.01 toks/s, output: 45.31 toks/s]
Processed prompts:  86%|████████▋ | 442/512 [00:09<00:01, 40.29it/s, est. speed input: 46287.35 toks/s, output: 45.20 toks/s]
Processed prompts:  87%|████████▋ | 447/512 [00:09<00:01, 42.12it/s, est. speed input: 46335.00 toks/s, output: 45.25 toks/s]
Processed prompts:  88%|████████▊ | 452/512 [00:09<00:01, 43.78it/s, est. speed input: 46382.02 toks/s, output: 45.29 toks/s]
Processed prompts:  89%|████████▉ | 457/512 [00:10<00:01, 45.22it/s, est. speed input: 46428.83 toks/s, output: 45.34 toks/s]
Processed prompts:  90%|█████████ | 462/512 [00:10<00:01, 37.17it/s, est. speed input: 46027.41 toks/s, output: 44.95 toks/s]
Processed prompts:  92%|█████████▏| 470/512 [00:10<00:01, 38.21it/s, est. speed input: 45931.20 toks/s, output: 44.85 toks/s]
Processed prompts:  93%|█████████▎| 478/512 [00:10<00:00, 38.85it/s, est. speed input: 45839.85 toks/s, output: 44.77 toks/s]
Processed prompts:  95%|█████████▍| 486/512 [00:10<00:00, 39.21it/s, est. speed input: 45748.78 toks/s, output: 44.68 toks/s]
Processed prompts:  96%|█████████▌| 491/512 [00:10<00:00, 41.17it/s, est. speed input: 45793.33 toks/s, output: 44.72 toks/s]
Processed prompts:  97%|█████████▋| 498/512 [00:11<00:00, 39.02it/s, est. speed input: 45616.56 toks/s, output: 44.55 toks/s]
Processed prompts:  99%|█████████▉| 506/512 [00:11<00:00, 39.39it/s, est. speed input: 45537.22 toks/s, output: 44.47 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:11<00:00, 39.39it/s, est. speed input: 45766.34 toks/s, output: 44.69 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:11<00:00, 44.69it/s, est. speed input: 45766.34 toks/s, output: 44.69 toks/s]
[rank0]:[W126 12:06:01.248992537 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 54.8s

测试结果:
  Requests/s:   40.01
  Tokens/s:     41010.45
  Total Reqs:   512
  Elapsed:      12.80s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     40970.44

============================================================
[5/8] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 12:06:15 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1333276) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1333276) WARNING 01-26 12:06:32 [backends.py:609] Failed to read file <frozen os>
Throughput: 41.02 requests/s, 42047.93 total tokens/s, 41.02 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-26 12:06:14] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 12:06:15] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:06:15] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 12:06:15] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:06:15] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:06:15] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:06:15] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:06:15] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:06:15] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:06:15] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 12:06:15] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 12:06:15] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 12:06:15] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 12:06:15] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 12:06:21] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 12:06:22] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:06:22] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 12:06:22] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:06:22] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:06:22] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:06:22] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:06:22] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:06:22] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:06:22] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 12:06:22] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 12:06:22] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 12:06:22] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 12:06:22] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1333276) [2026-01-26 12:06:24] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1333276) [2026-01-26 12:06:24] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1333276) [2026-01-26 12:06:24] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1333276) [2026-01-26 12:06:24] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=1333276) [2026-01-26 12:06:24] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=1333276) [2026-01-26 12:06:24] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=1333276) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1333276) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.14it/s]
(EngineCore_DP0 pid=1333276) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.54it/s]
(EngineCore_DP0 pid=1333276) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.61it/s]
(EngineCore_DP0 pid=1333276) 
(EngineCore_DP0 pid=1333276) [2026-01-26 12:06:26] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 4800] -> 1D uint8
(EngineCore_DP0 pid=1333276) [2026-01-26 12:06:26] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 13824000 bytes
(EngineCore_DP0 pid=1333276) [2026-01-26 12:06:26] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 4800] -> 1D uint8
(EngineCore_DP0 pid=1333276) [2026-01-26 12:06:26] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10752000 bytes
(EngineCore_DP0 pid=1333276) [2026-01-26 12:06:26] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 4800] -> 1D uint8
(EngineCore_DP0 pid=1333276) [2026-01-26 12:06:26] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 113664000 bytes
(EngineCore_DP0 pid=1333276) [2026-01-26 12:06:26] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 25280] -> 1D uint8
(EngineCore_DP0 pid=1333276) [2026-01-26 12:06:26] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 56627200 bytes
(EngineCore_DP0 pid=1333276) 2026-01-26 12:06:43,138 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1333276) 2026-01-26 12:06:43,163 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1333276) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 1/5 [00:00<00:00,  4.29it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00,  4.88it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 3/5 [00:00<00:00,  2.81it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:01<00:00,  3.90it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:01<00:00,  4.41it/s]
(EngineCore_DP0 pid=1333276) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 1/4 [00:00<00:00,  6.76it/s]
Capturing CUDA graphs (decode, FULL):  75%|███████▌  | 3/4 [00:00<00:00, 12.25it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 12.78it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 23/1024 [00:00<00:04, 227.33it/s]
Adding requests:   6%|▋         | 64/1024 [00:00<00:02, 328.67it/s]
Adding requests:  10%|▉         | 100/1024 [00:00<00:02, 341.60it/s]
Adding requests:  13%|█▎        | 138/1024 [00:00<00:02, 354.19it/s]
Adding requests:  17%|█▋        | 177/1024 [00:00<00:02, 366.81it/s]
Adding requests:  21%|██▏       | 219/1024 [00:00<00:02, 382.69it/s]
Adding requests:  25%|██▌       | 258/1024 [00:00<00:02, 378.75it/s]
Adding requests:  29%|██▉       | 299/1024 [00:00<00:01, 385.38it/s]
Adding requests:  33%|███▎      | 339/1024 [00:00<00:01, 389.79it/s]
Adding requests:  37%|███▋      | 380/1024 [00:01<00:01, 392.62it/s]
Adding requests:  41%|████      | 422/1024 [00:01<00:01, 399.64it/s]
Adding requests:  45%|████▌     | 462/1024 [00:01<00:01, 395.59it/s]
Adding requests:  49%|████▉     | 505/1024 [00:01<00:01, 403.62it/s]
Adding requests:  53%|█████▎    | 546/1024 [00:01<00:01, 403.27it/s]
Adding requests:  57%|█████▋    | 587/1024 [00:01<00:01, 398.49it/s]
Adding requests:  61%|██████    | 627/1024 [00:01<00:01, 396.09it/s]
Adding requests:  65%|██████▌   | 667/1024 [00:01<00:00, 385.62it/s]
Adding requests:  69%|██████▉   | 708/1024 [00:01<00:00, 391.10it/s]
Adding requests:  73%|███████▎  | 748/1024 [00:01<00:00, 384.16it/s]
Adding requests:  77%|███████▋  | 788/1024 [00:02<00:00, 386.56it/s]
Adding requests:  81%|████████  | 828/1024 [00:02<00:00, 387.97it/s]
Adding requests:  85%|████████▍ | 868/1024 [00:02<00:00, 389.06it/s]
Adding requests:  89%|████████▉ | 909/1024 [00:02<00:00, 393.19it/s]
Adding requests:  93%|█████████▎| 949/1024 [00:02<00:00, 384.12it/s]
Adding requests:  97%|█████████▋| 989/1024 [00:02<00:00, 386.61it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 383.61it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  10%|█         | 106/1024 [00:00<00:01, 872.61it/s, est. speed input: 893723.91 toks/s, output: 872.66 toks/s]
Processed prompts:  19%|█▉        | 194/1024 [00:02<00:11, 72.67it/s, est. speed input: 87575.63 toks/s, output: 85.52 toks/s]   
Processed prompts:  23%|██▎       | 233/1024 [00:03<00:12, 65.62it/s, est. speed input: 78723.76 toks/s, output: 76.88 toks/s]
Processed prompts:  25%|██▌       | 257/1024 [00:03<00:12, 59.10it/s, est. speed input: 72769.76 toks/s, output: 71.06 toks/s]
Processed prompts:  27%|██▋       | 273/1024 [00:04<00:13, 55.47it/s, est. speed input: 69782.19 toks/s, output: 68.15 toks/s]
Processed prompts:  28%|██▊       | 285/1024 [00:04<00:14, 50.01it/s, est. speed input: 66415.85 toks/s, output: 64.86 toks/s]
Processed prompts:  29%|██▊       | 294/1024 [00:04<00:14, 49.41it/s, est. speed input: 65572.87 toks/s, output: 64.04 toks/s]
Processed prompts:  29%|██▉       | 302/1024 [00:04<00:15, 47.99it/s, est. speed input: 64596.48 toks/s, output: 63.08 toks/s]
Processed prompts:  30%|███       | 309/1024 [00:04<00:15, 46.65it/s, est. speed input: 63776.06 toks/s, output: 62.28 toks/s]
Processed prompts:  31%|███       | 315/1024 [00:05<00:16, 43.28it/s, est. speed input: 62573.73 toks/s, output: 61.11 toks/s]
Processed prompts:  31%|███▏      | 322/1024 [00:05<00:16, 41.53it/s, est. speed input: 61633.15 toks/s, output: 60.19 toks/s]
Processed prompts:  32%|███▏      | 330/1024 [00:05<00:16, 41.38it/s, est. speed input: 60938.09 toks/s, output: 59.51 toks/s]
Processed prompts:  33%|███▎      | 338/1024 [00:05<00:16, 41.22it/s, est. speed input: 60284.03 toks/s, output: 58.87 toks/s]
Processed prompts:  34%|███▍      | 346/1024 [00:05<00:16, 41.16it/s, est. speed input: 59682.63 toks/s, output: 58.28 toks/s]
Processed prompts:  35%|███▍      | 354/1024 [00:06<00:16, 41.19it/s, est. speed input: 59131.47 toks/s, output: 57.75 toks/s]
Processed prompts:  35%|███▌      | 362/1024 [00:06<00:16, 41.11it/s, est. speed input: 58598.95 toks/s, output: 57.23 toks/s]
Processed prompts:  36%|███▌      | 370/1024 [00:06<00:15, 41.12it/s, est. speed input: 58107.13 toks/s, output: 56.74 toks/s]
Processed prompts:  37%|███▋      | 378/1024 [00:06<00:15, 40.94it/s, est. speed input: 57619.34 toks/s, output: 56.27 toks/s]
Processed prompts:  38%|███▊      | 386/1024 [00:06<00:15, 40.97it/s, est. speed input: 57180.06 toks/s, output: 55.84 toks/s]
Processed prompts:  38%|███▊      | 394/1024 [00:07<00:15, 40.97it/s, est. speed input: 56761.39 toks/s, output: 55.43 toks/s]
Processed prompts:  39%|███▉      | 402/1024 [00:07<00:15, 41.00it/s, est. speed input: 56368.41 toks/s, output: 55.05 toks/s]
Processed prompts:  40%|████      | 410/1024 [00:07<00:14, 41.03it/s, est. speed input: 55998.25 toks/s, output: 54.69 toks/s]
Processed prompts:  41%|████      | 418/1024 [00:07<00:14, 40.84it/s, est. speed input: 55621.79 toks/s, output: 54.32 toks/s]
Processed prompts:  42%|████▏     | 426/1024 [00:07<00:14, 40.91it/s, est. speed input: 55287.05 toks/s, output: 53.99 toks/s]
Processed prompts:  42%|████▏     | 434/1024 [00:08<00:13, 42.33it/s, est. speed input: 55112.70 toks/s, output: 53.82 toks/s]
Processed prompts:  43%|████▎     | 442/1024 [00:08<00:13, 42.02it/s, est. speed input: 54811.87 toks/s, output: 53.53 toks/s]
Processed prompts:  44%|████▍     | 450/1024 [00:08<00:13, 41.67it/s, est. speed input: 54511.93 toks/s, output: 53.23 toks/s]
Processed prompts:  45%|████▍     | 458/1024 [00:08<00:13, 41.30it/s, est. speed input: 54213.48 toks/s, output: 52.94 toks/s]
Processed prompts:  46%|████▌     | 466/1024 [00:08<00:13, 41.14it/s, est. speed input: 53936.51 toks/s, output: 52.67 toks/s]
Processed prompts:  46%|████▋     | 474/1024 [00:09<00:13, 41.16it/s, est. speed input: 53684.54 toks/s, output: 52.43 toks/s]
Processed prompts:  47%|████▋     | 482/1024 [00:09<00:13, 41.12it/s, est. speed input: 53438.51 toks/s, output: 52.19 toks/s]
Processed prompts:  48%|████▊     | 490/1024 [00:09<00:13, 41.05it/s, est. speed input: 53197.99 toks/s, output: 51.95 toks/s]
Processed prompts:  49%|████▊     | 498/1024 [00:09<00:12, 40.89it/s, est. speed input: 52958.47 toks/s, output: 51.72 toks/s]
Processed prompts:  49%|████▉     | 506/1024 [00:09<00:12, 40.84it/s, est. speed input: 52733.08 toks/s, output: 51.50 toks/s]
Processed prompts:  50%|█████     | 514/1024 [00:10<00:12, 40.95it/s, est. speed input: 52528.63 toks/s, output: 51.30 toks/s]
Processed prompts:  51%|█████     | 522/1024 [00:10<00:12, 40.99it/s, est. speed input: 52329.35 toks/s, output: 51.10 toks/s]
Processed prompts:  52%|█████▏    | 530/1024 [00:10<00:12, 40.95it/s, est. speed input: 52132.05 toks/s, output: 50.91 toks/s]
Processed prompts:  53%|█████▎    | 538/1024 [00:10<00:11, 40.80it/s, est. speed input: 51932.31 toks/s, output: 50.71 toks/s]
Processed prompts:  53%|█████▎    | 546/1024 [00:10<00:11, 40.81it/s, est. speed input: 51748.80 toks/s, output: 50.54 toks/s]
Processed prompts:  54%|█████▍    | 554/1024 [00:10<00:11, 40.91it/s, est. speed input: 51578.99 toks/s, output: 50.37 toks/s]
Processed prompts:  55%|█████▍    | 562/1024 [00:11<00:11, 40.90it/s, est. speed input: 51408.51 toks/s, output: 50.20 toks/s]
Processed prompts:  56%|█████▌    | 570/1024 [00:11<00:11, 40.89it/s, est. speed input: 51244.80 toks/s, output: 50.04 toks/s]
Processed prompts:  56%|█████▋    | 578/1024 [00:11<00:10, 40.75it/s, est. speed input: 51076.60 toks/s, output: 49.88 toks/s]
Processed prompts:  57%|█████▋    | 586/1024 [00:11<00:10, 40.69it/s, est. speed input: 50916.67 toks/s, output: 49.72 toks/s]
Processed prompts:  58%|█████▊    | 594/1024 [00:11<00:10, 40.86it/s, est. speed input: 50776.04 toks/s, output: 49.59 toks/s]
Processed prompts:  59%|█████▉    | 602/1024 [00:12<00:10, 40.81it/s, est. speed input: 50629.25 toks/s, output: 49.44 toks/s]
Processed prompts:  60%|█████▉    | 610/1024 [00:12<00:10, 40.84it/s, est. speed input: 50490.97 toks/s, output: 49.31 toks/s]
Processed prompts:  60%|██████    | 618/1024 [00:12<00:09, 40.79it/s, est. speed input: 50352.98 toks/s, output: 49.17 toks/s]
Processed prompts:  61%|██████    | 626/1024 [00:12<00:09, 40.74it/s, est. speed input: 50217.56 toks/s, output: 49.04 toks/s]
Processed prompts:  62%|██████▏   | 634/1024 [00:12<00:09, 40.91it/s, est. speed input: 50099.36 toks/s, output: 48.93 toks/s]
Processed prompts:  63%|██████▎   | 642/1024 [00:13<00:09, 40.85it/s, est. speed input: 49974.01 toks/s, output: 48.80 toks/s]
Processed prompts:  63%|██████▎   | 650/1024 [00:13<00:09, 40.86it/s, est. speed input: 49855.05 toks/s, output: 48.69 toks/s]
Processed prompts:  64%|██████▍   | 658/1024 [00:13<00:08, 40.77it/s, est. speed input: 49733.92 toks/s, output: 48.57 toks/s]
Processed prompts:  65%|██████▌   | 666/1024 [00:13<00:08, 40.62it/s, est. speed input: 49611.29 toks/s, output: 48.45 toks/s]
Processed prompts:  66%|██████▌   | 674/1024 [00:13<00:08, 40.75it/s, est. speed input: 49505.34 toks/s, output: 48.34 toks/s]
Processed prompts:  67%|██████▋   | 682/1024 [00:14<00:08, 40.78it/s, est. speed input: 49399.16 toks/s, output: 48.24 toks/s]
Processed prompts:  67%|██████▋   | 690/1024 [00:14<00:08, 40.89it/s, est. speed input: 49300.33 toks/s, output: 48.14 toks/s]
Processed prompts:  68%|██████▊   | 698/1024 [00:14<00:07, 40.90it/s, est. speed input: 49200.99 toks/s, output: 48.05 toks/s]
Processed prompts:  69%|██████▉   | 706/1024 [00:14<00:07, 40.79it/s, est. speed input: 49098.11 toks/s, output: 47.95 toks/s]
Processed prompts:  70%|██████▉   | 714/1024 [00:14<00:07, 40.92it/s, est. speed input: 49008.65 toks/s, output: 47.86 toks/s]
Processed prompts:  71%|███████   | 722/1024 [00:15<00:07, 41.04it/s, est. speed input: 48922.70 toks/s, output: 47.78 toks/s]
Processed prompts:  71%|███████▏  | 730/1024 [00:15<00:07, 41.05it/s, est. speed input: 48835.43 toks/s, output: 47.69 toks/s]
Processed prompts:  72%|███████▏  | 738/1024 [00:15<00:06, 41.07it/s, est. speed input: 48750.97 toks/s, output: 47.61 toks/s]
Processed prompts:  73%|███████▎  | 746/1024 [00:15<00:06, 40.89it/s, est. speed input: 48658.77 toks/s, output: 47.52 toks/s]
Processed prompts:  74%|███████▎  | 754/1024 [00:15<00:06, 40.96it/s, est. speed input: 48579.06 toks/s, output: 47.44 toks/s]
Processed prompts:  74%|███████▍  | 762/1024 [00:16<00:06, 41.02it/s, est. speed input: 48501.34 toks/s, output: 47.36 toks/s]
Processed prompts:  75%|███████▌  | 770/1024 [00:16<00:06, 41.13it/s, est. speed input: 48428.62 toks/s, output: 47.29 toks/s]
Processed prompts:  76%|███████▌  | 778/1024 [00:16<00:05, 41.06it/s, est. speed input: 48350.97 toks/s, output: 47.22 toks/s]
Processed prompts:  77%|███████▋  | 786/1024 [00:16<00:05, 42.37it/s, est. speed input: 48335.23 toks/s, output: 47.20 toks/s]
Processed prompts:  78%|███████▊  | 794/1024 [00:16<00:05, 41.87it/s, est. speed input: 48258.20 toks/s, output: 47.13 toks/s]
Processed prompts:  78%|███████▊  | 802/1024 [00:17<00:05, 41.65it/s, est. speed input: 48188.39 toks/s, output: 47.06 toks/s]
Processed prompts:  79%|███████▉  | 810/1024 [00:17<00:05, 41.57it/s, est. speed input: 48123.33 toks/s, output: 47.00 toks/s]
Processed prompts:  80%|███████▉  | 818/1024 [00:17<00:04, 41.45it/s, est. speed input: 48057.00 toks/s, output: 46.93 toks/s]
Processed prompts:  81%|████████  | 826/1024 [00:17<00:04, 41.19it/s, est. speed input: 47984.29 toks/s, output: 46.86 toks/s]
Processed prompts:  81%|████████▏ | 834/1024 [00:17<00:04, 41.01it/s, est. speed input: 47913.51 toks/s, output: 46.79 toks/s]
Processed prompts:  82%|████████▏ | 842/1024 [00:18<00:04, 41.09it/s, est. speed input: 47852.73 toks/s, output: 46.73 toks/s]
Processed prompts:  83%|████████▎ | 850/1024 [00:18<00:04, 41.16it/s, est. speed input: 47794.03 toks/s, output: 46.67 toks/s]
Processed prompts:  84%|████████▍ | 858/1024 [00:18<00:04, 41.20it/s, est. speed input: 47735.88 toks/s, output: 46.62 toks/s]
Processed prompts:  85%|████████▍ | 866/1024 [00:18<00:03, 41.09it/s, est. speed input: 47673.58 toks/s, output: 46.56 toks/s]
Processed prompts:  85%|████████▌ | 874/1024 [00:18<00:03, 40.96it/s, est. speed input: 47610.22 toks/s, output: 46.49 toks/s]
Processed prompts:  86%|████████▌ | 882/1024 [00:18<00:03, 40.88it/s, est. speed input: 47548.98 toks/s, output: 46.43 toks/s]
Processed prompts:  87%|████████▋ | 890/1024 [00:19<00:03, 41.00it/s, est. speed input: 47495.60 toks/s, output: 46.38 toks/s]
Processed prompts:  88%|████████▊ | 898/1024 [00:19<00:03, 41.06it/s, est. speed input: 47442.33 toks/s, output: 46.33 toks/s]
Processed prompts:  88%|████████▊ | 906/1024 [00:19<00:02, 41.01it/s, est. speed input: 47386.73 toks/s, output: 46.28 toks/s]
Processed prompts:  89%|████████▉ | 914/1024 [00:19<00:02, 40.85it/s, est. speed input: 47327.36 toks/s, output: 46.22 toks/s]
Processed prompts:  90%|█████████ | 922/1024 [00:19<00:02, 40.81it/s, est. speed input: 47272.12 toks/s, output: 46.16 toks/s]
Processed prompts:  91%|█████████ | 930/1024 [00:20<00:02, 40.93it/s, est. speed input: 47223.31 toks/s, output: 46.12 toks/s]
Processed prompts:  92%|█████████▏| 938/1024 [00:20<00:02, 41.06it/s, est. speed input: 47177.09 toks/s, output: 46.07 toks/s]
Processed prompts:  92%|█████████▏| 946/1024 [00:20<00:01, 41.03it/s, est. speed input: 47127.25 toks/s, output: 46.02 toks/s]
Processed prompts:  93%|█████████▎| 954/1024 [00:20<00:01, 40.93it/s, est. speed input: 47075.50 toks/s, output: 45.97 toks/s]
Processed prompts:  94%|█████████▍| 962/1024 [00:20<00:01, 40.91it/s, est. speed input: 47026.81 toks/s, output: 45.92 toks/s]
Processed prompts:  95%|█████████▍| 970/1024 [00:21<00:01, 40.95it/s, est. speed input: 46980.69 toks/s, output: 45.88 toks/s]
Processed prompts:  96%|█████████▌| 978/1024 [00:21<00:01, 41.05it/s, est. speed input: 46937.80 toks/s, output: 45.84 toks/s]
Processed prompts:  96%|█████████▋| 986/1024 [00:21<00:00, 41.05it/s, est. speed input: 46893.45 toks/s, output: 45.79 toks/s]
Processed prompts:  97%|█████████▋| 994/1024 [00:21<00:00, 40.89it/s, est. speed input: 46844.55 toks/s, output: 45.75 toks/s]
Processed prompts:  98%|█████████▊| 1002/1024 [00:21<00:00, 40.83it/s, est. speed input: 46798.01 toks/s, output: 45.70 toks/s]
Processed prompts:  99%|█████████▊| 1010/1024 [00:22<00:00, 40.96it/s, est. speed input: 46758.37 toks/s, output: 45.66 toks/s]
Processed prompts:  99%|█████████▉| 1018/1024 [00:22<00:00, 42.52it/s, est. speed input: 46766.05 toks/s, output: 45.67 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:22<00:00, 42.52it/s, est. speed input: 47041.11 toks/s, output: 45.94 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:22<00:00, 45.94it/s, est. speed input: 47041.11 toks/s, output: 45.94 toks/s]
[rank0]:[W126 12:07:11.378420596 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 70.3s

测试结果:
  Requests/s:   41.02
  Tokens/s:     42047.93
  Total Reqs:   1024
  Elapsed:      24.96s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     42006.91

============================================================
[6/8] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 12:07:31 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1334794) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1334794) WARNING 01-26 12:07:48 [backends.py:609] Failed to read file <frozen os>
Throughput: 42.02 requests/s, 43075.10 total tokens/s, 42.02 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-26 12:07:30] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 12:07:31] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:07:31] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 12:07:31] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:07:31] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:07:31] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:07:31] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:07:31] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:07:31] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:07:31] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 12:07:31] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 12:07:31] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 12:07:31] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 12:07:31] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 12:07:37] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 12:07:38] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:07:38] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 12:07:38] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:07:38] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:07:38] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:07:38] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:07:38] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:07:38] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:07:38] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 12:07:38] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 12:07:38] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 12:07:38] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 12:07:38] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1334794) [2026-01-26 12:07:40] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1334794) [2026-01-26 12:07:40] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1334794) [2026-01-26 12:07:40] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1334794) [2026-01-26 12:07:40] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=1334794) [2026-01-26 12:07:40] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=1334794) [2026-01-26 12:07:40] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=1334794) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1334794) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.13it/s]
(EngineCore_DP0 pid=1334794) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.53it/s]
(EngineCore_DP0 pid=1334794) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.60it/s]
(EngineCore_DP0 pid=1334794) 
(EngineCore_DP0 pid=1334794) [2026-01-26 12:07:42] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 4800] -> 1D uint8
(EngineCore_DP0 pid=1334794) [2026-01-26 12:07:42] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 13824000 bytes
(EngineCore_DP0 pid=1334794) [2026-01-26 12:07:42] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 4800] -> 1D uint8
(EngineCore_DP0 pid=1334794) [2026-01-26 12:07:42] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10752000 bytes
(EngineCore_DP0 pid=1334794) [2026-01-26 12:07:42] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 4800] -> 1D uint8
(EngineCore_DP0 pid=1334794) [2026-01-26 12:07:42] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 113664000 bytes
(EngineCore_DP0 pid=1334794) [2026-01-26 12:07:42] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 25280] -> 1D uint8
(EngineCore_DP0 pid=1334794) [2026-01-26 12:07:42] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 56627200 bytes
(EngineCore_DP0 pid=1334794) [rank0]:W0126 12:07:53.672000 1334794 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1334794) [rank0]:W0126 12:07:53.750000 1334794 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1334794) [rank0]:W0126 12:07:54.970000 1334794 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1334794) [rank0]:W0126 12:07:55.102000 1334794 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1334794) 2026-01-26 12:07:58,800 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1334794) 2026-01-26 12:07:58,825 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1334794) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 1/7 [00:00<00:03,  1.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:01,  2.88it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00,  6.12it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:00<00:00,  9.10it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:01<00:00,  6.56it/s]
(EngineCore_DP0 pid=1334794) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00, 18.99it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 20.01it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 19.87it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   1%|          | 23/2048 [00:00<00:08, 227.98it/s]
Adding requests:   3%|▎         | 63/2048 [00:00<00:06, 325.07it/s]
Adding requests:   5%|▍         | 97/2048 [00:00<00:05, 328.26it/s]
Adding requests:   7%|▋         | 135/2048 [00:00<00:05, 347.29it/s]
Adding requests:   8%|▊         | 174/2048 [00:00<00:05, 361.49it/s]
Adding requests:  10%|█         | 214/2048 [00:00<00:04, 371.94it/s]
Adding requests:  12%|█▏        | 252/2048 [00:00<00:04, 372.88it/s]
Adding requests:  14%|█▍        | 290/2048 [00:00<00:04, 373.79it/s]
Adding requests:  16%|█▌        | 330/2048 [00:00<00:04, 380.10it/s]
Adding requests:  18%|█▊        | 370/2048 [00:01<00:04, 384.93it/s]
Adding requests:  20%|██        | 411/2048 [00:01<00:04, 388.97it/s]
Adding requests:  22%|██▏       | 450/2048 [00:01<00:04, 384.97it/s]
Adding requests:  24%|██▍       | 493/2048 [00:01<00:03, 396.14it/s]
Adding requests:  26%|██▌       | 535/2048 [00:01<00:03, 401.33it/s]
Adding requests:  28%|██▊       | 576/2048 [00:01<00:03, 397.77it/s]
Adding requests:  30%|███       | 616/2048 [00:01<00:03, 386.11it/s]
Adding requests:  32%|███▏      | 655/2048 [00:01<00:03, 378.85it/s]
Adding requests:  34%|███▍      | 695/2048 [00:01<00:03, 382.42it/s]
Adding requests:  36%|███▌      | 734/2048 [00:01<00:03, 376.59it/s]
Adding requests:  38%|███▊      | 772/2048 [00:02<00:03, 369.18it/s]
Adding requests:  40%|███▉      | 810/2048 [00:02<00:03, 371.64it/s]
Adding requests:  42%|████▏     | 850/2048 [00:02<00:03, 379.24it/s]
Adding requests:  43%|████▎     | 890/2048 [00:02<00:03, 383.54it/s]
Adding requests:  45%|████▌     | 929/2048 [00:02<00:02, 375.41it/s]
Adding requests:  47%|████▋     | 969/2048 [00:02<00:02, 379.20it/s]
Adding requests:  49%|████▉     | 1007/2048 [00:02<00:02, 372.43it/s]
Adding requests:  51%|█████     | 1045/2048 [00:02<00:02, 372.78it/s]
Adding requests:  53%|█████▎    | 1083/2048 [00:02<00:02, 372.45it/s]
Adding requests:  55%|█████▍    | 1121/2048 [00:02<00:02, 371.68it/s]
Adding requests:  57%|█████▋    | 1159/2048 [00:03<00:02, 371.22it/s]
Adding requests:  58%|█████▊    | 1198/2048 [00:03<00:02, 374.16it/s]
Adding requests:  60%|██████    | 1238/2048 [00:03<00:02, 380.46it/s]
Adding requests:  62%|██████▏   | 1277/2048 [00:03<00:02, 376.23it/s]
Adding requests:  64%|██████▍   | 1315/2048 [00:03<00:01, 376.22it/s]
Adding requests:  66%|██████▌   | 1355/2048 [00:03<00:01, 381.10it/s]
Adding requests:  68%|██████▊   | 1394/2048 [00:03<00:01, 380.58it/s]
Adding requests:  70%|██████▉   | 1433/2048 [00:03<00:01, 377.54it/s]
Adding requests:  72%|███████▏  | 1472/2048 [00:03<00:01, 378.02it/s]
Adding requests:  74%|███████▍  | 1513/2048 [00:04<00:01, 384.86it/s]
Adding requests:  76%|███████▌  | 1552/2048 [00:04<00:01, 380.04it/s]
Adding requests:  78%|███████▊  | 1591/2048 [00:04<00:01, 374.69it/s]
Adding requests:  80%|███████▉  | 1629/2048 [00:04<00:01, 367.88it/s]
Adding requests:  81%|████████▏ | 1666/2048 [00:04<00:01, 359.99it/s]
Adding requests:  83%|████████▎ | 1705/2048 [00:04<00:00, 366.81it/s]
Adding requests:  85%|████████▌ | 1743/2048 [00:04<00:00, 369.64it/s]
Adding requests:  87%|████████▋ | 1783/2048 [00:04<00:00, 377.42it/s]
Adding requests:  89%|████████▉ | 1821/2048 [00:04<00:00, 372.90it/s]
Adding requests:  91%|█████████ | 1860/2048 [00:04<00:00, 377.81it/s]
Adding requests:  93%|█████████▎| 1898/2048 [00:05<00:00, 377.16it/s]
Adding requests:  95%|█████████▍| 1936/2048 [00:05<00:00, 373.44it/s]
Adding requests:  96%|█████████▋| 1975/2048 [00:05<00:00, 377.11it/s]
Adding requests:  98%|█████████▊| 2013/2048 [00:05<00:00, 373.82it/s]
Adding requests: 100%|██████████| 2048/2048 [00:05<00:00, 374.61it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  11%|█         | 226/2048 [00:00<00:02, 677.13it/s, est. speed input: 693433.95 toks/s, output: 677.15 toks/s]
Processed prompts:  14%|█▍        | 294/2048 [00:01<00:13, 127.96it/s, est. speed input: 161179.88 toks/s, output: 157.40 toks/s]
Processed prompts:  16%|█▌        | 325/2048 [00:02<00:17, 95.87it/s, est. speed input: 127733.50 toks/s, output: 124.74 toks/s] 
Processed prompts:  17%|█▋        | 344/2048 [00:02<00:19, 85.81it/s, est. speed input: 117851.91 toks/s, output: 115.09 toks/s]
Processed prompts:  17%|█▋        | 358/2048 [00:03<00:22, 74.28it/s, est. speed input: 108802.81 toks/s, output: 106.25 toks/s]
Processed prompts:  18%|█▊        | 370/2048 [00:03<00:26, 63.49it/s, est. speed input: 101021.49 toks/s, output: 98.65 toks/s] 
Processed prompts:  19%|█▉        | 386/2048 [00:04<00:28, 57.70it/s, est. speed input: 95612.87 toks/s, output: 93.37 toks/s] 
Processed prompts:  20%|█▉        | 402/2048 [00:04<00:30, 53.38it/s, est. speed input: 91163.78 toks/s, output: 89.03 toks/s]
Processed prompts:  20%|██        | 418/2048 [00:04<00:32, 50.01it/s, est. speed input: 87333.80 toks/s, output: 85.29 toks/s]
Processed prompts:  21%|██        | 434/2048 [00:05<00:33, 48.40it/s, est. speed input: 84435.32 toks/s, output: 82.46 toks/s]
Processed prompts:  22%|██▏       | 450/2048 [00:05<00:34, 46.55it/s, est. speed input: 81651.69 toks/s, output: 79.74 toks/s]
Processed prompts:  23%|██▎       | 466/2048 [00:06<00:35, 45.09it/s, est. speed input: 79164.42 toks/s, output: 77.31 toks/s]
Processed prompts:  24%|██▎       | 482/2048 [00:06<00:35, 44.09it/s, est. speed input: 76984.26 toks/s, output: 75.18 toks/s]
Processed prompts:  24%|██▍       | 498/2048 [00:06<00:35, 43.51it/s, est. speed input: 75092.61 toks/s, output: 73.33 toks/s]
Processed prompts:  25%|██▌       | 514/2048 [00:07<00:35, 43.09it/s, est. speed input: 73397.18 toks/s, output: 71.68 toks/s]
Processed prompts:  26%|██▌       | 530/2048 [00:07<00:35, 42.85it/s, est. speed input: 71888.74 toks/s, output: 70.20 toks/s]
Processed prompts:  27%|██▋       | 546/2048 [00:07<00:35, 42.62it/s, est. speed input: 70508.41 toks/s, output: 68.86 toks/s]
Processed prompts:  27%|██▋       | 562/2048 [00:08<00:34, 42.48it/s, est. speed input: 69257.91 toks/s, output: 67.63 toks/s]
Processed prompts:  28%|██▊       | 578/2048 [00:08<00:34, 42.40it/s, est. speed input: 68122.23 toks/s, output: 66.53 toks/s]
Processed prompts:  29%|██▉       | 594/2048 [00:09<00:34, 42.28it/s, est. speed input: 67066.89 toks/s, output: 65.49 toks/s]
Processed prompts:  30%|██▉       | 610/2048 [00:09<00:34, 42.24it/s, est. speed input: 66105.84 toks/s, output: 64.56 toks/s]
Processed prompts:  31%|███       | 626/2048 [00:09<00:33, 42.20it/s, est. speed input: 65217.32 toks/s, output: 63.69 toks/s]
Processed prompts:  31%|███▏      | 642/2048 [00:10<00:33, 42.17it/s, est. speed input: 64395.22 toks/s, output: 62.89 toks/s]
Processed prompts:  32%|███▏      | 658/2048 [00:10<00:32, 42.13it/s, est. speed input: 63628.16 toks/s, output: 62.14 toks/s]
Processed prompts:  33%|███▎      | 674/2048 [00:10<00:32, 42.12it/s, est. speed input: 62916.50 toks/s, output: 61.44 toks/s]
Processed prompts:  34%|███▎      | 690/2048 [00:11<00:32, 42.12it/s, est. speed input: 62254.25 toks/s, output: 60.79 toks/s]
Processed prompts:  34%|███▍      | 706/2048 [00:11<00:31, 42.10it/s, est. speed input: 61631.64 toks/s, output: 60.19 toks/s]
Processed prompts:  35%|███▌      | 722/2048 [00:12<00:31, 42.10it/s, est. speed input: 61050.12 toks/s, output: 59.62 toks/s]
Processed prompts:  36%|███▌      | 738/2048 [00:12<00:31, 42.11it/s, est. speed input: 60506.64 toks/s, output: 59.09 toks/s]
Processed prompts:  37%|███▋      | 754/2048 [00:12<00:30, 42.05it/s, est. speed input: 59984.46 toks/s, output: 58.58 toks/s]
Processed prompts:  38%|███▊      | 770/2048 [00:13<00:30, 42.09it/s, est. speed input: 59503.28 toks/s, output: 58.11 toks/s]
Processed prompts:  38%|███▊      | 786/2048 [00:13<00:29, 42.71it/s, est. speed input: 59125.77 toks/s, output: 57.74 toks/s]
Processed prompts:  39%|███▉      | 802/2048 [00:13<00:29, 42.41it/s, est. speed input: 58676.80 toks/s, output: 57.30 toks/s]
Processed prompts:  40%|███▉      | 818/2048 [00:14<00:29, 42.34it/s, est. speed input: 58268.24 toks/s, output: 56.90 toks/s]
Processed prompts:  41%|████      | 834/2048 [00:14<00:28, 42.23it/s, est. speed input: 57872.94 toks/s, output: 56.52 toks/s]
Processed prompts:  42%|████▏     | 850/2048 [00:15<00:28, 42.15it/s, est. speed input: 57497.40 toks/s, output: 56.15 toks/s]
Processed prompts:  42%|████▏     | 866/2048 [00:15<00:28, 42.14it/s, est. speed input: 57146.19 toks/s, output: 55.81 toks/s]
Processed prompts:  43%|████▎     | 882/2048 [00:15<00:27, 42.09it/s, est. speed input: 56805.97 toks/s, output: 55.47 toks/s]
Processed prompts:  44%|████▍     | 898/2048 [00:16<00:27, 42.01it/s, est. speed input: 56477.46 toks/s, output: 55.15 toks/s]
Processed prompts:  45%|████▍     | 914/2048 [00:16<00:27, 41.97it/s, est. speed input: 56165.65 toks/s, output: 54.85 toks/s]
Processed prompts:  45%|████▌     | 930/2048 [00:17<00:26, 41.99it/s, est. speed input: 55872.77 toks/s, output: 54.56 toks/s]
Processed prompts:  46%|████▌     | 946/2048 [00:17<00:26, 41.91it/s, est. speed input: 55583.35 toks/s, output: 54.28 toks/s]
Processed prompts:  47%|████▋     | 962/2048 [00:17<00:25, 41.91it/s, est. speed input: 55312.41 toks/s, output: 54.02 toks/s]
Processed prompts:  48%|████▊     | 978/2048 [00:18<00:25, 41.92it/s, est. speed input: 55052.79 toks/s, output: 53.76 toks/s]
Processed prompts:  49%|████▊     | 994/2048 [00:18<00:25, 41.85it/s, est. speed input: 54797.08 toks/s, output: 53.51 toks/s]
Processed prompts:  49%|████▉     | 1010/2048 [00:18<00:24, 41.87it/s, est. speed input: 54557.91 toks/s, output: 53.28 toks/s]
Processed prompts:  50%|█████     | 1026/2048 [00:19<00:24, 41.80it/s, est. speed input: 54321.40 toks/s, output: 53.05 toks/s]
Processed prompts:  51%|█████     | 1042/2048 [00:19<00:24, 41.81it/s, est. speed input: 54098.46 toks/s, output: 52.83 toks/s]
Processed prompts:  52%|█████▏    | 1058/2048 [00:20<00:23, 41.74it/s, est. speed input: 53878.17 toks/s, output: 52.62 toks/s]
Processed prompts:  52%|█████▏    | 1074/2048 [00:20<00:23, 41.79it/s, est. speed input: 53673.58 toks/s, output: 52.42 toks/s]
Processed prompts:  53%|█████▎    | 1090/2048 [00:20<00:22, 41.77it/s, est. speed input: 53472.35 toks/s, output: 52.22 toks/s]
Processed prompts:  54%|█████▍    | 1106/2048 [00:21<00:22, 41.74it/s, est. speed input: 53277.37 toks/s, output: 52.03 toks/s]
Processed prompts:  55%|█████▍    | 1122/2048 [00:21<00:22, 41.84it/s, est. speed input: 53097.94 toks/s, output: 51.85 toks/s]
Processed prompts:  56%|█████▌    | 1138/2048 [00:22<00:21, 41.88it/s, est. speed input: 52922.83 toks/s, output: 51.68 toks/s]
Processed prompts:  56%|█████▋    | 1154/2048 [00:22<00:21, 41.83it/s, est. speed input: 52748.10 toks/s, output: 51.51 toks/s]
Processed prompts:  57%|█████▋    | 1170/2048 [00:22<00:20, 41.87it/s, est. speed input: 52584.41 toks/s, output: 51.35 toks/s]
Processed prompts:  58%|█████▊    | 1186/2048 [00:23<00:20, 41.89it/s, est. speed input: 52425.68 toks/s, output: 51.20 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [00:23<00:19, 42.54it/s, est. speed input: 52314.44 toks/s, output: 51.09 toks/s]
Processed prompts:  59%|█████▉    | 1218/2048 [00:23<00:19, 42.31it/s, est. speed input: 52162.01 toks/s, output: 50.94 toks/s]
Processed prompts:  60%|██████    | 1234/2048 [00:24<00:18, 42.87it/s, est. speed input: 52059.69 toks/s, output: 50.84 toks/s]
Processed prompts:  61%|██████    | 1250/2048 [00:24<00:18, 42.51it/s, est. speed input: 51913.74 toks/s, output: 50.70 toks/s]
Processed prompts:  62%|██████▏   | 1266/2048 [00:25<00:18, 42.33it/s, est. speed input: 51776.92 toks/s, output: 50.56 toks/s]
Processed prompts:  63%|██████▎   | 1282/2048 [00:25<00:18, 42.21it/s, est. speed input: 51643.81 toks/s, output: 50.43 toks/s]
Processed prompts:  63%|██████▎   | 1298/2048 [00:25<00:17, 42.06it/s, est. speed input: 51511.53 toks/s, output: 50.30 toks/s]
Processed prompts:  64%|██████▍   | 1314/2048 [00:26<00:17, 42.02it/s, est. speed input: 51386.31 toks/s, output: 50.18 toks/s]
Processed prompts:  65%|██████▍   | 1330/2048 [00:26<00:16, 42.63it/s, est. speed input: 51301.68 toks/s, output: 50.10 toks/s]
Processed prompts:  66%|██████▌   | 1346/2048 [00:26<00:16, 42.35it/s, est. speed input: 51179.39 toks/s, output: 49.98 toks/s]
Processed prompts:  67%|██████▋   | 1362/2048 [00:27<00:16, 42.26it/s, est. speed input: 51066.10 toks/s, output: 49.87 toks/s]
Processed prompts:  67%|██████▋   | 1378/2048 [00:27<00:15, 42.11it/s, est. speed input: 50951.13 toks/s, output: 49.76 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [00:28<00:15, 41.95it/s, est. speed input: 50836.79 toks/s, output: 49.65 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [00:28<00:15, 41.85it/s, est. speed input: 50725.78 toks/s, output: 49.54 toks/s]
Processed prompts:  70%|██████▉   | 1426/2048 [00:28<00:14, 41.87it/s, est. speed input: 50622.39 toks/s, output: 49.44 toks/s]
Processed prompts:  70%|███████   | 1442/2048 [00:29<00:14, 41.83it/s, est. speed input: 50518.86 toks/s, output: 49.33 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [00:29<00:13, 42.49it/s, est. speed input: 50453.63 toks/s, output: 49.27 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [00:29<00:13, 42.39it/s, est. speed input: 50361.20 toks/s, output: 49.18 toks/s]
Processed prompts:  73%|███████▎  | 1490/2048 [00:30<00:13, 42.14it/s, est. speed input: 50262.21 toks/s, output: 49.08 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [00:30<00:12, 42.04it/s, est. speed input: 50169.41 toks/s, output: 48.99 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [00:31<00:12, 42.60it/s, est. speed input: 50109.05 toks/s, output: 48.93 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [00:31<00:12, 42.31it/s, est. speed input: 50018.43 toks/s, output: 48.85 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [00:31<00:11, 42.86it/s, est. speed input: 49964.37 toks/s, output: 48.79 toks/s]
Processed prompts:  77%|███████▋  | 1570/2048 [00:32<00:11, 42.53it/s, est. speed input: 49879.11 toks/s, output: 48.71 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [00:32<00:10, 42.25it/s, est. speed input: 49793.14 toks/s, output: 48.63 toks/s]
Processed prompts:  78%|███████▊  | 1602/2048 [00:32<00:10, 42.13it/s, est. speed input: 49713.05 toks/s, output: 48.55 toks/s]
Processed prompts:  79%|███████▉  | 1618/2048 [00:33<00:10, 42.61it/s, est. speed input: 49659.40 toks/s, output: 48.50 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [00:33<00:09, 42.34it/s, est. speed input: 49580.44 toks/s, output: 48.42 toks/s]
Processed prompts:  81%|████████  | 1650/2048 [00:34<00:09, 42.13it/s, est. speed input: 49502.61 toks/s, output: 48.34 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [00:34<00:09, 41.97it/s, est. speed input: 49425.73 toks/s, output: 48.27 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [00:34<00:08, 41.82it/s, est. speed input: 49348.83 toks/s, output: 48.19 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [00:35<00:08, 41.81it/s, est. speed input: 49277.49 toks/s, output: 48.12 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [00:35<00:08, 41.73it/s, est. speed input: 49205.00 toks/s, output: 48.05 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [00:36<00:07, 42.40it/s, est. speed input: 49163.59 toks/s, output: 48.01 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [00:36<00:07, 42.85it/s, est. speed input: 49121.99 toks/s, output: 47.97 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [00:36<00:06, 42.43it/s, est. speed input: 49052.25 toks/s, output: 47.90 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [00:37<00:06, 42.18it/s, est. speed input: 48985.57 toks/s, output: 47.84 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [00:37<00:06, 42.04it/s, est. speed input: 48921.45 toks/s, output: 47.77 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [00:37<00:05, 41.86it/s, est. speed input: 48855.57 toks/s, output: 47.71 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [00:38<00:05, 41.78it/s, est. speed input: 48792.61 toks/s, output: 47.65 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [00:38<00:04, 41.67it/s, est. speed input: 48728.88 toks/s, output: 47.59 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [00:39<00:04, 41.65it/s, est. speed input: 48668.50 toks/s, output: 47.53 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [00:39<00:04, 41.61it/s, est. speed input: 48608.50 toks/s, output: 47.47 toks/s]
Processed prompts:  92%|█████████▏| 1890/2048 [00:39<00:03, 42.31it/s, est. speed input: 48576.53 toks/s, output: 47.44 toks/s]
Processed prompts:  93%|█████████▎| 1906/2048 [00:40<00:03, 42.10it/s, est. speed input: 48519.64 toks/s, output: 47.38 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [00:40<00:03, 41.93it/s, est. speed input: 48462.77 toks/s, output: 47.33 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [00:40<00:02, 41.92it/s, est. speed input: 48410.97 toks/s, output: 47.28 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [00:41<00:02, 41.78it/s, est. speed input: 48355.37 toks/s, output: 47.22 toks/s]
Processed prompts:  96%|█████████▌| 1970/2048 [00:41<00:01, 41.70it/s, est. speed input: 48301.46 toks/s, output: 47.17 toks/s]
Processed prompts:  97%|█████████▋| 1986/2048 [00:42<00:01, 42.37it/s, est. speed input: 48273.54 toks/s, output: 47.14 toks/s]
Processed prompts:  98%|█████████▊| 2002/2048 [00:42<00:01, 42.06it/s, est. speed input: 48219.83 toks/s, output: 47.09 toks/s]
Processed prompts:  99%|█████████▊| 2018/2048 [00:42<00:00, 41.87it/s, est. speed input: 48167.49 toks/s, output: 47.04 toks/s]
Processed prompts:  99%|█████████▉| 2034/2048 [00:43<00:00, 42.50it/s, est. speed input: 48141.88 toks/s, output: 47.01 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:43<00:00, 42.50it/s, est. speed input: 48472.85 toks/s, output: 47.34 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:43<00:00, 47.34it/s, est. speed input: 48472.85 toks/s, output: 47.34 toks/s]
[rank0]:[W126 12:08:50.738870472 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 99.7s

测试结果:
  Requests/s:   42.02
  Tokens/s:     43075.10
  Total Reqs:   2048
  Elapsed:      48.73s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     43033.08

============================================================
[7/8] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 12:09:21 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1336757) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1336757) WARNING 01-26 12:09:38 [backends.py:609] Failed to read file <frozen os>
Throughput: 41.43 requests/s, 42463.70 total tokens/s, 41.43 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-26 12:09:21] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 12:09:21] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:09:21] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 12:09:21] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:09:21] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:09:21] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:09:21] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:09:21] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:09:21] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:09:21] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 12:09:21] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 12:09:21] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 12:09:21] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 12:09:21] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 12:09:28] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 12:09:28] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:09:28] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 12:09:28] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:09:28] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:09:28] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:09:28] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:09:28] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:09:28] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:09:28] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 12:09:28] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 12:09:28] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 12:09:28] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 12:09:28] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1336757) [2026-01-26 12:09:30] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1336757) [2026-01-26 12:09:30] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1336757) [2026-01-26 12:09:30] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1336757) [2026-01-26 12:09:30] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=1336757) [2026-01-26 12:09:30] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=1336757) [2026-01-26 12:09:30] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=1336757) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1336757) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.15it/s]
(EngineCore_DP0 pid=1336757) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.55it/s]
(EngineCore_DP0 pid=1336757) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.61it/s]
(EngineCore_DP0 pid=1336757) 
(EngineCore_DP0 pid=1336757) [2026-01-26 12:09:32] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 4800] -> 1D uint8
(EngineCore_DP0 pid=1336757) [2026-01-26 12:09:32] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 13824000 bytes
(EngineCore_DP0 pid=1336757) [2026-01-26 12:09:32] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 4800] -> 1D uint8
(EngineCore_DP0 pid=1336757) [2026-01-26 12:09:32] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10752000 bytes
(EngineCore_DP0 pid=1336757) [2026-01-26 12:09:32] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 4800] -> 1D uint8
(EngineCore_DP0 pid=1336757) [2026-01-26 12:09:32] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 113664000 bytes
(EngineCore_DP0 pid=1336757) [2026-01-26 12:09:32] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 25280] -> 1D uint8
(EngineCore_DP0 pid=1336757) [2026-01-26 12:09:32] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 56627200 bytes
(EngineCore_DP0 pid=1336757) [rank0]:W0126 12:09:43.632000 1336757 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1336757) [rank0]:W0126 12:09:43.710000 1336757 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1336757) [rank0]:W0126 12:09:44.683000 1336757 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1336757) [rank0]:W0126 12:09:44.811000 1336757 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1336757) 2026-01-26 12:09:48,564 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1336757) 2026-01-26 12:09:48,590 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1336757) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▉         | 1/11 [00:00<00:07,  1.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:01<00:04,  1.82it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 3/11 [00:01<00:02,  2.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 5/11 [00:01<00:01,  5.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▎   | 7/11 [00:01<00:00,  8.19it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 9/11 [00:01<00:00, 10.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00, 12.17it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00,  6.49it/s]
(EngineCore_DP0 pid=1336757) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00,  9.77it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00,  6.02it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 5/7 [00:00<00:00,  4.75it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:01<00:00,  6.92it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:01<00:00,  6.51it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 26/4096 [00:00<00:16, 253.07it/s]
Adding requests:   2%|▏         | 66/4096 [00:00<00:11, 337.74it/s]
Adding requests:   2%|▏         | 102/4096 [00:00<00:11, 346.90it/s]
Adding requests:   3%|▎         | 139/4096 [00:00<00:11, 353.63it/s]
Adding requests:   4%|▍         | 177/4096 [00:00<00:10, 362.83it/s]
Adding requests:   5%|▌         | 216/4096 [00:00<00:10, 371.85it/s]
Adding requests:   6%|▌         | 254/4096 [00:00<00:10, 370.76it/s]
Adding requests:   7%|▋         | 292/4096 [00:00<00:10, 373.03it/s]
Adding requests:   8%|▊         | 331/4096 [00:00<00:09, 376.95it/s]
Adding requests:   9%|▉         | 371/4096 [00:01<00:09, 381.66it/s]
Adding requests:  10%|█         | 411/4096 [00:01<00:09, 385.84it/s]
Adding requests:  11%|█         | 450/4096 [00:01<00:09, 381.92it/s]
Adding requests:  12%|█▏        | 492/4096 [00:01<00:09, 393.02it/s]
Adding requests:  13%|█▎        | 533/4096 [00:01<00:08, 398.04it/s]
Adding requests:  14%|█▍        | 573/4096 [00:01<00:08, 394.82it/s]
Adding requests:  15%|█▍        | 613/4096 [00:01<00:09, 381.79it/s]
Adding requests:  16%|█▌        | 652/4096 [00:01<00:09, 374.48it/s]
Adding requests:  17%|█▋        | 691/4096 [00:01<00:08, 378.80it/s]
Adding requests:  18%|█▊        | 729/4096 [00:01<00:09, 370.41it/s]
Adding requests:  19%|█▊        | 767/4096 [00:02<00:08, 369.96it/s]
Adding requests:  20%|█▉        | 805/4096 [00:02<00:08, 371.74it/s]
Adding requests:  21%|██        | 844/4096 [00:02<00:08, 377.03it/s]
Adding requests:  22%|██▏       | 884/4096 [00:02<00:08, 382.20it/s]
Adding requests:  23%|██▎       | 923/4096 [00:02<00:08, 376.81it/s]
Adding requests:  23%|██▎       | 962/4096 [00:02<00:08, 379.40it/s]
Adding requests:  24%|██▍       | 1000/4096 [00:02<00:08, 375.31it/s]
Adding requests:  25%|██▌       | 1038/4096 [00:02<00:08, 375.77it/s]
Adding requests:  26%|██▋       | 1076/4096 [00:02<00:08, 372.84it/s]
Adding requests:  27%|██▋       | 1114/4096 [00:02<00:07, 373.47it/s]
Adding requests:  28%|██▊       | 1152/4096 [00:03<00:07, 375.15it/s]
Adding requests:  29%|██▉       | 1190/4096 [00:03<00:07, 366.52it/s]
Adding requests:  30%|███       | 1229/4096 [00:03<00:07, 373.27it/s]
Adding requests:  31%|███       | 1268/4096 [00:03<00:07, 375.25it/s]
Adding requests:  32%|███▏      | 1306/4096 [00:03<00:07, 376.56it/s]
Adding requests:  33%|███▎      | 1345/4096 [00:03<00:07, 380.31it/s]
Adding requests:  34%|███▍      | 1385/4096 [00:03<00:07, 385.14it/s]
Adding requests:  35%|███▍      | 1424/4096 [00:03<00:07, 381.51it/s]
Adding requests:  36%|███▌      | 1464/4096 [00:03<00:06, 384.36it/s]
Adding requests:  37%|███▋      | 1503/4096 [00:03<00:06, 385.66it/s]
Adding requests:  38%|███▊      | 1542/4096 [00:04<00:06, 386.79it/s]
Adding requests:  39%|███▊      | 1581/4096 [00:04<00:06, 376.69it/s]
Adding requests:  40%|███▉      | 1619/4096 [00:04<00:06, 371.95it/s]
Adding requests:  40%|████      | 1657/4096 [00:04<00:06, 366.75it/s]
Adding requests:  41%|████▏     | 1694/4096 [00:04<00:06, 367.14it/s]
Adding requests:  42%|████▏     | 1733/4096 [00:04<00:06, 372.97it/s]
Adding requests:  43%|████▎     | 1773/4096 [00:04<00:06, 380.10it/s]
Adding requests:  44%|████▍     | 1812/4096 [00:04<00:06, 377.96it/s]
Adding requests:  45%|████▌     | 1850/4096 [00:04<00:05, 375.24it/s]
Adding requests:  46%|████▌     | 1889/4096 [00:05<00:05, 377.67it/s]
Adding requests:  47%|████▋     | 1928/4096 [00:05<00:05, 380.40it/s]
Adding requests:  48%|████▊     | 1967/4096 [00:05<00:05, 379.68it/s]
Adding requests:  49%|████▉     | 2005/4096 [00:05<00:05, 375.36it/s]
Adding requests:  50%|████▉     | 2043/4096 [00:05<00:05, 369.62it/s]
Adding requests:  51%|█████     | 2080/4096 [00:05<00:05, 358.23it/s]
Adding requests:  52%|█████▏    | 2119/4096 [00:05<00:05, 365.56it/s]
Adding requests:  53%|█████▎    | 2156/4096 [00:05<00:05, 363.96it/s]
Adding requests:  54%|█████▎    | 2193/4096 [00:05<00:05, 359.62it/s]
Adding requests:  54%|█████▍    | 2229/4096 [00:05<00:05, 359.40it/s]
Adding requests:  55%|█████▌    | 2268/4096 [00:06<00:04, 367.64it/s]
Adding requests:  56%|█████▋    | 2306/4096 [00:06<00:04, 370.57it/s]
Adding requests:  57%|█████▋    | 2346/4096 [00:06<00:04, 375.90it/s]
Adding requests:  58%|█████▊    | 2386/4096 [00:06<00:04, 382.29it/s]
Adding requests:  59%|█████▉    | 2425/4096 [00:06<00:04, 371.65it/s]
Adding requests:  60%|██████    | 2463/4096 [00:06<00:04, 368.53it/s]
Adding requests:  61%|██████    | 2503/4096 [00:06<00:04, 374.74it/s]
Adding requests:  62%|██████▏   | 2543/4096 [00:06<00:04, 380.86it/s]
Adding requests:  63%|██████▎   | 2584/4096 [00:06<00:03, 388.95it/s]
Adding requests:  64%|██████▍   | 2623/4096 [00:06<00:03, 382.08it/s]
Adding requests:  65%|██████▍   | 2662/4096 [00:07<00:03, 374.33it/s]
Adding requests:  66%|██████▌   | 2700/4096 [00:07<00:03, 371.96it/s]
Adding requests:  67%|██████▋   | 2738/4096 [00:07<00:03, 370.80it/s]
Adding requests:  68%|██████▊   | 2777/4096 [00:07<00:03, 375.45it/s]
Adding requests:  69%|██████▉   | 2818/4096 [00:07<00:03, 384.87it/s]
Adding requests:  70%|██████▉   | 2857/4096 [00:07<00:03, 382.35it/s]
Adding requests:  71%|███████   | 2896/4096 [00:07<00:03, 380.87it/s]
Adding requests:  72%|███████▏  | 2935/4096 [00:07<00:03, 381.73it/s]
Adding requests:  73%|███████▎  | 2974/4096 [00:07<00:02, 380.14it/s]
Adding requests:  74%|███████▎  | 3015/4096 [00:08<00:02, 385.43it/s]
Adding requests:  75%|███████▍  | 3055/4096 [00:08<00:02, 386.98it/s]
Adding requests:  76%|███████▌  | 3094/4096 [00:08<00:02, 385.13it/s]
Adding requests:  77%|███████▋  | 3134/4096 [00:08<00:02, 388.80it/s]
Adding requests:  77%|███████▋  | 3173/4096 [00:08<00:02, 380.29it/s]
Adding requests:  78%|███████▊  | 3212/4096 [00:08<00:02, 375.99it/s]
Adding requests:  79%|███████▉  | 3253/4096 [00:08<00:02, 383.73it/s]
Adding requests:  80%|████████  | 3292/4096 [00:08<00:02, 368.36it/s]
Adding requests:  81%|████████▏ | 3329/4096 [00:08<00:02, 364.89it/s]
Adding requests:  82%|████████▏ | 3368/4096 [00:08<00:01, 369.64it/s]
Adding requests:  83%|████████▎ | 3407/4096 [00:09<00:01, 374.04it/s]
Adding requests:  84%|████████▍ | 3446/4096 [00:09<00:01, 377.95it/s]
Adding requests:  85%|████████▌ | 3484/4096 [00:09<00:01, 376.32it/s]
Adding requests:  86%|████████▌ | 3523/4096 [00:09<00:01, 378.63it/s]
Adding requests:  87%|████████▋ | 3564/4096 [00:09<00:01, 387.17it/s]
Adding requests:  88%|████████▊ | 3603/4096 [00:09<00:01, 380.79it/s]
Adding requests:  89%|████████▉ | 3642/4096 [00:09<00:01, 381.67it/s]
Adding requests:  90%|████████▉ | 3681/4096 [00:09<00:01, 373.62it/s]
Adding requests:  91%|█████████ | 3721/4096 [00:09<00:00, 378.23it/s]
Adding requests:  92%|█████████▏| 3759/4096 [00:10<00:00, 368.69it/s]
Adding requests:  93%|█████████▎| 3796/4096 [00:10<00:00, 348.30it/s]
Adding requests:  94%|█████████▎| 3832/4096 [00:10<00:00, 348.68it/s]
Adding requests:  95%|█████████▍| 3871/4096 [00:10<00:00, 357.41it/s]
Adding requests:  95%|█████████▌| 3907/4096 [00:10<00:00, 353.59it/s]
Adding requests:  96%|█████████▋| 3943/4096 [00:10<00:00, 355.32it/s]
Adding requests:  97%|█████████▋| 3979/4096 [00:10<00:00, 356.44it/s]
Adding requests:  98%|█████████▊| 4017/4096 [00:10<00:00, 362.86it/s]
Adding requests:  99%|█████████▉| 4054/4096 [00:10<00:00, 361.59it/s]
Adding requests: 100%|█████████▉| 4091/4096 [00:10<00:00, 363.16it/s]
Adding requests: 100%|██████████| 4096/4096 [00:10<00:00, 373.76it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  11%|█         | 437/4096 [00:00<00:04, 855.77it/s, est. speed input: 876352.87 toks/s, output: 855.78 toks/s]
Processed prompts:  13%|█▎        | 523/4096 [00:02<00:17, 207.86it/s, est. speed input: 262708.21 toks/s, output: 256.55 toks/s]
Processed prompts:  14%|█▎        | 562/4096 [00:02<00:23, 150.33it/s, est. speed input: 205303.93 toks/s, output: 200.49 toks/s]
Processed prompts:  14%|█▍        | 586/4096 [00:03<00:32, 109.38it/s, est. speed input: 168146.94 toks/s, output: 164.21 toks/s]
Processed prompts:  15%|█▍        | 602/4096 [00:04<00:43, 80.28it/s, est. speed input: 142286.50 toks/s, output: 138.95 toks/s] 
Processed prompts:  15%|█▌        | 629/4096 [00:05<00:52, 65.85it/s, est. speed input: 126326.50 toks/s, output: 123.36 toks/s]
Processed prompts:  16%|█▌        | 661/4096 [00:05<00:58, 58.29it/s, est. speed input: 115411.67 toks/s, output: 112.71 toks/s]
Processed prompts:  17%|█▋        | 693/4096 [00:06<01:03, 53.17it/s, est. speed input: 107021.38 toks/s, output: 104.51 toks/s]
Processed prompts:  18%|█▊        | 725/4096 [00:07<01:07, 49.69it/s, est. speed input: 100378.83 toks/s, output: 98.03 toks/s] 
Processed prompts:  18%|█▊        | 757/4096 [00:08<01:10, 47.59it/s, est. speed input: 95163.81 toks/s, output: 92.93 toks/s] 
Processed prompts:  19%|█▉        | 789/4096 [00:08<01:12, 45.79it/s, est. speed input: 90645.57 toks/s, output: 88.52 toks/s]
Processed prompts:  20%|██        | 821/4096 [00:09<01:13, 44.51it/s, est. speed input: 86825.39 toks/s, output: 84.79 toks/s]
Processed prompts:  21%|██        | 853/4096 [00:10<01:14, 43.67it/s, est. speed input: 83590.74 toks/s, output: 81.63 toks/s]
Processed prompts:  22%|██▏       | 885/4096 [00:11<01:14, 43.10it/s, est. speed input: 80800.50 toks/s, output: 78.91 toks/s]
Processed prompts:  22%|██▏       | 917/4096 [00:11<01:14, 42.65it/s, est. speed input: 78352.51 toks/s, output: 76.52 toks/s]
Processed prompts:  23%|██▎       | 949/4096 [00:12<01:14, 42.34it/s, est. speed input: 76195.77 toks/s, output: 74.41 toks/s]
Processed prompts:  24%|██▍       | 981/4096 [00:13<01:13, 42.11it/s, est. speed input: 74284.62 toks/s, output: 72.54 toks/s]
Processed prompts:  25%|██▍       | 1013/4096 [00:14<01:13, 41.98it/s, est. speed input: 72584.37 toks/s, output: 70.88 toks/s]
Processed prompts:  26%|██▌       | 1045/4096 [00:15<01:12, 41.83it/s, est. speed input: 71044.56 toks/s, output: 69.38 toks/s]
Processed prompts:  26%|██▋       | 1077/4096 [00:15<01:12, 41.75it/s, est. speed input: 69658.54 toks/s, output: 68.03 toks/s]
Processed prompts:  27%|██▋       | 1109/4096 [00:16<01:11, 41.69it/s, est. speed input: 68401.87 toks/s, output: 66.80 toks/s]
Processed prompts:  28%|██▊       | 1141/4096 [00:17<01:10, 41.70it/s, est. speed input: 67267.20 toks/s, output: 65.69 toks/s]
Processed prompts:  29%|██▊       | 1173/4096 [00:18<01:10, 41.67it/s, est. speed input: 66220.39 toks/s, output: 64.67 toks/s]
Processed prompts:  29%|██▉       | 1205/4096 [00:18<01:08, 41.96it/s, est. speed input: 65325.87 toks/s, output: 63.79 toks/s]
Processed prompts:  30%|███       | 1237/4096 [00:19<01:07, 42.13it/s, est. speed input: 64490.92 toks/s, output: 62.98 toks/s]
Processed prompts:  31%|███       | 1269/4096 [00:20<01:07, 41.96it/s, est. speed input: 63664.66 toks/s, output: 62.17 toks/s]
Processed prompts:  32%|███▏      | 1301/4096 [00:21<01:06, 41.83it/s, est. speed input: 62896.46 toks/s, output: 61.42 toks/s]
Processed prompts:  33%|███▎      | 1333/4096 [00:21<01:05, 42.03it/s, est. speed input: 62230.99 toks/s, output: 60.77 toks/s]
Processed prompts:  33%|███▎      | 1365/4096 [00:22<01:05, 41.82it/s, est. speed input: 61553.21 toks/s, output: 60.11 toks/s]
Processed prompts:  34%|███▍      | 1397/4096 [00:23<01:04, 41.73it/s, est. speed input: 60928.32 toks/s, output: 59.50 toks/s]
Processed prompts:  35%|███▍      | 1429/4096 [00:24<01:03, 41.94it/s, est. speed input: 60383.80 toks/s, output: 58.97 toks/s]
Processed prompts:  36%|███▌      | 1461/4096 [00:25<01:03, 41.77it/s, est. speed input: 59827.78 toks/s, output: 58.43 toks/s]
Processed prompts:  36%|███▋      | 1493/4096 [00:25<01:02, 41.70it/s, est. speed input: 59310.85 toks/s, output: 57.92 toks/s]
Processed prompts:  37%|███▋      | 1525/4096 [00:26<01:01, 41.88it/s, est. speed input: 58854.74 toks/s, output: 57.48 toks/s]
Processed prompts:  38%|███▊      | 1557/4096 [00:27<01:00, 42.03it/s, est. speed input: 58427.61 toks/s, output: 57.06 toks/s]
Processed prompts:  39%|███▉      | 1589/4096 [00:28<00:59, 41.85it/s, est. speed input: 57986.92 toks/s, output: 56.63 toks/s]
Processed prompts:  40%|███▉      | 1621/4096 [00:28<00:58, 41.98it/s, est. speed input: 57601.53 toks/s, output: 56.25 toks/s]
Processed prompts:  40%|████      | 1653/4096 [00:29<00:58, 41.78it/s, est. speed input: 57201.01 toks/s, output: 55.86 toks/s]
Processed prompts:  41%|████      | 1685/4096 [00:30<00:57, 41.67it/s, est. speed input: 56824.57 toks/s, output: 55.49 toks/s]
Processed prompts:  42%|████▏     | 1717/4096 [00:31<00:56, 41.84it/s, est. speed input: 56494.77 toks/s, output: 55.17 toks/s]
Processed prompts:  43%|████▎     | 1749/4096 [00:31<00:55, 41.97it/s, est. speed input: 56180.68 toks/s, output: 54.86 toks/s]
Processed prompts:  43%|████▎     | 1781/4096 [00:32<00:55, 41.79it/s, est. speed input: 55853.72 toks/s, output: 54.54 toks/s]
Processed prompts:  44%|████▍     | 1813/4096 [00:33<00:54, 41.65it/s, est. speed input: 55540.28 toks/s, output: 54.24 toks/s]
Processed prompts:  45%|████▌     | 1845/4096 [00:34<00:54, 41.53it/s, est. speed input: 55239.05 toks/s, output: 53.94 toks/s]
Processed prompts:  46%|████▌     | 1877/4096 [00:34<00:53, 41.79it/s, est. speed input: 54983.43 toks/s, output: 53.69 toks/s]
Processed prompts:  47%|████▋     | 1909/4096 [00:35<00:52, 41.66it/s, est. speed input: 54710.62 toks/s, output: 53.43 toks/s]
Processed prompts:  47%|████▋     | 1941/4096 [00:36<00:51, 41.52it/s, est. speed input: 54444.04 toks/s, output: 53.17 toks/s]
Processed prompts:  48%|████▊     | 1973/4096 [00:37<00:50, 41.76it/s, est. speed input: 54218.76 toks/s, output: 52.95 toks/s]
Processed prompts:  49%|████▉     | 2005/4096 [00:38<00:50, 41.61it/s, est. speed input: 53974.74 toks/s, output: 52.71 toks/s]
Processed prompts:  50%|████▉     | 2037/4096 [00:38<00:49, 41.52it/s, est. speed input: 53741.57 toks/s, output: 52.48 toks/s]
Processed prompts:  51%|█████     | 2069/4096 [00:39<00:48, 41.67it/s, est. speed input: 53535.51 toks/s, output: 52.28 toks/s]
Processed prompts:  51%|█████▏    | 2101/4096 [00:40<00:48, 41.51it/s, est. speed input: 53315.45 toks/s, output: 52.07 toks/s]
Processed prompts:  52%|█████▏    | 2133/4096 [00:41<00:47, 41.43it/s, est. speed input: 53106.61 toks/s, output: 51.86 toks/s]
Processed prompts:  53%|█████▎    | 2165/4096 [00:41<00:46, 41.65it/s, est. speed input: 52927.21 toks/s, output: 51.69 toks/s]
Processed prompts:  54%|█████▎    | 2197/4096 [00:42<00:45, 41.52it/s, est. speed input: 52731.70 toks/s, output: 51.50 toks/s]
Processed prompts:  54%|█████▍    | 2229/4096 [00:43<00:45, 41.43it/s, est. speed input: 52544.19 toks/s, output: 51.31 toks/s]
Processed prompts:  55%|█████▌    | 2261/4096 [00:44<00:44, 41.39it/s, est. speed input: 52364.56 toks/s, output: 51.14 toks/s]
Processed prompts:  56%|█████▌    | 2293/4096 [00:44<00:43, 41.34it/s, est. speed input: 52189.44 toks/s, output: 50.97 toks/s]
Processed prompts:  57%|█████▋    | 2325/4096 [00:45<00:42, 41.28it/s, est. speed input: 52018.12 toks/s, output: 50.80 toks/s]
Processed prompts:  58%|█████▊    | 2357/4096 [00:46<00:42, 41.26it/s, est. speed input: 51854.38 toks/s, output: 50.64 toks/s]
Processed prompts:  58%|█████▊    | 2389/4096 [00:47<00:41, 41.25it/s, est. speed input: 51696.20 toks/s, output: 50.48 toks/s]
Processed prompts:  59%|█████▉    | 2421/4096 [00:48<00:40, 41.25it/s, est. speed input: 51543.60 toks/s, output: 50.34 toks/s]
Processed prompts:  60%|█████▉    | 2453/4096 [00:48<00:39, 41.21it/s, est. speed input: 51393.36 toks/s, output: 50.19 toks/s]
Processed prompts:  61%|██████    | 2485/4096 [00:49<00:39, 41.22it/s, est. speed input: 51250.16 toks/s, output: 50.05 toks/s]
Processed prompts:  61%|██████▏   | 2517/4096 [00:50<00:38, 41.45it/s, est. speed input: 51125.32 toks/s, output: 49.93 toks/s]
Processed prompts:  62%|██████▏   | 2549/4096 [00:51<00:37, 41.36it/s, est. speed input: 50988.73 toks/s, output: 49.79 toks/s]
Processed prompts:  63%|██████▎   | 2581/4096 [00:51<00:36, 41.53it/s, est. speed input: 50870.93 toks/s, output: 49.68 toks/s]
Processed prompts:  64%|██████▍   | 2613/4096 [00:52<00:35, 41.42it/s, est. speed input: 50742.52 toks/s, output: 49.55 toks/s]
Processed prompts:  65%|██████▍   | 2645/4096 [00:53<00:35, 41.33it/s, est. speed input: 50616.69 toks/s, output: 49.43 toks/s]
Processed prompts:  65%|██████▌   | 2677/4096 [00:54<00:34, 41.28it/s, est. speed input: 50495.66 toks/s, output: 49.31 toks/s]
Processed prompts:  66%|██████▌   | 2709/4096 [00:55<00:33, 41.28it/s, est. speed input: 50379.63 toks/s, output: 49.20 toks/s]
Processed prompts:  67%|██████▋   | 2741/4096 [00:55<00:32, 41.52it/s, est. speed input: 50280.80 toks/s, output: 49.10 toks/s]
Processed prompts:  68%|██████▊   | 2773/4096 [00:56<00:31, 41.40it/s, est. speed input: 50168.10 toks/s, output: 48.99 toks/s]
Processed prompts:  68%|██████▊   | 2805/4096 [00:57<00:31, 41.32it/s, est. speed input: 50059.07 toks/s, output: 48.89 toks/s]
Processed prompts:  69%|██████▉   | 2837/4096 [00:58<00:30, 41.27it/s, est. speed input: 49953.40 toks/s, output: 48.78 toks/s]
Processed prompts:  70%|███████   | 2869/4096 [00:58<00:29, 41.21it/s, est. speed input: 49849.07 toks/s, output: 48.68 toks/s]
Processed prompts:  71%|███████   | 2901/4096 [00:59<00:28, 42.26it/s, est. speed input: 49803.07 toks/s, output: 48.64 toks/s]
Processed prompts:  72%|███████▏  | 2933/4096 [01:00<00:27, 41.91it/s, est. speed input: 49703.93 toks/s, output: 48.54 toks/s]
Processed prompts:  72%|███████▏  | 2965/4096 [01:01<00:27, 41.67it/s, est. speed input: 49607.27 toks/s, output: 48.44 toks/s]
Processed prompts:  73%|███████▎  | 2997/4096 [01:01<00:26, 41.49it/s, est. speed input: 49512.69 toks/s, output: 48.35 toks/s]
Processed prompts:  74%|███████▍  | 3029/4096 [01:02<00:25, 41.39it/s, est. speed input: 49421.08 toks/s, output: 48.26 toks/s]
Processed prompts:  75%|███████▍  | 3061/4096 [01:03<00:25, 41.28it/s, est. speed input: 49330.25 toks/s, output: 48.17 toks/s]
Processed prompts:  76%|███████▌  | 3093/4096 [01:04<00:24, 41.23it/s, est. speed input: 49242.98 toks/s, output: 48.09 toks/s]
Processed prompts:  76%|███████▋  | 3125/4096 [01:05<00:23, 41.16it/s, est. speed input: 49155.54 toks/s, output: 48.00 toks/s]
Processed prompts:  77%|███████▋  | 3157/4096 [01:05<00:22, 41.13it/s, est. speed input: 49071.75 toks/s, output: 47.92 toks/s]
Processed prompts:  78%|███████▊  | 3189/4096 [01:06<00:22, 41.09it/s, est. speed input: 48988.81 toks/s, output: 47.84 toks/s]
Processed prompts:  79%|███████▊  | 3221/4096 [01:07<00:21, 41.07it/s, est. speed input: 48907.74 toks/s, output: 47.76 toks/s]
Processed prompts:  79%|███████▉  | 3253/4096 [01:08<00:20, 41.03it/s, est. speed input: 48828.00 toks/s, output: 47.68 toks/s]
Processed prompts:  80%|████████  | 3285/4096 [01:08<00:19, 41.05it/s, est. speed input: 48751.62 toks/s, output: 47.61 toks/s]
Processed prompts:  81%|████████  | 3317/4096 [01:09<00:18, 41.03it/s, est. speed input: 48675.74 toks/s, output: 47.53 toks/s]
Processed prompts:  82%|████████▏ | 3349/4096 [01:10<00:18, 41.03it/s, est. speed input: 48602.04 toks/s, output: 47.46 toks/s]
Processed prompts:  83%|████████▎ | 3381/4096 [01:11<00:17, 41.02it/s, est. speed input: 48529.72 toks/s, output: 47.39 toks/s]
Processed prompts:  83%|████████▎ | 3413/4096 [01:12<00:16, 41.02it/s, est. speed input: 48459.16 toks/s, output: 47.32 toks/s]
Processed prompts:  84%|████████▍ | 3445/4096 [01:12<00:15, 41.00it/s, est. speed input: 48389.26 toks/s, output: 47.26 toks/s]
Processed prompts:  85%|████████▍ | 3477/4096 [01:13<00:15, 41.00it/s, est. speed input: 48321.22 toks/s, output: 47.19 toks/s]
Processed prompts:  86%|████████▌ | 3509/4096 [01:14<00:14, 41.01it/s, est. speed input: 48255.43 toks/s, output: 47.12 toks/s]
Processed prompts:  86%|████████▋ | 3541/4096 [01:15<00:13, 41.26it/s, est. speed input: 48200.39 toks/s, output: 47.07 toks/s]
Processed prompts:  87%|████████▋ | 3573/4096 [01:16<00:12, 41.18it/s, est. speed input: 48136.76 toks/s, output: 47.01 toks/s]
Processed prompts:  88%|████████▊ | 3605/4096 [01:16<00:11, 41.12it/s, est. speed input: 48073.94 toks/s, output: 46.95 toks/s]
Processed prompts:  89%|████████▉ | 3637/4096 [01:17<00:11, 41.07it/s, est. speed input: 48012.04 toks/s, output: 46.89 toks/s]
Processed prompts:  90%|████████▉ | 3669/4096 [01:18<00:10, 41.28it/s, est. speed input: 47960.77 toks/s, output: 46.84 toks/s]
Processed prompts:  90%|█████████ | 3701/4096 [01:19<00:09, 41.18it/s, est. speed input: 47901.32 toks/s, output: 46.78 toks/s]
Processed prompts:  91%|█████████ | 3733/4096 [01:19<00:08, 41.11it/s, est. speed input: 47842.81 toks/s, output: 46.72 toks/s]
Processed prompts:  92%|█████████▏| 3765/4096 [01:20<00:08, 41.03it/s, est. speed input: 47784.50 toks/s, output: 46.66 toks/s]
Processed prompts:  93%|█████████▎| 3797/4096 [01:21<00:07, 41.00it/s, est. speed input: 47728.19 toks/s, output: 46.61 toks/s]
Processed prompts:  93%|█████████▎| 3829/4096 [01:22<00:06, 41.00it/s, est. speed input: 47673.70 toks/s, output: 46.56 toks/s]
Processed prompts:  94%|█████████▍| 3861/4096 [01:23<00:05, 40.98it/s, est. speed input: 47619.42 toks/s, output: 46.50 toks/s]
Processed prompts:  95%|█████████▌| 3893/4096 [01:23<00:04, 40.94it/s, est. speed input: 47565.26 toks/s, output: 46.45 toks/s]
Processed prompts:  96%|█████████▌| 3925/4096 [01:24<00:04, 41.68it/s, est. speed input: 47539.16 toks/s, output: 46.42 toks/s]
Processed prompts:  97%|█████████▋| 3957/4096 [01:25<00:03, 41.44it/s, est. speed input: 47487.16 toks/s, output: 46.37 toks/s]
Processed prompts:  97%|█████████▋| 3989/4096 [01:26<00:02, 41.60it/s, est. speed input: 47447.28 toks/s, output: 46.34 toks/s]
Processed prompts:  98%|█████████▊| 4021/4096 [01:26<00:01, 41.39it/s, est. speed input: 47397.15 toks/s, output: 46.29 toks/s]
Processed prompts:  99%|█████████▉| 4053/4096 [01:27<00:01, 41.54it/s, est. speed input: 47358.18 toks/s, output: 46.25 toks/s]
Processed prompts: 100%|█████████▉| 4085/4096 [01:27<00:00, 51.57it/s, est. speed input: 47584.82 toks/s, output: 46.47 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [01:27<00:00, 51.57it/s, est. speed input: 47712.63 toks/s, output: 46.59 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [01:27<00:00, 46.59it/s, est. speed input: 47712.63 toks/s, output: 46.59 toks/s]
[rank0]:[W126 12:11:32.179980932 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 161.7s

测试结果:
  Requests/s:   41.43
  Tokens/s:     42463.70
  Total Reqs:   4096
  Elapsed:      98.87s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     42422.27

============================================================
[8/8] 测试 M=65536
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 65536
│   M_prefill     = 65536 (= 64 x 1024)
│   M_decode      = 64
│   batched_tokens = 65536 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 8192
│   --max-num-seqs           = 64
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 65536
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 12:12:25 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1339655) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1339655) WARNING 01-26 12:12:42 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]     self.model_runner.profile_run()
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]     hidden_states = self.model(
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]                     ^^^^^^^^^^^
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]     def forward(
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]     raise e
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]   File "<eval_with_key>.58", line 325, in forward
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]     return range_entry.runnable(*args)
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]     return self._compiled_fn(*args)
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]     return compiled_fn(full_args)
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]                             ^^^^^^^
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 724, in inner_fn
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]     outs = compiled_fn(args)
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]            ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]     return self.current_callable(inputs)
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]     out = model(new_inputs)
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]   File "/tmp/torchinductor_root/y5/cy5qjxhrsxksh4kah7hlt7wnw7gzmouaejj2p54lihbs2v3ejlob.py", line 1093, in call
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]     buf17 = torch.ops.slidesparse.quant_slide_fp8.default(buf16, 'Qwen2.5-7B-FP8', 6)
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 521, in _quant_slide_fp8_impl
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]     return fn(input, L)
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]            ^^^^^^^^^^^^
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]   File "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/H100_cc90_py312_cu129_x86_64/quant_slide_tuned_Qwen2.5-7B.py", line 233, in quant_slide_fp8_triton
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]     _quant_slide_fp8_kernel[(M,)](
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 756, in run
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]     launch_metadata = kernel.launch_metadata(grid, stream, *bound_args.values())
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 490, in launch_metadata
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]     self._init_handles()
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 473, in _init_handles
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]     self.module, self.function, self.n_regs, self.n_spills, self.n_max_threads = driver.active.utils.load_binary(
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866]                                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1339655) ERROR 01-26 12:12:51 [core.py:866] RuntimeError: Triton Error [CUDA]: an illegal memory access was encountered


─── STDERR ───
[2026-01-26 12:12:24] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 12:12:25] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:12:25] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 12:12:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:12:25] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:12:25] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:12:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:12:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:12:25] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:12:25] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 12:12:25] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 12:12:25] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 12:12:25] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 12:12:25] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 12:12:32] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 12:12:32] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:12:32] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 12:12:32] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:12:32] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:12:32] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:12:32] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:12:32] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:12:32] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:12:32] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 12:12:32] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 12:12:32] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 12:12:32] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 12:12:32] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1339655) [2026-01-26 12:12:34] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1339655) [2026-01-26 12:12:34] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1339655) [2026-01-26 12:12:34] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1339655) [2026-01-26 12:12:34] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=1339655) [2026-01-26 12:12:34] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=1339655) [2026-01-26 12:12:34] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=1339655) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1339655) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.14it/s]
(EngineCore_DP0 pid=1339655) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.54it/s]
(EngineCore_DP0 pid=1339655) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.61it/s]
(EngineCore_DP0 pid=1339655) 
(EngineCore_DP0 pid=1339655) [2026-01-26 12:12:36] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 4800] -> 1D uint8
(EngineCore_DP0 pid=1339655) [2026-01-26 12:12:37] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 13824000 bytes
(EngineCore_DP0 pid=1339655) [2026-01-26 12:12:37] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 4800] -> 1D uint8
(EngineCore_DP0 pid=1339655) [2026-01-26 12:12:37] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10752000 bytes
(EngineCore_DP0 pid=1339655) [2026-01-26 12:12:37] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 4800] -> 1D uint8
(EngineCore_DP0 pid=1339655) [2026-01-26 12:12:37] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 113664000 bytes
(EngineCore_DP0 pid=1339655) [2026-01-26 12:12:37] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 25280] -> 1D uint8
(EngineCore_DP0 pid=1339655) [2026-01-26 12:12:37] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 56627200 bytes
(EngineCore_DP0 pid=1339655) [rank0]:W0126 12:12:48.466000 1339655 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1339655) [rank0]:W0126 12:12:48.546000 1339655 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1339655) [rank0]:W0126 12:12:49.989000 1339655 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1339655) [rank0]:W0126 12:12:50.112000 1339655 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1339655) Process EngineCore_DP0:
(EngineCore_DP0 pid=1339655) Traceback (most recent call last):
(EngineCore_DP0 pid=1339655)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=1339655)     self.run()
(EngineCore_DP0 pid=1339655)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=1339655)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=1339655)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=1339655)     raise e
(EngineCore_DP0 pid=1339655)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1339655)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1339655)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1339655)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1339655)     super().__init__(
(EngineCore_DP0 pid=1339655)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=1339655)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=1339655)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1339655)   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=1339655)     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=1339655)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1339655)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=1339655)     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=1339655)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1339655)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=1339655)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=1339655)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1339655)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=1339655)     return func(*args, **kwargs)
(EngineCore_DP0 pid=1339655)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1339655)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=1339655)     return func(*args, **kwargs)
(EngineCore_DP0 pid=1339655)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1339655)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=1339655)     self.model_runner.profile_run()
(EngineCore_DP0 pid=1339655)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=1339655)     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=1339655)                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1339655)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=1339655)     return func(*args, **kwargs)
(EngineCore_DP0 pid=1339655)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1339655)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=1339655)     outputs = self.model(
(EngineCore_DP0 pid=1339655)               ^^^^^^^^^^^
(EngineCore_DP0 pid=1339655)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=1339655)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=1339655)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1339655)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=1339655)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=1339655)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1339655)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=1339655)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=1339655)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1339655)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=1339655)     hidden_states = self.model(
(EngineCore_DP0 pid=1339655)                     ^^^^^^^^^^^
(EngineCore_DP0 pid=1339655)   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=1339655)     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=1339655)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1339655)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=1339655)     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=1339655)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1339655)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=1339655)     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=1339655)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1339655)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
(EngineCore_DP0 pid=1339655)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=1339655)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1339655)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
(EngineCore_DP0 pid=1339655)     def forward(
(EngineCore_DP0 pid=1339655)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=1339655)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=1339655)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1339655)   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=1339655)     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=1339655)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1339655)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=1339655)     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=1339655)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1339655)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=1339655)     raise e
(EngineCore_DP0 pid=1339655)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=1339655)     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=1339655)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1339655)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=1339655)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=1339655)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1339655)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=1339655)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=1339655)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1339655)   File "<eval_with_key>.58", line 325, in forward
(EngineCore_DP0 pid=1339655)     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
(EngineCore_DP0 pid=1339655)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1339655)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=1339655)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=1339655)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1339655)   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=1339655)     return range_entry.runnable(*args)
(EngineCore_DP0 pid=1339655)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1339655)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=1339655)     return self._compiled_fn(*args)
(EngineCore_DP0 pid=1339655)            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1339655)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=1339655)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=1339655)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1339655)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=1339655)     return compiled_fn(full_args)
(EngineCore_DP0 pid=1339655)            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1339655)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=1339655)     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=1339655)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1339655)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=1339655)     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=1339655)                             ^^^^^^^
(EngineCore_DP0 pid=1339655)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 724, in inner_fn
(EngineCore_DP0 pid=1339655)     outs = compiled_fn(args)
(EngineCore_DP0 pid=1339655)            ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1339655)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=1339655)     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=1339655)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1339655)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=1339655)     return self.current_callable(inputs)
(EngineCore_DP0 pid=1339655)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1339655)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=1339655)     out = model(new_inputs)
(EngineCore_DP0 pid=1339655)           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1339655)   File "/tmp/torchinductor_root/y5/cy5qjxhrsxksh4kah7hlt7wnw7gzmouaejj2p54lihbs2v3ejlob.py", line 1093, in call
(EngineCore_DP0 pid=1339655)     buf17 = torch.ops.slidesparse.quant_slide_fp8.default(buf16, 'Qwen2.5-7B-FP8', 6)
(EngineCore_DP0 pid=1339655)             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1339655)   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
(EngineCore_DP0 pid=1339655)     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=1339655)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1339655)   File "/root/vllmbench/slidesparse/core/kernels.py", line 521, in _quant_slide_fp8_impl
(EngineCore_DP0 pid=1339655)     return fn(input, L)
(EngineCore_DP0 pid=1339655)            ^^^^^^^^^^^^
(EngineCore_DP0 pid=1339655)   File "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/H100_cc90_py312_cu129_x86_64/quant_slide_tuned_Qwen2.5-7B.py", line 233, in quant_slide_fp8_triton
(EngineCore_DP0 pid=1339655)     _quant_slide_fp8_kernel[(M,)](
(EngineCore_DP0 pid=1339655)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=1339655)     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=1339655)                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1339655)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 756, in run
(EngineCore_DP0 pid=1339655)     launch_metadata = kernel.launch_metadata(grid, stream, *bound_args.values())
(EngineCore_DP0 pid=1339655)                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1339655)   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 490, in launch_metadata
(EngineCore_DP0 pid=1339655)     self._init_handles()
(EngineCore_DP0 pid=1339655)   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 473, in _init_handles
(EngineCore_DP0 pid=1339655)     self.module, self.function, self.n_regs, self.n_spills, self.n_max_threads = driver.active.utils.load_binary(
(EngineCore_DP0 pid=1339655)                                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1339655) RuntimeError: Triton Error [CUDA]: an illegal memory access was encountered
[rank0]:[W126 12:12:51.907742918 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[ERROR] 测试失败: M=65536 (exit code: 1)


------------------------------------------------------------
  生成 CSV: Qwen2.5-7B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/Qwen2.5-7B-FP8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,35.1435,18028.6190,3.6422
1024,1024,1,128,128,32.5241,33337.2276,3.9355
2048,1024,2,256,128,37.3345,38267.8870,6.8569
4096,1024,4,512,128,40.0102,41010.4465,12.7967
8192,1024,8,1024,128,41.0224,42047.9277,24.9620
16384,1024,16,2048,128,42.0245,43075.0999,48.7335
32768,1024,32,4096,128,41.4280,42463.7003,98.8703
65536,1024,64,8192,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[INFO] 完成: 7 成功, 1 失败

============================================================
  Qwen2.5-7B-FP8 | cuSPARSELt (2_8) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_8
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8

============================================================
[1/8] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 12:13:01 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1340646) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1340646) WARNING 01-26 12:13:18 [backends.py:609] Failed to read file <frozen os>
Throughput: 36.24 requests/s, 18590.28 total tokens/s, 36.24 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-26 12:13:01] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 12:13:01] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:13:01] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 12:13:01] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:13:01] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:13:01] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:13:01] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:13:01] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:13:01] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:13:01] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 12:13:01] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 12:13:01] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 12:13:01] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 12:13:01] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 12:13:08] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 12:13:08] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:13:08] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 12:13:08] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:13:08] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:13:08] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:13:08] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:13:08] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:13:08] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:13:08] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 12:13:08] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 12:13:08] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 12:13:08] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 12:13:08] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1340646) [2026-01-26 12:13:10] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1340646) [2026-01-26 12:13:10] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1340646) [2026-01-26 12:13:10] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1340646) [2026-01-26 12:13:10] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=1340646) [2026-01-26 12:13:10] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=1340646) [2026-01-26 12:13:10] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=1340646) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1340646) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.95it/s]
(EngineCore_DP0 pid=1340646) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.41it/s]
(EngineCore_DP0 pid=1340646) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.47it/s]
(EngineCore_DP0 pid=1340646) 
(EngineCore_DP0 pid=1340646) [2026-01-26 12:13:12] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5376] -> 1D uint8
(EngineCore_DP0 pid=1340646) [2026-01-26 12:13:12] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15482880 bytes
(EngineCore_DP0 pid=1340646) [2026-01-26 12:13:12] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5376] -> 1D uint8
(EngineCore_DP0 pid=1340646) [2026-01-26 12:13:12] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 12042240 bytes
(EngineCore_DP0 pid=1340646) [2026-01-26 12:13:12] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5376] -> 1D uint8
(EngineCore_DP0 pid=1340646) [2026-01-26 12:13:12] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 127303680 bytes
(EngineCore_DP0 pid=1340646) [2026-01-26 12:13:12] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 28416] -> 1D uint8
(EngineCore_DP0 pid=1340646) [2026-01-26 12:13:12] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 63651840 bytes
(EngineCore_DP0 pid=1340646) 2026-01-26 12:13:28,686 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1340646) 2026-01-26 12:13:28,709 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1340646) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  1.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:01<00:00,  1.99it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:01<00:00,  1.86it/s]
(EngineCore_DP0 pid=1340646) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 18.00it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  28%|██▊       | 36/128 [00:00<00:00, 355.76it/s]
Adding requests:  83%|████████▎ | 106/128 [00:00<00:00, 551.68it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 545.02it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:22,  5.75it/s, est. speed input: 2944.11 toks/s, output: 5.75 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:06, 19.10it/s, est. speed input: 8585.04 toks/s, output: 16.77 toks/s]
Processed prompts:   8%|▊         | 10/128 [00:00<00:04, 28.19it/s, est. speed input: 12190.49 toks/s, output: 23.81 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:00<00:03, 33.01it/s, est. speed input: 14183.64 toks/s, output: 27.70 toks/s]
Processed prompts:  16%|█▌        | 20/128 [00:00<00:03, 35.79it/s, est. speed input: 15437.64 toks/s, output: 30.15 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:00<00:02, 37.49it/s, est. speed input: 16293.30 toks/s, output: 31.82 toks/s]
Processed prompts:  23%|██▎       | 30/128 [00:00<00:02, 38.62it/s, est. speed input: 16925.41 toks/s, output: 33.06 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:01<00:02, 39.38it/s, est. speed input: 17409.45 toks/s, output: 34.00 toks/s]
Processed prompts:  31%|███▏      | 40/128 [00:01<00:02, 39.86it/s, est. speed input: 17784.54 toks/s, output: 34.73 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:01<00:02, 40.23it/s, est. speed input: 18094.33 toks/s, output: 35.34 toks/s]
Processed prompts:  39%|███▉      | 50/128 [00:01<00:01, 40.48it/s, est. speed input: 18349.45 toks/s, output: 35.84 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:01<00:01, 40.64it/s, est. speed input: 18563.03 toks/s, output: 36.26 toks/s]
Processed prompts:  47%|████▋     | 60/128 [00:01<00:01, 40.80it/s, est. speed input: 18748.76 toks/s, output: 36.62 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:01<00:01, 40.85it/s, est. speed input: 18903.08 toks/s, output: 36.92 toks/s]
Processed prompts:  55%|█████▍    | 70/128 [00:01<00:01, 40.90it/s, est. speed input: 19039.43 toks/s, output: 37.19 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:02<00:01, 40.91it/s, est. speed input: 19155.98 toks/s, output: 37.41 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:02<00:01, 40.92it/s, est. speed input: 19260.06 toks/s, output: 37.62 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:02<00:01, 40.91it/s, est. speed input: 19351.07 toks/s, output: 37.79 toks/s]
Processed prompts:  70%|███████   | 90/128 [00:02<00:00, 40.95it/s, est. speed input: 19436.43 toks/s, output: 37.96 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:02<00:00, 41.02it/s, est. speed input: 19516.43 toks/s, output: 38.12 toks/s]
Processed prompts:  78%|███████▊  | 100/128 [00:02<00:00, 41.05it/s, est. speed input: 19587.99 toks/s, output: 38.26 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:02<00:00, 41.03it/s, est. speed input: 19650.63 toks/s, output: 38.38 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:02<00:00, 41.05it/s, est. speed input: 19709.55 toks/s, output: 38.49 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:02<00:00, 41.02it/s, est. speed input: 19761.37 toks/s, output: 38.60 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:03<00:00, 41.03it/s, est. speed input: 19810.89 toks/s, output: 38.69 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:03<00:00, 41.08it/s, est. speed input: 19858.99 toks/s, output: 38.79 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 41.08it/s, est. speed input: 19886.02 toks/s, output: 38.84 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 38.84it/s, est. speed input: 19886.02 toks/s, output: 38.84 toks/s]
[rank0]:[W126 12:13:35.171942320 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 42.5s

测试结果:
  Requests/s:   36.24
  Tokens/s:     18590.28
  Total Reqs:   128
  Elapsed:      3.53s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     18554.04

============================================================
[2/8] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 12:13:44 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1341789) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1341789) WARNING 01-26 12:14:01 [backends.py:609] Failed to read file <frozen os>
Throughput: 31.12 requests/s, 31897.87 total tokens/s, 31.12 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-26 12:13:43] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 12:13:44] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:13:44] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 12:13:44] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:13:44] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:13:44] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:13:44] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:13:44] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:13:44] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:13:44] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 12:13:44] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 12:13:44] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 12:13:44] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 12:13:44] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 12:13:51] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 12:13:52] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:13:52] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 12:13:52] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:13:52] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:13:52] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:13:52] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:13:52] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:13:52] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:13:52] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 12:13:52] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 12:13:52] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 12:13:52] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 12:13:52] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1341789) [2026-01-26 12:13:53] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1341789) [2026-01-26 12:13:53] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1341789) [2026-01-26 12:13:53] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1341789) [2026-01-26 12:13:53] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=1341789) [2026-01-26 12:13:53] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=1341789) [2026-01-26 12:13:53] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=1341789) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1341789) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.97it/s]
(EngineCore_DP0 pid=1341789) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.42it/s]
(EngineCore_DP0 pid=1341789) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.48it/s]
(EngineCore_DP0 pid=1341789) 
(EngineCore_DP0 pid=1341789) [2026-01-26 12:13:55] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5376] -> 1D uint8
(EngineCore_DP0 pid=1341789) [2026-01-26 12:13:55] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15482880 bytes
(EngineCore_DP0 pid=1341789) [2026-01-26 12:13:55] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5376] -> 1D uint8
(EngineCore_DP0 pid=1341789) [2026-01-26 12:13:55] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 12042240 bytes
(EngineCore_DP0 pid=1341789) [2026-01-26 12:13:55] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5376] -> 1D uint8
(EngineCore_DP0 pid=1341789) [2026-01-26 12:13:55] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 127303680 bytes
(EngineCore_DP0 pid=1341789) [2026-01-26 12:13:55] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 28416] -> 1D uint8
(EngineCore_DP0 pid=1341789) [2026-01-26 12:13:55] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 63651840 bytes
(EngineCore_DP0 pid=1341789) 2026-01-26 12:14:11,763 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1341789) 2026-01-26 12:14:11,787 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1341789) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  2.70it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  2.22it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  2.28it/s]
(EngineCore_DP0 pid=1341789) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  9.94it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  9.93it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  16%|█▌        | 20/128 [00:00<00:00, 197.88it/s]
Adding requests:  48%|████▊     | 61/128 [00:00<00:00, 317.39it/s]
Adding requests:  77%|███████▋  | 98/128 [00:00<00:00, 337.02it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 331.38it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:02, 42.70it/s, est. speed input: 43729.99 toks/s, output: 42.70 toks/s]
Processed prompts:   8%|▊         | 10/128 [00:00<00:03, 37.22it/s, est. speed input: 38869.68 toks/s, output: 37.95 toks/s]
Processed prompts:  11%|█         | 14/128 [00:00<00:03, 35.97it/s, est. speed input: 37704.68 toks/s, output: 36.82 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:00<00:03, 35.42it/s, est. speed input: 37151.46 toks/s, output: 36.28 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:00<00:03, 34.96it/s, est. speed input: 36726.46 toks/s, output: 35.86 toks/s]
Processed prompts:  20%|██        | 26/128 [00:00<00:02, 34.70it/s, est. speed input: 36447.66 toks/s, output: 35.59 toks/s]
Processed prompts:  23%|██▎       | 30/128 [00:00<00:02, 34.53it/s, est. speed input: 36245.81 toks/s, output: 35.39 toks/s]
Processed prompts:  27%|██▋       | 34/128 [00:00<00:02, 34.30it/s, est. speed input: 36047.94 toks/s, output: 35.20 toks/s]
Processed prompts:  30%|██▉       | 38/128 [00:01<00:02, 34.10it/s, est. speed input: 35874.65 toks/s, output: 35.03 toks/s]
Processed prompts:  33%|███▎      | 42/128 [00:01<00:02, 34.05it/s, est. speed input: 35763.26 toks/s, output: 34.92 toks/s]
Processed prompts:  36%|███▌      | 46/128 [00:01<00:02, 34.11it/s, est. speed input: 35701.84 toks/s, output: 34.86 toks/s]
Processed prompts:  39%|███▉      | 50/128 [00:01<00:02, 34.15it/s, est. speed input: 35649.41 toks/s, output: 34.81 toks/s]
Processed prompts:  42%|████▏     | 54/128 [00:01<00:02, 34.12it/s, est. speed input: 35590.62 toks/s, output: 34.76 toks/s]
Processed prompts:  45%|████▌     | 58/128 [00:01<00:02, 34.13it/s, est. speed input: 35548.70 toks/s, output: 34.71 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:01<00:01, 34.18it/s, est. speed input: 35519.28 toks/s, output: 34.69 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:01<00:01, 34.14it/s, est. speed input: 35479.18 toks/s, output: 34.65 toks/s]
Processed prompts:  55%|█████▍    | 70/128 [00:02<00:01, 34.04it/s, est. speed input: 35429.71 toks/s, output: 34.60 toks/s]
Processed prompts:  58%|█████▊    | 74/128 [00:02<00:01, 34.04it/s, est. speed input: 35397.67 toks/s, output: 34.57 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:02<00:01, 34.06it/s, est. speed input: 35372.57 toks/s, output: 34.54 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:02<00:01, 34.08it/s, est. speed input: 35352.32 toks/s, output: 34.52 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:02<00:01, 34.07it/s, est. speed input: 35328.69 toks/s, output: 34.50 toks/s]
Processed prompts:  70%|███████   | 90/128 [00:02<00:01, 34.12it/s, est. speed input: 35317.22 toks/s, output: 34.49 toks/s]
Processed prompts:  73%|███████▎  | 94/128 [00:02<00:00, 34.14it/s, est. speed input: 35304.30 toks/s, output: 34.48 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:02<00:00, 34.12it/s, est. speed input: 35286.94 toks/s, output: 34.46 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:02<00:00, 34.10it/s, est. speed input: 35270.80 toks/s, output: 34.44 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:03<00:00, 34.12it/s, est. speed input: 35259.37 toks/s, output: 34.43 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:03<00:00, 34.09it/s, est. speed input: 35243.52 toks/s, output: 34.42 toks/s]
Processed prompts:  89%|████████▉ | 114/128 [00:03<00:00, 34.02it/s, est. speed input: 35222.87 toks/s, output: 34.40 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:03<00:00, 33.99it/s, est. speed input: 35206.48 toks/s, output: 34.38 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:03<00:00, 34.03it/s, est. speed input: 35197.54 toks/s, output: 34.37 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:03<00:00, 34.06it/s, est. speed input: 35190.12 toks/s, output: 34.37 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 34.06it/s, est. speed input: 35186.78 toks/s, output: 34.36 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 34.36it/s, est. speed input: 35186.78 toks/s, output: 34.36 toks/s]
[rank0]:[W126 12:14:18.689437153 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 43.6s

测试结果:
  Requests/s:   31.12
  Tokens/s:     31897.87
  Total Reqs:   128
  Elapsed:      4.11s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     31866.75

============================================================
[3/8] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 12:14:28 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1342899) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1342899) WARNING 01-26 12:14:45 [backends.py:609] Failed to read file <frozen os>
Throughput: 36.52 requests/s, 37436.97 total tokens/s, 36.52 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-26 12:14:28] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 12:14:28] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:14:28] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 12:14:28] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:14:28] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:14:28] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:14:28] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:14:28] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:14:28] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:14:28] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 12:14:28] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 12:14:28] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 12:14:28] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 12:14:28] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 12:14:35] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 12:14:35] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:14:35] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 12:14:35] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:14:35] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:14:35] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:14:35] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:14:35] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:14:35] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:14:35] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 12:14:35] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 12:14:35] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 12:14:35] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 12:14:35] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1342899) [2026-01-26 12:14:37] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1342899) [2026-01-26 12:14:37] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1342899) [2026-01-26 12:14:37] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1342899) [2026-01-26 12:14:37] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=1342899) [2026-01-26 12:14:37] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=1342899) [2026-01-26 12:14:37] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=1342899) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1342899) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.96it/s]
(EngineCore_DP0 pid=1342899) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.40it/s]
(EngineCore_DP0 pid=1342899) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.46it/s]
(EngineCore_DP0 pid=1342899) 
(EngineCore_DP0 pid=1342899) [2026-01-26 12:14:39] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5376] -> 1D uint8
(EngineCore_DP0 pid=1342899) [2026-01-26 12:14:39] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15482880 bytes
(EngineCore_DP0 pid=1342899) [2026-01-26 12:14:39] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5376] -> 1D uint8
(EngineCore_DP0 pid=1342899) [2026-01-26 12:14:39] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 12042240 bytes
(EngineCore_DP0 pid=1342899) [2026-01-26 12:14:39] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5376] -> 1D uint8
(EngineCore_DP0 pid=1342899) [2026-01-26 12:14:39] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 127303680 bytes
(EngineCore_DP0 pid=1342899) [2026-01-26 12:14:39] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 28416] -> 1D uint8
(EngineCore_DP0 pid=1342899) [2026-01-26 12:14:39] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 63651840 bytes
(EngineCore_DP0 pid=1342899) 2026-01-26 12:14:56,475 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1342899) 2026-01-26 12:14:56,522 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1342899) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:00<00:00, 14.33it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 14.41it/s]
(EngineCore_DP0 pid=1342899) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 18.83it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 18.81it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   8%|▊         | 21/256 [00:00<00:01, 206.33it/s]
Adding requests:  24%|██▍       | 62/256 [00:00<00:00, 324.31it/s]
Adding requests:  38%|███▊      | 98/256 [00:00<00:00, 339.00it/s]
Adding requests:  53%|█████▎    | 136/256 [00:00<00:00, 354.16it/s]
Adding requests:  69%|██████▉   | 176/256 [00:00<00:00, 367.57it/s]
Adding requests:  84%|████████▍ | 216/256 [00:00<00:00, 376.20it/s]
Adding requests: 100%|█████████▉| 255/256 [00:00<00:00, 378.35it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 359.09it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  11%|█         | 28/256 [00:00<00:00, 233.05it/s, est. speed input: 238785.07 toks/s, output: 233.07 toks/s]
Processed prompts:  20%|██        | 52/256 [00:00<00:03, 58.96it/s, est. speed input: 68662.12 toks/s, output: 67.05 toks/s]   
Processed prompts:  25%|██▌       | 64/256 [00:01<00:03, 50.44it/s, est. speed input: 59530.41 toks/s, output: 58.13 toks/s]
Processed prompts:  29%|██▊       | 73/256 [00:01<00:03, 47.91it/s, est. speed input: 56631.56 toks/s, output: 55.30 toks/s]
Processed prompts:  31%|███▏      | 80/256 [00:01<00:04, 43.54it/s, est. speed input: 53282.97 toks/s, output: 52.03 toks/s]
Processed prompts:  34%|███▎      | 86/256 [00:01<00:04, 41.99it/s, est. speed input: 51783.69 toks/s, output: 50.57 toks/s]
Processed prompts:  36%|███▌      | 91/256 [00:01<00:03, 42.65it/s, est. speed input: 51478.70 toks/s, output: 50.27 toks/s]
Processed prompts:  38%|███▊      | 96/256 [00:01<00:04, 39.33it/s, est. speed input: 49818.03 toks/s, output: 48.65 toks/s]
Processed prompts:  39%|███▉      | 101/256 [00:02<00:03, 40.69it/s, est. speed input: 49666.03 toks/s, output: 48.50 toks/s]
Processed prompts:  41%|████▏     | 106/256 [00:02<00:03, 37.57it/s, est. speed input: 48333.72 toks/s, output: 47.20 toks/s]
Processed prompts:  43%|████▎     | 110/256 [00:02<00:03, 37.33it/s, est. speed input: 47825.13 toks/s, output: 46.70 toks/s]
Processed prompts:  45%|████▍     | 114/256 [00:02<00:03, 37.18it/s, est. speed input: 47371.79 toks/s, output: 46.26 toks/s]
Processed prompts:  46%|████▌     | 118/256 [00:02<00:03, 37.04it/s, est. speed input: 46955.57 toks/s, output: 45.85 toks/s]
Processed prompts:  48%|████▊     | 122/256 [00:02<00:03, 36.95it/s, est. speed input: 46574.12 toks/s, output: 45.48 toks/s]
Processed prompts:  49%|████▉     | 126/256 [00:02<00:03, 36.88it/s, est. speed input: 46223.73 toks/s, output: 45.14 toks/s]
Processed prompts:  51%|█████     | 130/256 [00:02<00:03, 36.85it/s, est. speed input: 45902.88 toks/s, output: 44.83 toks/s]
Processed prompts:  52%|█████▏    | 134/256 [00:03<00:03, 36.76it/s, est. speed input: 45592.94 toks/s, output: 44.52 toks/s]
Processed prompts:  54%|█████▍    | 138/256 [00:03<00:03, 36.81it/s, est. speed input: 45322.51 toks/s, output: 44.26 toks/s]
Processed prompts:  55%|█████▌    | 142/256 [00:03<00:03, 36.71it/s, est. speed input: 45052.34 toks/s, output: 44.00 toks/s]
Processed prompts:  57%|█████▋    | 146/256 [00:03<00:03, 36.61it/s, est. speed input: 44795.32 toks/s, output: 43.74 toks/s]
Processed prompts:  59%|█████▊    | 150/256 [00:03<00:02, 36.65it/s, est. speed input: 44568.55 toks/s, output: 43.52 toks/s]
Processed prompts:  60%|██████    | 154/256 [00:03<00:02, 36.62it/s, est. speed input: 44349.02 toks/s, output: 43.31 toks/s]
Processed prompts:  62%|██████▏   | 158/256 [00:03<00:02, 36.68it/s, est. speed input: 44152.10 toks/s, output: 43.12 toks/s]
Processed prompts:  63%|██████▎   | 162/256 [00:03<00:02, 36.65it/s, est. speed input: 43957.36 toks/s, output: 42.93 toks/s]
Processed prompts:  65%|██████▍   | 166/256 [00:03<00:02, 36.61it/s, est. speed input: 43772.68 toks/s, output: 42.75 toks/s]
Processed prompts:  66%|██████▋   | 170/256 [00:03<00:02, 36.59it/s, est. speed input: 43598.81 toks/s, output: 42.58 toks/s]
Processed prompts:  68%|██████▊   | 174/256 [00:04<00:02, 36.60it/s, est. speed input: 43436.33 toks/s, output: 42.42 toks/s]
Processed prompts:  70%|██████▉   | 178/256 [00:04<00:02, 36.49it/s, est. speed input: 43270.45 toks/s, output: 42.26 toks/s]
Processed prompts:  71%|███████   | 182/256 [00:04<00:02, 36.57it/s, est. speed input: 43128.90 toks/s, output: 42.12 toks/s]
Processed prompts:  73%|███████▎  | 186/256 [00:04<00:01, 36.68it/s, est. speed input: 42998.92 toks/s, output: 41.99 toks/s]
Processed prompts:  74%|███████▍  | 190/256 [00:04<00:01, 36.69it/s, est. speed input: 42868.88 toks/s, output: 41.86 toks/s]
Processed prompts:  76%|███████▌  | 194/256 [00:04<00:01, 36.72it/s, est. speed input: 42747.53 toks/s, output: 41.75 toks/s]
Processed prompts:  77%|███████▋  | 198/256 [00:04<00:01, 36.69it/s, est. speed input: 42627.19 toks/s, output: 41.63 toks/s]
Processed prompts:  80%|███████▉  | 204/256 [00:04<00:01, 38.41it/s, est. speed input: 42621.06 toks/s, output: 41.62 toks/s]
Processed prompts:  81%|████████▏ | 208/256 [00:05<00:01, 37.92it/s, est. speed input: 42510.76 toks/s, output: 41.51 toks/s]
Processed prompts:  83%|████████▎ | 212/256 [00:05<00:01, 37.59it/s, est. speed input: 42407.12 toks/s, output: 41.41 toks/s]
Processed prompts:  84%|████████▍ | 216/256 [00:05<00:01, 37.29it/s, est. speed input: 42303.53 toks/s, output: 41.31 toks/s]
Processed prompts:  86%|████████▌ | 220/256 [00:05<00:00, 37.15it/s, est. speed input: 42209.69 toks/s, output: 41.22 toks/s]
Processed prompts:  88%|████████▊ | 224/256 [00:05<00:00, 37.04it/s, est. speed input: 42118.57 toks/s, output: 41.13 toks/s]
Processed prompts:  89%|████████▉ | 228/256 [00:05<00:00, 36.92it/s, est. speed input: 42027.92 toks/s, output: 41.04 toks/s]
Processed prompts:  91%|█████████ | 232/256 [00:05<00:00, 36.89it/s, est. speed input: 41945.03 toks/s, output: 40.96 toks/s]
Processed prompts:  92%|█████████▏| 236/256 [00:05<00:00, 36.75it/s, est. speed input: 41856.46 toks/s, output: 40.88 toks/s]
Processed prompts:  94%|█████████▍| 240/256 [00:05<00:00, 36.79it/s, est. speed input: 41781.30 toks/s, output: 40.80 toks/s]
Processed prompts:  95%|█████████▌| 244/256 [00:05<00:00, 36.65it/s, est. speed input: 41697.25 toks/s, output: 40.72 toks/s]
Processed prompts:  97%|█████████▋| 248/256 [00:06<00:00, 36.71it/s, est. speed input: 41626.72 toks/s, output: 40.65 toks/s]
Processed prompts:  98%|█████████▊| 252/256 [00:06<00:00, 36.71it/s, est. speed input: 41555.64 toks/s, output: 40.58 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:06<00:00, 36.71it/s, est. speed input: 41647.64 toks/s, output: 40.67 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:06<00:00, 40.67it/s, est. speed input: 41647.64 toks/s, output: 40.67 toks/s]
[rank0]:[W126 12:15:05.639758696 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 47.0s

测试结果:
  Requests/s:   36.52
  Tokens/s:     37436.97
  Total Reqs:   256
  Elapsed:      7.01s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     37400.45

============================================================
[4/8] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 12:15:17 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1344064) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1344064) WARNING 01-26 12:15:33 [backends.py:609] Failed to read file <frozen os>
Throughput: 37.68 requests/s, 38626.34 total tokens/s, 37.68 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-26 12:15:16] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 12:15:17] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:15:17] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 12:15:17] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:15:17] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:15:17] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:15:17] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:15:17] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:15:17] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:15:17] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 12:15:17] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 12:15:17] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 12:15:17] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 12:15:17] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 12:15:23] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 12:15:24] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:15:24] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 12:15:24] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:15:24] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:15:24] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:15:24] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:15:24] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:15:24] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:15:24] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 12:15:24] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 12:15:24] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 12:15:24] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 12:15:24] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1344064) [2026-01-26 12:15:26] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1344064) [2026-01-26 12:15:26] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1344064) [2026-01-26 12:15:26] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1344064) [2026-01-26 12:15:26] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=1344064) [2026-01-26 12:15:26] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=1344064) [2026-01-26 12:15:26] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=1344064) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1344064) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.97it/s]
(EngineCore_DP0 pid=1344064) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.41it/s]
(EngineCore_DP0 pid=1344064) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.47it/s]
(EngineCore_DP0 pid=1344064) 
(EngineCore_DP0 pid=1344064) [2026-01-26 12:15:27] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5376] -> 1D uint8
(EngineCore_DP0 pid=1344064) [2026-01-26 12:15:27] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15482880 bytes
(EngineCore_DP0 pid=1344064) [2026-01-26 12:15:27] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5376] -> 1D uint8
(EngineCore_DP0 pid=1344064) [2026-01-26 12:15:27] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 12042240 bytes
(EngineCore_DP0 pid=1344064) [2026-01-26 12:15:27] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5376] -> 1D uint8
(EngineCore_DP0 pid=1344064) [2026-01-26 12:15:27] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 127303680 bytes
(EngineCore_DP0 pid=1344064) [2026-01-26 12:15:27] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 28416] -> 1D uint8
(EngineCore_DP0 pid=1344064) [2026-01-26 12:15:27] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 63651840 bytes
(EngineCore_DP0 pid=1344064) 2026-01-26 12:15:44,375 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1344064) 2026-01-26 12:15:44,399 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1344064) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 1/4 [00:00<00:00,  8.84it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00,  2.63it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 3/4 [00:00<00:00,  3.26it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00,  4.10it/s]
(EngineCore_DP0 pid=1344064) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00, 19.39it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 19.87it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   4%|▎         | 19/512 [00:00<00:02, 189.75it/s]
Adding requests:  11%|█▏        | 58/512 [00:00<00:01, 307.19it/s]
Adding requests:  19%|█▊        | 95/512 [00:00<00:01, 334.27it/s]
Adding requests:  26%|██▌       | 133/512 [00:00<00:01, 349.62it/s]
Adding requests:  34%|███▎      | 172/512 [00:00<00:00, 363.16it/s]
Adding requests:  41%|████▏     | 212/512 [00:00<00:00, 373.50it/s]
Adding requests:  49%|████▉     | 250/512 [00:00<00:00, 375.19it/s]
Adding requests:  56%|█████▋    | 288/512 [00:00<00:00, 374.88it/s]
Adding requests:  64%|██████▍   | 327/512 [00:00<00:00, 378.98it/s]
Adding requests:  72%|███████▏  | 368/512 [00:01<00:00, 387.75it/s]
Adding requests:  80%|███████▉  | 409/512 [00:01<00:00, 392.92it/s]
Adding requests:  88%|████████▊ | 449/512 [00:01<00:00, 386.95it/s]
Adding requests:  96%|█████████▌| 492/512 [00:01<00:00, 397.43it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 373.70it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  10%|▉         | 50/512 [00:00<00:01, 434.10it/s, est. speed input: 444606.23 toks/s, output: 434.13 toks/s]
Processed prompts:  18%|█▊        | 94/512 [00:01<00:06, 63.67it/s, est. speed input: 75473.97 toks/s, output: 73.70 toks/s]   
Processed prompts:  22%|██▏       | 115/512 [00:01<00:07, 54.59it/s, est. speed input: 65256.54 toks/s, output: 63.73 toks/s]
Processed prompts:  25%|██▌       | 128/512 [00:02<00:07, 51.22it/s, est. speed input: 61745.04 toks/s, output: 60.30 toks/s]
Processed prompts:  27%|██▋       | 138/512 [00:02<00:08, 46.32it/s, est. speed input: 58046.39 toks/s, output: 56.69 toks/s]
Processed prompts:  29%|██▊       | 146/512 [00:02<00:08, 44.62it/s, est. speed input: 56493.14 toks/s, output: 55.17 toks/s]
Processed prompts:  30%|██▉       | 152/512 [00:02<00:07, 46.06it/s, est. speed input: 56530.63 toks/s, output: 55.21 toks/s]
Processed prompts:  31%|███       | 158/512 [00:02<00:08, 41.21it/s, est. speed input: 54492.35 toks/s, output: 53.21 toks/s]
Processed prompts:  32%|███▏      | 163/512 [00:03<00:08, 42.20it/s, est. speed input: 54293.15 toks/s, output: 53.02 toks/s]
Processed prompts:  33%|███▎      | 168/512 [00:03<00:07, 43.16it/s, est. speed input: 54103.09 toks/s, output: 52.83 toks/s]
Processed prompts:  34%|███▍      | 173/512 [00:03<00:07, 44.06it/s, est. speed input: 53929.51 toks/s, output: 52.67 toks/s]
Processed prompts:  35%|███▍      | 178/512 [00:03<00:09, 36.69it/s, est. speed input: 52186.02 toks/s, output: 50.96 toks/s]
Processed prompts:  36%|███▌      | 183/512 [00:03<00:08, 38.99it/s, est. speed input: 52086.46 toks/s, output: 50.87 toks/s]
Processed prompts:  37%|███▋      | 188/512 [00:03<00:07, 40.79it/s, est. speed input: 51961.55 toks/s, output: 50.74 toks/s]
Processed prompts:  38%|███▊      | 193/512 [00:03<00:07, 42.32it/s, est. speed input: 51852.31 toks/s, output: 50.64 toks/s]
Processed prompts:  39%|███▊      | 198/512 [00:04<00:09, 34.49it/s, est. speed input: 50384.95 toks/s, output: 49.20 toks/s]
Processed prompts:  40%|████      | 206/512 [00:04<00:08, 36.90it/s, est. speed input: 50015.19 toks/s, output: 48.84 toks/s]
Processed prompts:  41%|████      | 210/512 [00:04<00:08, 37.14it/s, est. speed input: 49745.71 toks/s, output: 48.58 toks/s]
Processed prompts:  42%|████▏     | 214/512 [00:04<00:07, 37.36it/s, est. speed input: 49490.42 toks/s, output: 48.33 toks/s]
Processed prompts:  43%|████▎     | 218/512 [00:04<00:07, 37.57it/s, est. speed input: 49250.32 toks/s, output: 48.10 toks/s]
Processed prompts:  43%|████▎     | 222/512 [00:04<00:07, 37.70it/s, est. speed input: 49017.60 toks/s, output: 47.87 toks/s]
Processed prompts:  44%|████▍     | 226/512 [00:04<00:07, 37.65it/s, est. speed input: 48779.83 toks/s, output: 47.64 toks/s]
Processed prompts:  45%|████▍     | 230/512 [00:04<00:07, 37.52it/s, est. speed input: 48542.61 toks/s, output: 47.40 toks/s]
Processed prompts:  46%|████▌     | 234/512 [00:04<00:07, 37.58it/s, est. speed input: 48330.95 toks/s, output: 47.20 toks/s]
Processed prompts:  46%|████▋     | 238/512 [00:05<00:07, 37.49it/s, est. speed input: 48115.33 toks/s, output: 46.99 toks/s]
Processed prompts:  47%|████▋     | 242/512 [00:05<00:07, 37.68it/s, est. speed input: 47932.15 toks/s, output: 46.81 toks/s]
Processed prompts:  48%|████▊     | 246/512 [00:05<00:07, 37.78it/s, est. speed input: 47752.70 toks/s, output: 46.63 toks/s]
Processed prompts:  49%|████▉     | 250/512 [00:05<00:06, 37.80it/s, est. speed input: 47575.27 toks/s, output: 46.46 toks/s]
Processed prompts:  50%|████▉     | 254/512 [00:05<00:06, 37.80it/s, est. speed input: 47404.60 toks/s, output: 46.29 toks/s]
Processed prompts:  50%|█████     | 258/512 [00:05<00:06, 37.89it/s, est. speed input: 47246.76 toks/s, output: 46.14 toks/s]
Processed prompts:  51%|█████     | 262/512 [00:05<00:06, 37.86it/s, est. speed input: 47087.92 toks/s, output: 45.98 toks/s]
Processed prompts:  52%|█████▏    | 266/512 [00:05<00:06, 37.72it/s, est. speed input: 46926.04 toks/s, output: 45.83 toks/s]
Processed prompts:  53%|█████▎    | 270/512 [00:05<00:06, 37.61it/s, est. speed input: 46769.18 toks/s, output: 45.67 toks/s]
Processed prompts:  54%|█████▎    | 274/512 [00:06<00:06, 37.62it/s, est. speed input: 46623.48 toks/s, output: 45.53 toks/s]
Processed prompts:  54%|█████▍    | 278/512 [00:06<00:06, 37.72it/s, est. speed input: 46490.05 toks/s, output: 45.40 toks/s]
Processed prompts:  55%|█████▌    | 282/512 [00:06<00:06, 37.72it/s, est. speed input: 46356.01 toks/s, output: 45.27 toks/s]
Processed prompts:  56%|█████▌    | 286/512 [00:06<00:05, 37.87it/s, est. speed input: 46237.27 toks/s, output: 45.15 toks/s]
Processed prompts:  57%|█████▋    | 290/512 [00:06<00:05, 37.89it/s, est. speed input: 46115.89 toks/s, output: 45.03 toks/s]
Processed prompts:  57%|█████▋    | 294/512 [00:06<00:05, 37.85it/s, est. speed input: 45995.35 toks/s, output: 44.92 toks/s]
Processed prompts:  58%|█████▊    | 298/512 [00:06<00:05, 37.85it/s, est. speed input: 45880.42 toks/s, output: 44.80 toks/s]
Processed prompts:  59%|█████▉    | 302/512 [00:06<00:05, 37.89it/s, est. speed input: 45771.20 toks/s, output: 44.70 toks/s]
Processed prompts:  61%|██████    | 310/512 [00:06<00:05, 39.37it/s, est. speed input: 45672.70 toks/s, output: 44.60 toks/s]
Processed prompts:  61%|██████▏   | 314/512 [00:07<00:05, 38.95it/s, est. speed input: 45566.83 toks/s, output: 44.50 toks/s]
Processed prompts:  62%|██████▏   | 318/512 [00:07<00:05, 38.60it/s, est. speed input: 45462.91 toks/s, output: 44.40 toks/s]
Processed prompts:  63%|██████▎   | 322/512 [00:07<00:04, 38.36it/s, est. speed input: 45363.47 toks/s, output: 44.30 toks/s]
Processed prompts:  64%|██████▎   | 326/512 [00:07<00:04, 38.23it/s, est. speed input: 45269.67 toks/s, output: 44.21 toks/s]
Processed prompts:  64%|██████▍   | 330/512 [00:07<00:04, 38.10it/s, est. speed input: 45176.35 toks/s, output: 44.12 toks/s]
Processed prompts:  65%|██████▌   | 334/512 [00:07<00:04, 38.01it/s, est. speed input: 45085.77 toks/s, output: 44.03 toks/s]
Processed prompts:  66%|██████▌   | 338/512 [00:07<00:04, 37.85it/s, est. speed input: 44992.87 toks/s, output: 43.94 toks/s]
Processed prompts:  67%|██████▋   | 342/512 [00:07<00:04, 37.92it/s, est. speed input: 44911.96 toks/s, output: 43.86 toks/s]
Processed prompts:  68%|██████▊   | 346/512 [00:07<00:04, 37.84it/s, est. speed input: 44826.44 toks/s, output: 43.78 toks/s]
Processed prompts:  68%|██████▊   | 350/512 [00:08<00:04, 37.82it/s, est. speed input: 44745.34 toks/s, output: 43.70 toks/s]
Processed prompts:  69%|██████▉   | 354/512 [00:08<00:04, 37.74it/s, est. speed input: 44662.94 toks/s, output: 43.62 toks/s]
Processed prompts:  70%|██████▉   | 358/512 [00:08<00:04, 37.68it/s, est. speed input: 44582.41 toks/s, output: 43.54 toks/s]
Processed prompts:  71%|███████   | 362/512 [00:08<00:03, 37.72it/s, est. speed input: 44507.75 toks/s, output: 43.46 toks/s]
Processed prompts:  71%|███████▏  | 366/512 [00:08<00:03, 37.78it/s, est. speed input: 44437.16 toks/s, output: 43.40 toks/s]
Processed prompts:  72%|███████▏  | 370/512 [00:08<00:03, 37.71it/s, est. speed input: 44362.18 toks/s, output: 43.32 toks/s]
Processed prompts:  73%|███████▎  | 374/512 [00:08<00:03, 37.68it/s, est. speed input: 44290.13 toks/s, output: 43.25 toks/s]
Processed prompts:  74%|███████▍  | 378/512 [00:08<00:03, 37.73it/s, est. speed input: 44223.47 toks/s, output: 43.19 toks/s]
Processed prompts:  75%|███████▍  | 382/512 [00:08<00:03, 37.82it/s, est. speed input: 44160.61 toks/s, output: 43.13 toks/s]
Processed prompts:  75%|███████▌  | 386/512 [00:08<00:03, 37.78it/s, est. speed input: 44094.58 toks/s, output: 43.06 toks/s]
Processed prompts:  76%|███████▌  | 390/512 [00:09<00:03, 37.64it/s, est. speed input: 44024.93 toks/s, output: 42.99 toks/s]
Processed prompts:  77%|███████▋  | 394/512 [00:09<00:03, 37.66it/s, est. speed input: 43962.45 toks/s, output: 42.93 toks/s]
Processed prompts:  78%|███████▊  | 398/512 [00:09<00:03, 37.68it/s, est. speed input: 43901.64 toks/s, output: 42.87 toks/s]
Processed prompts:  79%|███████▊  | 402/512 [00:09<00:02, 37.63it/s, est. speed input: 43839.25 toks/s, output: 42.81 toks/s]
Processed prompts:  79%|███████▉  | 406/512 [00:09<00:02, 37.63it/s, est. speed input: 43780.03 toks/s, output: 42.75 toks/s]
Processed prompts:  80%|████████  | 410/512 [00:09<00:02, 37.66it/s, est. speed input: 43723.01 toks/s, output: 42.70 toks/s]
Processed prompts:  81%|████████  | 414/512 [00:09<00:02, 37.65it/s, est. speed input: 43666.35 toks/s, output: 42.64 toks/s]
Processed prompts:  82%|████████▏ | 418/512 [00:09<00:02, 37.68it/s, est. speed input: 43612.32 toks/s, output: 42.59 toks/s]
Processed prompts:  82%|████████▏ | 422/512 [00:09<00:02, 37.63it/s, est. speed input: 43556.20 toks/s, output: 42.54 toks/s]
Processed prompts:  83%|████████▎ | 426/512 [00:10<00:02, 37.64it/s, est. speed input: 43503.56 toks/s, output: 42.48 toks/s]
Processed prompts:  84%|████████▍ | 430/512 [00:10<00:02, 37.68it/s, est. speed input: 43453.10 toks/s, output: 42.43 toks/s]
Processed prompts:  86%|████████▌ | 438/512 [00:10<00:01, 39.23it/s, est. speed input: 43429.51 toks/s, output: 42.41 toks/s]
Processed prompts:  86%|████████▋ | 442/512 [00:10<00:01, 38.82it/s, est. speed input: 43379.40 toks/s, output: 42.36 toks/s]
Processed prompts:  87%|████████▋ | 446/512 [00:10<00:01, 38.54it/s, est. speed input: 43331.99 toks/s, output: 42.32 toks/s]
Processed prompts:  88%|████████▊ | 450/512 [00:10<00:01, 38.31it/s, est. speed input: 43285.07 toks/s, output: 42.27 toks/s]
Processed prompts:  89%|████████▊ | 454/512 [00:10<00:01, 38.16it/s, est. speed input: 43239.73 toks/s, output: 42.23 toks/s]
Processed prompts:  89%|████████▉ | 458/512 [00:10<00:01, 38.06it/s, est. speed input: 43195.62 toks/s, output: 42.18 toks/s]
Processed prompts:  90%|█████████ | 462/512 [00:10<00:01, 37.89it/s, est. speed input: 43148.66 toks/s, output: 42.14 toks/s]
Processed prompts:  91%|█████████ | 466/512 [00:11<00:01, 37.74it/s, est. speed input: 43101.61 toks/s, output: 42.09 toks/s]
Processed prompts:  92%|█████████▏| 470/512 [00:11<00:01, 37.80it/s, est. speed input: 43061.70 toks/s, output: 42.05 toks/s]
Processed prompts:  93%|█████████▎| 474/512 [00:11<00:01, 37.61it/s, est. speed input: 43013.78 toks/s, output: 42.01 toks/s]
Processed prompts:  93%|█████████▎| 478/512 [00:11<00:00, 37.56it/s, est. speed input: 42970.10 toks/s, output: 41.96 toks/s]
Processed prompts:  94%|█████████▍| 482/512 [00:11<00:00, 37.68it/s, est. speed input: 42932.44 toks/s, output: 41.93 toks/s]
Processed prompts:  95%|█████████▍| 486/512 [00:11<00:00, 37.66it/s, est. speed input: 42892.00 toks/s, output: 41.89 toks/s]
Processed prompts:  96%|█████████▌| 490/512 [00:11<00:00, 37.69it/s, est. speed input: 42853.61 toks/s, output: 41.85 toks/s]
Processed prompts:  96%|█████████▋| 494/512 [00:11<00:00, 37.79it/s, est. speed input: 42818.85 toks/s, output: 41.82 toks/s]
Processed prompts:  97%|█████████▋| 498/512 [00:11<00:00, 37.81it/s, est. speed input: 42782.89 toks/s, output: 41.78 toks/s]
Processed prompts:  98%|█████████▊| 502/512 [00:12<00:00, 37.75it/s, est. speed input: 42745.27 toks/s, output: 41.74 toks/s]
Processed prompts:  99%|█████████▉| 506/512 [00:12<00:00, 37.74it/s, est. speed input: 42709.27 toks/s, output: 41.71 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:12<00:00, 37.74it/s, est. speed input: 42923.40 toks/s, output: 41.92 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:12<00:00, 41.92it/s, est. speed input: 42923.40 toks/s, output: 41.92 toks/s]
[rank0]:[W126 12:16:00.926184292 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 55.1s

测试结果:
  Requests/s:   37.68
  Tokens/s:     38626.34
  Total Reqs:   512
  Elapsed:      13.59s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     38588.65

============================================================
[5/8] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 12:16:14 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1345344) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1345344) WARNING 01-26 12:16:31 [backends.py:609] Failed to read file <frozen os>
Throughput: 38.03 requests/s, 38977.55 total tokens/s, 38.03 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-26 12:16:14] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 12:16:14] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:16:14] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 12:16:14] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:16:14] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:16:14] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:16:14] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:16:14] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:16:14] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:16:14] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 12:16:14] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 12:16:14] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 12:16:14] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 12:16:14] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 12:16:21] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 12:16:21] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:16:21] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 12:16:21] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:16:21] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:16:21] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:16:21] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:16:21] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:16:21] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:16:21] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 12:16:21] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 12:16:21] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 12:16:21] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 12:16:21] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1345344) [2026-01-26 12:16:23] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1345344) [2026-01-26 12:16:23] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1345344) [2026-01-26 12:16:23] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1345344) [2026-01-26 12:16:23] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=1345344) [2026-01-26 12:16:23] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=1345344) [2026-01-26 12:16:23] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=1345344) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1345344) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.96it/s]
(EngineCore_DP0 pid=1345344) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.41it/s]
(EngineCore_DP0 pid=1345344) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.47it/s]
(EngineCore_DP0 pid=1345344) 
(EngineCore_DP0 pid=1345344) [2026-01-26 12:16:24] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5376] -> 1D uint8
(EngineCore_DP0 pid=1345344) [2026-01-26 12:16:25] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15482880 bytes
(EngineCore_DP0 pid=1345344) [2026-01-26 12:16:25] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5376] -> 1D uint8
(EngineCore_DP0 pid=1345344) [2026-01-26 12:16:25] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 12042240 bytes
(EngineCore_DP0 pid=1345344) [2026-01-26 12:16:25] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5376] -> 1D uint8
(EngineCore_DP0 pid=1345344) [2026-01-26 12:16:25] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 127303680 bytes
(EngineCore_DP0 pid=1345344) [2026-01-26 12:16:25] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 28416] -> 1D uint8
(EngineCore_DP0 pid=1345344) [2026-01-26 12:16:25] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 63651840 bytes
(EngineCore_DP0 pid=1345344) 2026-01-26 12:16:42,278 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1345344) 2026-01-26 12:16:42,376 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1345344) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 1/5 [00:00<00:00,  4.72it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 3/5 [00:00<00:00, 10.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 10.06it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00,  9.50it/s]
(EngineCore_DP0 pid=1345344) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00,  8.88it/s]
Capturing CUDA graphs (decode, FULL):  75%|███████▌  | 3/4 [00:00<00:00,  3.56it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00,  4.84it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 23/1024 [00:00<00:04, 227.94it/s]
Adding requests:   6%|▋         | 64/1024 [00:00<00:02, 330.33it/s]
Adding requests:  10%|▉         | 100/1024 [00:00<00:02, 343.10it/s]
Adding requests:  13%|█▎        | 138/1024 [00:00<00:02, 355.25it/s]
Adding requests:  17%|█▋        | 178/1024 [00:00<00:02, 367.69it/s]
Adding requests:  21%|██▏       | 220/1024 [00:00<00:02, 384.53it/s]
Adding requests:  25%|██▌       | 259/1024 [00:00<00:02, 381.15it/s]
Adding requests:  29%|██▉       | 299/1024 [00:00<00:01, 386.65it/s]
Adding requests:  33%|███▎      | 340/1024 [00:00<00:01, 392.48it/s]
Adding requests:  37%|███▋      | 381/1024 [00:01<00:01, 395.49it/s]
Adding requests:  41%|████▏     | 424/1024 [00:01<00:01, 402.62it/s]
Adding requests:  45%|████▌     | 465/1024 [00:01<00:01, 397.27it/s]
Adding requests:  50%|████▉     | 509/1024 [00:01<00:01, 407.13it/s]
Adding requests:  54%|█████▎    | 550/1024 [00:01<00:01, 405.43it/s]
Adding requests:  58%|█████▊    | 591/1024 [00:01<00:01, 400.33it/s]
Adding requests:  62%|██████▏   | 632/1024 [00:01<00:00, 396.28it/s]
Adding requests:  66%|██████▌   | 672/1024 [00:01<00:00, 387.06it/s]
Adding requests:  70%|██████▉   | 714/1024 [00:01<00:00, 394.59it/s]
Adding requests:  74%|███████▎  | 754/1024 [00:01<00:00, 386.98it/s]
Adding requests:  77%|███████▋  | 793/1024 [00:02<00:00, 386.81it/s]
Adding requests:  82%|████████▏ | 835/1024 [00:02<00:00, 393.75it/s]
Adding requests:  85%|████████▌ | 875/1024 [00:02<00:00, 392.71it/s]
Adding requests:  89%|████████▉ | 915/1024 [00:02<00:00, 391.81it/s]
Adding requests:  93%|█████████▎| 955/1024 [00:02<00:00, 391.01it/s]
Adding requests:  97%|█████████▋| 995/1024 [00:02<00:00, 384.46it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 385.72it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   9%|▉         | 90/1024 [00:00<00:02, 399.55it/s, est. speed input: 409178.73 toks/s, output: 399.56 toks/s]
Processed prompts:  13%|█▎        | 130/1024 [00:01<00:10, 86.07it/s, est. speed input: 105293.96 toks/s, output: 102.82 toks/s]
Processed prompts:  15%|█▍        | 149/1024 [00:01<00:12, 72.87it/s, est. speed input: 90995.55 toks/s, output: 88.86 toks/s]  
Processed prompts:  16%|█▌        | 162/1024 [00:02<00:14, 59.24it/s, est. speed input: 79090.48 toks/s, output: 77.24 toks/s]
Processed prompts:  17%|█▋        | 171/1024 [00:02<00:15, 56.33it/s, est. speed input: 75994.66 toks/s, output: 74.21 toks/s]
Processed prompts:  17%|█▋        | 179/1024 [00:02<00:16, 52.73it/s, est. speed input: 73052.01 toks/s, output: 71.34 toks/s]
Processed prompts:  18%|█▊        | 186/1024 [00:02<00:17, 48.30it/s, est. speed input: 70095.99 toks/s, output: 68.45 toks/s]
Processed prompts:  19%|█▉        | 194/1024 [00:02<00:18, 45.68it/s, est. speed input: 67838.15 toks/s, output: 66.25 toks/s]
Processed prompts:  20%|█▉        | 202/1024 [00:03<00:18, 44.86it/s, est. speed input: 66353.32 toks/s, output: 64.80 toks/s]
Processed prompts:  21%|██        | 210/1024 [00:03<00:18, 43.26it/s, est. speed input: 64722.08 toks/s, output: 63.20 toks/s]
Processed prompts:  21%|██▏       | 218/1024 [00:03<00:19, 42.11it/s, est. speed input: 63296.43 toks/s, output: 61.81 toks/s]
Processed prompts:  22%|██▏       | 226/1024 [00:03<00:19, 41.00it/s, est. speed input: 61949.76 toks/s, output: 60.50 toks/s]
Processed prompts:  23%|██▎       | 234/1024 [00:03<00:19, 40.06it/s, est. speed input: 60706.77 toks/s, output: 59.28 toks/s]
Processed prompts:  24%|██▎       | 242/1024 [00:04<00:19, 39.52it/s, est. speed input: 59619.65 toks/s, output: 58.22 toks/s]
Processed prompts:  24%|██▍       | 250/1024 [00:04<00:19, 39.34it/s, est. speed input: 58686.52 toks/s, output: 57.31 toks/s]
Processed prompts:  25%|██▌       | 258/1024 [00:04<00:19, 39.17it/s, est. speed input: 57829.08 toks/s, output: 56.47 toks/s]
Processed prompts:  26%|██▌       | 266/1024 [00:04<00:19, 38.93it/s, est. speed input: 57019.61 toks/s, output: 55.68 toks/s]
Processed prompts:  27%|██▋       | 274/1024 [00:04<00:19, 38.69it/s, est. speed input: 56264.26 toks/s, output: 54.95 toks/s]
Processed prompts:  28%|██▊       | 282/1024 [00:05<00:19, 38.54it/s, est. speed input: 55572.21 toks/s, output: 54.27 toks/s]
Processed prompts:  28%|██▊       | 290/1024 [00:05<00:18, 38.69it/s, est. speed input: 54981.09 toks/s, output: 53.69 toks/s]
Processed prompts:  29%|██▉       | 298/1024 [00:05<00:18, 38.76it/s, est. speed input: 54426.87 toks/s, output: 53.15 toks/s]
Processed prompts:  30%|██▉       | 306/1024 [00:05<00:18, 39.83it/s, est. speed input: 54076.58 toks/s, output: 52.81 toks/s]
Processed prompts:  31%|███       | 314/1024 [00:06<00:18, 39.40it/s, est. speed input: 53565.28 toks/s, output: 52.31 toks/s]
Processed prompts:  31%|███▏      | 322/1024 [00:06<00:17, 39.03it/s, est. speed input: 53078.14 toks/s, output: 51.83 toks/s]
Processed prompts:  32%|███▏      | 330/1024 [00:06<00:17, 38.91it/s, est. speed input: 52641.70 toks/s, output: 51.41 toks/s]
Processed prompts:  33%|███▎      | 338/1024 [00:06<00:17, 38.95it/s, est. speed input: 52250.37 toks/s, output: 51.03 toks/s]
Processed prompts:  34%|███▍      | 346/1024 [00:06<00:17, 38.71it/s, est. speed input: 51846.69 toks/s, output: 50.63 toks/s]
Processed prompts:  35%|███▍      | 354/1024 [00:07<00:17, 38.59it/s, est. speed input: 51472.80 toks/s, output: 50.27 toks/s]
Processed prompts:  35%|███▌      | 362/1024 [00:07<00:17, 38.63it/s, est. speed input: 51135.15 toks/s, output: 49.94 toks/s]
Processed prompts:  36%|███▌      | 370/1024 [00:07<00:16, 38.51it/s, est. speed input: 50798.64 toks/s, output: 49.61 toks/s]
Processed prompts:  37%|███▋      | 378/1024 [00:07<00:16, 38.60it/s, est. speed input: 50501.65 toks/s, output: 49.32 toks/s]
Processed prompts:  38%|███▊      | 386/1024 [00:07<00:16, 38.45it/s, est. speed input: 50195.61 toks/s, output: 49.02 toks/s]
Processed prompts:  38%|███▊      | 394/1024 [00:08<00:16, 38.43it/s, est. speed input: 49915.18 toks/s, output: 48.75 toks/s]
Processed prompts:  39%|███▉      | 402/1024 [00:08<00:16, 38.52it/s, est. speed input: 49659.29 toks/s, output: 48.50 toks/s]
Processed prompts:  40%|████      | 410/1024 [00:08<00:15, 38.45it/s, est. speed input: 49402.07 toks/s, output: 48.24 toks/s]
Processed prompts:  41%|████      | 418/1024 [00:08<00:15, 38.52it/s, est. speed input: 49169.44 toks/s, output: 48.02 toks/s]
Processed prompts:  42%|████▏     | 426/1024 [00:08<00:15, 38.35it/s, est. speed input: 48926.75 toks/s, output: 47.78 toks/s]
Processed prompts:  42%|████▏     | 434/1024 [00:09<00:14, 39.60it/s, est. speed input: 48822.98 toks/s, output: 47.68 toks/s]
Processed prompts:  43%|████▎     | 442/1024 [00:09<00:14, 39.38it/s, est. speed input: 48623.45 toks/s, output: 47.48 toks/s]
Processed prompts:  44%|████▍     | 450/1024 [00:09<00:14, 39.14it/s, est. speed input: 48425.04 toks/s, output: 47.29 toks/s]
Processed prompts:  45%|████▍     | 458/1024 [00:09<00:14, 38.98it/s, est. speed input: 48235.40 toks/s, output: 47.10 toks/s]
Processed prompts:  46%|████▌     | 466/1024 [00:09<00:14, 38.82it/s, est. speed input: 48050.55 toks/s, output: 46.92 toks/s]
Processed prompts:  46%|████▋     | 474/1024 [00:10<00:14, 38.70it/s, est. speed input: 47871.15 toks/s, output: 46.75 toks/s]
Processed prompts:  47%|████▋     | 482/1024 [00:10<00:14, 38.71it/s, est. speed input: 47707.55 toks/s, output: 46.59 toks/s]
Processed prompts:  48%|████▊     | 490/1024 [00:10<00:13, 38.61it/s, est. speed input: 47541.75 toks/s, output: 46.43 toks/s]
Processed prompts:  49%|████▊     | 498/1024 [00:10<00:13, 38.52it/s, est. speed input: 47380.72 toks/s, output: 46.27 toks/s]
Processed prompts:  49%|████▉     | 506/1024 [00:10<00:13, 38.52it/s, est. speed input: 47230.32 toks/s, output: 46.12 toks/s]
Processed prompts:  50%|█████     | 514/1024 [00:11<00:13, 38.45it/s, est. speed input: 47080.37 toks/s, output: 45.98 toks/s]
Processed prompts:  51%|█████     | 522/1024 [00:11<00:13, 38.47it/s, est. speed input: 46940.72 toks/s, output: 45.84 toks/s]
Processed prompts:  52%|█████▏    | 530/1024 [00:11<00:12, 38.46it/s, est. speed input: 46804.70 toks/s, output: 45.71 toks/s]
Processed prompts:  53%|█████▎    | 538/1024 [00:11<00:12, 38.42it/s, est. speed input: 46670.96 toks/s, output: 45.58 toks/s]
Processed prompts:  53%|█████▎    | 546/1024 [00:12<00:12, 38.40it/s, est. speed input: 46542.68 toks/s, output: 45.45 toks/s]
Processed prompts:  54%|█████▍    | 554/1024 [00:12<00:12, 38.50it/s, est. speed input: 46426.28 toks/s, output: 45.34 toks/s]
Processed prompts:  55%|█████▍    | 562/1024 [00:12<00:11, 38.59it/s, est. speed input: 46315.60 toks/s, output: 45.23 toks/s]
Processed prompts:  56%|█████▌    | 570/1024 [00:12<00:11, 38.56it/s, est. speed input: 46201.91 toks/s, output: 45.12 toks/s]
Processed prompts:  56%|█████▋    | 578/1024 [00:12<00:11, 38.45it/s, est. speed input: 46085.88 toks/s, output: 45.01 toks/s]
Processed prompts:  57%|█████▋    | 586/1024 [00:13<00:11, 38.40it/s, est. speed input: 45975.84 toks/s, output: 44.90 toks/s]
Processed prompts:  58%|█████▊    | 594/1024 [00:13<00:11, 38.43it/s, est. speed input: 45872.96 toks/s, output: 44.80 toks/s]
Processed prompts:  59%|█████▉    | 602/1024 [00:13<00:10, 38.52it/s, est. speed input: 45777.76 toks/s, output: 44.70 toks/s]
Processed prompts:  60%|█████▉    | 610/1024 [00:13<00:10, 38.38it/s, est. speed input: 45673.41 toks/s, output: 44.60 toks/s]
Processed prompts:  60%|██████    | 618/1024 [00:13<00:10, 38.30it/s, est. speed input: 45572.53 toks/s, output: 44.50 toks/s]
Processed prompts:  61%|██████    | 626/1024 [00:14<00:10, 38.24it/s, est. speed input: 45474.93 toks/s, output: 44.41 toks/s]
Processed prompts:  62%|██████▏   | 634/1024 [00:14<00:10, 38.32it/s, est. speed input: 45387.53 toks/s, output: 44.32 toks/s]
Processed prompts:  63%|██████▎   | 642/1024 [00:14<00:09, 38.35it/s, est. speed input: 45300.37 toks/s, output: 44.24 toks/s]
Processed prompts:  63%|██████▎   | 650/1024 [00:14<00:09, 38.38it/s, est. speed input: 45216.89 toks/s, output: 44.16 toks/s]
Processed prompts:  64%|██████▍   | 658/1024 [00:14<00:09, 38.47it/s, est. speed input: 45139.16 toks/s, output: 44.08 toks/s]
Processed prompts:  65%|██████▌   | 666/1024 [00:15<00:09, 38.39it/s, est. speed input: 45055.97 toks/s, output: 44.00 toks/s]
Processed prompts:  66%|██████▌   | 674/1024 [00:15<00:09, 38.37it/s, est. speed input: 44976.73 toks/s, output: 43.92 toks/s]
Processed prompts:  67%|██████▋   | 682/1024 [00:15<00:08, 38.29it/s, est. speed input: 44896.62 toks/s, output: 43.84 toks/s]
Processed prompts:  67%|██████▋   | 690/1024 [00:15<00:08, 38.26it/s, est. speed input: 44819.34 toks/s, output: 43.77 toks/s]
Processed prompts:  68%|██████▊   | 698/1024 [00:15<00:08, 38.25it/s, est. speed input: 44745.05 toks/s, output: 43.70 toks/s]
Processed prompts:  69%|██████▉   | 706/1024 [00:16<00:08, 38.19it/s, est. speed input: 44669.88 toks/s, output: 43.62 toks/s]
Processed prompts:  70%|██████▉   | 714/1024 [00:16<00:08, 38.21it/s, est. speed input: 44599.67 toks/s, output: 43.55 toks/s]
Processed prompts:  71%|███████   | 722/1024 [00:16<00:07, 38.24it/s, est. speed input: 44532.48 toks/s, output: 43.49 toks/s]
Processed prompts:  71%|███████▏  | 730/1024 [00:16<00:07, 38.24it/s, est. speed input: 44465.37 toks/s, output: 43.42 toks/s]
Processed prompts:  72%|███████▏  | 738/1024 [00:17<00:07, 38.29it/s, est. speed input: 44402.75 toks/s, output: 43.36 toks/s]
Processed prompts:  73%|███████▎  | 746/1024 [00:17<00:07, 38.35it/s, est. speed input: 44342.22 toks/s, output: 43.30 toks/s]
Processed prompts:  74%|███████▎  | 754/1024 [00:17<00:07, 38.27it/s, est. speed input: 44277.94 toks/s, output: 43.24 toks/s]
Processed prompts:  74%|███████▍  | 762/1024 [00:17<00:06, 38.29it/s, est. speed input: 44218.69 toks/s, output: 43.18 toks/s]
Processed prompts:  75%|███████▌  | 770/1024 [00:17<00:06, 38.16it/s, est. speed input: 44154.06 toks/s, output: 43.12 toks/s]
Processed prompts:  76%|███████▌  | 778/1024 [00:18<00:06, 38.28it/s, est. speed input: 44100.67 toks/s, output: 43.07 toks/s]
Processed prompts:  77%|███████▋  | 786/1024 [00:18<00:06, 39.61it/s, est. speed input: 44100.92 toks/s, output: 43.07 toks/s]
Processed prompts:  78%|███████▊  | 794/1024 [00:18<00:05, 39.16it/s, est. speed input: 44043.68 toks/s, output: 43.01 toks/s]
Processed prompts:  78%|███████▊  | 802/1024 [00:18<00:05, 38.96it/s, est. speed input: 43992.14 toks/s, output: 42.96 toks/s]
Processed prompts:  79%|███████▉  | 810/1024 [00:18<00:05, 38.73it/s, est. speed input: 43938.01 toks/s, output: 42.91 toks/s]
Processed prompts:  80%|███████▉  | 818/1024 [00:19<00:05, 38.54it/s, est. speed input: 43884.14 toks/s, output: 42.86 toks/s]
Processed prompts:  81%|████████  | 826/1024 [00:19<00:05, 38.46it/s, est. speed input: 43833.35 toks/s, output: 42.81 toks/s]
Processed prompts:  81%|████████▏ | 834/1024 [00:19<00:04, 38.36it/s, est. speed input: 43781.64 toks/s, output: 42.76 toks/s]
Processed prompts:  82%|████████▏ | 842/1024 [00:19<00:04, 38.34it/s, est. speed input: 43733.11 toks/s, output: 42.71 toks/s]
Processed prompts:  83%|████████▎ | 850/1024 [00:19<00:04, 38.34it/s, est. speed input: 43686.56 toks/s, output: 42.66 toks/s]
Processed prompts:  84%|████████▍ | 858/1024 [00:20<00:04, 38.30it/s, est. speed input: 43639.10 toks/s, output: 42.62 toks/s]
Processed prompts:  85%|████████▍ | 866/1024 [00:20<00:04, 38.26it/s, est. speed input: 43592.06 toks/s, output: 42.57 toks/s]
Processed prompts:  85%|████████▌ | 874/1024 [00:20<00:03, 38.44it/s, est. speed input: 43554.27 toks/s, output: 42.53 toks/s]
Processed prompts:  86%|████████▌ | 882/1024 [00:20<00:03, 38.38it/s, est. speed input: 43509.80 toks/s, output: 42.49 toks/s]
Processed prompts:  87%|████████▋ | 890/1024 [00:20<00:03, 38.42it/s, est. speed input: 43469.48 toks/s, output: 42.45 toks/s]
Processed prompts:  88%|████████▊ | 898/1024 [00:21<00:03, 38.38it/s, est. speed input: 43427.43 toks/s, output: 42.41 toks/s]
Processed prompts:  88%|████████▊ | 906/1024 [00:21<00:03, 38.38it/s, est. speed input: 43387.03 toks/s, output: 42.37 toks/s]
Processed prompts:  89%|████████▉ | 914/1024 [00:21<00:02, 38.50it/s, est. speed input: 43351.90 toks/s, output: 42.34 toks/s]
Processed prompts:  90%|█████████ | 922/1024 [00:21<00:02, 38.43it/s, est. speed input: 43312.24 toks/s, output: 42.30 toks/s]
Processed prompts:  91%|█████████ | 930/1024 [00:22<00:02, 38.51it/s, est. speed input: 43277.32 toks/s, output: 42.26 toks/s]
Processed prompts:  92%|█████████▏| 938/1024 [00:22<00:02, 38.40it/s, est. speed input: 43237.64 toks/s, output: 42.22 toks/s]
Processed prompts:  92%|█████████▏| 946/1024 [00:22<00:02, 38.39it/s, est. speed input: 43200.91 toks/s, output: 42.19 toks/s]
Processed prompts:  93%|█████████▎| 954/1024 [00:22<00:01, 38.43it/s, est. speed input: 43166.30 toks/s, output: 42.15 toks/s]
Processed prompts:  94%|█████████▍| 962/1024 [00:22<00:01, 38.50it/s, est. speed input: 43134.14 toks/s, output: 42.12 toks/s]
Processed prompts:  95%|█████████▍| 970/1024 [00:23<00:01, 38.51it/s, est. speed input: 43100.99 toks/s, output: 42.09 toks/s]
Processed prompts:  96%|█████████▌| 978/1024 [00:23<00:01, 38.43it/s, est. speed input: 43065.64 toks/s, output: 42.06 toks/s]
Processed prompts:  96%|█████████▋| 986/1024 [00:23<00:00, 38.38it/s, est. speed input: 43030.99 toks/s, output: 42.02 toks/s]
Processed prompts:  97%|█████████▋| 994/1024 [00:23<00:00, 38.37it/s, est. speed input: 42997.75 toks/s, output: 41.99 toks/s]
Processed prompts:  98%|█████████▊| 1002/1024 [00:23<00:00, 38.39it/s, est. speed input: 42966.08 toks/s, output: 41.96 toks/s]
Processed prompts:  99%|█████████▊| 1010/1024 [00:24<00:00, 38.47it/s, est. speed input: 42937.19 toks/s, output: 41.93 toks/s]
Processed prompts:  99%|█████████▉| 1018/1024 [00:24<00:00, 39.87it/s, est. speed input: 42949.79 toks/s, output: 41.94 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:24<00:00, 39.87it/s, est. speed input: 43202.18 toks/s, output: 42.19 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:24<00:00, 42.19it/s, est. speed input: 43202.18 toks/s, output: 42.19 toks/s]
[rank0]:[W126 12:17:12.489762750 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 71.7s

测试结果:
  Requests/s:   38.03
  Tokens/s:     38977.55
  Total Reqs:   1024
  Elapsed:      26.93s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     38939.52

============================================================
[6/8] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 12:17:32 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1346882) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1346882) WARNING 01-26 12:17:49 [backends.py:609] Failed to read file <frozen os>
Throughput: 39.49 requests/s, 40476.94 total tokens/s, 39.49 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-26 12:17:31] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 12:17:32] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:17:32] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 12:17:32] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:17:32] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:17:32] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:17:32] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:17:32] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:17:32] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:17:32] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 12:17:32] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 12:17:32] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 12:17:32] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 12:17:32] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 12:17:39] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 12:17:40] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:17:40] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 12:17:40] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:17:40] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:17:40] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:17:40] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:17:40] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:17:40] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:17:40] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 12:17:40] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 12:17:40] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 12:17:40] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 12:17:40] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1346882) [2026-01-26 12:17:41] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1346882) [2026-01-26 12:17:41] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1346882) [2026-01-26 12:17:41] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1346882) [2026-01-26 12:17:41] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=1346882) [2026-01-26 12:17:41] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=1346882) [2026-01-26 12:17:41] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=1346882) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1346882) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.98it/s]
(EngineCore_DP0 pid=1346882) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.42it/s]
(EngineCore_DP0 pid=1346882) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.49it/s]
(EngineCore_DP0 pid=1346882) 
(EngineCore_DP0 pid=1346882) [2026-01-26 12:17:43] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5376] -> 1D uint8
(EngineCore_DP0 pid=1346882) [2026-01-26 12:17:43] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15482880 bytes
(EngineCore_DP0 pid=1346882) [2026-01-26 12:17:43] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5376] -> 1D uint8
(EngineCore_DP0 pid=1346882) [2026-01-26 12:17:43] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 12042240 bytes
(EngineCore_DP0 pid=1346882) [2026-01-26 12:17:43] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5376] -> 1D uint8
(EngineCore_DP0 pid=1346882) [2026-01-26 12:17:43] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 127303680 bytes
(EngineCore_DP0 pid=1346882) [2026-01-26 12:17:43] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 28416] -> 1D uint8
(EngineCore_DP0 pid=1346882) [2026-01-26 12:17:43] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 63651840 bytes
(EngineCore_DP0 pid=1346882) [rank0]:W0126 12:17:54.827000 1346882 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1346882) [rank0]:W0126 12:17:54.904000 1346882 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1346882) [rank0]:W0126 12:17:56.152000 1346882 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1346882) [rank0]:W0126 12:17:56.275000 1346882 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1346882) 2026-01-26 12:17:59,794 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1346882) 2026-01-26 12:17:59,820 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1346882) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00, 13.07it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00,  7.90it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 5/7 [00:00<00:00,  5.32it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:01<00:00,  4.16it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:01<00:00,  5.60it/s]
(EngineCore_DP0 pid=1346882) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00, 19.23it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 19.84it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 19.76it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   1%|          | 22/2048 [00:00<00:09, 218.34it/s]
Adding requests:   3%|▎         | 62/2048 [00:00<00:06, 324.67it/s]
Adding requests:   5%|▍         | 98/2048 [00:00<00:05, 339.31it/s]
Adding requests:   7%|▋         | 136/2048 [00:00<00:05, 354.74it/s]
Adding requests:   9%|▊         | 176/2048 [00:00<00:05, 367.56it/s]
Adding requests:  11%|█         | 217/2048 [00:00<00:04, 379.98it/s]
Adding requests:  12%|█▎        | 256/2048 [00:00<00:04, 380.13it/s]
Adding requests:  14%|█▍        | 296/2048 [00:00<00:04, 384.36it/s]
Adding requests:  16%|█▋        | 337/2048 [00:00<00:04, 392.04it/s]
Adding requests:  18%|█▊        | 377/2048 [00:01<00:04, 393.57it/s]
Adding requests:  20%|██        | 419/2048 [00:01<00:04, 400.48it/s]
Adding requests:  22%|██▏       | 460/2048 [00:01<00:04, 395.68it/s]
Adding requests:  25%|██▍       | 503/2048 [00:01<00:03, 403.54it/s]
Adding requests:  27%|██▋       | 546/2048 [00:01<00:03, 407.91it/s]
Adding requests:  29%|██▊       | 587/2048 [00:01<00:03, 402.32it/s]
Adding requests:  31%|███       | 628/2048 [00:01<00:03, 397.77it/s]
Adding requests:  33%|███▎      | 668/2048 [00:01<00:03, 386.74it/s]
Adding requests:  35%|███▍      | 709/2048 [00:01<00:03, 391.52it/s]
Adding requests:  37%|███▋      | 749/2048 [00:01<00:03, 379.41it/s]
Adding requests:  39%|███▊      | 789/2048 [00:02<00:03, 382.56it/s]
Adding requests:  40%|████      | 829/2048 [00:02<00:03, 387.15it/s]
Adding requests:  42%|████▏     | 869/2048 [00:02<00:03, 389.80it/s]
Adding requests:  44%|████▍     | 909/2048 [00:02<00:02, 392.29it/s]
Adding requests:  46%|████▋     | 949/2048 [00:02<00:02, 383.86it/s]
Adding requests:  48%|████▊     | 989/2048 [00:02<00:02, 386.24it/s]
Adding requests:  50%|█████     | 1028/2048 [00:02<00:02, 383.58it/s]
Adding requests:  52%|█████▏    | 1067/2048 [00:02<00:02, 380.49it/s]
Adding requests:  54%|█████▍    | 1106/2048 [00:02<00:02, 378.60it/s]
Adding requests:  56%|█████▌    | 1147/2048 [00:02<00:02, 385.07it/s]
Adding requests:  58%|█████▊    | 1186/2048 [00:03<00:02, 383.51it/s]
Adding requests:  60%|█████▉    | 1227/2048 [00:03<00:02, 389.50it/s]
Adding requests:  62%|██████▏   | 1266/2048 [00:03<00:02, 386.37it/s]
Adding requests:  64%|██████▎   | 1305/2048 [00:03<00:01, 383.86it/s]
Adding requests:  66%|██████▌   | 1344/2048 [00:03<00:01, 383.84it/s]
Adding requests:  68%|██████▊   | 1384/2048 [00:03<00:01, 388.32it/s]
Adding requests:  69%|██████▉   | 1423/2048 [00:03<00:01, 385.75it/s]
Adding requests:  71%|███████▏  | 1463/2048 [00:03<00:01, 388.67it/s]
Adding requests:  73%|███████▎  | 1503/2048 [00:03<00:01, 390.82it/s]
Adding requests:  75%|███████▌  | 1543/2048 [00:04<00:01, 390.14it/s]
Adding requests:  77%|███████▋  | 1583/2048 [00:04<00:01, 383.36it/s]
Adding requests:  79%|███████▉  | 1622/2048 [00:04<00:01, 377.43it/s]
Adding requests:  81%|████████  | 1660/2048 [00:04<00:01, 371.25it/s]
Adding requests:  83%|████████▎ | 1699/2048 [00:04<00:00, 375.00it/s]
Adding requests:  85%|████████▍ | 1738/2048 [00:04<00:00, 379.35it/s]
Adding requests:  87%|████████▋ | 1779/2048 [00:04<00:00, 387.41it/s]
Adding requests:  89%|████████▉ | 1818/2048 [00:04<00:00, 381.90it/s]
Adding requests:  91%|█████████ | 1858/2048 [00:04<00:00, 387.00it/s]
Adding requests:  93%|█████████▎| 1897/2048 [00:04<00:00, 386.08it/s]
Adding requests:  95%|█████████▍| 1936/2048 [00:05<00:00, 381.20it/s]
Adding requests:  96%|█████████▋| 1976/2048 [00:05<00:00, 385.00it/s]
Adding requests:  98%|█████████▊| 2015/2048 [00:05<00:00, 380.28it/s]
Adding requests: 100%|██████████| 2048/2048 [00:05<00:00, 383.31it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  10%|█         | 210/2048 [00:00<00:03, 463.89it/s, est. speed input: 475048.45 toks/s, output: 463.90 toks/s]
Processed prompts:  13%|█▎        | 257/2048 [00:01<00:10, 171.26it/s, est. speed input: 207446.18 toks/s, output: 202.58 toks/s]
Processed prompts:  14%|█▎        | 280/2048 [00:02<00:17, 99.19it/s, est. speed input: 138073.06 toks/s, output: 134.84 toks/s] 
Processed prompts:  14%|█▍        | 294/2048 [00:02<00:21, 82.16it/s, est. speed input: 121431.34 toks/s, output: 118.58 toks/s]
Processed prompts:  15%|█▍        | 306/2048 [00:02<00:25, 68.92it/s, est. speed input: 109628.45 toks/s, output: 107.06 toks/s]
Processed prompts:  16%|█▌        | 322/2048 [00:03<00:28, 60.50it/s, est. speed input: 101037.48 toks/s, output: 98.67 toks/s] 
Processed prompts:  17%|█▋        | 338/2048 [00:03<00:31, 54.52it/s, est. speed input: 94420.71 toks/s, output: 92.21 toks/s] 
Processed prompts:  17%|█▋        | 354/2048 [00:04<00:33, 50.19it/s, est. speed input: 89091.98 toks/s, output: 87.00 toks/s]
Processed prompts:  18%|█▊        | 370/2048 [00:04<00:35, 47.02it/s, est. speed input: 84685.82 toks/s, output: 82.70 toks/s]
Processed prompts:  19%|█▉        | 386/2048 [00:04<00:37, 44.87it/s, est. speed input: 81054.39 toks/s, output: 79.15 toks/s]
Processed prompts:  20%|█▉        | 402/2048 [00:05<00:38, 43.24it/s, est. speed input: 77924.74 toks/s, output: 76.10 toks/s]
Processed prompts:  20%|██        | 418/2048 [00:05<00:38, 42.18it/s, est. speed input: 75278.83 toks/s, output: 73.51 toks/s]
Processed prompts:  21%|██        | 434/2048 [00:06<00:38, 42.08it/s, est. speed input: 73234.15 toks/s, output: 71.52 toks/s]
Processed prompts:  22%|██▏       | 450/2048 [00:06<00:38, 41.31it/s, est. speed input: 71189.54 toks/s, output: 69.52 toks/s]
Processed prompts:  23%|██▎       | 466/2048 [00:06<00:38, 40.85it/s, est. speed input: 69406.24 toks/s, output: 67.78 toks/s]
Processed prompts:  24%|██▎       | 482/2048 [00:07<00:38, 40.36it/s, est. speed input: 67769.85 toks/s, output: 66.18 toks/s]
Processed prompts:  24%|██▍       | 498/2048 [00:07<00:38, 40.13it/s, est. speed input: 66337.30 toks/s, output: 64.78 toks/s]
Processed prompts:  25%|██▌       | 514/2048 [00:08<00:38, 39.96it/s, est. speed input: 65048.63 toks/s, output: 63.52 toks/s]
Processed prompts:  26%|██▌       | 530/2048 [00:08<00:38, 39.83it/s, est. speed input: 63878.42 toks/s, output: 62.38 toks/s]
Processed prompts:  27%|██▋       | 546/2048 [00:08<00:37, 39.73it/s, est. speed input: 62810.16 toks/s, output: 61.34 toks/s]
Processed prompts:  27%|██▋       | 562/2048 [00:09<00:37, 39.63it/s, est. speed input: 61830.27 toks/s, output: 60.38 toks/s]
Processed prompts:  28%|██▊       | 578/2048 [00:09<00:37, 39.62it/s, est. speed input: 60945.21 toks/s, output: 59.52 toks/s]
Processed prompts:  29%|██▉       | 594/2048 [00:10<00:36, 39.57it/s, est. speed input: 60122.20 toks/s, output: 58.71 toks/s]
Processed prompts:  30%|██▉       | 610/2048 [00:10<00:36, 39.54it/s, est. speed input: 59362.39 toks/s, output: 57.97 toks/s]
Processed prompts:  31%|███       | 626/2048 [00:10<00:36, 39.49it/s, est. speed input: 58653.69 toks/s, output: 57.28 toks/s]
Processed prompts:  31%|███▏      | 642/2048 [00:11<00:35, 39.48it/s, est. speed input: 58002.02 toks/s, output: 56.64 toks/s]
Processed prompts:  32%|███▏      | 658/2048 [00:11<00:35, 39.46it/s, est. speed input: 57391.80 toks/s, output: 56.05 toks/s]
Processed prompts:  33%|███▎      | 674/2048 [00:12<00:34, 39.45it/s, est. speed input: 56822.50 toks/s, output: 55.49 toks/s]
Processed prompts:  34%|███▎      | 690/2048 [00:12<00:34, 39.44it/s, est. speed input: 56289.85 toks/s, output: 54.97 toks/s]
Processed prompts:  34%|███▍      | 706/2048 [00:12<00:33, 39.47it/s, est. speed input: 55797.42 toks/s, output: 54.49 toks/s]
Processed prompts:  35%|███▌      | 722/2048 [00:13<00:33, 39.42it/s, est. speed input: 55323.79 toks/s, output: 54.03 toks/s]
Processed prompts:  36%|███▌      | 738/2048 [00:13<00:33, 39.44it/s, est. speed input: 54884.93 toks/s, output: 53.60 toks/s]
Processed prompts:  37%|███▋      | 754/2048 [00:14<00:32, 39.44it/s, est. speed input: 54470.28 toks/s, output: 53.19 toks/s]
Processed prompts:  38%|███▊      | 770/2048 [00:14<00:32, 39.37it/s, est. speed input: 54069.52 toks/s, output: 52.80 toks/s]
Processed prompts:  38%|███▊      | 786/2048 [00:14<00:31, 40.03it/s, est. speed input: 53777.28 toks/s, output: 52.52 toks/s]
Processed prompts:  39%|███▉      | 802/2048 [00:15<00:31, 39.90it/s, est. speed input: 53429.53 toks/s, output: 52.18 toks/s]
Processed prompts:  40%|███▉      | 818/2048 [00:15<00:30, 39.79it/s, est. speed input: 53096.90 toks/s, output: 51.85 toks/s]
Processed prompts:  41%|████      | 834/2048 [00:16<00:30, 39.66it/s, est. speed input: 52775.67 toks/s, output: 51.54 toks/s]
Processed prompts:  42%|████▏     | 850/2048 [00:16<00:30, 39.55it/s, est. speed input: 52467.67 toks/s, output: 51.24 toks/s]
Processed prompts:  42%|████▏     | 866/2048 [00:16<00:29, 39.54it/s, est. speed input: 52181.93 toks/s, output: 50.96 toks/s]
Processed prompts:  43%|████▎     | 882/2048 [00:17<00:29, 39.49it/s, est. speed input: 51904.94 toks/s, output: 50.69 toks/s]
Processed prompts:  44%|████▍     | 898/2048 [00:17<00:29, 39.43it/s, est. speed input: 51638.03 toks/s, output: 50.43 toks/s]
Processed prompts:  45%|████▍     | 914/2048 [00:18<00:28, 39.45it/s, est. speed input: 51389.45 toks/s, output: 50.18 toks/s]
Processed prompts:  45%|████▌     | 930/2048 [00:18<00:28, 39.41it/s, est. speed input: 51146.19 toks/s, output: 49.95 toks/s]
Processed prompts:  46%|████▌     | 946/2048 [00:19<00:27, 39.40it/s, est. speed input: 50914.95 toks/s, output: 49.72 toks/s]
Processed prompts:  47%|████▋     | 962/2048 [00:19<00:27, 39.41it/s, est. speed input: 50695.03 toks/s, output: 49.51 toks/s]
Processed prompts:  48%|████▊     | 978/2048 [00:19<00:27, 39.43it/s, est. speed input: 50485.48 toks/s, output: 49.30 toks/s]
Processed prompts:  49%|████▊     | 994/2048 [00:20<00:26, 39.38it/s, est. speed input: 50278.05 toks/s, output: 49.10 toks/s]
Processed prompts:  49%|████▉     | 1010/2048 [00:20<00:26, 39.36it/s, est. speed input: 50080.57 toks/s, output: 48.91 toks/s]
Processed prompts:  50%|█████     | 1026/2048 [00:21<00:25, 39.37it/s, est. speed input: 49892.73 toks/s, output: 48.72 toks/s]
Processed prompts:  51%|█████     | 1042/2048 [00:21<00:25, 39.35it/s, est. speed input: 49709.71 toks/s, output: 48.54 toks/s]
Processed prompts:  52%|█████▏    | 1058/2048 [00:21<00:25, 39.32it/s, est. speed input: 49532.57 toks/s, output: 48.37 toks/s]
Processed prompts:  52%|█████▏    | 1074/2048 [00:22<00:24, 39.38it/s, est. speed input: 49367.54 toks/s, output: 48.21 toks/s]
Processed prompts:  53%|█████▎    | 1090/2048 [00:22<00:24, 39.32it/s, est. speed input: 49201.32 toks/s, output: 48.05 toks/s]
Processed prompts:  54%|█████▍    | 1106/2048 [00:23<00:23, 39.38it/s, est. speed input: 49048.33 toks/s, output: 47.90 toks/s]
Processed prompts:  55%|█████▍    | 1122/2048 [00:23<00:23, 39.37it/s, est. speed input: 48896.90 toks/s, output: 47.75 toks/s]
Processed prompts:  56%|█████▌    | 1138/2048 [00:23<00:23, 39.28it/s, est. speed input: 48744.35 toks/s, output: 47.60 toks/s]
Processed prompts:  56%|█████▋    | 1154/2048 [00:24<00:22, 39.35it/s, est. speed input: 48606.44 toks/s, output: 47.47 toks/s]
Processed prompts:  57%|█████▋    | 1170/2048 [00:24<00:22, 39.35it/s, est. speed input: 48469.73 toks/s, output: 47.33 toks/s]
Processed prompts:  58%|█████▊    | 1186/2048 [00:25<00:21, 39.34it/s, est. speed input: 48336.44 toks/s, output: 47.20 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [00:25<00:21, 39.92it/s, est. speed input: 48245.71 toks/s, output: 47.11 toks/s]
Processed prompts:  59%|█████▉    | 1218/2048 [00:25<00:20, 39.75it/s, est. speed input: 48120.82 toks/s, output: 46.99 toks/s]
Processed prompts:  60%|██████    | 1234/2048 [00:26<00:20, 40.29it/s, est. speed input: 48040.14 toks/s, output: 46.91 toks/s]
Processed prompts:  61%|██████    | 1250/2048 [00:26<00:19, 39.92it/s, est. speed input: 47917.20 toks/s, output: 46.79 toks/s]
Processed prompts:  62%|██████▏   | 1266/2048 [00:27<00:19, 39.72it/s, est. speed input: 47801.20 toks/s, output: 46.68 toks/s]
Processed prompts:  63%|██████▎   | 1282/2048 [00:27<00:19, 39.57it/s, est. speed input: 47688.35 toks/s, output: 46.57 toks/s]
Processed prompts:  63%|██████▎   | 1298/2048 [00:27<00:19, 39.42it/s, est. speed input: 47575.71 toks/s, output: 46.46 toks/s]
Processed prompts:  64%|██████▍   | 1314/2048 [00:28<00:18, 39.39it/s, est. speed input: 47470.73 toks/s, output: 46.36 toks/s]
Processed prompts:  65%|██████▍   | 1330/2048 [00:28<00:17, 39.95it/s, est. speed input: 47401.44 toks/s, output: 46.29 toks/s]
Processed prompts:  66%|██████▌   | 1346/2048 [00:29<00:17, 39.77it/s, est. speed input: 47302.29 toks/s, output: 46.19 toks/s]
Processed prompts:  67%|██████▋   | 1362/2048 [00:29<00:17, 39.58it/s, est. speed input: 47202.30 toks/s, output: 46.10 toks/s]
Processed prompts:  67%|██████▋   | 1378/2048 [00:29<00:16, 39.48it/s, est. speed input: 47107.01 toks/s, output: 46.00 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [00:30<00:16, 39.37it/s, est. speed input: 47011.82 toks/s, output: 45.91 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [00:30<00:16, 39.29it/s, est. speed input: 46919.03 toks/s, output: 45.82 toks/s]
Processed prompts:  70%|██████▉   | 1426/2048 [00:31<00:15, 39.28it/s, est. speed input: 46831.58 toks/s, output: 45.73 toks/s]
Processed prompts:  70%|███████   | 1442/2048 [00:31<00:15, 39.24it/s, est. speed input: 46743.93 toks/s, output: 45.65 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [00:31<00:14, 39.94it/s, est. speed input: 46695.06 toks/s, output: 45.60 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [00:32<00:14, 39.70it/s, est. speed input: 46611.83 toks/s, output: 45.52 toks/s]
Processed prompts:  73%|███████▎  | 1490/2048 [00:32<00:14, 39.51it/s, est. speed input: 46529.55 toks/s, output: 45.44 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [00:33<00:13, 39.50it/s, est. speed input: 46454.81 toks/s, output: 45.37 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [00:33<00:13, 39.99it/s, est. speed input: 46405.10 toks/s, output: 45.32 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [00:33<00:12, 39.71it/s, est. speed input: 46328.25 toks/s, output: 45.24 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [00:34<00:12, 40.25it/s, est. speed input: 46285.99 toks/s, output: 45.20 toks/s]
Processed prompts:  77%|███████▋  | 1570/2048 [00:34<00:11, 39.89it/s, est. speed input: 46212.04 toks/s, output: 45.13 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [00:35<00:11, 39.68it/s, est. speed input: 46141.63 toks/s, output: 45.06 toks/s]
Processed prompts:  78%|███████▊  | 1602/2048 [00:35<00:11, 39.46it/s, est. speed input: 46069.85 toks/s, output: 44.99 toks/s]
Processed prompts:  79%|███████▉  | 1618/2048 [00:35<00:10, 40.06it/s, est. speed input: 46031.65 toks/s, output: 44.95 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [00:36<00:10, 39.81it/s, est. speed input: 45966.37 toks/s, output: 44.89 toks/s]
Processed prompts:  81%|████████  | 1650/2048 [00:36<00:10, 39.55it/s, est. speed input: 45898.65 toks/s, output: 44.82 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [00:37<00:09, 39.44it/s, est. speed input: 45835.24 toks/s, output: 44.76 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [00:37<00:09, 39.34it/s, est. speed input: 45772.41 toks/s, output: 44.70 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [00:38<00:08, 39.30it/s, est. speed input: 45711.96 toks/s, output: 44.64 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [00:38<00:08, 39.22it/s, est. speed input: 45650.69 toks/s, output: 44.58 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [00:38<00:07, 39.87it/s, est. speed input: 45619.18 toks/s, output: 44.55 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [00:39<00:07, 40.37it/s, est. speed input: 45589.25 toks/s, output: 44.52 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [00:39<00:07, 39.91it/s, est. speed input: 45529.22 toks/s, output: 44.46 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [00:40<00:06, 39.68it/s, est. speed input: 45473.97 toks/s, output: 44.41 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [00:40<00:06, 39.48it/s, est. speed input: 45418.11 toks/s, output: 44.35 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [00:40<00:06, 39.40it/s, est. speed input: 45365.38 toks/s, output: 44.30 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [00:41<00:05, 39.30it/s, est. speed input: 45312.08 toks/s, output: 44.25 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [00:41<00:05, 39.22it/s, est. speed input: 45259.72 toks/s, output: 44.20 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [00:42<00:04, 39.18it/s, est. speed input: 45208.77 toks/s, output: 44.15 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [00:42<00:04, 39.14it/s, est. speed input: 45158.41 toks/s, output: 44.10 toks/s]
Processed prompts:  92%|█████████▏| 1890/2048 [00:42<00:03, 39.79it/s, est. speed input: 45133.48 toks/s, output: 44.08 toks/s]
Processed prompts:  93%|█████████▎| 1906/2048 [00:43<00:03, 39.58it/s, est. speed input: 45085.13 toks/s, output: 44.03 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [00:43<00:03, 39.45it/s, est. speed input: 45038.44 toks/s, output: 43.98 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [00:44<00:02, 39.33it/s, est. speed input: 44991.45 toks/s, output: 43.94 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [00:44<00:02, 39.24it/s, est. speed input: 44945.40 toks/s, output: 43.89 toks/s]
Processed prompts:  96%|█████████▌| 1970/2048 [00:44<00:01, 39.21it/s, est. speed input: 44901.07 toks/s, output: 43.85 toks/s]
Processed prompts:  97%|█████████▋| 1986/2048 [00:45<00:01, 39.82it/s, est. speed input: 44879.02 toks/s, output: 43.83 toks/s]
Processed prompts:  98%|█████████▊| 2002/2048 [00:45<00:01, 39.60it/s, est. speed input: 44835.76 toks/s, output: 43.78 toks/s]
Processed prompts:  99%|█████████▊| 2018/2048 [00:46<00:00, 39.44it/s, est. speed input: 44792.65 toks/s, output: 43.74 toks/s]
Processed prompts:  99%|█████████▉| 2034/2048 [00:46<00:00, 40.13it/s, est. speed input: 44776.85 toks/s, output: 43.73 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:46<00:00, 40.13it/s, est. speed input: 45084.41 toks/s, output: 44.03 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:46<00:00, 44.03it/s, est. speed input: 45084.41 toks/s, output: 44.03 toks/s]
[rank0]:[W126 12:18:55.110988548 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 102.5s

测试结果:
  Requests/s:   39.49
  Tokens/s:     40476.94
  Total Reqs:   2048
  Elapsed:      51.86s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     40437.45

============================================================
[7/8] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 12:19:25 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1348868) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1348868) WARNING 01-26 12:19:43 [backends.py:609] Failed to read file <frozen os>
Throughput: 39.01 requests/s, 39989.66 total tokens/s, 39.01 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-26 12:19:24] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 12:19:25] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:19:25] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 12:19:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:19:25] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:19:25] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:19:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:19:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:19:25] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:19:25] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 12:19:25] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 12:19:25] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 12:19:25] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 12:19:25] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 12:19:31] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 12:19:32] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:19:32] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 12:19:32] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:19:32] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:19:32] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:19:32] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:19:32] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:19:32] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:19:32] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 12:19:32] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 12:19:32] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 12:19:32] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 12:19:32] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1348868) [2026-01-26 12:19:34] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1348868) [2026-01-26 12:19:34] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1348868) [2026-01-26 12:19:34] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1348868) [2026-01-26 12:19:34] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=1348868) [2026-01-26 12:19:34] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=1348868) [2026-01-26 12:19:34] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=1348868) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1348868) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.94it/s]
(EngineCore_DP0 pid=1348868) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.41it/s]
(EngineCore_DP0 pid=1348868) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.47it/s]
(EngineCore_DP0 pid=1348868) 
(EngineCore_DP0 pid=1348868) [2026-01-26 12:19:37] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5376] -> 1D uint8
(EngineCore_DP0 pid=1348868) [2026-01-26 12:19:37] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15482880 bytes
(EngineCore_DP0 pid=1348868) [2026-01-26 12:19:37] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5376] -> 1D uint8
(EngineCore_DP0 pid=1348868) [2026-01-26 12:19:37] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 12042240 bytes
(EngineCore_DP0 pid=1348868) [2026-01-26 12:19:37] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5376] -> 1D uint8
(EngineCore_DP0 pid=1348868) [2026-01-26 12:19:37] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 127303680 bytes
(EngineCore_DP0 pid=1348868) [2026-01-26 12:19:37] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 28416] -> 1D uint8
(EngineCore_DP0 pid=1348868) [2026-01-26 12:19:37] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 63651840 bytes
(EngineCore_DP0 pid=1348868) [rank0]:W0126 12:19:48.628000 1348868 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1348868) [rank0]:W0126 12:19:48.706000 1348868 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1348868) [rank0]:W0126 12:19:49.926000 1348868 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1348868) [rank0]:W0126 12:19:50.054000 1348868 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1348868) 2026-01-26 12:19:53,719 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1348868) 2026-01-26 12:19:53,746 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1348868) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▉         | 1/11 [00:00<00:08,  1.12it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:01<00:04,  2.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:01<00:01,  4.84it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 6/11 [00:01<00:00,  7.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:01<00:00,  9.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 10/11 [00:01<00:00, 12.18it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00,  7.16it/s]
(EngineCore_DP0 pid=1348868) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00, 19.11it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 5/7 [00:00<00:00, 10.89it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:01<00:00,  5.12it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:01<00:00,  6.11it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 25/4096 [00:00<00:16, 249.47it/s]
Adding requests:   2%|▏         | 65/4096 [00:00<00:12, 335.79it/s]
Adding requests:   2%|▏         | 101/4096 [00:00<00:11, 344.77it/s]
Adding requests:   3%|▎         | 139/4096 [00:00<00:11, 357.39it/s]
Adding requests:   4%|▍         | 178/4096 [00:00<00:10, 367.73it/s]
Adding requests:   5%|▌         | 217/4096 [00:00<00:10, 373.70it/s]
Adding requests:   6%|▌         | 255/4096 [00:00<00:10, 375.14it/s]
Adding requests:   7%|▋         | 295/4096 [00:00<00:10, 380.02it/s]
Adding requests:   8%|▊         | 336/4096 [00:00<00:09, 389.14it/s]
Adding requests:   9%|▉         | 376/4096 [00:01<00:09, 391.98it/s]
Adding requests:  10%|█         | 418/4096 [00:01<00:09, 398.94it/s]
Adding requests:  11%|█         | 458/4096 [00:01<00:09, 394.40it/s]
Adding requests:  12%|█▏        | 500/4096 [00:01<00:08, 401.84it/s]
Adding requests:  13%|█▎        | 542/4096 [00:01<00:08, 406.97it/s]
Adding requests:  14%|█▍        | 583/4096 [00:01<00:08, 401.73it/s]
Adding requests:  15%|█▌        | 624/4096 [00:01<00:08, 394.61it/s]
Adding requests:  16%|█▌        | 664/4096 [00:01<00:08, 388.32it/s]
Adding requests:  17%|█▋        | 704/4096 [00:01<00:08, 390.25it/s]
Adding requests:  18%|█▊        | 744/4096 [00:01<00:08, 383.99it/s]
Adding requests:  19%|█▉        | 784/4096 [00:02<00:08, 386.86it/s]
Adding requests:  20%|██        | 823/4096 [00:02<00:08, 385.79it/s]
Adding requests:  21%|██        | 864/4096 [00:02<00:08, 390.03it/s]
Adding requests:  22%|██▏       | 905/4096 [00:02<00:08, 395.84it/s]
Adding requests:  23%|██▎       | 945/4096 [00:02<00:08, 385.62it/s]
Adding requests:  24%|██▍       | 984/4096 [00:02<00:08, 386.78it/s]
Adding requests:  25%|██▍       | 1023/4096 [00:02<00:08, 382.38it/s]
Adding requests:  26%|██▌       | 1062/4096 [00:02<00:07, 380.32it/s]
Adding requests:  27%|██▋       | 1101/4096 [00:02<00:08, 373.92it/s]
Adding requests:  28%|██▊       | 1141/4096 [00:02<00:07, 379.34it/s]
Adding requests:  29%|██▉       | 1179/4096 [00:03<00:07, 378.68it/s]
Adding requests:  30%|██▉       | 1220/4096 [00:03<00:07, 384.94it/s]
Adding requests:  31%|███       | 1259/4096 [00:03<00:07, 382.94it/s]
Adding requests:  32%|███▏      | 1298/4096 [00:03<00:07, 380.48it/s]
Adding requests:  33%|███▎      | 1337/4096 [00:03<00:07, 382.00it/s]
Adding requests:  34%|███▎      | 1378/4096 [00:03<00:07, 387.39it/s]
Adding requests:  35%|███▍      | 1417/4096 [00:03<00:06, 382.80it/s]
Adding requests:  36%|███▌      | 1457/4096 [00:03<00:06, 386.90it/s]
Adding requests:  37%|███▋      | 1497/4096 [00:03<00:06, 390.38it/s]
Adding requests:  38%|███▊      | 1537/4096 [00:04<00:06, 387.83it/s]
Adding requests:  38%|███▊      | 1576/4096 [00:04<00:06, 383.22it/s]
Adding requests:  39%|███▉      | 1615/4096 [00:04<00:06, 380.91it/s]
Adding requests:  40%|████      | 1654/4096 [00:04<00:06, 372.71it/s]
Adding requests:  41%|████▏     | 1692/4096 [00:04<00:06, 373.92it/s]
Adding requests:  42%|████▏     | 1732/4096 [00:04<00:06, 378.59it/s]
Adding requests:  43%|████▎     | 1773/4096 [00:04<00:06, 385.40it/s]
Adding requests:  44%|████▍     | 1812/4096 [00:04<00:05, 383.07it/s]
Adding requests:  45%|████▌     | 1851/4096 [00:04<00:05, 383.09it/s]
Adding requests:  46%|████▌     | 1891/4096 [00:04<00:05, 387.71it/s]
Adding requests:  47%|████▋     | 1931/4096 [00:05<00:05, 390.83it/s]
Adding requests:  48%|████▊     | 1971/4096 [00:05<00:05, 391.76it/s]
Adding requests:  49%|████▉     | 2011/4096 [00:05<00:05, 387.81it/s]
Adding requests:  50%|█████     | 2050/4096 [00:05<00:05, 381.42it/s]
Adding requests:  51%|█████     | 2089/4096 [00:05<00:05, 370.30it/s]
Adding requests:  52%|█████▏    | 2129/4096 [00:05<00:05, 376.67it/s]
Adding requests:  53%|█████▎    | 2167/4096 [00:05<00:05, 374.31it/s]
Adding requests:  54%|█████▍    | 2205/4096 [00:05<00:05, 369.87it/s]
Adding requests:  55%|█████▍    | 2244/4096 [00:05<00:04, 375.17it/s]
Adding requests:  56%|█████▌    | 2285/4096 [00:05<00:04, 382.41it/s]
Adding requests:  57%|█████▋    | 2324/4096 [00:06<00:04, 379.34it/s]
Adding requests:  58%|█████▊    | 2364/4096 [00:06<00:04, 383.12it/s]
Adding requests:  59%|█████▊    | 2405/4096 [00:06<00:04, 388.78it/s]
Adding requests:  60%|█████▉    | 2445/4096 [00:06<00:04, 390.18it/s]
Adding requests:  61%|██████    | 2485/4096 [00:06<00:04, 388.81it/s]
Adding requests:  62%|██████▏   | 2526/4096 [00:06<00:03, 393.19it/s]
Adding requests:  63%|██████▎   | 2570/4096 [00:06<00:03, 404.61it/s]
Adding requests:  64%|██████▎   | 2611/4096 [00:06<00:03, 401.08it/s]
Adding requests:  65%|██████▍   | 2652/4096 [00:06<00:03, 391.23it/s]
Adding requests:  66%|██████▌   | 2692/4096 [00:07<00:03, 388.85it/s]
Adding requests:  67%|██████▋   | 2731/4096 [00:07<00:03, 386.07it/s]
Adding requests:  68%|██████▊   | 2772/4096 [00:07<00:03, 392.06it/s]
Adding requests:  69%|██████▊   | 2813/4096 [00:07<00:03, 395.41it/s]
Adding requests:  70%|██████▉   | 2853/4096 [00:07<00:03, 395.68it/s]
Adding requests:  71%|███████   | 2893/4096 [00:07<00:03, 394.07it/s]
Adding requests:  72%|███████▏  | 2933/4096 [00:07<00:02, 394.72it/s]
Adding requests:  73%|███████▎  | 2973/4096 [00:07<00:02, 395.69it/s]
Adding requests:  74%|███████▎  | 3013/4096 [00:07<00:02, 395.09it/s]
Adding requests:  75%|███████▍  | 3054/4096 [00:07<00:02, 399.24it/s]
Adding requests:  76%|███████▌  | 3094/4096 [00:08<00:02, 397.96it/s]
Adding requests:  77%|███████▋  | 3135/4096 [00:08<00:02, 400.74it/s]
Adding requests:  78%|███████▊  | 3176/4096 [00:08<00:02, 392.64it/s]
Adding requests:  79%|███████▊  | 3216/4096 [00:08<00:02, 389.91it/s]
Adding requests:  80%|███████▉  | 3257/4096 [00:08<00:02, 394.95it/s]
Adding requests:  80%|████████  | 3297/4096 [00:08<00:02, 381.16it/s]
Adding requests:  81%|████████▏ | 3336/4096 [00:08<00:02, 379.33it/s]
Adding requests:  82%|████████▏ | 3377/4096 [00:08<00:01, 386.95it/s]
Adding requests:  83%|████████▎ | 3417/4096 [00:08<00:01, 387.72it/s]
Adding requests:  84%|████████▍ | 3456/4096 [00:08<00:01, 387.65it/s]
Adding requests:  85%|████████▌ | 3495/4096 [00:09<00:01, 385.02it/s]
Adding requests:  86%|████████▋ | 3538/4096 [00:09<00:01, 397.67it/s]
Adding requests:  87%|████████▋ | 3578/4096 [00:09<00:01, 395.35it/s]
Adding requests:  88%|████████▊ | 3618/4096 [00:09<00:01, 396.45it/s]
Adding requests:  89%|████████▉ | 3658/4096 [00:09<00:01, 394.76it/s]
Adding requests:  90%|█████████ | 3698/4096 [00:09<00:01, 377.66it/s]
Adding requests:  91%|█████████▏| 3738/4096 [00:09<00:00, 382.90it/s]
Adding requests:  92%|█████████▏| 3777/4096 [00:09<00:00, 373.95it/s]
Adding requests:  93%|█████████▎| 3815/4096 [00:09<00:00, 364.11it/s]
Adding requests:  94%|█████████▍| 3854/4096 [00:10<00:00, 370.24it/s]
Adding requests:  95%|█████████▌| 3892/4096 [00:10<00:00, 370.99it/s]
Adding requests:  96%|█████████▌| 3930/4096 [00:10<00:00, 366.43it/s]
Adding requests:  97%|█████████▋| 3969/4096 [00:10<00:00, 370.60it/s]
Adding requests:  98%|█████████▊| 4007/4096 [00:10<00:00, 371.28it/s]
Adding requests:  99%|█████████▉| 4045/4096 [00:10<00:00, 370.37it/s]
Adding requests: 100%|█████████▉| 4083/4096 [00:10<00:00, 371.33it/s]
Adding requests: 100%|██████████| 4096/4096 [00:10<00:00, 384.12it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  10%|▉         | 405/4096 [00:00<00:07, 525.49it/s, est. speed input: 538157.67 toks/s, output: 525.50 toks/s]
Processed prompts:  11%|█         | 458/4096 [00:01<00:14, 248.71it/s, est. speed input: 296047.28 toks/s, output: 289.11 toks/s]
Processed prompts:  12%|█▏        | 484/4096 [00:02<00:24, 148.59it/s, est. speed input: 206734.27 toks/s, output: 201.89 toks/s]
Processed prompts:  12%|█▏        | 501/4096 [00:03<00:36, 97.88it/s, est. speed input: 159855.69 toks/s, output: 156.11 toks/s] 
Processed prompts:  13%|█▎        | 533/4096 [00:04<00:46, 76.73it/s, est. speed input: 135743.82 toks/s, output: 132.56 toks/s]
Processed prompts:  14%|█▍        | 565/4096 [00:04<00:55, 64.02it/s, est. speed input: 119729.95 toks/s, output: 116.92 toks/s]
Processed prompts:  15%|█▍        | 597/4096 [00:05<01:02, 55.97it/s, est. speed input: 108319.75 toks/s, output: 105.78 toks/s]
Processed prompts:  15%|█▌        | 629/4096 [00:06<01:08, 50.62it/s, est. speed input: 99713.43 toks/s, output: 97.38 toks/s]  
Processed prompts:  16%|█▌        | 661/4096 [00:07<01:12, 47.09it/s, est. speed input: 93066.60 toks/s, output: 90.88 toks/s]
Processed prompts:  17%|█▋        | 693/4096 [00:08<01:16, 44.72it/s, est. speed input: 87773.47 toks/s, output: 85.72 toks/s]
Processed prompts:  18%|█▊        | 725/4096 [00:08<01:18, 43.05it/s, est. speed input: 83419.65 toks/s, output: 81.46 toks/s]
Processed prompts:  18%|█▊        | 757/4096 [00:09<01:19, 42.23it/s, est. speed input: 79967.16 toks/s, output: 78.09 toks/s]
Processed prompts:  19%|█▉        | 789/4096 [00:10<01:19, 41.34it/s, est. speed input: 76890.96 toks/s, output: 75.09 toks/s]
Processed prompts:  20%|██        | 821/4096 [00:11<01:20, 40.75it/s, est. speed input: 74266.09 toks/s, output: 72.53 toks/s]
Processed prompts:  21%|██        | 853/4096 [00:12<01:20, 40.27it/s, est. speed input: 71967.34 toks/s, output: 70.28 toks/s]
Processed prompts:  22%|██▏       | 885/4096 [00:12<01:20, 39.97it/s, est. speed input: 69971.72 toks/s, output: 68.33 toks/s]
Processed prompts:  22%|██▏       | 917/4096 [00:13<01:19, 39.75it/s, est. speed input: 68204.74 toks/s, output: 66.61 toks/s]
Processed prompts:  23%|██▎       | 949/4096 [00:14<01:19, 39.58it/s, est. speed input: 66633.38 toks/s, output: 65.07 toks/s]
Processed prompts:  24%|██▍       | 981/4096 [00:15<01:18, 39.49it/s, est. speed input: 65236.93 toks/s, output: 63.71 toks/s]
Processed prompts:  25%|██▍       | 1013/4096 [00:16<01:18, 39.38it/s, est. speed input: 63967.70 toks/s, output: 62.47 toks/s]
Processed prompts:  26%|██▌       | 1045/4096 [00:17<01:17, 39.33it/s, est. speed input: 62825.22 toks/s, output: 61.35 toks/s]
Processed prompts:  26%|██▋       | 1077/4096 [00:17<01:16, 39.27it/s, est. speed input: 61783.92 toks/s, output: 60.34 toks/s]
Processed prompts:  27%|██▋       | 1109/4096 [00:18<01:16, 39.25it/s, est. speed input: 60836.47 toks/s, output: 59.41 toks/s]
Processed prompts:  28%|██▊       | 1141/4096 [00:19<01:15, 39.23it/s, est. speed input: 59967.97 toks/s, output: 58.56 toks/s]
Processed prompts:  29%|██▊       | 1173/4096 [00:20<01:14, 39.17it/s, est. speed input: 59160.01 toks/s, output: 57.77 toks/s]
Processed prompts:  29%|██▉       | 1205/4096 [00:21<01:13, 39.46it/s, est. speed input: 58478.50 toks/s, output: 57.11 toks/s]
Processed prompts:  30%|███       | 1237/4096 [00:21<01:12, 39.64it/s, est. speed input: 57842.42 toks/s, output: 56.49 toks/s]
Processed prompts:  31%|███       | 1269/4096 [00:22<01:11, 39.44it/s, est. speed input: 57194.11 toks/s, output: 55.85 toks/s]
Processed prompts:  32%|███▏      | 1301/4096 [00:23<01:11, 39.33it/s, est. speed input: 56596.44 toks/s, output: 55.27 toks/s]
Processed prompts:  33%|███▎      | 1333/4096 [00:24<01:09, 39.55it/s, est. speed input: 56085.37 toks/s, output: 54.77 toks/s]
Processed prompts:  33%|███▎      | 1365/4096 [00:25<01:09, 39.43it/s, est. speed input: 55566.53 toks/s, output: 54.26 toks/s]
Processed prompts:  34%|███▍      | 1397/4096 [00:25<01:08, 39.33it/s, est. speed input: 55076.60 toks/s, output: 53.79 toks/s]
Processed prompts:  35%|███▍      | 1429/4096 [00:26<01:07, 39.58it/s, est. speed input: 54662.08 toks/s, output: 53.38 toks/s]
Processed prompts:  36%|███▌      | 1461/4096 [00:27<01:06, 39.42it/s, est. speed input: 54226.10 toks/s, output: 52.96 toks/s]
Processed prompts:  36%|███▋      | 1493/4096 [00:28<01:06, 39.29it/s, est. speed input: 53813.47 toks/s, output: 52.55 toks/s]
Processed prompts:  37%|███▋      | 1525/4096 [00:29<01:05, 39.54it/s, est. speed input: 53466.32 toks/s, output: 52.21 toks/s]
Processed prompts:  38%|███▊      | 1557/4096 [00:30<01:03, 39.72it/s, est. speed input: 53138.20 toks/s, output: 51.89 toks/s]
Processed prompts:  39%|███▉      | 1589/4096 [00:30<01:03, 39.48it/s, est. speed input: 52783.94 toks/s, output: 51.55 toks/s]
Processed prompts:  40%|███▉      | 1621/4096 [00:31<01:02, 39.69it/s, est. speed input: 52490.91 toks/s, output: 51.26 toks/s]
Processed prompts:  40%|████      | 1653/4096 [00:32<01:01, 39.45it/s, est. speed input: 52169.96 toks/s, output: 50.95 toks/s]
Processed prompts:  41%|████      | 1685/4096 [00:33<01:01, 39.33it/s, est. speed input: 51869.66 toks/s, output: 50.65 toks/s]
Processed prompts:  42%|████▏     | 1717/4096 [00:34<01:00, 39.55it/s, est. speed input: 51616.02 toks/s, output: 50.41 toks/s]
Processed prompts:  43%|████▎     | 1749/4096 [00:34<00:59, 39.67it/s, est. speed input: 51370.05 toks/s, output: 50.17 toks/s]
Processed prompts:  43%|████▎     | 1781/4096 [00:35<00:58, 39.44it/s, est. speed input: 51104.19 toks/s, output: 49.91 toks/s]
Processed prompts:  44%|████▍     | 1813/4096 [00:36<00:58, 39.30it/s, est. speed input: 50852.79 toks/s, output: 49.66 toks/s]
Processed prompts:  45%|████▌     | 1845/4096 [00:37<00:57, 39.17it/s, est. speed input: 50609.54 toks/s, output: 49.42 toks/s]
Processed prompts:  46%|████▌     | 1877/4096 [00:38<00:56, 39.44it/s, est. speed input: 50409.33 toks/s, output: 49.23 toks/s]
Processed prompts:  47%|████▋     | 1909/4096 [00:38<00:55, 39.32it/s, est. speed input: 50189.21 toks/s, output: 49.01 toks/s]
Processed prompts:  47%|████▋     | 1941/4096 [00:39<00:55, 39.16it/s, est. speed input: 49972.83 toks/s, output: 48.80 toks/s]
Processed prompts:  48%|████▊     | 1973/4096 [00:40<00:53, 39.42it/s, est. speed input: 49795.96 toks/s, output: 48.63 toks/s]
Processed prompts:  49%|████▉     | 2005/4096 [00:41<00:53, 39.28it/s, est. speed input: 49599.50 toks/s, output: 48.44 toks/s]
Processed prompts:  50%|████▉     | 2037/4096 [00:42<00:52, 39.12it/s, est. speed input: 49405.72 toks/s, output: 48.25 toks/s]
Processed prompts:  51%|█████     | 2069/4096 [00:43<00:51, 39.37it/s, est. speed input: 49247.74 toks/s, output: 48.09 toks/s]
Processed prompts:  51%|█████▏    | 2101/4096 [00:43<00:50, 39.19it/s, est. speed input: 49068.28 toks/s, output: 47.92 toks/s]
Processed prompts:  52%|█████▏    | 2133/4096 [00:44<00:50, 39.12it/s, est. speed input: 48899.21 toks/s, output: 47.75 toks/s]
Processed prompts:  53%|█████▎    | 2165/4096 [00:45<00:49, 39.35it/s, est. speed input: 48757.77 toks/s, output: 47.61 toks/s]
Processed prompts:  54%|█████▎    | 2197/4096 [00:46<00:48, 39.20it/s, est. speed input: 48598.16 toks/s, output: 47.46 toks/s]
Processed prompts:  54%|█████▍    | 2229/4096 [00:47<00:47, 39.08it/s, est. speed input: 48442.78 toks/s, output: 47.31 toks/s]
Processed prompts:  55%|█████▌    | 2261/4096 [00:47<00:47, 39.00it/s, est. speed input: 48293.54 toks/s, output: 47.16 toks/s]
Processed prompts:  56%|█████▌    | 2293/4096 [00:48<00:46, 38.93it/s, est. speed input: 48147.97 toks/s, output: 47.02 toks/s]
Processed prompts:  57%|█████▋    | 2325/4096 [00:49<00:45, 38.92it/s, est. speed input: 48009.94 toks/s, output: 46.88 toks/s]
Processed prompts:  58%|█████▊    | 2357/4096 [00:50<00:44, 38.89it/s, est. speed input: 47874.76 toks/s, output: 46.75 toks/s]
Processed prompts:  58%|█████▊    | 2389/4096 [00:51<00:43, 38.89it/s, est. speed input: 47745.79 toks/s, output: 46.63 toks/s]
Processed prompts:  59%|█████▉    | 2421/4096 [00:52<00:43, 38.86it/s, est. speed input: 47618.20 toks/s, output: 46.50 toks/s]
Processed prompts:  60%|█████▉    | 2453/4096 [00:52<00:42, 38.78it/s, est. speed input: 47491.46 toks/s, output: 46.38 toks/s]
Processed prompts:  61%|██████    | 2485/4096 [00:53<00:41, 38.77it/s, est. speed input: 47371.06 toks/s, output: 46.26 toks/s]
Processed prompts:  61%|██████▏   | 2517/4096 [00:54<00:40, 39.09it/s, est. speed input: 47274.54 toks/s, output: 46.17 toks/s]
Processed prompts:  62%|██████▏   | 2549/4096 [00:55<00:39, 38.96it/s, est. speed input: 47159.65 toks/s, output: 46.05 toks/s]
Processed prompts:  63%|██████▎   | 2581/4096 [00:56<00:38, 39.23it/s, est. speed input: 47069.42 toks/s, output: 45.97 toks/s]
Processed prompts:  64%|██████▍   | 2613/4096 [00:56<00:37, 39.08it/s, est. speed input: 46961.80 toks/s, output: 45.86 toks/s]
Processed prompts:  65%|██████▍   | 2645/4096 [00:57<00:37, 38.97it/s, est. speed input: 46857.44 toks/s, output: 45.76 toks/s]
Processed prompts:  65%|██████▌   | 2677/4096 [00:58<00:36, 38.89it/s, est. speed input: 46755.56 toks/s, output: 45.66 toks/s]
Processed prompts:  66%|██████▌   | 2709/4096 [00:59<00:35, 38.83it/s, est. speed input: 46656.49 toks/s, output: 45.56 toks/s]
Processed prompts:  67%|██████▋   | 2741/4096 [01:00<00:34, 39.09it/s, est. speed input: 46576.33 toks/s, output: 45.48 toks/s]
Processed prompts:  68%|██████▊   | 2773/4096 [01:01<00:33, 38.97it/s, est. speed input: 46482.13 toks/s, output: 45.39 toks/s]
Processed prompts:  68%|██████▊   | 2805/4096 [01:01<00:33, 38.91it/s, est. speed input: 46391.81 toks/s, output: 45.30 toks/s]
Processed prompts:  69%|██████▉   | 2837/4096 [01:02<00:32, 38.86it/s, est. speed input: 46303.29 toks/s, output: 45.22 toks/s]
Processed prompts:  70%|███████   | 2869/4096 [01:03<00:31, 38.79it/s, est. speed input: 46215.18 toks/s, output: 45.13 toks/s]
Processed prompts:  71%|███████   | 2901/4096 [01:04<00:29, 39.88it/s, est. speed input: 46185.80 toks/s, output: 45.10 toks/s]
Processed prompts:  72%|███████▏  | 2933/4096 [01:05<00:29, 39.53it/s, est. speed input: 46103.33 toks/s, output: 45.02 toks/s]
Processed prompts:  72%|███████▏  | 2965/4096 [01:05<00:28, 39.27it/s, est. speed input: 46021.68 toks/s, output: 44.94 toks/s]
Processed prompts:  73%|███████▎  | 2997/4096 [01:06<00:28, 39.06it/s, est. speed input: 45940.94 toks/s, output: 44.86 toks/s]
Processed prompts:  74%|███████▍  | 3029/4096 [01:07<00:27, 38.94it/s, est. speed input: 45863.07 toks/s, output: 44.79 toks/s]
Processed prompts:  75%|███████▍  | 3061/4096 [01:08<00:26, 38.83it/s, est. speed input: 45786.22 toks/s, output: 44.71 toks/s]
Processed prompts:  76%|███████▌  | 3093/4096 [01:09<00:25, 38.77it/s, est. speed input: 45711.80 toks/s, output: 44.64 toks/s]
Processed prompts:  76%|███████▋  | 3125/4096 [01:10<00:25, 38.76it/s, est. speed input: 45640.32 toks/s, output: 44.57 toks/s]
Processed prompts:  77%|███████▋  | 3157/4096 [01:10<00:24, 38.70it/s, est. speed input: 45568.30 toks/s, output: 44.50 toks/s]
Processed prompts:  78%|███████▊  | 3189/4096 [01:11<00:23, 38.67it/s, est. speed input: 45498.44 toks/s, output: 44.43 toks/s]
Processed prompts:  79%|███████▊  | 3221/4096 [01:12<00:22, 38.66it/s, est. speed input: 45430.87 toks/s, output: 44.37 toks/s]
Processed prompts:  79%|███████▉  | 3253/4096 [01:13<00:21, 38.63it/s, est. speed input: 45363.85 toks/s, output: 44.30 toks/s]
Processed prompts:  80%|████████  | 3285/4096 [01:14<00:21, 38.61it/s, est. speed input: 45298.26 toks/s, output: 44.24 toks/s]
Processed prompts:  81%|████████  | 3317/4096 [01:15<00:20, 38.60it/s, est. speed input: 45234.07 toks/s, output: 44.17 toks/s]
Processed prompts:  82%|████████▏ | 3349/4096 [01:15<00:19, 38.61it/s, est. speed input: 45172.08 toks/s, output: 44.11 toks/s]
Processed prompts:  83%|████████▎ | 3381/4096 [01:16<00:18, 38.59it/s, est. speed input: 45110.53 toks/s, output: 44.05 toks/s]
Processed prompts:  83%|████████▎ | 3413/4096 [01:17<00:17, 38.63it/s, est. speed input: 45052.31 toks/s, output: 44.00 toks/s]
Processed prompts:  84%|████████▍ | 3445/4096 [01:18<00:16, 38.63it/s, est. speed input: 44994.38 toks/s, output: 43.94 toks/s]
Processed prompts:  85%|████████▍ | 3477/4096 [01:19<00:16, 38.61it/s, est. speed input: 44936.78 toks/s, output: 43.88 toks/s]
Processed prompts:  86%|████████▌ | 3509/4096 [01:20<00:15, 38.61it/s, est. speed input: 44880.77 toks/s, output: 43.83 toks/s]
Processed prompts:  86%|████████▋ | 3541/4096 [01:20<00:14, 38.91it/s, est. speed input: 44837.90 toks/s, output: 43.79 toks/s]
Processed prompts:  87%|████████▋ | 3573/4096 [01:21<00:13, 38.81it/s, est. speed input: 44783.56 toks/s, output: 43.73 toks/s]
Processed prompts:  88%|████████▊ | 3605/4096 [01:22<00:12, 38.75it/s, est. speed input: 44730.88 toks/s, output: 43.68 toks/s]
Processed prompts:  89%|████████▉ | 3637/4096 [01:23<00:11, 38.73it/s, est. speed input: 44680.30 toks/s, output: 43.63 toks/s]
Processed prompts:  90%|████████▉ | 3669/4096 [01:24<00:10, 39.02it/s, est. speed input: 44641.94 toks/s, output: 43.60 toks/s]
Processed prompts:  90%|█████████ | 3701/4096 [01:24<00:10, 38.85it/s, est. speed input: 44590.46 toks/s, output: 43.55 toks/s]
Processed prompts:  91%|█████████ | 3733/4096 [01:25<00:09, 38.76it/s, est. speed input: 44540.78 toks/s, output: 43.50 toks/s]
Processed prompts:  92%|█████████▏| 3765/4096 [01:26<00:08, 38.69it/s, est. speed input: 44492.06 toks/s, output: 43.45 toks/s]
Processed prompts:  93%|█████████▎| 3797/4096 [01:27<00:07, 38.67it/s, est. speed input: 44445.17 toks/s, output: 43.40 toks/s]
Processed prompts:  93%|█████████▎| 3829/4096 [01:28<00:06, 38.63it/s, est. speed input: 44398.40 toks/s, output: 43.36 toks/s]
Processed prompts:  94%|█████████▍| 3861/4096 [01:29<00:06, 38.58it/s, est. speed input: 44351.46 toks/s, output: 43.31 toks/s]
Processed prompts:  95%|█████████▌| 3893/4096 [01:29<00:05, 38.57it/s, est. speed input: 44306.51 toks/s, output: 43.27 toks/s]
Processed prompts:  96%|█████████▌| 3925/4096 [01:30<00:04, 39.28it/s, est. speed input: 44286.93 toks/s, output: 43.25 toks/s]
Processed prompts:  97%|█████████▋| 3957/4096 [01:31<00:03, 39.09it/s, est. speed input: 44244.26 toks/s, output: 43.21 toks/s]
Processed prompts:  97%|█████████▋| 3989/4096 [01:32<00:02, 39.21it/s, est. speed input: 44211.01 toks/s, output: 43.17 toks/s]
Processed prompts:  98%|█████████▊| 4021/4096 [01:33<00:01, 38.98it/s, est. speed input: 44167.88 toks/s, output: 43.13 toks/s]
Processed prompts:  99%|█████████▉| 4053/4096 [01:34<00:01, 39.19it/s, est. speed input: 44137.52 toks/s, output: 43.10 toks/s]
Processed prompts: 100%|█████████▉| 4085/4096 [01:34<00:00, 48.60it/s, est. speed input: 44349.59 toks/s, output: 43.31 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [01:34<00:00, 48.60it/s, est. speed input: 44468.78 toks/s, output: 43.43 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [01:34<00:00, 43.43it/s, est. speed input: 44468.78 toks/s, output: 43.43 toks/s]
[rank0]:[W126 12:21:43.369900211 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 168.4s

测试结果:
  Requests/s:   39.01
  Tokens/s:     39989.66
  Total Reqs:   4096
  Elapsed:      104.99s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     39950.65

============================================================
[8/8] 测试 M=65536
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 65536
│   M_prefill     = 65536 (= 64 x 1024)
│   M_decode      = 64
│   batched_tokens = 65536 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 8192
│   --max-num-seqs           = 64
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 65536
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 12:22:35 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1351827) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1351827) WARNING 01-26 12:22:52 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]     self.model_runner.profile_run()
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]     hidden_states = self.model(
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]                     ^^^^^^^^^^^
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]     def forward(
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]     raise e
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]   File "<eval_with_key>.58", line 325, in forward
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]     return range_entry.runnable(*args)
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]     return self._compiled_fn(*args)
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]     return compiled_fn(full_args)
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]                             ^^^^^^^
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 724, in inner_fn
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]     outs = compiled_fn(args)
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]            ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]     return self.current_callable(inputs)
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]     out = model(new_inputs)
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]   File "/tmp/torchinductor_root/u6/cu6dytawyh3h74e6lffae3logvtiims3p4oi2vj4ej76rdwc6mjk.py", line 1093, in call
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]     buf17 = torch.ops.slidesparse.quant_slide_fp8.default(buf16, 'Qwen2.5-7B-FP8', 8)
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 521, in _quant_slide_fp8_impl
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]     return fn(input, L)
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]            ^^^^^^^^^^^^
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]   File "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/H100_cc90_py312_cu129_x86_64/quant_slide_tuned_Qwen2.5-7B.py", line 233, in quant_slide_fp8_triton
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]     _quant_slide_fp8_kernel[(M,)](
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 756, in run
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]     launch_metadata = kernel.launch_metadata(grid, stream, *bound_args.values())
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 490, in launch_metadata
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]     self._init_handles()
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 473, in _init_handles
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]     self.module, self.function, self.n_regs, self.n_spills, self.n_max_threads = driver.active.utils.load_binary(
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866]                                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1351827) ERROR 01-26 12:23:00 [core.py:866] RuntimeError: Triton Error [CUDA]: an illegal memory access was encountered


─── STDERR ───
[2026-01-26 12:22:34] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 12:22:35] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:22:35] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 12:22:35] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:22:35] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:22:35] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:22:35] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:22:35] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:22:35] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:22:35] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 12:22:35] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 12:22:35] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 12:22:35] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 12:22:35] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 12:22:42] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 12:22:42] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:22:42] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 12:22:42] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:22:42] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:22:42] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:22:42] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:22:42] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:22:42] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:22:42] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 12:22:42] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 12:22:42] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 12:22:42] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 12:22:42] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1351827) [2026-01-26 12:22:44] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1351827) [2026-01-26 12:22:44] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1351827) [2026-01-26 12:22:44] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1351827) [2026-01-26 12:22:44] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=1351827) [2026-01-26 12:22:44] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=1351827) [2026-01-26 12:22:44] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=1351827) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1351827) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.95it/s]
(EngineCore_DP0 pid=1351827) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.40it/s]
(EngineCore_DP0 pid=1351827) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.46it/s]
(EngineCore_DP0 pid=1351827) 
(EngineCore_DP0 pid=1351827) [2026-01-26 12:22:46] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5376] -> 1D uint8
(EngineCore_DP0 pid=1351827) [2026-01-26 12:22:46] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15482880 bytes
(EngineCore_DP0 pid=1351827) [2026-01-26 12:22:46] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5376] -> 1D uint8
(EngineCore_DP0 pid=1351827) [2026-01-26 12:22:46] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 12042240 bytes
(EngineCore_DP0 pid=1351827) [2026-01-26 12:22:46] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5376] -> 1D uint8
(EngineCore_DP0 pid=1351827) [2026-01-26 12:22:46] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 127303680 bytes
(EngineCore_DP0 pid=1351827) [2026-01-26 12:22:46] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 28416] -> 1D uint8
(EngineCore_DP0 pid=1351827) [2026-01-26 12:22:46] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 63651840 bytes
(EngineCore_DP0 pid=1351827) [rank0]:W0126 12:22:57.943000 1351827 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1351827) [rank0]:W0126 12:22:58.018000 1351827 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1351827) [rank0]:W0126 12:22:59.248000 1351827 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1351827) [rank0]:W0126 12:22:59.366000 1351827 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1351827) Process EngineCore_DP0:
(EngineCore_DP0 pid=1351827) Traceback (most recent call last):
(EngineCore_DP0 pid=1351827)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=1351827)     self.run()
(EngineCore_DP0 pid=1351827)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=1351827)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=1351827)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=1351827)     raise e
(EngineCore_DP0 pid=1351827)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1351827)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1351827)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1351827)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1351827)     super().__init__(
(EngineCore_DP0 pid=1351827)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=1351827)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=1351827)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1351827)   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=1351827)     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=1351827)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1351827)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=1351827)     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=1351827)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1351827)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=1351827)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=1351827)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1351827)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=1351827)     return func(*args, **kwargs)
(EngineCore_DP0 pid=1351827)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1351827)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=1351827)     return func(*args, **kwargs)
(EngineCore_DP0 pid=1351827)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1351827)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=1351827)     self.model_runner.profile_run()
(EngineCore_DP0 pid=1351827)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=1351827)     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=1351827)                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1351827)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=1351827)     return func(*args, **kwargs)
(EngineCore_DP0 pid=1351827)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1351827)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=1351827)     outputs = self.model(
(EngineCore_DP0 pid=1351827)               ^^^^^^^^^^^
(EngineCore_DP0 pid=1351827)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=1351827)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=1351827)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1351827)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=1351827)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=1351827)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1351827)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=1351827)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=1351827)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1351827)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=1351827)     hidden_states = self.model(
(EngineCore_DP0 pid=1351827)                     ^^^^^^^^^^^
(EngineCore_DP0 pid=1351827)   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=1351827)     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=1351827)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1351827)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=1351827)     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=1351827)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1351827)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=1351827)     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=1351827)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1351827)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
(EngineCore_DP0 pid=1351827)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=1351827)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1351827)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
(EngineCore_DP0 pid=1351827)     def forward(
(EngineCore_DP0 pid=1351827)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=1351827)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=1351827)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1351827)   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=1351827)     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=1351827)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1351827)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=1351827)     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=1351827)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1351827)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=1351827)     raise e
(EngineCore_DP0 pid=1351827)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=1351827)     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=1351827)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1351827)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=1351827)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=1351827)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1351827)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=1351827)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=1351827)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1351827)   File "<eval_with_key>.58", line 325, in forward
(EngineCore_DP0 pid=1351827)     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
(EngineCore_DP0 pid=1351827)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1351827)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=1351827)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=1351827)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1351827)   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=1351827)     return range_entry.runnable(*args)
(EngineCore_DP0 pid=1351827)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1351827)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=1351827)     return self._compiled_fn(*args)
(EngineCore_DP0 pid=1351827)            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1351827)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=1351827)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=1351827)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1351827)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=1351827)     return compiled_fn(full_args)
(EngineCore_DP0 pid=1351827)            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1351827)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=1351827)     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=1351827)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1351827)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=1351827)     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=1351827)                             ^^^^^^^
(EngineCore_DP0 pid=1351827)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 724, in inner_fn
(EngineCore_DP0 pid=1351827)     outs = compiled_fn(args)
(EngineCore_DP0 pid=1351827)            ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1351827)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=1351827)     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=1351827)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1351827)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=1351827)     return self.current_callable(inputs)
(EngineCore_DP0 pid=1351827)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1351827)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=1351827)     out = model(new_inputs)
(EngineCore_DP0 pid=1351827)           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1351827)   File "/tmp/torchinductor_root/u6/cu6dytawyh3h74e6lffae3logvtiims3p4oi2vj4ej76rdwc6mjk.py", line 1093, in call
(EngineCore_DP0 pid=1351827)     buf17 = torch.ops.slidesparse.quant_slide_fp8.default(buf16, 'Qwen2.5-7B-FP8', 8)
(EngineCore_DP0 pid=1351827)             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1351827)   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
(EngineCore_DP0 pid=1351827)     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=1351827)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1351827)   File "/root/vllmbench/slidesparse/core/kernels.py", line 521, in _quant_slide_fp8_impl
(EngineCore_DP0 pid=1351827)     return fn(input, L)
(EngineCore_DP0 pid=1351827)            ^^^^^^^^^^^^
(EngineCore_DP0 pid=1351827)   File "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/H100_cc90_py312_cu129_x86_64/quant_slide_tuned_Qwen2.5-7B.py", line 233, in quant_slide_fp8_triton
(EngineCore_DP0 pid=1351827)     _quant_slide_fp8_kernel[(M,)](
(EngineCore_DP0 pid=1351827)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=1351827)     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=1351827)                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1351827)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 756, in run
(EngineCore_DP0 pid=1351827)     launch_metadata = kernel.launch_metadata(grid, stream, *bound_args.values())
(EngineCore_DP0 pid=1351827)                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1351827)   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 490, in launch_metadata
(EngineCore_DP0 pid=1351827)     self._init_handles()
(EngineCore_DP0 pid=1351827)   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 473, in _init_handles
(EngineCore_DP0 pid=1351827)     self.module, self.function, self.n_regs, self.n_spills, self.n_max_threads = driver.active.utils.load_binary(
(EngineCore_DP0 pid=1351827)                                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1351827) RuntimeError: Triton Error [CUDA]: an illegal memory access was encountered
[rank0]:[W126 12:23:01.272064520 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[ERROR] 测试失败: M=65536 (exit code: 1)


------------------------------------------------------------
  生成 CSV: Qwen2.5-7B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/Qwen2.5-7B-FP8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,36.2384,18590.2796,3.5322
1024,1024,1,128,128,31.1199,31897.8650,4.1131
2048,1024,2,256,128,36.5239,37436.9720,7.0091
4096,1024,4,512,128,37.6842,38626.3382,13.5866
8192,1024,8,1024,128,38.0269,38977.5502,26.9283
16384,1024,16,2048,128,39.4897,40476.9440,51.8616
32768,1024,32,4096,128,39.0143,39989.6624,104.9871
65536,1024,64,8192,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[INFO] 完成: 7 成功, 1 失败

============================================================
  Qwen2.5-7B-FP8 | cuSPARSELt (2_10) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_10
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10

============================================================
[1/8] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 12:23:11 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1352824) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1352824) WARNING 01-26 12:23:28 [backends.py:609] Failed to read file <frozen os>
Throughput: 37.10 requests/s, 19032.15 total tokens/s, 37.10 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-26 12:23:10] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 12:23:11] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:23:11] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 12:23:11] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:23:11] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:23:11] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:23:11] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:23:11] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:23:11] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:23:11] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 12:23:11] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 12:23:11] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 12:23:11] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 12:23:11] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 12:23:17] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 12:23:18] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:23:18] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 12:23:18] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:23:18] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:23:18] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:23:18] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:23:18] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:23:18] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:23:18] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 12:23:18] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 12:23:18] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 12:23:18] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 12:23:18] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1352824) [2026-01-26 12:23:20] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1352824) [2026-01-26 12:23:20] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1352824) [2026-01-26 12:23:20] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1352824) [2026-01-26 12:23:20] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=1352824) [2026-01-26 12:23:20] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=1352824) [2026-01-26 12:23:20] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=1352824) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1352824) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.86it/s]
(EngineCore_DP0 pid=1352824) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.33it/s]
(EngineCore_DP0 pid=1352824) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.39it/s]
(EngineCore_DP0 pid=1352824) 
(EngineCore_DP0 pid=1352824) [2026-01-26 12:23:22] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=1352824) [2026-01-26 12:23:22] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16588800 bytes
(EngineCore_DP0 pid=1352824) [2026-01-26 12:23:22] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=1352824) [2026-01-26 12:23:22] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 12902400 bytes
(EngineCore_DP0 pid=1352824) [2026-01-26 12:23:22] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=1352824) [2026-01-26 12:23:22] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 136396800 bytes
(EngineCore_DP0 pid=1352824) [2026-01-26 12:23:22] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=1352824) [2026-01-26 12:23:22] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 67952640 bytes
(EngineCore_DP0 pid=1352824) 2026-01-26 12:23:40,518 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1352824) 2026-01-26 12:23:40,565 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1352824) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  5.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  2.85it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.07it/s]
(EngineCore_DP0 pid=1352824) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  9.60it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  9.58it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  29%|██▉       | 37/128 [00:00<00:00, 364.48it/s]
Adding requests:  84%|████████▍ | 108/128 [00:00<00:00, 565.51it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 556.25it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:20,  6.19it/s, est. speed input: 3170.75 toks/s, output: 6.19 toks/s]
Processed prompts:   5%|▍         | 6/128 [00:00<00:05, 24.30it/s, est. speed input: 10856.88 toks/s, output: 21.20 toks/s]
Processed prompts:   9%|▊         | 11/128 [00:00<00:03, 31.55it/s, est. speed input: 13949.75 toks/s, output: 27.24 toks/s]
Processed prompts:  12%|█▎        | 16/128 [00:00<00:03, 35.33it/s, est. speed input: 15636.00 toks/s, output: 30.54 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:02, 37.47it/s, est. speed input: 16681.35 toks/s, output: 32.58 toks/s]
Processed prompts:  20%|██        | 26/128 [00:00<00:02, 38.83it/s, est. speed input: 17404.01 toks/s, output: 33.99 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:00<00:02, 39.69it/s, est. speed input: 17926.39 toks/s, output: 35.01 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:01<00:02, 40.29it/s, est. speed input: 18330.27 toks/s, output: 35.80 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:01<00:02, 40.70it/s, est. speed input: 18647.02 toks/s, output: 36.42 toks/s]
Processed prompts:  36%|███▌      | 46/128 [00:01<00:02, 40.95it/s, est. speed input: 18899.66 toks/s, output: 36.91 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:01<00:01, 41.01it/s, est. speed input: 19091.97 toks/s, output: 37.29 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:01<00:01, 41.15it/s, est. speed input: 19265.37 toks/s, output: 37.63 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:01<00:01, 41.31it/s, est. speed input: 19419.93 toks/s, output: 37.93 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:01<00:01, 41.39it/s, est. speed input: 19550.32 toks/s, output: 38.18 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:01<00:01, 41.44it/s, est. speed input: 19662.18 toks/s, output: 38.40 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:01<00:01, 41.51it/s, est. speed input: 19764.82 toks/s, output: 38.60 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:02<00:01, 41.53it/s, est. speed input: 19852.50 toks/s, output: 38.77 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:02<00:01, 41.57it/s, est. speed input: 19932.42 toks/s, output: 38.93 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:02<00:00, 41.55it/s, est. speed input: 20000.53 toks/s, output: 39.06 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:02<00:00, 41.55it/s, est. speed input: 20063.40 toks/s, output: 39.19 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:02<00:00, 41.60it/s, est. speed input: 20123.59 toks/s, output: 39.30 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:02<00:00, 41.65it/s, est. speed input: 20179.56 toks/s, output: 39.41 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:02<00:00, 41.68it/s, est. speed input: 20230.97 toks/s, output: 39.51 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:02<00:00, 41.57it/s, est. speed input: 20269.11 toks/s, output: 39.59 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:03<00:00, 41.58it/s, est. speed input: 20309.67 toks/s, output: 39.67 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:03<00:00, 41.61it/s, est. speed input: 20348.36 toks/s, output: 39.74 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 41.61it/s, est. speed input: 20363.99 toks/s, output: 39.77 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 39.77it/s, est. speed input: 20363.99 toks/s, output: 39.77 toks/s]
[rank0]:[W126 12:23:46.624739682 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 44.6s

测试结果:
  Requests/s:   37.10
  Tokens/s:     19032.15
  Total Reqs:   128
  Elapsed:      3.45s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     18995.05

============================================================
[2/8] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 12:23:55 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1353986) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1353986) WARNING 01-26 12:24:12 [backends.py:609] Failed to read file <frozen os>
Throughput: 30.52 requests/s, 31283.71 total tokens/s, 30.52 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-26 12:23:55] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 12:23:55] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:23:55] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 12:23:55] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:23:55] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:23:55] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:23:55] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:23:55] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:23:55] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:23:55] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 12:23:55] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 12:23:55] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 12:23:55] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 12:23:55] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 12:24:02] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 12:24:03] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:24:03] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 12:24:03] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:24:03] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:24:03] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:24:03] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:24:03] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:24:03] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:24:03] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 12:24:03] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 12:24:03] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 12:24:03] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 12:24:03] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1353986) [2026-01-26 12:24:04] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1353986) [2026-01-26 12:24:04] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1353986) [2026-01-26 12:24:04] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1353986) [2026-01-26 12:24:04] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=1353986) [2026-01-26 12:24:04] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=1353986) [2026-01-26 12:24:04] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=1353986) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1353986) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.86it/s]
(EngineCore_DP0 pid=1353986) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.33it/s]
(EngineCore_DP0 pid=1353986) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.39it/s]
(EngineCore_DP0 pid=1353986) 
(EngineCore_DP0 pid=1353986) [2026-01-26 12:24:06] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=1353986) [2026-01-26 12:24:06] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16588800 bytes
(EngineCore_DP0 pid=1353986) [2026-01-26 12:24:06] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=1353986) [2026-01-26 12:24:06] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 12902400 bytes
(EngineCore_DP0 pid=1353986) [2026-01-26 12:24:06] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=1353986) [2026-01-26 12:24:06] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 136396800 bytes
(EngineCore_DP0 pid=1353986) [2026-01-26 12:24:06] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=1353986) [2026-01-26 12:24:06] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 67952640 bytes
(EngineCore_DP0 pid=1353986) 2026-01-26 12:24:23,979 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1353986) 2026-01-26 12:24:24,022 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1353986) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 15.34it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 15.33it/s]
(EngineCore_DP0 pid=1353986) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 17.90it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  16%|█▌        | 20/128 [00:00<00:00, 195.71it/s]
Adding requests:  45%|████▌     | 58/128 [00:00<00:00, 301.06it/s]
Adding requests:  74%|███████▍  | 95/128 [00:00<00:00, 329.76it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 323.96it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   6%|▋         | 8/128 [00:00<00:01, 78.19it/s, est. speed input: 80076.95 toks/s, output: 78.19 toks/s]
Processed prompts:  12%|█▎        | 16/128 [00:00<00:02, 42.85it/s, est. speed input: 47073.55 toks/s, output: 45.97 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:00<00:02, 38.24it/s, est. speed input: 42365.34 toks/s, output: 41.37 toks/s]
Processed prompts:  21%|██        | 27/128 [00:00<00:02, 36.24it/s, est. speed input: 40336.06 toks/s, output: 39.39 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:00<00:02, 35.16it/s, est. speed input: 39256.62 toks/s, output: 38.34 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:00<00:02, 34.35it/s, est. speed input: 38449.76 toks/s, output: 37.55 toks/s]
Processed prompts:  30%|███       | 39/128 [00:01<00:02, 33.77it/s, est. speed input: 37827.67 toks/s, output: 36.94 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:01<00:02, 33.33it/s, est. speed input: 37326.59 toks/s, output: 36.45 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:01<00:02, 33.06it/s, est. speed input: 36935.68 toks/s, output: 36.07 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:01<00:02, 32.87it/s, est. speed input: 36613.28 toks/s, output: 35.75 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:01<00:02, 32.75it/s, est. speed input: 36344.40 toks/s, output: 35.49 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:01<00:02, 32.72it/s, est. speed input: 36131.73 toks/s, output: 35.28 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:01<00:01, 32.65it/s, est. speed input: 35935.64 toks/s, output: 35.09 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:01<00:01, 32.61it/s, est. speed input: 35766.41 toks/s, output: 34.93 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:02<00:01, 32.55it/s, est. speed input: 35609.93 toks/s, output: 34.77 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:02<00:01, 32.49it/s, est. speed input: 35467.83 toks/s, output: 34.64 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:02<00:01, 32.48it/s, est. speed input: 35348.13 toks/s, output: 34.52 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:02<00:01, 32.49it/s, est. speed input: 35243.97 toks/s, output: 34.42 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:02<00:01, 32.42it/s, est. speed input: 35134.75 toks/s, output: 34.31 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:02<00:01, 32.47it/s, est. speed input: 35054.06 toks/s, output: 34.23 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:02<00:01, 32.50it/s, est. speed input: 34979.30 toks/s, output: 34.16 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:02<00:00, 32.46it/s, est. speed input: 34900.26 toks/s, output: 34.08 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:03<00:00, 32.46it/s, est. speed input: 34833.64 toks/s, output: 34.02 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:03<00:00, 32.50it/s, est. speed input: 34775.81 toks/s, output: 33.96 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:03<00:00, 32.50it/s, est. speed input: 34719.34 toks/s, output: 33.91 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:03<00:00, 32.50it/s, est. speed input: 34667.58 toks/s, output: 33.85 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:03<00:00, 32.51it/s, est. speed input: 34620.58 toks/s, output: 33.81 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:03<00:00, 32.46it/s, est. speed input: 34569.24 toks/s, output: 33.76 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:03<00:00, 32.47it/s, est. speed input: 34527.11 toks/s, output: 33.72 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 32.47it/s, est. speed input: 34520.52 toks/s, output: 33.71 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 33.71it/s, est. speed input: 34520.52 toks/s, output: 33.71 toks/s]
[rank0]:[W126 12:24:30.208881521 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 43.8s

测试结果:
  Requests/s:   30.52
  Tokens/s:     31283.71
  Total Reqs:   128
  Elapsed:      4.19s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     31253.19

============================================================
[3/8] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 12:24:41 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1355116) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1355116) WARNING 01-26 12:24:58 [backends.py:609] Failed to read file <frozen os>
Throughput: 34.13 requests/s, 34979.82 total tokens/s, 34.13 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-26 12:24:40] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 12:24:41] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:24:41] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 12:24:41] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:24:41] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:24:41] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:24:41] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:24:41] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:24:41] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:24:41] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 12:24:41] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 12:24:41] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 12:24:41] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 12:24:41] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 12:24:48] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 12:24:48] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:24:48] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 12:24:48] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:24:48] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:24:48] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:24:48] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:24:48] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:24:48] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:24:48] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 12:24:48] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 12:24:48] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 12:24:48] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 12:24:48] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1355116) [2026-01-26 12:24:49] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1355116) [2026-01-26 12:24:49] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1355116) [2026-01-26 12:24:49] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1355116) [2026-01-26 12:24:49] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=1355116) [2026-01-26 12:24:49] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=1355116) [2026-01-26 12:24:49] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=1355116) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1355116) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.87it/s]
(EngineCore_DP0 pid=1355116) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.33it/s]
(EngineCore_DP0 pid=1355116) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.39it/s]
(EngineCore_DP0 pid=1355116) 
(EngineCore_DP0 pid=1355116) [2026-01-26 12:24:51] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=1355116) [2026-01-26 12:24:51] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16588800 bytes
(EngineCore_DP0 pid=1355116) [2026-01-26 12:24:51] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=1355116) [2026-01-26 12:24:51] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 12902400 bytes
(EngineCore_DP0 pid=1355116) [2026-01-26 12:24:51] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=1355116) [2026-01-26 12:24:51] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 136396800 bytes
(EngineCore_DP0 pid=1355116) [2026-01-26 12:24:51] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=1355116) [2026-01-26 12:24:51] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 67952640 bytes
(EngineCore_DP0 pid=1355116) 2026-01-26 12:25:09,133 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1355116) 2026-01-26 12:25:09,183 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1355116) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:00<00:00, 15.72it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 15.19it/s]
(EngineCore_DP0 pid=1355116) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 18.97it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 18.94it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   8%|▊         | 20/256 [00:00<00:01, 196.83it/s]
Adding requests:  24%|██▍       | 61/256 [00:00<00:00, 317.66it/s]
Adding requests:  38%|███▊      | 97/256 [00:00<00:00, 335.97it/s]
Adding requests:  53%|█████▎    | 135/256 [00:00<00:00, 352.10it/s]
Adding requests:  68%|██████▊   | 175/256 [00:00<00:00, 366.33it/s]
Adding requests:  84%|████████▍ | 215/256 [00:00<00:00, 377.62it/s]
Adding requests:  99%|█████████▉| 253/256 [00:00<00:00, 378.24it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 357.44it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   8%|▊         | 20/256 [00:00<00:01, 146.40it/s, est. speed input: 149938.55 toks/s, output: 146.41 toks/s]
Processed prompts:  14%|█▎        | 35/256 [00:00<00:03, 59.52it/s, est. speed input: 67850.22 toks/s, output: 66.26 toks/s]   
Processed prompts:  17%|█▋        | 44/256 [00:00<00:04, 47.00it/s, est. speed input: 55586.88 toks/s, output: 54.28 toks/s]
Processed prompts:  20%|█▉        | 50/256 [00:00<00:04, 43.34it/s, est. speed input: 51936.54 toks/s, output: 50.72 toks/s]
Processed prompts:  21%|██▏       | 55/256 [00:01<00:04, 43.43it/s, est. speed input: 51194.65 toks/s, output: 49.99 toks/s]
Processed prompts:  23%|██▎       | 60/256 [00:01<00:04, 39.31it/s, est. speed input: 48491.90 toks/s, output: 47.35 toks/s]
Processed prompts:  25%|██▌       | 65/256 [00:01<00:04, 40.45it/s, est. speed input: 48234.62 toks/s, output: 47.10 toks/s]
Processed prompts:  27%|██▋       | 70/256 [00:01<00:05, 36.96it/s, est. speed input: 46306.80 toks/s, output: 45.22 toks/s]
Processed prompts:  29%|██▉       | 74/256 [00:01<00:04, 36.55it/s, est. speed input: 45611.47 toks/s, output: 44.54 toks/s]
Processed prompts:  30%|███       | 78/256 [00:01<00:04, 36.25it/s, est. speed input: 45011.94 toks/s, output: 43.96 toks/s]
Processed prompts:  32%|███▏      | 82/256 [00:01<00:04, 35.84it/s, est. speed input: 44435.55 toks/s, output: 43.39 toks/s]
Processed prompts:  34%|███▎      | 86/256 [00:02<00:04, 35.51it/s, est. speed input: 43921.07 toks/s, output: 42.89 toks/s]
Processed prompts:  35%|███▌      | 90/256 [00:02<00:04, 35.34it/s, est. speed input: 43478.61 toks/s, output: 42.46 toks/s]
Processed prompts:  37%|███▋      | 94/256 [00:02<00:04, 35.44it/s, est. speed input: 43130.15 toks/s, output: 42.12 toks/s]
Processed prompts:  38%|███▊      | 98/256 [00:02<00:04, 35.46it/s, est. speed input: 42805.15 toks/s, output: 41.80 toks/s]
Processed prompts:  40%|███▉      | 102/256 [00:02<00:04, 35.47it/s, est. speed input: 42508.19 toks/s, output: 41.51 toks/s]
Processed prompts:  41%|████▏     | 106/256 [00:02<00:04, 35.56it/s, est. speed input: 42253.31 toks/s, output: 41.26 toks/s]
Processed prompts:  43%|████▎     | 110/256 [00:02<00:04, 35.47it/s, est. speed input: 41993.69 toks/s, output: 41.01 toks/s]
Processed prompts:  45%|████▍     | 114/256 [00:02<00:04, 35.39it/s, est. speed input: 41751.11 toks/s, output: 40.77 toks/s]
Processed prompts:  46%|████▌     | 118/256 [00:02<00:03, 35.33it/s, est. speed input: 41527.78 toks/s, output: 40.55 toks/s]
Processed prompts:  48%|████▊     | 122/256 [00:03<00:03, 35.20it/s, est. speed input: 41308.10 toks/s, output: 40.34 toks/s]
Processed prompts:  49%|████▉     | 126/256 [00:03<00:03, 35.08it/s, est. speed input: 41100.57 toks/s, output: 40.14 toks/s]
Processed prompts:  51%|█████     | 130/256 [00:03<00:03, 35.22it/s, est. speed input: 40938.91 toks/s, output: 39.98 toks/s]
Processed prompts:  52%|█████▏    | 134/256 [00:03<00:03, 35.26it/s, est. speed input: 40778.83 toks/s, output: 39.82 toks/s]
Processed prompts:  54%|█████▍    | 138/256 [00:03<00:03, 35.33it/s, est. speed input: 40635.47 toks/s, output: 39.68 toks/s]
Processed prompts:  55%|█████▌    | 142/256 [00:03<00:03, 35.50it/s, est. speed input: 40515.02 toks/s, output: 39.57 toks/s]
Processed prompts:  57%|█████▋    | 146/256 [00:03<00:03, 35.31it/s, est. speed input: 40367.00 toks/s, output: 39.42 toks/s]
Processed prompts:  59%|█████▊    | 150/256 [00:03<00:03, 35.19it/s, est. speed input: 40228.44 toks/s, output: 39.29 toks/s]
Processed prompts:  60%|██████    | 154/256 [00:03<00:02, 35.22it/s, est. speed input: 40110.13 toks/s, output: 39.17 toks/s]
Processed prompts:  62%|██████▏   | 158/256 [00:04<00:02, 35.19it/s, est. speed input: 39993.54 toks/s, output: 39.06 toks/s]
Processed prompts:  63%|██████▎   | 162/256 [00:04<00:02, 35.11it/s, est. speed input: 39877.06 toks/s, output: 38.94 toks/s]
Processed prompts:  65%|██████▍   | 166/256 [00:04<00:02, 35.20it/s, est. speed input: 39781.72 toks/s, output: 38.85 toks/s]
Processed prompts:  66%|██████▋   | 170/256 [00:04<00:02, 35.23it/s, est. speed input: 39687.92 toks/s, output: 38.76 toks/s]
Processed prompts:  68%|██████▊   | 174/256 [00:04<00:02, 35.27it/s, est. speed input: 39599.99 toks/s, output: 38.67 toks/s]
Processed prompts:  70%|██████▉   | 178/256 [00:04<00:02, 35.34it/s, est. speed input: 39521.34 toks/s, output: 38.59 toks/s]
Processed prompts:  71%|███████   | 182/256 [00:04<00:02, 35.27it/s, est. speed input: 39434.87 toks/s, output: 38.51 toks/s]
Processed prompts:  73%|███████▎  | 186/256 [00:04<00:01, 35.10it/s, est. speed input: 39342.26 toks/s, output: 38.42 toks/s]
Processed prompts:  74%|███████▍  | 190/256 [00:04<00:01, 35.17it/s, est. speed input: 39270.24 toks/s, output: 38.35 toks/s]
Processed prompts:  76%|███████▌  | 194/256 [00:05<00:01, 35.23it/s, est. speed input: 39201.90 toks/s, output: 38.28 toks/s]
Processed prompts:  77%|███████▋  | 198/256 [00:05<00:01, 35.15it/s, est. speed input: 39127.09 toks/s, output: 38.21 toks/s]
Processed prompts:  80%|███████▉  | 204/256 [00:05<00:01, 36.82it/s, est. speed input: 39172.13 toks/s, output: 38.25 toks/s]
Processed prompts:  81%|████████▏ | 208/256 [00:05<00:01, 36.49it/s, est. speed input: 39116.69 toks/s, output: 38.20 toks/s]
Processed prompts:  83%|████████▎ | 212/256 [00:05<00:01, 36.14it/s, est. speed input: 39055.38 toks/s, output: 38.14 toks/s]
Processed prompts:  84%|████████▍ | 216/256 [00:05<00:01, 35.93it/s, est. speed input: 38999.43 toks/s, output: 38.09 toks/s]
Processed prompts:  86%|████████▌ | 220/256 [00:05<00:01, 35.69it/s, est. speed input: 38940.08 toks/s, output: 38.03 toks/s]
Processed prompts:  88%|████████▊ | 224/256 [00:05<00:00, 35.50it/s, est. speed input: 38880.53 toks/s, output: 37.97 toks/s]
Processed prompts:  89%|████████▉ | 228/256 [00:06<00:00, 35.44it/s, est. speed input: 38829.22 toks/s, output: 37.92 toks/s]
Processed prompts:  91%|█████████ | 232/256 [00:06<00:00, 35.29it/s, est. speed input: 38772.43 toks/s, output: 37.86 toks/s]
Processed prompts:  92%|█████████▏| 236/256 [00:06<00:00, 35.20it/s, est. speed input: 38718.30 toks/s, output: 37.81 toks/s]
Processed prompts:  94%|█████████▍| 240/256 [00:06<00:00, 35.29it/s, est. speed input: 38676.12 toks/s, output: 37.77 toks/s]
Processed prompts:  95%|█████████▌| 244/256 [00:06<00:00, 35.28it/s, est. speed input: 38631.38 toks/s, output: 37.73 toks/s]
Processed prompts:  97%|█████████▋| 248/256 [00:06<00:00, 35.18it/s, est. speed input: 38581.51 toks/s, output: 37.68 toks/s]
Processed prompts:  98%|█████████▊| 252/256 [00:06<00:00, 35.20it/s, est. speed input: 38539.45 toks/s, output: 37.64 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:06<00:00, 35.20it/s, est. speed input: 38644.53 toks/s, output: 37.74 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:06<00:00, 37.74it/s, est. speed input: 38644.53 toks/s, output: 37.74 toks/s]
[rank0]:[W126 12:25:18.766370833 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 48.3s

测试结果:
  Requests/s:   34.13
  Tokens/s:     34979.82
  Total Reqs:   256
  Elapsed:      7.50s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     34945.70

============================================================
[4/8] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 12:25:30 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1356274) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1356274) WARNING 01-26 12:25:46 [backends.py:609] Failed to read file <frozen os>
Throughput: 35.66 requests/s, 36550.54 total tokens/s, 35.66 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-26 12:25:29] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 12:25:30] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:25:30] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 12:25:30] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:25:30] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:25:30] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:25:30] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:25:30] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:25:30] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:25:30] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 12:25:30] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 12:25:30] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 12:25:30] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 12:25:30] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 12:25:36] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 12:25:37] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:25:37] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 12:25:37] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:25:37] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:25:37] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:25:37] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:25:37] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:25:37] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:25:37] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 12:25:37] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 12:25:37] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 12:25:37] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 12:25:37] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1356274) [2026-01-26 12:25:38] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1356274) [2026-01-26 12:25:38] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1356274) [2026-01-26 12:25:38] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1356274) [2026-01-26 12:25:38] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=1356274) [2026-01-26 12:25:38] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=1356274) [2026-01-26 12:25:38] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=1356274) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1356274) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.82it/s]
(EngineCore_DP0 pid=1356274) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.32it/s]
(EngineCore_DP0 pid=1356274) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.38it/s]
(EngineCore_DP0 pid=1356274) 
(EngineCore_DP0 pid=1356274) [2026-01-26 12:25:40] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=1356274) [2026-01-26 12:25:40] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16588800 bytes
(EngineCore_DP0 pid=1356274) [2026-01-26 12:25:40] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=1356274) [2026-01-26 12:25:40] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 12902400 bytes
(EngineCore_DP0 pid=1356274) [2026-01-26 12:25:40] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=1356274) [2026-01-26 12:25:40] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 136396800 bytes
(EngineCore_DP0 pid=1356274) [2026-01-26 12:25:40] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=1356274) [2026-01-26 12:25:40] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 67952640 bytes
(EngineCore_DP0 pid=1356274) 2026-01-26 12:25:57,307 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1356274) 2026-01-26 12:25:57,331 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1356274) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 1/4 [00:00<00:01,  2.26it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 3/4 [00:00<00:00,  5.22it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00,  6.13it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00,  5.27it/s]
(EngineCore_DP0 pid=1356274) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00, 19.10it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 19.07it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   6%|▌         | 29/512 [00:00<00:01, 285.77it/s]
Adding requests:  13%|█▎        | 69/512 [00:00<00:01, 351.72it/s]
Adding requests:  21%|██        | 106/512 [00:00<00:01, 355.89it/s]
Adding requests:  28%|██▊       | 144/512 [00:00<00:01, 361.94it/s]
Adding requests:  36%|███▌      | 184/512 [00:00<00:00, 375.00it/s]
Adding requests:  44%|████▍     | 224/512 [00:00<00:00, 381.39it/s]
Adding requests:  51%|█████▏    | 263/512 [00:00<00:00, 380.15it/s]
Adding requests:  59%|█████▉    | 303/512 [00:00<00:00, 384.03it/s]
Adding requests:  67%|██████▋   | 343/512 [00:00<00:00, 388.44it/s]
Adding requests:  75%|███████▌  | 384/512 [00:01<00:00, 392.24it/s]
Adding requests:  83%|████████▎ | 425/512 [00:01<00:00, 397.51it/s]
Adding requests:  91%|█████████ | 465/512 [00:01<00:00, 393.71it/s]
Adding requests:  99%|█████████▉| 507/512 [00:01<00:00, 401.13it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 384.02it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   9%|▉         | 46/512 [00:00<00:01, 380.95it/s, est. speed input: 390167.81 toks/s, output: 380.97 toks/s]
Processed prompts:  17%|█▋        | 85/512 [00:01<00:06, 65.47it/s, est. speed input: 77457.81 toks/s, output: 75.64 toks/s]   
Processed prompts:  20%|██        | 103/512 [00:01<00:07, 51.55it/s, est. speed input: 62841.32 toks/s, output: 61.37 toks/s]
Processed prompts:  22%|██▏       | 115/512 [00:02<00:08, 47.14it/s, est. speed input: 58321.12 toks/s, output: 56.95 toks/s]
Processed prompts:  24%|██▍       | 124/512 [00:02<00:08, 45.84it/s, est. speed input: 56670.72 toks/s, output: 55.34 toks/s]
Processed prompts:  26%|██▌       | 131/512 [00:02<00:08, 42.73it/s, est. speed input: 54513.34 toks/s, output: 53.24 toks/s]
Processed prompts:  27%|██▋       | 137/512 [00:02<00:08, 44.29it/s, est. speed input: 54552.31 toks/s, output: 53.27 toks/s]
Processed prompts:  28%|██▊       | 143/512 [00:02<00:09, 39.21it/s, est. speed input: 52336.43 toks/s, output: 51.11 toks/s]
Processed prompts:  29%|██▉       | 148/512 [00:02<00:09, 40.08it/s, est. speed input: 52069.45 toks/s, output: 50.85 toks/s]
Processed prompts:  30%|██▉       | 153/512 [00:03<00:08, 40.89it/s, est. speed input: 51815.58 toks/s, output: 50.60 toks/s]
Processed prompts:  31%|███       | 158/512 [00:03<00:10, 34.38it/s, est. speed input: 49847.27 toks/s, output: 48.68 toks/s]
Processed prompts:  32%|███▏      | 162/512 [00:03<00:10, 34.73it/s, est. speed input: 49421.19 toks/s, output: 48.26 toks/s]
Processed prompts:  32%|███▏      | 166/512 [00:03<00:09, 35.05it/s, est. speed input: 49027.62 toks/s, output: 47.88 toks/s]
Processed prompts:  33%|███▎      | 170/512 [00:03<00:09, 35.17it/s, est. speed input: 48630.09 toks/s, output: 47.49 toks/s]
Processed prompts:  34%|███▍      | 174/512 [00:03<00:09, 35.28it/s, est. speed input: 48258.73 toks/s, output: 47.13 toks/s]
Processed prompts:  35%|███▍      | 178/512 [00:03<00:09, 35.35it/s, est. speed input: 47907.87 toks/s, output: 46.78 toks/s]
Processed prompts:  36%|███▌      | 182/512 [00:03<00:09, 35.36it/s, est. speed input: 47571.52 toks/s, output: 46.46 toks/s]
Processed prompts:  36%|███▋      | 186/512 [00:04<00:09, 35.48it/s, est. speed input: 47267.98 toks/s, output: 46.16 toks/s]
Processed prompts:  37%|███▋      | 190/512 [00:04<00:09, 35.58it/s, est. speed input: 46982.62 toks/s, output: 45.88 toks/s]
Processed prompts:  38%|███▊      | 194/512 [00:04<00:08, 35.85it/s, est. speed input: 46736.14 toks/s, output: 45.64 toks/s]
Processed prompts:  39%|███▊      | 198/512 [00:04<00:08, 35.86it/s, est. speed input: 46480.43 toks/s, output: 45.39 toks/s]
Processed prompts:  40%|████      | 206/512 [00:04<00:08, 37.37it/s, est. speed input: 46204.34 toks/s, output: 45.12 toks/s]
Processed prompts:  41%|████      | 210/512 [00:04<00:08, 36.86it/s, est. speed input: 45963.73 toks/s, output: 44.89 toks/s]
Processed prompts:  42%|████▏     | 214/512 [00:04<00:08, 36.54it/s, est. speed input: 45742.05 toks/s, output: 44.67 toks/s]
Processed prompts:  43%|████▎     | 218/512 [00:04<00:08, 36.24it/s, est. speed input: 45524.86 toks/s, output: 44.46 toks/s]
Processed prompts:  43%|████▎     | 222/512 [00:05<00:08, 35.98it/s, est. speed input: 45313.52 toks/s, output: 44.25 toks/s]
Processed prompts:  44%|████▍     | 226/512 [00:05<00:07, 35.91it/s, est. speed input: 45123.27 toks/s, output: 44.07 toks/s]
Processed prompts:  45%|████▍     | 230/512 [00:05<00:07, 35.93it/s, est. speed input: 44948.21 toks/s, output: 43.89 toks/s]
Processed prompts:  46%|████▌     | 234/512 [00:05<00:07, 36.00it/s, est. speed input: 44784.52 toks/s, output: 43.73 toks/s]
Processed prompts:  46%|████▋     | 238/512 [00:05<00:07, 35.98it/s, est. speed input: 44621.62 toks/s, output: 43.58 toks/s]
Processed prompts:  47%|████▋     | 242/512 [00:05<00:07, 35.96it/s, est. speed input: 44464.43 toks/s, output: 43.42 toks/s]
Processed prompts:  48%|████▊     | 246/512 [00:05<00:07, 35.82it/s, est. speed input: 44304.06 toks/s, output: 43.27 toks/s]
Processed prompts:  49%|████▉     | 250/512 [00:05<00:07, 35.72it/s, est. speed input: 44148.57 toks/s, output: 43.11 toks/s]
Processed prompts:  50%|████▉     | 254/512 [00:05<00:07, 35.72it/s, est. speed input: 44005.47 toks/s, output: 42.97 toks/s]
Processed prompts:  50%|█████     | 258/512 [00:06<00:07, 35.59it/s, est. speed input: 43857.43 toks/s, output: 42.83 toks/s]
Processed prompts:  51%|█████     | 262/512 [00:06<00:07, 35.57it/s, est. speed input: 43720.36 toks/s, output: 42.70 toks/s]
Processed prompts:  52%|█████▏    | 266/512 [00:06<00:06, 35.66it/s, est. speed input: 43595.46 toks/s, output: 42.57 toks/s]
Processed prompts:  53%|█████▎    | 270/512 [00:06<00:06, 35.71it/s, est. speed input: 43474.37 toks/s, output: 42.46 toks/s]
Processed prompts:  54%|█████▎    | 274/512 [00:06<00:06, 35.87it/s, est. speed input: 43365.78 toks/s, output: 42.35 toks/s]
Processed prompts:  54%|█████▍    | 278/512 [00:06<00:06, 35.79it/s, est. speed input: 43247.78 toks/s, output: 42.23 toks/s]
Processed prompts:  55%|█████▌    | 282/512 [00:06<00:06, 35.77it/s, est. speed input: 43136.34 toks/s, output: 42.13 toks/s]
Processed prompts:  56%|█████▌    | 286/512 [00:06<00:06, 35.69it/s, est. speed input: 43024.21 toks/s, output: 42.02 toks/s]
Processed prompts:  57%|█████▋    | 290/512 [00:06<00:06, 35.69it/s, est. speed input: 42919.44 toks/s, output: 41.91 toks/s]
Processed prompts:  57%|█████▋    | 294/512 [00:07<00:06, 35.72it/s, est. speed input: 42819.93 toks/s, output: 41.82 toks/s]
Processed prompts:  58%|█████▊    | 298/512 [00:07<00:06, 35.63it/s, est. speed input: 42715.95 toks/s, output: 41.71 toks/s]
Processed prompts:  59%|█████▉    | 302/512 [00:07<00:05, 35.56it/s, est. speed input: 42615.52 toks/s, output: 41.62 toks/s]
Processed prompts:  61%|██████    | 310/512 [00:07<00:05, 37.45it/s, est. speed input: 42568.72 toks/s, output: 41.57 toks/s]
Processed prompts:  61%|██████▏   | 314/512 [00:07<00:05, 36.99it/s, est. speed input: 42479.43 toks/s, output: 41.48 toks/s]
Processed prompts:  62%|██████▏   | 318/512 [00:07<00:05, 36.56it/s, est. speed input: 42387.39 toks/s, output: 41.39 toks/s]
Processed prompts:  63%|██████▎   | 322/512 [00:07<00:05, 36.27it/s, est. speed input: 42300.79 toks/s, output: 41.31 toks/s]
Processed prompts:  64%|██████▎   | 326/512 [00:07<00:05, 36.05it/s, est. speed input: 42215.74 toks/s, output: 41.23 toks/s]
Processed prompts:  64%|██████▍   | 330/512 [00:08<00:05, 35.93it/s, est. speed input: 42135.72 toks/s, output: 41.15 toks/s]
Processed prompts:  65%|██████▌   | 334/512 [00:08<00:04, 35.95it/s, est. speed input: 42063.75 toks/s, output: 41.08 toks/s]
Processed prompts:  66%|██████▌   | 338/512 [00:08<00:04, 35.79it/s, est. speed input: 41984.03 toks/s, output: 41.00 toks/s]
Processed prompts:  67%|██████▋   | 342/512 [00:08<00:04, 35.82it/s, est. speed input: 41914.32 toks/s, output: 40.93 toks/s]
Processed prompts:  68%|██████▊   | 346/512 [00:08<00:04, 35.82it/s, est. speed input: 41845.36 toks/s, output: 40.86 toks/s]
Processed prompts:  68%|██████▊   | 350/512 [00:08<00:04, 35.71it/s, est. speed input: 41772.55 toks/s, output: 40.79 toks/s]
Processed prompts:  69%|██████▉   | 354/512 [00:08<00:04, 35.69it/s, est. speed input: 41704.54 toks/s, output: 40.73 toks/s]
Processed prompts:  70%|██████▉   | 358/512 [00:08<00:04, 35.67it/s, est. speed input: 41637.85 toks/s, output: 40.66 toks/s]
Processed prompts:  71%|███████   | 362/512 [00:08<00:04, 35.63it/s, est. speed input: 41571.80 toks/s, output: 40.60 toks/s]
Processed prompts:  71%|███████▏  | 366/512 [00:09<00:04, 35.62it/s, est. speed input: 41507.88 toks/s, output: 40.53 toks/s]
Processed prompts:  72%|███████▏  | 370/512 [00:09<00:03, 35.66it/s, est. speed input: 41447.91 toks/s, output: 40.48 toks/s]
Processed prompts:  73%|███████▎  | 374/512 [00:09<00:03, 35.77it/s, est. speed input: 41393.06 toks/s, output: 40.42 toks/s]
Processed prompts:  74%|███████▍  | 378/512 [00:09<00:03, 35.77it/s, est. speed input: 41336.16 toks/s, output: 40.37 toks/s]
Processed prompts:  75%|███████▍  | 382/512 [00:09<00:03, 35.64it/s, est. speed input: 41274.96 toks/s, output: 40.31 toks/s]
Processed prompts:  75%|███████▌  | 386/512 [00:09<00:03, 35.60it/s, est. speed input: 41216.98 toks/s, output: 40.25 toks/s]
Processed prompts:  76%|███████▌  | 390/512 [00:09<00:03, 35.59it/s, est. speed input: 41161.17 toks/s, output: 40.20 toks/s]
Processed prompts:  77%|███████▋  | 394/512 [00:09<00:03, 35.65it/s, est. speed input: 41110.16 toks/s, output: 40.15 toks/s]
Processed prompts:  78%|███████▊  | 398/512 [00:09<00:03, 35.65it/s, est. speed input: 41057.92 toks/s, output: 40.10 toks/s]
Processed prompts:  79%|███████▊  | 402/512 [00:10<00:03, 35.62it/s, est. speed input: 41005.77 toks/s, output: 40.04 toks/s]
Processed prompts:  79%|███████▉  | 406/512 [00:10<00:02, 35.70it/s, est. speed input: 40959.14 toks/s, output: 40.00 toks/s]
Processed prompts:  80%|████████  | 410/512 [00:10<00:02, 35.76it/s, est. speed input: 40913.53 toks/s, output: 39.95 toks/s]
Processed prompts:  81%|████████  | 414/512 [00:10<00:02, 35.73it/s, est. speed input: 40865.73 toks/s, output: 39.91 toks/s]
Processed prompts:  82%|████████▏ | 418/512 [00:10<00:02, 35.60it/s, est. speed input: 40814.85 toks/s, output: 39.86 toks/s]
Processed prompts:  82%|████████▏ | 422/512 [00:10<00:02, 35.56it/s, est. speed input: 40767.14 toks/s, output: 39.81 toks/s]
Processed prompts:  83%|████████▎ | 426/512 [00:10<00:02, 35.67it/s, est. speed input: 40725.90 toks/s, output: 39.77 toks/s]
Processed prompts:  84%|████████▍ | 430/512 [00:10<00:02, 35.66it/s, est. speed input: 40681.96 toks/s, output: 39.73 toks/s]
Processed prompts:  86%|████████▌ | 438/512 [00:11<00:01, 37.39it/s, est. speed input: 40679.91 toks/s, output: 39.73 toks/s]
Processed prompts:  86%|████████▋ | 442/512 [00:11<00:01, 36.83it/s, est. speed input: 40632.98 toks/s, output: 39.68 toks/s]
Processed prompts:  87%|████████▋ | 446/512 [00:11<00:01, 36.61it/s, est. speed input: 40595.67 toks/s, output: 39.64 toks/s]
Processed prompts:  88%|████████▊ | 450/512 [00:11<00:01, 36.28it/s, est. speed input: 40553.00 toks/s, output: 39.60 toks/s]
Processed prompts:  89%|████████▊ | 454/512 [00:11<00:01, 36.04it/s, est. speed input: 40511.06 toks/s, output: 39.56 toks/s]
Processed prompts:  89%|████████▉ | 458/512 [00:11<00:01, 35.91it/s, est. speed input: 40471.37 toks/s, output: 39.52 toks/s]
Processed prompts:  90%|█████████ | 462/512 [00:11<00:01, 35.83it/s, est. speed input: 40433.12 toks/s, output: 39.49 toks/s]
Processed prompts:  91%|█████████ | 466/512 [00:11<00:01, 35.88it/s, est. speed input: 40399.52 toks/s, output: 39.45 toks/s]
Processed prompts:  92%|█████████▏| 470/512 [00:11<00:01, 35.83it/s, est. speed input: 40363.63 toks/s, output: 39.42 toks/s]
Processed prompts:  93%|█████████▎| 474/512 [00:12<00:01, 35.72it/s, est. speed input: 40325.76 toks/s, output: 39.38 toks/s]
Processed prompts:  93%|█████████▎| 478/512 [00:12<00:00, 35.66it/s, est. speed input: 40289.16 toks/s, output: 39.34 toks/s]
Processed prompts:  94%|█████████▍| 482/512 [00:12<00:00, 35.63it/s, est. speed input: 40253.67 toks/s, output: 39.31 toks/s]
Processed prompts:  95%|█████████▍| 486/512 [00:12<00:00, 35.63it/s, est. speed input: 40219.46 toks/s, output: 39.28 toks/s]
Processed prompts:  96%|█████████▌| 490/512 [00:12<00:00, 35.57it/s, est. speed input: 40183.87 toks/s, output: 39.24 toks/s]
Processed prompts:  96%|█████████▋| 494/512 [00:12<00:00, 35.51it/s, est. speed input: 40148.07 toks/s, output: 39.21 toks/s]
Processed prompts:  97%|█████████▋| 498/512 [00:12<00:00, 35.57it/s, est. speed input: 40116.69 toks/s, output: 39.18 toks/s]
Processed prompts:  98%|█████████▊| 502/512 [00:12<00:00, 35.62it/s, est. speed input: 40085.98 toks/s, output: 39.15 toks/s]
Processed prompts:  99%|█████████▉| 506/512 [00:12<00:00, 35.60it/s, est. speed input: 40054.00 toks/s, output: 39.12 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:13<00:00, 35.60it/s, est. speed input: 40258.72 toks/s, output: 39.32 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:13<00:00, 39.31it/s, est. speed input: 40258.72 toks/s, output: 39.32 toks/s]
[rank0]:[W126 12:26:14.449872782 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 55.9s

测试结果:
  Requests/s:   35.66
  Tokens/s:     36550.54
  Total Reqs:   512
  Elapsed:      14.36s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     36514.89

============================================================
[5/8] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 12:26:29 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1357583) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1357583) WARNING 01-26 12:26:46 [backends.py:609] Failed to read file <frozen os>
Throughput: 36.40 requests/s, 37307.48 total tokens/s, 36.40 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-26 12:26:28] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 12:26:28] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:26:28] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 12:26:28] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:26:28] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:26:28] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:26:28] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:26:28] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:26:28] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:26:28] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 12:26:28] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 12:26:28] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 12:26:28] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 12:26:28] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 12:26:35] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 12:26:35] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:26:35] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 12:26:35] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:26:35] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:26:35] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:26:35] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:26:35] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:26:35] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:26:35] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 12:26:35] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 12:26:35] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 12:26:35] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 12:26:35] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1357583) [2026-01-26 12:26:37] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1357583) [2026-01-26 12:26:37] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1357583) [2026-01-26 12:26:37] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1357583) [2026-01-26 12:26:37] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=1357583) [2026-01-26 12:26:37] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=1357583) [2026-01-26 12:26:37] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=1357583) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1357583) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.82it/s]
(EngineCore_DP0 pid=1357583) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.32it/s]
(EngineCore_DP0 pid=1357583) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.38it/s]
(EngineCore_DP0 pid=1357583) 
(EngineCore_DP0 pid=1357583) [2026-01-26 12:26:40] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=1357583) [2026-01-26 12:26:40] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16588800 bytes
(EngineCore_DP0 pid=1357583) [2026-01-26 12:26:40] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=1357583) [2026-01-26 12:26:40] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 12902400 bytes
(EngineCore_DP0 pid=1357583) [2026-01-26 12:26:40] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=1357583) [2026-01-26 12:26:40] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 136396800 bytes
(EngineCore_DP0 pid=1357583) [2026-01-26 12:26:40] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=1357583) [2026-01-26 12:26:40] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 67952640 bytes
(EngineCore_DP0 pid=1357583) 2026-01-26 12:26:56,774 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1357583) 2026-01-26 12:26:56,800 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1357583) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 1/5 [00:00<00:00,  4.39it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00,  4.58it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 3/5 [00:00<00:00,  2.80it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:01<00:00,  3.77it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:01<00:00,  4.29it/s]
(EngineCore_DP0 pid=1357583) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 1/4 [00:00<00:00,  6.82it/s]
Capturing CUDA graphs (decode, FULL):  75%|███████▌  | 3/4 [00:00<00:00, 12.19it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 12.72it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 18/1024 [00:00<00:05, 175.41it/s]
Adding requests:   6%|▌         | 57/1024 [00:00<00:03, 296.59it/s]
Adding requests:   9%|▉         | 95/1024 [00:00<00:02, 331.18it/s]
Adding requests:  13%|█▎        | 133/1024 [00:00<00:02, 347.42it/s]
Adding requests:  17%|█▋        | 172/1024 [00:00<00:02, 361.12it/s]
Adding requests:  21%|██        | 212/1024 [00:00<00:02, 373.39it/s]
Adding requests:  25%|██▍       | 251/1024 [00:00<00:02, 377.36it/s]
Adding requests:  28%|██▊       | 290/1024 [00:00<00:01, 380.22it/s]
Adding requests:  32%|███▏      | 331/1024 [00:00<00:01, 388.14it/s]
Adding requests:  36%|███▋      | 372/1024 [00:01<00:01, 392.96it/s]
Adding requests:  40%|████      | 413/1024 [00:01<00:01, 397.43it/s]
Adding requests:  44%|████▍     | 453/1024 [00:01<00:01, 396.62it/s]
Adding requests:  48%|████▊     | 496/1024 [00:01<00:01, 405.77it/s]
Adding requests:  52%|█████▏    | 537/1024 [00:01<00:01, 404.13it/s]
Adding requests:  56%|█████▋    | 578/1024 [00:01<00:01, 401.59it/s]
Adding requests:  60%|██████    | 619/1024 [00:01<00:01, 392.98it/s]
Adding requests:  64%|██████▍   | 659/1024 [00:01<00:00, 388.11it/s]
Adding requests:  68%|██████▊   | 698/1024 [00:01<00:00, 388.64it/s]
Adding requests:  72%|███████▏  | 737/1024 [00:01<00:00, 383.10it/s]
Adding requests:  76%|███████▌  | 777/1024 [00:02<00:00, 386.38it/s]
Adding requests:  80%|███████▉  | 816/1024 [00:02<00:00, 385.86it/s]
Adding requests:  84%|████████▎ | 857/1024 [00:02<00:00, 392.53it/s]
Adding requests:  88%|████████▊ | 897/1024 [00:02<00:00, 394.15it/s]
Adding requests:  92%|█████████▏| 937/1024 [00:02<00:00, 386.49it/s]
Adding requests:  95%|█████████▌| 976/1024 [00:02<00:00, 387.29it/s]
Adding requests:  99%|█████████▉| 1015/1024 [00:02<00:00, 380.53it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 381.28it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  10%|▉         | 98/1024 [00:00<00:02, 440.63it/s, est. speed input: 451255.05 toks/s, output: 440.65 toks/s]
Processed prompts:  14%|█▍        | 143/1024 [00:01<00:09, 90.27it/s, est. speed input: 110502.27 toks/s, output: 107.91 toks/s]
Processed prompts:  16%|█▌        | 164/1024 [00:01<00:13, 65.17it/s, est. speed input: 84533.21 toks/s, output: 82.55 toks/s]  
Processed prompts:  17%|█▋        | 177/1024 [00:02<00:13, 64.23it/s, est. speed input: 82203.78 toks/s, output: 80.28 toks/s]
Processed prompts:  18%|█▊        | 187/1024 [00:02<00:16, 50.47it/s, est. speed input: 72241.83 toks/s, output: 70.55 toks/s]
Processed prompts:  19%|█▉        | 195/1024 [00:02<00:17, 47.93it/s, est. speed input: 69628.66 toks/s, output: 68.00 toks/s]
Processed prompts:  20%|█▉        | 202/1024 [00:03<00:18, 45.64it/s, est. speed input: 67552.06 toks/s, output: 65.97 toks/s]
Processed prompts:  21%|██        | 210/1024 [00:03<00:18, 43.35it/s, est. speed input: 65482.17 toks/s, output: 63.95 toks/s]
Processed prompts:  21%|██▏       | 218/1024 [00:03<00:19, 41.43it/s, est. speed input: 63649.32 toks/s, output: 62.16 toks/s]
Processed prompts:  22%|██▏       | 226/1024 [00:03<00:19, 40.06it/s, est. speed input: 62080.22 toks/s, output: 60.62 toks/s]
Processed prompts:  23%|██▎       | 234/1024 [00:03<00:20, 39.13it/s, est. speed input: 60723.33 toks/s, output: 59.30 toks/s]
Processed prompts:  24%|██▎       | 242/1024 [00:04<00:20, 38.47it/s, est. speed input: 59517.94 toks/s, output: 58.12 toks/s]
Processed prompts:  24%|██▍       | 250/1024 [00:04<00:20, 37.72it/s, est. speed input: 58361.78 toks/s, output: 56.99 toks/s]
Processed prompts:  25%|██▌       | 258/1024 [00:04<00:20, 37.22it/s, est. speed input: 57324.39 toks/s, output: 55.98 toks/s]
Processed prompts:  26%|██▌       | 266/1024 [00:04<00:20, 37.04it/s, est. speed input: 56427.34 toks/s, output: 55.10 toks/s]
Processed prompts:  27%|██▋       | 274/1024 [00:05<00:20, 36.94it/s, est. speed input: 55612.78 toks/s, output: 54.31 toks/s]
Processed prompts:  28%|██▊       | 282/1024 [00:05<00:20, 36.69it/s, est. speed input: 54828.22 toks/s, output: 53.54 toks/s]
Processed prompts:  28%|██▊       | 290/1024 [00:05<00:20, 36.48it/s, est. speed input: 54099.85 toks/s, output: 52.83 toks/s]
Processed prompts:  29%|██▉       | 298/1024 [00:05<00:19, 36.39it/s, est. speed input: 53438.98 toks/s, output: 52.19 toks/s]
Processed prompts:  30%|██▉       | 306/1024 [00:05<00:19, 37.72it/s, est. speed input: 53072.11 toks/s, output: 51.83 toks/s]
Processed prompts:  31%|███       | 314/1024 [00:06<00:19, 37.33it/s, est. speed input: 52507.67 toks/s, output: 51.28 toks/s]
Processed prompts:  31%|███▏      | 322/1024 [00:06<00:19, 36.91it/s, est. speed input: 51958.18 toks/s, output: 50.74 toks/s]
Processed prompts:  32%|███▏      | 330/1024 [00:06<00:18, 36.71it/s, est. speed input: 51459.37 toks/s, output: 50.25 toks/s]
Processed prompts:  33%|███▎      | 338/1024 [00:06<00:18, 36.59it/s, est. speed input: 50995.21 toks/s, output: 49.80 toks/s]
Processed prompts:  34%|███▍      | 346/1024 [00:07<00:18, 36.55it/s, est. speed input: 50568.22 toks/s, output: 49.38 toks/s]
Processed prompts:  35%|███▍      | 354/1024 [00:07<00:18, 36.49it/s, est. speed input: 50162.13 toks/s, output: 48.99 toks/s]
Processed prompts:  35%|███▌      | 362/1024 [00:07<00:18, 36.37it/s, est. speed input: 49768.99 toks/s, output: 48.60 toks/s]
Processed prompts:  36%|███▌      | 370/1024 [00:07<00:18, 36.26it/s, est. speed input: 49395.75 toks/s, output: 48.24 toks/s]
Processed prompts:  37%|███▋      | 378/1024 [00:07<00:17, 36.40it/s, est. speed input: 49070.29 toks/s, output: 47.92 toks/s]
Processed prompts:  38%|███▊      | 386/1024 [00:08<00:17, 36.35it/s, est. speed input: 48744.45 toks/s, output: 47.60 toks/s]
Processed prompts:  38%|███▊      | 394/1024 [00:08<00:17, 36.30it/s, est. speed input: 48433.43 toks/s, output: 47.30 toks/s]
Processed prompts:  39%|███▉      | 402/1024 [00:08<00:17, 36.25it/s, est. speed input: 48137.47 toks/s, output: 47.01 toks/s]
Processed prompts:  40%|████      | 410/1024 [00:08<00:16, 36.29it/s, est. speed input: 47864.63 toks/s, output: 46.74 toks/s]
Processed prompts:  41%|████      | 418/1024 [00:08<00:16, 36.32it/s, est. speed input: 47605.12 toks/s, output: 46.49 toks/s]
Processed prompts:  42%|████▏     | 426/1024 [00:09<00:16, 36.32it/s, est. speed input: 47356.39 toks/s, output: 46.25 toks/s]
Processed prompts:  42%|████▏     | 434/1024 [00:09<00:15, 37.68it/s, est. speed input: 47251.56 toks/s, output: 46.14 toks/s]
Processed prompts:  43%|████▎     | 442/1024 [00:09<00:15, 37.16it/s, est. speed input: 47011.69 toks/s, output: 45.91 toks/s]
Processed prompts:  44%|████▍     | 450/1024 [00:09<00:15, 36.84it/s, est. speed input: 46785.89 toks/s, output: 45.69 toks/s]
Processed prompts:  45%|████▍     | 458/1024 [00:10<00:15, 36.72it/s, est. speed input: 46579.89 toks/s, output: 45.49 toks/s]
Processed prompts:  46%|████▌     | 466/1024 [00:10<00:15, 36.51it/s, est. speed input: 46370.68 toks/s, output: 45.28 toks/s]
Processed prompts:  46%|████▋     | 474/1024 [00:10<00:15, 36.47it/s, est. speed input: 46179.79 toks/s, output: 45.10 toks/s]
Processed prompts:  47%|████▋     | 482/1024 [00:10<00:14, 36.38it/s, est. speed input: 45991.50 toks/s, output: 44.91 toks/s]
Processed prompts:  48%|████▊     | 490/1024 [00:10<00:14, 36.41it/s, est. speed input: 45818.19 toks/s, output: 44.74 toks/s]
Processed prompts:  49%|████▊     | 498/1024 [00:11<00:14, 36.41it/s, est. speed input: 45650.32 toks/s, output: 44.58 toks/s]
Processed prompts:  49%|████▉     | 506/1024 [00:11<00:14, 36.30it/s, est. speed input: 45480.33 toks/s, output: 44.41 toks/s]
Processed prompts:  50%|█████     | 514/1024 [00:11<00:14, 36.32it/s, est. speed input: 45324.30 toks/s, output: 44.26 toks/s]
Processed prompts:  51%|█████     | 522/1024 [00:11<00:13, 36.27it/s, est. speed input: 45168.98 toks/s, output: 44.11 toks/s]
Processed prompts:  52%|█████▏    | 530/1024 [00:12<00:13, 36.25it/s, est. speed input: 45020.89 toks/s, output: 43.97 toks/s]
Processed prompts:  53%|█████▎    | 538/1024 [00:12<00:13, 36.22it/s, est. speed input: 44876.30 toks/s, output: 43.82 toks/s]
Processed prompts:  53%|█████▎    | 546/1024 [00:12<00:13, 36.17it/s, est. speed input: 44735.27 toks/s, output: 43.69 toks/s]
Processed prompts:  54%|█████▍    | 554/1024 [00:12<00:13, 36.09it/s, est. speed input: 44595.79 toks/s, output: 43.55 toks/s]
Processed prompts:  55%|█████▍    | 562/1024 [00:12<00:12, 36.06it/s, est. speed input: 44462.47 toks/s, output: 43.42 toks/s]
Processed prompts:  56%|█████▌    | 570/1024 [00:13<00:12, 36.22it/s, est. speed input: 44346.38 toks/s, output: 43.31 toks/s]
Processed prompts:  56%|█████▋    | 578/1024 [00:13<00:12, 36.24it/s, est. speed input: 44228.38 toks/s, output: 43.19 toks/s]
Processed prompts:  57%|█████▋    | 586/1024 [00:13<00:12, 36.22it/s, est. speed input: 44111.59 toks/s, output: 43.08 toks/s]
Processed prompts:  58%|█████▊    | 594/1024 [00:13<00:11, 36.30it/s, est. speed input: 44004.20 toks/s, output: 42.97 toks/s]
Processed prompts:  59%|█████▉    | 602/1024 [00:14<00:11, 36.30it/s, est. speed input: 43897.31 toks/s, output: 42.87 toks/s]
Processed prompts:  60%|█████▉    | 610/1024 [00:14<00:11, 36.31it/s, est. speed input: 43794.03 toks/s, output: 42.77 toks/s]
Processed prompts:  60%|██████    | 618/1024 [00:14<00:11, 36.35it/s, est. speed input: 43695.95 toks/s, output: 42.67 toks/s]
Processed prompts:  61%|██████    | 626/1024 [00:14<00:10, 36.32it/s, est. speed input: 43597.32 toks/s, output: 42.58 toks/s]
Processed prompts:  62%|██████▏   | 634/1024 [00:14<00:10, 36.36it/s, est. speed input: 43504.94 toks/s, output: 42.49 toks/s]
Processed prompts:  63%|██████▎   | 642/1024 [00:15<00:10, 36.33it/s, est. speed input: 43412.07 toks/s, output: 42.39 toks/s]
Processed prompts:  63%|██████▎   | 650/1024 [00:15<00:10, 36.31it/s, est. speed input: 43322.23 toks/s, output: 42.31 toks/s]
Processed prompts:  64%|██████▍   | 658/1024 [00:15<00:10, 36.37it/s, est. speed input: 43238.39 toks/s, output: 42.22 toks/s]
Processed prompts:  65%|██████▌   | 666/1024 [00:15<00:09, 36.37it/s, est. speed input: 43154.93 toks/s, output: 42.14 toks/s]
Processed prompts:  66%|██████▌   | 674/1024 [00:16<00:09, 36.34it/s, est. speed input: 43072.43 toks/s, output: 42.06 toks/s]
Processed prompts:  67%|██████▋   | 682/1024 [00:16<00:09, 36.34it/s, est. speed input: 42992.92 toks/s, output: 41.99 toks/s]
Processed prompts:  67%|██████▋   | 690/1024 [00:16<00:09, 36.34it/s, est. speed input: 42915.67 toks/s, output: 41.91 toks/s]
Processed prompts:  68%|██████▊   | 698/1024 [00:16<00:08, 36.35it/s, est. speed input: 42840.97 toks/s, output: 41.84 toks/s]
Processed prompts:  69%|██████▉   | 706/1024 [00:16<00:08, 36.35it/s, est. speed input: 42767.93 toks/s, output: 41.77 toks/s]
Processed prompts:  70%|██████▉   | 714/1024 [00:17<00:08, 36.31it/s, est. speed input: 42694.60 toks/s, output: 41.69 toks/s]
Processed prompts:  71%|███████   | 722/1024 [00:17<00:08, 36.31it/s, est. speed input: 42624.68 toks/s, output: 41.63 toks/s]
Processed prompts:  71%|███████▏  | 730/1024 [00:17<00:08, 36.32it/s, est. speed input: 42556.99 toks/s, output: 41.56 toks/s]
Processed prompts:  72%|███████▏  | 738/1024 [00:17<00:07, 36.37it/s, est. speed input: 42492.87 toks/s, output: 41.50 toks/s]
Processed prompts:  73%|███████▎  | 746/1024 [00:18<00:07, 36.32it/s, est. speed input: 42426.38 toks/s, output: 41.43 toks/s]
Processed prompts:  74%|███████▎  | 754/1024 [00:18<00:07, 36.32it/s, est. speed input: 42362.99 toks/s, output: 41.37 toks/s]
Processed prompts:  74%|███████▍  | 762/1024 [00:18<00:07, 36.33it/s, est. speed input: 42301.91 toks/s, output: 41.31 toks/s]
Processed prompts:  75%|███████▌  | 770/1024 [00:18<00:06, 36.37it/s, est. speed input: 42243.44 toks/s, output: 41.25 toks/s]
Processed prompts:  76%|███████▌  | 778/1024 [00:18<00:06, 36.33it/s, est. speed input: 42183.37 toks/s, output: 41.19 toks/s]
Processed prompts:  77%|███████▋  | 786/1024 [00:19<00:06, 37.59it/s, est. speed input: 42180.19 toks/s, output: 41.19 toks/s]
Processed prompts:  78%|███████▊  | 794/1024 [00:19<00:06, 37.15it/s, est. speed input: 42121.37 toks/s, output: 41.13 toks/s]
Processed prompts:  78%|███████▊  | 802/1024 [00:19<00:06, 36.93it/s, est. speed input: 42067.10 toks/s, output: 41.08 toks/s]
Processed prompts:  79%|███████▉  | 810/1024 [00:19<00:05, 36.74it/s, est. speed input: 42012.53 toks/s, output: 41.03 toks/s]
Processed prompts:  80%|███████▉  | 818/1024 [00:19<00:05, 36.59it/s, est. speed input: 41958.49 toks/s, output: 40.98 toks/s]
Processed prompts:  81%|████████  | 826/1024 [00:20<00:05, 36.51it/s, est. speed input: 41906.35 toks/s, output: 40.92 toks/s]
Processed prompts:  81%|████████▏ | 834/1024 [00:20<00:05, 36.42it/s, est. speed input: 41854.06 toks/s, output: 40.87 toks/s]
Processed prompts:  82%|████████▏ | 842/1024 [00:20<00:04, 36.44it/s, est. speed input: 41806.17 toks/s, output: 40.83 toks/s]
Processed prompts:  83%|████████▎ | 850/1024 [00:20<00:04, 36.30it/s, est. speed input: 41753.25 toks/s, output: 40.77 toks/s]
Processed prompts:  84%|████████▍ | 858/1024 [00:21<00:04, 36.31it/s, est. speed input: 41705.96 toks/s, output: 40.73 toks/s]
Processed prompts:  85%|████████▍ | 866/1024 [00:21<00:04, 36.36it/s, est. speed input: 41661.09 toks/s, output: 40.68 toks/s]
Processed prompts:  85%|████████▌ | 874/1024 [00:21<00:04, 36.34it/s, est. speed input: 41614.90 toks/s, output: 40.64 toks/s]
Processed prompts:  86%|████████▌ | 882/1024 [00:21<00:03, 36.29it/s, est. speed input: 41568.39 toks/s, output: 40.59 toks/s]
Processed prompts:  87%|████████▋ | 890/1024 [00:21<00:03, 36.23it/s, est. speed input: 41521.91 toks/s, output: 40.55 toks/s]
Processed prompts:  88%|████████▊ | 898/1024 [00:22<00:03, 36.31it/s, est. speed input: 41481.00 toks/s, output: 40.51 toks/s]
Processed prompts:  88%|████████▊ | 906/1024 [00:22<00:03, 36.35it/s, est. speed input: 41440.13 toks/s, output: 40.47 toks/s]
Processed prompts:  89%|████████▉ | 914/1024 [00:22<00:03, 36.29it/s, est. speed input: 41396.67 toks/s, output: 40.43 toks/s]
Processed prompts:  90%|█████████ | 922/1024 [00:22<00:02, 36.22it/s, est. speed input: 41353.36 toks/s, output: 40.38 toks/s]
Processed prompts:  91%|█████████ | 930/1024 [00:23<00:02, 36.29it/s, est. speed input: 41314.88 toks/s, output: 40.35 toks/s]
Processed prompts:  92%|█████████▏| 938/1024 [00:23<00:02, 36.32it/s, est. speed input: 41276.74 toks/s, output: 40.31 toks/s]
Processed prompts:  92%|█████████▏| 946/1024 [00:23<00:02, 36.40it/s, est. speed input: 41241.30 toks/s, output: 40.27 toks/s]
Processed prompts:  93%|█████████▎| 954/1024 [00:23<00:01, 36.35it/s, est. speed input: 41202.77 toks/s, output: 40.24 toks/s]
Processed prompts:  94%|█████████▍| 962/1024 [00:23<00:01, 36.30it/s, est. speed input: 41164.45 toks/s, output: 40.20 toks/s]
Processed prompts:  95%|█████████▍| 970/1024 [00:24<00:01, 36.32it/s, est. speed input: 41128.53 toks/s, output: 40.16 toks/s]
Processed prompts:  96%|█████████▌| 978/1024 [00:24<00:01, 36.31it/s, est. speed input: 41092.59 toks/s, output: 40.13 toks/s]
Processed prompts:  96%|█████████▋| 986/1024 [00:24<00:01, 36.26it/s, est. speed input: 41055.91 toks/s, output: 40.09 toks/s]
Processed prompts:  97%|█████████▋| 994/1024 [00:24<00:00, 36.29it/s, est. speed input: 41021.95 toks/s, output: 40.06 toks/s]
Processed prompts:  98%|█████████▊| 1002/1024 [00:25<00:00, 36.30it/s, est. speed input: 40988.48 toks/s, output: 40.03 toks/s]
Processed prompts:  99%|█████████▊| 1010/1024 [00:25<00:00, 36.28it/s, est. speed input: 40954.47 toks/s, output: 39.99 toks/s]
Processed prompts:  99%|█████████▉| 1018/1024 [00:25<00:00, 37.74it/s, est. speed input: 40967.11 toks/s, output: 40.01 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:25<00:00, 37.74it/s, est. speed input: 41207.87 toks/s, output: 40.24 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:25<00:00, 40.24it/s, est. speed input: 41207.87 toks/s, output: 40.24 toks/s]
[rank0]:[W126 12:27:28.234958777 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 73.5s

测试结果:
  Requests/s:   36.40
  Tokens/s:     37307.48
  Total Reqs:   1024
  Elapsed:      28.13s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     37271.08

============================================================
[6/8] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 12:27:47 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1359131) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1359131) WARNING 01-26 12:28:05 [backends.py:609] Failed to read file <frozen os>
Throughput: 37.58 requests/s, 38517.92 total tokens/s, 37.58 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-26 12:27:47] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 12:27:47] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:27:47] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 12:27:47] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:27:47] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:27:47] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:27:47] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:27:47] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:27:47] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:27:47] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 12:27:47] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 12:27:47] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 12:27:47] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 12:27:47] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 12:27:54] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 12:27:54] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:27:54] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 12:27:54] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:27:54] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:27:54] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:27:54] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:27:54] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:27:54] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:27:54] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 12:27:54] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 12:27:54] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 12:27:54] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 12:27:54] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1359131) [2026-01-26 12:27:56] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1359131) [2026-01-26 12:27:56] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1359131) [2026-01-26 12:27:56] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1359131) [2026-01-26 12:27:56] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=1359131) [2026-01-26 12:27:56] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=1359131) [2026-01-26 12:27:56] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=1359131) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1359131) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.86it/s]
(EngineCore_DP0 pid=1359131) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.34it/s]
(EngineCore_DP0 pid=1359131) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.40it/s]
(EngineCore_DP0 pid=1359131) 
(EngineCore_DP0 pid=1359131) [2026-01-26 12:27:59] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=1359131) [2026-01-26 12:27:59] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16588800 bytes
(EngineCore_DP0 pid=1359131) [2026-01-26 12:27:59] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=1359131) [2026-01-26 12:27:59] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 12902400 bytes
(EngineCore_DP0 pid=1359131) [2026-01-26 12:27:59] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=1359131) [2026-01-26 12:27:59] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 136396800 bytes
(EngineCore_DP0 pid=1359131) [2026-01-26 12:27:59] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=1359131) [2026-01-26 12:27:59] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 67952640 bytes
(EngineCore_DP0 pid=1359131) [rank0]:W0126 12:28:10.780000 1359131 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1359131) [rank0]:W0126 12:28:10.858000 1359131 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1359131) [rank0]:W0126 12:28:12.017000 1359131 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1359131) [rank0]:W0126 12:28:12.149000 1359131 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1359131) 2026-01-26 12:28:15,995 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1359131) 2026-01-26 12:28:16,022 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1359131) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 1/7 [00:00<00:00,  8.70it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 3/7 [00:00<00:00,  7.70it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00,  3.71it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:01<00:00,  5.94it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:01<00:00,  5.99it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:01<00:00,  5.77it/s]
(EngineCore_DP0 pid=1359131) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00, 14.75it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 18.09it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 17.61it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   1%|          | 22/2048 [00:00<00:09, 218.71it/s]
Adding requests:   3%|▎         | 62/2048 [00:00<00:06, 323.74it/s]
Adding requests:   5%|▍         | 96/2048 [00:00<00:05, 330.34it/s]
Adding requests:   7%|▋         | 134/2048 [00:00<00:05, 349.56it/s]
Adding requests:   8%|▊         | 174/2048 [00:00<00:05, 365.86it/s]
Adding requests:  10%|█         | 215/2048 [00:00<00:04, 377.85it/s]
Adding requests:  12%|█▏        | 254/2048 [00:00<00:04, 379.08it/s]
Adding requests:  14%|█▍        | 293/2048 [00:00<00:04, 382.20it/s]
Adding requests:  16%|█▋        | 335/2048 [00:00<00:04, 391.42it/s]
Adding requests:  18%|█▊        | 375/2048 [00:01<00:04, 393.08it/s]
Adding requests:  20%|██        | 417/2048 [00:01<00:04, 400.54it/s]
Adding requests:  22%|██▏       | 458/2048 [00:01<00:04, 391.19it/s]
Adding requests:  24%|██▍       | 501/2048 [00:01<00:03, 400.08it/s]
Adding requests:  27%|██▋       | 543/2048 [00:01<00:03, 405.77it/s]
Adding requests:  29%|██▊       | 584/2048 [00:01<00:03, 400.10it/s]
Adding requests:  31%|███       | 625/2048 [00:01<00:03, 391.37it/s]
Adding requests:  32%|███▏      | 665/2048 [00:01<00:03, 383.11it/s]
Adding requests:  34%|███▍      | 705/2048 [00:01<00:03, 385.71it/s]
Adding requests:  36%|███▋      | 744/2048 [00:01<00:03, 380.35it/s]
Adding requests:  38%|███▊      | 783/2048 [00:02<00:03, 375.53it/s]
Adding requests:  40%|████      | 822/2048 [00:02<00:03, 378.14it/s]
Adding requests:  42%|████▏     | 862/2048 [00:02<00:03, 382.67it/s]
Adding requests:  44%|████▍     | 902/2048 [00:02<00:02, 385.59it/s]
Adding requests:  46%|████▌     | 941/2048 [00:02<00:02, 381.50it/s]
Adding requests:  48%|████▊     | 980/2048 [00:02<00:02, 379.45it/s]
Adding requests:  50%|████▉     | 1018/2048 [00:02<00:02, 375.93it/s]
Adding requests:  52%|█████▏    | 1056/2048 [00:02<00:02, 373.79it/s]
Adding requests:  53%|█████▎    | 1094/2048 [00:02<00:02, 373.48it/s]
Adding requests:  55%|█████▌    | 1135/2048 [00:02<00:02, 381.28it/s]
Adding requests:  57%|█████▋    | 1174/2048 [00:03<00:02, 378.10it/s]
Adding requests:  59%|█████▉    | 1213/2048 [00:03<00:02, 380.41it/s]
Adding requests:  61%|██████    | 1252/2048 [00:03<00:02, 382.93it/s]
Adding requests:  63%|██████▎   | 1291/2048 [00:03<00:02, 375.51it/s]
Adding requests:  65%|██████▍   | 1330/2048 [00:03<00:01, 379.04it/s]
Adding requests:  67%|██████▋   | 1369/2048 [00:03<00:01, 381.27it/s]
Adding requests:  69%|██████▉   | 1408/2048 [00:03<00:01, 379.60it/s]
Adding requests:  71%|███████   | 1446/2048 [00:03<00:01, 378.83it/s]
Adding requests:  73%|███████▎  | 1487/2048 [00:03<00:01, 384.68it/s]
Adding requests:  75%|███████▍  | 1526/2048 [00:04<00:01, 385.39it/s]
Adding requests:  76%|███████▋  | 1565/2048 [00:04<00:01, 381.88it/s]
Adding requests:  78%|███████▊  | 1604/2048 [00:04<00:01, 380.18it/s]
Adding requests:  80%|████████  | 1643/2048 [00:04<00:01, 373.84it/s]
Adding requests:  82%|████████▏ | 1681/2048 [00:04<00:00, 374.15it/s]
Adding requests:  84%|████████▍ | 1721/2048 [00:04<00:00, 378.39it/s]
Adding requests:  86%|████████▌ | 1761/2048 [00:04<00:00, 383.56it/s]
Adding requests:  88%|████████▊ | 1801/2048 [00:04<00:00, 386.46it/s]
Adding requests:  90%|████████▉ | 1841/2048 [00:04<00:00, 387.73it/s]
Adding requests:  92%|█████████▏| 1881/2048 [00:04<00:00, 389.03it/s]
Adding requests:  94%|█████████▍| 1920/2048 [00:05<00:00, 382.66it/s]
Adding requests:  96%|█████████▌| 1961/2048 [00:05<00:00, 388.26it/s]
Adding requests:  98%|█████████▊| 2000/2048 [00:05<00:00, 383.19it/s]
Adding requests: 100%|█████████▉| 2039/2048 [00:05<00:00, 378.96it/s]
Adding requests: 100%|██████████| 2048/2048 [00:05<00:00, 380.40it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   9%|▉         | 194/2048 [00:00<00:02, 712.71it/s, est. speed input: 729874.84 toks/s, output: 712.72 toks/s]
Processed prompts:  13%|█▎        | 266/2048 [00:01<00:16, 109.76it/s, est. speed input: 137928.43 toks/s, output: 134.69 toks/s]
Processed prompts:  15%|█▍        | 298/2048 [00:02<00:21, 81.43it/s, est. speed input: 107871.15 toks/s, output: 105.34 toks/s] 
Processed prompts:  16%|█▌        | 318/2048 [00:03<00:23, 74.74it/s, est. speed input: 100719.31 toks/s, output: 98.36 toks/s] 
Processed prompts:  16%|█▌        | 332/2048 [00:03<00:26, 64.74it/s, est. speed input: 92938.03 toks/s, output: 90.76 toks/s] 
Processed prompts:  17%|█▋        | 342/2048 [00:04<00:31, 54.20it/s, est. speed input: 85748.23 toks/s, output: 83.74 toks/s]
Processed prompts:  17%|█▋        | 354/2048 [00:04<00:35, 47.33it/s, est. speed input: 80423.13 toks/s, output: 78.54 toks/s]
Processed prompts:  18%|█▊        | 370/2048 [00:04<00:37, 44.59it/s, est. speed input: 76778.61 toks/s, output: 74.98 toks/s]
Processed prompts:  19%|█▉        | 386/2048 [00:05<00:39, 42.56it/s, est. speed input: 73715.48 toks/s, output: 71.99 toks/s]
Processed prompts:  20%|█▉        | 402/2048 [00:05<00:39, 41.19it/s, est. speed input: 71150.25 toks/s, output: 69.48 toks/s]
Processed prompts:  20%|██        | 418/2048 [00:06<00:40, 40.12it/s, est. speed input: 68905.99 toks/s, output: 67.29 toks/s]
Processed prompts:  21%|██        | 434/2048 [00:06<00:40, 40.05it/s, est. speed input: 67202.82 toks/s, output: 65.63 toks/s]
Processed prompts:  22%|██▏       | 450/2048 [00:07<00:40, 39.36it/s, est. speed input: 65481.80 toks/s, output: 63.95 toks/s]
Processed prompts:  23%|██▎       | 466/2048 [00:07<00:40, 38.80it/s, est. speed input: 63934.26 toks/s, output: 62.44 toks/s]
Processed prompts:  24%|██▎       | 482/2048 [00:07<00:40, 38.46it/s, est. speed input: 62566.77 toks/s, output: 61.10 toks/s]
Processed prompts:  24%|██▍       | 498/2048 [00:08<00:40, 38.19it/s, est. speed input: 61331.31 toks/s, output: 59.89 toks/s]
Processed prompts:  25%|██▌       | 514/2048 [00:08<00:40, 38.04it/s, est. speed input: 60228.12 toks/s, output: 58.82 toks/s]
Processed prompts:  26%|██▌       | 530/2048 [00:09<00:40, 37.92it/s, est. speed input: 59221.66 toks/s, output: 57.83 toks/s]
Processed prompts:  27%|██▋       | 546/2048 [00:09<00:39, 37.87it/s, est. speed input: 58311.96 toks/s, output: 56.95 toks/s]
Processed prompts:  27%|██▋       | 562/2048 [00:10<00:39, 37.76it/s, est. speed input: 57464.72 toks/s, output: 56.12 toks/s]
Processed prompts:  28%|██▊       | 578/2048 [00:10<00:38, 37.74it/s, est. speed input: 56697.53 toks/s, output: 55.37 toks/s]
Processed prompts:  29%|██▉       | 594/2048 [00:10<00:38, 37.72it/s, est. speed input: 55989.45 toks/s, output: 54.68 toks/s]
Processed prompts:  30%|██▉       | 610/2048 [00:11<00:38, 37.73it/s, est. speed input: 55338.74 toks/s, output: 54.04 toks/s]
Processed prompts:  31%|███       | 626/2048 [00:11<00:37, 37.71it/s, est. speed input: 54730.47 toks/s, output: 53.45 toks/s]
Processed prompts:  31%|███▏      | 642/2048 [00:12<00:37, 37.62it/s, est. speed input: 54151.76 toks/s, output: 52.88 toks/s]
Processed prompts:  32%|███▏      | 658/2048 [00:12<00:36, 37.64it/s, est. speed input: 53626.69 toks/s, output: 52.37 toks/s]
Processed prompts:  33%|███▎      | 674/2048 [00:12<00:36, 37.62it/s, est. speed input: 53129.08 toks/s, output: 51.88 toks/s]
Processed prompts:  34%|███▎      | 690/2048 [00:13<00:36, 37.57it/s, est. speed input: 52658.62 toks/s, output: 51.42 toks/s]
Processed prompts:  34%|███▍      | 706/2048 [00:13<00:35, 37.56it/s, est. speed input: 52220.88 toks/s, output: 51.00 toks/s]
Processed prompts:  35%|███▌      | 722/2048 [00:14<00:35, 37.61it/s, est. speed input: 51817.06 toks/s, output: 50.60 toks/s]
Processed prompts:  36%|███▌      | 738/2048 [00:14<00:34, 37.59it/s, est. speed input: 51428.65 toks/s, output: 50.22 toks/s]
Processed prompts:  37%|███▋      | 754/2048 [00:15<00:34, 37.54it/s, est. speed input: 51057.91 toks/s, output: 49.86 toks/s]
Processed prompts:  38%|███▊      | 770/2048 [00:15<00:34, 37.54it/s, est. speed input: 50712.67 toks/s, output: 49.52 toks/s]
Processed prompts:  38%|███▊      | 786/2048 [00:15<00:33, 38.20it/s, est. speed input: 50462.87 toks/s, output: 49.28 toks/s]
Processed prompts:  39%|███▉      | 802/2048 [00:16<00:32, 37.96it/s, est. speed input: 50145.60 toks/s, output: 48.97 toks/s]
Processed prompts:  40%|███▉      | 818/2048 [00:16<00:32, 37.83it/s, est. speed input: 49847.86 toks/s, output: 48.68 toks/s]
Processed prompts:  41%|████      | 834/2048 [00:17<00:32, 37.77it/s, est. speed input: 49568.74 toks/s, output: 48.41 toks/s]
Processed prompts:  42%|████▏     | 850/2048 [00:17<00:31, 37.65it/s, est. speed input: 49295.09 toks/s, output: 48.14 toks/s]
Processed prompts:  42%|████▏     | 866/2048 [00:18<00:31, 37.60it/s, est. speed input: 49036.97 toks/s, output: 47.89 toks/s]
Processed prompts:  43%|████▎     | 882/2048 [00:18<00:31, 37.58it/s, est. speed input: 48793.14 toks/s, output: 47.65 toks/s]
Processed prompts:  44%|████▍     | 898/2048 [00:18<00:30, 37.52it/s, est. speed input: 48555.57 toks/s, output: 47.42 toks/s]
Processed prompts:  45%|████▍     | 914/2048 [00:19<00:30, 37.51it/s, est. speed input: 48331.31 toks/s, output: 47.20 toks/s]
Processed prompts:  45%|████▌     | 930/2048 [00:19<00:29, 37.48it/s, est. speed input: 48114.34 toks/s, output: 46.99 toks/s]
Processed prompts:  46%|████▌     | 946/2048 [00:20<00:29, 37.51it/s, est. speed input: 47911.50 toks/s, output: 46.79 toks/s]
Processed prompts:  47%|████▋     | 962/2048 [00:20<00:28, 37.48it/s, est. speed input: 47712.92 toks/s, output: 46.59 toks/s]
Processed prompts:  48%|████▊     | 978/2048 [00:21<00:28, 37.45it/s, est. speed input: 47521.10 toks/s, output: 46.41 toks/s]
Processed prompts:  49%|████▊     | 994/2048 [00:21<00:28, 37.45it/s, est. speed input: 47339.14 toks/s, output: 46.23 toks/s]
Processed prompts:  49%|████▉     | 1010/2048 [00:21<00:27, 37.43it/s, est. speed input: 47162.05 toks/s, output: 46.06 toks/s]
Processed prompts:  50%|█████     | 1026/2048 [00:22<00:27, 37.45it/s, est. speed input: 46994.44 toks/s, output: 45.89 toks/s]
Processed prompts:  51%|█████     | 1042/2048 [00:22<00:26, 37.37it/s, est. speed input: 46826.38 toks/s, output: 45.73 toks/s]
Processed prompts:  52%|█████▏    | 1058/2048 [00:23<00:26, 37.42it/s, est. speed input: 46672.03 toks/s, output: 45.58 toks/s]
Processed prompts:  52%|█████▏    | 1074/2048 [00:23<00:26, 37.44it/s, est. speed input: 46522.98 toks/s, output: 45.43 toks/s]
Processed prompts:  53%|█████▎    | 1090/2048 [00:24<00:25, 37.40it/s, est. speed input: 46374.42 toks/s, output: 45.29 toks/s]
Processed prompts:  54%|█████▍    | 1106/2048 [00:24<00:25, 37.45it/s, est. speed input: 46237.09 toks/s, output: 45.15 toks/s]
Processed prompts:  55%|█████▍    | 1122/2048 [00:24<00:24, 37.41it/s, est. speed input: 46099.15 toks/s, output: 45.02 toks/s]
Processed prompts:  56%|█████▌    | 1138/2048 [00:25<00:24, 37.45it/s, est. speed input: 45970.44 toks/s, output: 44.89 toks/s]
Processed prompts:  56%|█████▋    | 1154/2048 [00:25<00:23, 37.41it/s, est. speed input: 45841.48 toks/s, output: 44.77 toks/s]
Processed prompts:  57%|█████▋    | 1170/2048 [00:26<00:23, 37.45it/s, est. speed input: 45721.00 toks/s, output: 44.65 toks/s]
Processed prompts:  58%|█████▊    | 1186/2048 [00:26<00:23, 37.45it/s, est. speed input: 45602.98 toks/s, output: 44.53 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [00:27<00:22, 38.07it/s, est. speed input: 45527.22 toks/s, output: 44.46 toks/s]
Processed prompts:  59%|█████▉    | 1218/2048 [00:27<00:21, 37.88it/s, est. speed input: 45415.47 toks/s, output: 44.35 toks/s]
Processed prompts:  60%|██████    | 1234/2048 [00:27<00:21, 38.36it/s, est. speed input: 45344.03 toks/s, output: 44.28 toks/s]
Processed prompts:  61%|██████    | 1250/2048 [00:28<00:20, 38.08it/s, est. speed input: 45238.00 toks/s, output: 44.18 toks/s]
Processed prompts:  62%|██████▏   | 1266/2048 [00:28<00:20, 37.86it/s, est. speed input: 45134.03 toks/s, output: 44.08 toks/s]
Processed prompts:  63%|██████▎   | 1282/2048 [00:29<00:20, 37.72it/s, est. speed input: 45033.32 toks/s, output: 43.98 toks/s]
Processed prompts:  63%|██████▎   | 1298/2048 [00:29<00:19, 37.59it/s, est. speed input: 44934.06 toks/s, output: 43.88 toks/s]
Processed prompts:  64%|██████▍   | 1314/2048 [00:30<00:19, 37.51it/s, est. speed input: 44838.24 toks/s, output: 43.79 toks/s]
Processed prompts:  65%|██████▍   | 1330/2048 [00:30<00:18, 38.14it/s, est. speed input: 44782.93 toks/s, output: 43.73 toks/s]
Processed prompts:  66%|██████▌   | 1346/2048 [00:30<00:18, 37.84it/s, est. speed input: 44688.49 toks/s, output: 43.64 toks/s]
Processed prompts:  67%|██████▋   | 1362/2048 [00:31<00:18, 37.71it/s, est. speed input: 44601.52 toks/s, output: 43.56 toks/s]
Processed prompts:  67%|██████▋   | 1378/2048 [00:31<00:17, 37.53it/s, est. speed input: 44511.86 toks/s, output: 43.47 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [00:32<00:17, 37.51it/s, est. speed input: 44429.91 toks/s, output: 43.39 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [00:32<00:17, 37.45it/s, est. speed input: 44347.90 toks/s, output: 43.31 toks/s]
Processed prompts:  70%|██████▉   | 1426/2048 [00:32<00:16, 37.38it/s, est. speed input: 44266.82 toks/s, output: 43.23 toks/s]
Processed prompts:  70%|███████   | 1442/2048 [00:33<00:16, 37.42it/s, est. speed input: 44191.85 toks/s, output: 43.16 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [00:33<00:15, 37.95it/s, est. speed input: 44143.85 toks/s, output: 43.11 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [00:34<00:15, 37.80it/s, est. speed input: 44071.60 toks/s, output: 43.04 toks/s]
Processed prompts:  73%|███████▎  | 1490/2048 [00:34<00:14, 37.62it/s, est. speed input: 43997.61 toks/s, output: 42.97 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [00:35<00:14, 37.49it/s, est. speed input: 43924.83 toks/s, output: 42.90 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [00:35<00:13, 38.06it/s, est. speed input: 43884.66 toks/s, output: 42.86 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [00:35<00:13, 37.78it/s, est. speed input: 43814.48 toks/s, output: 42.79 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [00:36<00:12, 38.30it/s, est. speed input: 43777.91 toks/s, output: 42.75 toks/s]
Processed prompts:  77%|███████▋  | 1570/2048 [00:36<00:12, 37.94it/s, est. speed input: 43710.58 toks/s, output: 42.69 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [00:37<00:12, 37.75it/s, est. speed input: 43646.93 toks/s, output: 42.62 toks/s]
Processed prompts:  78%|███████▊  | 1602/2048 [00:37<00:11, 37.57it/s, est. speed input: 43582.82 toks/s, output: 42.56 toks/s]
Processed prompts:  79%|███████▉  | 1618/2048 [00:38<00:11, 38.10it/s, est. speed input: 43548.33 toks/s, output: 42.53 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [00:38<00:10, 37.85it/s, est. speed input: 43488.19 toks/s, output: 42.47 toks/s]
Processed prompts:  81%|████████  | 1650/2048 [00:38<00:10, 37.57it/s, est. speed input: 43425.06 toks/s, output: 42.41 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [00:39<00:10, 37.49it/s, est. speed input: 43368.18 toks/s, output: 42.35 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [00:39<00:09, 37.38it/s, est. speed input: 43310.27 toks/s, output: 42.30 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [00:40<00:09, 37.35it/s, est. speed input: 43255.54 toks/s, output: 42.24 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [00:40<00:08, 37.26it/s, est. speed input: 43198.79 toks/s, output: 42.19 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [00:41<00:08, 37.88it/s, est. speed input: 43170.85 toks/s, output: 42.16 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [00:41<00:07, 38.32it/s, est. speed input: 43143.14 toks/s, output: 42.13 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [00:41<00:07, 37.93it/s, est. speed input: 43089.48 toks/s, output: 42.08 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [00:42<00:07, 37.72it/s, est. speed input: 43039.16 toks/s, output: 42.03 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [00:42<00:06, 37.57it/s, est. speed input: 42989.57 toks/s, output: 41.98 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [00:43<00:06, 37.46it/s, est. speed input: 42940.92 toks/s, output: 41.93 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [00:43<00:05, 37.38it/s, est. speed input: 42892.93 toks/s, output: 41.89 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [00:44<00:05, 37.33it/s, est. speed input: 42846.37 toks/s, output: 41.84 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [00:44<00:05, 37.29it/s, est. speed input: 42800.31 toks/s, output: 41.80 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [00:44<00:04, 37.24it/s, est. speed input: 42754.45 toks/s, output: 41.75 toks/s]
Processed prompts:  92%|█████████▏| 1890/2048 [00:45<00:04, 37.88it/s, est. speed input: 42733.48 toks/s, output: 41.73 toks/s]
Processed prompts:  93%|█████████▎| 1906/2048 [00:45<00:03, 37.63it/s, est. speed input: 42688.20 toks/s, output: 41.69 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [00:46<00:03, 37.53it/s, est. speed input: 42646.53 toks/s, output: 41.65 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [00:46<00:02, 37.35it/s, est. speed input: 42601.71 toks/s, output: 41.60 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [00:47<00:02, 37.37it/s, est. speed input: 42562.65 toks/s, output: 41.57 toks/s]
Processed prompts:  96%|█████████▌| 1970/2048 [00:47<00:02, 37.27it/s, est. speed input: 42520.54 toks/s, output: 41.52 toks/s]
Processed prompts:  97%|█████████▋| 1986/2048 [00:47<00:01, 37.89it/s, est. speed input: 42502.36 toks/s, output: 41.51 toks/s]
Processed prompts:  98%|█████████▊| 2002/2048 [00:48<00:01, 37.67it/s, est. speed input: 42462.51 toks/s, output: 41.47 toks/s]
Processed prompts:  99%|█████████▊| 2018/2048 [00:48<00:00, 37.46it/s, est. speed input: 42421.76 toks/s, output: 41.43 toks/s]
Processed prompts:  99%|█████████▉| 2034/2048 [00:49<00:00, 38.16it/s, est. speed input: 42408.98 toks/s, output: 41.41 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:49<00:00, 38.16it/s, est. speed input: 42700.40 toks/s, output: 41.70 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:49<00:00, 41.70it/s, est. speed input: 42700.40 toks/s, output: 41.70 toks/s]
[rank0]:[W126 12:29:13.980652061 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 106.4s

测试结果:
  Requests/s:   37.58
  Tokens/s:     38517.92
  Total Reqs:   2048
  Elapsed:      54.50s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     38480.34

============================================================
[7/8] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 12:29:44 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1361187) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1361187) WARNING 01-26 12:30:01 [backends.py:609] Failed to read file <frozen os>
Throughput: 37.29 requests/s, 38224.13 total tokens/s, 37.29 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-26 12:29:44] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 12:29:44] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:29:44] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 12:29:44] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:29:44] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:29:44] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:29:44] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:29:44] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:29:44] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:29:44] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 12:29:44] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 12:29:44] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 12:29:44] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 12:29:44] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 12:29:51] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 12:29:52] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:29:52] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 12:29:52] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:29:52] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:29:52] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:29:52] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:29:52] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:29:52] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:29:52] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 12:29:52] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 12:29:52] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 12:29:52] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 12:29:52] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1361187) [2026-01-26 12:29:53] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1361187) [2026-01-26 12:29:53] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1361187) [2026-01-26 12:29:53] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1361187) [2026-01-26 12:29:53] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=1361187) [2026-01-26 12:29:53] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=1361187) [2026-01-26 12:29:53] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=1361187) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1361187) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.86it/s]
(EngineCore_DP0 pid=1361187) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.34it/s]
(EngineCore_DP0 pid=1361187) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.39it/s]
(EngineCore_DP0 pid=1361187) 
(EngineCore_DP0 pid=1361187) [2026-01-26 12:29:55] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=1361187) [2026-01-26 12:29:55] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16588800 bytes
(EngineCore_DP0 pid=1361187) [2026-01-26 12:29:55] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=1361187) [2026-01-26 12:29:55] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 12902400 bytes
(EngineCore_DP0 pid=1361187) [2026-01-26 12:29:55] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=1361187) [2026-01-26 12:29:55] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 136396800 bytes
(EngineCore_DP0 pid=1361187) [2026-01-26 12:29:55] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=1361187) [2026-01-26 12:29:55] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 67952640 bytes
(EngineCore_DP0 pid=1361187) [rank0]:W0126 12:30:07.082000 1361187 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1361187) [rank0]:W0126 12:30:07.160000 1361187 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1361187) [rank0]:W0126 12:30:08.139000 1361187 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1361187) [rank0]:W0126 12:30:08.265000 1361187 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1361187) 2026-01-26 12:30:11,910 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1361187) 2026-01-26 12:30:11,937 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1361187) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▉         | 1/11 [00:00<00:05,  1.77it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:04,  2.12it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:01<00:01,  4.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 6/11 [00:01<00:00,  7.41it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:01<00:00,  9.91it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 10/11 [00:01<00:00, 12.07it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00,  7.19it/s]
(EngineCore_DP0 pid=1361187) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00, 19.21it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00,  9.55it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00,  9.88it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 10.46it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 25/4096 [00:00<00:16, 243.30it/s]
Adding requests:   2%|▏         | 65/4096 [00:00<00:12, 331.90it/s]
Adding requests:   2%|▏         | 101/4096 [00:00<00:11, 341.35it/s]
Adding requests:   3%|▎         | 139/4096 [00:00<00:11, 354.86it/s]
Adding requests:   4%|▍         | 178/4096 [00:00<00:10, 366.13it/s]
Adding requests:   5%|▌         | 219/4096 [00:00<00:10, 380.16it/s]
Adding requests:   6%|▋         | 258/4096 [00:00<00:10, 373.34it/s]
Adding requests:   7%|▋         | 298/4096 [00:00<00:10, 379.48it/s]
Adding requests:   8%|▊         | 338/4096 [00:00<00:09, 384.36it/s]
Adding requests:   9%|▉         | 378/4096 [00:01<00:09, 386.33it/s]
Adding requests:  10%|█         | 420/4096 [00:01<00:09, 393.72it/s]
Adding requests:  11%|█         | 460/4096 [00:01<00:09, 388.67it/s]
Adding requests:  12%|█▏        | 502/4096 [00:01<00:09, 396.11it/s]
Adding requests:  13%|█▎        | 545/4096 [00:01<00:08, 402.84it/s]
Adding requests:  14%|█▍        | 586/4096 [00:01<00:08, 398.42it/s]
Adding requests:  15%|█▌        | 626/4096 [00:01<00:08, 395.29it/s]
Adding requests:  16%|█▋        | 666/4096 [00:01<00:08, 387.51it/s]
Adding requests:  17%|█▋        | 706/4096 [00:01<00:08, 390.28it/s]
Adding requests:  18%|█▊        | 746/4096 [00:01<00:08, 384.76it/s]
Adding requests:  19%|█▉        | 786/4096 [00:02<00:08, 388.34it/s]
Adding requests:  20%|██        | 825/4096 [00:02<00:08, 386.62it/s]
Adding requests:  21%|██        | 866/4096 [00:02<00:08, 391.46it/s]
Adding requests:  22%|██▏       | 907/4096 [00:02<00:08, 394.63it/s]
Adding requests:  23%|██▎       | 947/4096 [00:02<00:08, 385.54it/s]
Adding requests:  24%|██▍       | 986/4096 [00:02<00:08, 386.74it/s]
Adding requests:  25%|██▌       | 1025/4096 [00:02<00:08, 382.18it/s]
Adding requests:  26%|██▌       | 1064/4096 [00:02<00:07, 382.25it/s]
Adding requests:  27%|██▋       | 1103/4096 [00:02<00:08, 373.57it/s]
Adding requests:  28%|██▊       | 1143/4096 [00:02<00:07, 379.32it/s]
Adding requests:  29%|██▉       | 1182/4096 [00:03<00:07, 379.61it/s]
Adding requests:  30%|██▉       | 1222/4096 [00:03<00:07, 385.11it/s]
Adding requests:  31%|███       | 1261/4096 [00:03<00:07, 383.69it/s]
Adding requests:  32%|███▏      | 1300/4096 [00:03<00:07, 381.04it/s]
Adding requests:  33%|███▎      | 1339/4096 [00:03<00:07, 383.11it/s]
Adding requests:  34%|███▎      | 1379/4096 [00:03<00:07, 387.57it/s]
Adding requests:  35%|███▍      | 1418/4096 [00:03<00:06, 382.92it/s]
Adding requests:  36%|███▌      | 1458/4096 [00:03<00:06, 387.56it/s]
Adding requests:  37%|███▋      | 1499/4096 [00:03<00:06, 390.95it/s]
Adding requests:  38%|███▊      | 1539/4096 [00:04<00:06, 391.56it/s]
Adding requests:  39%|███▊      | 1579/4096 [00:04<00:06, 383.21it/s]
Adding requests:  40%|███▉      | 1618/4096 [00:04<00:06, 378.51it/s]
Adding requests:  40%|████      | 1656/4096 [00:04<00:06, 372.93it/s]
Adding requests:  41%|████▏     | 1694/4096 [00:04<00:06, 373.28it/s]
Adding requests:  42%|████▏     | 1734/4096 [00:04<00:06, 378.98it/s]
Adding requests:  43%|████▎     | 1775/4096 [00:04<00:05, 387.83it/s]
Adding requests:  44%|████▍     | 1814/4096 [00:04<00:05, 382.81it/s]
Adding requests:  45%|████▌     | 1853/4096 [00:04<00:05, 384.10it/s]
Adding requests:  46%|████▌     | 1893/4096 [00:04<00:05, 386.96it/s]
Adding requests:  47%|████▋     | 1934/4096 [00:05<00:05, 391.33it/s]
Adding requests:  48%|████▊     | 1974/4096 [00:05<00:05, 391.44it/s]
Adding requests:  49%|████▉     | 2014/4096 [00:05<00:05, 387.94it/s]
Adding requests:  50%|█████     | 2053/4096 [00:05<00:05, 380.81it/s]
Adding requests:  51%|█████     | 2092/4096 [00:05<00:05, 373.69it/s]
Adding requests:  52%|█████▏    | 2132/4096 [00:05<00:05, 378.55it/s]
Adding requests:  53%|█████▎    | 2170/4096 [00:05<00:05, 375.02it/s]
Adding requests:  54%|█████▍    | 2208/4096 [00:05<00:05, 371.45it/s]
Adding requests:  55%|█████▍    | 2247/4096 [00:05<00:04, 375.62it/s]
Adding requests:  56%|█████▌    | 2288/4096 [00:05<00:04, 384.49it/s]
Adding requests:  57%|█████▋    | 2327/4096 [00:06<00:04, 379.77it/s]
Adding requests:  58%|█████▊    | 2367/4096 [00:06<00:04, 383.56it/s]
Adding requests:  59%|█████▉    | 2408/4096 [00:06<00:04, 389.99it/s]
Adding requests:  60%|█████▉    | 2448/4096 [00:06<00:04, 391.10it/s]
Adding requests:  61%|██████    | 2488/4096 [00:06<00:04, 392.05it/s]
Adding requests:  62%|██████▏   | 2529/4096 [00:06<00:03, 394.89it/s]
Adding requests:  63%|██████▎   | 2572/4096 [00:06<00:03, 403.93it/s]
Adding requests:  64%|██████▍   | 2613/4096 [00:06<00:03, 400.81it/s]
Adding requests:  65%|██████▍   | 2654/4096 [00:06<00:03, 390.92it/s]
Adding requests:  66%|██████▌   | 2694/4096 [00:07<00:03, 387.56it/s]
Adding requests:  67%|██████▋   | 2733/4096 [00:07<00:03, 385.73it/s]
Adding requests:  68%|██████▊   | 2774/4096 [00:07<00:03, 391.82it/s]
Adding requests:  69%|██████▉   | 2816/4096 [00:07<00:03, 398.39it/s]
Adding requests:  70%|██████▉   | 2856/4096 [00:07<00:03, 396.68it/s]
Adding requests:  71%|███████   | 2896/4096 [00:07<00:03, 393.92it/s]
Adding requests:  72%|███████▏  | 2937/4096 [00:07<00:02, 398.00it/s]
Adding requests:  73%|███████▎  | 2977/4096 [00:07<00:02, 395.91it/s]
Adding requests:  74%|███████▎  | 3018/4096 [00:07<00:02, 397.66it/s]
Adding requests:  75%|███████▍  | 3059/4096 [00:07<00:02, 398.96it/s]
Adding requests:  76%|███████▌  | 3100/4096 [00:08<00:02, 401.48it/s]
Adding requests:  77%|███████▋  | 3141/4096 [00:08<00:02, 401.78it/s]
Adding requests:  78%|███████▊  | 3182/4096 [00:08<00:02, 394.02it/s]
Adding requests:  79%|███████▊  | 3222/4096 [00:08<00:02, 392.40it/s]
Adding requests:  80%|███████▉  | 3262/4096 [00:08<00:02, 393.18it/s]
Adding requests:  81%|████████  | 3302/4096 [00:08<00:02, 382.07it/s]
Adding requests:  82%|████████▏ | 3341/4096 [00:08<00:01, 383.06it/s]
Adding requests:  83%|████████▎ | 3381/4096 [00:08<00:01, 387.00it/s]
Adding requests:  84%|████████▎ | 3421/4096 [00:08<00:01, 389.22it/s]
Adding requests:  84%|████████▍ | 3461/4096 [00:08<00:01, 389.23it/s]
Adding requests:  85%|████████▌ | 3500/4096 [00:09<00:01, 389.19it/s]
Adding requests:  86%|████████▋ | 3543/4096 [00:09<00:01, 399.10it/s]
Adding requests:  87%|████████▋ | 3583/4096 [00:09<00:01, 395.60it/s]
Adding requests:  88%|████████▊ | 3623/4096 [00:09<00:01, 396.63it/s]
Adding requests:  89%|████████▉ | 3663/4096 [00:09<00:01, 393.30it/s]
Adding requests:  90%|█████████ | 3703/4096 [00:09<00:01, 380.51it/s]
Adding requests:  91%|█████████▏| 3742/4096 [00:09<00:00, 381.16it/s]
Adding requests:  92%|█████████▏| 3781/4096 [00:09<00:00, 372.99it/s]
Adding requests:  93%|█████████▎| 3819/4096 [00:09<00:00, 365.80it/s]
Adding requests:  94%|█████████▍| 3858/4096 [00:10<00:00, 371.79it/s]
Adding requests:  95%|█████████▌| 3896/4096 [00:10<00:00, 372.60it/s]
Adding requests:  96%|█████████▌| 3934/4096 [00:10<00:00, 369.21it/s]
Adding requests:  97%|█████████▋| 3972/4096 [00:10<00:00, 371.56it/s]
Adding requests:  98%|█████████▊| 4011/4096 [00:10<00:00, 373.77it/s]
Adding requests:  99%|█████████▉| 4049/4096 [00:10<00:00, 373.31it/s]
Adding requests: 100%|█████████▉| 4087/4096 [00:10<00:00, 373.38it/s]
Adding requests: 100%|██████████| 4096/4096 [00:10<00:00, 384.52it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   9%|▉         | 371/4096 [00:00<00:02, 1802.21it/s, est. speed input: 1845677.99 toks/s, output: 1802.28 toks/s]
Processed prompts:  13%|█▎        | 552/4096 [00:04<00:35, 100.75it/s, est. speed input: 127420.97 toks/s, output: 124.43 toks/s]   
Processed prompts:  15%|█▌        | 629/4096 [00:06<00:50, 68.68it/s, est. speed input: 92185.77 toks/s, output: 90.03 toks/s]   
Processed prompts:  16%|█▋        | 673/4096 [00:07<00:52, 65.68it/s, est. speed input: 87940.61 toks/s, output: 85.88 toks/s]
Processed prompts:  17%|█▋        | 702/4096 [00:08<00:57, 59.28it/s, est. speed input: 82753.26 toks/s, output: 80.81 toks/s]
Processed prompts:  18%|█▊        | 723/4096 [00:09<01:05, 51.50it/s, est. speed input: 77619.60 toks/s, output: 75.80 toks/s]
Processed prompts:  18%|█▊        | 755/4096 [00:10<01:09, 48.15it/s, est. speed input: 74432.25 toks/s, output: 72.69 toks/s]
Processed prompts:  19%|█▉        | 787/4096 [00:11<01:12, 45.65it/s, est. speed input: 71811.87 toks/s, output: 70.13 toks/s]
Processed prompts:  20%|█▉        | 819/4096 [00:12<01:15, 43.42it/s, est. speed input: 69440.65 toks/s, output: 67.81 toks/s]
Processed prompts:  21%|██        | 851/4096 [00:12<01:17, 41.72it/s, est. speed input: 67372.26 toks/s, output: 65.79 toks/s]
Processed prompts:  22%|██▏       | 883/4096 [00:13<01:19, 40.51it/s, est. speed input: 65576.06 toks/s, output: 64.04 toks/s]
Processed prompts:  22%|██▏       | 915/4096 [00:14<01:20, 39.67it/s, est. speed input: 64003.52 toks/s, output: 62.50 toks/s]
Processed prompts:  23%|██▎       | 947/4096 [00:15<01:20, 39.01it/s, est. speed input: 62585.41 toks/s, output: 61.12 toks/s]
Processed prompts:  24%|██▍       | 979/4096 [00:16<01:20, 38.59it/s, est. speed input: 61329.98 toks/s, output: 59.89 toks/s]
Processed prompts:  25%|██▍       | 1011/4096 [00:17<01:20, 38.23it/s, est. speed input: 60181.96 toks/s, output: 58.77 toks/s]
Processed prompts:  25%|██▌       | 1043/4096 [00:18<01:20, 37.98it/s, est. speed input: 59144.40 toks/s, output: 57.76 toks/s]
Processed prompts:  26%|██▌       | 1075/4096 [00:18<01:19, 37.84it/s, est. speed input: 58210.08 toks/s, output: 56.85 toks/s]
Processed prompts:  27%|██▋       | 1107/4096 [00:19<01:19, 37.74it/s, est. speed input: 57354.81 toks/s, output: 56.01 toks/s]
Processed prompts:  28%|██▊       | 1139/4096 [00:20<01:18, 37.62it/s, est. speed input: 56560.82 toks/s, output: 55.24 toks/s]
Processed prompts:  29%|██▊       | 1171/4096 [00:21<01:17, 37.53it/s, est. speed input: 55827.74 toks/s, output: 54.52 toks/s]
Processed prompts:  29%|██▉       | 1203/4096 [00:22<01:16, 37.62it/s, est. speed input: 55180.53 toks/s, output: 53.89 toks/s]
Processed prompts:  30%|███       | 1235/4096 [00:23<01:15, 37.77it/s, est. speed input: 54596.34 toks/s, output: 53.32 toks/s]
Processed prompts:  31%|███       | 1267/4096 [00:24<01:15, 37.66it/s, est. speed input: 54014.99 toks/s, output: 52.75 toks/s]
Processed prompts:  32%|███▏      | 1299/4096 [00:24<01:14, 37.55it/s, est. speed input: 53469.33 toks/s, output: 52.22 toks/s]
Processed prompts:  32%|███▏      | 1331/4096 [00:25<01:13, 37.71it/s, est. speed input: 52997.58 toks/s, output: 51.76 toks/s]
Processed prompts:  33%|███▎      | 1363/4096 [00:26<01:12, 37.57it/s, est. speed input: 52517.20 toks/s, output: 51.29 toks/s]
Processed prompts:  34%|███▍      | 1395/4096 [00:27<01:11, 37.55it/s, est. speed input: 52077.56 toks/s, output: 50.86 toks/s]
Processed prompts:  35%|███▍      | 1427/4096 [00:28<01:11, 37.46it/s, est. speed input: 51655.20 toks/s, output: 50.44 toks/s]
Processed prompts:  36%|███▌      | 1459/4096 [00:29<01:10, 37.59it/s, est. speed input: 51282.37 toks/s, output: 50.08 toks/s]
Processed prompts:  36%|███▋      | 1491/4096 [00:29<01:09, 37.50it/s, est. speed input: 50907.33 toks/s, output: 49.71 toks/s]
Processed prompts:  37%|███▋      | 1523/4096 [00:30<01:08, 37.63it/s, est. speed input: 50577.41 toks/s, output: 49.39 toks/s]
Processed prompts:  38%|███▊      | 1555/4096 [00:31<01:07, 37.78it/s, est. speed input: 50272.04 toks/s, output: 49.09 toks/s]
Processed prompts:  39%|███▊      | 1587/4096 [00:32<01:06, 37.59it/s, est. speed input: 49948.84 toks/s, output: 48.78 toks/s]
Processed prompts:  40%|███▉      | 1619/4096 [00:33<01:05, 37.77it/s, est. speed input: 49676.67 toks/s, output: 48.51 toks/s]
Processed prompts:  40%|████      | 1651/4096 [00:34<01:05, 37.55it/s, est. speed input: 49380.75 toks/s, output: 48.22 toks/s]
Processed prompts:  41%|████      | 1683/4096 [00:35<01:04, 37.44it/s, est. speed input: 49103.24 toks/s, output: 47.95 toks/s]
Processed prompts:  42%|████▏     | 1715/4096 [00:35<01:03, 37.53it/s, est. speed input: 48856.58 toks/s, output: 47.71 toks/s]
Processed prompts:  43%|████▎     | 1747/4096 [00:36<01:02, 37.68it/s, est. speed input: 48629.67 toks/s, output: 47.49 toks/s]
Processed prompts:  43%|████▎     | 1779/4096 [00:37<01:01, 37.52it/s, est. speed input: 48387.95 toks/s, output: 47.25 toks/s]
Processed prompts:  44%|████▍     | 1811/4096 [00:38<01:01, 37.41it/s, est. speed input: 48156.33 toks/s, output: 47.03 toks/s]
Processed prompts:  45%|████▍     | 1843/4096 [00:39<01:00, 37.36it/s, est. speed input: 47938.10 toks/s, output: 46.81 toks/s]
Processed prompts:  46%|████▌     | 1875/4096 [00:40<00:59, 37.50it/s, est. speed input: 47743.97 toks/s, output: 46.62 toks/s]
Processed prompts:  47%|████▋     | 1907/4096 [00:41<00:58, 37.40it/s, est. speed input: 47540.89 toks/s, output: 46.43 toks/s]
Processed prompts:  47%|████▋     | 1939/4096 [00:41<00:57, 37.34it/s, est. speed input: 47347.20 toks/s, output: 46.24 toks/s]
Processed prompts:  48%|████▊     | 1971/4096 [00:42<00:56, 37.45it/s, est. speed input: 47173.82 toks/s, output: 46.07 toks/s]
Processed prompts:  49%|████▉     | 2003/4096 [00:43<00:56, 37.33it/s, est. speed input: 46991.13 toks/s, output: 45.89 toks/s]
Processed prompts:  50%|████▉     | 2035/4096 [00:44<00:55, 37.28it/s, est. speed input: 46818.78 toks/s, output: 45.72 toks/s]
Processed prompts:  50%|█████     | 2067/4096 [00:45<00:54, 37.43it/s, est. speed input: 46667.13 toks/s, output: 45.57 toks/s]
Processed prompts:  51%|█████     | 2099/4096 [00:46<00:53, 37.36it/s, est. speed input: 46507.28 toks/s, output: 45.42 toks/s]
Processed prompts:  52%|█████▏    | 2131/4096 [00:47<00:52, 37.28it/s, est. speed input: 46351.33 toks/s, output: 45.26 toks/s]
Processed prompts:  53%|█████▎    | 2163/4096 [00:47<00:51, 37.25it/s, est. speed input: 46202.20 toks/s, output: 45.12 toks/s]
Processed prompts:  54%|█████▎    | 2195/4096 [00:48<00:50, 37.40it/s, est. speed input: 46071.76 toks/s, output: 44.99 toks/s]
Processed prompts:  54%|█████▍    | 2227/4096 [00:49<00:50, 37.32it/s, est. speed input: 45931.99 toks/s, output: 44.86 toks/s]
Processed prompts:  55%|█████▌    | 2259/4096 [00:50<00:49, 37.26it/s, est. speed input: 45796.97 toks/s, output: 44.72 toks/s]
Processed prompts:  56%|█████▌    | 2291/4096 [00:51<00:48, 37.24it/s, est. speed input: 45667.54 toks/s, output: 44.60 toks/s]
Processed prompts:  57%|█████▋    | 2323/4096 [00:52<00:47, 37.19it/s, est. speed input: 45540.13 toks/s, output: 44.47 toks/s]
Processed prompts:  57%|█████▋    | 2355/4096 [00:53<00:46, 37.13it/s, est. speed input: 45415.38 toks/s, output: 44.35 toks/s]
Processed prompts:  58%|█████▊    | 2387/4096 [00:53<00:46, 37.15it/s, est. speed input: 45298.62 toks/s, output: 44.24 toks/s]
Processed prompts:  59%|█████▉    | 2419/4096 [00:54<00:45, 37.09it/s, est. speed input: 45180.47 toks/s, output: 44.12 toks/s]
Processed prompts:  60%|█████▉    | 2451/4096 [00:55<00:44, 37.07it/s, est. speed input: 45067.82 toks/s, output: 44.01 toks/s]
Processed prompts:  61%|██████    | 2483/4096 [00:56<00:43, 37.06it/s, est. speed input: 44958.79 toks/s, output: 43.91 toks/s]
Processed prompts:  61%|██████▏   | 2515/4096 [00:57<00:42, 37.34it/s, est. speed input: 44870.45 toks/s, output: 43.82 toks/s]
Processed prompts:  62%|██████▏   | 2547/4096 [00:58<00:41, 37.26it/s, est. speed input: 44767.79 toks/s, output: 43.72 toks/s]
Processed prompts:  63%|██████▎   | 2579/4096 [00:59<00:40, 37.42it/s, est. speed input: 44681.23 toks/s, output: 43.63 toks/s]
Processed prompts:  64%|██████▎   | 2611/4096 [00:59<00:39, 37.30it/s, est. speed input: 44583.83 toks/s, output: 43.54 toks/s]
Processed prompts:  65%|██████▍   | 2643/4096 [01:00<00:39, 37.25it/s, est. speed input: 44490.72 toks/s, output: 43.45 toks/s]
Processed prompts:  65%|██████▌   | 2675/4096 [01:01<00:38, 37.20it/s, est. speed input: 44399.37 toks/s, output: 43.36 toks/s]
Processed prompts:  66%|██████▌   | 2707/4096 [01:02<00:37, 37.13it/s, est. speed input: 44309.00 toks/s, output: 43.27 toks/s]
Processed prompts:  67%|██████▋   | 2739/4096 [01:03<00:36, 37.33it/s, est. speed input: 44234.03 toks/s, output: 43.20 toks/s]
Processed prompts:  68%|██████▊   | 2771/4096 [01:04<00:35, 37.24it/s, est. speed input: 44149.26 toks/s, output: 43.11 toks/s]
Processed prompts:  68%|██████▊   | 2803/4096 [01:05<00:34, 37.14it/s, est. speed input: 44064.56 toks/s, output: 43.03 toks/s]
Processed prompts:  69%|██████▉   | 2835/4096 [01:06<00:33, 37.11it/s, est. speed input: 43984.14 toks/s, output: 42.95 toks/s]
Processed prompts:  70%|██████▉   | 2867/4096 [01:06<00:33, 37.05it/s, est. speed input: 43904.03 toks/s, output: 42.87 toks/s]
Processed prompts:  71%|███████   | 2899/4096 [01:07<00:31, 37.91it/s, est. speed input: 43870.34 toks/s, output: 42.84 toks/s]
Processed prompts:  72%|███████▏  | 2931/4096 [01:08<00:30, 37.63it/s, est. speed input: 43794.81 toks/s, output: 42.77 toks/s]
Processed prompts:  72%|███████▏  | 2963/4096 [01:09<00:30, 37.46it/s, est. speed input: 43721.97 toks/s, output: 42.70 toks/s]
Processed prompts:  73%|███████▎  | 2995/4096 [01:10<00:29, 37.31it/s, est. speed input: 43649.83 toks/s, output: 42.63 toks/s]
Processed prompts:  74%|███████▍  | 3027/4096 [01:11<00:28, 37.20it/s, est. speed input: 43578.72 toks/s, output: 42.56 toks/s]
Processed prompts:  75%|███████▍  | 3059/4096 [01:11<00:27, 37.11it/s, est. speed input: 43509.07 toks/s, output: 42.49 toks/s]
Processed prompts:  75%|███████▌  | 3091/4096 [01:12<00:27, 37.07it/s, est. speed input: 43442.00 toks/s, output: 42.42 toks/s]
Processed prompts:  76%|███████▌  | 3123/4096 [01:13<00:26, 37.04it/s, est. speed input: 43376.50 toks/s, output: 42.36 toks/s]
Processed prompts:  77%|███████▋  | 3155/4096 [01:14<00:25, 37.00it/s, est. speed input: 43311.58 toks/s, output: 42.30 toks/s]
Processed prompts:  78%|███████▊  | 3187/4096 [01:15<00:24, 36.94it/s, est. speed input: 43246.80 toks/s, output: 42.23 toks/s]
Processed prompts:  79%|███████▊  | 3219/4096 [01:16<00:23, 36.89it/s, est. speed input: 43183.07 toks/s, output: 42.17 toks/s]
Processed prompts:  79%|███████▉  | 3251/4096 [01:17<00:22, 36.88it/s, est. speed input: 43122.00 toks/s, output: 42.11 toks/s]
Processed prompts:  80%|████████  | 3283/4096 [01:18<00:22, 36.86it/s, est. speed input: 43061.65 toks/s, output: 42.05 toks/s]
Processed prompts:  81%|████████  | 3315/4096 [01:18<00:21, 36.88it/s, est. speed input: 43003.84 toks/s, output: 42.00 toks/s]
Processed prompts:  82%|████████▏ | 3347/4096 [01:19<00:20, 36.86it/s, est. speed input: 42946.29 toks/s, output: 41.94 toks/s]
Processed prompts:  82%|████████▏ | 3379/4096 [01:20<00:19, 36.83it/s, est. speed input: 42888.76 toks/s, output: 41.88 toks/s]
Processed prompts:  83%|████████▎ | 3411/4096 [01:21<00:18, 36.81it/s, est. speed input: 42832.84 toks/s, output: 41.83 toks/s]
Processed prompts:  84%|████████▍ | 3443/4096 [01:22<00:17, 36.78it/s, est. speed input: 42777.52 toks/s, output: 41.77 toks/s]
Processed prompts:  85%|████████▍ | 3475/4096 [01:23<00:16, 36.79it/s, est. speed input: 42724.60 toks/s, output: 41.72 toks/s]
Processed prompts:  86%|████████▌ | 3507/4096 [01:24<00:16, 36.81it/s, est. speed input: 42673.06 toks/s, output: 41.67 toks/s]
Processed prompts:  86%|████████▋ | 3539/4096 [01:25<00:15, 37.04it/s, est. speed input: 42631.27 toks/s, output: 41.63 toks/s]
Processed prompts:  87%|████████▋ | 3571/4096 [01:25<00:14, 36.96it/s, est. speed input: 42580.64 toks/s, output: 41.58 toks/s]
Processed prompts:  88%|████████▊ | 3603/4096 [01:26<00:13, 36.91it/s, est. speed input: 42531.58 toks/s, output: 41.53 toks/s]
Processed prompts:  89%|████████▊ | 3635/4096 [01:27<00:12, 36.86it/s, est. speed input: 42482.85 toks/s, output: 41.49 toks/s]
Processed prompts:  90%|████████▉ | 3667/4096 [01:28<00:11, 37.09it/s, est. speed input: 42444.83 toks/s, output: 41.45 toks/s]
Processed prompts:  90%|█████████ | 3699/4096 [01:29<00:10, 37.00it/s, est. speed input: 42398.61 toks/s, output: 41.40 toks/s]
Processed prompts:  91%|█████████ | 3731/4096 [01:30<00:09, 36.92it/s, est. speed input: 42352.32 toks/s, output: 41.36 toks/s]
Processed prompts:  92%|█████████▏| 3763/4096 [01:31<00:09, 36.88it/s, est. speed input: 42307.71 toks/s, output: 41.32 toks/s]
Processed prompts:  93%|█████████▎| 3795/4096 [01:31<00:08, 36.85it/s, est. speed input: 42263.61 toks/s, output: 41.27 toks/s]
Processed prompts:  93%|█████████▎| 3827/4096 [01:32<00:07, 36.83it/s, est. speed input: 42220.59 toks/s, output: 41.23 toks/s]
Processed prompts:  94%|█████████▍| 3859/4096 [01:33<00:06, 36.81it/s, est. speed input: 42178.05 toks/s, output: 41.19 toks/s]
Processed prompts:  95%|█████████▍| 3891/4096 [01:34<00:05, 36.81it/s, est. speed input: 42136.70 toks/s, output: 41.15 toks/s]
Processed prompts:  96%|█████████▌| 3923/4096 [01:35<00:04, 37.44it/s, est. speed input: 42118.00 toks/s, output: 41.13 toks/s]
Processed prompts:  97%|█████████▋| 3955/4096 [01:36<00:03, 37.22it/s, est. speed input: 42076.81 toks/s, output: 41.09 toks/s]
Processed prompts:  97%|█████████▋| 3987/4096 [01:37<00:02, 37.37it/s, est. speed input: 42046.70 toks/s, output: 41.06 toks/s]
Processed prompts:  98%|█████████▊| 4019/4096 [01:37<00:02, 37.21it/s, est. speed input: 42008.36 toks/s, output: 41.02 toks/s]
Processed prompts:  99%|█████████▉| 4051/4096 [01:38<00:01, 37.37it/s, est. speed input: 41979.62 toks/s, output: 41.00 toks/s]
Processed prompts: 100%|█████████▉| 4083/4096 [01:39<00:00, 45.15it/s, est. speed input: 42155.72 toks/s, output: 41.17 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [01:39<00:00, 45.15it/s, est. speed input: 42289.67 toks/s, output: 41.30 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [01:39<00:00, 41.30it/s, est. speed input: 42289.67 toks/s, output: 41.30 toks/s]
[rank0]:[W126 12:32:05.991828768 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 171.6s

测试结果:
  Requests/s:   37.29
  Tokens/s:     38224.13
  Total Reqs:   4096
  Elapsed:      109.84s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     38186.84

============================================================
[8/8] 测试 M=65536
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 65536
│   M_prefill     = 65536 (= 64 x 1024)
│   M_decode      = 64
│   batched_tokens = 65536 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 8192
│   --max-num-seqs           = 64
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 65536
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 12:32:58 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1364190) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1364190) WARNING 01-26 12:33:15 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]     self.model_runner.profile_run()
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]     hidden_states = self.model(
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]                     ^^^^^^^^^^^
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]     def forward(
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]     raise e
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]   File "<eval_with_key>.58", line 325, in forward
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]     return range_entry.runnable(*args)
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]     return self._compiled_fn(*args)
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]     return compiled_fn(full_args)
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]                             ^^^^^^^
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 724, in inner_fn
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]     outs = compiled_fn(args)
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]            ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]     return self.current_callable(inputs)
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]     out = model(new_inputs)
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]   File "/tmp/torchinductor_root/kv/ckvsudmg3oshri77cui5ty5sl2ycn552zf2cmu7poci642f5nfto.py", line 1093, in call
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]     buf17 = torch.ops.slidesparse.quant_slide_fp8.default(buf16, 'Qwen2.5-7B-FP8', 10)
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 521, in _quant_slide_fp8_impl
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]     return fn(input, L)
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]            ^^^^^^^^^^^^
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]   File "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/H100_cc90_py312_cu129_x86_64/quant_slide_tuned_Qwen2.5-7B.py", line 233, in quant_slide_fp8_triton
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]     _quant_slide_fp8_kernel[(M,)](
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 756, in run
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]     launch_metadata = kernel.launch_metadata(grid, stream, *bound_args.values())
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 490, in launch_metadata
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]     self._init_handles()
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 473, in _init_handles
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]     self.module, self.function, self.n_regs, self.n_spills, self.n_max_threads = driver.active.utils.load_binary(
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866]                                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1364190) ERROR 01-26 12:33:23 [core.py:866] RuntimeError: Triton Error [CUDA]: an illegal memory access was encountered


─── STDERR ───
[2026-01-26 12:32:57] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 12:32:58] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:32:58] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 12:32:58] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:32:58] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:32:58] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:32:58] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:32:58] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:32:58] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:32:58] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 12:32:58] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 12:32:58] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 12:32:58] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 12:32:58] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 12:33:04] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 12:33:05] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:33:05] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-26 12:33:05] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:33:05] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:33:05] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:33:05] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:33:05] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-26 12:33:05] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-26 12:33:05] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 12:33:05] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 12:33:05] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 12:33:05] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 12:33:05] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1364190) [2026-01-26 12:33:07] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1364190) [2026-01-26 12:33:07] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1364190) [2026-01-26 12:33:07] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1364190) [2026-01-26 12:33:07] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=1364190) [2026-01-26 12:33:07] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=1364190) [2026-01-26 12:33:07] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=1364190) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1364190) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.86it/s]
(EngineCore_DP0 pid=1364190) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.33it/s]
(EngineCore_DP0 pid=1364190) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.39it/s]
(EngineCore_DP0 pid=1364190) 
(EngineCore_DP0 pid=1364190) [2026-01-26 12:33:09] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=1364190) [2026-01-26 12:33:09] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16588800 bytes
(EngineCore_DP0 pid=1364190) [2026-01-26 12:33:09] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=1364190) [2026-01-26 12:33:09] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 12902400 bytes
(EngineCore_DP0 pid=1364190) [2026-01-26 12:33:09] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=1364190) [2026-01-26 12:33:09] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 136396800 bytes
(EngineCore_DP0 pid=1364190) [2026-01-26 12:33:09] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=1364190) [2026-01-26 12:33:09] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 67952640 bytes
(EngineCore_DP0 pid=1364190) [rank0]:W0126 12:33:21.277000 1364190 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1364190) [rank0]:W0126 12:33:21.355000 1364190 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1364190) [rank0]:W0126 12:33:22.678000 1364190 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1364190) [rank0]:W0126 12:33:22.800000 1364190 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1364190) Process EngineCore_DP0:
(EngineCore_DP0 pid=1364190) Traceback (most recent call last):
(EngineCore_DP0 pid=1364190)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=1364190)     self.run()
(EngineCore_DP0 pid=1364190)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=1364190)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=1364190)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=1364190)     raise e
(EngineCore_DP0 pid=1364190)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1364190)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1364190)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1364190)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1364190)     super().__init__(
(EngineCore_DP0 pid=1364190)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=1364190)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=1364190)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1364190)   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=1364190)     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=1364190)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1364190)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=1364190)     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=1364190)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1364190)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=1364190)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=1364190)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1364190)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=1364190)     return func(*args, **kwargs)
(EngineCore_DP0 pid=1364190)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1364190)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=1364190)     return func(*args, **kwargs)
(EngineCore_DP0 pid=1364190)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1364190)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=1364190)     self.model_runner.profile_run()
(EngineCore_DP0 pid=1364190)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=1364190)     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=1364190)                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1364190)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=1364190)     return func(*args, **kwargs)
(EngineCore_DP0 pid=1364190)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1364190)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=1364190)     outputs = self.model(
(EngineCore_DP0 pid=1364190)               ^^^^^^^^^^^
(EngineCore_DP0 pid=1364190)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=1364190)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=1364190)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1364190)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=1364190)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=1364190)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1364190)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=1364190)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=1364190)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1364190)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=1364190)     hidden_states = self.model(
(EngineCore_DP0 pid=1364190)                     ^^^^^^^^^^^
(EngineCore_DP0 pid=1364190)   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=1364190)     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=1364190)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1364190)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=1364190)     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=1364190)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1364190)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=1364190)     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=1364190)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1364190)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
(EngineCore_DP0 pid=1364190)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=1364190)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1364190)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
(EngineCore_DP0 pid=1364190)     def forward(
(EngineCore_DP0 pid=1364190)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=1364190)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=1364190)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1364190)   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=1364190)     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=1364190)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1364190)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=1364190)     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=1364190)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1364190)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=1364190)     raise e
(EngineCore_DP0 pid=1364190)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=1364190)     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=1364190)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1364190)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=1364190)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=1364190)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1364190)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=1364190)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=1364190)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1364190)   File "<eval_with_key>.58", line 325, in forward
(EngineCore_DP0 pid=1364190)     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
(EngineCore_DP0 pid=1364190)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1364190)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=1364190)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=1364190)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1364190)   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=1364190)     return range_entry.runnable(*args)
(EngineCore_DP0 pid=1364190)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1364190)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=1364190)     return self._compiled_fn(*args)
(EngineCore_DP0 pid=1364190)            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1364190)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=1364190)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=1364190)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1364190)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=1364190)     return compiled_fn(full_args)
(EngineCore_DP0 pid=1364190)            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1364190)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=1364190)     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=1364190)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1364190)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=1364190)     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=1364190)                             ^^^^^^^
(EngineCore_DP0 pid=1364190)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 724, in inner_fn
(EngineCore_DP0 pid=1364190)     outs = compiled_fn(args)
(EngineCore_DP0 pid=1364190)            ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1364190)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=1364190)     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=1364190)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1364190)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=1364190)     return self.current_callable(inputs)
(EngineCore_DP0 pid=1364190)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1364190)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=1364190)     out = model(new_inputs)
(EngineCore_DP0 pid=1364190)           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1364190)   File "/tmp/torchinductor_root/kv/ckvsudmg3oshri77cui5ty5sl2ycn552zf2cmu7poci642f5nfto.py", line 1093, in call
(EngineCore_DP0 pid=1364190)     buf17 = torch.ops.slidesparse.quant_slide_fp8.default(buf16, 'Qwen2.5-7B-FP8', 10)
(EngineCore_DP0 pid=1364190)             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1364190)   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
(EngineCore_DP0 pid=1364190)     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=1364190)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1364190)   File "/root/vllmbench/slidesparse/core/kernels.py", line 521, in _quant_slide_fp8_impl
(EngineCore_DP0 pid=1364190)     return fn(input, L)
(EngineCore_DP0 pid=1364190)            ^^^^^^^^^^^^
(EngineCore_DP0 pid=1364190)   File "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/H100_cc90_py312_cu129_x86_64/quant_slide_tuned_Qwen2.5-7B.py", line 233, in quant_slide_fp8_triton
(EngineCore_DP0 pid=1364190)     _quant_slide_fp8_kernel[(M,)](
(EngineCore_DP0 pid=1364190)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=1364190)     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=1364190)                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1364190)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 756, in run
(EngineCore_DP0 pid=1364190)     launch_metadata = kernel.launch_metadata(grid, stream, *bound_args.values())
(EngineCore_DP0 pid=1364190)                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1364190)   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 490, in launch_metadata
(EngineCore_DP0 pid=1364190)     self._init_handles()
(EngineCore_DP0 pid=1364190)   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 473, in _init_handles
(EngineCore_DP0 pid=1364190)     self.module, self.function, self.n_regs, self.n_spills, self.n_max_threads = driver.active.utils.load_binary(
(EngineCore_DP0 pid=1364190)                                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1364190) RuntimeError: Triton Error [CUDA]: an illegal memory access was encountered
[rank0]:[W126 12:33:24.414817257 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[ERROR] 测试失败: M=65536 (exit code: 1)


------------------------------------------------------------
  生成 CSV: Qwen2.5-7B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/Qwen2.5-7B-FP8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,37.0997,19032.1485,3.4502
1024,1024,1,128,128,30.5207,31283.7119,4.1939
2048,1024,2,256,128,34.1267,34979.8240,7.5015
4096,1024,4,512,128,35.6591,36550.5442,14.3582
8192,1024,8,1024,128,36.3975,37307.4796,28.1338
16384,1024,16,2048,128,37.5785,38517.9216,54.4993
32768,1024,32,4096,128,37.2918,38224.1325,109.8364
65536,1024,64,8192,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[INFO] 完成: 7 成功, 1 失败


============================================================
  Benchmark 完成!
============================================================


总计: 35 成功, 5 失败
============================================================
