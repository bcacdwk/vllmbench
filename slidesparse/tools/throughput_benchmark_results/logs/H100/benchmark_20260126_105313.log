======================================================================
SlideSparse vLLM Throughput Benchmark Log
Created: 2026-01-26 10:53:13
======================================================================

原始命令:
  /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-int8 --backend cublaslt,cusparselt --stage prefill --sparsity 2_4,2_6,2_8,2_10 --M 512,1024,2048,4096,8192,16384,32768,65536

命令行参数:
  --model: qwen2.5-7b-int8
  --backend: cublaslt,cusparselt
  --sparsity: 2_4,2_6,2_8,2_10
  --stage: prefill
  --M: 512,1024,2048,4096,8192,16384,32768,65536
  --N: None
  --inner-32: False
  --eager: False
  --gpu-id: 0
  --gpu-mem: 0.8
  --dry-run: False
  --list-models: False

硬件信息:
  GPU: H100
  Compute Capability: cc90
  VRAM: 79.2 GB
  CUDA: 12.9
  Python: py312

Backend 环境变量 (初始状态):
  DISABLE_SLIDESPARSE: 未设置
  USE_CUBLASLT: 未设置
  USE_CUSPARSELT: 未设置
  SPARSITY: 未设置
  INNER_DTYPE_32: 未设置

======================================================================


============================================================
  Qwen2.5-7B-INT8 | cuBLASLt | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints/Qwen2.5-7B-INT8
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_INT8_py312_cu129_x86_64/cublaslt

============================================================
[1/8] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 10:53:26 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1243426) WARNING 01-26 10:53:42 [backends.py:609] Failed to read file <frozen os>
Throughput: 28.37 requests/s, 14551.74 total tokens/s, 28.37 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-26 10:53:25] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 10:53:26] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 10:53:26] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 10:53:26] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:53:26] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:53:26] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:53:26] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:53:26] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:53:26] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 10:53:26] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 10:53:26] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 10:53:26] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 10:53:26] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 10:53:26] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 10:53:32] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 10:53:33] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 10:53:33] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 10:53:33] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:53:33] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:53:33] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:53:33] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:53:33] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:53:33] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 10:53:33] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 10:53:33] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 10:53:33] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 10:53:33] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 10:53:33] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1243426) [2026-01-26 10:53:35] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=1243426) [2026-01-26 10:53:35] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1243426) [2026-01-26 10:53:35] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=1243426) [2026-01-26 10:53:35] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=1243426) [2026-01-26 10:53:35] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=1243426) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1243426) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.59it/s]
(EngineCore_DP0 pid=1243426) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  1.93it/s]
(EngineCore_DP0 pid=1243426) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.01it/s]
(EngineCore_DP0 pid=1243426) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=1243426) 2026-01-26 10:53:56,332 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1243426) 2026-01-26 10:53:56,388 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1243426) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  1.33it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:01<00:00,  1.15it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:01<00:00,  1.18it/s]
(EngineCore_DP0 pid=1243426) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 15.93it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  27%|██▋       | 35/128 [00:00<00:00, 348.42it/s]
Adding requests:  83%|████████▎ | 106/128 [00:00<00:00, 556.98it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 549.03it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<01:15,  1.68it/s, est. speed input: 861.91 toks/s, output: 1.68 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:14,  8.78it/s, est. speed input: 3588.78 toks/s, output: 7.01 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:08, 14.69it/s, est. speed input: 5519.17 toks/s, output: 10.78 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:05, 19.66it/s, est. speed input: 7001.89 toks/s, output: 13.68 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:01<00:04, 23.60it/s, est. speed input: 8165.19 toks/s, output: 15.95 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:01<00:04, 26.59it/s, est. speed input: 9097.41 toks/s, output: 17.77 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:01<00:03, 28.82it/s, est. speed input: 9863.70 toks/s, output: 19.26 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:01<00:03, 30.48it/s, est. speed input: 10506.74 toks/s, output: 20.52 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:01<00:02, 31.67it/s, est. speed input: 11051.30 toks/s, output: 21.58 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:01<00:02, 32.49it/s, est. speed input: 11516.15 toks/s, output: 22.49 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:01<00:02, 33.13it/s, est. speed input: 11924.82 toks/s, output: 23.29 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:01<00:02, 33.57it/s, est. speed input: 12281.98 toks/s, output: 23.99 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:01<00:02, 33.84it/s, est. speed input: 12595.06 toks/s, output: 24.60 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:02<00:02, 34.07it/s, est. speed input: 12875.93 toks/s, output: 25.15 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:02<00:02, 34.21it/s, est. speed input: 13126.56 toks/s, output: 25.64 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:02<00:01, 34.28it/s, est. speed input: 13350.21 toks/s, output: 26.07 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:02<00:01, 34.38it/s, est. speed input: 13556.15 toks/s, output: 26.48 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:02<00:01, 34.49it/s, est. speed input: 13745.40 toks/s, output: 26.85 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:02<00:01, 34.49it/s, est. speed input: 13914.92 toks/s, output: 27.18 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:02<00:01, 34.54it/s, est. speed input: 14072.36 toks/s, output: 27.48 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:02<00:01, 34.56it/s, est. speed input: 14216.98 toks/s, output: 27.77 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:03<00:01, 34.54it/s, est. speed input: 14348.82 toks/s, output: 28.02 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:03<00:01, 34.57it/s, est. speed input: 14472.81 toks/s, output: 28.27 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:03<00:01, 34.58it/s, est. speed input: 14587.96 toks/s, output: 28.49 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:03<00:00, 34.55it/s, est. speed input: 14692.85 toks/s, output: 28.70 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:03<00:00, 34.55it/s, est. speed input: 14792.31 toks/s, output: 28.89 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:03<00:00, 34.57it/s, est. speed input: 14885.94 toks/s, output: 29.07 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:03<00:00, 34.54it/s, est. speed input: 14971.81 toks/s, output: 29.24 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:03<00:00, 34.55it/s, est. speed input: 15054.31 toks/s, output: 29.40 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:03<00:00, 34.57it/s, est. speed input: 15132.08 toks/s, output: 29.55 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:04<00:00, 34.54it/s, est. speed input: 15203.97 toks/s, output: 29.69 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:04<00:00, 34.55it/s, est. speed input: 15272.77 toks/s, output: 29.83 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:04<00:00, 34.55it/s, est. speed input: 15321.31 toks/s, output: 29.92 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:04<00:00, 29.92it/s, est. speed input: 15321.31 toks/s, output: 29.92 toks/s]
[rank0]:[W126 10:54:04.470709166 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 53.4s

测试结果:
  Requests/s:   28.37
  Tokens/s:     14551.74
  Total Reqs:   128
  Elapsed:      4.51s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     14523.37

============================================================
[2/8] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 10:54:14 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1244904) WARNING 01-26 10:54:30 [backends.py:609] Failed to read file <frozen os>
Throughput: 28.42 requests/s, 29132.98 total tokens/s, 28.42 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-26 10:54:13] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 10:54:14] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 10:54:14] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 10:54:14] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:54:14] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:54:14] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:54:14] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:54:14] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:54:14] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 10:54:14] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 10:54:14] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 10:54:14] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 10:54:14] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 10:54:14] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 10:54:21] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 10:54:22] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 10:54:22] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 10:54:22] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:54:22] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:54:22] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:54:22] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:54:22] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:54:22] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 10:54:22] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 10:54:22] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 10:54:22] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 10:54:22] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 10:54:22] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1244904) [2026-01-26 10:54:23] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=1244904) [2026-01-26 10:54:24] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1244904) [2026-01-26 10:54:24] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=1244904) [2026-01-26 10:54:24] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=1244904) [2026-01-26 10:54:24] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=1244904) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1244904) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.57it/s]
(EngineCore_DP0 pid=1244904) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.92it/s]
(EngineCore_DP0 pid=1244904) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  2.00it/s]
(EngineCore_DP0 pid=1244904) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=1244904) 2026-01-26 10:54:41,706 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1244904) 2026-01-26 10:54:41,759 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1244904) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  9.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  9.38it/s]
(EngineCore_DP0 pid=1244904) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 15.51it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  16%|█▌        | 20/128 [00:00<00:00, 197.70it/s]
Adding requests:  45%|████▌     | 58/128 [00:00<00:00, 302.02it/s]
Adding requests:  73%|███████▎  | 94/128 [00:00<00:00, 327.68it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 324.58it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   3%|▎         | 4/128 [00:00<00:03, 33.84it/s, est. speed input: 34659.12 toks/s, output: 33.84 toks/s]
Processed prompts:   6%|▋         | 8/128 [00:00<00:03, 32.36it/s, est. speed input: 33360.85 toks/s, output: 32.58 toks/s]
Processed prompts:   9%|▉         | 12/128 [00:00<00:03, 31.87it/s, est. speed input: 32910.74 toks/s, output: 32.14 toks/s]
Processed prompts:  12%|█▎        | 16/128 [00:00<00:03, 31.52it/s, est. speed input: 32611.42 toks/s, output: 31.85 toks/s]
Processed prompts:  16%|█▌        | 20/128 [00:00<00:03, 31.45it/s, est. speed input: 32502.06 toks/s, output: 31.74 toks/s]
Processed prompts:  19%|█▉        | 24/128 [00:00<00:03, 31.38it/s, est. speed input: 32418.90 toks/s, output: 31.66 toks/s]
Processed prompts:  22%|██▏       | 28/128 [00:00<00:03, 31.36it/s, est. speed input: 32370.24 toks/s, output: 31.61 toks/s]
Processed prompts:  25%|██▌       | 32/128 [00:01<00:03, 31.07it/s, est. speed input: 32219.00 toks/s, output: 31.46 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:01<00:02, 31.01it/s, est. speed input: 32150.08 toks/s, output: 31.40 toks/s]
Processed prompts:  31%|███▏      | 40/128 [00:01<00:02, 31.08it/s, est. speed input: 32133.55 toks/s, output: 31.38 toks/s]
Processed prompts:  34%|███▍      | 44/128 [00:01<00:02, 31.13it/s, est. speed input: 32121.67 toks/s, output: 31.37 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:01<00:02, 31.15it/s, est. speed input: 32105.36 toks/s, output: 31.35 toks/s]
Processed prompts:  41%|████      | 52/128 [00:01<00:02, 31.19it/s, est. speed input: 32101.20 toks/s, output: 31.35 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:01<00:02, 31.20it/s, est. speed input: 32090.37 toks/s, output: 31.34 toks/s]
Processed prompts:  47%|████▋     | 60/128 [00:01<00:02, 31.12it/s, est. speed input: 32063.43 toks/s, output: 31.31 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:02<00:02, 31.02it/s, est. speed input: 32029.69 toks/s, output: 31.28 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:02<00:01, 31.02it/s, est. speed input: 32013.73 toks/s, output: 31.26 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:02<00:01, 31.07it/s, est. speed input: 32009.11 toks/s, output: 31.26 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:02<00:01, 31.11it/s, est. speed input: 32006.51 toks/s, output: 31.26 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:02<00:01, 31.10it/s, est. speed input: 31996.85 toks/s, output: 31.25 toks/s]
Processed prompts:  66%|██████▌   | 84/128 [00:02<00:01, 31.08it/s, est. speed input: 31987.15 toks/s, output: 31.24 toks/s]
Processed prompts:  69%|██████▉   | 88/128 [00:02<00:01, 31.11it/s, est. speed input: 31983.80 toks/s, output: 31.23 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:02<00:01, 31.03it/s, est. speed input: 31965.94 toks/s, output: 31.22 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:03<00:01, 31.00it/s, est. speed input: 31953.34 toks/s, output: 31.20 toks/s]
Processed prompts:  78%|███████▊  | 100/128 [00:03<00:00, 30.99it/s, est. speed input: 31944.03 toks/s, output: 31.20 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:03<00:00, 31.06it/s, est. speed input: 31945.59 toks/s, output: 31.20 toks/s]
Processed prompts:  84%|████████▍ | 108/128 [00:03<00:00, 31.07it/s, est. speed input: 31940.98 toks/s, output: 31.19 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:03<00:00, 31.05it/s, est. speed input: 31935.08 toks/s, output: 31.19 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:03<00:00, 31.10it/s, est. speed input: 31935.45 toks/s, output: 31.19 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:03<00:00, 31.06it/s, est. speed input: 31927.82 toks/s, output: 31.18 toks/s]
Processed prompts:  97%|█████████▋| 124/128 [00:03<00:00, 31.02it/s, est. speed input: 31919.76 toks/s, output: 31.17 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:04<00:00, 31.01it/s, est. speed input: 31913.07 toks/s, output: 31.16 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:04<00:00, 31.01it/s, est. speed input: 31913.07 toks/s, output: 31.16 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:04<00:00, 31.16it/s, est. speed input: 31913.07 toks/s, output: 31.16 toks/s]
[rank0]:[W126 10:54:48.335852240 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 43.8s

测试结果:
  Requests/s:   28.42
  Tokens/s:     29132.98
  Total Reqs:   128
  Elapsed:      4.50s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     29104.56

============================================================
[3/8] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 10:54:58 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1246019) WARNING 01-26 10:55:14 [backends.py:609] Failed to read file <frozen os>
Throughput: 35.64 requests/s, 36530.02 total tokens/s, 35.64 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-26 10:54:58] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 10:54:58] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 10:54:58] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 10:54:58] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:54:58] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:54:58] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:54:58] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:54:58] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:54:58] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 10:54:58] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 10:54:58] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 10:54:58] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 10:54:58] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 10:54:58] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 10:55:05] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 10:55:06] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 10:55:06] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 10:55:06] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:55:06] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:55:06] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:55:06] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:55:06] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:55:06] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 10:55:06] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 10:55:06] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 10:55:06] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 10:55:06] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 10:55:06] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1246019) [2026-01-26 10:55:07] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=1246019) [2026-01-26 10:55:07] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1246019) [2026-01-26 10:55:07] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=1246019) [2026-01-26 10:55:07] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=1246019) [2026-01-26 10:55:07] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=1246019) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1246019) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.60it/s]
(EngineCore_DP0 pid=1246019) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  1.94it/s]
(EngineCore_DP0 pid=1246019) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.01it/s]
(EngineCore_DP0 pid=1246019) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=1246019) 2026-01-26 10:55:25,388 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1246019) 2026-01-26 10:55:25,414 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1246019) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:00<00:00, 16.04it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 10.05it/s]
(EngineCore_DP0 pid=1246019) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 1/2 [00:00<00:00,  9.88it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00,  2.61it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00,  2.93it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   8%|▊         | 21/256 [00:00<00:01, 205.14it/s]
Adding requests:  24%|██▍       | 62/256 [00:00<00:00, 322.80it/s]
Adding requests:  38%|███▊      | 96/256 [00:00<00:00, 329.89it/s]
Adding requests:  52%|█████▏    | 134/256 [00:00<00:00, 347.09it/s]
Adding requests:  68%|██████▊   | 174/256 [00:00<00:00, 364.83it/s]
Adding requests:  84%|████████▎ | 214/256 [00:00<00:00, 374.94it/s]
Adding requests:  99%|█████████▉| 253/256 [00:00<00:00, 377.74it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 356.39it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   6%|▋         | 16/256 [00:00<00:01, 140.14it/s, est. speed input: 143562.77 toks/s, output: 140.16 toks/s]
Processed prompts:  12%|█▏        | 31/256 [00:00<00:03, 58.14it/s, est. speed input: 65472.77 toks/s, output: 63.94 toks/s]   
Processed prompts:  16%|█▌        | 40/256 [00:00<00:04, 47.16it/s, est. speed input: 54674.63 toks/s, output: 53.39 toks/s]
Processed prompts:  18%|█▊        | 46/256 [00:00<00:04, 44.21it/s, est. speed input: 51650.98 toks/s, output: 50.44 toks/s]
Processed prompts:  20%|██        | 52/256 [00:01<00:04, 42.19it/s, est. speed input: 49615.43 toks/s, output: 48.45 toks/s]
Processed prompts:  22%|██▏       | 57/256 [00:01<00:04, 43.20it/s, est. speed input: 49472.87 toks/s, output: 48.31 toks/s]
Processed prompts:  24%|██▍       | 62/256 [00:01<00:04, 39.54it/s, est. speed input: 47429.56 toks/s, output: 46.32 toks/s]
Processed prompts:  26%|██▌       | 67/256 [00:01<00:04, 41.31it/s, est. speed input: 47500.21 toks/s, output: 46.39 toks/s]
Processed prompts:  28%|██▊       | 72/256 [00:01<00:04, 38.08it/s, est. speed input: 45994.36 toks/s, output: 44.92 toks/s]
Processed prompts:  30%|██▉       | 76/256 [00:01<00:04, 37.99it/s, est. speed input: 45535.78 toks/s, output: 44.47 toks/s]
Processed prompts:  31%|███▏      | 80/256 [00:01<00:04, 37.94it/s, est. speed input: 45136.88 toks/s, output: 44.08 toks/s]
Processed prompts:  33%|███▎      | 84/256 [00:01<00:04, 37.73it/s, est. speed input: 44739.09 toks/s, output: 43.69 toks/s]
Processed prompts:  34%|███▍      | 88/256 [00:02<00:04, 37.57it/s, est. speed input: 44384.39 toks/s, output: 43.34 toks/s]
Processed prompts:  36%|███▌      | 92/256 [00:02<00:04, 37.48it/s, est. speed input: 44071.60 toks/s, output: 43.04 toks/s]
Processed prompts:  38%|███▊      | 96/256 [00:02<00:04, 37.51it/s, est. speed input: 43806.83 toks/s, output: 42.78 toks/s]
Processed prompts:  39%|███▉      | 100/256 [00:02<00:04, 37.59it/s, est. speed input: 43575.54 toks/s, output: 42.55 toks/s]
Processed prompts:  41%|████      | 104/256 [00:02<00:04, 37.63it/s, est. speed input: 43362.10 toks/s, output: 42.35 toks/s]
Processed prompts:  42%|████▏     | 108/256 [00:02<00:03, 37.66it/s, est. speed input: 43166.65 toks/s, output: 42.15 toks/s]
Processed prompts:  44%|████▍     | 112/256 [00:02<00:03, 37.66it/s, est. speed input: 42984.06 toks/s, output: 41.98 toks/s]
Processed prompts:  45%|████▌     | 116/256 [00:02<00:03, 37.69it/s, est. speed input: 42818.18 toks/s, output: 41.81 toks/s]
Processed prompts:  47%|████▋     | 120/256 [00:02<00:03, 37.65it/s, est. speed input: 42657.30 toks/s, output: 41.66 toks/s]
Processed prompts:  48%|████▊     | 124/256 [00:02<00:03, 37.55it/s, est. speed input: 42498.34 toks/s, output: 41.50 toks/s]
Processed prompts:  50%|█████     | 128/256 [00:03<00:03, 37.53it/s, est. speed input: 42356.31 toks/s, output: 41.36 toks/s]
Processed prompts:  52%|█████▏    | 132/256 [00:03<00:03, 37.53it/s, est. speed input: 42225.08 toks/s, output: 41.23 toks/s]
Processed prompts:  53%|█████▎    | 136/256 [00:03<00:03, 37.58it/s, est. speed input: 42108.79 toks/s, output: 41.12 toks/s]
Processed prompts:  55%|█████▍    | 140/256 [00:03<00:03, 37.55it/s, est. speed input: 41992.03 toks/s, output: 41.01 toks/s]
Processed prompts:  56%|█████▋    | 144/256 [00:03<00:02, 37.58it/s, est. speed input: 41888.68 toks/s, output: 40.91 toks/s]
Processed prompts:  58%|█████▊    | 148/256 [00:03<00:02, 37.64it/s, est. speed input: 41795.33 toks/s, output: 40.82 toks/s]
Processed prompts:  59%|█████▉    | 152/256 [00:03<00:02, 37.65it/s, est. speed input: 41703.88 toks/s, output: 40.73 toks/s]
Processed prompts:  61%|██████    | 156/256 [00:03<00:02, 37.58it/s, est. speed input: 41609.68 toks/s, output: 40.63 toks/s]
Processed prompts:  62%|██████▎   | 160/256 [00:03<00:02, 37.50it/s, est. speed input: 41517.21 toks/s, output: 40.54 toks/s]
Processed prompts:  64%|██████▍   | 164/256 [00:04<00:02, 37.49it/s, est. speed input: 41433.77 toks/s, output: 40.46 toks/s]
Processed prompts:  66%|██████▌   | 168/256 [00:04<00:02, 37.46it/s, est. speed input: 41352.93 toks/s, output: 40.38 toks/s]
Processed prompts:  67%|██████▋   | 172/256 [00:04<00:02, 37.46it/s, est. speed input: 41278.03 toks/s, output: 40.31 toks/s]
Processed prompts:  69%|██████▉   | 176/256 [00:04<00:02, 37.48it/s, est. speed input: 41208.43 toks/s, output: 40.24 toks/s]
Processed prompts:  70%|███████   | 180/256 [00:04<00:02, 37.52it/s, est. speed input: 41144.45 toks/s, output: 40.18 toks/s]
Processed prompts:  72%|███████▏  | 184/256 [00:04<00:01, 37.62it/s, est. speed input: 41089.34 toks/s, output: 40.13 toks/s]
Processed prompts:  73%|███████▎  | 188/256 [00:04<00:01, 37.61it/s, est. speed input: 41030.96 toks/s, output: 40.07 toks/s]
Processed prompts:  75%|███████▌  | 192/256 [00:04<00:01, 37.62it/s, est. speed input: 40976.04 toks/s, output: 40.02 toks/s]
Processed prompts:  77%|███████▋  | 196/256 [00:04<00:01, 37.62it/s, est. speed input: 40922.86 toks/s, output: 39.96 toks/s]
Processed prompts:  78%|███████▊  | 200/256 [00:05<00:01, 37.51it/s, est. speed input: 40863.33 toks/s, output: 39.91 toks/s]
Processed prompts:  80%|████████  | 206/256 [00:05<00:01, 39.50it/s, est. speed input: 40950.69 toks/s, output: 39.99 toks/s]
Processed prompts:  82%|████████▏ | 210/256 [00:05<00:01, 38.90it/s, est. speed input: 40895.96 toks/s, output: 39.94 toks/s]
Processed prompts:  84%|████████▎ | 214/256 [00:05<00:01, 38.52it/s, est. speed input: 40848.03 toks/s, output: 39.89 toks/s]
Processed prompts:  85%|████████▌ | 218/256 [00:05<00:00, 38.26it/s, est. speed input: 40802.92 toks/s, output: 39.85 toks/s]
Processed prompts:  87%|████████▋ | 222/256 [00:05<00:00, 38.06it/s, est. speed input: 40758.29 toks/s, output: 39.80 toks/s]
Processed prompts:  88%|████████▊ | 226/256 [00:05<00:00, 37.89it/s, est. speed input: 40713.82 toks/s, output: 39.76 toks/s]
Processed prompts:  90%|████████▉ | 230/256 [00:05<00:00, 37.80it/s, est. speed input: 40672.69 toks/s, output: 39.72 toks/s]
Processed prompts:  91%|█████████▏| 234/256 [00:05<00:00, 37.72it/s, est. speed input: 40632.28 toks/s, output: 39.68 toks/s]
Processed prompts:  93%|█████████▎| 238/256 [00:06<00:00, 37.66it/s, est. speed input: 40593.34 toks/s, output: 39.64 toks/s]
Processed prompts:  95%|█████████▍| 242/256 [00:06<00:00, 37.62it/s, est. speed input: 40555.48 toks/s, output: 39.60 toks/s]
Processed prompts:  96%|█████████▌| 246/256 [00:06<00:00, 37.57it/s, est. speed input: 40517.54 toks/s, output: 39.57 toks/s]
Processed prompts:  98%|█████████▊| 250/256 [00:06<00:00, 37.59it/s, est. speed input: 40484.70 toks/s, output: 39.54 toks/s]
Processed prompts:  99%|█████████▉| 254/256 [00:06<00:00, 37.58it/s, est. speed input: 40450.72 toks/s, output: 39.50 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:06<00:00, 37.58it/s, est. speed input: 40561.00 toks/s, output: 39.61 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:06<00:00, 39.61it/s, est. speed input: 40561.00 toks/s, output: 39.61 toks/s]
[rank0]:[W126 10:55:35.385726228 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 46.8s

测试结果:
  Requests/s:   35.64
  Tokens/s:     36530.02
  Total Reqs:   256
  Elapsed:      7.18s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     36494.38

============================================================
[4/8] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 10:55:47 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1247201) WARNING 01-26 10:56:03 [backends.py:609] Failed to read file <frozen os>
Throughput: 38.99 requests/s, 39967.25 total tokens/s, 38.99 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-26 10:55:46] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 10:55:47] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 10:55:47] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 10:55:47] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:55:47] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:55:47] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:55:47] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:55:47] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:55:47] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 10:55:47] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 10:55:47] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 10:55:47] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 10:55:47] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 10:55:47] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 10:55:53] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 10:55:54] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 10:55:54] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 10:55:54] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:55:54] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:55:54] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:55:54] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:55:54] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:55:54] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 10:55:54] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 10:55:54] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 10:55:54] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 10:55:54] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 10:55:54] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1247201) [2026-01-26 10:55:56] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=1247201) [2026-01-26 10:55:56] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1247201) [2026-01-26 10:55:56] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=1247201) [2026-01-26 10:55:56] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=1247201) [2026-01-26 10:55:56] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=1247201) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1247201) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.56it/s]
(EngineCore_DP0 pid=1247201) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  1.93it/s]
(EngineCore_DP0 pid=1247201) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.00it/s]
(EngineCore_DP0 pid=1247201) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=1247201) 2026-01-26 10:56:15,025 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1247201) 2026-01-26 10:56:15,063 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1247201) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00, 14.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00,  9.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 10.02it/s]
(EngineCore_DP0 pid=1247201) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00, 16.28it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 11.37it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   4%|▎         | 19/512 [00:00<00:02, 189.09it/s]
Adding requests:  12%|█▏        | 59/512 [00:00<00:01, 311.30it/s]
Adding requests:  19%|█▉        | 96/512 [00:00<00:01, 335.81it/s]
Adding requests:  26%|██▌       | 134/512 [00:00<00:01, 352.50it/s]
Adding requests:  34%|███▍      | 174/512 [00:00<00:00, 368.32it/s]
Adding requests:  42%|████▏     | 215/512 [00:00<00:00, 378.87it/s]
Adding requests:  50%|████▉     | 254/512 [00:00<00:00, 380.45it/s]
Adding requests:  57%|█████▋    | 293/512 [00:00<00:00, 382.60it/s]
Adding requests:  65%|██████▌   | 335/512 [00:00<00:00, 391.92it/s]
Adding requests:  73%|███████▎  | 375/512 [00:01<00:00, 393.86it/s]
Adding requests:  81%|████████▏ | 417/512 [00:01<00:00, 401.04it/s]
Adding requests:  89%|████████▉ | 458/512 [00:01<00:00, 396.73it/s]
Adding requests:  98%|█████████▊| 500/512 [00:01<00:00, 400.91it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 379.06it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  11%|█         | 54/512 [00:00<00:01, 283.80it/s, est. speed input: 290644.90 toks/s, output: 283.81 toks/s]
Processed prompts:  16%|█▌        | 83/512 [00:00<00:05, 78.66it/s, est. speed input: 93784.79 toks/s, output: 91.59 toks/s]   
Processed prompts:  19%|█▉        | 98/512 [00:01<00:06, 61.60it/s, est. speed input: 76345.03 toks/s, output: 74.55 toks/s]
Processed prompts:  21%|██        | 108/512 [00:01<00:06, 58.86it/s, est. speed input: 72899.49 toks/s, output: 71.19 toks/s]
Processed prompts:  23%|██▎       | 116/512 [00:01<00:07, 54.05it/s, est. speed input: 69007.98 toks/s, output: 67.39 toks/s]
Processed prompts:  24%|██▍       | 123/512 [00:01<00:07, 48.77it/s, est. speed input: 65316.91 toks/s, output: 63.79 toks/s]
Processed prompts:  25%|██▌       | 129/512 [00:02<00:07, 50.24it/s, est. speed input: 65022.91 toks/s, output: 63.50 toks/s]
Processed prompts:  26%|██▋       | 135/512 [00:02<00:08, 43.86it/s, est. speed input: 61841.06 toks/s, output: 60.39 toks/s]
Processed prompts:  27%|██▋       | 140/512 [00:02<00:08, 44.84it/s, est. speed input: 61350.85 toks/s, output: 59.91 toks/s]
Processed prompts:  28%|██▊       | 145/512 [00:02<00:08, 45.76it/s, est. speed input: 60901.08 toks/s, output: 59.47 toks/s]
Processed prompts:  29%|██▉       | 150/512 [00:02<00:09, 37.91it/s, est. speed input: 58137.83 toks/s, output: 56.77 toks/s]
Processed prompts:  30%|███       | 155/512 [00:02<00:08, 40.10it/s, est. speed input: 57809.28 toks/s, output: 56.45 toks/s]
Processed prompts:  31%|███▏      | 160/512 [00:02<00:08, 42.03it/s, est. speed input: 57512.41 toks/s, output: 56.16 toks/s]
Processed prompts:  32%|███▏      | 165/512 [00:02<00:07, 43.63it/s, est. speed input: 57237.07 toks/s, output: 55.90 toks/s]
Processed prompts:  33%|███▎      | 170/512 [00:03<00:09, 35.73it/s, est. speed input: 55156.27 toks/s, output: 53.86 toks/s]
Processed prompts:  34%|███▍      | 174/512 [00:03<00:09, 36.57it/s, est. speed input: 54695.01 toks/s, output: 53.41 toks/s]
Processed prompts:  35%|███▍      | 178/512 [00:03<00:08, 37.26it/s, est. speed input: 54261.39 toks/s, output: 52.99 toks/s]
Processed prompts:  36%|███▌      | 182/512 [00:03<00:08, 37.81it/s, est. speed input: 53853.00 toks/s, output: 52.59 toks/s]
Processed prompts:  36%|███▋      | 186/512 [00:03<00:08, 38.17it/s, est. speed input: 53457.82 toks/s, output: 52.20 toks/s]
Processed prompts:  37%|███▋      | 190/512 [00:03<00:08, 38.36it/s, est. speed input: 53073.19 toks/s, output: 51.83 toks/s]
Processed prompts:  38%|███▊      | 194/512 [00:03<00:08, 38.48it/s, est. speed input: 52707.66 toks/s, output: 51.47 toks/s]
Processed prompts:  39%|███▊      | 198/512 [00:03<00:08, 38.54it/s, est. speed input: 52358.18 toks/s, output: 51.13 toks/s]
Processed prompts:  40%|████      | 206/512 [00:04<00:07, 40.57it/s, est. speed input: 51992.44 toks/s, output: 50.77 toks/s]
Processed prompts:  41%|████      | 211/512 [00:04<00:07, 42.70it/s, est. speed input: 51949.28 toks/s, output: 50.73 toks/s]
Processed prompts:  42%|████▏     | 216/512 [00:04<00:06, 44.42it/s, est. speed input: 51912.21 toks/s, output: 50.70 toks/s]
Processed prompts:  43%|████▎     | 221/512 [00:04<00:06, 45.69it/s, est. speed input: 51872.59 toks/s, output: 50.66 toks/s]
Processed prompts:  44%|████▍     | 226/512 [00:04<00:07, 36.27it/s, est. speed input: 50660.82 toks/s, output: 49.47 toks/s]
Processed prompts:  45%|████▍     | 230/512 [00:04<00:07, 36.93it/s, est. speed input: 50426.66 toks/s, output: 49.24 toks/s]
Processed prompts:  46%|████▌     | 234/512 [00:04<00:07, 37.45it/s, est. speed input: 50200.54 toks/s, output: 49.02 toks/s]
Processed prompts:  46%|████▋     | 238/512 [00:04<00:07, 37.77it/s, est. speed input: 49974.52 toks/s, output: 48.80 toks/s]
Processed prompts:  47%|████▋     | 242/512 [00:04<00:07, 38.08it/s, est. speed input: 49765.46 toks/s, output: 48.60 toks/s]
Processed prompts:  48%|████▊     | 246/512 [00:05<00:06, 38.39it/s, est. speed input: 49571.24 toks/s, output: 48.41 toks/s]
Processed prompts:  49%|████▉     | 250/512 [00:05<00:06, 38.53it/s, est. speed input: 49377.70 toks/s, output: 48.22 toks/s]
Processed prompts:  50%|████▉     | 254/512 [00:05<00:06, 38.70it/s, est. speed input: 49197.04 toks/s, output: 48.04 toks/s]
Processed prompts:  50%|█████     | 258/512 [00:05<00:06, 38.86it/s, est. speed input: 49027.14 toks/s, output: 47.88 toks/s]
Processed prompts:  51%|█████     | 262/512 [00:05<00:06, 38.92it/s, est. speed input: 48858.39 toks/s, output: 47.71 toks/s]
Processed prompts:  52%|█████▏    | 266/512 [00:05<00:06, 38.95it/s, est. speed input: 48695.59 toks/s, output: 47.55 toks/s]
Processed prompts:  53%|█████▎    | 270/512 [00:05<00:06, 38.98it/s, est. speed input: 48538.52 toks/s, output: 47.40 toks/s]
Processed prompts:  54%|█████▎    | 274/512 [00:05<00:06, 39.04it/s, est. speed input: 48390.63 toks/s, output: 47.26 toks/s]
Processed prompts:  54%|█████▍    | 278/512 [00:05<00:05, 39.04it/s, est. speed input: 48244.14 toks/s, output: 47.11 toks/s]
Processed prompts:  55%|█████▌    | 282/512 [00:06<00:05, 39.06it/s, est. speed input: 48105.19 toks/s, output: 46.98 toks/s]
Processed prompts:  56%|█████▌    | 286/512 [00:06<00:05, 39.06it/s, est. speed input: 47968.44 toks/s, output: 46.84 toks/s]
Processed prompts:  57%|█████▋    | 290/512 [00:06<00:05, 39.01it/s, est. speed input: 47833.47 toks/s, output: 46.71 toks/s]
Processed prompts:  57%|█████▋    | 294/512 [00:06<00:05, 39.03it/s, est. speed input: 47707.03 toks/s, output: 46.59 toks/s]
Processed prompts:  58%|█████▊    | 298/512 [00:06<00:05, 39.11it/s, est. speed input: 47588.26 toks/s, output: 46.47 toks/s]
Processed prompts:  59%|█████▉    | 302/512 [00:06<00:05, 39.13it/s, est. speed input: 47471.12 toks/s, output: 46.36 toks/s]
Processed prompts:  61%|██████    | 310/512 [00:06<00:04, 41.00it/s, est. speed input: 47388.71 toks/s, output: 46.28 toks/s]
Processed prompts:  62%|██████▏   | 315/512 [00:06<00:04, 43.04it/s, est. speed input: 47429.53 toks/s, output: 46.32 toks/s]
Processed prompts:  62%|██████▎   | 320/512 [00:06<00:04, 44.59it/s, est. speed input: 47467.21 toks/s, output: 46.35 toks/s]
Processed prompts:  63%|██████▎   | 325/512 [00:07<00:04, 45.77it/s, est. speed input: 47504.26 toks/s, output: 46.39 toks/s]
Processed prompts:  64%|██████▍   | 330/512 [00:07<00:05, 36.28it/s, est. speed input: 46861.60 toks/s, output: 45.76 toks/s]
Processed prompts:  65%|██████▌   | 334/512 [00:07<00:04, 36.89it/s, est. speed input: 46761.57 toks/s, output: 45.67 toks/s]
Processed prompts:  66%|██████▌   | 338/512 [00:07<00:04, 37.45it/s, est. speed input: 46668.13 toks/s, output: 45.57 toks/s]
Processed prompts:  67%|██████▋   | 342/512 [00:07<00:04, 37.87it/s, est. speed input: 46577.23 toks/s, output: 45.49 toks/s]
Processed prompts:  68%|██████▊   | 346/512 [00:07<00:04, 38.21it/s, est. speed input: 46489.12 toks/s, output: 45.40 toks/s]
Processed prompts:  68%|██████▊   | 350/512 [00:07<00:04, 38.39it/s, est. speed input: 46399.83 toks/s, output: 45.31 toks/s]
Processed prompts:  69%|██████▉   | 354/512 [00:07<00:04, 38.56it/s, est. speed input: 46314.91 toks/s, output: 45.23 toks/s]
Processed prompts:  70%|██████▉   | 358/512 [00:07<00:03, 38.68it/s, est. speed input: 46231.64 toks/s, output: 45.15 toks/s]
Processed prompts:  71%|███████   | 362/512 [00:08<00:03, 38.77it/s, est. speed input: 46151.25 toks/s, output: 45.07 toks/s]
Processed prompts:  71%|███████▏  | 366/512 [00:08<00:03, 38.85it/s, est. speed input: 46073.61 toks/s, output: 44.99 toks/s]
Processed prompts:  72%|███████▏  | 370/512 [00:08<00:03, 38.89it/s, est. speed input: 45996.66 toks/s, output: 44.92 toks/s]
Processed prompts:  73%|███████▎  | 374/512 [00:08<00:03, 38.95it/s, est. speed input: 45923.54 toks/s, output: 44.85 toks/s]
Processed prompts:  74%|███████▍  | 378/512 [00:08<00:03, 39.02it/s, est. speed input: 45853.32 toks/s, output: 44.78 toks/s]
Processed prompts:  75%|███████▍  | 382/512 [00:08<00:03, 38.97it/s, est. speed input: 45780.17 toks/s, output: 44.71 toks/s]
Processed prompts:  75%|███████▌  | 386/512 [00:08<00:03, 38.91it/s, est. speed input: 45707.74 toks/s, output: 44.64 toks/s]
Processed prompts:  76%|███████▌  | 390/512 [00:08<00:03, 38.94it/s, est. speed input: 45640.19 toks/s, output: 44.57 toks/s]
Processed prompts:  77%|███████▋  | 394/512 [00:08<00:03, 38.98it/s, est. speed input: 45575.08 toks/s, output: 44.51 toks/s]
Processed prompts:  78%|███████▊  | 398/512 [00:08<00:02, 39.00it/s, est. speed input: 45511.26 toks/s, output: 44.44 toks/s]
Processed prompts:  79%|███████▊  | 402/512 [00:09<00:02, 39.03it/s, est. speed input: 45449.25 toks/s, output: 44.38 toks/s]
Processed prompts:  79%|███████▉  | 406/512 [00:09<00:02, 39.03it/s, est. speed input: 45388.00 toks/s, output: 44.32 toks/s]
Processed prompts:  80%|████████  | 410/512 [00:09<00:02, 39.01it/s, est. speed input: 45327.19 toks/s, output: 44.26 toks/s]
Processed prompts:  81%|████████  | 414/512 [00:09<00:02, 38.95it/s, est. speed input: 45265.61 toks/s, output: 44.20 toks/s]
Processed prompts:  82%|████████▏ | 418/512 [00:09<00:02, 38.86it/s, est. speed input: 45203.70 toks/s, output: 44.14 toks/s]
Processed prompts:  82%|████████▏ | 422/512 [00:09<00:02, 38.86it/s, est. speed input: 45145.62 toks/s, output: 44.09 toks/s]
Processed prompts:  83%|████████▎ | 426/512 [00:09<00:02, 38.92it/s, est. speed input: 45090.88 toks/s, output: 44.03 toks/s]
Processed prompts:  84%|████████▍ | 430/512 [00:09<00:02, 38.95it/s, est. speed input: 45037.16 toks/s, output: 43.98 toks/s]
Processed prompts:  86%|████████▌ | 438/512 [00:09<00:01, 41.03it/s, est. speed input: 45032.68 toks/s, output: 43.98 toks/s]
Processed prompts:  87%|████████▋ | 443/512 [00:10<00:01, 43.04it/s, est. speed input: 45083.32 toks/s, output: 44.03 toks/s]
Processed prompts:  88%|████████▊ | 448/512 [00:10<00:01, 44.47it/s, est. speed input: 45127.94 toks/s, output: 44.07 toks/s]
Processed prompts:  88%|████████▊ | 453/512 [00:10<00:01, 45.60it/s, est. speed input: 45174.08 toks/s, output: 44.12 toks/s]
Processed prompts:  89%|████████▉ | 458/512 [00:10<00:01, 36.13it/s, est. speed input: 44773.05 toks/s, output: 43.72 toks/s]
Processed prompts:  90%|█████████ | 462/512 [00:10<00:01, 36.80it/s, est. speed input: 44725.23 toks/s, output: 43.68 toks/s]
Processed prompts:  91%|█████████ | 466/512 [00:10<00:01, 37.44it/s, est. speed input: 44682.56 toks/s, output: 43.64 toks/s]
Processed prompts:  92%|█████████▏| 470/512 [00:10<00:01, 37.91it/s, est. speed input: 44639.64 toks/s, output: 43.59 toks/s]
Processed prompts:  93%|█████████▎| 474/512 [00:10<00:00, 38.26it/s, est. speed input: 44597.20 toks/s, output: 43.55 toks/s]
Processed prompts:  93%|█████████▎| 478/512 [00:10<00:00, 38.47it/s, est. speed input: 44553.72 toks/s, output: 43.51 toks/s]
Processed prompts:  94%|█████████▍| 482/512 [00:11<00:00, 38.67it/s, est. speed input: 44512.87 toks/s, output: 43.47 toks/s]
Processed prompts:  95%|█████████▍| 486/512 [00:11<00:00, 38.85it/s, est. speed input: 44474.01 toks/s, output: 43.43 toks/s]
Processed prompts:  96%|█████████▌| 490/512 [00:11<00:00, 38.88it/s, est. speed input: 44432.33 toks/s, output: 43.39 toks/s]
Processed prompts:  96%|█████████▋| 494/512 [00:11<00:00, 38.94it/s, est. speed input: 44392.79 toks/s, output: 43.35 toks/s]
Processed prompts:  97%|█████████▋| 498/512 [00:11<00:00, 38.95it/s, est. speed input: 44352.49 toks/s, output: 43.31 toks/s]
Processed prompts:  98%|█████████▊| 502/512 [00:11<00:00, 39.08it/s, est. speed input: 44317.38 toks/s, output: 43.28 toks/s]
Processed prompts:  99%|█████████▉| 506/512 [00:11<00:00, 39.21it/s, est. speed input: 44284.20 toks/s, output: 43.25 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:11<00:00, 39.21it/s, est. speed input: 44514.44 toks/s, output: 43.47 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:11<00:00, 43.47it/s, est. speed input: 44514.44 toks/s, output: 43.47 toks/s]
[rank0]:[W126 10:56:30.668684765 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 55.9s

测试结果:
  Requests/s:   38.99
  Tokens/s:     39967.25
  Total Reqs:   512
  Elapsed:      13.13s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     39928.26

============================================================
[5/8] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 10:56:45 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1248481) WARNING 01-26 10:57:00 [backends.py:609] Failed to read file <frozen os>
Throughput: 36.13 requests/s, 37030.60 total tokens/s, 36.13 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-26 10:56:44] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 10:56:45] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 10:56:45] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 10:56:45] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:56:45] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:56:45] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:56:45] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:56:45] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:56:45] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 10:56:45] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 10:56:45] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 10:56:45] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 10:56:45] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 10:56:45] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 10:56:52] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 10:56:52] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 10:56:52] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 10:56:52] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:56:52] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:56:52] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:56:52] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:56:52] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:56:52] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 10:56:52] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 10:56:52] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 10:56:52] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 10:56:52] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 10:56:52] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1248481) [2026-01-26 10:56:53] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=1248481) [2026-01-26 10:56:53] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1248481) [2026-01-26 10:56:53] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=1248481) [2026-01-26 10:56:53] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=1248481) [2026-01-26 10:56:53] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=1248481) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1248481) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.59it/s]
(EngineCore_DP0 pid=1248481) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  1.94it/s]
(EngineCore_DP0 pid=1248481) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.01it/s]
(EngineCore_DP0 pid=1248481) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=1248481) 2026-01-26 10:57:11,939 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1248481) 2026-01-26 10:57:11,968 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1248481) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 1/5 [00:00<00:01,  2.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:01,  2.45it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00,  4.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:01<00:00,  4.78it/s]
(EngineCore_DP0 pid=1248481) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 1/4 [00:00<00:00,  3.47it/s]
Capturing CUDA graphs (decode, FULL):  75%|███████▌  | 3/4 [00:00<00:00,  7.87it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00,  8.23it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 21/1024 [00:00<00:04, 204.98it/s]
Adding requests:   6%|▌         | 61/1024 [00:00<00:03, 317.19it/s]
Adding requests:   9%|▉         | 95/1024 [00:00<00:02, 327.27it/s]
Adding requests:  13%|█▎        | 132/1024 [00:00<00:02, 343.70it/s]
Adding requests:  17%|█▋        | 171/1024 [00:00<00:02, 357.48it/s]
Adding requests:  21%|██        | 212/1024 [00:00<00:02, 372.37it/s]
Adding requests:  25%|██▍       | 251/1024 [00:00<00:02, 375.45it/s]
Adding requests:  28%|██▊       | 290/1024 [00:00<00:01, 377.57it/s]
Adding requests:  32%|███▏      | 330/1024 [00:00<00:01, 384.40it/s]
Adding requests:  36%|███▌      | 370/1024 [00:01<00:01, 388.98it/s]
Adding requests:  40%|████      | 411/1024 [00:01<00:01, 393.09it/s]
Adding requests:  44%|████▍     | 451/1024 [00:01<00:01, 389.36it/s]
Adding requests:  48%|████▊     | 495/1024 [00:01<00:01, 401.25it/s]
Adding requests:  52%|█████▏    | 536/1024 [00:01<00:01, 401.04it/s]
Adding requests:  56%|█████▋    | 577/1024 [00:01<00:01, 398.04it/s]
Adding requests:  60%|██████    | 617/1024 [00:01<00:01, 388.48it/s]
Adding requests:  64%|██████▍   | 656/1024 [00:01<00:00, 383.72it/s]
Adding requests:  68%|██████▊   | 696/1024 [00:01<00:00, 385.76it/s]
Adding requests:  72%|███████▏  | 735/1024 [00:01<00:00, 379.48it/s]
Adding requests:  76%|███████▌  | 774/1024 [00:02<00:00, 381.23it/s]
Adding requests:  79%|███████▉  | 813/1024 [00:02<00:00, 380.71it/s]
Adding requests:  83%|████████▎ | 854/1024 [00:02<00:00, 387.91it/s]
Adding requests:  87%|████████▋ | 894/1024 [00:02<00:00, 388.44it/s]
Adding requests:  91%|█████████ | 933/1024 [00:02<00:00, 381.82it/s]
Adding requests:  95%|█████████▌| 973/1024 [00:02<00:00, 384.56it/s]
Adding requests:  99%|█████████▉| 1012/1024 [00:02<00:00, 377.69it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 378.26it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  10%|▉         | 98/1024 [00:00<00:02, 408.54it/s, est. speed input: 418421.37 toks/s, output: 408.56 toks/s]
Processed prompts:  14%|█▎        | 139/1024 [00:01<00:10, 86.03it/s, est. speed input: 105747.93 toks/s, output: 103.27 toks/s]
Processed prompts:  16%|█▌        | 159/1024 [00:01<00:11, 72.31it/s, est. speed input: 90868.86 toks/s, output: 88.74 toks/s]  
Processed prompts:  17%|█▋        | 172/1024 [00:02<00:14, 58.53it/s, est. speed input: 78826.68 toks/s, output: 76.98 toks/s]
Processed prompts:  18%|█▊        | 181/1024 [00:02<00:15, 55.06it/s, est. speed input: 75386.41 toks/s, output: 73.62 toks/s]
Processed prompts:  18%|█▊        | 188/1024 [00:02<00:16, 50.06it/s, est. speed input: 71789.45 toks/s, output: 70.11 toks/s]
Processed prompts:  19%|█▉        | 194/1024 [00:02<00:18, 44.82it/s, est. speed input: 68459.63 toks/s, output: 66.85 toks/s]
Processed prompts:  20%|█▉        | 202/1024 [00:03<00:18, 43.84it/s, est. speed input: 66740.69 toks/s, output: 65.18 toks/s]
Processed prompts:  21%|██        | 210/1024 [00:03<00:19, 41.78it/s, est. speed input: 64756.56 toks/s, output: 63.24 toks/s]
Processed prompts:  21%|██▏       | 218/1024 [00:03<00:20, 40.09it/s, est. speed input: 62973.49 toks/s, output: 61.50 toks/s]
Processed prompts:  22%|██▏       | 226/1024 [00:03<00:20, 38.97it/s, est. speed input: 61446.16 toks/s, output: 60.01 toks/s]
Processed prompts:  23%|██▎       | 234/1024 [00:03<00:20, 38.25it/s, est. speed input: 60115.96 toks/s, output: 58.71 toks/s]
Processed prompts:  24%|██▎       | 242/1024 [00:04<00:20, 37.70it/s, est. speed input: 58920.14 toks/s, output: 57.54 toks/s]
Processed prompts:  24%|██▍       | 250/1024 [00:04<00:20, 37.23it/s, est. speed input: 57820.10 toks/s, output: 56.46 toks/s]
Processed prompts:  25%|██▌       | 258/1024 [00:04<00:20, 36.84it/s, est. speed input: 56814.02 toks/s, output: 55.48 toks/s]
Processed prompts:  26%|██▌       | 266/1024 [00:04<00:20, 36.61it/s, est. speed input: 55908.52 toks/s, output: 54.60 toks/s]
Processed prompts:  27%|██▋       | 274/1024 [00:05<00:20, 36.53it/s, est. speed input: 55099.30 toks/s, output: 53.81 toks/s]
Processed prompts:  28%|██▊       | 282/1024 [00:05<00:20, 36.40it/s, est. speed input: 54344.22 toks/s, output: 53.07 toks/s]
Processed prompts:  28%|██▊       | 290/1024 [00:05<00:20, 36.22it/s, est. speed input: 53630.22 toks/s, output: 52.37 toks/s]
Processed prompts:  29%|██▉       | 298/1024 [00:05<00:20, 36.16it/s, est. speed input: 52983.40 toks/s, output: 51.74 toks/s]
Processed prompts:  30%|██▉       | 306/1024 [00:05<00:19, 37.27it/s, est. speed input: 52587.68 toks/s, output: 51.35 toks/s]
Processed prompts:  31%|███       | 314/1024 [00:06<00:19, 36.97it/s, est. speed input: 52036.78 toks/s, output: 50.82 toks/s]
Processed prompts:  31%|███▏      | 322/1024 [00:06<00:19, 36.69it/s, est. speed input: 51512.97 toks/s, output: 50.31 toks/s]
Processed prompts:  32%|███▏      | 330/1024 [00:06<00:19, 36.47it/s, est. speed input: 51019.66 toks/s, output: 49.82 toks/s]
Processed prompts:  33%|███▎      | 338/1024 [00:06<00:18, 36.37it/s, est. speed input: 50566.79 toks/s, output: 49.38 toks/s]
Processed prompts:  34%|███▍      | 346/1024 [00:07<00:18, 36.33it/s, est. speed input: 50145.29 toks/s, output: 48.97 toks/s]
Processed prompts:  35%|███▍      | 354/1024 [00:07<00:18, 36.29it/s, est. speed input: 49749.27 toks/s, output: 48.58 toks/s]
Processed prompts:  35%|███▌      | 362/1024 [00:07<00:18, 36.27it/s, est. speed input: 49376.41 toks/s, output: 48.22 toks/s]
Processed prompts:  36%|███▌      | 370/1024 [00:07<00:18, 36.25it/s, est. speed input: 49024.25 toks/s, output: 47.88 toks/s]
Processed prompts:  37%|███▋      | 378/1024 [00:07<00:17, 36.18it/s, est. speed input: 48685.23 toks/s, output: 47.54 toks/s]
Processed prompts:  38%|███▊      | 386/1024 [00:08<00:17, 36.14it/s, est. speed input: 48365.60 toks/s, output: 47.23 toks/s]
Processed prompts:  38%|███▊      | 394/1024 [00:08<00:17, 36.13it/s, est. speed input: 48064.85 toks/s, output: 46.94 toks/s]
Processed prompts:  39%|███▉      | 402/1024 [00:08<00:17, 36.09it/s, est. speed input: 47776.30 toks/s, output: 46.66 toks/s]
Processed prompts:  40%|████      | 410/1024 [00:08<00:17, 36.07it/s, est. speed input: 47502.92 toks/s, output: 46.39 toks/s]
Processed prompts:  41%|████      | 418/1024 [00:09<00:16, 36.09it/s, est. speed input: 47246.10 toks/s, output: 46.14 toks/s]
Processed prompts:  42%|████▏     | 426/1024 [00:09<00:16, 36.11it/s, est. speed input: 47002.46 toks/s, output: 45.90 toks/s]
Processed prompts:  42%|████▏     | 434/1024 [00:09<00:15, 37.25it/s, est. speed input: 46879.82 toks/s, output: 45.78 toks/s]
Processed prompts:  43%|████▎     | 442/1024 [00:09<00:15, 36.88it/s, est. speed input: 46652.32 toks/s, output: 45.56 toks/s]
Processed prompts:  44%|████▍     | 450/1024 [00:09<00:15, 36.65it/s, est. speed input: 46436.82 toks/s, output: 45.35 toks/s]
Processed prompts:  45%|████▍     | 458/1024 [00:10<00:15, 36.46it/s, est. speed input: 46227.42 toks/s, output: 45.14 toks/s]
Processed prompts:  46%|████▌     | 466/1024 [00:10<00:15, 36.39it/s, est. speed input: 46033.36 toks/s, output: 44.95 toks/s]
Processed prompts:  46%|████▋     | 474/1024 [00:10<00:15, 36.31it/s, est. speed input: 45844.34 toks/s, output: 44.77 toks/s]
Processed prompts:  47%|████▋     | 482/1024 [00:10<00:14, 36.24it/s, est. speed input: 45661.32 toks/s, output: 44.59 toks/s]
Processed prompts:  48%|████▊     | 490/1024 [00:11<00:14, 36.22it/s, est. speed input: 45488.59 toks/s, output: 44.42 toks/s]
Processed prompts:  49%|████▊     | 498/1024 [00:11<00:14, 36.18it/s, est. speed input: 45320.32 toks/s, output: 44.26 toks/s]
Processed prompts:  49%|████▉     | 506/1024 [00:11<00:14, 36.10it/s, est. speed input: 45154.71 toks/s, output: 44.10 toks/s]
Processed prompts:  50%|█████     | 514/1024 [00:11<00:14, 36.08it/s, est. speed input: 44998.08 toks/s, output: 43.94 toks/s]
Processed prompts:  51%|█████     | 522/1024 [00:11<00:13, 36.06it/s, est. speed input: 44846.47 toks/s, output: 43.80 toks/s]
Processed prompts:  52%|█████▏    | 530/1024 [00:12<00:13, 36.08it/s, est. speed input: 44703.15 toks/s, output: 43.66 toks/s]
Processed prompts:  53%|█████▎    | 538/1024 [00:12<00:13, 36.09it/s, est. speed input: 44564.70 toks/s, output: 43.52 toks/s]
Processed prompts:  53%|█████▎    | 546/1024 [00:12<00:13, 36.05it/s, est. speed input: 44427.65 toks/s, output: 43.39 toks/s]
Processed prompts:  54%|█████▍    | 554/1024 [00:12<00:13, 36.08it/s, est. speed input: 44299.93 toks/s, output: 43.26 toks/s]
Processed prompts:  55%|█████▍    | 562/1024 [00:13<00:12, 36.10it/s, est. speed input: 44176.16 toks/s, output: 43.14 toks/s]
Processed prompts:  56%|█████▌    | 570/1024 [00:13<00:12, 36.09it/s, est. speed input: 44054.77 toks/s, output: 43.02 toks/s]
Processed prompts:  56%|█████▋    | 578/1024 [00:13<00:12, 36.07it/s, est. speed input: 43936.56 toks/s, output: 42.91 toks/s]
Processed prompts:  57%|█████▋    | 586/1024 [00:13<00:12, 36.01it/s, est. speed input: 43819.39 toks/s, output: 42.79 toks/s]
Processed prompts:  58%|█████▊    | 594/1024 [00:13<00:11, 36.00it/s, est. speed input: 43707.95 toks/s, output: 42.68 toks/s]
Processed prompts:  59%|█████▉    | 602/1024 [00:14<00:11, 36.03it/s, est. speed input: 43602.12 toks/s, output: 42.58 toks/s]
Processed prompts:  60%|█████▉    | 610/1024 [00:14<00:11, 36.01it/s, est. speed input: 43497.11 toks/s, output: 42.48 toks/s]
Processed prompts:  60%|██████    | 618/1024 [00:14<00:11, 36.00it/s, est. speed input: 43395.62 toks/s, output: 42.38 toks/s]
Processed prompts:  61%|██████    | 626/1024 [00:14<00:11, 36.03it/s, est. speed input: 43299.31 toks/s, output: 42.28 toks/s]
Processed prompts:  62%|██████▏   | 634/1024 [00:15<00:10, 36.07it/s, est. speed input: 43207.10 toks/s, output: 42.19 toks/s]
Processed prompts:  63%|██████▎   | 642/1024 [00:15<00:10, 36.11it/s, est. speed input: 43117.95 toks/s, output: 42.11 toks/s]
Processed prompts:  63%|██████▎   | 650/1024 [00:15<00:10, 36.05it/s, est. speed input: 43026.50 toks/s, output: 42.02 toks/s]
Processed prompts:  64%|██████▍   | 658/1024 [00:15<00:10, 36.05it/s, est. speed input: 42940.23 toks/s, output: 41.93 toks/s]
Processed prompts:  65%|██████▌   | 666/1024 [00:15<00:09, 36.01it/s, est. speed input: 42853.78 toks/s, output: 41.85 toks/s]
Processed prompts:  66%|██████▌   | 674/1024 [00:16<00:09, 35.99it/s, est. speed input: 42770.27 toks/s, output: 41.77 toks/s]
Processed prompts:  67%|██████▋   | 682/1024 [00:16<00:09, 35.97it/s, est. speed input: 42689.16 toks/s, output: 41.69 toks/s]
Processed prompts:  67%|██████▋   | 690/1024 [00:16<00:09, 35.97it/s, est. speed input: 42610.57 toks/s, output: 41.61 toks/s]
Processed prompts:  68%|██████▊   | 698/1024 [00:16<00:09, 36.00it/s, est. speed input: 42535.80 toks/s, output: 41.54 toks/s]
Processed prompts:  69%|██████▉   | 706/1024 [00:17<00:08, 36.02it/s, est. speed input: 42462.48 toks/s, output: 41.47 toks/s]
Processed prompts:  70%|██████▉   | 714/1024 [00:17<00:08, 36.04it/s, est. speed input: 42391.97 toks/s, output: 41.40 toks/s]
Processed prompts:  71%|███████   | 722/1024 [00:17<00:08, 36.01it/s, est. speed input: 42320.66 toks/s, output: 41.33 toks/s]
Processed prompts:  71%|███████▏  | 730/1024 [00:17<00:08, 36.03it/s, est. speed input: 42253.31 toks/s, output: 41.26 toks/s]
Processed prompts:  72%|███████▏  | 738/1024 [00:17<00:07, 36.01it/s, est. speed input: 42185.71 toks/s, output: 41.20 toks/s]
Processed prompts:  73%|███████▎  | 746/1024 [00:18<00:07, 35.95it/s, est. speed input: 42118.01 toks/s, output: 41.13 toks/s]
Processed prompts:  74%|███████▎  | 754/1024 [00:18<00:07, 35.98it/s, est. speed input: 42054.88 toks/s, output: 41.07 toks/s]
Processed prompts:  74%|███████▍  | 762/1024 [00:18<00:07, 35.96it/s, est. speed input: 41991.80 toks/s, output: 41.01 toks/s]
Processed prompts:  75%|███████▌  | 770/1024 [00:18<00:07, 35.99it/s, est. speed input: 41931.95 toks/s, output: 40.95 toks/s]
Processed prompts:  76%|███████▌  | 778/1024 [00:19<00:06, 35.96it/s, est. speed input: 41871.41 toks/s, output: 40.89 toks/s]
Processed prompts:  77%|███████▋  | 786/1024 [00:19<00:06, 37.12it/s, est. speed input: 41863.38 toks/s, output: 40.88 toks/s]
Processed prompts:  78%|███████▊  | 794/1024 [00:19<00:06, 36.83it/s, est. speed input: 41808.63 toks/s, output: 40.83 toks/s]
Processed prompts:  78%|███████▊  | 802/1024 [00:19<00:06, 36.60it/s, est. speed input: 41753.46 toks/s, output: 40.77 toks/s]
Processed prompts:  79%|███████▉  | 810/1024 [00:19<00:05, 36.44it/s, est. speed input: 41699.86 toks/s, output: 40.72 toks/s]
Processed prompts:  80%|███████▉  | 818/1024 [00:20<00:05, 36.25it/s, est. speed input: 41644.25 toks/s, output: 40.67 toks/s]
Processed prompts:  81%|████████  | 826/1024 [00:20<00:05, 36.09it/s, est. speed input: 41588.43 toks/s, output: 40.61 toks/s]
Processed prompts:  81%|████████▏ | 834/1024 [00:20<00:05, 36.09it/s, est. speed input: 41538.41 toks/s, output: 40.56 toks/s]
Processed prompts:  82%|████████▏ | 842/1024 [00:20<00:05, 36.05it/s, est. speed input: 41487.99 toks/s, output: 40.52 toks/s]
Processed prompts:  83%|████████▎ | 850/1024 [00:21<00:04, 36.01it/s, est. speed input: 41437.94 toks/s, output: 40.47 toks/s]
Processed prompts:  84%|████████▍ | 858/1024 [00:21<00:04, 36.01it/s, est. speed input: 41390.25 toks/s, output: 40.42 toks/s]
Processed prompts:  85%|████████▍ | 866/1024 [00:21<00:04, 36.04it/s, est. speed input: 41344.64 toks/s, output: 40.38 toks/s]
Processed prompts:  85%|████████▌ | 874/1024 [00:21<00:04, 36.04it/s, est. speed input: 41299.07 toks/s, output: 40.33 toks/s]
Processed prompts:  86%|████████▌ | 882/1024 [00:21<00:03, 35.99it/s, est. speed input: 41252.64 toks/s, output: 40.29 toks/s]
Processed prompts:  87%|████████▋ | 890/1024 [00:22<00:03, 35.96it/s, est. speed input: 41207.27 toks/s, output: 40.24 toks/s]
Processed prompts:  88%|████████▊ | 898/1024 [00:22<00:03, 35.99it/s, est. speed input: 41164.79 toks/s, output: 40.20 toks/s]
Processed prompts:  88%|████████▊ | 906/1024 [00:22<00:03, 36.01it/s, est. speed input: 41123.17 toks/s, output: 40.16 toks/s]
Processed prompts:  89%|████████▉ | 914/1024 [00:22<00:03, 36.05it/s, est. speed input: 41083.05 toks/s, output: 40.12 toks/s]
Processed prompts:  90%|█████████ | 922/1024 [00:23<00:02, 36.04it/s, est. speed input: 41042.60 toks/s, output: 40.08 toks/s]
Processed prompts:  91%|█████████ | 930/1024 [00:23<00:02, 36.09it/s, est. speed input: 41004.70 toks/s, output: 40.04 toks/s]
Processed prompts:  92%|█████████▏| 938/1024 [00:23<00:02, 36.14it/s, est. speed input: 40968.26 toks/s, output: 40.01 toks/s]
Processed prompts:  92%|█████████▏| 946/1024 [00:23<00:02, 36.12it/s, est. speed input: 40930.40 toks/s, output: 39.97 toks/s]
Processed prompts:  93%|█████████▎| 954/1024 [00:23<00:01, 36.13it/s, est. speed input: 40894.12 toks/s, output: 39.94 toks/s]
Processed prompts:  94%|█████████▍| 962/1024 [00:24<00:01, 36.13it/s, est. speed input: 40858.53 toks/s, output: 39.90 toks/s]
Processed prompts:  95%|█████████▍| 970/1024 [00:24<00:01, 36.12it/s, est. speed input: 40822.93 toks/s, output: 39.87 toks/s]
Processed prompts:  96%|█████████▌| 978/1024 [00:24<00:01, 36.11it/s, est. speed input: 40788.03 toks/s, output: 39.83 toks/s]
Processed prompts:  96%|█████████▋| 986/1024 [00:24<00:01, 36.05it/s, est. speed input: 40751.97 toks/s, output: 39.80 toks/s]
Processed prompts:  97%|█████████▋| 994/1024 [00:24<00:00, 36.02it/s, est. speed input: 40716.98 toks/s, output: 39.76 toks/s]
Processed prompts:  98%|█████████▊| 1002/1024 [00:25<00:00, 36.05it/s, est. speed input: 40684.05 toks/s, output: 39.73 toks/s]
Processed prompts:  99%|█████████▊| 1010/1024 [00:25<00:00, 36.08it/s, est. speed input: 40652.27 toks/s, output: 39.70 toks/s]
Processed prompts:  99%|█████████▉| 1018/1024 [00:25<00:00, 37.52it/s, est. speed input: 40665.14 toks/s, output: 39.71 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:25<00:00, 37.52it/s, est. speed input: 40904.22 toks/s, output: 39.95 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:25<00:00, 39.95it/s, est. speed input: 40904.22 toks/s, output: 39.95 toks/s]
[rank0]:[W126 10:57:43.652465479 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 72.4s

测试结果:
  Requests/s:   36.13
  Tokens/s:     37030.60
  Total Reqs:   1024
  Elapsed:      28.34s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     36994.47

============================================================
[6/8] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 10:58:03 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1250044) WARNING 01-26 10:58:19 [backends.py:609] Failed to read file <frozen os>
Throughput: 36.76 requests/s, 37681.08 total tokens/s, 36.76 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-26 10:58:02] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 10:58:03] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 10:58:03] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 10:58:03] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:58:03] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:58:03] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:58:03] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:58:03] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:58:03] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 10:58:03] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 10:58:03] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 10:58:03] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 10:58:03] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 10:58:03] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 10:58:09] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 10:58:10] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 10:58:10] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 10:58:10] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:58:10] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:58:10] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:58:10] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:58:10] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 10:58:10] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 10:58:10] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 10:58:10] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 10:58:10] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 10:58:10] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 10:58:10] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1250044) [2026-01-26 10:58:12] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=1250044) [2026-01-26 10:58:12] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1250044) [2026-01-26 10:58:12] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=1250044) [2026-01-26 10:58:12] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=1250044) [2026-01-26 10:58:12] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=1250044) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1250044) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.49it/s]
(EngineCore_DP0 pid=1250044) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.90it/s]
(EngineCore_DP0 pid=1250044) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.97it/s]
(EngineCore_DP0 pid=1250044) 
(EngineCore_DP0 pid=1250044) [rank0]:W0126 10:58:25.625000 1250044 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1250044) [rank0]:W0126 10:58:26.353000 1250044 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=1250044) [rank0]:W0126 10:58:28.422000 1250044 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1250044) [rank0]:W0126 10:58:28.552000 1250044 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1250044) 2026-01-26 10:58:33,209 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1250044) 2026-01-26 10:58:33,238 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1250044) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 1/7 [00:00<00:01,  3.17it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:02,  2.08it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:01<00:00,  4.48it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:01<00:00,  6.88it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:01<00:00,  5.62it/s]
(EngineCore_DP0 pid=1250044) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00, 16.48it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00, 10.67it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 12.15it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   1%|          | 21/2048 [00:00<00:09, 209.49it/s]
Adding requests:   3%|▎         | 61/2048 [00:00<00:06, 320.03it/s]
Adding requests:   5%|▍         | 97/2048 [00:00<00:05, 337.97it/s]
Adding requests:   7%|▋         | 135/2048 [00:00<00:05, 353.85it/s]
Adding requests:   9%|▊         | 175/2048 [00:00<00:05, 367.02it/s]
Adding requests:  11%|█         | 216/2048 [00:00<00:04, 378.05it/s]
Adding requests:  12%|█▏        | 255/2048 [00:00<00:04, 378.90it/s]
Adding requests:  14%|█▍        | 294/2048 [00:00<00:04, 382.25it/s]
Adding requests:  16%|█▋        | 336/2048 [00:00<00:04, 390.98it/s]
Adding requests:  18%|█▊        | 376/2048 [00:01<00:04, 392.49it/s]
Adding requests:  20%|██        | 418/2048 [00:01<00:04, 398.55it/s]
Adding requests:  22%|██▏       | 458/2048 [00:01<00:04, 393.42it/s]
Adding requests:  24%|██▍       | 500/2048 [00:01<00:03, 399.98it/s]
Adding requests:  26%|██▋       | 542/2048 [00:01<00:03, 405.36it/s]
Adding requests:  28%|██▊       | 583/2048 [00:01<00:03, 399.25it/s]
Adding requests:  30%|███       | 623/2048 [00:01<00:03, 392.15it/s]
Adding requests:  32%|███▏      | 663/2048 [00:01<00:03, 379.24it/s]
Adding requests:  34%|███▍      | 703/2048 [00:01<00:03, 383.06it/s]
Adding requests:  36%|███▌      | 742/2048 [00:01<00:03, 378.34it/s]
Adding requests:  38%|███▊      | 781/2048 [00:02<00:03, 379.16it/s]
Adding requests:  40%|████      | 820/2048 [00:02<00:03, 379.43it/s]
Adding requests:  42%|████▏     | 861/2048 [00:02<00:03, 387.66it/s]
Adding requests:  44%|████▍     | 901/2048 [00:02<00:02, 388.03it/s]
Adding requests:  46%|████▌     | 940/2048 [00:02<00:02, 383.96it/s]
Adding requests:  48%|████▊     | 979/2048 [00:02<00:02, 383.28it/s]
Adding requests:  50%|████▉     | 1018/2048 [00:02<00:02, 379.38it/s]
Adding requests:  52%|█████▏    | 1056/2048 [00:02<00:02, 377.21it/s]
Adding requests:  53%|█████▎    | 1094/2048 [00:02<00:02, 376.75it/s]
Adding requests:  55%|█████▌    | 1135/2048 [00:02<00:02, 384.87it/s]
Adding requests:  57%|█████▋    | 1174/2048 [00:03<00:02, 382.16it/s]
Adding requests:  59%|█████▉    | 1213/2048 [00:03<00:02, 384.30it/s]
Adding requests:  61%|██████    | 1253/2048 [00:03<00:02, 386.62it/s]
Adding requests:  63%|██████▎   | 1292/2048 [00:03<00:01, 379.37it/s]
Adding requests:  65%|██████▌   | 1332/2048 [00:03<00:01, 382.39it/s]
Adding requests:  67%|██████▋   | 1372/2048 [00:03<00:01, 386.91it/s]
Adding requests:  69%|██████▉   | 1411/2048 [00:03<00:01, 385.37it/s]
Adding requests:  71%|███████   | 1450/2048 [00:03<00:01, 383.28it/s]
Adding requests:  73%|███████▎  | 1492/2048 [00:03<00:01, 392.13it/s]
Adding requests:  75%|███████▍  | 1532/2048 [00:04<00:01, 388.02it/s]
Adding requests:  77%|███████▋  | 1571/2048 [00:04<00:01, 380.91it/s]
Adding requests:  79%|███████▊  | 1610/2048 [00:04<00:01, 381.24it/s]
Adding requests:  81%|████████  | 1649/2048 [00:04<00:01, 370.59it/s]
Adding requests:  82%|████████▏ | 1687/2048 [00:04<00:00, 371.10it/s]
Adding requests:  84%|████████▍ | 1727/2048 [00:04<00:00, 377.22it/s]
Adding requests:  86%|████████▋ | 1767/2048 [00:04<00:00, 383.26it/s]
Adding requests:  88%|████████▊ | 1806/2048 [00:04<00:00, 379.90it/s]
Adding requests:  90%|█████████ | 1845/2048 [00:04<00:00, 372.00it/s]
Adding requests:  92%|█████████▏| 1884/2048 [00:04<00:00, 376.99it/s]
Adding requests:  94%|█████████▍| 1924/2048 [00:05<00:00, 381.62it/s]
Adding requests:  96%|█████████▌| 1964/2048 [00:05<00:00, 385.84it/s]
Adding requests:  98%|█████████▊| 2003/2048 [00:05<00:00, 380.11it/s]
Adding requests: 100%|█████████▉| 2042/2048 [00:05<00:00, 376.09it/s]
Adding requests: 100%|██████████| 2048/2048 [00:05<00:00, 380.54it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   9%|▉         | 194/2048 [00:00<00:03, 510.87it/s, est. speed input: 523160.86 toks/s, output: 510.88 toks/s]
Processed prompts:  12%|█▏        | 246/2048 [00:01<00:15, 119.09it/s, est. speed input: 148979.11 toks/s, output: 145.49 toks/s]
Processed prompts:  13%|█▎        | 270/2048 [00:02<00:17, 100.82it/s, est. speed input: 129869.37 toks/s, output: 126.82 toks/s]
Processed prompts:  14%|█▍        | 286/2048 [00:02<00:21, 82.46it/s, est. speed input: 114255.57 toks/s, output: 111.58 toks/s] 
Processed prompts:  15%|█▍        | 297/2048 [00:02<00:26, 65.78it/s, est. speed input: 101406.23 toks/s, output: 99.03 toks/s] 
Processed prompts:  15%|█▍        | 306/2048 [00:03<00:32, 53.47it/s, est. speed input: 91927.35 toks/s, output: 89.77 toks/s] 
Processed prompts:  16%|█▌        | 322/2048 [00:03<00:35, 48.46it/s, est. speed input: 85777.46 toks/s, output: 83.77 toks/s]
Processed prompts:  17%|█▋        | 338/2048 [00:04<00:38, 45.00it/s, est. speed input: 80907.46 toks/s, output: 79.01 toks/s]
Processed prompts:  17%|█▋        | 354/2048 [00:04<00:39, 42.53it/s, est. speed input: 76910.45 toks/s, output: 75.11 toks/s]
Processed prompts:  18%|█▊        | 370/2048 [00:05<00:41, 40.78it/s, est. speed input: 73582.79 toks/s, output: 71.86 toks/s]
Processed prompts:  19%|█▉        | 386/2048 [00:05<00:41, 39.57it/s, est. speed input: 70780.57 toks/s, output: 69.12 toks/s]
Processed prompts:  20%|█▉        | 402/2048 [00:06<00:42, 38.73it/s, est. speed input: 68384.60 toks/s, output: 66.78 toks/s]
Processed prompts:  20%|██        | 418/2048 [00:06<00:42, 38.15it/s, est. speed input: 66317.18 toks/s, output: 64.76 toks/s]
Processed prompts:  21%|██        | 434/2048 [00:06<00:42, 38.34it/s, est. speed input: 64721.47 toks/s, output: 63.20 toks/s]
Processed prompts:  22%|██▏       | 450/2048 [00:07<00:42, 37.87it/s, est. speed input: 63109.46 toks/s, output: 61.63 toks/s]
Processed prompts:  23%|██▎       | 466/2048 [00:07<00:42, 37.57it/s, est. speed input: 61687.30 toks/s, output: 60.24 toks/s]
Processed prompts:  24%|██▎       | 482/2048 [00:08<00:41, 37.30it/s, est. speed input: 60399.27 toks/s, output: 58.98 toks/s]
Processed prompts:  24%|██▍       | 498/2048 [00:08<00:41, 37.12it/s, est. speed input: 59244.24 toks/s, output: 57.86 toks/s]
Processed prompts:  25%|██▌       | 514/2048 [00:09<00:41, 37.02it/s, est. speed input: 58205.86 toks/s, output: 56.84 toks/s]
Processed prompts:  26%|██▌       | 530/2048 [00:09<00:41, 36.92it/s, est. speed input: 57257.48 toks/s, output: 55.92 toks/s]
Processed prompts:  27%|██▋       | 546/2048 [00:09<00:40, 36.87it/s, est. speed input: 56394.75 toks/s, output: 55.07 toks/s]
Processed prompts:  27%|██▋       | 562/2048 [00:10<00:40, 36.81it/s, est. speed input: 55600.25 toks/s, output: 54.30 toks/s]
Processed prompts:  28%|██▊       | 578/2048 [00:10<00:39, 36.77it/s, est. speed input: 54871.82 toks/s, output: 53.59 toks/s]
Processed prompts:  29%|██▉       | 594/2048 [00:11<00:39, 36.76it/s, est. speed input: 54201.88 toks/s, output: 52.93 toks/s]
Processed prompts:  30%|██▉       | 610/2048 [00:11<00:39, 36.74it/s, est. speed input: 53580.31 toks/s, output: 52.32 toks/s]
Processed prompts:  31%|███       | 626/2048 [00:12<00:38, 36.70it/s, est. speed input: 52997.85 toks/s, output: 51.76 toks/s]
Processed prompts:  31%|███▏      | 642/2048 [00:12<00:38, 36.69it/s, est. speed input: 52459.82 toks/s, output: 51.23 toks/s]
Processed prompts:  32%|███▏      | 658/2048 [00:12<00:37, 36.67it/s, est. speed input: 51955.48 toks/s, output: 50.74 toks/s]
Processed prompts:  33%|███▎      | 674/2048 [00:13<00:37, 36.69it/s, est. speed input: 51489.85 toks/s, output: 50.28 toks/s]
Processed prompts:  34%|███▎      | 690/2048 [00:13<00:37, 36.69it/s, est. speed input: 51051.27 toks/s, output: 49.85 toks/s]
Processed prompts:  34%|███▍      | 706/2048 [00:14<00:36, 36.69it/s, est. speed input: 50639.52 toks/s, output: 49.45 toks/s]
Processed prompts:  35%|███▌      | 722/2048 [00:14<00:36, 36.65it/s, est. speed input: 50247.46 toks/s, output: 49.07 toks/s]
Processed prompts:  36%|███▌      | 738/2048 [00:15<00:35, 36.66it/s, est. speed input: 49881.62 toks/s, output: 48.71 toks/s]
Processed prompts:  37%|███▋      | 754/2048 [00:15<00:35, 36.64it/s, est. speed input: 49533.76 toks/s, output: 48.37 toks/s]
Processed prompts:  38%|███▊      | 770/2048 [00:16<00:34, 36.66it/s, est. speed input: 49208.61 toks/s, output: 48.06 toks/s]
Processed prompts:  38%|███▊      | 786/2048 [00:16<00:33, 37.31it/s, est. speed input: 48974.66 toks/s, output: 47.83 toks/s]
Processed prompts:  39%|███▉      | 802/2048 [00:16<00:33, 37.10it/s, est. speed input: 48676.98 toks/s, output: 47.54 toks/s]
Processed prompts:  40%|███▉      | 818/2048 [00:17<00:33, 36.98it/s, est. speed input: 48398.10 toks/s, output: 47.26 toks/s]
Processed prompts:  41%|████      | 834/2048 [00:17<00:32, 36.87it/s, est. speed input: 48129.74 toks/s, output: 47.00 toks/s]
Processed prompts:  42%|████▏     | 850/2048 [00:18<00:32, 36.82it/s, est. speed input: 47876.39 toks/s, output: 46.75 toks/s]
Processed prompts:  42%|████▏     | 866/2048 [00:18<00:32, 36.80it/s, est. speed input: 47637.22 toks/s, output: 46.52 toks/s]
Processed prompts:  43%|████▎     | 882/2048 [00:19<00:31, 36.71it/s, est. speed input: 47401.00 toks/s, output: 46.29 toks/s]
Processed prompts:  44%|████▍     | 898/2048 [00:19<00:31, 36.71it/s, est. speed input: 47181.83 toks/s, output: 46.08 toks/s]
Processed prompts:  45%|████▍     | 914/2048 [00:19<00:30, 36.66it/s, est. speed input: 46967.45 toks/s, output: 45.87 toks/s]
Processed prompts:  45%|████▌     | 930/2048 [00:20<00:30, 36.68it/s, est. speed input: 46767.20 toks/s, output: 45.67 toks/s]
Processed prompts:  46%|████▌     | 946/2048 [00:20<00:30, 36.66it/s, est. speed input: 46572.29 toks/s, output: 45.48 toks/s]
Processed prompts:  47%|████▋     | 962/2048 [00:21<00:29, 36.65it/s, est. speed input: 46385.48 toks/s, output: 45.30 toks/s]
Processed prompts:  48%|████▊     | 978/2048 [00:21<00:29, 36.64it/s, est. speed input: 46206.50 toks/s, output: 45.12 toks/s]
Processed prompts:  49%|████▊     | 994/2048 [00:22<00:28, 36.59it/s, est. speed input: 46030.80 toks/s, output: 44.95 toks/s]
Processed prompts:  49%|████▉     | 1010/2048 [00:22<00:28, 36.61it/s, est. speed input: 45866.10 toks/s, output: 44.79 toks/s]
Processed prompts:  50%|█████     | 1026/2048 [00:22<00:27, 36.58it/s, est. speed input: 45704.86 toks/s, output: 44.63 toks/s]
Processed prompts:  51%|█████     | 1042/2048 [00:23<00:27, 36.61it/s, est. speed input: 45553.22 toks/s, output: 44.49 toks/s]
Processed prompts:  52%|█████▏    | 1058/2048 [00:23<00:27, 36.61it/s, est. speed input: 45405.35 toks/s, output: 44.34 toks/s]
Processed prompts:  52%|█████▏    | 1074/2048 [00:24<00:26, 36.59it/s, est. speed input: 45261.80 toks/s, output: 44.20 toks/s]
Processed prompts:  53%|█████▎    | 1090/2048 [00:24<00:26, 36.60it/s, est. speed input: 45124.26 toks/s, output: 44.07 toks/s]
Processed prompts:  54%|█████▍    | 1106/2048 [00:25<00:25, 36.59it/s, est. speed input: 44991.05 toks/s, output: 43.94 toks/s]
Processed prompts:  55%|█████▍    | 1122/2048 [00:25<00:25, 36.58it/s, est. speed input: 44861.83 toks/s, output: 43.81 toks/s]
Processed prompts:  56%|█████▌    | 1138/2048 [00:26<00:24, 36.57it/s, est. speed input: 44737.16 toks/s, output: 43.69 toks/s]
Processed prompts:  56%|█████▋    | 1154/2048 [00:26<00:24, 36.58it/s, est. speed input: 44617.36 toks/s, output: 43.57 toks/s]
Processed prompts:  57%|█████▋    | 1170/2048 [00:26<00:23, 36.60it/s, est. speed input: 44502.45 toks/s, output: 43.46 toks/s]
Processed prompts:  58%|█████▊    | 1186/2048 [00:27<00:23, 36.59it/s, est. speed input: 44389.25 toks/s, output: 43.35 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [00:27<00:22, 37.17it/s, est. speed input: 44316.95 toks/s, output: 43.28 toks/s]
Processed prompts:  59%|█████▉    | 1218/2048 [00:28<00:22, 36.97it/s, est. speed input: 44209.20 toks/s, output: 43.17 toks/s]
Processed prompts:  60%|██████    | 1234/2048 [00:28<00:21, 37.43it/s, est. speed input: 44140.37 toks/s, output: 43.11 toks/s]
Processed prompts:  61%|██████    | 1250/2048 [00:29<00:21, 37.17it/s, est. speed input: 44040.02 toks/s, output: 43.01 toks/s]
Processed prompts:  62%|██████▏   | 1266/2048 [00:29<00:21, 37.00it/s, est. speed input: 43942.94 toks/s, output: 42.91 toks/s]
Processed prompts:  63%|██████▎   | 1282/2048 [00:29<00:20, 36.89it/s, est. speed input: 43849.28 toks/s, output: 42.82 toks/s]
Processed prompts:  63%|██████▎   | 1298/2048 [00:30<00:20, 36.74it/s, est. speed input: 43753.77 toks/s, output: 42.73 toks/s]
Processed prompts:  64%|██████▍   | 1314/2048 [00:30<00:20, 36.67it/s, est. speed input: 43663.14 toks/s, output: 42.64 toks/s]
Processed prompts:  65%|██████▍   | 1330/2048 [00:31<00:19, 37.22it/s, est. speed input: 43607.91 toks/s, output: 42.59 toks/s]
Processed prompts:  66%|██████▌   | 1346/2048 [00:31<00:18, 36.97it/s, est. speed input: 43520.13 toks/s, output: 42.50 toks/s]
Processed prompts:  67%|██████▋   | 1362/2048 [00:32<00:18, 36.84it/s, est. speed input: 43436.91 toks/s, output: 42.42 toks/s]
Processed prompts:  67%|██████▋   | 1378/2048 [00:32<00:18, 36.75it/s, est. speed input: 43356.08 toks/s, output: 42.34 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [00:32<00:17, 36.69it/s, est. speed input: 43277.51 toks/s, output: 42.26 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [00:33<00:17, 36.64it/s, est. speed input: 43200.41 toks/s, output: 42.19 toks/s]
Processed prompts:  70%|██████▉   | 1426/2048 [00:33<00:16, 36.62it/s, est. speed input: 43126.03 toks/s, output: 42.12 toks/s]
Processed prompts:  70%|███████   | 1442/2048 [00:34<00:16, 36.56it/s, est. speed input: 43051.32 toks/s, output: 42.04 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [00:34<00:15, 37.15it/s, est. speed input: 43009.05 toks/s, output: 42.00 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [00:35<00:15, 36.94it/s, est. speed input: 42938.43 toks/s, output: 41.93 toks/s]
Processed prompts:  73%|███████▎  | 1490/2048 [00:35<00:15, 36.78it/s, est. speed input: 42868.41 toks/s, output: 41.86 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [00:36<00:14, 36.66it/s, est. speed input: 42800.01 toks/s, output: 41.80 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [00:36<00:14, 37.18it/s, est. speed input: 42761.09 toks/s, output: 41.76 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [00:36<00:13, 37.00it/s, est. speed input: 42698.27 toks/s, output: 41.70 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [00:37<00:13, 37.47it/s, est. speed input: 42663.24 toks/s, output: 41.66 toks/s]
Processed prompts:  77%|███████▋  | 1570/2048 [00:37<00:12, 37.18it/s, est. speed input: 42601.93 toks/s, output: 41.60 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [00:38<00:12, 36.92it/s, est. speed input: 42539.53 toks/s, output: 41.54 toks/s]
Processed prompts:  78%|███████▊  | 1602/2048 [00:38<00:12, 36.76it/s, est. speed input: 42479.57 toks/s, output: 41.48 toks/s]
Processed prompts:  79%|███████▉  | 1618/2048 [00:39<00:11, 37.28it/s, est. speed input: 42447.57 toks/s, output: 41.45 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [00:39<00:11, 37.01it/s, est. speed input: 42389.76 toks/s, output: 41.40 toks/s]
Processed prompts:  81%|████████  | 1650/2048 [00:39<00:10, 36.84it/s, est. speed input: 42334.20 toks/s, output: 41.34 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [00:40<00:10, 36.73it/s, est. speed input: 42279.84 toks/s, output: 41.29 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [00:40<00:09, 36.67it/s, est. speed input: 42227.51 toks/s, output: 41.24 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [00:41<00:09, 36.64it/s, est. speed input: 42176.81 toks/s, output: 41.19 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [00:41<00:09, 36.60it/s, est. speed input: 42126.37 toks/s, output: 41.14 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [00:42<00:08, 37.18it/s, est. speed input: 42100.71 toks/s, output: 41.11 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [00:42<00:08, 37.56it/s, est. speed input: 42074.28 toks/s, output: 41.09 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [00:42<00:07, 37.21it/s, est. speed input: 42025.36 toks/s, output: 41.04 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [00:43<00:07, 36.99it/s, est. speed input: 41978.41 toks/s, output: 40.99 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [00:43<00:06, 36.83it/s, est. speed input: 41931.63 toks/s, output: 40.95 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [00:44<00:06, 36.70it/s, est. speed input: 41885.63 toks/s, output: 40.90 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [00:44<00:06, 36.63it/s, est. speed input: 41841.01 toks/s, output: 40.86 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [00:45<00:05, 36.59it/s, est. speed input: 41797.47 toks/s, output: 40.82 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [00:45<00:05, 36.55it/s, est. speed input: 41754.49 toks/s, output: 40.78 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [00:46<00:04, 36.54it/s, est. speed input: 41713.13 toks/s, output: 40.74 toks/s]
Processed prompts:  92%|█████████▏| 1890/2048 [00:46<00:04, 37.12it/s, est. speed input: 41692.91 toks/s, output: 40.72 toks/s]
Processed prompts:  93%|█████████▎| 1906/2048 [00:46<00:03, 36.92it/s, est. speed input: 41652.21 toks/s, output: 40.68 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [00:47<00:03, 36.74it/s, est. speed input: 41610.75 toks/s, output: 40.64 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [00:47<00:03, 36.64it/s, est. speed input: 41570.81 toks/s, output: 40.60 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [00:48<00:02, 36.55it/s, est. speed input: 41531.13 toks/s, output: 40.56 toks/s]
Processed prompts:  96%|█████████▌| 1970/2048 [00:48<00:02, 36.50it/s, est. speed input: 41492.33 toks/s, output: 40.52 toks/s]
Processed prompts:  97%|█████████▋| 1986/2048 [00:49<00:01, 37.06it/s, est. speed input: 41474.21 toks/s, output: 40.50 toks/s]
Processed prompts:  98%|█████████▊| 2002/2048 [00:49<00:01, 36.87it/s, est. speed input: 41437.18 toks/s, output: 40.47 toks/s]
Processed prompts:  99%|█████████▊| 2018/2048 [00:49<00:00, 36.72it/s, est. speed input: 41400.31 toks/s, output: 40.43 toks/s]
Processed prompts:  99%|█████████▉| 2034/2048 [00:50<00:00, 37.35it/s, est. speed input: 41387.63 toks/s, output: 40.42 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:50<00:00, 37.35it/s, est. speed input: 41671.90 toks/s, output: 40.70 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:50<00:00, 40.70it/s, est. speed input: 41671.90 toks/s, output: 40.70 toks/s]
[rank0]:[W126 10:59:32.492221834 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 109.2s

测试结果:
  Requests/s:   36.76
  Tokens/s:     37681.08
  Total Reqs:   2048
  Elapsed:      55.71s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     37644.32

============================================================
[7/8] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 11:00:03 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1252293) WARNING 01-26 11:00:19 [backends.py:609] Failed to read file <frozen os>
Throughput: 36.74 requests/s, 37655.83 total tokens/s, 36.74 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-26 11:00:02] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:00:03] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:00:03] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 11:00:03] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:00:03] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:00:03] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:00:03] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:00:03] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:00:03] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:00:03] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:00:03] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:00:03] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:00:03] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:00:03] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 11:00:09] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:00:10] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:00:10] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 11:00:10] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:00:10] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:00:10] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:00:10] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:00:10] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:00:10] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:00:10] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:00:10] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:00:10] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:00:10] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:00:10] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1252293) [2026-01-26 11:00:12] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=1252293) [2026-01-26 11:00:12] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1252293) [2026-01-26 11:00:12] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=1252293) [2026-01-26 11:00:12] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=1252293) [2026-01-26 11:00:12] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=1252293) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1252293) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.57it/s]
(EngineCore_DP0 pid=1252293) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  1.93it/s]
(EngineCore_DP0 pid=1252293) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.00it/s]
(EngineCore_DP0 pid=1252293) 
(EngineCore_DP0 pid=1252293) [rank0]:W0126 11:00:25.433000 1252293 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1252293) [rank0]:W0126 11:00:25.681000 1252293 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=1252293) [rank0]:W0126 11:00:26.917000 1252293 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1252293) [rank0]:W0126 11:00:27.047000 1252293 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1252293) 2026-01-26 11:00:31,308 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1252293) 2026-01-26 11:00:31,338 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1252293) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▉         | 1/11 [00:01<00:10,  1.04s/it]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 3/11 [00:01<00:02,  3.09it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 5/11 [00:01<00:01,  5.33it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▎   | 7/11 [00:01<00:00,  7.48it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 9/11 [00:01<00:00,  9.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00,  9.44it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00,  6.27it/s]
(EngineCore_DP0 pid=1252293) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00,  7.10it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 3/7 [00:00<00:01,  3.62it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00,  4.40it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:01<00:00,  5.87it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:01<00:00,  5.75it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 29/4096 [00:00<00:14, 283.25it/s]
Adding requests:   2%|▏         | 69/4096 [00:00<00:11, 350.69it/s]
Adding requests:   3%|▎         | 106/4096 [00:00<00:11, 357.57it/s]
Adding requests:   4%|▎         | 144/4096 [00:00<00:10, 362.83it/s]
Adding requests:   5%|▍         | 185/4096 [00:00<00:10, 376.99it/s]
Adding requests:   5%|▌         | 225/4096 [00:00<00:10, 382.37it/s]
Adding requests:   6%|▋         | 264/4096 [00:00<00:10, 380.98it/s]
Adding requests:   7%|▋         | 304/4096 [00:00<00:09, 385.93it/s]
Adding requests:   8%|▊         | 345/4096 [00:00<00:09, 392.14it/s]
Adding requests:   9%|▉         | 386/4096 [00:01<00:09, 394.89it/s]
Adding requests:  10%|█         | 428/4096 [00:01<00:09, 400.82it/s]
Adding requests:  11%|█▏        | 469/4096 [00:01<00:09, 398.85it/s]
Adding requests:  12%|█▎        | 512/4096 [00:01<00:08, 404.60it/s]
Adding requests:  14%|█▎        | 555/4096 [00:01<00:08, 409.64it/s]
Adding requests:  15%|█▍        | 596/4096 [00:01<00:08, 401.38it/s]
Adding requests:  16%|█▌        | 637/4096 [00:01<00:08, 396.96it/s]
Adding requests:  17%|█▋        | 677/4096 [00:01<00:08, 389.74it/s]
Adding requests:  18%|█▊        | 717/4096 [00:01<00:08, 391.67it/s]
Adding requests:  18%|█▊        | 757/4096 [00:01<00:08, 386.28it/s]
Adding requests:  19%|█▉        | 796/4096 [00:02<00:08, 386.56it/s]
Adding requests:  20%|██        | 837/4096 [00:02<00:08, 390.39it/s]
Adding requests:  21%|██▏       | 877/4096 [00:02<00:08, 390.95it/s]
Adding requests:  22%|██▏       | 917/4096 [00:02<00:08, 388.65it/s]
Adding requests:  23%|██▎       | 956/4096 [00:02<00:08, 388.03it/s]
Adding requests:  24%|██▍       | 995/4096 [00:02<00:08, 382.35it/s]
Adding requests:  25%|██▌       | 1034/4096 [00:02<00:08, 375.62it/s]
Adding requests:  26%|██▌       | 1073/4096 [00:02<00:08, 376.95it/s]
Adding requests:  27%|██▋       | 1111/4096 [00:02<00:07, 375.58it/s]
Adding requests:  28%|██▊       | 1150/4096 [00:02<00:07, 379.20it/s]
Adding requests:  29%|██▉       | 1189/4096 [00:03<00:07, 379.63it/s]
Adding requests:  30%|███       | 1230/4096 [00:03<00:07, 386.61it/s]
Adding requests:  31%|███       | 1269/4096 [00:03<00:07, 383.50it/s]
Adding requests:  32%|███▏      | 1308/4096 [00:03<00:07, 381.61it/s]
Adding requests:  33%|███▎      | 1348/4096 [00:03<00:07, 385.43it/s]
Adding requests:  34%|███▍      | 1388/4096 [00:03<00:06, 387.05it/s]
Adding requests:  35%|███▍      | 1427/4096 [00:03<00:06, 382.94it/s]
Adding requests:  36%|███▌      | 1467/4096 [00:03<00:06, 386.88it/s]
Adding requests:  37%|███▋      | 1507/4096 [00:03<00:06, 388.72it/s]
Adding requests:  38%|███▊      | 1546/4096 [00:04<00:06, 386.15it/s]
Adding requests:  39%|███▊      | 1585/4096 [00:04<00:06, 379.36it/s]
Adding requests:  40%|███▉      | 1623/4096 [00:04<00:06, 374.95it/s]
Adding requests:  41%|████      | 1661/4096 [00:04<00:06, 369.84it/s]
Adding requests:  42%|████▏     | 1700/4096 [00:04<00:06, 374.18it/s]
Adding requests:  42%|████▏     | 1740/4096 [00:04<00:06, 380.29it/s]
Adding requests:  43%|████▎     | 1781/4096 [00:04<00:05, 387.55it/s]
Adding requests:  44%|████▍     | 1820/4096 [00:04<00:05, 381.39it/s]
Adding requests:  45%|████▌     | 1861/4096 [00:04<00:05, 387.61it/s]
Adding requests:  46%|████▋     | 1900/4096 [00:04<00:05, 385.96it/s]
Adding requests:  47%|████▋     | 1942/4096 [00:05<00:05, 395.72it/s]
Adding requests:  48%|████▊     | 1982/4096 [00:05<00:05, 395.52it/s]
Adding requests:  49%|████▉     | 2022/4096 [00:05<00:05, 382.23it/s]
Adding requests:  50%|█████     | 2061/4096 [00:05<00:05, 380.79it/s]
Adding requests:  51%|█████▏    | 2100/4096 [00:05<00:05, 377.22it/s]
Adding requests:  52%|█████▏    | 2139/4096 [00:05<00:05, 379.97it/s]
Adding requests:  53%|█████▎    | 2178/4096 [00:05<00:05, 371.17it/s]
Adding requests:  54%|█████▍    | 2216/4096 [00:05<00:05, 364.46it/s]
Adding requests:  55%|█████▌    | 2257/4096 [00:05<00:04, 374.35it/s]
Adding requests:  56%|█████▌    | 2297/4096 [00:05<00:04, 380.32it/s]
Adding requests:  57%|█████▋    | 2338/4096 [00:06<00:04, 385.93it/s]
Adding requests:  58%|█████▊    | 2378/4096 [00:06<00:04, 388.15it/s]
Adding requests:  59%|█████▉    | 2420/4096 [00:06<00:04, 394.87it/s]
Adding requests:  60%|██████    | 2460/4096 [00:06<00:04, 390.55it/s]
Adding requests:  61%|██████    | 2501/4096 [00:06<00:04, 395.18it/s]
Adding requests:  62%|██████▏   | 2542/4096 [00:06<00:03, 399.29it/s]
Adding requests:  63%|██████▎   | 2584/4096 [00:06<00:03, 403.58it/s]
Adding requests:  64%|██████▍   | 2625/4096 [00:06<00:03, 397.15it/s]
Adding requests:  65%|██████▌   | 2665/4096 [00:06<00:03, 390.06it/s]
Adding requests:  66%|██████▌   | 2705/4096 [00:07<00:03, 386.79it/s]
Adding requests:  67%|██████▋   | 2745/4096 [00:07<00:03, 388.85it/s]
Adding requests:  68%|██████▊   | 2786/4096 [00:07<00:03, 392.87it/s]
Adding requests:  69%|██████▉   | 2827/4096 [00:07<00:03, 396.38it/s]
Adding requests:  70%|██████▉   | 2867/4096 [00:07<00:03, 395.80it/s]
Adding requests:  71%|███████   | 2907/4096 [00:07<00:03, 395.02it/s]
Adding requests:  72%|███████▏  | 2948/4096 [00:07<00:02, 396.95it/s]
Adding requests:  73%|███████▎  | 2988/4096 [00:07<00:02, 396.82it/s]
Adding requests:  74%|███████▍  | 3028/4096 [00:07<00:02, 397.59it/s]
Adding requests:  75%|███████▍  | 3070/4096 [00:07<00:02, 401.91it/s]
Adding requests:  76%|███████▌  | 3111/4096 [00:08<00:02, 400.44it/s]
Adding requests:  77%|███████▋  | 3152/4096 [00:08<00:02, 400.22it/s]
Adding requests:  78%|███████▊  | 3193/4096 [00:08<00:02, 395.45it/s]
Adding requests:  79%|███████▉  | 3234/4096 [00:08<00:02, 397.17it/s]
Adding requests:  80%|███████▉  | 3274/4096 [00:08<00:02, 391.64it/s]
Adding requests:  81%|████████  | 3314/4096 [00:08<00:02, 382.89it/s]
Adding requests:  82%|████████▏ | 3353/4096 [00:08<00:01, 384.04it/s]
Adding requests:  83%|████████▎ | 3394/4096 [00:08<00:01, 391.06it/s]
Adding requests:  84%|████████▍ | 3434/4096 [00:08<00:01, 390.46it/s]
Adding requests:  85%|████████▍ | 3474/4096 [00:08<00:01, 392.52it/s]
Adding requests:  86%|████████▌ | 3514/4096 [00:09<00:01, 392.58it/s]
Adding requests:  87%|████████▋ | 3554/4096 [00:09<00:01, 391.33it/s]
Adding requests:  88%|████████▊ | 3594/4096 [00:09<00:01, 390.25it/s]
Adding requests:  89%|████████▊ | 3635/4096 [00:09<00:01, 393.24it/s]
Adding requests:  90%|████████▉ | 3675/4096 [00:09<00:01, 386.65it/s]
Adding requests:  91%|█████████ | 3714/4096 [00:09<00:00, 386.67it/s]
Adding requests:  92%|█████████▏| 3753/4096 [00:09<00:00, 382.40it/s]
Adding requests:  93%|█████████▎| 3792/4096 [00:09<00:00, 371.64it/s]
Adding requests:  94%|█████████▎| 3830/4096 [00:09<00:00, 369.30it/s]
Adding requests:  94%|█████████▍| 3870/4096 [00:10<00:00, 376.25it/s]
Adding requests:  95%|█████████▌| 3908/4096 [00:10<00:00, 371.99it/s]
Adding requests:  96%|█████████▋| 3946/4096 [00:10<00:00, 373.33it/s]
Adding requests:  97%|█████████▋| 3984/4096 [00:10<00:00, 372.97it/s]
Adding requests:  98%|█████████▊| 4022/4096 [00:10<00:00, 374.63it/s]
Adding requests:  99%|█████████▉| 4060/4096 [00:10<00:00, 375.36it/s]
Adding requests: 100%|██████████| 4096/4096 [00:10<00:00, 385.75it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   9%|▉         | 367/4096 [00:00<00:02, 1360.53it/s, est. speed input: 1393307.54 toks/s, output: 1360.57 toks/s]
Processed prompts:  12%|█▏        | 504/4096 [00:03<00:33, 108.29it/s, est. speed input: 138794.32 toks/s, output: 135.54 toks/s]   
Processed prompts:  14%|█▎        | 563/4096 [00:05<00:45, 78.05it/s, est. speed input: 105574.19 toks/s, output: 103.10 toks/s] 
Processed prompts:  15%|█▍        | 597/4096 [00:06<00:50, 69.27it/s, est. speed input: 96542.14 toks/s, output: 94.28 toks/s]  
Processed prompts:  15%|█▌        | 623/4096 [00:07<00:58, 59.69it/s, est. speed input: 88568.41 toks/s, output: 86.49 toks/s]
Processed prompts:  16%|█▌        | 655/4096 [00:08<01:03, 53.80it/s, est. speed input: 83087.59 toks/s, output: 81.14 toks/s]
Processed prompts:  17%|█▋        | 687/4096 [00:08<01:09, 49.23it/s, est. speed input: 78672.35 toks/s, output: 76.83 toks/s]
Processed prompts:  18%|█▊        | 719/4096 [00:09<01:13, 45.76it/s, est. speed input: 75032.69 toks/s, output: 73.27 toks/s]
Processed prompts:  18%|█▊        | 751/4096 [00:10<01:17, 43.18it/s, est. speed input: 71973.74 toks/s, output: 70.29 toks/s]
Processed prompts:  19%|█▉        | 783/4096 [00:11<01:19, 41.72it/s, est. speed input: 69557.47 toks/s, output: 67.93 toks/s]
Processed prompts:  20%|█▉        | 815/4096 [00:12<01:21, 40.24it/s, est. speed input: 67307.30 toks/s, output: 65.73 toks/s]
Processed prompts:  21%|██        | 847/4096 [00:13<01:22, 39.21it/s, est. speed input: 65361.26 toks/s, output: 63.83 toks/s]
Processed prompts:  21%|██▏       | 879/4096 [00:14<01:23, 38.48it/s, est. speed input: 63652.55 toks/s, output: 62.16 toks/s]
Processed prompts:  22%|██▏       | 911/4096 [00:15<01:23, 37.95it/s, est. speed input: 62139.29 toks/s, output: 60.68 toks/s]
Processed prompts:  23%|██▎       | 943/4096 [00:15<01:23, 37.58it/s, est. speed input: 60793.29 toks/s, output: 59.37 toks/s]
Processed prompts:  24%|██▍       | 975/4096 [00:16<01:23, 37.28it/s, est. speed input: 59575.36 toks/s, output: 58.18 toks/s]
Processed prompts:  25%|██▍       | 1007/4096 [00:17<01:23, 37.09it/s, est. speed input: 58482.52 toks/s, output: 57.11 toks/s]
Processed prompts:  25%|██▌       | 1039/4096 [00:18<01:22, 36.98it/s, est. speed input: 57499.87 toks/s, output: 56.15 toks/s]
Processed prompts:  26%|██▌       | 1071/4096 [00:19<01:22, 36.89it/s, est. speed input: 56600.53 toks/s, output: 55.27 toks/s]
Processed prompts:  27%|██▋       | 1103/4096 [00:20<01:21, 36.81it/s, est. speed input: 55777.42 toks/s, output: 54.47 toks/s]
Processed prompts:  28%|██▊       | 1135/4096 [00:21<01:20, 36.75it/s, est. speed input: 55020.29 toks/s, output: 53.73 toks/s]
Processed prompts:  28%|██▊       | 1167/4096 [00:21<01:19, 36.68it/s, est. speed input: 54319.11 toks/s, output: 53.05 toks/s]
Processed prompts:  29%|██▉       | 1199/4096 [00:22<01:18, 37.00it/s, est. speed input: 53738.46 toks/s, output: 52.48 toks/s]
Processed prompts:  30%|███       | 1231/4096 [00:23<01:16, 37.24it/s, est. speed input: 53202.95 toks/s, output: 51.96 toks/s]
Processed prompts:  31%|███       | 1263/4096 [00:24<01:16, 37.08it/s, est. speed input: 52647.66 toks/s, output: 51.41 toks/s]
Processed prompts:  32%|███▏      | 1295/4096 [00:25<01:15, 36.92it/s, est. speed input: 52124.24 toks/s, output: 50.90 toks/s]
Processed prompts:  32%|███▏      | 1327/4096 [00:26<01:14, 37.17it/s, est. speed input: 51691.37 toks/s, output: 50.48 toks/s]
Processed prompts:  33%|███▎      | 1359/4096 [00:27<01:14, 36.97it/s, est. speed input: 51230.07 toks/s, output: 50.03 toks/s]
Processed prompts:  34%|███▍      | 1391/4096 [00:28<01:13, 36.85it/s, est. speed input: 50799.78 toks/s, output: 49.61 toks/s]
Processed prompts:  35%|███▍      | 1423/4096 [00:28<01:12, 36.79it/s, est. speed input: 50399.52 toks/s, output: 49.22 toks/s]
Processed prompts:  36%|███▌      | 1455/4096 [00:29<01:11, 37.10it/s, est. speed input: 50068.45 toks/s, output: 48.89 toks/s]
Processed prompts:  36%|███▋      | 1487/4096 [00:30<01:10, 36.93it/s, est. speed input: 49706.17 toks/s, output: 48.54 toks/s]
Processed prompts:  37%|███▋      | 1519/4096 [00:31<01:09, 37.21it/s, est. speed input: 49413.67 toks/s, output: 48.26 toks/s]
Processed prompts:  38%|███▊      | 1551/4096 [00:32<01:08, 37.34it/s, est. speed input: 49128.43 toks/s, output: 47.98 toks/s]
Processed prompts:  39%|███▊      | 1583/4096 [00:33<01:07, 37.12it/s, est. speed input: 48821.42 toks/s, output: 47.68 toks/s]
Processed prompts:  39%|███▉      | 1615/4096 [00:34<01:06, 37.34it/s, est. speed input: 48571.78 toks/s, output: 47.43 toks/s]
Processed prompts:  40%|████      | 1647/4096 [00:34<01:06, 37.08it/s, est. speed input: 48290.33 toks/s, output: 47.16 toks/s]
Processed prompts:  41%|████      | 1679/4096 [00:35<01:05, 36.89it/s, est. speed input: 48021.66 toks/s, output: 46.90 toks/s]
Processed prompts:  42%|████▏     | 1711/4096 [00:36<01:04, 36.77it/s, est. speed input: 47767.26 toks/s, output: 46.65 toks/s]
Processed prompts:  43%|████▎     | 1743/4096 [00:37<01:02, 37.41it/s, est. speed input: 47595.51 toks/s, output: 46.48 toks/s]
Processed prompts:  43%|████▎     | 1775/4096 [00:38<01:02, 37.14it/s, est. speed input: 47362.83 toks/s, output: 46.25 toks/s]
Processed prompts:  44%|████▍     | 1807/4096 [00:39<01:01, 36.95it/s, est. speed input: 47139.88 toks/s, output: 46.04 toks/s]
Processed prompts:  45%|████▍     | 1839/4096 [00:40<01:01, 36.81it/s, est. speed input: 46926.65 toks/s, output: 45.83 toks/s]
Processed prompts:  46%|████▌     | 1871/4096 [00:40<01:00, 37.07it/s, est. speed input: 46753.60 toks/s, output: 45.66 toks/s]
Processed prompts:  46%|████▋     | 1903/4096 [00:41<00:59, 36.90it/s, est. speed input: 46557.28 toks/s, output: 45.47 toks/s]
Processed prompts:  47%|████▋     | 1935/4096 [00:42<00:58, 36.77it/s, est. speed input: 46368.29 toks/s, output: 45.28 toks/s]
Processed prompts:  48%|████▊     | 1967/4096 [00:43<00:57, 37.08it/s, est. speed input: 46220.13 toks/s, output: 45.14 toks/s]
Processed prompts:  49%|████▉     | 1999/4096 [00:44<00:56, 36.91it/s, est. speed input: 46046.01 toks/s, output: 44.97 toks/s]
Processed prompts:  50%|████▉     | 2031/4096 [00:45<00:56, 36.78it/s, est. speed input: 45877.68 toks/s, output: 44.80 toks/s]
Processed prompts:  50%|█████     | 2063/4096 [00:46<00:54, 37.07it/s, est. speed input: 45745.31 toks/s, output: 44.67 toks/s]
Processed prompts:  51%|█████     | 2095/4096 [00:47<00:54, 36.88it/s, est. speed input: 45588.63 toks/s, output: 44.52 toks/s]
Processed prompts:  52%|█████▏    | 2127/4096 [00:47<00:53, 36.75it/s, est. speed input: 45437.21 toks/s, output: 44.37 toks/s]
Processed prompts:  53%|█████▎    | 2159/4096 [00:48<00:52, 36.67it/s, est. speed input: 45291.74 toks/s, output: 44.23 toks/s]
Processed prompts:  53%|█████▎    | 2191/4096 [00:49<00:51, 37.00it/s, est. speed input: 45179.62 toks/s, output: 44.12 toks/s]
Processed prompts:  54%|█████▍    | 2223/4096 [00:50<00:50, 36.83it/s, est. speed input: 45043.02 toks/s, output: 43.99 toks/s]
Processed prompts:  55%|█████▌    | 2255/4096 [00:51<00:50, 36.72it/s, est. speed input: 44911.35 toks/s, output: 43.86 toks/s]
Processed prompts:  56%|█████▌    | 2287/4096 [00:52<00:49, 36.62it/s, est. speed input: 44782.80 toks/s, output: 43.73 toks/s]
Processed prompts:  57%|█████▋    | 2319/4096 [00:53<00:48, 36.57it/s, est. speed input: 44659.63 toks/s, output: 43.61 toks/s]
Processed prompts:  57%|█████▋    | 2351/4096 [00:54<00:47, 36.54it/s, est. speed input: 44541.25 toks/s, output: 43.50 toks/s]
Processed prompts:  58%|█████▊    | 2383/4096 [00:54<00:46, 36.51it/s, est. speed input: 44425.52 toks/s, output: 43.38 toks/s]
Processed prompts:  59%|█████▉    | 2415/4096 [00:55<00:46, 36.49it/s, est. speed input: 44313.75 toks/s, output: 43.28 toks/s]
Processed prompts:  60%|█████▉    | 2447/4096 [00:56<00:45, 36.46it/s, est. speed input: 44204.60 toks/s, output: 43.17 toks/s]
Processed prompts:  61%|██████    | 2479/4096 [00:57<00:44, 36.44it/s, est. speed input: 44098.53 toks/s, output: 43.06 toks/s]
Processed prompts:  61%|██████▏   | 2511/4096 [00:58<00:43, 36.44it/s, est. speed input: 43996.30 toks/s, output: 42.97 toks/s]
Processed prompts:  62%|██████▏   | 2543/4096 [00:59<00:42, 36.80it/s, est. speed input: 43918.83 toks/s, output: 42.89 toks/s]
Processed prompts:  63%|██████▎   | 2575/4096 [01:00<00:41, 37.08it/s, est. speed input: 43844.41 toks/s, output: 42.82 toks/s]
Processed prompts:  64%|██████▎   | 2607/4096 [01:01<00:40, 36.86it/s, est. speed input: 43749.25 toks/s, output: 42.72 toks/s]
Processed prompts:  64%|██████▍   | 2639/4096 [01:01<00:39, 36.72it/s, est. speed input: 43656.87 toks/s, output: 42.63 toks/s]
Processed prompts:  65%|██████▌   | 2671/4096 [01:02<00:38, 36.63it/s, est. speed input: 43568.21 toks/s, output: 42.55 toks/s]
Processed prompts:  66%|██████▌   | 2703/4096 [01:03<00:38, 36.56it/s, est. speed input: 43480.95 toks/s, output: 42.46 toks/s]
Processed prompts:  67%|██████▋   | 2735/4096 [01:04<00:36, 36.85it/s, est. speed input: 43414.76 toks/s, output: 42.40 toks/s]
Processed prompts:  68%|██████▊   | 2767/4096 [01:05<00:36, 36.70it/s, est. speed input: 43331.33 toks/s, output: 42.32 toks/s]
Processed prompts:  68%|██████▊   | 2799/4096 [01:06<00:35, 36.59it/s, est. speed input: 43249.93 toks/s, output: 42.24 toks/s]
Processed prompts:  69%|██████▉   | 2831/4096 [01:07<00:34, 36.55it/s, est. speed input: 43172.50 toks/s, output: 42.16 toks/s]
Processed prompts:  70%|██████▉   | 2863/4096 [01:08<00:33, 36.49it/s, est. speed input: 43095.34 toks/s, output: 42.09 toks/s]
Processed prompts:  71%|███████   | 2895/4096 [01:08<00:32, 37.49it/s, est. speed input: 43071.18 toks/s, output: 42.06 toks/s]
Processed prompts:  71%|███████▏  | 2927/4096 [01:09<00:31, 37.12it/s, est. speed input: 42996.49 toks/s, output: 41.99 toks/s]
Processed prompts:  72%|███████▏  | 2959/4096 [01:10<00:30, 36.89it/s, est. speed input: 42924.46 toks/s, output: 41.92 toks/s]
Processed prompts:  73%|███████▎  | 2991/4096 [01:11<00:30, 36.73it/s, est. speed input: 42854.37 toks/s, output: 41.85 toks/s]
Processed prompts:  74%|███████▍  | 3023/4096 [01:12<00:29, 36.63it/s, est. speed input: 42786.78 toks/s, output: 41.78 toks/s]
Processed prompts:  75%|███████▍  | 3055/4096 [01:13<00:28, 36.55it/s, est. speed input: 42720.04 toks/s, output: 41.72 toks/s]
Processed prompts:  75%|███████▌  | 3087/4096 [01:14<00:27, 36.49it/s, est. speed input: 42654.73 toks/s, output: 41.65 toks/s]
Processed prompts:  76%|███████▌  | 3119/4096 [01:14<00:26, 36.45it/s, est. speed input: 42591.15 toks/s, output: 41.59 toks/s]
Processed prompts:  77%|███████▋  | 3151/4096 [01:15<00:25, 36.41it/s, est. speed input: 42528.27 toks/s, output: 41.53 toks/s]
Processed prompts:  78%|███████▊  | 3183/4096 [01:16<00:25, 36.39it/s, est. speed input: 42467.45 toks/s, output: 41.47 toks/s]
Processed prompts:  78%|███████▊  | 3215/4096 [01:17<00:24, 36.38it/s, est. speed input: 42408.03 toks/s, output: 41.41 toks/s]
Processed prompts:  79%|███████▉  | 3247/4096 [01:18<00:23, 36.38it/s, est. speed input: 42350.12 toks/s, output: 41.36 toks/s]
Processed prompts:  80%|████████  | 3279/4096 [01:19<00:22, 36.37it/s, est. speed input: 42293.37 toks/s, output: 41.30 toks/s]
Processed prompts:  81%|████████  | 3311/4096 [01:20<00:21, 36.36it/s, est. speed input: 42237.58 toks/s, output: 41.25 toks/s]
Processed prompts:  82%|████████▏ | 3343/4096 [01:21<00:20, 36.36it/s, est. speed input: 42183.48 toks/s, output: 41.19 toks/s]
Processed prompts:  82%|████████▏ | 3375/4096 [01:22<00:19, 36.37it/s, est. speed input: 42130.57 toks/s, output: 41.14 toks/s]
Processed prompts:  83%|████████▎ | 3407/4096 [01:22<00:18, 36.37it/s, est. speed input: 42078.66 toks/s, output: 41.09 toks/s]
Processed prompts:  84%|████████▍ | 3439/4096 [01:23<00:18, 36.35it/s, est. speed input: 42027.24 toks/s, output: 41.04 toks/s]
Processed prompts:  85%|████████▍ | 3471/4096 [01:24<00:17, 36.35it/s, est. speed input: 41977.15 toks/s, output: 40.99 toks/s]
Processed prompts:  86%|████████▌ | 3503/4096 [01:25<00:16, 36.35it/s, est. speed input: 41928.13 toks/s, output: 40.95 toks/s]
Processed prompts:  86%|████████▋ | 3535/4096 [01:26<00:15, 36.35it/s, est. speed input: 41880.13 toks/s, output: 40.90 toks/s]
Processed prompts:  87%|████████▋ | 3567/4096 [01:27<00:14, 36.73it/s, est. speed input: 41847.67 toks/s, output: 40.87 toks/s]
Processed prompts:  88%|████████▊ | 3599/4096 [01:28<00:13, 36.59it/s, est. speed input: 41800.81 toks/s, output: 40.82 toks/s]
Processed prompts:  89%|████████▊ | 3631/4096 [01:29<00:12, 36.49it/s, est. speed input: 41754.22 toks/s, output: 40.78 toks/s]
Processed prompts:  89%|████████▉ | 3663/4096 [01:29<00:11, 36.79it/s, est. speed input: 41722.60 toks/s, output: 40.74 toks/s]
Processed prompts:  90%|█████████ | 3695/4096 [01:30<00:10, 36.64it/s, est. speed input: 41678.40 toks/s, output: 40.70 toks/s]
Processed prompts:  91%|█████████ | 3727/4096 [01:31<00:10, 36.54it/s, est. speed input: 41634.96 toks/s, output: 40.66 toks/s]
Processed prompts:  92%|█████████▏| 3759/4096 [01:32<00:09, 36.46it/s, est. speed input: 41592.44 toks/s, output: 40.62 toks/s]
Processed prompts:  93%|█████████▎| 3791/4096 [01:33<00:08, 36.40it/s, est. speed input: 41550.28 toks/s, output: 40.58 toks/s]
Processed prompts:  93%|█████████▎| 3823/4096 [01:34<00:07, 36.38it/s, est. speed input: 41509.67 toks/s, output: 40.54 toks/s]
Processed prompts:  94%|█████████▍| 3855/4096 [01:35<00:06, 36.35it/s, est. speed input: 41469.32 toks/s, output: 40.50 toks/s]
Processed prompts:  95%|█████████▍| 3887/4096 [01:36<00:05, 36.33it/s, est. speed input: 41429.66 toks/s, output: 40.46 toks/s]
Processed prompts:  96%|█████████▌| 3919/4096 [01:36<00:04, 36.99it/s, est. speed input: 41413.73 toks/s, output: 40.44 toks/s]
Processed prompts:  96%|█████████▋| 3951/4096 [01:37<00:03, 36.78it/s, est. speed input: 41375.53 toks/s, output: 40.41 toks/s]
Processed prompts:  97%|█████████▋| 3983/4096 [01:38<00:03, 36.97it/s, est. speed input: 41349.02 toks/s, output: 40.38 toks/s]
Processed prompts:  98%|█████████▊| 4015/4096 [01:39<00:02, 36.77it/s, est. speed input: 41312.12 toks/s, output: 40.34 toks/s]
Processed prompts:  99%|█████████▉| 4047/4096 [01:40<00:01, 36.63it/s, est. speed input: 41275.82 toks/s, output: 40.31 toks/s]
Processed prompts: 100%|█████████▉| 4079/4096 [01:40<00:00, 42.50it/s, est. speed input: 41407.87 toks/s, output: 40.44 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [01:40<00:00, 42.50it/s, est. speed input: 41580.18 toks/s, output: 40.61 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [01:40<00:00, 40.61it/s, est. speed input: 41580.18 toks/s, output: 40.61 toks/s]
[rank0]:[W126 11:02:27.747375040 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 174.9s

测试结果:
  Requests/s:   36.74
  Tokens/s:     37655.83
  Total Reqs:   4096
  Elapsed:      111.49s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     37619.09

============================================================
[8/8] 测试 M=65536
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 65536
│   M_prefill     = 65536 (= 64 x 1024)
│   M_decode      = 64
│   batched_tokens = 65536 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 8192
│   --max-num-seqs           = 64
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 65536
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 11:03:20 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1255357) WARNING 01-26 11:03:36 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]     self.model_runner.profile_run()
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]     hidden_states = self.model(
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]                     ^^^^^^^^^^^
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]     def forward(
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]     raise e
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]   File "<eval_with_key>.58", line 325, in forward
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]     return range_entry.runnable(*args)
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]     return self._compiled_fn(*args)
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]     return compiled_fn(full_args)
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]                             ^^^^^^^
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 724, in inner_fn
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]     outs = compiled_fn(args)
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]            ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]     return self.current_callable(inputs)
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]     out = model(new_inputs)
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]   File "/tmp/torchinductor_root/px/cpxprakauq56cywh5g2wry5eahubyqtszpthcig5qwp2iwewnerw.py", line 1090, in call
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]     triton_poi_fused_mul_quant_only_int8_silu_slice_1.run(buf15, buf16, triton_poi_fused_mul_quant_only_int8_silu_slice_1_xnumel, stream=stream0)
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 1272, in run
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]     self.autotune_to_one_config(*args, **kwargs)
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 1048, in autotune_to_one_config
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]     timings = self.benchmark_all_configs(*args, **kwargs)
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 1023, in benchmark_all_configs
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]     launcher: self.bench(launcher, *args, **kwargs)
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 891, in bench
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]     return benchmarker.benchmark_gpu(kernel_call, rep=40)
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/benchmarking.py", line 39, in wrapper
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]     return fn(self, *args, **kwargs)
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/benchmarking.py", line 247, in benchmark_gpu
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]     torch.cuda.synchronize()
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py", line 1083, in synchronize
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]     return torch._C._cuda_synchronize()
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866] torch.AcceleratorError: CUDA error: an illegal memory access was encountered
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866] Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866] For debugging consider passing CUDA_LAUNCH_BLOCKING=1
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
(EngineCore_DP0 pid=1255357) ERROR 01-26 11:03:45 [core.py:866] 


─── STDERR ───
[2026-01-26 11:03:19] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:03:20] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:03:20] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 11:03:20] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:03:20] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:03:20] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:03:20] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:03:20] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:03:20] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:03:20] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:03:20] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:03:20] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:03:20] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:03:20] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 11:03:26] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:03:27] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:03:27] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 11:03:27] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:03:27] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:03:27] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:03:27] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:03:27] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:03:27] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:03:27] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:03:27] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:03:27] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:03:27] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:03:27] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1255357) [2026-01-26 11:03:29] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=1255357) [2026-01-26 11:03:29] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1255357) [2026-01-26 11:03:29] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=1255357) [2026-01-26 11:03:29] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=1255357) [2026-01-26 11:03:29] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=1255357) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1255357) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.56it/s]
(EngineCore_DP0 pid=1255357) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  1.93it/s]
(EngineCore_DP0 pid=1255357) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.00it/s]
(EngineCore_DP0 pid=1255357) 
(EngineCore_DP0 pid=1255357) [rank0]:W0126 11:03:42.662000 1255357 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1255357) [rank0]:W0126 11:03:42.923000 1255357 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1255357) [2026-01-26 11:03:43] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=4608, K=3584), falling back to default heuristic
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=1255357) [rank0]:W0126 11:03:44.200000 1255357 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1255357) [rank0]:W0126 11:03:44.317000 1255357 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1255357) [2026-01-26 11:03:44] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3584, K=3584), falling back to default heuristic
(EngineCore_DP0 pid=1255357) [2026-01-26 11:03:44] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=37888, K=3584), falling back to default heuristic
(EngineCore_DP0 pid=1255357) Process EngineCore_DP0:
(EngineCore_DP0 pid=1255357) Traceback (most recent call last):
(EngineCore_DP0 pid=1255357)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=1255357)     self.run()
(EngineCore_DP0 pid=1255357)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=1255357)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=1255357)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=1255357)     raise e
(EngineCore_DP0 pid=1255357)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1255357)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1255357)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1255357)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1255357)     super().__init__(
(EngineCore_DP0 pid=1255357)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=1255357)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=1255357)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1255357)   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=1255357)     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=1255357)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1255357)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=1255357)     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=1255357)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1255357)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=1255357)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=1255357)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1255357)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=1255357)     return func(*args, **kwargs)
(EngineCore_DP0 pid=1255357)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1255357)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=1255357)     return func(*args, **kwargs)
(EngineCore_DP0 pid=1255357)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1255357)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=1255357)     self.model_runner.profile_run()
(EngineCore_DP0 pid=1255357)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=1255357)     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=1255357)                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1255357)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=1255357)     return func(*args, **kwargs)
(EngineCore_DP0 pid=1255357)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1255357)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=1255357)     outputs = self.model(
(EngineCore_DP0 pid=1255357)               ^^^^^^^^^^^
(EngineCore_DP0 pid=1255357)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=1255357)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=1255357)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1255357)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=1255357)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=1255357)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1255357)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=1255357)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=1255357)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1255357)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=1255357)     hidden_states = self.model(
(EngineCore_DP0 pid=1255357)                     ^^^^^^^^^^^
(EngineCore_DP0 pid=1255357)   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=1255357)     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=1255357)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1255357)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=1255357)     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=1255357)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1255357)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=1255357)     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=1255357)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1255357)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
(EngineCore_DP0 pid=1255357)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=1255357)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1255357)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
(EngineCore_DP0 pid=1255357)     def forward(
(EngineCore_DP0 pid=1255357)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=1255357)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=1255357)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1255357)   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=1255357)     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=1255357)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1255357)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=1255357)     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=1255357)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1255357)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=1255357)     raise e
(EngineCore_DP0 pid=1255357)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=1255357)     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=1255357)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1255357)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=1255357)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=1255357)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1255357)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=1255357)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=1255357)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1255357)   File "<eval_with_key>.58", line 325, in forward
(EngineCore_DP0 pid=1255357)     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
(EngineCore_DP0 pid=1255357)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1255357)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=1255357)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=1255357)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1255357)   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=1255357)     return range_entry.runnable(*args)
(EngineCore_DP0 pid=1255357)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1255357)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=1255357)     return self._compiled_fn(*args)
(EngineCore_DP0 pid=1255357)            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1255357)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=1255357)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=1255357)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1255357)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=1255357)     return compiled_fn(full_args)
(EngineCore_DP0 pid=1255357)            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1255357)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=1255357)     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=1255357)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1255357)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=1255357)     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=1255357)                             ^^^^^^^
(EngineCore_DP0 pid=1255357)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 724, in inner_fn
(EngineCore_DP0 pid=1255357)     outs = compiled_fn(args)
(EngineCore_DP0 pid=1255357)            ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1255357)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=1255357)     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=1255357)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1255357)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=1255357)     return self.current_callable(inputs)
(EngineCore_DP0 pid=1255357)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1255357)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=1255357)     out = model(new_inputs)
(EngineCore_DP0 pid=1255357)           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1255357)   File "/tmp/torchinductor_root/px/cpxprakauq56cywh5g2wry5eahubyqtszpthcig5qwp2iwewnerw.py", line 1090, in call
(EngineCore_DP0 pid=1255357)     triton_poi_fused_mul_quant_only_int8_silu_slice_1.run(buf15, buf16, triton_poi_fused_mul_quant_only_int8_silu_slice_1_xnumel, stream=stream0)
(EngineCore_DP0 pid=1255357)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 1272, in run
(EngineCore_DP0 pid=1255357)     self.autotune_to_one_config(*args, **kwargs)
(EngineCore_DP0 pid=1255357)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 1048, in autotune_to_one_config
(EngineCore_DP0 pid=1255357)     timings = self.benchmark_all_configs(*args, **kwargs)
(EngineCore_DP0 pid=1255357)               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1255357)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 1023, in benchmark_all_configs
(EngineCore_DP0 pid=1255357)     launcher: self.bench(launcher, *args, **kwargs)
(EngineCore_DP0 pid=1255357)               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1255357)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 891, in bench
(EngineCore_DP0 pid=1255357)     return benchmarker.benchmark_gpu(kernel_call, rep=40)
(EngineCore_DP0 pid=1255357)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1255357)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/benchmarking.py", line 39, in wrapper
(EngineCore_DP0 pid=1255357)     return fn(self, *args, **kwargs)
(EngineCore_DP0 pid=1255357)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1255357)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/benchmarking.py", line 247, in benchmark_gpu
(EngineCore_DP0 pid=1255357)     torch.cuda.synchronize()
(EngineCore_DP0 pid=1255357)   File "/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py", line 1083, in synchronize
(EngineCore_DP0 pid=1255357)     return torch._C._cuda_synchronize()
(EngineCore_DP0 pid=1255357)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1255357) torch.AcceleratorError: CUDA error: an illegal memory access was encountered
(EngineCore_DP0 pid=1255357) Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
(EngineCore_DP0 pid=1255357) CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
(EngineCore_DP0 pid=1255357) For debugging consider passing CUDA_LAUNCH_BLOCKING=1
(EngineCore_DP0 pid=1255357) Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
(EngineCore_DP0 pid=1255357) 
[rank0]:[W126 11:03:46.433273576 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[ERROR] 测试失败: M=65536 (exit code: 1)


------------------------------------------------------------
  生成 CSV: Qwen2.5-7B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_INT8_py312_cu129_x86_64/cublaslt/Qwen2.5-7B-INT8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,28.3660,14551.7367,4.5125
1024,1024,1,128,128,28.4224,29132.9799,4.5035
2048,1024,2,256,128,35.6390,36530.0165,7.1831
4096,1024,4,512,128,38.9924,39967.2543,13.1307
8192,1024,8,1024,128,36.1274,37030.5967,28.3441
16384,1024,16,2048,128,36.7620,37681.0813,55.7097
32768,1024,32,4096,128,36.7374,37655.8257,111.4940
65536,1024,64,8192,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[INFO] 完成: 7 成功, 1 失败

============================================================
  Qwen2.5-7B-INT8 | cuSPARSELt (2_4) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_4
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_INT8_py312_cu129_x86_64/cusparselt/2_4

============================================================
[1/8] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 11:03:56 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1256353) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1256353) WARNING 01-26 11:04:12 [backends.py:609] Failed to read file <frozen os>
Throughput: 32.63 requests/s, 16740.05 total tokens/s, 32.63 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-26 11:03:55] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:03:56] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:03:56] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 11:03:56] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:03:56] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:03:56] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:03:56] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:03:56] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:03:56] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:03:56] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:03:56] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:03:56] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:03:56] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:03:56] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 11:04:02] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:04:03] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:04:03] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 11:04:03] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:04:03] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:04:03] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:04:03] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:04:03] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:04:03] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:04:03] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:04:03] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:04:03] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:04:03] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:04:03] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1256353) [2026-01-26 11:04:05] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1256353) [2026-01-26 11:04:05] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1256353) [2026-01-26 11:04:05] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1256353) [2026-01-26 11:04:05] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=1256353) [2026-01-26 11:04:05] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=1256353) [2026-01-26 11:04:05] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=1256353) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1256353) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.59it/s]
(EngineCore_DP0 pid=1256353) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.89it/s]
(EngineCore_DP0 pid=1256353) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.97it/s]
(EngineCore_DP0 pid=1256353) 
(EngineCore_DP0 pid=1256353) [2026-01-26 11:04:06] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 3584] -> 1D uint8
(EngineCore_DP0 pid=1256353) [2026-01-26 11:04:06] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 10321920 bytes
(EngineCore_DP0 pid=1256353) [2026-01-26 11:04:06] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 3584] -> 1D uint8
(EngineCore_DP0 pid=1256353) [2026-01-26 11:04:06] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 8028160 bytes
(EngineCore_DP0 pid=1256353) [2026-01-26 11:04:06] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 3584] -> 1D uint8
(EngineCore_DP0 pid=1256353) [2026-01-26 11:04:06] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 84869120 bytes
(EngineCore_DP0 pid=1256353) [2026-01-26 11:04:06] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 18944] -> 1D uint8
(EngineCore_DP0 pid=1256353) [2026-01-26 11:04:06] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 42434560 bytes
(EngineCore_DP0 pid=1256353) 2026-01-26 11:04:24,545 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1256353) 2026-01-26 11:04:24,588 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1256353) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  2.25it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:01<00:00,  1.27it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:01<00:00,  1.36it/s]
(EngineCore_DP0 pid=1256353) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 12.77it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  30%|███       | 39/128 [00:00<00:00, 389.93it/s]
Adding requests:  87%|████████▋ | 111/128 [00:00<00:00, 580.05it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 566.60it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<01:20,  1.58it/s, est. speed input: 806.72 toks/s, output: 1.58 toks/s]
Processed prompts:   5%|▍         | 6/128 [00:00<00:12, 10.08it/s, est. speed input: 4063.68 toks/s, output: 7.94 toks/s]
Processed prompts:   9%|▊         | 11/128 [00:00<00:06, 17.39it/s, est. speed input: 6420.99 toks/s, output: 12.54 toks/s]
Processed prompts:  12%|█▎        | 16/128 [00:00<00:04, 23.39it/s, est. speed input: 8208.54 toks/s, output: 16.03 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:01<00:03, 28.15it/s, est. speed input: 9613.08 toks/s, output: 18.78 toks/s]
Processed prompts:  20%|██        | 26/128 [00:01<00:03, 31.83it/s, est. speed input: 10748.44 toks/s, output: 20.99 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:01<00:02, 34.58it/s, est. speed input: 11682.76 toks/s, output: 22.82 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:01<00:02, 36.61it/s, est. speed input: 12467.23 toks/s, output: 24.35 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:01<00:02, 38.05it/s, est. speed input: 13130.43 toks/s, output: 25.64 toks/s]
Processed prompts:  36%|███▌      | 46/128 [00:01<00:02, 39.09it/s, est. speed input: 13701.60 toks/s, output: 26.76 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:01<00:01, 39.81it/s, est. speed input: 14196.56 toks/s, output: 27.73 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:01<00:01, 40.33it/s, est. speed input: 14631.06 toks/s, output: 28.58 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:02<00:01, 40.68it/s, est. speed input: 15014.39 toks/s, output: 29.32 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:02<00:01, 40.93it/s, est. speed input: 15356.27 toks/s, output: 29.99 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:02<00:01, 41.14it/s, est. speed input: 15664.77 toks/s, output: 30.59 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:02<00:01, 41.29it/s, est. speed input: 15943.09 toks/s, output: 31.14 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:02<00:01, 41.41it/s, est. speed input: 16196.09 toks/s, output: 31.63 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:02<00:01, 41.44it/s, est. speed input: 16423.24 toks/s, output: 32.08 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:02<00:00, 41.50it/s, est. speed input: 16633.26 toks/s, output: 32.49 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:02<00:00, 41.56it/s, est. speed input: 16826.58 toks/s, output: 32.86 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:03<00:00, 41.59it/s, est. speed input: 17004.93 toks/s, output: 33.21 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:03<00:00, 41.62it/s, est. speed input: 17169.40 toks/s, output: 33.53 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:03<00:00, 41.64it/s, est. speed input: 17321.85 toks/s, output: 33.83 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:03<00:00, 41.60it/s, est. speed input: 17461.05 toks/s, output: 34.10 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:03<00:00, 41.02it/s, est. speed input: 17563.63 toks/s, output: 34.30 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:03<00:00, 41.23it/s, est. speed input: 17688.36 toks/s, output: 34.55 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 41.23it/s, est. speed input: 17736.65 toks/s, output: 34.64 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 34.64it/s, est. speed input: 17736.65 toks/s, output: 34.64 toks/s]
[rank0]:[W126 11:04:31.858279574 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 44.6s

测试结果:
  Requests/s:   32.63
  Tokens/s:     16740.05
  Total Reqs:   128
  Elapsed:      3.92s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     16707.41

============================================================
[2/8] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 11:04:41 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1257628) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1257628) WARNING 01-26 11:04:57 [backends.py:609] Failed to read file <frozen os>
Throughput: 34.39 requests/s, 35246.97 total tokens/s, 34.39 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-26 11:04:40] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:04:41] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:04:41] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 11:04:41] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:04:41] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:04:41] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:04:41] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:04:41] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:04:41] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:04:41] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:04:41] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:04:41] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:04:41] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:04:41] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 11:04:47] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:04:48] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:04:48] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 11:04:48] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:04:48] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:04:48] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:04:48] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:04:48] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:04:48] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:04:48] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:04:48] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:04:48] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:04:48] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:04:48] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1257628) [2026-01-26 11:04:50] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1257628) [2026-01-26 11:04:50] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1257628) [2026-01-26 11:04:50] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1257628) [2026-01-26 11:04:50] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=1257628) [2026-01-26 11:04:50] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=1257628) [2026-01-26 11:04:50] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=1257628) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1257628) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.59it/s]
(EngineCore_DP0 pid=1257628) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.88it/s]
(EngineCore_DP0 pid=1257628) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.96it/s]
(EngineCore_DP0 pid=1257628) 
(EngineCore_DP0 pid=1257628) [2026-01-26 11:04:51] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 3584] -> 1D uint8
(EngineCore_DP0 pid=1257628) [2026-01-26 11:04:51] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 10321920 bytes
(EngineCore_DP0 pid=1257628) [2026-01-26 11:04:51] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 3584] -> 1D uint8
(EngineCore_DP0 pid=1257628) [2026-01-26 11:04:51] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 8028160 bytes
(EngineCore_DP0 pid=1257628) [2026-01-26 11:04:51] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 3584] -> 1D uint8
(EngineCore_DP0 pid=1257628) [2026-01-26 11:04:51] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 84869120 bytes
(EngineCore_DP0 pid=1257628) [2026-01-26 11:04:51] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 18944] -> 1D uint8
(EngineCore_DP0 pid=1257628) [2026-01-26 11:04:51] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 42434560 bytes
(EngineCore_DP0 pid=1257628) 2026-01-26 11:05:08,354 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1257628) 2026-01-26 11:05:08,378 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1257628) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  1.93it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  2.68it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  2.53it/s]
(EngineCore_DP0 pid=1257628) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 15.71it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  14%|█▍        | 18/128 [00:00<00:00, 179.93it/s]
Adding requests:  45%|████▍     | 57/128 [00:00<00:00, 301.53it/s]
Adding requests:  74%|███████▍  | 95/128 [00:00<00:00, 334.96it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 327.66it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 2/128 [00:00<00:07, 17.16it/s, est. speed input: 17575.03 toks/s, output: 17.16 toks/s]
Processed prompts:   5%|▍         | 6/128 [00:00<00:04, 29.61it/s, est. speed input: 28274.13 toks/s, output: 27.61 toks/s]
Processed prompts:   8%|▊         | 10/128 [00:00<00:03, 33.90it/s, est. speed input: 32086.28 toks/s, output: 31.33 toks/s]
Processed prompts:  11%|█         | 14/128 [00:00<00:03, 35.98it/s, est. speed input: 34051.79 toks/s, output: 33.25 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:00<00:02, 37.16it/s, est. speed input: 35252.92 toks/s, output: 34.42 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:00<00:02, 37.87it/s, est. speed input: 36060.29 toks/s, output: 35.21 toks/s]
Processed prompts:  20%|██        | 26/128 [00:00<00:02, 38.27it/s, est. speed input: 36621.01 toks/s, output: 35.76 toks/s]
Processed prompts:  23%|██▎       | 30/128 [00:00<00:02, 38.54it/s, est. speed input: 37045.25 toks/s, output: 36.18 toks/s]
Processed prompts:  27%|██▋       | 34/128 [00:00<00:02, 38.78it/s, est. speed input: 37394.25 toks/s, output: 36.52 toks/s]
Processed prompts:  30%|██▉       | 38/128 [00:01<00:02, 38.93it/s, est. speed input: 37672.58 toks/s, output: 36.79 toks/s]
Processed prompts:  33%|███▎      | 42/128 [00:01<00:02, 39.07it/s, est. speed input: 37909.43 toks/s, output: 37.02 toks/s]
Processed prompts:  36%|███▌      | 46/128 [00:01<00:02, 39.11it/s, est. speed input: 38093.78 toks/s, output: 37.20 toks/s]
Processed prompts:  39%|███▉      | 50/128 [00:01<00:01, 39.05it/s, est. speed input: 38229.52 toks/s, output: 37.33 toks/s]
Processed prompts:  42%|████▏     | 54/128 [00:01<00:01, 39.10it/s, est. speed input: 38365.34 toks/s, output: 37.47 toks/s]
Processed prompts:  45%|████▌     | 58/128 [00:01<00:01, 39.06it/s, est. speed input: 38467.92 toks/s, output: 37.57 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:01<00:01, 39.06it/s, est. speed input: 38562.50 toks/s, output: 37.66 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:01<00:01, 39.05it/s, est. speed input: 38645.08 toks/s, output: 37.74 toks/s]
Processed prompts:  55%|█████▍    | 70/128 [00:01<00:01, 39.06it/s, est. speed input: 38720.17 toks/s, output: 37.81 toks/s]
Processed prompts:  58%|█████▊    | 74/128 [00:01<00:01, 39.06it/s, est. speed input: 38787.28 toks/s, output: 37.88 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:02<00:01, 39.10it/s, est. speed input: 38854.46 toks/s, output: 37.94 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:02<00:01, 39.13it/s, est. speed input: 38915.87 toks/s, output: 38.00 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:02<00:01, 39.18it/s, est. speed input: 38974.82 toks/s, output: 38.06 toks/s]
Processed prompts:  70%|███████   | 90/128 [00:02<00:00, 39.19it/s, est. speed input: 39025.31 toks/s, output: 38.11 toks/s]
Processed prompts:  73%|███████▎  | 94/128 [00:02<00:00, 39.21it/s, est. speed input: 39073.90 toks/s, output: 38.16 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:02<00:00, 39.25it/s, est. speed input: 39122.10 toks/s, output: 38.20 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:02<00:00, 39.26it/s, est. speed input: 39164.20 toks/s, output: 38.25 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:02<00:00, 39.19it/s, est. speed input: 39194.43 toks/s, output: 38.28 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:02<00:00, 39.19it/s, est. speed input: 39227.90 toks/s, output: 38.31 toks/s]
Processed prompts:  89%|████████▉ | 114/128 [00:02<00:00, 39.21it/s, est. speed input: 39260.58 toks/s, output: 38.34 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:03<00:00, 39.23it/s, est. speed input: 39292.10 toks/s, output: 38.37 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:03<00:00, 39.21it/s, est. speed input: 39318.46 toks/s, output: 38.40 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:03<00:00, 39.26it/s, est. speed input: 39349.44 toks/s, output: 38.43 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 39.26it/s, est. speed input: 39364.52 toks/s, output: 38.44 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 38.44it/s, est. speed input: 39364.52 toks/s, output: 38.44 toks/s]
[rank0]:[W126 11:05:14.745741148 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 42.9s

测试结果:
  Requests/s:   34.39
  Tokens/s:     35246.97
  Total Reqs:   128
  Elapsed:      3.72s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     35212.58

============================================================
[3/8] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 11:05:24 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1258754) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1258754) WARNING 01-26 11:05:40 [backends.py:609] Failed to read file <frozen os>
Throughput: 46.28 requests/s, 47438.43 total tokens/s, 46.28 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-26 11:05:24] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:05:24] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:05:24] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 11:05:24] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:05:24] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:05:24] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:05:24] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:05:24] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:05:24] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:05:24] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:05:24] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:05:24] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:05:24] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:05:24] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 11:05:31] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:05:32] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:05:32] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 11:05:32] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:05:32] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:05:32] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:05:32] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:05:32] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:05:32] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:05:32] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:05:32] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:05:32] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:05:32] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:05:32] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1258754) [2026-01-26 11:05:33] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1258754) [2026-01-26 11:05:33] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1258754) [2026-01-26 11:05:33] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1258754) [2026-01-26 11:05:33] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=1258754) [2026-01-26 11:05:33] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=1258754) [2026-01-26 11:05:33] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=1258754) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1258754) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.59it/s]
(EngineCore_DP0 pid=1258754) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.89it/s]
(EngineCore_DP0 pid=1258754) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.97it/s]
(EngineCore_DP0 pid=1258754) 
(EngineCore_DP0 pid=1258754) [2026-01-26 11:05:34] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 3584] -> 1D uint8
(EngineCore_DP0 pid=1258754) [2026-01-26 11:05:35] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 10321920 bytes
(EngineCore_DP0 pid=1258754) [2026-01-26 11:05:35] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 3584] -> 1D uint8
(EngineCore_DP0 pid=1258754) [2026-01-26 11:05:35] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 8028160 bytes
(EngineCore_DP0 pid=1258754) [2026-01-26 11:05:35] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 3584] -> 1D uint8
(EngineCore_DP0 pid=1258754) [2026-01-26 11:05:35] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 84869120 bytes
(EngineCore_DP0 pid=1258754) [2026-01-26 11:05:35] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 18944] -> 1D uint8
(EngineCore_DP0 pid=1258754) [2026-01-26 11:05:35] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 42434560 bytes
(EngineCore_DP0 pid=1258754) 2026-01-26 11:05:51,736 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1258754) 2026-01-26 11:05:51,759 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1258754) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 1/3 [00:00<00:00,  5.15it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:00<00:00,  2.24it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00,  3.19it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00,  3.09it/s]
(EngineCore_DP0 pid=1258754) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 18.22it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 18.19it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   7%|▋         | 19/256 [00:00<00:01, 185.45it/s]
Adding requests:  23%|██▎       | 59/256 [00:00<00:00, 308.42it/s]
Adding requests:  38%|███▊      | 96/256 [00:00<00:00, 333.99it/s]
Adding requests:  52%|█████▏    | 134/256 [00:00<00:00, 350.69it/s]
Adding requests:  68%|██████▊   | 174/256 [00:00<00:00, 366.86it/s]
Adding requests:  84%|████████▎ | 214/256 [00:00<00:00, 377.93it/s]
Adding requests:  99%|█████████▉| 253/256 [00:00<00:00, 379.06it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 356.39it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  12%|█▏        | 30/256 [00:00<00:00, 283.86it/s, est. speed input: 290721.04 toks/s, output: 283.87 toks/s]
Processed prompts:  23%|██▎       | 59/256 [00:00<00:02, 76.18it/s, est. speed input: 87805.74 toks/s, output: 85.75 toks/s]   
Processed prompts:  29%|██▉       | 74/256 [00:01<00:02, 62.62it/s, est. speed input: 73779.88 toks/s, output: 72.05 toks/s]
Processed prompts:  33%|███▎      | 84/256 [00:01<00:02, 58.69it/s, est. speed input: 69655.42 toks/s, output: 68.02 toks/s]
Processed prompts:  36%|███▌      | 92/256 [00:01<00:02, 56.12it/s, est. speed input: 67197.20 toks/s, output: 65.62 toks/s]
Processed prompts:  39%|███▊      | 99/256 [00:01<00:02, 56.12it/s, est. speed input: 66401.25 toks/s, output: 64.84 toks/s]
Processed prompts:  41%|████▏     | 106/256 [00:01<00:02, 52.18it/s, est. speed input: 64093.11 toks/s, output: 62.59 toks/s]
Processed prompts:  44%|████▍     | 112/256 [00:01<00:02, 51.07it/s, est. speed input: 63010.86 toks/s, output: 61.53 toks/s]
Processed prompts:  46%|████▌     | 118/256 [00:01<00:02, 50.05it/s, est. speed input: 62035.02 toks/s, output: 60.58 toks/s]
Processed prompts:  48%|████▊     | 124/256 [00:02<00:02, 49.49it/s, est. speed input: 61249.06 toks/s, output: 59.81 toks/s]
Processed prompts:  51%|█████     | 130/256 [00:02<00:02, 49.01it/s, est. speed input: 60541.81 toks/s, output: 59.12 toks/s]
Processed prompts:  53%|█████▎    | 136/256 [00:02<00:02, 48.68it/s, est. speed input: 59917.37 toks/s, output: 58.51 toks/s]
Processed prompts:  55%|█████▌    | 142/256 [00:02<00:02, 48.50it/s, est. speed input: 59371.38 toks/s, output: 57.98 toks/s]
Processed prompts:  58%|█████▊    | 148/256 [00:02<00:02, 48.46it/s, est. speed input: 58895.19 toks/s, output: 57.51 toks/s]
Processed prompts:  60%|██████    | 154/256 [00:02<00:02, 48.21it/s, est. speed input: 58422.57 toks/s, output: 57.05 toks/s]
Processed prompts:  62%|██████▎   | 160/256 [00:02<00:02, 47.97it/s, est. speed input: 57979.94 toks/s, output: 56.62 toks/s]
Processed prompts:  65%|██████▍   | 166/256 [00:02<00:01, 47.79it/s, est. speed input: 57572.36 toks/s, output: 56.22 toks/s]
Processed prompts:  67%|██████▋   | 172/256 [00:03<00:01, 47.70it/s, est. speed input: 57206.57 toks/s, output: 55.87 toks/s]
Processed prompts:  70%|██████▉   | 178/256 [00:03<00:01, 47.64it/s, est. speed input: 56868.16 toks/s, output: 55.53 toks/s]
Processed prompts:  72%|███████▏  | 184/256 [00:03<00:01, 47.76it/s, est. speed input: 56580.25 toks/s, output: 55.25 toks/s]
Processed prompts:  74%|███████▍  | 190/256 [00:03<00:01, 47.83it/s, est. speed input: 56311.47 toks/s, output: 54.99 toks/s]
Processed prompts:  77%|███████▋  | 196/256 [00:03<00:01, 47.86it/s, est. speed input: 56058.62 toks/s, output: 54.74 toks/s]
Processed prompts:  79%|███████▉  | 202/256 [00:03<00:01, 49.15it/s, est. speed input: 55985.91 toks/s, output: 54.67 toks/s]
Processed prompts:  81%|████████▏ | 208/256 [00:03<00:00, 48.73it/s, est. speed input: 55754.40 toks/s, output: 54.45 toks/s]
Processed prompts:  84%|████████▎ | 214/256 [00:03<00:00, 48.27it/s, est. speed input: 55516.07 toks/s, output: 54.21 toks/s]
Processed prompts:  86%|████████▌ | 220/256 [00:04<00:00, 48.18it/s, est. speed input: 55319.86 toks/s, output: 54.02 toks/s]
Processed prompts:  88%|████████▊ | 226/256 [00:04<00:00, 47.93it/s, est. speed input: 55113.60 toks/s, output: 53.82 toks/s]
Processed prompts:  91%|█████████ | 232/256 [00:04<00:00, 47.80it/s, est. speed input: 54924.42 toks/s, output: 53.64 toks/s]
Processed prompts:  93%|█████████▎| 238/256 [00:04<00:00, 47.86it/s, est. speed input: 54762.40 toks/s, output: 53.48 toks/s]
Processed prompts:  95%|█████████▌| 244/256 [00:04<00:00, 47.93it/s, est. speed input: 54611.71 toks/s, output: 53.33 toks/s]
Processed prompts:  98%|█████████▊| 250/256 [00:04<00:00, 47.75it/s, est. speed input: 54445.94 toks/s, output: 53.17 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:04<00:00, 49.71it/s, est. speed input: 54488.44 toks/s, output: 53.21 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:04<00:00, 49.71it/s, est. speed input: 54488.44 toks/s, output: 53.21 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:04<00:00, 53.21it/s, est. speed input: 54488.44 toks/s, output: 53.21 toks/s]
[rank0]:[W126 11:06:00.168787220 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 45.4s

测试结果:
  Requests/s:   46.28
  Tokens/s:     47438.43
  Total Reqs:   256
  Elapsed:      5.53s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     47392.15

============================================================
[4/8] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 11:06:11 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1259902) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1259902) WARNING 01-26 11:06:28 [backends.py:609] Failed to read file <frozen os>
Throughput: 49.12 requests/s, 50347.01 total tokens/s, 49.12 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-26 11:06:10] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:06:11] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:06:11] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 11:06:11] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:06:11] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:06:11] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:06:11] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:06:11] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:06:11] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:06:11] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:06:11] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:06:11] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:06:11] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:06:11] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 11:06:17] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:06:19] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:06:19] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 11:06:19] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:06:19] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:06:19] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:06:19] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:06:19] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:06:19] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:06:19] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:06:19] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:06:19] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:06:19] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:06:19] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1259902) [2026-01-26 11:06:20] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1259902) [2026-01-26 11:06:20] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1259902) [2026-01-26 11:06:20] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1259902) [2026-01-26 11:06:20] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=1259902) [2026-01-26 11:06:20] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=1259902) [2026-01-26 11:06:20] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=1259902) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1259902) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.57it/s]
(EngineCore_DP0 pid=1259902) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.88it/s]
(EngineCore_DP0 pid=1259902) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.96it/s]
(EngineCore_DP0 pid=1259902) 
(EngineCore_DP0 pid=1259902) [2026-01-26 11:06:21] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 3584] -> 1D uint8
(EngineCore_DP0 pid=1259902) [2026-01-26 11:06:22] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 10321920 bytes
(EngineCore_DP0 pid=1259902) [2026-01-26 11:06:22] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 3584] -> 1D uint8
(EngineCore_DP0 pid=1259902) [2026-01-26 11:06:22] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 8028160 bytes
(EngineCore_DP0 pid=1259902) [2026-01-26 11:06:22] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 3584] -> 1D uint8
(EngineCore_DP0 pid=1259902) [2026-01-26 11:06:22] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 84869120 bytes
(EngineCore_DP0 pid=1259902) [2026-01-26 11:06:22] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 18944] -> 1D uint8
(EngineCore_DP0 pid=1259902) [2026-01-26 11:06:22] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 42434560 bytes
(EngineCore_DP0 pid=1259902) 2026-01-26 11:06:39,926 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1259902) 2026-01-26 11:06:39,949 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1259902) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00,  8.68it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 10.55it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 10.21it/s]
(EngineCore_DP0 pid=1259902) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00,  7.22it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00,  7.26it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00,  7.25it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   4%|▍         | 20/512 [00:00<00:02, 194.57it/s]
Adding requests:  12%|█▏        | 61/512 [00:00<00:01, 316.90it/s]
Adding requests:  19%|█▉        | 98/512 [00:00<00:01, 337.08it/s]
Adding requests:  27%|██▋       | 136/512 [00:00<00:01, 353.78it/s]
Adding requests:  34%|███▍      | 176/512 [00:00<00:00, 367.61it/s]
Adding requests:  42%|████▏     | 217/512 [00:00<00:00, 380.75it/s]
Adding requests:  50%|█████     | 256/512 [00:00<00:00, 380.95it/s]
Adding requests:  58%|█████▊    | 296/512 [00:00<00:00, 385.36it/s]
Adding requests:  66%|██████▌   | 337/512 [00:00<00:00, 391.57it/s]
Adding requests:  74%|███████▎  | 377/512 [00:01<00:00, 393.67it/s]
Adding requests:  82%|████████▏ | 419/512 [00:01<00:00, 401.04it/s]
Adding requests:  90%|████████▉ | 460/512 [00:01<00:00, 397.22it/s]
Adding requests:  98%|█████████▊| 503/512 [00:01<00:00, 404.96it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 380.81it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  12%|█▏        | 62/512 [00:00<00:00, 569.79it/s, est. speed input: 583588.59 toks/s, output: 569.83 toks/s]
Processed prompts:  23%|██▎       | 119/512 [00:01<00:04, 83.27it/s, est. speed input: 98404.75 toks/s, output: 96.10 toks/s]  
Processed prompts:  29%|██▊       | 146/512 [00:01<00:05, 69.17it/s, est. speed input: 83021.40 toks/s, output: 81.07 toks/s]
Processed prompts:  32%|███▏      | 163/512 [00:02<00:05, 65.03it/s, est. speed input: 78550.86 toks/s, output: 76.71 toks/s]
Processed prompts:  34%|███▍      | 176/512 [00:02<00:05, 62.87it/s, est. speed input: 76264.53 toks/s, output: 74.48 toks/s]
Processed prompts:  36%|███▋      | 186/512 [00:02<00:05, 58.08it/s, est. speed input: 73202.86 toks/s, output: 71.49 toks/s]
Processed prompts:  38%|███▊      | 194/512 [00:02<00:05, 56.50it/s, est. speed input: 71873.68 toks/s, output: 70.19 toks/s]
Processed prompts:  39%|███▉      | 202/512 [00:02<00:05, 55.71it/s, est. speed input: 70924.08 toks/s, output: 69.26 toks/s]
Processed prompts:  41%|████      | 210/512 [00:03<00:05, 54.32it/s, est. speed input: 69870.24 toks/s, output: 68.23 toks/s]
Processed prompts:  43%|████▎     | 218/512 [00:03<00:05, 53.17it/s, est. speed input: 68923.52 toks/s, output: 67.31 toks/s]
Processed prompts:  44%|████▍     | 226/512 [00:03<00:05, 52.36it/s, est. speed input: 68092.21 toks/s, output: 66.49 toks/s]
Processed prompts:  46%|████▌     | 234/512 [00:03<00:05, 51.75it/s, est. speed input: 67336.75 toks/s, output: 65.76 toks/s]
Processed prompts:  47%|████▋     | 242/512 [00:03<00:05, 51.06it/s, est. speed input: 66602.78 toks/s, output: 65.04 toks/s]
Processed prompts:  49%|████▉     | 250/512 [00:03<00:05, 50.52it/s, est. speed input: 65921.22 toks/s, output: 64.38 toks/s]
Processed prompts:  50%|█████     | 258/512 [00:04<00:05, 50.16it/s, est. speed input: 65301.74 toks/s, output: 63.77 toks/s]
Processed prompts:  52%|█████▏    | 266/512 [00:04<00:04, 50.11it/s, est. speed input: 64763.48 toks/s, output: 63.24 toks/s]
Processed prompts:  54%|█████▎    | 274/512 [00:04<00:04, 50.08it/s, est. speed input: 64266.51 toks/s, output: 62.76 toks/s]
Processed prompts:  55%|█████▌    | 282/512 [00:04<00:04, 50.15it/s, est. speed input: 63819.37 toks/s, output: 62.32 toks/s]
Processed prompts:  57%|█████▋    | 290/512 [00:04<00:04, 50.08it/s, est. speed input: 63385.03 toks/s, output: 61.90 toks/s]
Processed prompts:  58%|█████▊    | 298/512 [00:04<00:04, 49.91it/s, est. speed input: 62962.14 toks/s, output: 61.49 toks/s]
Processed prompts:  60%|█████▉    | 306/512 [00:04<00:04, 51.01it/s, est. speed input: 62727.66 toks/s, output: 61.26 toks/s]
Processed prompts:  61%|██████▏   | 314/512 [00:05<00:03, 50.57it/s, est. speed input: 62353.14 toks/s, output: 60.89 toks/s]
Processed prompts:  63%|██████▎   | 322/512 [00:05<00:03, 50.29it/s, est. speed input: 62004.48 toks/s, output: 60.55 toks/s]
Processed prompts:  64%|██████▍   | 330/512 [00:05<00:03, 50.14it/s, est. speed input: 61680.46 toks/s, output: 60.23 toks/s]
Processed prompts:  66%|██████▌   | 338/512 [00:05<00:03, 49.97it/s, est. speed input: 61368.14 toks/s, output: 59.93 toks/s]
Processed prompts:  68%|██████▊   | 346/512 [00:05<00:03, 49.82it/s, est. speed input: 61069.71 toks/s, output: 59.64 toks/s]
Processed prompts:  69%|██████▉   | 354/512 [00:05<00:03, 49.68it/s, est. speed input: 60783.23 toks/s, output: 59.36 toks/s]
Processed prompts:  71%|███████   | 362/512 [00:06<00:03, 49.64it/s, est. speed input: 60519.13 toks/s, output: 59.10 toks/s]
Processed prompts:  72%|███████▏  | 370/512 [00:06<00:02, 49.79it/s, est. speed input: 60286.15 toks/s, output: 58.87 toks/s]
Processed prompts:  74%|███████▍  | 378/512 [00:06<00:02, 49.86it/s, est. speed input: 60061.30 toks/s, output: 58.65 toks/s]
Processed prompts:  75%|███████▌  | 386/512 [00:06<00:02, 49.87it/s, est. speed input: 59843.09 toks/s, output: 58.44 toks/s]
Processed prompts:  77%|███████▋  | 394/512 [00:06<00:02, 49.83it/s, est. speed input: 59631.84 toks/s, output: 58.23 toks/s]
Processed prompts:  79%|███████▊  | 402/512 [00:06<00:02, 49.77it/s, est. speed input: 59426.79 toks/s, output: 58.03 toks/s]
Processed prompts:  80%|████████  | 410/512 [00:07<00:02, 49.65it/s, est. speed input: 59223.90 toks/s, output: 57.84 toks/s]
Processed prompts:  82%|████████▏ | 418/512 [00:07<00:01, 49.54it/s, est. speed input: 59027.71 toks/s, output: 57.64 toks/s]
Processed prompts:  83%|████████▎ | 426/512 [00:07<00:01, 49.55it/s, est. speed input: 58848.17 toks/s, output: 57.47 toks/s]
Processed prompts:  85%|████████▍ | 434/512 [00:07<00:01, 50.85it/s, est. speed input: 58782.03 toks/s, output: 57.40 toks/s]
Processed prompts:  86%|████████▋ | 442/512 [00:07<00:01, 50.48it/s, est. speed input: 58616.01 toks/s, output: 57.24 toks/s]
Processed prompts:  88%|████████▊ | 450/512 [00:07<00:01, 50.17it/s, est. speed input: 58452.32 toks/s, output: 57.08 toks/s]
Processed prompts:  89%|████████▉ | 458/512 [00:08<00:01, 50.00it/s, est. speed input: 58298.73 toks/s, output: 56.93 toks/s]
Processed prompts:  91%|█████████ | 466/512 [00:08<00:00, 49.98it/s, est. speed input: 58158.93 toks/s, output: 56.80 toks/s]
Processed prompts:  93%|█████████▎| 474/512 [00:08<00:00, 49.93it/s, est. speed input: 58021.34 toks/s, output: 56.66 toks/s]
Processed prompts:  94%|█████████▍| 482/512 [00:08<00:00, 49.83it/s, est. speed input: 57885.07 toks/s, output: 56.53 toks/s]
Processed prompts:  96%|█████████▌| 490/512 [00:08<00:00, 49.74it/s, est. speed input: 57751.87 toks/s, output: 56.40 toks/s]
Processed prompts:  97%|█████████▋| 498/512 [00:08<00:00, 49.57it/s, est. speed input: 57616.05 toks/s, output: 56.27 toks/s]
Processed prompts:  99%|█████████▉| 506/512 [00:09<00:00, 49.45it/s, est. speed input: 57484.40 toks/s, output: 56.14 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:09<00:00, 49.45it/s, est. speed input: 57759.13 toks/s, output: 56.41 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:09<00:00, 56.40it/s, est. speed input: 57759.13 toks/s, output: 56.41 toks/s]
[rank0]:[W126 11:06:52.985309752 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 52.8s

测试结果:
  Requests/s:   49.12
  Tokens/s:     50347.01
  Total Reqs:   512
  Elapsed:      10.42s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     50297.89

============================================================
[5/8] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 11:07:07 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1261159) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1261159) WARNING 01-26 11:07:22 [backends.py:609] Failed to read file <frozen os>
Throughput: 50.53 requests/s, 51796.96 total tokens/s, 50.53 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-26 11:07:06] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:07:07] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:07:07] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 11:07:07] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:07:07] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:07:07] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:07:07] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:07:07] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:07:07] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:07:07] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:07:07] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:07:07] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:07:07] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:07:07] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 11:07:13] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:07:13] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:07:13] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 11:07:13] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:07:13] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:07:13] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:07:13] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:07:13] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:07:13] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:07:13] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:07:13] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:07:13] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:07:13] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:07:13] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1261159) [2026-01-26 11:07:15] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1261159) [2026-01-26 11:07:15] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1261159) [2026-01-26 11:07:15] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1261159) [2026-01-26 11:07:15] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=1261159) [2026-01-26 11:07:15] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=1261159) [2026-01-26 11:07:15] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=1261159) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1261159) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.59it/s]
(EngineCore_DP0 pid=1261159) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.89it/s]
(EngineCore_DP0 pid=1261159) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.97it/s]
(EngineCore_DP0 pid=1261159) 
(EngineCore_DP0 pid=1261159) [2026-01-26 11:07:16] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 3584] -> 1D uint8
(EngineCore_DP0 pid=1261159) [2026-01-26 11:07:16] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 10321920 bytes
(EngineCore_DP0 pid=1261159) [2026-01-26 11:07:16] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 3584] -> 1D uint8
(EngineCore_DP0 pid=1261159) [2026-01-26 11:07:16] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 8028160 bytes
(EngineCore_DP0 pid=1261159) [2026-01-26 11:07:16] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 3584] -> 1D uint8
(EngineCore_DP0 pid=1261159) [2026-01-26 11:07:16] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 84869120 bytes
(EngineCore_DP0 pid=1261159) [2026-01-26 11:07:16] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 18944] -> 1D uint8
(EngineCore_DP0 pid=1261159) [2026-01-26 11:07:16] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 42434560 bytes
(EngineCore_DP0 pid=1261159) 2026-01-26 11:07:33,718 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1261159) 2026-01-26 11:07:33,751 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1261159) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 1/5 [00:00<00:02,  1.88it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:01,  2.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00,  5.96it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00,  5.26it/s]
(EngineCore_DP0 pid=1261159) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00, 19.49it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 19.93it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 22/1024 [00:00<00:04, 216.81it/s]
Adding requests:   6%|▌         | 60/1024 [00:00<00:03, 310.39it/s]
Adding requests:   9%|▉         | 97/1024 [00:00<00:02, 334.52it/s]
Adding requests:  13%|█▎        | 135/1024 [00:00<00:02, 350.72it/s]
Adding requests:  17%|█▋        | 175/1024 [00:00<00:02, 364.77it/s]
Adding requests:  21%|██        | 216/1024 [00:00<00:02, 377.17it/s]
Adding requests:  25%|██▍       | 255/1024 [00:00<00:02, 378.61it/s]
Adding requests:  29%|██▉       | 295/1024 [00:00<00:01, 383.30it/s]
Adding requests:  33%|███▎      | 337/1024 [00:00<00:01, 391.86it/s]
Adding requests:  37%|███▋      | 378/1024 [00:01<00:01, 393.75it/s]
Adding requests:  41%|████      | 420/1024 [00:01<00:01, 401.27it/s]
Adding requests:  45%|████▌     | 461/1024 [00:01<00:01, 395.75it/s]
Adding requests:  49%|████▉     | 504/1024 [00:01<00:01, 403.94it/s]
Adding requests:  53%|█████▎    | 546/1024 [00:01<00:01, 408.41it/s]
Adding requests:  57%|█████▋    | 587/1024 [00:01<00:01, 402.28it/s]
Adding requests:  61%|██████▏   | 628/1024 [00:01<00:01, 393.75it/s]
Adding requests:  65%|██████▌   | 668/1024 [00:01<00:00, 384.39it/s]
Adding requests:  69%|██████▉   | 709/1024 [00:01<00:00, 390.54it/s]
Adding requests:  73%|███████▎  | 749/1024 [00:01<00:00, 382.93it/s]
Adding requests:  77%|███████▋  | 789/1024 [00:02<00:00, 385.97it/s]
Adding requests:  81%|████████  | 829/1024 [00:02<00:00, 388.78it/s]
Adding requests:  85%|████████▍ | 868/1024 [00:02<00:00, 388.85it/s]
Adding requests:  89%|████████▉ | 909/1024 [00:02<00:00, 393.12it/s]
Adding requests:  93%|█████████▎| 949/1024 [00:02<00:00, 384.50it/s]
Adding requests:  97%|█████████▋| 989/1024 [00:02<00:00, 386.87it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 383.00it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  13%|█▎        | 130/1024 [00:00<00:00, 1270.10it/s, est. speed input: 1300886.64 toks/s, output: 1270.18 toks/s]
Processed prompts:  25%|██▌       | 258/1024 [00:02<00:09, 84.67it/s, est. speed input: 100950.27 toks/s, output: 98.58 toks/s]     
Processed prompts:  31%|███       | 314/1024 [00:03<00:09, 72.22it/s, est. speed input: 86795.97 toks/s, output: 84.76 toks/s] 
Processed prompts:  34%|███▍      | 347/1024 [00:04<00:10, 67.49it/s, est. speed input: 81950.91 toks/s, output: 80.03 toks/s]
Processed prompts:  36%|███▌      | 369/1024 [00:04<00:09, 67.73it/s, est. speed input: 81190.13 toks/s, output: 79.29 toks/s]
Processed prompts:  38%|███▊      | 386/1024 [00:05<00:10, 60.25it/s, est. speed input: 77114.49 toks/s, output: 75.31 toks/s]
Processed prompts:  39%|███▉      | 399/1024 [00:05<00:10, 62.45it/s, est. speed input: 77315.51 toks/s, output: 75.50 toks/s]
Processed prompts:  40%|████      | 410/1024 [00:05<00:10, 56.18it/s, est. speed input: 74945.86 toks/s, output: 73.19 toks/s]
Processed prompts:  41%|████      | 419/1024 [00:05<00:10, 56.25it/s, est. speed input: 74478.57 toks/s, output: 72.73 toks/s]
Processed prompts:  42%|████▏     | 427/1024 [00:05<00:10, 55.34it/s, est. speed input: 73878.27 toks/s, output: 72.15 toks/s]
Processed prompts:  42%|████▏     | 434/1024 [00:06<00:10, 54.31it/s, est. speed input: 73335.48 toks/s, output: 71.62 toks/s]
Processed prompts:  43%|████▎     | 442/1024 [00:06<00:10, 53.52it/s, est. speed input: 72790.43 toks/s, output: 71.08 toks/s]
Processed prompts:  44%|████▍     | 450/1024 [00:06<00:10, 52.76it/s, est. speed input: 72259.99 toks/s, output: 70.57 toks/s]
Processed prompts:  45%|████▍     | 458/1024 [00:06<00:10, 52.09it/s, est. speed input: 71749.43 toks/s, output: 70.07 toks/s]
Processed prompts:  46%|████▌     | 466/1024 [00:06<00:10, 51.67it/s, est. speed input: 71276.03 toks/s, output: 69.61 toks/s]
Processed prompts:  46%|████▋     | 474/1024 [00:06<00:10, 51.43it/s, est. speed input: 70832.47 toks/s, output: 69.17 toks/s]
Processed prompts:  47%|████▋     | 482/1024 [00:07<00:10, 51.32it/s, est. speed input: 70417.70 toks/s, output: 68.77 toks/s]
Processed prompts:  48%|████▊     | 490/1024 [00:07<00:10, 51.14it/s, est. speed input: 70009.62 toks/s, output: 68.37 toks/s]
Processed prompts:  49%|████▊     | 498/1024 [00:07<00:10, 50.94it/s, est. speed input: 69612.55 toks/s, output: 67.98 toks/s]
Processed prompts:  49%|████▉     | 506/1024 [00:07<00:10, 50.78it/s, est. speed input: 69230.97 toks/s, output: 67.61 toks/s]
Processed prompts:  50%|█████     | 514/1024 [00:07<00:10, 50.64it/s, est. speed input: 68862.20 toks/s, output: 67.25 toks/s]
Processed prompts:  51%|█████     | 522/1024 [00:07<00:09, 50.53it/s, est. speed input: 68507.29 toks/s, output: 66.90 toks/s]
Processed prompts:  52%|█████▏    | 530/1024 [00:07<00:09, 50.59it/s, est. speed input: 68179.86 toks/s, output: 66.58 toks/s]
Processed prompts:  53%|█████▎    | 538/1024 [00:08<00:09, 50.55it/s, est. speed input: 67856.75 toks/s, output: 66.27 toks/s]
Processed prompts:  53%|█████▎    | 546/1024 [00:08<00:09, 50.55it/s, est. speed input: 67549.19 toks/s, output: 65.97 toks/s]
Processed prompts:  54%|█████▍    | 554/1024 [00:08<00:09, 50.49it/s, est. speed input: 67247.96 toks/s, output: 65.67 toks/s]
Processed prompts:  55%|█████▍    | 562/1024 [00:08<00:09, 50.50it/s, est. speed input: 66962.33 toks/s, output: 65.39 toks/s]
Processed prompts:  56%|█████▌    | 570/1024 [00:08<00:08, 50.60it/s, est. speed input: 66693.82 toks/s, output: 65.13 toks/s]
Processed prompts:  56%|█████▋    | 578/1024 [00:08<00:08, 50.69it/s, est. speed input: 66436.76 toks/s, output: 64.88 toks/s]
Processed prompts:  57%|█████▋    | 586/1024 [00:09<00:08, 50.72it/s, est. speed input: 66186.57 toks/s, output: 64.64 toks/s]
Processed prompts:  58%|█████▊    | 594/1024 [00:09<00:08, 50.56it/s, est. speed input: 65930.70 toks/s, output: 64.39 toks/s]
Processed prompts:  59%|█████▉    | 602/1024 [00:09<00:08, 50.47it/s, est. speed input: 65685.67 toks/s, output: 64.15 toks/s]
Processed prompts:  60%|█████▉    | 610/1024 [00:09<00:08, 50.38it/s, est. speed input: 65446.56 toks/s, output: 63.91 toks/s]
Processed prompts:  60%|██████    | 618/1024 [00:09<00:08, 50.40it/s, est. speed input: 65221.26 toks/s, output: 63.69 toks/s]
Processed prompts:  61%|██████    | 626/1024 [00:09<00:07, 50.48it/s, est. speed input: 65007.62 toks/s, output: 63.48 toks/s]
Processed prompts:  62%|██████▏   | 634/1024 [00:10<00:07, 50.48it/s, est. speed input: 64796.97 toks/s, output: 63.28 toks/s]
Processed prompts:  63%|██████▎   | 642/1024 [00:10<00:07, 50.47it/s, est. speed input: 64592.51 toks/s, output: 63.08 toks/s]
Processed prompts:  63%|██████▎   | 650/1024 [00:10<00:07, 50.39it/s, est. speed input: 64389.26 toks/s, output: 62.88 toks/s]
Processed prompts:  64%|██████▍   | 658/1024 [00:10<00:07, 50.30it/s, est. speed input: 64189.58 toks/s, output: 62.68 toks/s]
Processed prompts:  65%|██████▌   | 666/1024 [00:10<00:07, 50.26it/s, est. speed input: 63998.33 toks/s, output: 62.50 toks/s]
Processed prompts:  66%|██████▌   | 674/1024 [00:10<00:06, 50.40it/s, est. speed input: 63822.03 toks/s, output: 62.33 toks/s]
Processed prompts:  67%|██████▋   | 682/1024 [00:10<00:06, 50.46it/s, est. speed input: 63649.46 toks/s, output: 62.16 toks/s]
Processed prompts:  67%|██████▋   | 690/1024 [00:11<00:06, 50.48it/s, est. speed input: 63479.87 toks/s, output: 61.99 toks/s]
Processed prompts:  68%|██████▊   | 698/1024 [00:11<00:06, 50.50it/s, est. speed input: 63315.64 toks/s, output: 61.83 toks/s]
Processed prompts:  69%|██████▉   | 706/1024 [00:11<00:06, 50.41it/s, est. speed input: 63149.74 toks/s, output: 61.67 toks/s]
Processed prompts:  70%|██████▉   | 714/1024 [00:11<00:06, 50.38it/s, est. speed input: 62990.57 toks/s, output: 61.51 toks/s]
Processed prompts:  71%|███████   | 722/1024 [00:11<00:05, 50.46it/s, est. speed input: 62840.98 toks/s, output: 61.37 toks/s]
Processed prompts:  71%|███████▏  | 730/1024 [00:11<00:05, 50.43it/s, est. speed input: 62691.07 toks/s, output: 61.22 toks/s]
Processed prompts:  72%|███████▏  | 738/1024 [00:12<00:05, 50.45it/s, est. speed input: 62547.14 toks/s, output: 61.08 toks/s]
Processed prompts:  73%|███████▎  | 746/1024 [00:12<00:05, 50.46it/s, est. speed input: 62406.44 toks/s, output: 60.94 toks/s]
Processed prompts:  74%|███████▎  | 754/1024 [00:12<00:05, 50.44it/s, est. speed input: 62268.19 toks/s, output: 60.81 toks/s]
Processed prompts:  74%|███████▍  | 762/1024 [00:12<00:05, 50.32it/s, est. speed input: 62127.72 toks/s, output: 60.67 toks/s]
Processed prompts:  75%|███████▌  | 770/1024 [00:12<00:05, 50.45it/s, est. speed input: 62002.14 toks/s, output: 60.55 toks/s]
Processed prompts:  76%|███████▌  | 778/1024 [00:12<00:04, 50.34it/s, est. speed input: 61868.99 toks/s, output: 60.42 toks/s]
Processed prompts:  77%|███████▋  | 786/1024 [00:13<00:04, 51.94it/s, est. speed input: 61820.44 toks/s, output: 60.37 toks/s]
Processed prompts:  78%|███████▊  | 794/1024 [00:13<00:04, 51.51it/s, est. speed input: 61699.73 toks/s, output: 60.25 toks/s]
Processed prompts:  78%|███████▊  | 802/1024 [00:13<00:04, 51.16it/s, est. speed input: 61578.97 toks/s, output: 60.14 toks/s]
Processed prompts:  79%|███████▉  | 810/1024 [00:13<00:04, 50.88it/s, est. speed input: 61459.19 toks/s, output: 60.02 toks/s]
Processed prompts:  80%|███████▉  | 818/1024 [00:13<00:04, 50.78it/s, est. speed input: 61347.08 toks/s, output: 59.91 toks/s]
Processed prompts:  81%|████████  | 826/1024 [00:13<00:03, 50.85it/s, est. speed input: 61243.61 toks/s, output: 59.81 toks/s]
Processed prompts:  81%|████████▏ | 834/1024 [00:13<00:03, 50.75it/s, est. speed input: 61135.67 toks/s, output: 59.70 toks/s]
Processed prompts:  82%|████████▏ | 842/1024 [00:14<00:03, 50.83it/s, est. speed input: 61037.05 toks/s, output: 59.61 toks/s]
Processed prompts:  83%|████████▎ | 850/1024 [00:14<00:03, 50.73it/s, est. speed input: 60933.68 toks/s, output: 59.51 toks/s]
Processed prompts:  84%|████████▍ | 858/1024 [00:14<00:03, 50.72it/s, est. speed input: 60834.93 toks/s, output: 59.41 toks/s]
Processed prompts:  85%|████████▍ | 866/1024 [00:14<00:03, 50.64it/s, est. speed input: 60735.27 toks/s, output: 59.31 toks/s]
Processed prompts:  85%|████████▌ | 874/1024 [00:14<00:02, 50.55it/s, est. speed input: 60636.59 toks/s, output: 59.22 toks/s]
Processed prompts:  86%|████████▌ | 882/1024 [00:14<00:02, 50.48it/s, est. speed input: 60539.30 toks/s, output: 59.12 toks/s]
Processed prompts:  87%|████████▋ | 890/1024 [00:15<00:02, 50.56it/s, est. speed input: 60449.66 toks/s, output: 59.03 toks/s]
Processed prompts:  88%|████████▊ | 898/1024 [00:15<00:02, 50.60it/s, est. speed input: 60361.16 toks/s, output: 58.95 toks/s]
Processed prompts:  88%|████████▊ | 906/1024 [00:15<00:02, 50.61it/s, est. speed input: 60273.66 toks/s, output: 58.86 toks/s]
Processed prompts:  89%|████████▉ | 914/1024 [00:15<00:02, 50.64it/s, est. speed input: 60189.04 toks/s, output: 58.78 toks/s]
Processed prompts:  90%|█████████ | 922/1024 [00:15<00:02, 50.69it/s, est. speed input: 60107.18 toks/s, output: 58.70 toks/s]
Processed prompts:  91%|█████████ | 930/1024 [00:15<00:01, 50.66it/s, est. speed input: 60024.50 toks/s, output: 58.62 toks/s]
Processed prompts:  92%|█████████▏| 938/1024 [00:16<00:01, 50.54it/s, est. speed input: 59939.62 toks/s, output: 58.53 toks/s]
Processed prompts:  92%|█████████▏| 946/1024 [00:16<00:01, 50.59it/s, est. speed input: 59861.50 toks/s, output: 58.46 toks/s]
Processed prompts:  93%|█████████▎| 954/1024 [00:16<00:01, 50.48it/s, est. speed input: 59779.22 toks/s, output: 58.38 toks/s]
Processed prompts:  94%|█████████▍| 962/1024 [00:16<00:01, 50.53it/s, est. speed input: 59703.58 toks/s, output: 58.30 toks/s]
Processed prompts:  95%|█████████▍| 970/1024 [00:16<00:01, 50.54it/s, est. speed input: 59628.21 toks/s, output: 58.23 toks/s]
Processed prompts:  96%|█████████▌| 978/1024 [00:16<00:00, 50.50it/s, est. speed input: 59552.43 toks/s, output: 58.16 toks/s]
Processed prompts:  96%|█████████▋| 986/1024 [00:16<00:00, 50.54it/s, est. speed input: 59480.83 toks/s, output: 58.09 toks/s]
Processed prompts:  97%|█████████▋| 994/1024 [00:17<00:00, 50.68it/s, est. speed input: 59414.52 toks/s, output: 58.02 toks/s]
Processed prompts:  98%|█████████▊| 1002/1024 [00:17<00:00, 50.58it/s, est. speed input: 59342.31 toks/s, output: 57.95 toks/s]
Processed prompts:  99%|█████████▊| 1010/1024 [00:17<00:00, 50.56it/s, est. speed input: 59273.26 toks/s, output: 57.88 toks/s]
Processed prompts:  99%|█████████▉| 1018/1024 [00:17<00:00, 52.48it/s, est. speed input: 59270.69 toks/s, output: 57.88 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:17<00:00, 52.48it/s, est. speed input: 59618.75 toks/s, output: 58.22 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:17<00:00, 58.22it/s, est. speed input: 59618.75 toks/s, output: 58.22 toks/s]
[rank0]:[W126 11:07:56.981400208 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 63.9s

测试结果:
  Requests/s:   50.53
  Tokens/s:     51796.96
  Total Reqs:   1024
  Elapsed:      20.26s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     51746.43

============================================================
[6/8] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 11:08:16 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1262609) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1262609) WARNING 01-26 11:08:32 [backends.py:609] Failed to read file <frozen os>
Throughput: 51.67 requests/s, 52963.66 total tokens/s, 51.67 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-26 11:08:15] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:08:16] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:08:16] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 11:08:16] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:08:16] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:08:16] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:08:16] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:08:16] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:08:16] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:08:16] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:08:16] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:08:16] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:08:16] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:08:16] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 11:08:22] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:08:23] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:08:23] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 11:08:23] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:08:23] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:08:23] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:08:23] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:08:23] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:08:23] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:08:23] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:08:23] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:08:23] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:08:23] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:08:23] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1262609) [2026-01-26 11:08:24] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1262609) [2026-01-26 11:08:24] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1262609) [2026-01-26 11:08:24] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1262609) [2026-01-26 11:08:24] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=1262609) [2026-01-26 11:08:24] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=1262609) [2026-01-26 11:08:24] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=1262609) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1262609) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.59it/s]
(EngineCore_DP0 pid=1262609) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.89it/s]
(EngineCore_DP0 pid=1262609) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.97it/s]
(EngineCore_DP0 pid=1262609) 
(EngineCore_DP0 pid=1262609) [2026-01-26 11:08:26] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 3584] -> 1D uint8
(EngineCore_DP0 pid=1262609) [2026-01-26 11:08:26] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 10321920 bytes
(EngineCore_DP0 pid=1262609) [2026-01-26 11:08:26] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 3584] -> 1D uint8
(EngineCore_DP0 pid=1262609) [2026-01-26 11:08:26] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 8028160 bytes
(EngineCore_DP0 pid=1262609) [2026-01-26 11:08:26] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 3584] -> 1D uint8
(EngineCore_DP0 pid=1262609) [2026-01-26 11:08:26] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 84869120 bytes
(EngineCore_DP0 pid=1262609) [2026-01-26 11:08:26] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 18944] -> 1D uint8
(EngineCore_DP0 pid=1262609) [2026-01-26 11:08:26] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 42434560 bytes
(EngineCore_DP0 pid=1262609) [rank0]:W0126 11:08:37.847000 1262609 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1262609) [rank0]:W0126 11:08:37.925000 1262609 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1262609) [rank0]:W0126 11:08:38.985000 1262609 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1262609) [rank0]:W0126 11:08:39.111000 1262609 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1262609) 2026-01-26 11:08:43,076 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1262609) 2026-01-26 11:08:43,102 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1262609) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 1/7 [00:00<00:04,  1.36it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:01,  2.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 5/7 [00:00<00:00,  7.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:01<00:00,  9.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:01<00:00,  6.33it/s]
(EngineCore_DP0 pid=1262609) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00, 19.46it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 20.30it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 20.18it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   1%|          | 25/2048 [00:00<00:08, 243.51it/s]
Adding requests:   3%|▎         | 65/2048 [00:00<00:05, 332.01it/s]
Adding requests:   5%|▍         | 101/2048 [00:00<00:05, 342.21it/s]
Adding requests:   7%|▋         | 140/2048 [00:00<00:05, 356.91it/s]
Adding requests:   9%|▊         | 179/2048 [00:00<00:05, 367.80it/s]
Adding requests:  11%|█         | 221/2048 [00:00<00:04, 383.80it/s]
Adding requests:  13%|█▎        | 260/2048 [00:00<00:04, 379.77it/s]
Adding requests:  15%|█▍        | 301/2048 [00:00<00:04, 385.77it/s]
Adding requests:  17%|█▋        | 342/2048 [00:00<00:04, 391.69it/s]
Adding requests:  19%|█▊        | 382/2048 [00:01<00:04, 393.54it/s]
Adding requests:  21%|██        | 424/2048 [00:01<00:04, 400.96it/s]
Adding requests:  23%|██▎       | 465/2048 [00:01<00:04, 395.63it/s]
Adding requests:  25%|██▍       | 508/2048 [00:01<00:03, 405.39it/s]
Adding requests:  27%|██▋       | 549/2048 [00:01<00:03, 406.63it/s]
Adding requests:  29%|██▉       | 590/2048 [00:01<00:03, 400.81it/s]
Adding requests:  31%|███       | 631/2048 [00:01<00:03, 396.96it/s]
Adding requests:  33%|███▎      | 671/2048 [00:01<00:03, 386.50it/s]
Adding requests:  35%|███▍      | 712/2048 [00:01<00:03, 392.30it/s]
Adding requests:  37%|███▋      | 752/2048 [00:01<00:03, 380.13it/s]
Adding requests:  39%|███▊      | 792/2048 [00:02<00:03, 381.66it/s]
Adding requests:  41%|████      | 833/2048 [00:02<00:03, 388.37it/s]
Adding requests:  43%|████▎     | 873/2048 [00:02<00:03, 391.27it/s]
Adding requests:  45%|████▍     | 913/2048 [00:02<00:02, 390.57it/s]
Adding requests:  47%|████▋     | 953/2048 [00:02<00:02, 386.56it/s]
Adding requests:  48%|████▊     | 992/2048 [00:02<00:02, 384.34it/s]
Adding requests:  50%|█████     | 1031/2048 [00:02<00:02, 382.46it/s]
Adding requests:  52%|█████▏    | 1070/2048 [00:02<00:02, 382.43it/s]
Adding requests:  54%|█████▍    | 1109/2048 [00:02<00:02, 378.07it/s]
Adding requests:  56%|█████▌    | 1149/2048 [00:02<00:02, 383.62it/s]
Adding requests:  58%|█████▊    | 1188/2048 [00:03<00:02, 382.69it/s]
Adding requests:  60%|██████    | 1229/2048 [00:03<00:02, 388.09it/s]
Adding requests:  62%|██████▏   | 1268/2048 [00:03<00:02, 386.09it/s]
Adding requests:  64%|██████▍   | 1307/2048 [00:03<00:01, 384.76it/s]
Adding requests:  66%|██████▌   | 1347/2048 [00:03<00:01, 385.90it/s]
Adding requests:  68%|██████▊   | 1387/2048 [00:03<00:01, 388.39it/s]
Adding requests:  70%|██████▉   | 1426/2048 [00:03<00:01, 384.93it/s]
Adding requests:  72%|███████▏  | 1466/2048 [00:03<00:01, 389.11it/s]
Adding requests:  74%|███████▎  | 1506/2048 [00:03<00:01, 391.67it/s]
Adding requests:  75%|███████▌  | 1546/2048 [00:04<00:01, 388.59it/s]
Adding requests:  77%|███████▋  | 1585/2048 [00:04<00:01, 381.90it/s]
Adding requests:  79%|███████▉  | 1624/2048 [00:04<00:01, 375.98it/s]
Adding requests:  81%|████████  | 1662/2048 [00:04<00:01, 371.05it/s]
Adding requests:  83%|████████▎ | 1701/2048 [00:04<00:00, 375.00it/s]
Adding requests:  85%|████████▌ | 1741/2048 [00:04<00:00, 379.13it/s]
Adding requests:  87%|████████▋ | 1782/2048 [00:04<00:00, 386.50it/s]
Adding requests:  89%|████████▉ | 1821/2048 [00:04<00:00, 382.34it/s]
Adding requests:  91%|█████████ | 1861/2048 [00:04<00:00, 386.67it/s]
Adding requests:  93%|█████████▎| 1900/2048 [00:04<00:00, 385.76it/s]
Adding requests:  95%|█████████▍| 1939/2048 [00:05<00:00, 385.09it/s]
Adding requests:  97%|█████████▋| 1979/2048 [00:05<00:00, 388.44it/s]
Adding requests:  99%|█████████▊| 2018/2048 [00:05<00:00, 379.03it/s]
Adding requests: 100%|██████████| 2048/2048 [00:05<00:00, 383.74it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  13%|█▎        | 274/2048 [00:00<00:02, 814.10it/s, est. speed input: 833694.78 toks/s, output: 814.11 toks/s]
Processed prompts:  17%|█▋        | 356/2048 [00:01<00:10, 154.89it/s, est. speed input: 195082.49 toks/s, output: 190.51 toks/s]
Processed prompts:  19%|█▉        | 393/2048 [00:02<00:13, 123.09it/s, est. speed input: 161692.37 toks/s, output: 157.90 toks/s]
Processed prompts:  20%|██        | 416/2048 [00:02<00:14, 113.57it/s, est. speed input: 152115.92 toks/s, output: 148.55 toks/s]
Processed prompts:  21%|██        | 433/2048 [00:03<00:16, 100.84it/s, est. speed input: 142501.34 toks/s, output: 139.16 toks/s]
Processed prompts:  22%|██▏       | 446/2048 [00:03<00:18, 88.30it/s, est. speed input: 134252.82 toks/s, output: 131.11 toks/s] 
Processed prompts:  22%|██▏       | 456/2048 [00:03<00:21, 74.04it/s, est. speed input: 125799.41 toks/s, output: 122.85 toks/s]
Processed prompts:  23%|██▎       | 466/2048 [00:04<00:25, 62.82it/s, est. speed input: 118602.55 toks/s, output: 115.82 toks/s]
Processed prompts:  24%|██▎       | 482/2048 [00:04<00:26, 59.71it/s, est. speed input: 113887.65 toks/s, output: 111.22 toks/s]
Processed prompts:  24%|██▍       | 498/2048 [00:04<00:26, 57.50it/s, est. speed input: 109846.06 toks/s, output: 107.27 toks/s]
Processed prompts:  25%|██▌       | 514/2048 [00:04<00:27, 55.71it/s, est. speed input: 106241.23 toks/s, output: 103.75 toks/s]
Processed prompts:  26%|██▌       | 530/2048 [00:05<00:27, 54.36it/s, est. speed input: 103036.78 toks/s, output: 100.62 toks/s]
Processed prompts:  27%|██▋       | 546/2048 [00:05<00:28, 53.61it/s, est. speed input: 100265.93 toks/s, output: 97.92 toks/s] 
Processed prompts:  27%|██▋       | 562/2048 [00:05<00:28, 52.93it/s, est. speed input: 97738.93 toks/s, output: 95.45 toks/s] 
Processed prompts:  28%|██▊       | 578/2048 [00:06<00:28, 52.46it/s, est. speed input: 95469.69 toks/s, output: 93.23 toks/s]
Processed prompts:  29%|██▉       | 594/2048 [00:06<00:27, 52.22it/s, est. speed input: 93444.49 toks/s, output: 91.25 toks/s]
Processed prompts:  30%|██▉       | 610/2048 [00:06<00:27, 52.00it/s, est. speed input: 91587.91 toks/s, output: 89.44 toks/s]
Processed prompts:  31%|███       | 626/2048 [00:07<00:27, 51.84it/s, est. speed input: 89894.10 toks/s, output: 87.79 toks/s]
Processed prompts:  31%|███▏      | 642/2048 [00:07<00:27, 51.76it/s, est. speed input: 88347.31 toks/s, output: 86.28 toks/s]
Processed prompts:  32%|███▏      | 658/2048 [00:07<00:26, 51.72it/s, est. speed input: 86927.57 toks/s, output: 84.89 toks/s]
Processed prompts:  33%|███▎      | 674/2048 [00:08<00:26, 51.65it/s, est. speed input: 85610.21 toks/s, output: 83.60 toks/s]
Processed prompts:  34%|███▎      | 690/2048 [00:08<00:26, 51.62it/s, est. speed input: 84393.75 toks/s, output: 82.42 toks/s]
Processed prompts:  34%|███▍      | 706/2048 [00:08<00:25, 51.63it/s, est. speed input: 83269.24 toks/s, output: 81.32 toks/s]
Processed prompts:  35%|███▌      | 722/2048 [00:08<00:25, 51.55it/s, est. speed input: 82207.69 toks/s, output: 80.28 toks/s]
Processed prompts:  36%|███▌      | 738/2048 [00:09<00:25, 51.61it/s, est. speed input: 81235.71 toks/s, output: 79.33 toks/s]
Processed prompts:  37%|███▋      | 754/2048 [00:09<00:25, 51.59it/s, est. speed input: 80316.74 toks/s, output: 78.43 toks/s]
Processed prompts:  38%|███▊      | 770/2048 [00:09<00:24, 51.56it/s, est. speed input: 79453.49 toks/s, output: 77.59 toks/s]
Processed prompts:  38%|███▊      | 786/2048 [00:10<00:23, 52.59it/s, est. speed input: 78801.22 toks/s, output: 76.95 toks/s]
Processed prompts:  39%|███▉      | 802/2048 [00:10<00:23, 52.26it/s, est. speed input: 78031.17 toks/s, output: 76.20 toks/s]
Processed prompts:  40%|███▉      | 818/2048 [00:10<00:23, 52.04it/s, est. speed input: 77307.54 toks/s, output: 75.50 toks/s]
Processed prompts:  41%|████      | 834/2048 [00:11<00:23, 51.86it/s, est. speed input: 76620.94 toks/s, output: 74.82 toks/s]
Processed prompts:  42%|████▏     | 850/2048 [00:11<00:23, 51.78it/s, est. speed input: 75976.52 toks/s, output: 74.20 toks/s]
Processed prompts:  42%|████▏     | 866/2048 [00:11<00:22, 51.73it/s, est. speed input: 75367.65 toks/s, output: 73.60 toks/s]
Processed prompts:  43%|████▎     | 882/2048 [00:12<00:22, 51.65it/s, est. speed input: 74784.28 toks/s, output: 73.03 toks/s]
Processed prompts:  44%|████▍     | 898/2048 [00:12<00:22, 51.69it/s, est. speed input: 74241.27 toks/s, output: 72.50 toks/s]
Processed prompts:  45%|████▍     | 914/2048 [00:12<00:21, 51.63it/s, est. speed input: 73714.29 toks/s, output: 71.99 toks/s]
Processed prompts:  45%|████▌     | 930/2048 [00:13<00:21, 51.60it/s, est. speed input: 73214.14 toks/s, output: 71.50 toks/s]
Processed prompts:  46%|████▌     | 946/2048 [00:13<00:21, 51.54it/s, est. speed input: 72733.11 toks/s, output: 71.03 toks/s]
Processed prompts:  47%|████▋     | 962/2048 [00:13<00:21, 51.59it/s, est. speed input: 72283.71 toks/s, output: 70.59 toks/s]
Processed prompts:  48%|████▊     | 978/2048 [00:13<00:20, 51.49it/s, est. speed input: 71840.89 toks/s, output: 70.16 toks/s]
Processed prompts:  49%|████▊     | 994/2048 [00:14<00:20, 51.49it/s, est. speed input: 71423.51 toks/s, output: 69.75 toks/s]
Processed prompts:  49%|████▉     | 1010/2048 [00:14<00:20, 51.50it/s, est. speed input: 71025.55 toks/s, output: 69.36 toks/s]
Processed prompts:  50%|█████     | 1026/2048 [00:14<00:19, 51.45it/s, est. speed input: 70639.09 toks/s, output: 68.98 toks/s]
Processed prompts:  51%|█████     | 1042/2048 [00:15<00:19, 51.42it/s, est. speed input: 70268.66 toks/s, output: 68.62 toks/s]
Processed prompts:  52%|█████▏    | 1058/2048 [00:15<00:19, 51.47it/s, est. speed input: 69919.72 toks/s, output: 68.28 toks/s]
Processed prompts:  52%|█████▏    | 1074/2048 [00:15<00:18, 51.53it/s, est. speed input: 69585.77 toks/s, output: 67.95 toks/s]
Processed prompts:  53%|█████▎    | 1090/2048 [00:16<00:18, 51.46it/s, est. speed input: 69255.62 toks/s, output: 67.63 toks/s]
Processed prompts:  54%|█████▍    | 1106/2048 [00:16<00:18, 51.51it/s, est. speed input: 68946.67 toks/s, output: 67.33 toks/s]
Processed prompts:  55%|█████▍    | 1122/2048 [00:16<00:17, 51.47it/s, est. speed input: 68642.57 toks/s, output: 67.03 toks/s]
Processed prompts:  56%|█████▌    | 1138/2048 [00:17<00:17, 51.40it/s, est. speed input: 68346.68 toks/s, output: 66.74 toks/s]
Processed prompts:  56%|█████▋    | 1154/2048 [00:17<00:17, 51.38it/s, est. speed input: 68062.87 toks/s, output: 66.47 toks/s]
Processed prompts:  57%|█████▋    | 1170/2048 [00:17<00:17, 51.38it/s, est. speed input: 67790.94 toks/s, output: 66.20 toks/s]
Processed prompts:  58%|█████▊    | 1186/2048 [00:17<00:16, 51.40it/s, est. speed input: 67529.44 toks/s, output: 65.95 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [00:18<00:16, 52.40it/s, est. speed input: 67348.95 toks/s, output: 65.77 toks/s]
Processed prompts:  59%|█████▉    | 1218/2048 [00:18<00:15, 52.11it/s, est. speed input: 67103.67 toks/s, output: 65.53 toks/s]
Processed prompts:  60%|██████    | 1234/2048 [00:18<00:15, 52.87it/s, est. speed input: 66932.48 toks/s, output: 65.36 toks/s]
Processed prompts:  61%|██████    | 1250/2048 [00:19<00:15, 52.43it/s, est. speed input: 66701.22 toks/s, output: 65.14 toks/s]
Processed prompts:  62%|██████▏   | 1266/2048 [00:19<00:15, 52.09it/s, est. speed input: 66474.96 toks/s, output: 64.92 toks/s]
Processed prompts:  63%|██████▎   | 1282/2048 [00:19<00:14, 51.77it/s, est. speed input: 66250.35 toks/s, output: 64.70 toks/s]
Processed prompts:  63%|██████▎   | 1298/2048 [00:20<00:14, 51.72it/s, est. speed input: 66043.55 toks/s, output: 64.50 toks/s]
Processed prompts:  64%|██████▍   | 1314/2048 [00:20<00:14, 51.60it/s, est. speed input: 65838.00 toks/s, output: 64.29 toks/s]
Processed prompts:  65%|██████▍   | 1330/2048 [00:20<00:13, 52.49it/s, est. speed input: 65699.00 toks/s, output: 64.16 toks/s]
Processed prompts:  66%|██████▌   | 1346/2048 [00:21<00:13, 52.12it/s, est. speed input: 65503.44 toks/s, output: 63.97 toks/s]
Processed prompts:  67%|██████▋   | 1362/2048 [00:21<00:13, 51.91it/s, est. speed input: 65316.56 toks/s, output: 63.79 toks/s]
Processed prompts:  67%|██████▋   | 1378/2048 [00:21<00:12, 51.72it/s, est. speed input: 65131.89 toks/s, output: 63.61 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [00:21<00:12, 51.62it/s, est. speed input: 64954.58 toks/s, output: 63.43 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [00:22<00:12, 51.61it/s, est. speed input: 64785.75 toks/s, output: 63.27 toks/s]
Processed prompts:  70%|██████▉   | 1426/2048 [00:22<00:12, 51.50it/s, est. speed input: 64615.46 toks/s, output: 63.10 toks/s]
Processed prompts:  70%|███████   | 1442/2048 [00:22<00:11, 51.44it/s, est. speed input: 64451.24 toks/s, output: 62.94 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [00:23<00:11, 52.42it/s, est. speed input: 64347.23 toks/s, output: 62.84 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [00:23<00:11, 52.09it/s, est. speed input: 64191.28 toks/s, output: 62.69 toks/s]
Processed prompts:  73%|███████▎  | 1490/2048 [00:23<00:10, 51.85it/s, est. speed input: 64038.36 toks/s, output: 62.54 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [00:24<00:10, 51.69it/s, est. speed input: 63889.84 toks/s, output: 62.39 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [00:24<00:09, 52.61it/s, est. speed input: 63798.26 toks/s, output: 62.30 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [00:24<00:09, 52.13it/s, est. speed input: 63652.41 toks/s, output: 62.16 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [00:25<00:09, 52.85it/s, est. speed input: 63561.98 toks/s, output: 62.07 toks/s]
Processed prompts:  77%|███████▋  | 1570/2048 [00:25<00:09, 52.43it/s, est. speed input: 63428.58 toks/s, output: 61.94 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [00:25<00:08, 51.99it/s, est. speed input: 63291.52 toks/s, output: 61.81 toks/s]
Processed prompts:  78%|███████▊  | 1602/2048 [00:25<00:08, 51.77it/s, est. speed input: 63161.97 toks/s, output: 61.68 toks/s]
Processed prompts:  79%|███████▉  | 1618/2048 [00:26<00:08, 52.65it/s, est. speed input: 63083.86 toks/s, output: 61.61 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [00:26<00:07, 52.16it/s, est. speed input: 62956.34 toks/s, output: 61.48 toks/s]
Processed prompts:  81%|████████  | 1650/2048 [00:26<00:07, 51.93it/s, est. speed input: 62836.76 toks/s, output: 61.36 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [00:27<00:07, 51.70it/s, est. speed input: 62716.86 toks/s, output: 61.25 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [00:27<00:07, 51.51it/s, est. speed input: 62598.11 toks/s, output: 61.13 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [00:27<00:06, 51.43it/s, est. speed input: 62484.79 toks/s, output: 61.02 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [00:28<00:06, 51.34it/s, est. speed input: 62372.09 toks/s, output: 60.91 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [00:28<00:06, 52.31it/s, est. speed input: 62307.05 toks/s, output: 60.85 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [00:28<00:05, 52.95it/s, est. speed input: 62240.56 toks/s, output: 60.78 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [00:29<00:05, 52.41it/s, est. speed input: 62134.99 toks/s, output: 60.68 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [00:29<00:05, 52.09it/s, est. speed input: 62033.57 toks/s, output: 60.58 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [00:29<00:04, 51.77it/s, est. speed input: 61930.49 toks/s, output: 60.48 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [00:29<00:04, 51.54it/s, est. speed input: 61828.97 toks/s, output: 60.38 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [00:30<00:04, 51.51it/s, est. speed input: 61734.92 toks/s, output: 60.29 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [00:30<00:04, 51.36it/s, est. speed input: 61637.67 toks/s, output: 60.19 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [00:30<00:03, 51.26it/s, est. speed input: 61542.40 toks/s, output: 60.10 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [00:31<00:03, 51.27it/s, est. speed input: 61452.48 toks/s, output: 60.01 toks/s]
Processed prompts:  92%|█████████▏| 1890/2048 [00:31<00:03, 52.21it/s, est. speed input: 61400.21 toks/s, output: 59.96 toks/s]
Processed prompts:  93%|█████████▎| 1906/2048 [00:31<00:02, 51.89it/s, est. speed input: 61311.67 toks/s, output: 59.87 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [00:32<00:02, 51.68it/s, est. speed input: 61225.42 toks/s, output: 59.79 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [00:32<00:02, 51.51it/s, est. speed input: 61139.80 toks/s, output: 59.71 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [00:32<00:01, 51.41it/s, est. speed input: 61056.34 toks/s, output: 59.63 toks/s]
Processed prompts:  96%|█████████▌| 1970/2048 [00:33<00:01, 51.28it/s, est. speed input: 60972.34 toks/s, output: 59.54 toks/s]
Processed prompts:  97%|█████████▋| 1986/2048 [00:33<00:01, 52.30it/s, est. speed input: 60930.17 toks/s, output: 59.50 toks/s]
Processed prompts:  98%|█████████▊| 2002/2048 [00:33<00:00, 51.99it/s, est. speed input: 60852.21 toks/s, output: 59.43 toks/s]
Processed prompts:  99%|█████████▊| 2018/2048 [00:34<00:00, 51.67it/s, est. speed input: 60772.01 toks/s, output: 59.35 toks/s]
Processed prompts:  99%|█████████▉| 2034/2048 [00:34<00:00, 52.58it/s, est. speed input: 60732.76 toks/s, output: 59.31 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:34<00:00, 52.58it/s, est. speed input: 61149.71 toks/s, output: 59.72 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:34<00:00, 59.72it/s, est. speed input: 61149.71 toks/s, output: 59.72 toks/s]
[rank0]:[W126 11:09:25.952147331 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 89.5s

测试结果:
  Requests/s:   51.67
  Tokens/s:     52963.66
  Total Reqs:   2048
  Elapsed:      39.63s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     52911.98

============================================================
[7/8] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 11:09:57 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1264432) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1264432) WARNING 01-26 11:10:13 [backends.py:609] Failed to read file <frozen os>
Throughput: 52.35 requests/s, 53656.89 total tokens/s, 52.35 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-26 11:09:56] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:09:56] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:09:56] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 11:09:56] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:09:56] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:09:56] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:09:56] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:09:56] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:09:56] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:09:57] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:09:57] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:09:57] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:09:57] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:09:57] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 11:10:03] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:10:04] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:10:04] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 11:10:04] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:10:04] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:10:04] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:10:04] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:10:04] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:10:04] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:10:04] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:10:04] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:10:04] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:10:04] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:10:04] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1264432) [2026-01-26 11:10:06] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1264432) [2026-01-26 11:10:06] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1264432) [2026-01-26 11:10:06] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1264432) [2026-01-26 11:10:06] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=1264432) [2026-01-26 11:10:06] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=1264432) [2026-01-26 11:10:06] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=1264432) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1264432) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.59it/s]
(EngineCore_DP0 pid=1264432) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.90it/s]
(EngineCore_DP0 pid=1264432) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.98it/s]
(EngineCore_DP0 pid=1264432) 
(EngineCore_DP0 pid=1264432) [2026-01-26 11:10:07] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 3584] -> 1D uint8
(EngineCore_DP0 pid=1264432) [2026-01-26 11:10:07] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 10321920 bytes
(EngineCore_DP0 pid=1264432) [2026-01-26 11:10:07] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 3584] -> 1D uint8
(EngineCore_DP0 pid=1264432) [2026-01-26 11:10:07] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 8028160 bytes
(EngineCore_DP0 pid=1264432) [2026-01-26 11:10:07] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 3584] -> 1D uint8
(EngineCore_DP0 pid=1264432) [2026-01-26 11:10:07] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 84869120 bytes
(EngineCore_DP0 pid=1264432) [2026-01-26 11:10:07] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 18944] -> 1D uint8
(EngineCore_DP0 pid=1264432) [2026-01-26 11:10:07] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 42434560 bytes
(EngineCore_DP0 pid=1264432) [rank0]:W0126 11:10:18.885000 1264432 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1264432) [rank0]:W0126 11:10:18.961000 1264432 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1264432) [rank0]:W0126 11:10:19.867000 1264432 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1264432) [rank0]:W0126 11:10:19.988000 1264432 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1264432) 2026-01-26 11:10:23,968 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1264432) 2026-01-26 11:10:23,995 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1264432) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▉         | 1/11 [00:00<00:05,  1.91it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:02,  3.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 3/11 [00:01<00:02,  2.74it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 5/11 [00:01<00:01,  5.33it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 6/11 [00:01<00:00,  5.80it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:01<00:00,  7.93it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 10/11 [00:01<00:00, 10.34it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00,  6.62it/s]
(EngineCore_DP0 pid=1264432) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 1/7 [00:00<00:00,  7.14it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 3/7 [00:00<00:00, 12.04it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00, 11.31it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00,  9.60it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 23/4096 [00:00<00:17, 228.02it/s]
Adding requests:   2%|▏         | 63/4096 [00:00<00:12, 326.90it/s]
Adding requests:   2%|▏         | 99/4096 [00:00<00:11, 337.53it/s]
Adding requests:   3%|▎         | 137/4096 [00:00<00:11, 351.16it/s]
Adding requests:   4%|▍         | 176/4096 [00:00<00:10, 363.82it/s]
Adding requests:   5%|▌         | 216/4096 [00:00<00:10, 375.06it/s]
Adding requests:   6%|▌         | 254/4096 [00:00<00:10, 376.03it/s]
Adding requests:   7%|▋         | 293/4096 [00:00<00:10, 379.60it/s]
Adding requests:   8%|▊         | 334/4096 [00:00<00:09, 387.29it/s]
Adding requests:   9%|▉         | 374/4096 [00:01<00:09, 390.62it/s]
Adding requests:  10%|█         | 415/4096 [00:01<00:09, 395.77it/s]
Adding requests:  11%|█         | 455/4096 [00:01<00:09, 393.32it/s]
Adding requests:  12%|█▏        | 498/4096 [00:01<00:08, 401.56it/s]
Adding requests:  13%|█▎        | 540/4096 [00:01<00:08, 406.03it/s]
Adding requests:  14%|█▍        | 581/4096 [00:01<00:08, 401.87it/s]
Adding requests:  15%|█▌        | 622/4096 [00:01<00:08, 394.93it/s]
Adding requests:  16%|█▌        | 662/4096 [00:01<00:08, 386.97it/s]
Adding requests:  17%|█▋        | 702/4096 [00:01<00:08, 388.58it/s]
Adding requests:  18%|█▊        | 741/4096 [00:01<00:08, 383.77it/s]
Adding requests:  19%|█▉        | 780/4096 [00:02<00:08, 384.00it/s]
Adding requests:  20%|█▉        | 819/4096 [00:02<00:08, 382.54it/s]
Adding requests:  21%|██        | 860/4096 [00:02<00:08, 390.20it/s]
Adding requests:  22%|██▏       | 900/4096 [00:02<00:08, 391.23it/s]
Adding requests:  23%|██▎       | 940/4096 [00:02<00:08, 384.93it/s]
Adding requests:  24%|██▍       | 979/4096 [00:02<00:08, 384.73it/s]
Adding requests:  25%|██▍       | 1018/4096 [00:02<00:08, 379.36it/s]
Adding requests:  26%|██▌       | 1056/4096 [00:02<00:08, 377.21it/s]
Adding requests:  27%|██▋       | 1094/4096 [00:02<00:08, 369.37it/s]
Adding requests:  28%|██▊       | 1134/4096 [00:02<00:07, 375.78it/s]
Adding requests:  29%|██▊       | 1172/4096 [00:03<00:07, 372.79it/s]
Adding requests:  30%|██▉       | 1212/4096 [00:03<00:07, 378.96it/s]
Adding requests:  31%|███       | 1252/4096 [00:03<00:07, 382.50it/s]
Adding requests:  32%|███▏      | 1291/4096 [00:03<00:07, 375.72it/s]
Adding requests:  32%|███▏      | 1331/4096 [00:03<00:07, 379.93it/s]
Adding requests:  33%|███▎      | 1371/4096 [00:03<00:07, 385.16it/s]
Adding requests:  34%|███▍      | 1410/4096 [00:03<00:07, 382.28it/s]
Adding requests:  35%|███▌      | 1449/4096 [00:03<00:06, 381.32it/s]
Adding requests:  36%|███▋      | 1490/4096 [00:03<00:06, 387.50it/s]
Adding requests:  37%|███▋      | 1530/4096 [00:04<00:06, 388.24it/s]
Adding requests:  38%|███▊      | 1569/4096 [00:04<00:06, 381.88it/s]
Adding requests:  39%|███▉      | 1608/4096 [00:04<00:06, 380.02it/s]
Adding requests:  40%|████      | 1647/4096 [00:04<00:06, 371.27it/s]
Adding requests:  41%|████      | 1685/4096 [00:04<00:06, 371.35it/s]
Adding requests:  42%|████▏     | 1724/4096 [00:04<00:06, 375.27it/s]
Adding requests:  43%|████▎     | 1764/4096 [00:04<00:06, 381.50it/s]
Adding requests:  44%|████▍     | 1803/4096 [00:04<00:05, 382.78it/s]
Adding requests:  45%|████▍     | 1842/4096 [00:04<00:05, 382.44it/s]
Adding requests:  46%|████▌     | 1881/4096 [00:04<00:05, 384.01it/s]
Adding requests:  47%|████▋     | 1921/4096 [00:05<00:05, 388.18it/s]
Adding requests:  48%|████▊     | 1962/4096 [00:05<00:05, 391.06it/s]
Adding requests:  49%|████▉     | 2002/4096 [00:05<00:05, 384.73it/s]
Adding requests:  50%|████▉     | 2041/4096 [00:05<00:05, 379.38it/s]
Adding requests:  51%|█████     | 2079/4096 [00:05<00:05, 371.16it/s]
Adding requests:  52%|█████▏    | 2119/4096 [00:05<00:05, 378.59it/s]
Adding requests:  53%|█████▎    | 2157/4096 [00:05<00:05, 376.71it/s]
Adding requests:  54%|█████▎    | 2195/4096 [00:05<00:05, 371.41it/s]
Adding requests:  55%|█████▍    | 2233/4096 [00:05<00:04, 373.29it/s]
Adding requests:  55%|█████▌    | 2273/4096 [00:05<00:04, 380.99it/s]
Adding requests:  56%|█████▋    | 2312/4096 [00:06<00:04, 371.41it/s]
Adding requests:  57%|█████▋    | 2353/4096 [00:06<00:04, 382.12it/s]
Adding requests:  58%|█████▊    | 2394/4096 [00:06<00:04, 387.27it/s]
Adding requests:  59%|█████▉    | 2434/4096 [00:06<00:04, 390.10it/s]
Adding requests:  60%|██████    | 2474/4096 [00:06<00:04, 390.55it/s]
Adding requests:  61%|██████▏   | 2515/4096 [00:06<00:04, 393.49it/s]
Adding requests:  62%|██████▏   | 2557/4096 [00:06<00:03, 400.20it/s]
Adding requests:  63%|██████▎   | 2599/4096 [00:06<00:03, 405.07it/s]
Adding requests:  64%|██████▍   | 2640/4096 [00:06<00:03, 389.62it/s]
Adding requests:  65%|██████▌   | 2680/4096 [00:07<00:03, 389.49it/s]
Adding requests:  66%|██████▋   | 2720/4096 [00:07<00:03, 386.32it/s]
Adding requests:  67%|██████▋   | 2761/4096 [00:07<00:03, 390.89it/s]
Adding requests:  68%|██████▊   | 2804/4096 [00:07<00:03, 398.89it/s]
Adding requests:  69%|██████▉   | 2844/4096 [00:07<00:03, 396.17it/s]
Adding requests:  70%|███████   | 2884/4096 [00:07<00:03, 391.86it/s]
Adding requests:  71%|███████▏  | 2924/4096 [00:07<00:02, 393.04it/s]
Adding requests:  72%|███████▏  | 2965/4096 [00:07<00:02, 396.96it/s]
Adding requests:  73%|███████▎  | 3006/4096 [00:07<00:02, 398.11it/s]
Adding requests:  74%|███████▍  | 3047/4096 [00:07<00:02, 399.60it/s]
Adding requests:  75%|███████▌  | 3088/4096 [00:08<00:02, 400.30it/s]
Adding requests:  76%|███████▋  | 3130/4096 [00:08<00:02, 403.03it/s]
Adding requests:  77%|███████▋  | 3171/4096 [00:08<00:02, 396.48it/s]
Adding requests:  78%|███████▊  | 3211/4096 [00:08<00:02, 392.80it/s]
Adding requests:  79%|███████▉  | 3253/4096 [00:08<00:02, 399.61it/s]
Adding requests:  80%|████████  | 3293/4096 [00:08<00:02, 384.06it/s]
Adding requests:  81%|████████▏ | 3332/4096 [00:08<00:02, 379.76it/s]
Adding requests:  82%|████████▏ | 3374/4096 [00:08<00:01, 388.66it/s]
Adding requests:  83%|████████▎ | 3414/4096 [00:08<00:01, 390.60it/s]
Adding requests:  84%|████████▍ | 3454/4096 [00:08<00:01, 390.36it/s]
Adding requests:  85%|████████▌ | 3494/4096 [00:09<00:01, 388.10it/s]
Adding requests:  86%|████████▋ | 3537/4096 [00:09<00:01, 400.00it/s]
Adding requests:  87%|████████▋ | 3578/4096 [00:09<00:01, 397.32it/s]
Adding requests:  88%|████████▊ | 3618/4096 [00:09<00:01, 396.47it/s]
Adding requests:  89%|████████▉ | 3658/4096 [00:09<00:01, 381.23it/s]
Adding requests:  90%|█████████ | 3697/4096 [00:09<00:01, 376.71it/s]
Adding requests:  91%|█████████ | 3737/4096 [00:09<00:00, 383.39it/s]
Adding requests:  92%|█████████▏| 3776/4096 [00:09<00:00, 374.30it/s]
Adding requests:  93%|█████████▎| 3814/4096 [00:09<00:00, 365.99it/s]
Adding requests:  94%|█████████▍| 3853/4096 [00:10<00:00, 371.85it/s]
Adding requests:  95%|█████████▌| 3892/4096 [00:10<00:00, 374.48it/s]
Adding requests:  96%|█████████▌| 3930/4096 [00:10<00:00, 370.67it/s]
Adding requests:  97%|█████████▋| 3969/4096 [00:10<00:00, 374.19it/s]
Adding requests:  98%|█████████▊| 4007/4096 [00:10<00:00, 374.68it/s]
Adding requests:  99%|█████████▉| 4045/4096 [00:10<00:00, 373.83it/s]
Adding requests: 100%|█████████▉| 4083/4096 [00:10<00:00, 374.40it/s]
Adding requests: 100%|██████████| 4096/4096 [00:10<00:00, 383.46it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  13%|█▎        | 546/4096 [00:00<00:03, 984.74it/s, est. speed input: 1008412.92 toks/s, output: 984.75 toks/s]
Processed prompts:  16%|█▌        | 645/4096 [00:02<00:15, 217.69it/s, est. speed input: 277885.53 toks/s, output: 271.37 toks/s] 
Processed prompts:  17%|█▋        | 689/4096 [00:02<00:19, 176.39it/s, est. speed input: 236077.52 toks/s, output: 230.54 toks/s]
Processed prompts:  18%|█▊        | 717/4096 [00:03<00:24, 139.08it/s, est. speed input: 204209.72 toks/s, output: 199.42 toks/s]
Processed prompts:  18%|█▊        | 738/4096 [00:04<00:30, 108.61it/s, est. speed input: 179734.38 toks/s, output: 175.52 toks/s]
Processed prompts:  19%|█▉        | 770/4096 [00:04<00:35, 93.00it/s, est. speed input: 164459.20 toks/s, output: 160.60 toks/s] 
Processed prompts:  20%|█▉        | 802/4096 [00:05<00:40, 81.03it/s, est. speed input: 152035.51 toks/s, output: 148.47 toks/s]
Processed prompts:  20%|██        | 834/4096 [00:06<00:45, 72.40it/s, est. speed input: 141992.31 toks/s, output: 138.66 toks/s]
Processed prompts:  21%|██        | 866/4096 [00:06<00:48, 66.57it/s, est. speed input: 133943.33 toks/s, output: 130.80 toks/s]
Processed prompts:  22%|██▏       | 898/4096 [00:07<00:51, 62.30it/s, est. speed input: 127149.83 toks/s, output: 124.17 toks/s]
Processed prompts:  23%|██▎       | 930/4096 [00:07<00:53, 59.36it/s, est. speed input: 121446.17 toks/s, output: 118.60 toks/s]
Processed prompts:  23%|██▎       | 962/4096 [00:08<00:54, 57.30it/s, est. speed input: 116560.58 toks/s, output: 113.83 toks/s]
Processed prompts:  24%|██▍       | 994/4096 [00:09<00:55, 55.85it/s, est. speed input: 112328.84 toks/s, output: 109.70 toks/s]
Processed prompts:  25%|██▌       | 1026/4096 [00:09<00:56, 54.81it/s, est. speed input: 108621.58 toks/s, output: 106.08 toks/s]
Processed prompts:  26%|██▌       | 1058/4096 [00:10<00:56, 54.12it/s, est. speed input: 105370.53 toks/s, output: 102.90 toks/s]
Processed prompts:  27%|██▋       | 1090/4096 [00:10<00:56, 53.58it/s, est. speed input: 102464.00 toks/s, output: 100.06 toks/s]
Processed prompts:  27%|██▋       | 1122/4096 [00:11<00:55, 53.30it/s, est. speed input: 99897.66 toks/s, output: 97.56 toks/s]  
Processed prompts:  28%|██▊       | 1154/4096 [00:12<00:55, 52.94it/s, est. speed input: 97538.24 toks/s, output: 95.25 toks/s]
Processed prompts:  29%|██▉       | 1186/4096 [00:12<00:54, 53.35it/s, est. speed input: 95596.88 toks/s, output: 93.36 toks/s]
Processed prompts:  30%|██▉       | 1218/4096 [00:13<00:53, 53.55it/s, est. speed input: 93802.89 toks/s, output: 91.60 toks/s]
Processed prompts:  31%|███       | 1250/4096 [00:13<00:53, 53.19it/s, est. speed input: 92036.22 toks/s, output: 89.88 toks/s]
Processed prompts:  31%|███▏      | 1282/4096 [00:14<00:53, 52.95it/s, est. speed input: 90421.41 toks/s, output: 88.30 toks/s]
Processed prompts:  32%|███▏      | 1314/4096 [00:15<00:52, 53.17it/s, est. speed input: 89024.08 toks/s, output: 86.94 toks/s]
Processed prompts:  33%|███▎      | 1346/4096 [00:15<00:51, 52.99it/s, est. speed input: 87662.76 toks/s, output: 85.61 toks/s]
Processed prompts:  34%|███▎      | 1378/4096 [00:16<00:51, 52.73it/s, est. speed input: 86373.82 toks/s, output: 84.35 toks/s]
Processed prompts:  34%|███▍      | 1410/4096 [00:16<00:51, 52.65it/s, est. speed input: 85199.21 toks/s, output: 83.20 toks/s]
Processed prompts:  35%|███▌      | 1442/4096 [00:17<00:50, 53.05it/s, est. speed input: 84189.69 toks/s, output: 82.22 toks/s]
Processed prompts:  36%|███▌      | 1474/4096 [00:18<00:49, 52.80it/s, est. speed input: 83152.02 toks/s, output: 81.20 toks/s]
Processed prompts:  37%|███▋      | 1506/4096 [00:18<00:48, 53.24it/s, est. speed input: 82285.22 toks/s, output: 80.36 toks/s]
Processed prompts:  38%|███▊      | 1538/4096 [00:19<00:47, 53.41it/s, est. speed input: 81449.97 toks/s, output: 79.54 toks/s]
Processed prompts:  38%|███▊      | 1570/4096 [00:19<00:47, 53.08it/s, est. speed input: 80595.72 toks/s, output: 78.71 toks/s]
Processed prompts:  39%|███▉      | 1602/4096 [00:20<00:46, 53.37it/s, est. speed input: 79868.42 toks/s, output: 78.00 toks/s]
Processed prompts:  40%|███▉      | 1634/4096 [00:21<00:46, 53.01it/s, est. speed input: 79102.48 toks/s, output: 77.25 toks/s]
Processed prompts:  41%|████      | 1666/4096 [00:21<00:45, 52.83it/s, est. speed input: 78389.05 toks/s, output: 76.55 toks/s]
Processed prompts:  41%|████▏     | 1698/4096 [00:22<00:45, 52.66it/s, est. speed input: 77708.12 toks/s, output: 75.89 toks/s]
Processed prompts:  42%|████▏     | 1730/4096 [00:22<00:44, 53.63it/s, est. speed input: 77202.12 toks/s, output: 75.39 toks/s]
Processed prompts:  43%|████▎     | 1762/4096 [00:23<00:43, 53.21it/s, est. speed input: 76586.18 toks/s, output: 74.79 toks/s]
Processed prompts:  44%|████▍     | 1794/4096 [00:24<00:43, 52.85it/s, est. speed input: 75993.55 toks/s, output: 74.21 toks/s]
Processed prompts:  45%|████▍     | 1826/4096 [00:24<00:43, 52.75it/s, est. speed input: 75447.04 toks/s, output: 73.68 toks/s]
Processed prompts:  45%|████▌     | 1858/4096 [00:25<00:42, 52.54it/s, est. speed input: 74911.03 toks/s, output: 73.16 toks/s]
Processed prompts:  46%|████▌     | 1890/4096 [00:25<00:41, 52.89it/s, est. speed input: 74455.62 toks/s, output: 72.71 toks/s]
Processed prompts:  47%|████▋     | 1922/4096 [00:26<00:41, 52.71it/s, est. speed input: 73974.09 toks/s, output: 72.24 toks/s]
Processed prompts:  48%|████▊     | 1954/4096 [00:27<00:40, 52.55it/s, est. speed input: 73511.94 toks/s, output: 71.79 toks/s]
Processed prompts:  48%|████▊     | 1986/4096 [00:27<00:39, 52.90it/s, est. speed input: 73115.43 toks/s, output: 71.40 toks/s]
Processed prompts:  49%|████▉     | 2018/4096 [00:28<00:39, 52.66it/s, est. speed input: 72689.17 toks/s, output: 70.99 toks/s]
Processed prompts:  50%|█████     | 2050/4096 [00:29<00:38, 53.01it/s, est. speed input: 72329.90 toks/s, output: 70.63 toks/s]
Processed prompts:  51%|█████     | 2082/4096 [00:29<00:38, 52.75it/s, est. speed input: 71937.43 toks/s, output: 70.25 toks/s]
Processed prompts:  52%|█████▏    | 2114/4096 [00:30<00:37, 52.56it/s, est. speed input: 71560.92 toks/s, output: 69.88 toks/s]
Processed prompts:  52%|█████▏    | 2146/4096 [00:30<00:37, 52.43it/s, est. speed input: 71199.04 toks/s, output: 69.53 toks/s]
Processed prompts:  53%|█████▎    | 2178/4096 [00:31<00:36, 52.85it/s, est. speed input: 70895.97 toks/s, output: 69.23 toks/s]
Processed prompts:  54%|█████▍    | 2210/4096 [00:32<00:35, 52.58it/s, est. speed input: 70556.11 toks/s, output: 68.90 toks/s]
Processed prompts:  55%|█████▍    | 2242/4096 [00:32<00:35, 52.46it/s, est. speed input: 70235.11 toks/s, output: 68.59 toks/s]
Processed prompts:  56%|█████▌    | 2274/4096 [00:33<00:34, 52.36it/s, est. speed input: 69924.35 toks/s, output: 68.29 toks/s]
Processed prompts:  56%|█████▋    | 2306/4096 [00:33<00:34, 52.25it/s, est. speed input: 69621.81 toks/s, output: 67.99 toks/s]
Processed prompts:  57%|█████▋    | 2338/4096 [00:34<00:33, 52.21it/s, est. speed input: 69332.66 toks/s, output: 67.71 toks/s]
Processed prompts:  58%|█████▊    | 2370/4096 [00:35<00:33, 52.19it/s, est. speed input: 69054.27 toks/s, output: 67.44 toks/s]
Processed prompts:  59%|█████▊    | 2402/4096 [00:35<00:32, 52.13it/s, est. speed input: 68782.26 toks/s, output: 67.17 toks/s]
Processed prompts:  59%|█████▉    | 2434/4096 [00:36<00:31, 52.09it/s, est. speed input: 68519.20 toks/s, output: 66.91 toks/s]
Processed prompts:  60%|██████    | 2466/4096 [00:36<00:31, 52.06it/s, est. speed input: 68265.35 toks/s, output: 66.67 toks/s]
Processed prompts:  61%|██████    | 2498/4096 [00:37<00:30, 52.03it/s, est. speed input: 68018.85 toks/s, output: 66.42 toks/s]
Processed prompts:  62%|██████▏   | 2530/4096 [00:38<00:29, 52.62it/s, est. speed input: 67822.49 toks/s, output: 66.23 toks/s]
Processed prompts:  63%|██████▎   | 2562/4096 [00:38<00:29, 52.35it/s, est. speed input: 67585.52 toks/s, output: 66.00 toks/s]
Processed prompts:  63%|██████▎   | 2594/4096 [00:39<00:28, 52.72it/s, est. speed input: 67393.19 toks/s, output: 65.81 toks/s]
Processed prompts:  64%|██████▍   | 2626/4096 [00:40<00:27, 52.55it/s, est. speed input: 67179.13 toks/s, output: 65.60 toks/s]
Processed prompts:  65%|██████▍   | 2658/4096 [00:40<00:27, 52.91it/s, est. speed input: 67001.30 toks/s, output: 65.43 toks/s]
Processed prompts:  66%|██████▌   | 2690/4096 [00:41<00:26, 52.68it/s, est. speed input: 66799.09 toks/s, output: 65.23 toks/s]
Processed prompts:  66%|██████▋   | 2722/4096 [00:41<00:25, 52.90it/s, est. speed input: 66625.57 toks/s, output: 65.06 toks/s]
Processed prompts:  67%|██████▋   | 2754/4096 [00:42<00:25, 52.59it/s, est. speed input: 66429.38 toks/s, output: 64.87 toks/s]
Processed prompts:  68%|██████▊   | 2786/4096 [00:43<00:24, 52.47it/s, est. speed input: 66244.94 toks/s, output: 64.69 toks/s]
Processed prompts:  69%|██████▉   | 2818/4096 [00:43<00:24, 52.30it/s, est. speed input: 66060.13 toks/s, output: 64.51 toks/s]
Processed prompts:  70%|██████▉   | 2850/4096 [00:44<00:23, 52.22it/s, est. speed input: 65882.75 toks/s, output: 64.34 toks/s]
Processed prompts:  70%|███████   | 2882/4096 [00:44<00:22, 53.07it/s, est. speed input: 65761.48 toks/s, output: 64.22 toks/s]
Processed prompts:  71%|███████   | 2914/4096 [00:45<00:22, 53.17it/s, est. speed input: 65615.34 toks/s, output: 64.08 toks/s]
Processed prompts:  72%|███████▏  | 2946/4096 [00:46<00:21, 52.81it/s, est. speed input: 65449.93 toks/s, output: 63.92 toks/s]
Processed prompts:  73%|███████▎  | 2978/4096 [00:46<00:21, 52.55it/s, est. speed input: 65288.53 toks/s, output: 63.76 toks/s]
Processed prompts:  73%|███████▎  | 3010/4096 [00:47<00:20, 52.40it/s, est. speed input: 65132.93 toks/s, output: 63.61 toks/s]
Processed prompts:  74%|███████▍  | 3042/4096 [00:47<00:20, 52.25it/s, est. speed input: 64978.88 toks/s, output: 63.46 toks/s]
Processed prompts:  75%|███████▌  | 3074/4096 [00:48<00:19, 52.08it/s, est. speed input: 64825.11 toks/s, output: 63.31 toks/s]
Processed prompts:  76%|███████▌  | 3106/4096 [00:49<00:19, 52.02it/s, est. speed input: 64678.39 toks/s, output: 63.16 toks/s]
Processed prompts:  77%|███████▋  | 3138/4096 [00:49<00:18, 51.96it/s, est. speed input: 64534.63 toks/s, output: 63.02 toks/s]
Processed prompts:  77%|███████▋  | 3170/4096 [00:50<00:17, 51.97it/s, est. speed input: 64396.68 toks/s, output: 62.89 toks/s]
Processed prompts:  78%|███████▊  | 3202/4096 [00:51<00:17, 52.01it/s, est. speed input: 64263.47 toks/s, output: 62.76 toks/s]
Processed prompts:  79%|███████▉  | 3234/4096 [00:51<00:16, 51.92it/s, est. speed input: 64127.93 toks/s, output: 62.62 toks/s]
Processed prompts:  80%|███████▉  | 3266/4096 [00:52<00:15, 51.91it/s, est. speed input: 63998.30 toks/s, output: 62.50 toks/s]
Processed prompts:  81%|████████  | 3298/4096 [00:52<00:15, 51.86it/s, est. speed input: 63869.63 toks/s, output: 62.37 toks/s]
Processed prompts:  81%|████████▏ | 3330/4096 [00:53<00:14, 51.80it/s, est. speed input: 63742.56 toks/s, output: 62.25 toks/s]
Processed prompts:  82%|████████▏ | 3362/4096 [00:54<00:14, 51.84it/s, est. speed input: 63622.36 toks/s, output: 62.13 toks/s]
Processed prompts:  83%|████████▎ | 3394/4096 [00:54<00:13, 51.87it/s, est. speed input: 63504.66 toks/s, output: 62.02 toks/s]
Processed prompts:  84%|████████▎ | 3426/4096 [00:55<00:12, 51.87it/s, est. speed input: 63389.12 toks/s, output: 61.90 toks/s]
Processed prompts:  84%|████████▍ | 3458/4096 [00:55<00:12, 51.87it/s, est. speed input: 63275.75 toks/s, output: 61.79 toks/s]
Processed prompts:  85%|████████▌ | 3490/4096 [00:56<00:11, 51.81it/s, est. speed input: 63162.41 toks/s, output: 61.68 toks/s]
Processed prompts:  86%|████████▌ | 3522/4096 [00:57<00:11, 51.80it/s, est. speed input: 63052.75 toks/s, output: 61.57 toks/s]
Processed prompts:  87%|████████▋ | 3554/4096 [00:57<00:10, 52.26it/s, est. speed input: 62965.73 toks/s, output: 61.49 toks/s]
Processed prompts:  88%|████████▊ | 3586/4096 [00:58<00:09, 52.08it/s, est. speed input: 62858.90 toks/s, output: 61.39 toks/s]
Processed prompts:  88%|████████▊ | 3618/4096 [00:59<00:09, 51.98it/s, est. speed input: 62755.65 toks/s, output: 61.28 toks/s]
Processed prompts:  89%|████████▉ | 3650/4096 [00:59<00:08, 51.86it/s, est. speed input: 62652.09 toks/s, output: 61.18 toks/s]
Processed prompts:  90%|████████▉ | 3682/4096 [01:00<00:07, 52.32it/s, est. speed input: 62573.05 toks/s, output: 61.11 toks/s]
Processed prompts:  91%|█████████ | 3714/4096 [01:00<00:07, 52.12it/s, est. speed input: 62474.90 toks/s, output: 61.01 toks/s]
Processed prompts:  91%|█████████▏| 3746/4096 [01:01<00:06, 52.01it/s, est. speed input: 62379.64 toks/s, output: 60.92 toks/s]
Processed prompts:  92%|█████████▏| 3778/4096 [01:02<00:06, 52.00it/s, est. speed input: 62288.90 toks/s, output: 60.83 toks/s]
Processed prompts:  93%|█████████▎| 3810/4096 [01:02<00:05, 51.93it/s, est. speed input: 62197.44 toks/s, output: 60.74 toks/s]
Processed prompts:  94%|█████████▍| 3842/4096 [01:03<00:04, 51.91it/s, est. speed input: 62108.78 toks/s, output: 60.65 toks/s]
Processed prompts:  95%|█████████▍| 3874/4096 [01:03<00:04, 51.86it/s, est. speed input: 62020.77 toks/s, output: 60.57 toks/s]
Processed prompts:  95%|█████████▌| 3906/4096 [01:04<00:03, 52.37it/s, est. speed input: 61954.58 toks/s, output: 60.50 toks/s]
Processed prompts:  96%|█████████▌| 3938/4096 [01:05<00:03, 52.63it/s, est. speed input: 61886.13 toks/s, output: 60.44 toks/s]
Processed prompts:  97%|█████████▋| 3970/4096 [01:05<00:02, 52.32it/s, est. speed input: 61800.76 toks/s, output: 60.35 toks/s]
Processed prompts:  98%|█████████▊| 4002/4096 [01:06<00:01, 52.66it/s, est. speed input: 61737.32 toks/s, output: 60.29 toks/s]
Processed prompts:  98%|█████████▊| 4034/4096 [01:06<00:01, 52.80it/s, est. speed input: 61671.46 toks/s, output: 60.23 toks/s]
Processed prompts:  99%|█████████▉| 4066/4096 [01:07<00:00, 53.55it/s, est. speed input: 61628.93 toks/s, output: 60.18 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [01:07<00:00, 53.55it/s, est. speed input: 62082.74 toks/s, output: 60.63 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [01:07<00:00, 60.63it/s, est. speed input: 62082.74 toks/s, output: 60.63 toks/s]
[rank0]:[W126 11:11:46.601725945 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 140.7s

测试结果:
  Requests/s:   52.35
  Tokens/s:     53656.89
  Total Reqs:   4096
  Elapsed:      78.25s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     53604.54

============================================================
[8/8] 测试 M=65536
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 65536
│   M_prefill     = 65536 (= 64 x 1024)
│   M_decode      = 64
│   batched_tokens = 65536 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 8192
│   --max-num-seqs           = 64
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 65536
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 11:12:39 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1267043) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1267043) WARNING 01-26 11:12:57 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]     self.model_runner.profile_run()
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]     hidden_states = self.model(
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]                     ^^^^^^^^^^^
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]     def forward(
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]     raise e
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]   File "<eval_with_key>.58", line 325, in forward
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]     return range_entry.runnable(*args)
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]     return self._compiled_fn(*args)
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]     return compiled_fn(full_args)
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]                             ^^^^^^^
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 724, in inner_fn
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]     outs = compiled_fn(args)
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]            ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]     return self.current_callable(inputs)
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]     out = model(new_inputs)
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]   File "/tmp/torchinductor_root/kh/ckhj7zmb322mko3v5rzl4ng6llpwtacrpf3ftoyspbamrbf3bcif.py", line 1090, in call
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]     triton_poi_fused_mul_quant_slide_int8_silu_slice_1.run(buf15, buf16, triton_poi_fused_mul_quant_slide_int8_silu_slice_1_xnumel, stream=stream0)
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 1272, in run
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]     self.autotune_to_one_config(*args, **kwargs)
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 1048, in autotune_to_one_config
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]     timings = self.benchmark_all_configs(*args, **kwargs)
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 1023, in benchmark_all_configs
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]     launcher: self.bench(launcher, *args, **kwargs)
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 891, in bench
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]     return benchmarker.benchmark_gpu(kernel_call, rep=40)
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/benchmarking.py", line 39, in wrapper
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]     return fn(self, *args, **kwargs)
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/benchmarking.py", line 247, in benchmark_gpu
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]     torch.cuda.synchronize()
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py", line 1083, in synchronize
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]     return torch._C._cuda_synchronize()
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866] torch.AcceleratorError: CUDA error: an illegal memory access was encountered
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866] Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866] For debugging consider passing CUDA_LAUNCH_BLOCKING=1
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
(EngineCore_DP0 pid=1267043) ERROR 01-26 11:13:05 [core.py:866] 


─── STDERR ───
[2026-01-26 11:12:38] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:12:39] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:12:39] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 11:12:39] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:12:39] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:12:39] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:12:39] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:12:39] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:12:39] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:12:39] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:12:39] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:12:39] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:12:39] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:12:39] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 11:12:46] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:12:47] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:12:47] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 11:12:47] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:12:47] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:12:47] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:12:47] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:12:47] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:12:47] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:12:47] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:12:47] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:12:47] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:12:47] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:12:47] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1267043) [2026-01-26 11:12:49] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1267043) [2026-01-26 11:12:49] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1267043) [2026-01-26 11:12:49] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1267043) [2026-01-26 11:12:49] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=1267043) [2026-01-26 11:12:49] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=1267043) [2026-01-26 11:12:49] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=1267043) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1267043) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.56it/s]
(EngineCore_DP0 pid=1267043) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.88it/s]
(EngineCore_DP0 pid=1267043) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.96it/s]
(EngineCore_DP0 pid=1267043) 
(EngineCore_DP0 pid=1267043) [2026-01-26 11:12:51] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 3584] -> 1D uint8
(EngineCore_DP0 pid=1267043) [2026-01-26 11:12:51] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 10321920 bytes
(EngineCore_DP0 pid=1267043) [2026-01-26 11:12:51] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 3584] -> 1D uint8
(EngineCore_DP0 pid=1267043) [2026-01-26 11:12:51] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 8028160 bytes
(EngineCore_DP0 pid=1267043) [2026-01-26 11:12:51] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 3584] -> 1D uint8
(EngineCore_DP0 pid=1267043) [2026-01-26 11:12:51] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 84869120 bytes
(EngineCore_DP0 pid=1267043) [2026-01-26 11:12:51] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 18944] -> 1D uint8
(EngineCore_DP0 pid=1267043) [2026-01-26 11:12:51] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 42434560 bytes
(EngineCore_DP0 pid=1267043) [rank0]:W0126 11:13:02.936000 1267043 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1267043) [rank0]:W0126 11:13:03.016000 1267043 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1267043) [rank0]:W0126 11:13:04.459000 1267043 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1267043) [rank0]:W0126 11:13:04.583000 1267043 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1267043) Process EngineCore_DP0:
(EngineCore_DP0 pid=1267043) Traceback (most recent call last):
(EngineCore_DP0 pid=1267043)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=1267043)     self.run()
(EngineCore_DP0 pid=1267043)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=1267043)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=1267043)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=1267043)     raise e
(EngineCore_DP0 pid=1267043)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1267043)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1267043)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1267043)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1267043)     super().__init__(
(EngineCore_DP0 pid=1267043)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=1267043)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=1267043)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1267043)   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=1267043)     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=1267043)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1267043)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=1267043)     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=1267043)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1267043)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=1267043)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=1267043)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1267043)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=1267043)     return func(*args, **kwargs)
(EngineCore_DP0 pid=1267043)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1267043)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=1267043)     return func(*args, **kwargs)
(EngineCore_DP0 pid=1267043)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1267043)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=1267043)     self.model_runner.profile_run()
(EngineCore_DP0 pid=1267043)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=1267043)     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=1267043)                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1267043)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=1267043)     return func(*args, **kwargs)
(EngineCore_DP0 pid=1267043)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1267043)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=1267043)     outputs = self.model(
(EngineCore_DP0 pid=1267043)               ^^^^^^^^^^^
(EngineCore_DP0 pid=1267043)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=1267043)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=1267043)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1267043)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=1267043)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=1267043)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1267043)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=1267043)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=1267043)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1267043)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=1267043)     hidden_states = self.model(
(EngineCore_DP0 pid=1267043)                     ^^^^^^^^^^^
(EngineCore_DP0 pid=1267043)   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=1267043)     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=1267043)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1267043)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=1267043)     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=1267043)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1267043)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=1267043)     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=1267043)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1267043)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
(EngineCore_DP0 pid=1267043)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=1267043)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1267043)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
(EngineCore_DP0 pid=1267043)     def forward(
(EngineCore_DP0 pid=1267043)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=1267043)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=1267043)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1267043)   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=1267043)     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=1267043)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1267043)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=1267043)     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=1267043)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1267043)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=1267043)     raise e
(EngineCore_DP0 pid=1267043)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=1267043)     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=1267043)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1267043)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=1267043)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=1267043)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1267043)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=1267043)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=1267043)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1267043)   File "<eval_with_key>.58", line 325, in forward
(EngineCore_DP0 pid=1267043)     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
(EngineCore_DP0 pid=1267043)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1267043)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=1267043)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=1267043)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1267043)   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=1267043)     return range_entry.runnable(*args)
(EngineCore_DP0 pid=1267043)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1267043)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=1267043)     return self._compiled_fn(*args)
(EngineCore_DP0 pid=1267043)            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1267043)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=1267043)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=1267043)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1267043)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=1267043)     return compiled_fn(full_args)
(EngineCore_DP0 pid=1267043)            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1267043)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=1267043)     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=1267043)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1267043)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=1267043)     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=1267043)                             ^^^^^^^
(EngineCore_DP0 pid=1267043)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 724, in inner_fn
(EngineCore_DP0 pid=1267043)     outs = compiled_fn(args)
(EngineCore_DP0 pid=1267043)            ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1267043)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=1267043)     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=1267043)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1267043)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=1267043)     return self.current_callable(inputs)
(EngineCore_DP0 pid=1267043)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1267043)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=1267043)     out = model(new_inputs)
(EngineCore_DP0 pid=1267043)           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1267043)   File "/tmp/torchinductor_root/kh/ckhj7zmb322mko3v5rzl4ng6llpwtacrpf3ftoyspbamrbf3bcif.py", line 1090, in call
(EngineCore_DP0 pid=1267043)     triton_poi_fused_mul_quant_slide_int8_silu_slice_1.run(buf15, buf16, triton_poi_fused_mul_quant_slide_int8_silu_slice_1_xnumel, stream=stream0)
(EngineCore_DP0 pid=1267043)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 1272, in run
(EngineCore_DP0 pid=1267043)     self.autotune_to_one_config(*args, **kwargs)
(EngineCore_DP0 pid=1267043)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 1048, in autotune_to_one_config
(EngineCore_DP0 pid=1267043)     timings = self.benchmark_all_configs(*args, **kwargs)
(EngineCore_DP0 pid=1267043)               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1267043)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 1023, in benchmark_all_configs
(EngineCore_DP0 pid=1267043)     launcher: self.bench(launcher, *args, **kwargs)
(EngineCore_DP0 pid=1267043)               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1267043)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/triton_heuristics.py", line 891, in bench
(EngineCore_DP0 pid=1267043)     return benchmarker.benchmark_gpu(kernel_call, rep=40)
(EngineCore_DP0 pid=1267043)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1267043)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/benchmarking.py", line 39, in wrapper
(EngineCore_DP0 pid=1267043)     return fn(self, *args, **kwargs)
(EngineCore_DP0 pid=1267043)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1267043)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/runtime/benchmarking.py", line 247, in benchmark_gpu
(EngineCore_DP0 pid=1267043)     torch.cuda.synchronize()
(EngineCore_DP0 pid=1267043)   File "/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py", line 1083, in synchronize
(EngineCore_DP0 pid=1267043)     return torch._C._cuda_synchronize()
(EngineCore_DP0 pid=1267043)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1267043) torch.AcceleratorError: CUDA error: an illegal memory access was encountered
(EngineCore_DP0 pid=1267043) Search for `cudaErrorIllegalAddress' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
(EngineCore_DP0 pid=1267043) CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
(EngineCore_DP0 pid=1267043) For debugging consider passing CUDA_LAUNCH_BLOCKING=1
(EngineCore_DP0 pid=1267043) Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
(EngineCore_DP0 pid=1267043) 
[rank0]:[W126 11:13:06.457235302 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[ERROR] 测试失败: M=65536 (exit code: 1)


------------------------------------------------------------
  生成 CSV: Qwen2.5-7B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_INT8_py312_cu129_x86_64/cusparselt/2_4/Qwen2.5-7B-INT8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,32.6317,16740.0460,3.9226
1024,1024,1,128,128,34.3873,35246.9651,3.7223
2048,1024,2,256,128,46.2814,47438.4307,5.5314
4096,1024,4,512,128,49.1190,50347.0128,10.4237
8192,1024,8,1024,128,50.5336,51796.9637,20.2637
16384,1024,16,2048,128,51.6719,52963.6558,39.6347
32768,1024,32,4096,128,52.3482,53656.8879,78.2453
65536,1024,64,8192,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[INFO] 完成: 7 成功, 1 失败

============================================================
  Qwen2.5-7B-INT8 | cuSPARSELt (2_6) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_6
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_INT8_py312_cu129_x86_64/cusparselt/2_6

============================================================
[1/8] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 11:13:16 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1268068) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1268068) WARNING 01-26 11:13:32 [backends.py:609] Failed to read file <frozen os>
Throughput: 36.23 requests/s, 18584.97 total tokens/s, 36.23 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-26 11:13:15] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:13:16] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:13:16] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 11:13:16] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:13:16] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:13:16] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:13:16] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:13:16] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:13:16] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:13:16] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:13:16] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:13:16] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:13:16] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:13:16] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 11:13:22] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:13:23] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:13:23] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 11:13:23] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:13:23] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:13:23] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:13:23] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:13:23] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:13:23] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:13:23] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:13:23] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:13:23] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:13:23] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:13:23] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1268068) [2026-01-26 11:13:25] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1268068) [2026-01-26 11:13:25] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1268068) [2026-01-26 11:13:25] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1268068) [2026-01-26 11:13:25] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=1268068) [2026-01-26 11:13:25] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=1268068) [2026-01-26 11:13:25] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=1268068) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1268068) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.12it/s]
(EngineCore_DP0 pid=1268068) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.53it/s]
(EngineCore_DP0 pid=1268068) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.59it/s]
(EngineCore_DP0 pid=1268068) 
(EngineCore_DP0 pid=1268068) [2026-01-26 11:13:26] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 4800] -> 1D uint8
(EngineCore_DP0 pid=1268068) [2026-01-26 11:13:26] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 13824000 bytes
(EngineCore_DP0 pid=1268068) [2026-01-26 11:13:26] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 4800] -> 1D uint8
(EngineCore_DP0 pid=1268068) [2026-01-26 11:13:26] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 10752000 bytes
(EngineCore_DP0 pid=1268068) [2026-01-26 11:13:26] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 4800] -> 1D uint8
(EngineCore_DP0 pid=1268068) [2026-01-26 11:13:26] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 113664000 bytes
(EngineCore_DP0 pid=1268068) [2026-01-26 11:13:26] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 25280] -> 1D uint8
(EngineCore_DP0 pid=1268068) [2026-01-26 11:13:26] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 56627200 bytes
(EngineCore_DP0 pid=1268068) 2026-01-26 11:13:43,767 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1268068) 2026-01-26 11:13:43,796 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1268068) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  2.19it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  2.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  2.49it/s]
(EngineCore_DP0 pid=1268068) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 17.92it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  28%|██▊       | 36/128 [00:00<00:00, 356.43it/s]
Adding requests:  83%|████████▎ | 106/128 [00:00<00:00, 552.05it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 545.89it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:23,  5.33it/s, est. speed input: 2731.80 toks/s, output: 5.34 toks/s]
Processed prompts:   5%|▍         | 6/128 [00:00<00:05, 22.32it/s, est. speed input: 9858.15 toks/s, output: 19.25 toks/s]
Processed prompts:   9%|▊         | 11/128 [00:00<00:03, 29.85it/s, est. speed input: 12977.60 toks/s, output: 25.35 toks/s]
Processed prompts:  12%|█▎        | 16/128 [00:00<00:03, 33.88it/s, est. speed input: 14721.28 toks/s, output: 28.75 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:02, 36.28it/s, est. speed input: 15839.01 toks/s, output: 30.93 toks/s]
Processed prompts:  20%|██        | 26/128 [00:00<00:02, 37.79it/s, est. speed input: 16614.12 toks/s, output: 32.45 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:00<00:02, 38.77it/s, est. speed input: 17183.88 toks/s, output: 33.56 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:01<00:02, 39.48it/s, est. speed input: 17630.40 toks/s, output: 34.43 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:01<00:02, 39.96it/s, est. speed input: 17983.09 toks/s, output: 35.12 toks/s]
Processed prompts:  36%|███▌      | 46/128 [00:01<00:02, 40.24it/s, est. speed input: 18261.63 toks/s, output: 35.67 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:01<00:01, 40.37it/s, est. speed input: 18485.67 toks/s, output: 36.10 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:01<00:01, 40.56it/s, est. speed input: 18684.46 toks/s, output: 36.49 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:01<00:01, 40.72it/s, est. speed input: 18857.47 toks/s, output: 36.83 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:01<00:01, 40.79it/s, est. speed input: 19001.69 toks/s, output: 37.11 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:01<00:01, 40.84it/s, est. speed input: 19127.93 toks/s, output: 37.36 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:02<00:01, 40.83it/s, est. speed input: 19235.36 toks/s, output: 37.57 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:02<00:01, 40.89it/s, est. speed input: 19335.66 toks/s, output: 37.76 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:02<00:01, 40.92it/s, est. speed input: 19424.84 toks/s, output: 37.94 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:02<00:00, 40.89it/s, est. speed input: 19500.49 toks/s, output: 38.09 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:02<00:00, 40.89it/s, est. speed input: 19570.65 toks/s, output: 38.22 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:02<00:00, 40.89it/s, est. speed input: 19633.92 toks/s, output: 38.35 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:02<00:00, 40.87it/s, est. speed input: 19690.46 toks/s, output: 38.46 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:02<00:00, 40.83it/s, est. speed input: 19739.76 toks/s, output: 38.55 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:03<00:00, 40.77it/s, est. speed input: 19783.74 toks/s, output: 38.64 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:03<00:00, 40.76it/s, est. speed input: 19825.55 toks/s, output: 38.72 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:03<00:00, 40.72it/s, est. speed input: 19862.59 toks/s, output: 38.79 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 40.72it/s, est. speed input: 19878.54 toks/s, output: 38.82 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 38.82it/s, est. speed input: 19878.54 toks/s, output: 38.82 toks/s]
[rank0]:[W126 11:13:50.044504348 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 42.9s

测试结果:
  Requests/s:   36.23
  Tokens/s:     18584.97
  Total Reqs:   128
  Elapsed:      3.53s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     18548.75

============================================================
[2/8] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 11:13:59 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1269216) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1269216) WARNING 01-26 11:14:16 [backends.py:609] Failed to read file <frozen os>
Throughput: 33.07 requests/s, 33896.27 total tokens/s, 33.07 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-26 11:13:58] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:13:59] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:13:59] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 11:13:59] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:13:59] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:13:59] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:13:59] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:13:59] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:13:59] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:13:59] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:13:59] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:13:59] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:13:59] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:13:59] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 11:14:06] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:14:07] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:14:07] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 11:14:07] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:14:07] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:14:07] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:14:07] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:14:07] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:14:07] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:14:07] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:14:07] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:14:07] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:14:07] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:14:07] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1269216) [2026-01-26 11:14:08] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1269216) [2026-01-26 11:14:08] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1269216) [2026-01-26 11:14:08] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1269216) [2026-01-26 11:14:08] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=1269216) [2026-01-26 11:14:08] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=1269216) [2026-01-26 11:14:08] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=1269216) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1269216) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.08it/s]
(EngineCore_DP0 pid=1269216) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.50it/s]
(EngineCore_DP0 pid=1269216) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.57it/s]
(EngineCore_DP0 pid=1269216) 
(EngineCore_DP0 pid=1269216) [2026-01-26 11:14:10] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 4800] -> 1D uint8
(EngineCore_DP0 pid=1269216) [2026-01-26 11:14:10] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 13824000 bytes
(EngineCore_DP0 pid=1269216) [2026-01-26 11:14:10] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 4800] -> 1D uint8
(EngineCore_DP0 pid=1269216) [2026-01-26 11:14:10] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 10752000 bytes
(EngineCore_DP0 pid=1269216) [2026-01-26 11:14:10] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 4800] -> 1D uint8
(EngineCore_DP0 pid=1269216) [2026-01-26 11:14:10] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 113664000 bytes
(EngineCore_DP0 pid=1269216) [2026-01-26 11:14:10] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 25280] -> 1D uint8
(EngineCore_DP0 pid=1269216) [2026-01-26 11:14:10] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 56627200 bytes
(EngineCore_DP0 pid=1269216) 2026-01-26 11:14:27,157 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1269216) 2026-01-26 11:14:27,180 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1269216) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  3.96it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  2.34it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  2.49it/s]
(EngineCore_DP0 pid=1269216) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  9.43it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  9.42it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  15%|█▍        | 19/128 [00:00<00:00, 183.34it/s]
Adding requests:  45%|████▌     | 58/128 [00:00<00:00, 303.07it/s]
Adding requests:  75%|███████▌  | 96/128 [00:00<00:00, 334.42it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 327.40it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 3/128 [00:00<00:05, 24.01it/s, est. speed input: 24596.64 toks/s, output: 24.01 toks/s]
Processed prompts:   5%|▌         | 7/128 [00:00<00:03, 31.83it/s, est. speed input: 31289.64 toks/s, output: 30.55 toks/s]
Processed prompts:   9%|▊         | 11/128 [00:00<00:03, 34.39it/s, est. speed input: 33583.48 toks/s, output: 32.79 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:00<00:03, 35.60it/s, est. speed input: 34748.23 toks/s, output: 33.93 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:00<00:03, 36.29it/s, est. speed input: 35463.84 toks/s, output: 34.63 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:00<00:02, 36.69it/s, est. speed input: 35939.57 toks/s, output: 35.10 toks/s]
Processed prompts:  21%|██        | 27/128 [00:00<00:02, 36.96it/s, est. speed input: 36285.48 toks/s, output: 35.43 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:00<00:02, 37.06it/s, est. speed input: 36517.73 toks/s, output: 35.66 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:00<00:02, 37.10it/s, est. speed input: 36690.92 toks/s, output: 35.83 toks/s]
Processed prompts:  30%|███       | 39/128 [00:01<00:02, 36.97it/s, est. speed input: 36777.01 toks/s, output: 35.91 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:01<00:02, 37.06it/s, est. speed input: 36901.77 toks/s, output: 36.04 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:01<00:02, 37.13it/s, est. speed input: 37006.64 toks/s, output: 36.14 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:01<00:02, 37.16it/s, est. speed input: 37091.75 toks/s, output: 36.22 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:01<00:01, 37.22it/s, est. speed input: 37173.99 toks/s, output: 36.30 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:01<00:01, 37.32it/s, est. speed input: 37258.88 toks/s, output: 36.38 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:01<00:01, 37.38it/s, est. speed input: 37329.55 toks/s, output: 36.45 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:01<00:01, 37.32it/s, est. speed input: 37374.37 toks/s, output: 36.50 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:01<00:01, 37.33it/s, est. speed input: 37422.23 toks/s, output: 36.54 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:02<00:01, 37.27it/s, est. speed input: 37453.13 toks/s, output: 36.57 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:02<00:01, 37.19it/s, est. speed input: 37475.37 toks/s, output: 36.60 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:02<00:01, 37.16it/s, est. speed input: 37498.89 toks/s, output: 36.62 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:02<00:01, 37.19it/s, est. speed input: 37528.98 toks/s, output: 36.65 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:02<00:00, 37.28it/s, est. speed input: 37565.73 toks/s, output: 36.68 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:02<00:00, 37.27it/s, est. speed input: 37590.31 toks/s, output: 36.71 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:02<00:00, 37.27it/s, est. speed input: 37612.80 toks/s, output: 36.73 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:02<00:00, 37.25it/s, est. speed input: 37632.20 toks/s, output: 36.75 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:02<00:00, 37.23it/s, est. speed input: 37648.32 toks/s, output: 36.77 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:03<00:00, 37.20it/s, est. speed input: 37660.99 toks/s, output: 36.78 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:03<00:00, 37.10it/s, est. speed input: 37664.67 toks/s, output: 36.78 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:03<00:00, 37.10it/s, est. speed input: 37675.27 toks/s, output: 36.79 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:03<00:00, 37.13it/s, est. speed input: 37689.28 toks/s, output: 36.81 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:03<00:00, 36.97it/s, est. speed input: 37682.50 toks/s, output: 36.80 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 36.97it/s, est. speed input: 37686.82 toks/s, output: 36.80 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 36.80it/s, est. speed input: 37686.82 toks/s, output: 36.80 toks/s]
[rank0]:[W126 11:14:33.753652406 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 43.8s

测试结果:
  Requests/s:   33.07
  Tokens/s:     33896.27
  Total Reqs:   128
  Elapsed:      3.87s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     33863.20

============================================================
[3/8] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 11:14:43 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1270321) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1270321) WARNING 01-26 11:15:00 [backends.py:609] Failed to read file <frozen os>
Throughput: 38.95 requests/s, 39924.94 total tokens/s, 38.95 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-26 11:14:43] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:14:43] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:14:43] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 11:14:43] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:14:43] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:14:43] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:14:43] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:14:43] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:14:43] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:14:43] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:14:43] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:14:43] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:14:43] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:14:43] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 11:14:50] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:14:51] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:14:51] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 11:14:51] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:14:51] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:14:51] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:14:51] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:14:51] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:14:51] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:14:51] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:14:51] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:14:51] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:14:51] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:14:51] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1270321) [2026-01-26 11:14:52] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1270321) [2026-01-26 11:14:52] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1270321) [2026-01-26 11:14:52] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1270321) [2026-01-26 11:14:52] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=1270321) [2026-01-26 11:14:52] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=1270321) [2026-01-26 11:14:52] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=1270321) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1270321) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.13it/s]
(EngineCore_DP0 pid=1270321) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.52it/s]
(EngineCore_DP0 pid=1270321) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.59it/s]
(EngineCore_DP0 pid=1270321) 
(EngineCore_DP0 pid=1270321) [2026-01-26 11:14:54] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 4800] -> 1D uint8
(EngineCore_DP0 pid=1270321) [2026-01-26 11:14:54] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 13824000 bytes
(EngineCore_DP0 pid=1270321) [2026-01-26 11:14:54] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 4800] -> 1D uint8
(EngineCore_DP0 pid=1270321) [2026-01-26 11:14:54] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 10752000 bytes
(EngineCore_DP0 pid=1270321) [2026-01-26 11:14:54] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 4800] -> 1D uint8
(EngineCore_DP0 pid=1270321) [2026-01-26 11:14:54] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 113664000 bytes
(EngineCore_DP0 pid=1270321) [2026-01-26 11:14:54] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 25280] -> 1D uint8
(EngineCore_DP0 pid=1270321) [2026-01-26 11:14:54] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 56627200 bytes
(EngineCore_DP0 pid=1270321) 2026-01-26 11:15:11,811 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1270321) 2026-01-26 11:15:11,856 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1270321) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:00<00:00, 14.80it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 14.64it/s]
(EngineCore_DP0 pid=1270321) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 18.90it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 18.88it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   8%|▊         | 20/256 [00:00<00:01, 197.10it/s]
Adding requests:  23%|██▎       | 58/256 [00:00<00:00, 303.63it/s]
Adding requests:  37%|███▋      | 94/256 [00:00<00:00, 326.39it/s]
Adding requests:  52%|█████▏    | 132/256 [00:00<00:00, 345.61it/s]
Adding requests:  67%|██████▋   | 171/256 [00:00<00:00, 360.61it/s]
Adding requests:  82%|████████▏ | 211/256 [00:00<00:00, 372.80it/s]
Adding requests:  98%|█████████▊| 250/256 [00:00<00:00, 377.35it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 353.34it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   9%|▊         | 22/256 [00:00<00:01, 182.62it/s, est. speed input: 187033.41 toks/s, output: 182.63 toks/s]
Processed prompts:  16%|█▌        | 41/256 [00:00<00:03, 65.56it/s, est. speed input: 74859.28 toks/s, output: 73.10 toks/s]   
Processed prompts:  20%|██        | 52/256 [00:00<00:03, 53.26it/s, est. speed input: 62487.42 toks/s, output: 61.02 toks/s]
Processed prompts:  23%|██▎       | 60/256 [00:01<00:03, 49.36it/s, est. speed input: 58474.24 toks/s, output: 57.10 toks/s]
Processed prompts:  26%|██▌       | 66/256 [00:01<00:04, 47.18it/s, est. speed input: 56356.76 toks/s, output: 55.03 toks/s]
Processed prompts:  28%|██▊       | 72/256 [00:01<00:04, 45.66it/s, est. speed input: 54821.51 toks/s, output: 53.54 toks/s]
Processed prompts:  30%|███       | 78/256 [00:01<00:04, 44.19it/s, est. speed input: 53465.53 toks/s, output: 52.21 toks/s]
Processed prompts:  33%|███▎      | 84/256 [00:01<00:03, 43.10it/s, est. speed input: 52355.09 toks/s, output: 51.13 toks/s]
Processed prompts:  35%|███▌      | 90/256 [00:01<00:03, 42.40it/s, est. speed input: 51467.61 toks/s, output: 50.26 toks/s]
Processed prompts:  37%|███▋      | 95/256 [00:01<00:03, 43.97it/s, est. speed input: 51435.66 toks/s, output: 50.23 toks/s]
Processed prompts:  39%|███▉      | 100/256 [00:02<00:03, 40.98it/s, est. speed input: 50267.24 toks/s, output: 49.09 toks/s]
Processed prompts:  41%|████▏     | 106/256 [00:02<00:03, 40.68it/s, est. speed input: 49630.81 toks/s, output: 48.47 toks/s]
Processed prompts:  44%|████▍     | 112/256 [00:02<00:03, 40.65it/s, est. speed input: 49118.95 toks/s, output: 47.97 toks/s]
Processed prompts:  46%|████▌     | 118/256 [00:02<00:03, 40.60it/s, est. speed input: 48661.83 toks/s, output: 47.52 toks/s]
Processed prompts:  48%|████▊     | 124/256 [00:02<00:03, 40.55it/s, est. speed input: 48253.78 toks/s, output: 47.12 toks/s]
Processed prompts:  51%|█████     | 130/256 [00:02<00:03, 40.50it/s, est. speed input: 47884.46 toks/s, output: 46.76 toks/s]
Processed prompts:  53%|█████▎    | 135/256 [00:02<00:02, 42.63it/s, est. speed input: 47996.46 toks/s, output: 46.87 toks/s]
Processed prompts:  55%|█████▍    | 140/256 [00:03<00:02, 39.57it/s, est. speed input: 47305.29 toks/s, output: 46.20 toks/s]
Processed prompts:  57%|█████▋    | 146/256 [00:03<00:02, 39.98it/s, est. speed input: 47054.42 toks/s, output: 45.95 toks/s]
Processed prompts:  59%|█████▉    | 152/256 [00:03<00:02, 40.35it/s, est. speed input: 46839.10 toks/s, output: 45.74 toks/s]
Processed prompts:  62%|██████▏   | 158/256 [00:03<00:02, 40.63it/s, est. speed input: 46647.49 toks/s, output: 45.55 toks/s]
Processed prompts:  64%|██████▍   | 164/256 [00:03<00:02, 40.69it/s, est. speed input: 46451.69 toks/s, output: 45.36 toks/s]
Processed prompts:  66%|██████▌   | 169/256 [00:03<00:02, 42.80it/s, est. speed input: 46575.24 toks/s, output: 45.48 toks/s]
Processed prompts:  68%|██████▊   | 174/256 [00:03<00:02, 39.94it/s, est. speed input: 46127.51 toks/s, output: 45.05 toks/s]
Processed prompts:  70%|███████   | 180/256 [00:04<00:01, 40.03it/s, est. speed input: 45943.95 toks/s, output: 44.87 toks/s]
Processed prompts:  72%|███████▏  | 185/256 [00:04<00:01, 42.31it/s, est. speed input: 46064.44 toks/s, output: 44.98 toks/s]
Processed prompts:  74%|███████▍  | 190/256 [00:04<00:01, 39.45it/s, est. speed input: 45661.80 toks/s, output: 44.59 toks/s]
Processed prompts:  77%|███████▋  | 196/256 [00:04<00:01, 40.01it/s, est. speed input: 45548.17 toks/s, output: 44.48 toks/s]
Processed prompts:  79%|███████▉  | 202/256 [00:04<00:01, 41.39it/s, est. speed input: 45554.06 toks/s, output: 44.49 toks/s]
Processed prompts:  81%|████████▏ | 208/256 [00:04<00:01, 41.43it/s, est. speed input: 45460.52 toks/s, output: 44.39 toks/s]
Processed prompts:  84%|████████▎ | 214/256 [00:04<00:01, 41.37it/s, est. speed input: 45362.78 toks/s, output: 44.30 toks/s]
Processed prompts:  86%|████████▌ | 220/256 [00:04<00:00, 41.39it/s, est. speed input: 45277.53 toks/s, output: 44.22 toks/s]
Processed prompts:  88%|████████▊ | 226/256 [00:05<00:00, 41.10it/s, est. speed input: 45166.12 toks/s, output: 44.11 toks/s]
Processed prompts:  91%|█████████ | 232/256 [00:05<00:00, 40.87it/s, est. speed input: 45057.02 toks/s, output: 44.00 toks/s]
Processed prompts:  93%|█████████▎| 238/256 [00:05<00:00, 40.69it/s, est. speed input: 44952.72 toks/s, output: 43.90 toks/s]
Processed prompts:  95%|█████████▌| 244/256 [00:05<00:00, 40.56it/s, est. speed input: 44853.00 toks/s, output: 43.80 toks/s]
Processed prompts:  98%|█████████▊| 250/256 [00:05<00:00, 40.59it/s, est. speed input: 44770.10 toks/s, output: 43.72 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:05<00:00, 42.33it/s, est. speed input: 44843.48 toks/s, output: 43.79 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:05<00:00, 42.33it/s, est. speed input: 44843.48 toks/s, output: 43.79 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:05<00:00, 43.79it/s, est. speed input: 44843.48 toks/s, output: 43.79 toks/s]
[rank0]:[W126 11:15:20.527664910 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 46.9s

测试结果:
  Requests/s:   38.95
  Tokens/s:     39924.94
  Total Reqs:   256
  Elapsed:      6.57s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     39885.98

============================================================
[4/8] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 11:15:32 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1271491) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1271491) WARNING 01-26 11:15:50 [backends.py:609] Failed to read file <frozen os>
Throughput: 40.93 requests/s, 41951.45 total tokens/s, 40.93 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-26 11:15:31] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:15:32] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:15:32] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 11:15:32] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:15:32] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:15:32] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:15:32] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:15:32] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:15:32] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:15:32] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:15:32] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:15:32] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:15:32] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:15:32] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 11:15:39] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:15:40] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:15:40] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 11:15:40] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:15:40] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:15:40] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:15:40] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:15:40] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:15:40] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:15:40] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:15:40] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:15:40] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:15:40] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:15:40] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1271491) [2026-01-26 11:15:41] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1271491) [2026-01-26 11:15:41] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1271491) [2026-01-26 11:15:41] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1271491) [2026-01-26 11:15:41] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=1271491) [2026-01-26 11:15:41] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=1271491) [2026-01-26 11:15:41] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=1271491) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1271491) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.08it/s]
(EngineCore_DP0 pid=1271491) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.52it/s]
(EngineCore_DP0 pid=1271491) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.58it/s]
(EngineCore_DP0 pid=1271491) 
(EngineCore_DP0 pid=1271491) [2026-01-26 11:15:44] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 4800] -> 1D uint8
(EngineCore_DP0 pid=1271491) [2026-01-26 11:15:44] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 13824000 bytes
(EngineCore_DP0 pid=1271491) [2026-01-26 11:15:44] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 4800] -> 1D uint8
(EngineCore_DP0 pid=1271491) [2026-01-26 11:15:44] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 10752000 bytes
(EngineCore_DP0 pid=1271491) [2026-01-26 11:15:44] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 4800] -> 1D uint8
(EngineCore_DP0 pid=1271491) [2026-01-26 11:15:44] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 113664000 bytes
(EngineCore_DP0 pid=1271491) [2026-01-26 11:15:44] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 25280] -> 1D uint8
(EngineCore_DP0 pid=1271491) [2026-01-26 11:15:44] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 56627200 bytes
(EngineCore_DP0 pid=1271491) 2026-01-26 11:16:01,350 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1271491) 2026-01-26 11:16:01,383 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1271491) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00,  8.87it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00,  4.06it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00,  4.42it/s]
(EngineCore_DP0 pid=1271491) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  33%|███▎      | 1/3 [00:00<00:00,  8.19it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 13.63it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   4%|▎         | 19/512 [00:00<00:02, 188.08it/s]
Adding requests:  12%|█▏        | 59/512 [00:00<00:01, 310.28it/s]
Adding requests:  19%|█▉        | 96/512 [00:00<00:01, 333.52it/s]
Adding requests:  26%|██▌       | 134/512 [00:00<00:01, 350.34it/s]
Adding requests:  34%|███▍      | 174/512 [00:00<00:00, 366.42it/s]
Adding requests:  42%|████▏     | 214/512 [00:00<00:00, 377.15it/s]
Adding requests:  49%|████▉     | 253/512 [00:00<00:00, 378.63it/s]
Adding requests:  57%|█████▋    | 292/512 [00:00<00:00, 381.58it/s]
Adding requests:  65%|██████▍   | 332/512 [00:00<00:00, 386.71it/s]
Adding requests:  73%|███████▎  | 373/512 [00:01<00:00, 392.91it/s]
Adding requests:  81%|████████  | 414/512 [00:01<00:00, 397.63it/s]
Adding requests:  89%|████████▊ | 454/512 [00:01<00:00, 395.19it/s]
Adding requests:  97%|█████████▋| 497/512 [00:01<00:00, 402.60it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 377.79it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  11%|█         | 54/512 [00:00<00:00, 526.46it/s, est. speed input: 539215.00 toks/s, output: 526.49 toks/s]
Processed prompts:  21%|██        | 107/512 [00:01<00:05, 67.98it/s, est. speed input: 80188.43 toks/s, output: 78.31 toks/s]  
Processed prompts:  26%|██▌       | 132/512 [00:01<00:06, 58.22it/s, est. speed input: 69227.02 toks/s, output: 67.60 toks/s]
Processed prompts:  29%|██▉       | 148/512 [00:02<00:06, 53.64it/s, est. speed input: 64718.06 toks/s, output: 63.20 toks/s]
Processed prompts:  31%|███       | 159/512 [00:02<00:07, 50.15it/s, est. speed input: 61860.42 toks/s, output: 60.41 toks/s]
Processed prompts:  33%|███▎      | 168/512 [00:02<00:06, 49.38it/s, est. speed input: 60820.26 toks/s, output: 59.39 toks/s]
Processed prompts:  34%|███▍      | 175/512 [00:03<00:07, 46.58it/s, est. speed input: 59229.36 toks/s, output: 57.84 toks/s]
Processed prompts:  36%|███▌      | 182/512 [00:03<00:07, 44.24it/s, est. speed input: 57879.59 toks/s, output: 56.52 toks/s]
Processed prompts:  37%|███▋      | 190/512 [00:03<00:07, 43.57it/s, est. speed input: 57005.65 toks/s, output: 55.67 toks/s]
Processed prompts:  39%|███▊      | 198/512 [00:03<00:07, 42.91it/s, est. speed input: 56194.19 toks/s, output: 54.88 toks/s]
Processed prompts:  40%|████      | 206/512 [00:03<00:07, 43.29it/s, est. speed input: 55684.03 toks/s, output: 54.38 toks/s]
Processed prompts:  42%|████▏     | 214/512 [00:03<00:06, 42.58it/s, est. speed input: 54997.44 toks/s, output: 53.71 toks/s]
Processed prompts:  43%|████▎     | 222/512 [00:04<00:06, 42.12it/s, est. speed input: 54389.20 toks/s, output: 53.11 toks/s]
Processed prompts:  45%|████▍     | 230/512 [00:04<00:06, 41.89it/s, est. speed input: 53853.61 toks/s, output: 52.59 toks/s]
Processed prompts:  46%|████▋     | 238/512 [00:04<00:06, 41.66it/s, est. speed input: 53352.16 toks/s, output: 52.10 toks/s]
Processed prompts:  48%|████▊     | 246/512 [00:04<00:06, 41.42it/s, est. speed input: 52878.23 toks/s, output: 51.64 toks/s]
Processed prompts:  50%|████▉     | 254/512 [00:04<00:06, 41.15it/s, est. speed input: 52425.14 toks/s, output: 51.20 toks/s]
Processed prompts:  51%|█████     | 262/512 [00:05<00:06, 41.15it/s, est. speed input: 52037.06 toks/s, output: 50.82 toks/s]
Processed prompts:  53%|█████▎    | 270/512 [00:05<00:05, 41.18it/s, est. speed input: 51681.99 toks/s, output: 50.47 toks/s]
Processed prompts:  54%|█████▍    | 278/512 [00:05<00:05, 41.15it/s, est. speed input: 51343.87 toks/s, output: 50.14 toks/s]
Processed prompts:  56%|█████▌    | 286/512 [00:05<00:05, 41.03it/s, est. speed input: 51015.82 toks/s, output: 49.82 toks/s]
Processed prompts:  57%|█████▋    | 294/512 [00:05<00:05, 41.00it/s, est. speed input: 50716.00 toks/s, output: 49.53 toks/s]
Processed prompts:  59%|█████▉    | 302/512 [00:06<00:05, 40.94it/s, est. speed input: 50430.03 toks/s, output: 49.25 toks/s]
Processed prompts:  61%|██████    | 310/512 [00:06<00:04, 42.26it/s, est. speed input: 50329.28 toks/s, output: 49.15 toks/s]
Processed prompts:  62%|██████▏   | 318/512 [00:06<00:04, 41.89it/s, est. speed input: 50081.20 toks/s, output: 48.91 toks/s]
Processed prompts:  64%|██████▎   | 326/512 [00:06<00:04, 41.54it/s, est. speed input: 49836.36 toks/s, output: 48.67 toks/s]
Processed prompts:  65%|██████▌   | 334/512 [00:06<00:04, 41.39it/s, est. speed input: 49614.97 toks/s, output: 48.45 toks/s]
Processed prompts:  67%|██████▋   | 342/512 [00:07<00:04, 41.16it/s, est. speed input: 49392.27 toks/s, output: 48.23 toks/s]
Processed prompts:  68%|██████▊   | 350/512 [00:07<00:03, 41.12it/s, est. speed input: 49195.19 toks/s, output: 48.04 toks/s]
Processed prompts:  70%|██████▉   | 358/512 [00:07<00:03, 41.10it/s, est. speed input: 49008.96 toks/s, output: 47.86 toks/s]
Processed prompts:  71%|███████▏  | 366/512 [00:07<00:03, 41.10it/s, est. speed input: 48833.53 toks/s, output: 47.69 toks/s]
Processed prompts:  73%|███████▎  | 374/512 [00:07<00:03, 41.05it/s, est. speed input: 48661.03 toks/s, output: 47.52 toks/s]
Processed prompts:  75%|███████▍  | 382/512 [00:08<00:03, 40.90it/s, est. speed input: 48486.65 toks/s, output: 47.35 toks/s]
Processed prompts:  76%|███████▌  | 390/512 [00:08<00:02, 40.96it/s, est. speed input: 48335.78 toks/s, output: 47.20 toks/s]
Processed prompts:  78%|███████▊  | 398/512 [00:08<00:02, 41.04it/s, est. speed input: 48195.40 toks/s, output: 47.07 toks/s]
Processed prompts:  79%|███████▉  | 406/512 [00:08<00:02, 41.02it/s, est. speed input: 48054.83 toks/s, output: 46.93 toks/s]
Processed prompts:  81%|████████  | 414/512 [00:08<00:02, 40.96it/s, est. speed input: 47915.97 toks/s, output: 46.79 toks/s]
Processed prompts:  82%|████████▏ | 422/512 [00:09<00:02, 40.85it/s, est. speed input: 47778.21 toks/s, output: 46.66 toks/s]
Processed prompts:  84%|████████▍ | 430/512 [00:09<00:02, 40.85it/s, est. speed input: 47651.63 toks/s, output: 46.53 toks/s]
Processed prompts:  86%|████████▌ | 438/512 [00:09<00:01, 42.20it/s, est. speed input: 47636.50 toks/s, output: 46.52 toks/s]
Processed prompts:  87%|████████▋ | 446/512 [00:09<00:01, 41.90it/s, est. speed input: 47526.64 toks/s, output: 46.41 toks/s]
Processed prompts:  89%|████████▊ | 454/512 [00:09<00:01, 41.54it/s, est. speed input: 47410.13 toks/s, output: 46.30 toks/s]
Processed prompts:  90%|█████████ | 462/512 [00:10<00:01, 41.26it/s, est. speed input: 47295.86 toks/s, output: 46.19 toks/s]
Processed prompts:  92%|█████████▏| 470/512 [00:10<00:01, 41.11it/s, est. speed input: 47189.02 toks/s, output: 46.08 toks/s]
Processed prompts:  93%|█████████▎| 478/512 [00:10<00:00, 41.15it/s, est. speed input: 47096.38 toks/s, output: 45.99 toks/s]
Processed prompts:  95%|█████████▍| 486/512 [00:10<00:00, 41.15it/s, est. speed input: 47004.95 toks/s, output: 45.90 toks/s]
Processed prompts:  96%|█████████▋| 494/512 [00:10<00:00, 41.02it/s, est. speed input: 46908.36 toks/s, output: 45.81 toks/s]
Processed prompts:  98%|█████████▊| 502/512 [00:10<00:00, 40.89it/s, est. speed input: 46812.43 toks/s, output: 45.72 toks/s]
Processed prompts: 100%|█████████▉| 510/512 [00:11<00:00, 42.47it/s, est. speed input: 46827.88 toks/s, output: 45.73 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:11<00:00, 42.47it/s, est. speed input: 47010.34 toks/s, output: 45.91 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:11<00:00, 45.91it/s, est. speed input: 47010.34 toks/s, output: 45.91 toks/s]
[rank0]:[W126 11:16:16.790752628 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 56.5s

测试结果:
  Requests/s:   40.93
  Tokens/s:     41951.45
  Total Reqs:   512
  Elapsed:      12.51s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     41910.52

============================================================
[5/8] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 11:16:31 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1272805) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1272805) WARNING 01-26 11:16:49 [backends.py:609] Failed to read file <frozen os>
Throughput: 42.48 requests/s, 43544.39 total tokens/s, 42.48 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-26 11:16:30] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:16:31] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:16:31] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 11:16:31] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:16:31] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:16:31] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:16:31] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:16:31] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:16:31] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:16:31] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:16:31] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:16:31] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:16:31] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:16:31] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 11:16:38] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:16:38] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:16:38] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 11:16:38] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:16:38] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:16:38] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:16:38] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:16:38] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:16:38] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:16:38] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:16:38] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:16:38] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:16:38] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:16:38] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1272805) [2026-01-26 11:16:40] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1272805) [2026-01-26 11:16:40] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1272805) [2026-01-26 11:16:40] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1272805) [2026-01-26 11:16:40] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=1272805) [2026-01-26 11:16:40] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=1272805) [2026-01-26 11:16:40] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=1272805) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1272805) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.05it/s]
(EngineCore_DP0 pid=1272805) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.52it/s]
(EngineCore_DP0 pid=1272805) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.58it/s]
(EngineCore_DP0 pid=1272805) 
(EngineCore_DP0 pid=1272805) [2026-01-26 11:16:42] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 4800] -> 1D uint8
(EngineCore_DP0 pid=1272805) [2026-01-26 11:16:42] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 13824000 bytes
(EngineCore_DP0 pid=1272805) [2026-01-26 11:16:42] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 4800] -> 1D uint8
(EngineCore_DP0 pid=1272805) [2026-01-26 11:16:42] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 10752000 bytes
(EngineCore_DP0 pid=1272805) [2026-01-26 11:16:42] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 4800] -> 1D uint8
(EngineCore_DP0 pid=1272805) [2026-01-26 11:16:42] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 113664000 bytes
(EngineCore_DP0 pid=1272805) [2026-01-26 11:16:42] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 25280] -> 1D uint8
(EngineCore_DP0 pid=1272805) [2026-01-26 11:16:42] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 56627200 bytes
(EngineCore_DP0 pid=1272805) 2026-01-26 11:16:59,525 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1272805) 2026-01-26 11:16:59,549 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1272805) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 1/5 [00:00<00:01,  3.15it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00,  5.08it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 3/5 [00:00<00:00,  3.02it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:01<00:00,  4.67it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:01<00:00,  4.27it/s]
(EngineCore_DP0 pid=1272805) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 1/4 [00:00<00:00,  9.93it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 17.20it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 16.30it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 20/1024 [00:00<00:05, 197.18it/s]
Adding requests:   6%|▌         | 61/1024 [00:00<00:03, 317.33it/s]
Adding requests:  10%|▉         | 98/1024 [00:00<00:02, 337.59it/s]
Adding requests:  13%|█▎        | 136/1024 [00:00<00:02, 352.08it/s]
Adding requests:  17%|█▋        | 176/1024 [00:00<00:02, 365.61it/s]
Adding requests:  21%|██        | 217/1024 [00:00<00:02, 379.07it/s]
Adding requests:  25%|██▍       | 255/1024 [00:00<00:02, 378.39it/s]
Adding requests:  29%|██▉       | 295/1024 [00:00<00:01, 383.12it/s]
Adding requests:  33%|███▎      | 337/1024 [00:00<00:01, 391.40it/s]
Adding requests:  37%|███▋      | 377/1024 [00:01<00:01, 393.58it/s]
Adding requests:  41%|████      | 419/1024 [00:01<00:01, 400.63it/s]
Adding requests:  45%|████▍     | 460/1024 [00:01<00:01, 395.68it/s]
Adding requests:  49%|████▉     | 503/1024 [00:01<00:01, 403.57it/s]
Adding requests:  53%|█████▎    | 544/1024 [00:01<00:01, 404.42it/s]
Adding requests:  57%|█████▋    | 585/1024 [00:01<00:01, 398.40it/s]
Adding requests:  61%|██████    | 625/1024 [00:01<00:01, 394.54it/s]
Adding requests:  65%|██████▍   | 665/1024 [00:01<00:00, 387.98it/s]
Adding requests:  69%|██████▉   | 705/1024 [00:01<00:00, 390.62it/s]
Adding requests:  73%|███████▎  | 745/1024 [00:01<00:00, 384.80it/s]
Adding requests:  77%|███████▋  | 785/1024 [00:02<00:00, 387.58it/s]
Adding requests:  80%|████████  | 824/1024 [00:02<00:00, 387.19it/s]
Adding requests:  84%|████████▍ | 864/1024 [00:02<00:00, 389.72it/s]
Adding requests:  88%|████████▊ | 906/1024 [00:02<00:00, 396.78it/s]
Adding requests:  92%|█████████▏| 946/1024 [00:02<00:00, 385.50it/s]
Adding requests:  96%|█████████▋| 986/1024 [00:02<00:00, 387.11it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 383.17it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  11%|█         | 114/1024 [00:00<00:01, 528.23it/s, est. speed input: 540969.71 toks/s, output: 528.25 toks/s]
Processed prompts:  16%|█▋        | 167/1024 [00:01<00:08, 103.71it/s, est. speed input: 127116.77 toks/s, output: 124.14 toks/s]
Processed prompts:  19%|█▉        | 192/1024 [00:01<00:10, 80.76it/s, est. speed input: 102965.82 toks/s, output: 100.55 toks/s] 
Processed prompts:  20%|██        | 208/1024 [00:02<00:11, 71.71it/s, est. speed input: 94171.99 toks/s, output: 91.96 toks/s]  
Processed prompts:  21%|██▏       | 220/1024 [00:02<00:13, 60.28it/s, est. speed input: 85227.30 toks/s, output: 83.23 toks/s]
Processed prompts:  22%|██▏       | 229/1024 [00:02<00:13, 58.27it/s, est. speed input: 82854.97 toks/s, output: 80.91 toks/s]
Processed prompts:  23%|██▎       | 237/1024 [00:03<00:14, 55.37it/s, est. speed input: 80469.67 toks/s, output: 78.58 toks/s]
Processed prompts:  24%|██▍       | 244/1024 [00:03<00:15, 51.46it/s, est. speed input: 77987.21 toks/s, output: 76.16 toks/s]
Processed prompts:  24%|██▍       | 250/1024 [00:03<00:16, 46.80it/s, est. speed input: 75456.10 toks/s, output: 73.69 toks/s]
Processed prompts:  25%|██▌       | 258/1024 [00:03<00:16, 45.50it/s, est. speed input: 73704.97 toks/s, output: 71.98 toks/s]
Processed prompts:  26%|██▌       | 266/1024 [00:03<00:16, 44.64it/s, est. speed input: 72176.24 toks/s, output: 70.48 toks/s]
Processed prompts:  27%|██▋       | 274/1024 [00:03<00:17, 44.06it/s, est. speed input: 70816.66 toks/s, output: 69.16 toks/s]
Processed prompts:  28%|██▊       | 282/1024 [00:04<00:16, 43.74it/s, est. speed input: 69608.99 toks/s, output: 67.98 toks/s]
Processed prompts:  28%|██▊       | 290/1024 [00:04<00:16, 43.39it/s, est. speed input: 68477.77 toks/s, output: 66.87 toks/s]
Processed prompts:  29%|██▉       | 298/1024 [00:04<00:16, 42.97it/s, est. speed input: 67402.99 toks/s, output: 65.82 toks/s]
Processed prompts:  30%|██▉       | 306/1024 [00:04<00:16, 44.32it/s, est. speed input: 66755.06 toks/s, output: 65.19 toks/s]
Processed prompts:  31%|███       | 314/1024 [00:04<00:16, 43.80it/s, est. speed input: 65865.67 toks/s, output: 64.32 toks/s]
Processed prompts:  31%|███▏      | 322/1024 [00:05<00:16, 43.49it/s, est. speed input: 65050.05 toks/s, output: 63.52 toks/s]
Processed prompts:  32%|███▏      | 330/1024 [00:05<00:16, 43.14it/s, est. speed input: 64271.26 toks/s, output: 62.76 toks/s]
Processed prompts:  33%|███▎      | 338/1024 [00:05<00:15, 42.92it/s, est. speed input: 63549.96 toks/s, output: 62.06 toks/s]
Processed prompts:  34%|███▍      | 346/1024 [00:05<00:15, 42.63it/s, est. speed input: 62853.50 toks/s, output: 61.38 toks/s]
Processed prompts:  35%|███▍      | 354/1024 [00:05<00:15, 42.65it/s, est. speed input: 62237.11 toks/s, output: 60.78 toks/s]
Processed prompts:  35%|███▌      | 362/1024 [00:06<00:15, 42.76it/s, est. speed input: 61674.34 toks/s, output: 60.23 toks/s]
Processed prompts:  36%|███▌      | 370/1024 [00:06<00:15, 42.78it/s, est. speed input: 61138.06 toks/s, output: 59.70 toks/s]
Processed prompts:  37%|███▋      | 378/1024 [00:06<00:15, 42.70it/s, est. speed input: 60619.58 toks/s, output: 59.20 toks/s]
Processed prompts:  38%|███▊      | 386/1024 [00:06<00:14, 42.64it/s, est. speed input: 60129.78 toks/s, output: 58.72 toks/s]
Processed prompts:  38%|███▊      | 394/1024 [00:06<00:14, 42.53it/s, est. speed input: 59657.71 toks/s, output: 58.26 toks/s]
Processed prompts:  39%|███▉      | 402/1024 [00:06<00:14, 42.60it/s, est. speed input: 59230.33 toks/s, output: 57.84 toks/s]
Processed prompts:  40%|████      | 410/1024 [00:07<00:14, 42.72it/s, est. speed input: 58834.62 toks/s, output: 57.46 toks/s]
Processed prompts:  41%|████      | 418/1024 [00:07<00:14, 42.69it/s, est. speed input: 58444.84 toks/s, output: 57.07 toks/s]
Processed prompts:  42%|████▏     | 426/1024 [00:07<00:14, 42.50it/s, est. speed input: 58055.71 toks/s, output: 56.69 toks/s]
Processed prompts:  42%|████▏     | 434/1024 [00:07<00:13, 43.93it/s, est. speed input: 57854.24 toks/s, output: 56.50 toks/s]
Processed prompts:  43%|████▎     | 442/1024 [00:07<00:13, 43.42it/s, est. speed input: 57503.96 toks/s, output: 56.16 toks/s]
Processed prompts:  44%|████▍     | 450/1024 [00:08<00:13, 43.28it/s, est. speed input: 57191.91 toks/s, output: 55.85 toks/s]
Processed prompts:  45%|████▍     | 458/1024 [00:08<00:13, 43.06it/s, est. speed input: 56880.96 toks/s, output: 55.55 toks/s]
Processed prompts:  46%|████▌     | 466/1024 [00:08<00:13, 42.80it/s, est. speed input: 56574.46 toks/s, output: 55.25 toks/s]
Processed prompts:  46%|████▋     | 474/1024 [00:08<00:12, 42.59it/s, est. speed input: 56278.23 toks/s, output: 54.96 toks/s]
Processed prompts:  47%|████▋     | 482/1024 [00:08<00:12, 42.49it/s, est. speed input: 55998.34 toks/s, output: 54.69 toks/s]
Processed prompts:  48%|████▊     | 490/1024 [00:09<00:12, 42.63it/s, est. speed input: 55750.05 toks/s, output: 54.44 toks/s]
Processed prompts:  49%|████▊     | 498/1024 [00:09<00:12, 42.51it/s, est. speed input: 55491.98 toks/s, output: 54.19 toks/s]
Processed prompts:  49%|████▉     | 506/1024 [00:09<00:12, 42.53it/s, est. speed input: 55254.02 toks/s, output: 53.96 toks/s]
Processed prompts:  50%|█████     | 514/1024 [00:09<00:12, 42.37it/s, est. speed input: 55010.38 toks/s, output: 53.72 toks/s]
Processed prompts:  51%|█████     | 522/1024 [00:09<00:11, 42.31it/s, est. speed input: 54780.50 toks/s, output: 53.50 toks/s]
Processed prompts:  52%|█████▏    | 530/1024 [00:09<00:11, 42.39it/s, est. speed input: 54568.70 toks/s, output: 53.29 toks/s]
Processed prompts:  53%|█████▎    | 538/1024 [00:10<00:11, 42.43it/s, est. speed input: 54364.61 toks/s, output: 53.09 toks/s]
Processed prompts:  53%|█████▎    | 546/1024 [00:10<00:11, 42.31it/s, est. speed input: 54155.85 toks/s, output: 52.89 toks/s]
Processed prompts:  54%|█████▍    | 554/1024 [00:10<00:11, 42.41it/s, est. speed input: 53968.67 toks/s, output: 52.70 toks/s]
Processed prompts:  55%|█████▍    | 562/1024 [00:10<00:10, 42.27it/s, est. speed input: 53772.29 toks/s, output: 52.51 toks/s]
Processed prompts:  56%|█████▌    | 570/1024 [00:10<00:10, 42.32it/s, est. speed input: 53593.86 toks/s, output: 52.34 toks/s]
Processed prompts:  56%|█████▋    | 578/1024 [00:11<00:10, 42.42it/s, est. speed input: 53425.91 toks/s, output: 52.17 toks/s]
Processed prompts:  57%|█████▋    | 586/1024 [00:11<00:10, 42.39it/s, est. speed input: 53256.77 toks/s, output: 52.01 toks/s]
Processed prompts:  58%|█████▊    | 594/1024 [00:11<00:10, 42.35it/s, est. speed input: 53091.91 toks/s, output: 51.85 toks/s]
Processed prompts:  59%|█████▉    | 602/1024 [00:11<00:09, 42.40it/s, est. speed input: 52937.08 toks/s, output: 51.70 toks/s]
Processed prompts:  60%|█████▉    | 610/1024 [00:11<00:09, 42.37it/s, est. speed input: 52783.16 toks/s, output: 51.55 toks/s]
Processed prompts:  60%|██████    | 618/1024 [00:12<00:09, 42.36it/s, est. speed input: 52635.46 toks/s, output: 51.40 toks/s]
Processed prompts:  61%|██████    | 626/1024 [00:12<00:09, 42.37it/s, est. speed input: 52492.63 toks/s, output: 51.26 toks/s]
Processed prompts:  62%|██████▏   | 634/1024 [00:12<00:09, 42.40it/s, est. speed input: 52355.72 toks/s, output: 51.13 toks/s]
Processed prompts:  63%|██████▎   | 642/1024 [00:12<00:09, 42.31it/s, est. speed input: 52216.63 toks/s, output: 50.99 toks/s]
Processed prompts:  63%|██████▎   | 650/1024 [00:12<00:08, 42.38it/s, est. speed input: 52089.28 toks/s, output: 50.87 toks/s]
Processed prompts:  64%|██████▍   | 658/1024 [00:12<00:08, 42.36it/s, est. speed input: 51961.71 toks/s, output: 50.74 toks/s]
Processed prompts:  65%|██████▌   | 666/1024 [00:13<00:08, 42.33it/s, est. speed input: 51836.76 toks/s, output: 50.62 toks/s]
Processed prompts:  66%|██████▌   | 674/1024 [00:13<00:08, 42.35it/s, est. speed input: 51717.38 toks/s, output: 50.51 toks/s]
Processed prompts:  67%|██████▋   | 682/1024 [00:13<00:08, 42.26it/s, est. speed input: 51596.13 toks/s, output: 50.39 toks/s]
Processed prompts:  67%|██████▋   | 690/1024 [00:13<00:07, 42.24it/s, est. speed input: 51479.73 toks/s, output: 50.27 toks/s]
Processed prompts:  68%|██████▊   | 698/1024 [00:13<00:07, 42.32it/s, est. speed input: 51372.20 toks/s, output: 50.17 toks/s]
Processed prompts:  69%|██████▉   | 706/1024 [00:14<00:07, 42.17it/s, est. speed input: 51256.45 toks/s, output: 50.05 toks/s]
Processed prompts:  70%|██████▉   | 714/1024 [00:14<00:07, 42.27it/s, est. speed input: 51154.59 toks/s, output: 49.96 toks/s]
Processed prompts:  71%|███████   | 722/1024 [00:14<00:07, 42.25it/s, est. speed input: 51050.80 toks/s, output: 49.85 toks/s]
Processed prompts:  71%|███████▏  | 730/1024 [00:14<00:06, 42.25it/s, est. speed input: 50950.38 toks/s, output: 49.76 toks/s]
Processed prompts:  72%|███████▏  | 738/1024 [00:14<00:06, 42.47it/s, est. speed input: 50863.32 toks/s, output: 49.67 toks/s]
Processed prompts:  73%|███████▎  | 746/1024 [00:15<00:06, 42.35it/s, est. speed input: 50765.04 toks/s, output: 49.58 toks/s]
Processed prompts:  74%|███████▎  | 754/1024 [00:15<00:06, 42.32it/s, est. speed input: 50671.70 toks/s, output: 49.48 toks/s]
Processed prompts:  74%|███████▍  | 762/1024 [00:15<00:06, 42.30it/s, est. speed input: 50580.91 toks/s, output: 49.40 toks/s]
Processed prompts:  75%|███████▌  | 770/1024 [00:15<00:06, 42.20it/s, est. speed input: 50488.16 toks/s, output: 49.30 toks/s]
Processed prompts:  76%|███████▌  | 778/1024 [00:15<00:05, 42.21it/s, est. speed input: 50401.08 toks/s, output: 49.22 toks/s]
Processed prompts:  77%|███████▋  | 786/1024 [00:15<00:05, 43.80it/s, est. speed input: 50388.61 toks/s, output: 49.21 toks/s]
Processed prompts:  78%|███████▊  | 794/1024 [00:16<00:05, 43.18it/s, est. speed input: 50298.78 toks/s, output: 49.12 toks/s]
Processed prompts:  78%|███████▊  | 802/1024 [00:16<00:05, 42.85it/s, est. speed input: 50215.38 toks/s, output: 49.04 toks/s]
Processed prompts:  79%|███████▉  | 810/1024 [00:16<00:05, 42.60it/s, est. speed input: 50132.51 toks/s, output: 48.96 toks/s]
Processed prompts:  80%|███████▉  | 818/1024 [00:16<00:04, 42.49it/s, est. speed input: 50054.86 toks/s, output: 48.88 toks/s]
Processed prompts:  81%|████████  | 826/1024 [00:16<00:04, 42.47it/s, est. speed input: 49981.05 toks/s, output: 48.81 toks/s]
Processed prompts:  81%|████████▏ | 834/1024 [00:17<00:04, 42.49it/s, est. speed input: 49910.23 toks/s, output: 48.74 toks/s]
Processed prompts:  82%|████████▏ | 842/1024 [00:17<00:04, 42.48it/s, est. speed input: 49840.29 toks/s, output: 48.67 toks/s]
Processed prompts:  83%|████████▎ | 850/1024 [00:17<00:04, 42.34it/s, est. speed input: 49765.94 toks/s, output: 48.60 toks/s]
Processed prompts:  84%|████████▍ | 858/1024 [00:17<00:03, 42.38it/s, est. speed input: 49699.15 toks/s, output: 48.53 toks/s]
Processed prompts:  85%|████████▍ | 866/1024 [00:17<00:03, 42.45it/s, est. speed input: 49635.42 toks/s, output: 48.47 toks/s]
Processed prompts:  85%|████████▌ | 874/1024 [00:18<00:03, 42.54it/s, est. speed input: 49574.84 toks/s, output: 48.41 toks/s]
Processed prompts:  86%|████████▌ | 882/1024 [00:18<00:03, 42.38it/s, est. speed input: 49506.17 toks/s, output: 48.35 toks/s]
Processed prompts:  87%|████████▋ | 890/1024 [00:18<00:03, 42.42it/s, est. speed input: 49445.18 toks/s, output: 48.29 toks/s]
Processed prompts:  88%|████████▊ | 898/1024 [00:18<00:02, 42.37it/s, est. speed input: 49382.38 toks/s, output: 48.22 toks/s]
Processed prompts:  88%|████████▊ | 906/1024 [00:18<00:02, 42.38it/s, est. speed input: 49322.78 toks/s, output: 48.17 toks/s]
Processed prompts:  89%|████████▉ | 914/1024 [00:18<00:02, 42.47it/s, est. speed input: 49267.27 toks/s, output: 48.11 toks/s]
Processed prompts:  90%|█████████ | 922/1024 [00:19<00:02, 42.45it/s, est. speed input: 49209.81 toks/s, output: 48.06 toks/s]
Processed prompts:  91%|█████████ | 930/1024 [00:19<00:02, 42.47it/s, est. speed input: 49154.60 toks/s, output: 48.00 toks/s]
Processed prompts:  92%|█████████▏| 938/1024 [00:19<00:02, 42.38it/s, est. speed input: 49096.76 toks/s, output: 47.95 toks/s]
Processed prompts:  92%|█████████▏| 946/1024 [00:19<00:01, 42.45it/s, est. speed input: 49044.77 toks/s, output: 47.90 toks/s]
Processed prompts:  93%|█████████▎| 954/1024 [00:19<00:01, 42.41it/s, est. speed input: 48990.65 toks/s, output: 47.84 toks/s]
Processed prompts:  94%|█████████▍| 962/1024 [00:20<00:01, 42.43it/s, est. speed input: 48939.23 toks/s, output: 47.79 toks/s]
Processed prompts:  95%|█████████▍| 970/1024 [00:20<00:01, 42.43it/s, est. speed input: 48888.20 toks/s, output: 47.74 toks/s]
Processed prompts:  96%|█████████▌| 978/1024 [00:20<00:01, 42.47it/s, est. speed input: 48839.49 toks/s, output: 47.69 toks/s]
Processed prompts:  96%|█████████▋| 986/1024 [00:20<00:00, 42.43it/s, est. speed input: 48789.53 toks/s, output: 47.65 toks/s]
Processed prompts:  97%|█████████▋| 994/1024 [00:20<00:00, 42.40it/s, est. speed input: 48740.39 toks/s, output: 47.60 toks/s]
Processed prompts:  98%|█████████▊| 1002/1024 [00:21<00:00, 42.33it/s, est. speed input: 48690.31 toks/s, output: 47.55 toks/s]
Processed prompts:  99%|█████████▊| 1010/1024 [00:21<00:00, 42.32it/s, est. speed input: 48642.53 toks/s, output: 47.50 toks/s]
Processed prompts:  99%|█████████▉| 1018/1024 [00:21<00:00, 43.85it/s, est. speed input: 48645.66 toks/s, output: 47.51 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:21<00:00, 43.85it/s, est. speed input: 48931.40 toks/s, output: 47.78 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:21<00:00, 47.78it/s, est. speed input: 48931.40 toks/s, output: 47.78 toks/s]
[rank0]:[W126 11:17:26.871452611 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 69.7s

测试结果:
  Requests/s:   42.48
  Tokens/s:     43544.39
  Total Reqs:   1024
  Elapsed:      24.10s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     43501.91

============================================================
[6/8] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 11:17:46 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1274311) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1274311) WARNING 01-26 11:18:04 [backends.py:609] Failed to read file <frozen os>
Throughput: 43.42 requests/s, 44507.83 total tokens/s, 43.42 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-26 11:17:45] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:17:46] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:17:46] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 11:17:46] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:17:46] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:17:46] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:17:46] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:17:46] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:17:46] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:17:46] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:17:46] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:17:46] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:17:46] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:17:46] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 11:17:53] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:17:53] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:17:53] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 11:17:53] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:17:53] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:17:53] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:17:53] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:17:53] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:17:53] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:17:53] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:17:53] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:17:53] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:17:53] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:17:53] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1274311) [2026-01-26 11:17:55] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1274311) [2026-01-26 11:17:55] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1274311) [2026-01-26 11:17:55] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1274311) [2026-01-26 11:17:55] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=1274311) [2026-01-26 11:17:55] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=1274311) [2026-01-26 11:17:55] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=1274311) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1274311) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.13it/s]
(EngineCore_DP0 pid=1274311) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.53it/s]
(EngineCore_DP0 pid=1274311) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.60it/s]
(EngineCore_DP0 pid=1274311) 
(EngineCore_DP0 pid=1274311) [2026-01-26 11:17:57] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 4800] -> 1D uint8
(EngineCore_DP0 pid=1274311) [2026-01-26 11:17:58] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 13824000 bytes
(EngineCore_DP0 pid=1274311) [2026-01-26 11:17:58] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 4800] -> 1D uint8
(EngineCore_DP0 pid=1274311) [2026-01-26 11:17:58] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 10752000 bytes
(EngineCore_DP0 pid=1274311) [2026-01-26 11:17:58] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 4800] -> 1D uint8
(EngineCore_DP0 pid=1274311) [2026-01-26 11:17:58] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 113664000 bytes
(EngineCore_DP0 pid=1274311) [2026-01-26 11:17:58] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 25280] -> 1D uint8
(EngineCore_DP0 pid=1274311) [2026-01-26 11:17:58] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 56627200 bytes
(EngineCore_DP0 pid=1274311) [rank0]:W0126 11:18:09.654000 1274311 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1274311) [rank0]:W0126 11:18:09.731000 1274311 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1274311) [rank0]:W0126 11:18:10.645000 1274311 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1274311) [rank0]:W0126 11:18:10.768000 1274311 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1274311) 2026-01-26 11:18:14,651 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1274311) 2026-01-26 11:18:14,678 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1274311) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00, 13.65it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00,  5.93it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 5/7 [00:00<00:00,  6.42it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:01<00:00,  4.13it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:01<00:00,  5.52it/s]
(EngineCore_DP0 pid=1274311) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00, 19.06it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00, 11.52it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 13.15it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   1%|          | 19/2048 [00:00<00:11, 182.66it/s]
Adding requests:   3%|▎         | 57/2048 [00:00<00:06, 296.60it/s]
Adding requests:   5%|▍         | 95/2048 [00:00<00:05, 331.72it/s]
Adding requests:   6%|▋         | 133/2048 [00:00<00:05, 348.70it/s]
Adding requests:   8%|▊         | 172/2048 [00:00<00:05, 362.47it/s]
Adding requests:  10%|█         | 213/2048 [00:00<00:04, 377.36it/s]
Adding requests:  12%|█▏        | 252/2048 [00:00<00:04, 378.43it/s]
Adding requests:  14%|█▍        | 290/2048 [00:00<00:04, 374.27it/s]
Adding requests:  16%|█▌        | 331/2048 [00:00<00:04, 382.84it/s]
Adding requests:  18%|█▊        | 372/2048 [00:01<00:04, 389.66it/s]
Adding requests:  20%|██        | 413/2048 [00:01<00:04, 394.71it/s]
Adding requests:  22%|██▏       | 453/2048 [00:01<00:04, 394.76it/s]
Adding requests:  24%|██▍       | 496/2048 [00:01<00:03, 403.58it/s]
Adding requests:  26%|██▋       | 538/2048 [00:01<00:03, 407.26it/s]
Adding requests:  28%|██▊       | 579/2048 [00:01<00:03, 404.16it/s]
Adding requests:  30%|███       | 620/2048 [00:01<00:03, 393.23it/s]
Adding requests:  32%|███▏      | 660/2048 [00:01<00:03, 387.58it/s]
Adding requests:  34%|███▍      | 700/2048 [00:01<00:03, 389.87it/s]
Adding requests:  36%|███▌      | 740/2048 [00:01<00:03, 384.91it/s]
Adding requests:  38%|███▊      | 779/2048 [00:02<00:03, 377.18it/s]
Adding requests:  40%|███▉      | 818/2048 [00:02<00:03, 379.13it/s]
Adding requests:  42%|████▏     | 859/2048 [00:02<00:03, 387.80it/s]
Adding requests:  44%|████▍     | 899/2048 [00:02<00:02, 390.79it/s]
Adding requests:  46%|████▌     | 939/2048 [00:02<00:02, 384.56it/s]
Adding requests:  48%|████▊     | 978/2048 [00:02<00:02, 385.52it/s]
Adding requests:  50%|████▉     | 1017/2048 [00:02<00:02, 381.25it/s]
Adding requests:  52%|█████▏    | 1056/2048 [00:02<00:02, 378.61it/s]
Adding requests:  53%|█████▎    | 1094/2048 [00:02<00:02, 378.60it/s]
Adding requests:  55%|█████▌    | 1135/2048 [00:02<00:02, 386.04it/s]
Adding requests:  57%|█████▋    | 1174/2048 [00:03<00:02, 383.52it/s]
Adding requests:  59%|█████▉    | 1214/2048 [00:03<00:02, 385.78it/s]
Adding requests:  61%|██████    | 1254/2048 [00:03<00:02, 388.63it/s]
Adding requests:  63%|██████▎   | 1293/2048 [00:03<00:01, 380.38it/s]
Adding requests:  65%|██████▌   | 1333/2048 [00:03<00:01, 382.98it/s]
Adding requests:  67%|██████▋   | 1374/2048 [00:03<00:01, 389.14it/s]
Adding requests:  69%|██████▉   | 1413/2048 [00:03<00:01, 385.53it/s]
Adding requests:  71%|███████   | 1452/2048 [00:03<00:01, 385.70it/s]
Adding requests:  73%|███████▎  | 1493/2048 [00:03<00:01, 391.87it/s]
Adding requests:  75%|███████▍  | 1533/2048 [00:04<00:01, 387.83it/s]
Adding requests:  77%|███████▋  | 1572/2048 [00:04<00:01, 381.41it/s]
Adding requests:  79%|███████▊  | 1611/2048 [00:04<00:01, 382.68it/s]
Adding requests:  81%|████████  | 1650/2048 [00:04<00:01, 372.95it/s]
Adding requests:  82%|████████▏ | 1688/2048 [00:04<00:00, 371.13it/s]
Adding requests:  84%|████████▍ | 1728/2048 [00:04<00:00, 378.16it/s]
Adding requests:  86%|████████▋ | 1769/2048 [00:04<00:00, 386.50it/s]
Adding requests:  88%|████████▊ | 1808/2048 [00:04<00:00, 380.70it/s]
Adding requests:  90%|█████████ | 1848/2048 [00:04<00:00, 383.49it/s]
Adding requests:  92%|█████████▏| 1888/2048 [00:04<00:00, 387.31it/s]
Adding requests:  94%|█████████▍| 1927/2048 [00:05<00:00, 382.55it/s]
Adding requests:  96%|█████████▌| 1966/2048 [00:05<00:00, 384.72it/s]
Adding requests:  98%|█████████▊| 2005/2048 [00:05<00:00, 381.88it/s]
Adding requests: 100%|█████████▉| 2044/2048 [00:05<00:00, 377.11it/s]
Adding requests: 100%|██████████| 2048/2048 [00:05<00:00, 381.22it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  11%|█         | 226/2048 [00:00<00:02, 702.24it/s, est. speed input: 719196.33 toks/s, output: 702.27 toks/s]
Processed prompts:  15%|█▍        | 297/2048 [00:01<00:12, 135.33it/s, est. speed input: 169884.41 toks/s, output: 165.90 toks/s]
Processed prompts:  16%|█▌        | 329/2048 [00:02<00:16, 101.28it/s, est. speed input: 134505.85 toks/s, output: 131.35 toks/s]
Processed prompts:  17%|█▋        | 349/2048 [00:02<00:18, 91.07it/s, est. speed input: 124373.04 toks/s, output: 121.46 toks/s] 
Processed prompts:  18%|█▊        | 363/2048 [00:03<00:21, 78.48it/s, est. speed input: 114630.69 toks/s, output: 111.94 toks/s]
Processed prompts:  18%|█▊        | 374/2048 [00:03<00:25, 66.33it/s, est. speed input: 106139.24 toks/s, output: 103.65 toks/s]
Processed prompts:  19%|█▉        | 386/2048 [00:03<00:28, 57.34it/s, est. speed input: 99379.68 toks/s, output: 97.05 toks/s]  
Processed prompts:  20%|█▉        | 402/2048 [00:04<00:30, 53.51it/s, est. speed input: 94718.12 toks/s, output: 92.50 toks/s]
Processed prompts:  20%|██        | 418/2048 [00:04<00:32, 50.64it/s, est. speed input: 90779.12 toks/s, output: 88.65 toks/s]
Processed prompts:  21%|██        | 434/2048 [00:05<00:32, 49.30it/s, est. speed input: 87752.61 toks/s, output: 85.70 toks/s]
Processed prompts:  22%|██▏       | 450/2048 [00:05<00:33, 47.50it/s, est. speed input: 84784.30 toks/s, output: 82.80 toks/s]
Processed prompts:  23%|██▎       | 466/2048 [00:05<00:34, 46.29it/s, est. speed input: 82223.97 toks/s, output: 80.30 toks/s]
Processed prompts:  24%|██▎       | 482/2048 [00:06<00:34, 45.26it/s, est. speed input: 79904.18 toks/s, output: 78.03 toks/s]
Processed prompts:  24%|██▍       | 498/2048 [00:06<00:34, 44.77it/s, est. speed input: 77928.37 toks/s, output: 76.10 toks/s]
Processed prompts:  25%|██▌       | 514/2048 [00:06<00:34, 44.30it/s, est. speed input: 76124.91 toks/s, output: 74.34 toks/s]
Processed prompts:  26%|██▌       | 530/2048 [00:07<00:34, 43.99it/s, est. speed input: 74509.38 toks/s, output: 72.76 toks/s]
Processed prompts:  27%|██▋       | 546/2048 [00:07<00:34, 43.82it/s, est. speed input: 73060.71 toks/s, output: 71.35 toks/s]
Processed prompts:  27%|██▋       | 562/2048 [00:08<00:34, 43.63it/s, est. speed input: 71728.34 toks/s, output: 70.05 toks/s]
Processed prompts:  28%|██▊       | 578/2048 [00:08<00:33, 43.50it/s, est. speed input: 70515.84 toks/s, output: 68.86 toks/s]
Processed prompts:  29%|██▉       | 594/2048 [00:08<00:33, 43.44it/s, est. speed input: 69410.93 toks/s, output: 67.78 toks/s]
Processed prompts:  30%|██▉       | 610/2048 [00:09<00:33, 43.38it/s, est. speed input: 68392.92 toks/s, output: 66.79 toks/s]
Processed prompts:  31%|███       | 626/2048 [00:09<00:32, 43.37it/s, est. speed input: 67460.53 toks/s, output: 65.88 toks/s]
Processed prompts:  31%|███▏      | 642/2048 [00:09<00:32, 43.38it/s, est. speed input: 66600.81 toks/s, output: 65.04 toks/s]
Processed prompts:  32%|███▏      | 658/2048 [00:10<00:32, 43.31it/s, est. speed input: 65788.64 toks/s, output: 64.25 toks/s]
Processed prompts:  33%|███▎      | 674/2048 [00:10<00:31, 43.36it/s, est. speed input: 65052.16 toks/s, output: 63.53 toks/s]
Processed prompts:  34%|███▎      | 690/2048 [00:10<00:31, 43.31it/s, est. speed input: 64349.26 toks/s, output: 62.84 toks/s]
Processed prompts:  34%|███▍      | 706/2048 [00:11<00:30, 43.40it/s, est. speed input: 63712.09 toks/s, output: 62.22 toks/s]
Processed prompts:  35%|███▌      | 722/2048 [00:11<00:30, 43.47it/s, est. speed input: 63116.42 toks/s, output: 61.64 toks/s]
Processed prompts:  36%|███▌      | 738/2048 [00:12<00:30, 43.43it/s, est. speed input: 62544.36 toks/s, output: 61.08 toks/s]
Processed prompts:  37%|███▋      | 754/2048 [00:12<00:29, 43.46it/s, est. speed input: 62013.10 toks/s, output: 60.56 toks/s]
Processed prompts:  38%|███▊      | 770/2048 [00:12<00:29, 43.45it/s, est. speed input: 61509.25 toks/s, output: 60.07 toks/s]
Processed prompts:  38%|███▊      | 786/2048 [00:13<00:28, 44.22it/s, est. speed input: 61133.03 toks/s, output: 59.70 toks/s]
Processed prompts:  39%|███▉      | 802/2048 [00:13<00:28, 43.91it/s, est. speed input: 60671.05 toks/s, output: 59.25 toks/s]
Processed prompts:  40%|███▉      | 818/2048 [00:13<00:28, 43.83it/s, est. speed input: 60249.29 toks/s, output: 58.84 toks/s]
Processed prompts:  41%|████      | 834/2048 [00:14<00:27, 43.74it/s, est. speed input: 59845.49 toks/s, output: 58.44 toks/s]
Processed prompts:  42%|████▏     | 850/2048 [00:14<00:27, 43.56it/s, est. speed input: 59449.14 toks/s, output: 58.06 toks/s]
Processed prompts:  42%|████▏     | 866/2048 [00:15<00:27, 43.59it/s, est. speed input: 59089.45 toks/s, output: 57.70 toks/s]
Processed prompts:  43%|████▎     | 882/2048 [00:15<00:26, 43.53it/s, est. speed input: 58737.73 toks/s, output: 57.36 toks/s]
Processed prompts:  44%|████▍     | 898/2048 [00:15<00:26, 43.49it/s, est. speed input: 58403.13 toks/s, output: 57.03 toks/s]
Processed prompts:  45%|████▍     | 914/2048 [00:16<00:26, 43.46it/s, est. speed input: 58082.74 toks/s, output: 56.72 toks/s]
Processed prompts:  45%|████▌     | 930/2048 [00:16<00:25, 43.41it/s, est. speed input: 57774.87 toks/s, output: 56.42 toks/s]
Processed prompts:  46%|████▌     | 946/2048 [00:16<00:25, 43.43it/s, est. speed input: 57485.18 toks/s, output: 56.14 toks/s]
Processed prompts:  47%|████▋     | 962/2048 [00:17<00:24, 43.46it/s, est. speed input: 57209.39 toks/s, output: 55.87 toks/s]
Processed prompts:  48%|████▊     | 978/2048 [00:17<00:24, 43.36it/s, est. speed input: 56934.63 toks/s, output: 55.60 toks/s]
Processed prompts:  49%|████▊     | 994/2048 [00:17<00:24, 43.36it/s, est. speed input: 56677.27 toks/s, output: 55.35 toks/s]
Processed prompts:  49%|████▉     | 1010/2048 [00:18<00:23, 43.40it/s, est. speed input: 56433.27 toks/s, output: 55.11 toks/s]
Processed prompts:  50%|█████     | 1026/2048 [00:18<00:23, 43.37it/s, est. speed input: 56194.08 toks/s, output: 54.88 toks/s]
Processed prompts:  51%|█████     | 1042/2048 [00:19<00:23, 43.38it/s, est. speed input: 55966.91 toks/s, output: 54.66 toks/s]
Processed prompts:  52%|█████▏    | 1058/2048 [00:19<00:22, 43.33it/s, est. speed input: 55743.87 toks/s, output: 54.44 toks/s]
Processed prompts:  52%|█████▏    | 1074/2048 [00:19<00:22, 43.32it/s, est. speed input: 55531.09 toks/s, output: 54.23 toks/s]
Processed prompts:  53%|█████▎    | 1090/2048 [00:20<00:22, 43.32it/s, est. speed input: 55326.41 toks/s, output: 54.03 toks/s]
Processed prompts:  54%|█████▍    | 1106/2048 [00:20<00:21, 43.35it/s, est. speed input: 55131.24 toks/s, output: 53.84 toks/s]
Processed prompts:  55%|█████▍    | 1122/2048 [00:20<00:21, 43.37it/s, est. speed input: 54943.03 toks/s, output: 53.66 toks/s]
Processed prompts:  56%|█████▌    | 1138/2048 [00:21<00:21, 43.32it/s, est. speed input: 54757.09 toks/s, output: 53.47 toks/s]
Processed prompts:  56%|█████▋    | 1154/2048 [00:21<00:20, 43.34it/s, est. speed input: 54581.29 toks/s, output: 53.30 toks/s]
Processed prompts:  57%|█████▋    | 1170/2048 [00:22<00:20, 43.30it/s, est. speed input: 54407.53 toks/s, output: 53.13 toks/s]
Processed prompts:  58%|█████▊    | 1186/2048 [00:22<00:19, 43.32it/s, est. speed input: 54242.65 toks/s, output: 52.97 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [00:22<00:19, 44.06it/s, est. speed input: 54131.62 toks/s, output: 52.86 toks/s]
Processed prompts:  59%|█████▉    | 1218/2048 [00:23<00:18, 43.78it/s, est. speed input: 53971.79 toks/s, output: 52.71 toks/s]
Processed prompts:  60%|██████    | 1234/2048 [00:23<00:18, 44.43it/s, est. speed input: 53870.11 toks/s, output: 52.61 toks/s]
Processed prompts:  61%|██████    | 1250/2048 [00:23<00:18, 44.08it/s, est. speed input: 53722.42 toks/s, output: 52.46 toks/s]
Processed prompts:  62%|██████▏   | 1266/2048 [00:24<00:17, 43.78it/s, est. speed input: 53574.92 toks/s, output: 52.32 toks/s]
Processed prompts:  63%|██████▎   | 1282/2048 [00:24<00:17, 43.58it/s, est. speed input: 53432.75 toks/s, output: 52.18 toks/s]
Processed prompts:  63%|██████▎   | 1298/2048 [00:24<00:17, 43.45it/s, est. speed input: 53295.36 toks/s, output: 52.05 toks/s]
Processed prompts:  64%|██████▍   | 1314/2048 [00:25<00:16, 43.35it/s, est. speed input: 53161.08 toks/s, output: 51.92 toks/s]
Processed prompts:  65%|██████▍   | 1330/2048 [00:25<00:16, 44.00it/s, est. speed input: 53072.72 toks/s, output: 51.83 toks/s]
Processed prompts:  66%|██████▌   | 1346/2048 [00:26<00:16, 43.78it/s, est. speed input: 52948.33 toks/s, output: 51.71 toks/s]
Processed prompts:  67%|██████▋   | 1362/2048 [00:26<00:15, 43.59it/s, est. speed input: 52825.08 toks/s, output: 51.59 toks/s]
Processed prompts:  67%|██████▋   | 1378/2048 [00:26<00:15, 43.47it/s, est. speed input: 52706.16 toks/s, output: 51.47 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [00:27<00:15, 43.35it/s, est. speed input: 52588.58 toks/s, output: 51.36 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [00:27<00:14, 43.25it/s, est. speed input: 52473.28 toks/s, output: 51.24 toks/s]
Processed prompts:  70%|██████▉   | 1426/2048 [00:27<00:14, 43.25it/s, est. speed input: 52364.80 toks/s, output: 51.14 toks/s]
Processed prompts:  70%|███████   | 1442/2048 [00:28<00:14, 43.23it/s, est. speed input: 52257.86 toks/s, output: 51.03 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [00:28<00:13, 43.95it/s, est. speed input: 52191.54 toks/s, output: 50.97 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [00:28<00:13, 43.76it/s, est. speed input: 52091.45 toks/s, output: 50.87 toks/s]
Processed prompts:  73%|███████▎  | 1490/2048 [00:29<00:12, 43.57it/s, est. speed input: 51991.61 toks/s, output: 50.77 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [00:29<00:12, 43.47it/s, est. speed input: 51895.76 toks/s, output: 50.68 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [00:30<00:11, 44.17it/s, est. speed input: 51838.73 toks/s, output: 50.62 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [00:30<00:11, 43.83it/s, est. speed input: 51744.13 toks/s, output: 50.53 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [00:30<00:11, 44.39it/s, est. speed input: 51688.49 toks/s, output: 50.48 toks/s]
Processed prompts:  77%|███████▋  | 1570/2048 [00:31<00:10, 44.01it/s, est. speed input: 51599.24 toks/s, output: 50.39 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [00:31<00:10, 43.79it/s, est. speed input: 51513.98 toks/s, output: 50.31 toks/s]
Processed prompts:  78%|███████▊  | 1602/2048 [00:31<00:10, 43.65it/s, est. speed input: 51431.06 toks/s, output: 50.23 toks/s]
Processed prompts:  79%|███████▉  | 1618/2048 [00:32<00:09, 44.23it/s, est. speed input: 51380.35 toks/s, output: 50.18 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [00:32<00:09, 43.88it/s, est. speed input: 51297.68 toks/s, output: 50.10 toks/s]
Processed prompts:  81%|████████  | 1650/2048 [00:32<00:09, 43.66it/s, est. speed input: 51217.66 toks/s, output: 50.02 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [00:33<00:08, 43.44it/s, est. speed input: 51136.58 toks/s, output: 49.94 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [00:33<00:08, 43.40it/s, est. speed input: 51062.20 toks/s, output: 49.87 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [00:34<00:08, 43.31it/s, est. speed input: 50986.66 toks/s, output: 49.79 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [00:34<00:07, 43.31it/s, est. speed input: 50915.54 toks/s, output: 49.72 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [00:34<00:07, 44.00it/s, est. speed input: 50874.12 toks/s, output: 49.68 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [00:35<00:06, 44.50it/s, est. speed input: 50833.70 toks/s, output: 49.64 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [00:35<00:06, 44.02it/s, est. speed input: 50761.81 toks/s, output: 49.57 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [00:35<00:06, 43.68it/s, est. speed input: 50690.97 toks/s, output: 49.50 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [00:36<00:05, 43.59it/s, est. speed input: 50627.10 toks/s, output: 49.44 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [00:36<00:05, 43.42it/s, est. speed input: 50560.58 toks/s, output: 49.38 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [00:37<00:05, 43.30it/s, est. speed input: 50495.06 toks/s, output: 49.31 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [00:37<00:04, 43.22it/s, est. speed input: 50431.13 toks/s, output: 49.25 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [00:37<00:04, 43.20it/s, est. speed input: 50369.85 toks/s, output: 49.19 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [00:38<00:04, 43.15it/s, est. speed input: 50308.46 toks/s, output: 49.13 toks/s]
Processed prompts:  92%|█████████▏| 1890/2048 [00:38<00:03, 43.91it/s, est. speed input: 50277.52 toks/s, output: 49.10 toks/s]
Processed prompts:  93%|█████████▎| 1906/2048 [00:38<00:03, 43.67it/s, est. speed input: 50219.18 toks/s, output: 49.04 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [00:39<00:02, 43.47it/s, est. speed input: 50160.36 toks/s, output: 48.98 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [00:39<00:02, 43.41it/s, est. speed input: 50105.96 toks/s, output: 48.93 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [00:39<00:02, 43.31it/s, est. speed input: 50050.15 toks/s, output: 48.88 toks/s]
Processed prompts:  96%|█████████▌| 1970/2048 [00:40<00:01, 43.25it/s, est. speed input: 49995.69 toks/s, output: 48.82 toks/s]
Processed prompts:  97%|█████████▋| 1986/2048 [00:40<00:01, 43.93it/s, est. speed input: 49967.40 toks/s, output: 48.80 toks/s]
Processed prompts:  98%|█████████▊| 2002/2048 [00:41<00:01, 43.68it/s, est. speed input: 49914.83 toks/s, output: 48.74 toks/s]
Processed prompts:  99%|█████████▊| 2018/2048 [00:41<00:00, 43.43it/s, est. speed input: 49860.52 toks/s, output: 48.69 toks/s]
Processed prompts:  99%|█████████▉| 2034/2048 [00:41<00:00, 44.27it/s, est. speed input: 49840.93 toks/s, output: 48.67 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:41<00:00, 44.27it/s, est. speed input: 50183.23 toks/s, output: 49.01 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:41<00:00, 49.01it/s, est. speed input: 50183.23 toks/s, output: 49.01 toks/s]
[rank0]:[W126 11:19:05.350536051 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 98.7s

测试结果:
  Requests/s:   43.42
  Tokens/s:     44507.83
  Total Reqs:   2048
  Elapsed:      47.16s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     44464.40

============================================================
[7/8] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 11:19:36 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1276246) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1276246) WARNING 01-26 11:19:52 [backends.py:609] Failed to read file <frozen os>
Throughput: 43.57 requests/s, 44654.70 total tokens/s, 43.57 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-26 11:19:35] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:19:35] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:19:35] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 11:19:35] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:19:35] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:19:35] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:19:35] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:19:35] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:19:35] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:19:35] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:19:35] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:19:35] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:19:36] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:19:36] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 11:19:42] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:19:42] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:19:42] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 11:19:42] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:19:42] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:19:42] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:19:42] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:19:42] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:19:42] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:19:42] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:19:42] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:19:42] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:19:42] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:19:42] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1276246) [2026-01-26 11:19:44] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1276246) [2026-01-26 11:19:44] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1276246) [2026-01-26 11:19:44] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1276246) [2026-01-26 11:19:44] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=1276246) [2026-01-26 11:19:44] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=1276246) [2026-01-26 11:19:44] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=1276246) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1276246) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.14it/s]
(EngineCore_DP0 pid=1276246) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.54it/s]
(EngineCore_DP0 pid=1276246) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.61it/s]
(EngineCore_DP0 pid=1276246) 
(EngineCore_DP0 pid=1276246) [2026-01-26 11:19:46] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 4800] -> 1D uint8
(EngineCore_DP0 pid=1276246) [2026-01-26 11:19:46] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 13824000 bytes
(EngineCore_DP0 pid=1276246) [2026-01-26 11:19:46] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 4800] -> 1D uint8
(EngineCore_DP0 pid=1276246) [2026-01-26 11:19:46] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 10752000 bytes
(EngineCore_DP0 pid=1276246) [2026-01-26 11:19:46] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 4800] -> 1D uint8
(EngineCore_DP0 pid=1276246) [2026-01-26 11:19:46] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 113664000 bytes
(EngineCore_DP0 pid=1276246) [2026-01-26 11:19:46] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 25280] -> 1D uint8
(EngineCore_DP0 pid=1276246) [2026-01-26 11:19:46] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 56627200 bytes
(EngineCore_DP0 pid=1276246) [rank0]:W0126 11:19:57.811000 1276246 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1276246) [rank0]:W0126 11:19:57.889000 1276246 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1276246) [rank0]:W0126 11:19:59.226000 1276246 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1276246) [rank0]:W0126 11:19:59.352000 1276246 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1276246) 2026-01-26 11:20:03,416 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1276246) 2026-01-26 11:20:03,488 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1276246) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▉         | 1/11 [00:00<00:03,  2.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 3/11 [00:00<00:01,  6.84it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 5/11 [00:00<00:00, 10.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▎   | 7/11 [00:00<00:00,  9.65it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 9/11 [00:00<00:00, 11.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00, 13.22it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00, 10.44it/s]
(EngineCore_DP0 pid=1276246) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 1/7 [00:00<00:01,  5.48it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:01,  3.18it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 3/7 [00:00<00:01,  3.27it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 5/7 [00:01<00:00,  5.84it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:01<00:00,  8.51it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:01<00:00,  6.23it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 25/4096 [00:00<00:16, 247.99it/s]
Adding requests:   2%|▏         | 65/4096 [00:00<00:12, 335.32it/s]
Adding requests:   2%|▏         | 101/4096 [00:00<00:11, 344.78it/s]
Adding requests:   3%|▎         | 139/4096 [00:00<00:11, 357.56it/s]
Adding requests:   4%|▍         | 178/4096 [00:00<00:10, 368.84it/s]
Adding requests:   5%|▌         | 220/4096 [00:00<00:10, 384.78it/s]
Adding requests:   6%|▋         | 259/4096 [00:00<00:10, 380.20it/s]
Adding requests:   7%|▋         | 299/4096 [00:00<00:09, 385.22it/s]
Adding requests:   8%|▊         | 340/4096 [00:00<00:09, 390.80it/s]
Adding requests:   9%|▉         | 380/4096 [00:01<00:09, 392.98it/s]
Adding requests:  10%|█         | 422/4096 [00:01<00:09, 400.07it/s]
Adding requests:  11%|█▏        | 463/4096 [00:01<00:09, 396.16it/s]
Adding requests:  12%|█▏        | 506/4096 [00:01<00:08, 404.60it/s]
Adding requests:  13%|█▎        | 548/4096 [00:01<00:08, 407.03it/s]
Adding requests:  14%|█▍        | 589/4096 [00:01<00:08, 402.81it/s]
Adding requests:  15%|█▌        | 630/4096 [00:01<00:08, 396.90it/s]
Adding requests:  16%|█▋        | 670/4096 [00:01<00:08, 386.74it/s]
Adding requests:  17%|█▋        | 711/4096 [00:01<00:08, 393.28it/s]
Adding requests:  18%|█▊        | 751/4096 [00:01<00:08, 384.90it/s]
Adding requests:  19%|█▉        | 791/4096 [00:02<00:08, 387.46it/s]
Adding requests:  20%|██        | 831/4096 [00:02<00:08, 390.91it/s]
Adding requests:  21%|██▏       | 872/4096 [00:02<00:08, 393.35it/s]
Adding requests:  22%|██▏       | 912/4096 [00:02<00:08, 391.05it/s]
Adding requests:  23%|██▎       | 952/4096 [00:02<00:08, 387.03it/s]
Adding requests:  24%|██▍       | 991/4096 [00:02<00:08, 385.95it/s]
Adding requests:  25%|██▌       | 1030/4096 [00:02<00:07, 383.58it/s]
Adding requests:  26%|██▌       | 1069/4096 [00:02<00:07, 383.11it/s]
Adding requests:  27%|██▋       | 1108/4096 [00:02<00:08, 370.79it/s]
Adding requests:  28%|██▊       | 1148/4096 [00:02<00:07, 377.61it/s]
Adding requests:  29%|██▉       | 1187/4096 [00:03<00:07, 378.44it/s]
Adding requests:  30%|██▉       | 1228/4096 [00:03<00:07, 385.05it/s]
Adding requests:  31%|███       | 1267/4096 [00:03<00:07, 384.85it/s]
Adding requests:  32%|███▏      | 1306/4096 [00:03<00:07, 381.71it/s]
Adding requests:  33%|███▎      | 1345/4096 [00:03<00:07, 383.39it/s]
Adding requests:  34%|███▍      | 1385/4096 [00:03<00:06, 387.30it/s]
Adding requests:  35%|███▍      | 1424/4096 [00:03<00:06, 383.16it/s]
Adding requests:  36%|███▌      | 1464/4096 [00:03<00:06, 386.90it/s]
Adding requests:  37%|███▋      | 1504/4096 [00:03<00:06, 390.33it/s]
Adding requests:  38%|███▊      | 1544/4096 [00:04<00:06, 389.86it/s]
Adding requests:  39%|███▊      | 1583/4096 [00:04<00:06, 381.60it/s]
Adding requests:  40%|███▉      | 1622/4096 [00:04<00:06, 376.02it/s]
Adding requests:  41%|████      | 1660/4096 [00:04<00:06, 369.71it/s]
Adding requests:  41%|████▏     | 1698/4096 [00:04<00:06, 372.29it/s]
Adding requests:  42%|████▏     | 1738/4096 [00:04<00:06, 378.24it/s]
Adding requests:  43%|████▎     | 1779/4096 [00:04<00:06, 385.76it/s]
Adding requests:  44%|████▍     | 1818/4096 [00:04<00:05, 380.84it/s]
Adding requests:  45%|████▌     | 1858/4096 [00:04<00:05, 385.88it/s]
Adding requests:  46%|████▋     | 1897/4096 [00:04<00:05, 384.26it/s]
Adding requests:  47%|████▋     | 1939/4096 [00:05<00:05, 391.49it/s]
Adding requests:  48%|████▊     | 1979/4096 [00:05<00:05, 392.16it/s]
Adding requests:  49%|████▉     | 2019/4096 [00:05<00:05, 381.71it/s]
Adding requests:  50%|█████     | 2058/4096 [00:05<00:05, 380.21it/s]
Adding requests:  51%|█████     | 2097/4096 [00:05<00:05, 377.27it/s]
Adding requests:  52%|█████▏    | 2136/4096 [00:05<00:05, 378.86it/s]
Adding requests:  53%|█████▎    | 2174/4096 [00:05<00:05, 373.46it/s]
Adding requests:  54%|█████▍    | 2212/4096 [00:05<00:05, 371.37it/s]
Adding requests:  55%|█████▍    | 2252/4096 [00:05<00:04, 378.28it/s]
Adding requests:  56%|█████▌    | 2293/4096 [00:05<00:04, 386.92it/s]
Adding requests:  57%|█████▋    | 2332/4096 [00:06<00:04, 376.08it/s]
Adding requests:  58%|█████▊    | 2373/4096 [00:06<00:04, 384.54it/s]
Adding requests:  59%|█████▉    | 2415/4096 [00:06<00:04, 391.86it/s]
Adding requests:  60%|█████▉    | 2455/4096 [00:06<00:04, 389.37it/s]
Adding requests:  61%|██████    | 2495/4096 [00:06<00:04, 391.43it/s]
Adding requests:  62%|██████▏   | 2536/4096 [00:06<00:03, 396.25it/s]
Adding requests:  63%|██████▎   | 2579/4096 [00:06<00:03, 406.02it/s]
Adding requests:  64%|██████▍   | 2620/4096 [00:06<00:03, 400.76it/s]
Adding requests:  65%|██████▍   | 2661/4096 [00:06<00:03, 390.98it/s]
Adding requests:  66%|██████▌   | 2701/4096 [00:07<00:03, 387.55it/s]
Adding requests:  67%|██████▋   | 2740/4096 [00:07<00:03, 387.68it/s]
Adding requests:  68%|██████▊   | 2782/4096 [00:07<00:03, 394.39it/s]
Adding requests:  69%|██████▉   | 2823/4096 [00:07<00:03, 398.00it/s]
Adding requests:  70%|██████▉   | 2863/4096 [00:07<00:03, 398.43it/s]
Adding requests:  71%|███████   | 2903/4096 [00:07<00:03, 396.85it/s]
Adding requests:  72%|███████▏  | 2944/4096 [00:07<00:02, 399.40it/s]
Adding requests:  73%|███████▎  | 2984/4096 [00:07<00:02, 394.78it/s]
Adding requests:  74%|███████▍  | 3026/4096 [00:07<00:02, 399.14it/s]
Adding requests:  75%|███████▍  | 3068/4096 [00:07<00:02, 402.15it/s]
Adding requests:  76%|███████▌  | 3109/4096 [00:08<00:02, 401.55it/s]
Adding requests:  77%|███████▋  | 3150/4096 [00:08<00:02, 402.70it/s]
Adding requests:  78%|███████▊  | 3191/4096 [00:08<00:02, 397.75it/s]
Adding requests:  79%|███████▉  | 3232/4096 [00:08<00:02, 398.42it/s]
Adding requests:  80%|███████▉  | 3272/4096 [00:08<00:02, 392.46it/s]
Adding requests:  81%|████████  | 3312/4096 [00:08<00:02, 381.76it/s]
Adding requests:  82%|████████▏ | 3352/4096 [00:08<00:01, 383.84it/s]
Adding requests:  83%|████████▎ | 3392/4096 [00:08<00:01, 388.06it/s]
Adding requests:  84%|████████▍ | 3432/4096 [00:08<00:01, 391.13it/s]
Adding requests:  85%|████████▍ | 3472/4096 [00:08<00:01, 393.26it/s]
Adding requests:  86%|████████▌ | 3512/4096 [00:09<00:01, 393.89it/s]
Adding requests:  87%|████████▋ | 3554/4096 [00:09<00:01, 399.75it/s]
Adding requests:  88%|████████▊ | 3594/4096 [00:09<00:01, 399.10it/s]
Adding requests:  89%|████████▊ | 3635/4096 [00:09<00:01, 399.73it/s]
Adding requests:  90%|████████▉ | 3675/4096 [00:09<00:01, 382.37it/s]
Adding requests:  91%|█████████ | 3714/4096 [00:09<00:00, 383.73it/s]
Adding requests:  92%|█████████▏| 3753/4096 [00:09<00:00, 380.75it/s]
Adding requests:  93%|█████████▎| 3792/4096 [00:09<00:00, 371.26it/s]
Adding requests:  94%|█████████▎| 3830/4096 [00:09<00:00, 369.48it/s]
Adding requests:  94%|█████████▍| 3870/4096 [00:10<00:00, 376.98it/s]
Adding requests:  95%|█████████▌| 3908/4096 [00:10<00:00, 372.60it/s]
Adding requests:  96%|█████████▋| 3946/4096 [00:10<00:00, 373.56it/s]
Adding requests:  97%|█████████▋| 3984/4096 [00:10<00:00, 373.07it/s]
Adding requests:  98%|█████████▊| 4022/4096 [00:10<00:00, 374.66it/s]
Adding requests:  99%|█████████▉| 4060/4096 [00:10<00:00, 375.42it/s]
Adding requests: 100%|██████████| 4096/4096 [00:10<00:00, 385.66it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  11%|█▏        | 465/4096 [00:00<00:05, 614.20it/s, est. speed input: 628963.31 toks/s, output: 614.21 toks/s]
Processed prompts:  13%|█▎        | 527/4096 [00:01<00:11, 306.48it/s, est. speed input: 361824.38 toks/s, output: 353.34 toks/s]
Processed prompts:  14%|█▎        | 558/4096 [00:02<00:18, 186.53it/s, est. speed input: 256598.31 toks/s, output: 250.58 toks/s]
Processed prompts:  14%|█▍        | 577/4096 [00:02<00:28, 123.26it/s, est. speed input: 199398.02 toks/s, output: 194.72 toks/s]
Processed prompts:  14%|█▍        | 593/4096 [00:03<00:40, 86.93it/s, est. speed input: 164361.63 toks/s, output: 160.51 toks/s] 
Processed prompts:  15%|█▌        | 625/4096 [00:04<00:47, 72.35it/s, est. speed input: 144647.95 toks/s, output: 141.26 toks/s]
Processed prompts:  16%|█▌        | 657/4096 [00:05<00:54, 63.10it/s, est. speed input: 130548.59 toks/s, output: 127.49 toks/s]
Processed prompts:  17%|█▋        | 689/4096 [00:05<00:59, 56.87it/s, est. speed input: 119833.57 toks/s, output: 117.02 toks/s]
Processed prompts:  18%|█▊        | 721/4096 [00:06<01:04, 52.72it/s, est. speed input: 111501.45 toks/s, output: 108.89 toks/s]
Processed prompts:  18%|█▊        | 753/4096 [00:07<01:06, 49.98it/s, est. speed input: 104879.38 toks/s, output: 102.42 toks/s]
Processed prompts:  19%|█▉        | 785/4096 [00:08<01:08, 48.48it/s, est. speed input: 99689.27 toks/s, output: 97.35 toks/s]  
Processed prompts:  20%|█▉        | 817/4096 [00:08<01:09, 47.05it/s, est. speed input: 95134.20 toks/s, output: 92.90 toks/s]
Processed prompts:  21%|██        | 849/4096 [00:09<01:10, 46.03it/s, est. speed input: 91263.29 toks/s, output: 89.12 toks/s]
Processed prompts:  22%|██▏       | 881/4096 [00:10<01:10, 45.34it/s, est. speed input: 87946.94 toks/s, output: 85.89 toks/s]
Processed prompts:  22%|██▏       | 913/4096 [00:10<01:11, 44.81it/s, est. speed input: 85056.52 toks/s, output: 83.06 toks/s]
Processed prompts:  23%|██▎       | 945/4096 [00:11<01:10, 44.42it/s, est. speed input: 82518.32 toks/s, output: 80.58 toks/s]
Processed prompts:  24%|██▍       | 977/4096 [00:12<01:10, 44.21it/s, est. speed input: 80299.90 toks/s, output: 78.42 toks/s]
Processed prompts:  25%|██▍       | 1009/4096 [00:13<01:10, 43.99it/s, est. speed input: 78305.83 toks/s, output: 76.47 toks/s]
Processed prompts:  25%|██▌       | 1041/4096 [00:13<01:09, 43.86it/s, est. speed input: 76528.00 toks/s, output: 74.73 toks/s]
Processed prompts:  26%|██▌       | 1073/4096 [00:14<01:09, 43.76it/s, est. speed input: 74924.64 toks/s, output: 73.17 toks/s]
Processed prompts:  27%|██▋       | 1105/4096 [00:15<01:08, 43.76it/s, est. speed input: 73494.11 toks/s, output: 71.77 toks/s]
Processed prompts:  28%|██▊       | 1137/4096 [00:16<01:07, 43.69it/s, est. speed input: 72175.44 toks/s, output: 70.48 toks/s]
Processed prompts:  29%|██▊       | 1169/4096 [00:16<01:07, 43.60it/s, est. speed input: 70964.29 toks/s, output: 69.30 toks/s]
Processed prompts:  29%|██▉       | 1201/4096 [00:17<01:05, 43.99it/s, est. speed input: 69952.19 toks/s, output: 68.31 toks/s]
Processed prompts:  30%|███       | 1233/4096 [00:18<01:04, 44.27it/s, est. speed input: 69020.19 toks/s, output: 67.40 toks/s]
Processed prompts:  31%|███       | 1265/4096 [00:19<01:04, 44.00it/s, est. speed input: 68067.62 toks/s, output: 66.47 toks/s]
Processed prompts:  32%|███▏      | 1297/4096 [00:19<01:03, 43.78it/s, est. speed input: 67178.80 toks/s, output: 65.60 toks/s]
Processed prompts:  32%|███▏      | 1329/4096 [00:20<01:02, 44.12it/s, est. speed input: 66443.75 toks/s, output: 64.89 toks/s]
Processed prompts:  33%|███▎      | 1361/4096 [00:21<01:02, 43.92it/s, est. speed input: 65682.23 toks/s, output: 64.14 toks/s]
Processed prompts:  34%|███▍      | 1393/4096 [00:21<01:01, 43.77it/s, est. speed input: 64969.57 toks/s, output: 63.45 toks/s]
Processed prompts:  35%|███▍      | 1425/4096 [00:22<01:01, 43.64it/s, est. speed input: 64300.30 toks/s, output: 62.79 toks/s]
Processed prompts:  36%|███▌      | 1457/4096 [00:23<00:59, 44.00it/s, est. speed input: 63740.68 toks/s, output: 62.25 toks/s]
Processed prompts:  36%|███▋      | 1489/4096 [00:24<00:59, 43.85it/s, est. speed input: 63156.07 toks/s, output: 61.68 toks/s]
Processed prompts:  37%|███▋      | 1521/4096 [00:24<00:58, 44.11it/s, est. speed input: 62656.92 toks/s, output: 61.19 toks/s]
Processed prompts:  38%|███▊      | 1553/4096 [00:25<00:57, 44.30it/s, est. speed input: 62185.74 toks/s, output: 60.73 toks/s]
Processed prompts:  39%|███▊      | 1585/4096 [00:26<00:57, 44.00it/s, est. speed input: 61685.56 toks/s, output: 60.24 toks/s]
Processed prompts:  39%|███▉      | 1617/4096 [00:27<00:56, 44.20it/s, est. speed input: 61263.08 toks/s, output: 59.83 toks/s]
Processed prompts:  40%|████      | 1649/4096 [00:27<00:55, 43.97it/s, est. speed input: 60818.14 toks/s, output: 59.39 toks/s]
Processed prompts:  41%|████      | 1681/4096 [00:28<00:55, 43.78it/s, est. speed input: 60391.70 toks/s, output: 58.98 toks/s]
Processed prompts:  42%|████▏     | 1713/4096 [00:29<00:54, 44.06it/s, est. speed input: 60035.28 toks/s, output: 58.63 toks/s]
Processed prompts:  43%|████▎     | 1745/4096 [00:29<00:53, 44.28it/s, est. speed input: 59696.93 toks/s, output: 58.30 toks/s]
Processed prompts:  43%|████▎     | 1777/4096 [00:30<00:52, 43.96it/s, est. speed input: 59324.41 toks/s, output: 57.93 toks/s]
Processed prompts:  44%|████▍     | 1809/4096 [00:31<00:52, 43.75it/s, est. speed input: 58971.40 toks/s, output: 57.59 toks/s]
Processed prompts:  45%|████▍     | 1841/4096 [00:32<00:51, 43.59it/s, est. speed input: 58632.15 toks/s, output: 57.26 toks/s]
Processed prompts:  46%|████▌     | 1873/4096 [00:32<00:50, 43.89it/s, est. speed input: 58349.87 toks/s, output: 56.98 toks/s]
Processed prompts:  47%|████▋     | 1905/4096 [00:33<00:50, 43.74it/s, est. speed input: 58044.62 toks/s, output: 56.68 toks/s]
Processed prompts:  47%|████▋     | 1937/4096 [00:34<00:49, 43.63it/s, est. speed input: 57751.43 toks/s, output: 56.40 toks/s]
Processed prompts:  48%|████▊     | 1969/4096 [00:35<00:48, 43.93it/s, est. speed input: 57505.36 toks/s, output: 56.16 toks/s]
Processed prompts:  49%|████▉     | 2001/4096 [00:35<00:47, 43.72it/s, est. speed input: 57231.82 toks/s, output: 55.89 toks/s]
Processed prompts:  50%|████▉     | 2033/4096 [00:36<00:47, 43.57it/s, est. speed input: 56968.89 toks/s, output: 55.63 toks/s]
Processed prompts:  50%|█████     | 2065/4096 [00:37<00:46, 43.89it/s, est. speed input: 56752.54 toks/s, output: 55.42 toks/s]
Processed prompts:  51%|█████     | 2097/4096 [00:37<00:45, 43.68it/s, est. speed input: 56508.84 toks/s, output: 55.18 toks/s]
Processed prompts:  52%|█████▏    | 2129/4096 [00:38<00:45, 43.56it/s, est. speed input: 56276.26 toks/s, output: 54.96 toks/s]
Processed prompts:  53%|█████▎    | 2161/4096 [00:39<00:44, 43.47it/s, est. speed input: 56051.52 toks/s, output: 54.74 toks/s]
Processed prompts:  54%|█████▎    | 2193/4096 [00:40<00:43, 43.75it/s, est. speed input: 55862.19 toks/s, output: 54.55 toks/s]
Processed prompts:  54%|█████▍    | 2225/4096 [00:40<00:42, 43.59it/s, est. speed input: 55652.75 toks/s, output: 54.35 toks/s]
Processed prompts:  55%|█████▌    | 2257/4096 [00:41<00:42, 43.47it/s, est. speed input: 55449.41 toks/s, output: 54.15 toks/s]
Processed prompts:  56%|█████▌    | 2289/4096 [00:42<00:41, 43.40it/s, est. speed input: 55254.67 toks/s, output: 53.96 toks/s]
Processed prompts:  57%|█████▋    | 2321/4096 [00:43<00:40, 43.36it/s, est. speed input: 55066.75 toks/s, output: 53.78 toks/s]
Processed prompts:  57%|█████▋    | 2353/4096 [00:43<00:40, 43.30it/s, est. speed input: 54882.95 toks/s, output: 53.60 toks/s]
Processed prompts:  58%|█████▊    | 2385/4096 [00:44<00:39, 43.26it/s, est. speed input: 54706.04 toks/s, output: 53.42 toks/s]
Processed prompts:  59%|█████▉    | 2417/4096 [00:45<00:38, 43.26it/s, est. speed input: 54536.53 toks/s, output: 53.26 toks/s]
Processed prompts:  60%|█████▉    | 2449/4096 [00:46<00:38, 43.25it/s, est. speed input: 54371.58 toks/s, output: 53.10 toks/s]
Processed prompts:  61%|██████    | 2481/4096 [00:46<00:37, 43.22it/s, est. speed input: 54210.19 toks/s, output: 52.94 toks/s]
Processed prompts:  61%|██████▏   | 2513/4096 [00:47<00:36, 43.20it/s, est. speed input: 54053.97 toks/s, output: 52.79 toks/s]
Processed prompts:  62%|██████▏   | 2545/4096 [00:48<00:35, 43.59it/s, est. speed input: 53928.30 toks/s, output: 52.66 toks/s]
Processed prompts:  63%|██████▎   | 2577/4096 [00:49<00:34, 43.87it/s, est. speed input: 53806.30 toks/s, output: 52.55 toks/s]
Processed prompts:  64%|██████▎   | 2609/4096 [00:49<00:34, 43.62it/s, est. speed input: 53661.28 toks/s, output: 52.40 toks/s]
Processed prompts:  64%|██████▍   | 2641/4096 [00:50<00:33, 43.45it/s, est. speed input: 53520.33 toks/s, output: 52.27 toks/s]
Processed prompts:  65%|██████▌   | 2673/4096 [00:51<00:32, 43.34it/s, est. speed input: 53384.50 toks/s, output: 52.13 toks/s]
Processed prompts:  66%|██████▌   | 2705/4096 [00:52<00:32, 43.26it/s, est. speed input: 53251.65 toks/s, output: 52.00 toks/s]
Processed prompts:  67%|██████▋   | 2737/4096 [00:52<00:31, 43.56it/s, est. speed input: 53143.62 toks/s, output: 51.90 toks/s]
Processed prompts:  68%|██████▊   | 2769/4096 [00:53<00:30, 43.41it/s, est. speed input: 53017.85 toks/s, output: 51.78 toks/s]
Processed prompts:  68%|██████▊   | 2801/4096 [00:54<00:29, 43.30it/s, est. speed input: 52895.48 toks/s, output: 51.66 toks/s]
Processed prompts:  69%|██████▉   | 2833/4096 [00:54<00:29, 43.28it/s, est. speed input: 52779.02 toks/s, output: 51.54 toks/s]
Processed prompts:  70%|██████▉   | 2865/4096 [00:55<00:28, 43.20it/s, est. speed input: 52662.69 toks/s, output: 51.43 toks/s]
Processed prompts:  71%|███████   | 2897/4096 [00:56<00:26, 44.43it/s, est. speed input: 52615.81 toks/s, output: 51.38 toks/s]
Processed prompts:  72%|███████▏  | 2929/4096 [00:57<00:26, 44.07it/s, est. speed input: 52508.06 toks/s, output: 51.28 toks/s]
Processed prompts:  72%|███████▏  | 2961/4096 [00:57<00:25, 43.80it/s, est. speed input: 52401.75 toks/s, output: 51.17 toks/s]
Processed prompts:  73%|███████▎  | 2993/4096 [00:58<00:25, 43.60it/s, est. speed input: 52297.40 toks/s, output: 51.07 toks/s]
Processed prompts:  74%|███████▍  | 3025/4096 [00:59<00:24, 43.43it/s, est. speed input: 52194.47 toks/s, output: 50.97 toks/s]
Processed prompts:  75%|███████▍  | 3057/4096 [01:00<00:23, 43.30it/s, est. speed input: 52093.45 toks/s, output: 50.87 toks/s]
Processed prompts:  75%|███████▌  | 3089/4096 [01:00<00:23, 43.23it/s, est. speed input: 51995.62 toks/s, output: 50.78 toks/s]
Processed prompts:  76%|███████▌  | 3121/4096 [01:01<00:22, 43.17it/s, est. speed input: 51899.99 toks/s, output: 50.68 toks/s]
Processed prompts:  77%|███████▋  | 3153/4096 [01:02<00:21, 43.15it/s, est. speed input: 51807.34 toks/s, output: 50.59 toks/s]
Processed prompts:  78%|███████▊  | 3185/4096 [01:03<00:21, 43.10it/s, est. speed input: 51715.45 toks/s, output: 50.50 toks/s]
Processed prompts:  79%|███████▊  | 3217/4096 [01:03<00:20, 43.09it/s, est. speed input: 51626.92 toks/s, output: 50.42 toks/s]
Processed prompts:  79%|███████▉  | 3249/4096 [01:04<00:19, 43.04it/s, est. speed input: 51538.16 toks/s, output: 50.33 toks/s]
Processed prompts:  80%|████████  | 3281/4096 [01:05<00:18, 43.02it/s, est. speed input: 51452.35 toks/s, output: 50.25 toks/s]
Processed prompts:  81%|████████  | 3313/4096 [01:06<00:18, 43.03it/s, est. speed input: 51369.26 toks/s, output: 50.17 toks/s]
Processed prompts:  82%|████████▏ | 3345/4096 [01:06<00:17, 43.03it/s, est. speed input: 51288.08 toks/s, output: 50.09 toks/s]
Processed prompts:  82%|████████▏ | 3377/4096 [01:07<00:16, 42.99it/s, est. speed input: 51206.79 toks/s, output: 50.01 toks/s]
Processed prompts:  83%|████████▎ | 3409/4096 [01:08<00:15, 42.95it/s, est. speed input: 51126.84 toks/s, output: 49.93 toks/s]
Processed prompts:  84%|████████▍ | 3441/4096 [01:09<00:15, 42.99it/s, est. speed input: 51051.45 toks/s, output: 49.85 toks/s]
Processed prompts:  85%|████████▍ | 3473/4096 [01:09<00:14, 42.97it/s, est. speed input: 50975.39 toks/s, output: 49.78 toks/s]
Processed prompts:  86%|████████▌ | 3505/4096 [01:10<00:13, 42.99it/s, est. speed input: 50902.80 toks/s, output: 49.71 toks/s]
Processed prompts:  86%|████████▋ | 3537/4096 [01:11<00:12, 43.40it/s, est. speed input: 50847.37 toks/s, output: 49.66 toks/s]
Processed prompts:  87%|████████▋ | 3569/4096 [01:11<00:12, 43.23it/s, est. speed input: 50775.15 toks/s, output: 49.59 toks/s]
Processed prompts:  88%|████████▊ | 3601/4096 [01:12<00:11, 43.17it/s, est. speed input: 50706.31 toks/s, output: 49.52 toks/s]
Processed prompts:  89%|████████▊ | 3633/4096 [01:13<00:10, 43.13it/s, est. speed input: 50639.19 toks/s, output: 49.45 toks/s]
Processed prompts:  89%|████████▉ | 3665/4096 [01:14<00:09, 43.45it/s, est. speed input: 50587.07 toks/s, output: 49.40 toks/s]
Processed prompts:  90%|█████████ | 3697/4096 [01:14<00:09, 43.33it/s, est. speed input: 50522.34 toks/s, output: 49.34 toks/s]
Processed prompts:  91%|█████████ | 3729/4096 [01:15<00:08, 43.22it/s, est. speed input: 50458.30 toks/s, output: 49.28 toks/s]
Processed prompts:  92%|█████████▏| 3761/4096 [01:16<00:07, 43.14it/s, est. speed input: 50395.09 toks/s, output: 49.21 toks/s]
Processed prompts:  93%|█████████▎| 3793/4096 [01:17<00:07, 43.09it/s, est. speed input: 50333.35 toks/s, output: 49.15 toks/s]
Processed prompts:  93%|█████████▎| 3825/4096 [01:17<00:06, 43.05it/s, est. speed input: 50272.60 toks/s, output: 49.09 toks/s]
Processed prompts:  94%|█████████▍| 3857/4096 [01:18<00:05, 43.00it/s, est. speed input: 50212.47 toks/s, output: 49.04 toks/s]
Processed prompts:  95%|█████████▍| 3889/4096 [01:19<00:04, 42.97it/s, est. speed input: 50153.46 toks/s, output: 48.98 toks/s]
Processed prompts:  96%|█████████▌| 3921/4096 [01:20<00:04, 43.71it/s, est. speed input: 50122.54 toks/s, output: 48.95 toks/s]
Processed prompts:  97%|█████████▋| 3953/4096 [01:20<00:03, 43.50it/s, est. speed input: 50066.75 toks/s, output: 48.89 toks/s]
Processed prompts:  97%|█████████▋| 3985/4096 [01:21<00:02, 43.70it/s, est. speed input: 50023.95 toks/s, output: 48.85 toks/s]
Processed prompts:  98%|█████████▊| 4017/4096 [01:22<00:01, 43.44it/s, est. speed input: 49968.14 toks/s, output: 48.80 toks/s]
Processed prompts:  99%|█████████▉| 4049/4096 [01:23<00:01, 43.72it/s, est. speed input: 49928.95 toks/s, output: 48.76 toks/s]
Processed prompts: 100%|█████████▉| 4081/4096 [01:23<00:00, 51.77it/s, est. speed input: 50110.73 toks/s, output: 48.94 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [01:23<00:00, 51.77it/s, est. speed input: 50294.51 toks/s, output: 49.12 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [01:23<00:00, 49.12it/s, est. speed input: 50294.51 toks/s, output: 49.12 toks/s]
[rank0]:[W126 11:21:41.684421952 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 156.2s

测试结果:
  Requests/s:   43.57
  Tokens/s:     44654.70
  Total Reqs:   4096
  Elapsed:      94.02s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     44611.13

============================================================
[8/8] 测试 M=65536
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 65536
│   M_prefill     = 65536 (= 64 x 1024)
│   M_decode      = 64
│   batched_tokens = 65536 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 8192
│   --max-num-seqs           = 64
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 65536
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 11:22:33 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1279105) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1279105) WARNING 01-26 11:22:50 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]     self.model_runner.profile_run()
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]     hidden_states = self.model(
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]                     ^^^^^^^^^^^
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]     def forward(
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]     raise e
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]   File "<eval_with_key>.58", line 325, in forward
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]     return range_entry.runnable(*args)
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]     return self._compiled_fn(*args)
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]     return compiled_fn(full_args)
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]                             ^^^^^^^
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 724, in inner_fn
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]     outs = compiled_fn(args)
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]            ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]     return self.current_callable(inputs)
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]     out = model(new_inputs)
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]   File "/tmp/torchinductor_root/k3/ck3k5ang27fzki3pw5tmgzjzcez3bhliyianmu36s25pdmllr5et.py", line 1093, in call
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]     buf17 = torch.ops.slidesparse.quant_slide_int8.default(buf16, 'Qwen2.5-7B-INT8', 6)
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 564, in _quant_slide_int8_impl
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]     return fn(input, L)
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]            ^^^^^^^^^^^^
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]   File "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/H100_cc90_py312_cu129_x86_64/quant_slide_tuned_Qwen2.5-7B.py", line 339, in quant_slide_int8_triton
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]     _quant_slide_int8_kernel[(M,)](
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 756, in run
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]     launch_metadata = kernel.launch_metadata(grid, stream, *bound_args.values())
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 490, in launch_metadata
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]     self._init_handles()
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 473, in _init_handles
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]     self.module, self.function, self.n_regs, self.n_spills, self.n_max_threads = driver.active.utils.load_binary(
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866]                                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1279105) ERROR 01-26 11:22:59 [core.py:866] RuntimeError: Triton Error [CUDA]: an illegal memory access was encountered


─── STDERR ───
[2026-01-26 11:22:33] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:22:33] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:22:33] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 11:22:33] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:22:33] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:22:33] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:22:33] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:22:33] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:22:33] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:22:33] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:22:33] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:22:33] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:22:33] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:22:33] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 11:22:40] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:22:41] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:22:41] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 11:22:41] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:22:41] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:22:41] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:22:41] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:22:41] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:22:41] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:22:41] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:22:41] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:22:41] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:22:41] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:22:41] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1279105) [2026-01-26 11:22:43] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1279105) [2026-01-26 11:22:43] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1279105) [2026-01-26 11:22:43] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1279105) [2026-01-26 11:22:43] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=1279105) [2026-01-26 11:22:43] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=1279105) [2026-01-26 11:22:43] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=1279105) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1279105) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.14it/s]
(EngineCore_DP0 pid=1279105) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.53it/s]
(EngineCore_DP0 pid=1279105) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.60it/s]
(EngineCore_DP0 pid=1279105) 
(EngineCore_DP0 pid=1279105) [2026-01-26 11:22:44] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 4800] -> 1D uint8
(EngineCore_DP0 pid=1279105) [2026-01-26 11:22:44] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 13824000 bytes
(EngineCore_DP0 pid=1279105) [2026-01-26 11:22:44] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 4800] -> 1D uint8
(EngineCore_DP0 pid=1279105) [2026-01-26 11:22:44] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 10752000 bytes
(EngineCore_DP0 pid=1279105) [2026-01-26 11:22:44] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 4800] -> 1D uint8
(EngineCore_DP0 pid=1279105) [2026-01-26 11:22:44] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 113664000 bytes
(EngineCore_DP0 pid=1279105) [2026-01-26 11:22:44] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 25280] -> 1D uint8
(EngineCore_DP0 pid=1279105) [2026-01-26 11:22:44] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 56627200 bytes
(EngineCore_DP0 pid=1279105) [rank0]:W0126 11:22:56.483000 1279105 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1279105) [rank0]:W0126 11:22:56.561000 1279105 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1279105) [rank0]:W0126 11:22:58.225000 1279105 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1279105) [rank0]:W0126 11:22:58.347000 1279105 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1279105) Process EngineCore_DP0:
(EngineCore_DP0 pid=1279105) Traceback (most recent call last):
(EngineCore_DP0 pid=1279105)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=1279105)     self.run()
(EngineCore_DP0 pid=1279105)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=1279105)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=1279105)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=1279105)     raise e
(EngineCore_DP0 pid=1279105)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1279105)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1279105)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1279105)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1279105)     super().__init__(
(EngineCore_DP0 pid=1279105)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=1279105)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=1279105)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1279105)   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=1279105)     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=1279105)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1279105)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=1279105)     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=1279105)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1279105)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=1279105)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=1279105)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1279105)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=1279105)     return func(*args, **kwargs)
(EngineCore_DP0 pid=1279105)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1279105)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=1279105)     return func(*args, **kwargs)
(EngineCore_DP0 pid=1279105)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1279105)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=1279105)     self.model_runner.profile_run()
(EngineCore_DP0 pid=1279105)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=1279105)     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=1279105)                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1279105)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=1279105)     return func(*args, **kwargs)
(EngineCore_DP0 pid=1279105)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1279105)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=1279105)     outputs = self.model(
(EngineCore_DP0 pid=1279105)               ^^^^^^^^^^^
(EngineCore_DP0 pid=1279105)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=1279105)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=1279105)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1279105)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=1279105)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=1279105)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1279105)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=1279105)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=1279105)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1279105)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=1279105)     hidden_states = self.model(
(EngineCore_DP0 pid=1279105)                     ^^^^^^^^^^^
(EngineCore_DP0 pid=1279105)   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=1279105)     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=1279105)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1279105)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=1279105)     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=1279105)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1279105)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=1279105)     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=1279105)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1279105)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
(EngineCore_DP0 pid=1279105)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=1279105)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1279105)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
(EngineCore_DP0 pid=1279105)     def forward(
(EngineCore_DP0 pid=1279105)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=1279105)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=1279105)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1279105)   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=1279105)     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=1279105)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1279105)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=1279105)     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=1279105)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1279105)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=1279105)     raise e
(EngineCore_DP0 pid=1279105)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=1279105)     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=1279105)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1279105)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=1279105)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=1279105)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1279105)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=1279105)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=1279105)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1279105)   File "<eval_with_key>.58", line 325, in forward
(EngineCore_DP0 pid=1279105)     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
(EngineCore_DP0 pid=1279105)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1279105)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=1279105)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=1279105)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1279105)   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=1279105)     return range_entry.runnable(*args)
(EngineCore_DP0 pid=1279105)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1279105)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=1279105)     return self._compiled_fn(*args)
(EngineCore_DP0 pid=1279105)            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1279105)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=1279105)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=1279105)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1279105)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=1279105)     return compiled_fn(full_args)
(EngineCore_DP0 pid=1279105)            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1279105)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=1279105)     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=1279105)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1279105)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=1279105)     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=1279105)                             ^^^^^^^
(EngineCore_DP0 pid=1279105)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 724, in inner_fn
(EngineCore_DP0 pid=1279105)     outs = compiled_fn(args)
(EngineCore_DP0 pid=1279105)            ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1279105)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=1279105)     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=1279105)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1279105)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=1279105)     return self.current_callable(inputs)
(EngineCore_DP0 pid=1279105)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1279105)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=1279105)     out = model(new_inputs)
(EngineCore_DP0 pid=1279105)           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1279105)   File "/tmp/torchinductor_root/k3/ck3k5ang27fzki3pw5tmgzjzcez3bhliyianmu36s25pdmllr5et.py", line 1093, in call
(EngineCore_DP0 pid=1279105)     buf17 = torch.ops.slidesparse.quant_slide_int8.default(buf16, 'Qwen2.5-7B-INT8', 6)
(EngineCore_DP0 pid=1279105)             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1279105)   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
(EngineCore_DP0 pid=1279105)     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=1279105)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1279105)   File "/root/vllmbench/slidesparse/core/kernels.py", line 564, in _quant_slide_int8_impl
(EngineCore_DP0 pid=1279105)     return fn(input, L)
(EngineCore_DP0 pid=1279105)            ^^^^^^^^^^^^
(EngineCore_DP0 pid=1279105)   File "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/H100_cc90_py312_cu129_x86_64/quant_slide_tuned_Qwen2.5-7B.py", line 339, in quant_slide_int8_triton
(EngineCore_DP0 pid=1279105)     _quant_slide_int8_kernel[(M,)](
(EngineCore_DP0 pid=1279105)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=1279105)     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=1279105)                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1279105)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 756, in run
(EngineCore_DP0 pid=1279105)     launch_metadata = kernel.launch_metadata(grid, stream, *bound_args.values())
(EngineCore_DP0 pid=1279105)                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1279105)   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 490, in launch_metadata
(EngineCore_DP0 pid=1279105)     self._init_handles()
(EngineCore_DP0 pid=1279105)   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 473, in _init_handles
(EngineCore_DP0 pid=1279105)     self.module, self.function, self.n_regs, self.n_spills, self.n_max_threads = driver.active.utils.load_binary(
(EngineCore_DP0 pid=1279105)                                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1279105) RuntimeError: Triton Error [CUDA]: an illegal memory access was encountered
[rank0]:[W126 11:23:00.439351913 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[ERROR] 测试失败: M=65536 (exit code: 1)


------------------------------------------------------------
  生成 CSV: Qwen2.5-7B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_INT8_py312_cu129_x86_64/cusparselt/2_6/Qwen2.5-7B-INT8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,36.2280,18584.9734,3.5332
1024,1024,1,128,128,33.0695,33896.2656,3.8706
2048,1024,2,256,128,38.9512,39924.9352,6.5723
4096,1024,4,512,128,40.9282,41951.4484,12.5097
8192,1024,8,1024,128,42.4823,43544.3932,24.1041
16384,1024,16,2048,128,43.4223,44507.8253,47.1647
32768,1024,32,4096,128,43.5656,44654.6983,94.0192
65536,1024,64,8192,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[INFO] 完成: 7 成功, 1 失败

============================================================
  Qwen2.5-7B-INT8 | cuSPARSELt (2_8) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_8
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_INT8_py312_cu129_x86_64/cusparselt/2_8

============================================================
[1/8] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 11:23:10 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1280115) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1280115) WARNING 01-26 11:23:27 [backends.py:609] Failed to read file <frozen os>
Throughput: 35.23 requests/s, 18075.50 total tokens/s, 35.23 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-26 11:23:09] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:23:10] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:23:10] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 11:23:10] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:23:10] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:23:10] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:23:10] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:23:10] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:23:10] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:23:10] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:23:10] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:23:10] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:23:10] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:23:10] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 11:23:16] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:23:17] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:23:17] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 11:23:17] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:23:17] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:23:17] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:23:17] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:23:17] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:23:17] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:23:17] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:23:17] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:23:17] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:23:17] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:23:17] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1280115) [2026-01-26 11:23:18] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1280115) [2026-01-26 11:23:18] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1280115) [2026-01-26 11:23:18] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1280115) [2026-01-26 11:23:18] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=1280115) [2026-01-26 11:23:18] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=1280115) [2026-01-26 11:23:18] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=1280115) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1280115) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.94it/s]
(EngineCore_DP0 pid=1280115) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.39it/s]
(EngineCore_DP0 pid=1280115) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.46it/s]
(EngineCore_DP0 pid=1280115) 
(EngineCore_DP0 pid=1280115) [2026-01-26 11:23:21] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5376] -> 1D uint8
(EngineCore_DP0 pid=1280115) [2026-01-26 11:23:21] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15482880 bytes
(EngineCore_DP0 pid=1280115) [2026-01-26 11:23:21] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5376] -> 1D uint8
(EngineCore_DP0 pid=1280115) [2026-01-26 11:23:21] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12042240 bytes
(EngineCore_DP0 pid=1280115) [2026-01-26 11:23:21] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5376] -> 1D uint8
(EngineCore_DP0 pid=1280115) [2026-01-26 11:23:21] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 127303680 bytes
(EngineCore_DP0 pid=1280115) [2026-01-26 11:23:21] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 28416] -> 1D uint8
(EngineCore_DP0 pid=1280115) [2026-01-26 11:23:21] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 63651840 bytes
(EngineCore_DP0 pid=1280115) 2026-01-26 11:23:37,517 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1280115) 2026-01-26 11:23:37,540 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1280115) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  3.16it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:01<00:00,  1.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:01<00:00,  1.88it/s]
(EngineCore_DP0 pid=1280115) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  5.20it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  5.19it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  41%|████      | 52/128 [00:00<00:00, 510.62it/s]
Adding requests:  95%|█████████▌| 122/128 [00:00<00:00, 617.11it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 603.07it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:23,  5.30it/s, est. speed input: 2712.76 toks/s, output: 5.30 toks/s]
Processed prompts:   5%|▍         | 6/128 [00:00<00:05, 22.22it/s, est. speed input: 9811.28 toks/s, output: 19.16 toks/s]
Processed prompts:   9%|▊         | 11/128 [00:00<00:03, 29.61it/s, est. speed input: 12883.41 toks/s, output: 25.16 toks/s]
Processed prompts:  12%|█▎        | 16/128 [00:00<00:03, 33.57it/s, est. speed input: 14602.55 toks/s, output: 28.52 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:02, 35.88it/s, est. speed input: 15691.94 toks/s, output: 30.65 toks/s]
Processed prompts:  20%|██        | 26/128 [00:00<00:02, 37.36it/s, est. speed input: 16453.43 toks/s, output: 32.13 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:00<00:02, 38.31it/s, est. speed input: 17010.63 toks/s, output: 33.22 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:01<00:02, 38.81it/s, est. speed input: 17415.67 toks/s, output: 34.01 toks/s]
Processed prompts:  31%|███▏      | 40/128 [00:01<00:02, 38.86it/s, est. speed input: 17641.15 toks/s, output: 34.45 toks/s]
Processed prompts:  34%|███▍      | 44/128 [00:01<00:02, 38.85it/s, est. speed input: 17822.63 toks/s, output: 34.81 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:01<00:02, 38.87it/s, est. speed input: 17980.94 toks/s, output: 35.12 toks/s]
Processed prompts:  41%|████      | 52/128 [00:01<00:01, 38.84it/s, est. speed input: 18111.61 toks/s, output: 35.37 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:01<00:01, 38.85it/s, est. speed input: 18228.96 toks/s, output: 35.60 toks/s]
Processed prompts:  47%|████▋     | 60/128 [00:01<00:01, 38.83it/s, est. speed input: 18329.64 toks/s, output: 35.80 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:01<00:01, 38.82it/s, est. speed input: 18417.86 toks/s, output: 35.97 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:01<00:01, 38.82it/s, est. speed input: 18498.30 toks/s, output: 36.13 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:01<00:01, 38.86it/s, est. speed input: 18572.97 toks/s, output: 36.27 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:02<00:01, 38.92it/s, est. speed input: 18643.06 toks/s, output: 36.41 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:02<00:01, 38.99it/s, est. speed input: 18708.62 toks/s, output: 36.54 toks/s]
Processed prompts:  66%|██████▌   | 84/128 [00:02<00:01, 38.99it/s, est. speed input: 18764.86 toks/s, output: 36.65 toks/s]
Processed prompts:  69%|██████▉   | 88/128 [00:02<00:01, 38.98it/s, est. speed input: 18815.82 toks/s, output: 36.75 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:02<00:00, 38.97it/s, est. speed input: 18861.54 toks/s, output: 36.84 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:02<00:00, 38.98it/s, est. speed input: 18905.45 toks/s, output: 36.92 toks/s]
Processed prompts:  78%|███████▊  | 100/128 [00:02<00:00, 39.03it/s, est. speed input: 18948.43 toks/s, output: 37.01 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:02<00:00, 39.05it/s, est. speed input: 18987.38 toks/s, output: 37.08 toks/s]
Processed prompts:  84%|████████▍ | 108/128 [00:02<00:00, 39.09it/s, est. speed input: 19024.87 toks/s, output: 37.16 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:03<00:00, 39.08it/s, est. speed input: 19058.02 toks/s, output: 37.22 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:03<00:00, 39.07it/s, est. speed input: 19088.75 toks/s, output: 37.28 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:03<00:00, 39.05it/s, est. speed input: 19116.82 toks/s, output: 37.34 toks/s]
Processed prompts:  97%|█████████▋| 124/128 [00:03<00:00, 39.01it/s, est. speed input: 19141.91 toks/s, output: 37.39 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 39.04it/s, est. speed input: 19168.31 toks/s, output: 37.44 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 39.04it/s, est. speed input: 19168.31 toks/s, output: 37.44 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 37.43it/s, est. speed input: 19168.31 toks/s, output: 37.44 toks/s]
[rank0]:[W126 11:23:44.235754409 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 43.1s

测试结果:
  Requests/s:   35.23
  Tokens/s:     18075.50
  Total Reqs:   128
  Elapsed:      3.63s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     18040.26

============================================================
[2/8] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 11:23:53 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1281254) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1281254) WARNING 01-26 11:24:10 [backends.py:609] Failed to read file <frozen os>
Throughput: 32.73 requests/s, 33551.97 total tokens/s, 32.73 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-26 11:23:52] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:23:53] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:23:53] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 11:23:53] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:23:53] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:23:53] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:23:53] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:23:53] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:23:53] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:23:53] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:23:53] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:23:53] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:23:53] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:23:53] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 11:24:00] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:24:01] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:24:01] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 11:24:01] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:24:01] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:24:01] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:24:01] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:24:01] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:24:01] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:24:01] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:24:01] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:24:01] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:24:01] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:24:01] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1281254) [2026-01-26 11:24:02] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1281254) [2026-01-26 11:24:02] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1281254) [2026-01-26 11:24:02] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1281254) [2026-01-26 11:24:02] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=1281254) [2026-01-26 11:24:02] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=1281254) [2026-01-26 11:24:02] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=1281254) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1281254) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.94it/s]
(EngineCore_DP0 pid=1281254) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.39it/s]
(EngineCore_DP0 pid=1281254) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.45it/s]
(EngineCore_DP0 pid=1281254) 
(EngineCore_DP0 pid=1281254) [2026-01-26 11:24:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5376] -> 1D uint8
(EngineCore_DP0 pid=1281254) [2026-01-26 11:24:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15482880 bytes
(EngineCore_DP0 pid=1281254) [2026-01-26 11:24:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5376] -> 1D uint8
(EngineCore_DP0 pid=1281254) [2026-01-26 11:24:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12042240 bytes
(EngineCore_DP0 pid=1281254) [2026-01-26 11:24:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5376] -> 1D uint8
(EngineCore_DP0 pid=1281254) [2026-01-26 11:24:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 127303680 bytes
(EngineCore_DP0 pid=1281254) [2026-01-26 11:24:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 28416] -> 1D uint8
(EngineCore_DP0 pid=1281254) [2026-01-26 11:24:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 63651840 bytes
(EngineCore_DP0 pid=1281254) 2026-01-26 11:24:22,310 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1281254) 2026-01-26 11:24:22,355 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1281254) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 14.48it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 14.46it/s]
(EngineCore_DP0 pid=1281254) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 17.85it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  13%|█▎        | 17/128 [00:00<00:00, 167.41it/s]
Adding requests:  41%|████      | 52/128 [00:00<00:00, 271.51it/s]
Adding requests:  71%|███████   | 91/128 [00:00<00:00, 322.53it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 316.31it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   6%|▋         | 8/128 [00:00<00:01, 66.96it/s, est. speed input: 68573.42 toks/s, output: 66.96 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:00<00:02, 45.07it/s, est. speed input: 48706.34 toks/s, output: 47.56 toks/s]
Processed prompts:  16%|█▌        | 20/128 [00:00<00:02, 41.21it/s, est. speed input: 45005.69 toks/s, output: 43.95 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:00<00:02, 39.19it/s, est. speed input: 43049.27 toks/s, output: 42.04 toks/s]
Processed prompts:  23%|██▎       | 30/128 [00:00<00:02, 37.98it/s, est. speed input: 41810.54 toks/s, output: 40.83 toks/s]
Processed prompts:  27%|██▋       | 34/128 [00:00<00:02, 37.32it/s, est. speed input: 41110.74 toks/s, output: 40.15 toks/s]
Processed prompts:  30%|██▉       | 38/128 [00:00<00:02, 36.56it/s, est. speed input: 40449.60 toks/s, output: 39.50 toks/s]
Processed prompts:  33%|███▎      | 42/128 [00:01<00:02, 36.14it/s, est. speed input: 39976.87 toks/s, output: 39.04 toks/s]
Processed prompts:  36%|███▌      | 46/128 [00:01<00:02, 35.95it/s, est. speed input: 39630.12 toks/s, output: 38.70 toks/s]
Processed prompts:  39%|███▉      | 50/128 [00:01<00:02, 35.75it/s, est. speed input: 39326.15 toks/s, output: 38.40 toks/s]
Processed prompts:  42%|████▏     | 54/128 [00:01<00:02, 35.69it/s, est. speed input: 39093.04 toks/s, output: 38.18 toks/s]
Processed prompts:  45%|████▌     | 58/128 [00:01<00:01, 35.69it/s, est. speed input: 38906.89 toks/s, output: 37.99 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:01<00:01, 35.69it/s, est. speed input: 38744.25 toks/s, output: 37.84 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:01<00:01, 35.68it/s, est. speed input: 38600.52 toks/s, output: 37.70 toks/s]
Processed prompts:  55%|█████▍    | 70/128 [00:01<00:01, 35.68it/s, est. speed input: 38477.51 toks/s, output: 37.57 toks/s]
Processed prompts:  58%|█████▊    | 74/128 [00:01<00:01, 35.54it/s, est. speed input: 38337.30 toks/s, output: 37.44 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:02<00:01, 35.40it/s, est. speed input: 38205.64 toks/s, output: 37.31 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:02<00:01, 35.38it/s, est. speed input: 38101.30 toks/s, output: 37.21 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:02<00:01, 35.42it/s, est. speed input: 38017.55 toks/s, output: 37.13 toks/s]
Processed prompts:  70%|███████   | 90/128 [00:02<00:01, 35.40it/s, est. speed input: 37932.52 toks/s, output: 37.04 toks/s]
Processed prompts:  73%|███████▎  | 94/128 [00:02<00:00, 35.39it/s, est. speed input: 37856.95 toks/s, output: 36.97 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:02<00:00, 35.44it/s, est. speed input: 37795.23 toks/s, output: 36.91 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:02<00:00, 35.48it/s, est. speed input: 37739.27 toks/s, output: 36.85 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:02<00:00, 35.48it/s, est. speed input: 37684.26 toks/s, output: 36.80 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:02<00:00, 35.30it/s, est. speed input: 37609.10 toks/s, output: 36.73 toks/s]
Processed prompts:  89%|████████▉ | 114/128 [00:03<00:00, 35.28it/s, est. speed input: 37553.59 toks/s, output: 36.67 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:03<00:00, 35.29it/s, est. speed input: 37504.60 toks/s, output: 36.63 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:03<00:00, 35.33it/s, est. speed input: 37462.32 toks/s, output: 36.58 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:03<00:00, 35.40it/s, est. speed input: 37428.09 toks/s, output: 36.55 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 35.40it/s, est. speed input: 37408.69 toks/s, output: 36.53 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 36.53it/s, est. speed input: 37408.69 toks/s, output: 36.53 toks/s]
[rank0]:[W126 11:24:28.246846207 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 44.0s

测试结果:
  Requests/s:   32.73
  Tokens/s:     33551.97
  Total Reqs:   128
  Elapsed:      3.91s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     33519.24

============================================================
[3/8] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 11:24:38 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1282384) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1282384) WARNING 01-26 11:24:55 [backends.py:609] Failed to read file <frozen os>
Throughput: 36.82 requests/s, 37739.68 total tokens/s, 36.82 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-26 11:24:37] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:24:38] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:24:38] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 11:24:38] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:24:38] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:24:38] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:24:38] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:24:38] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:24:38] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:24:38] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:24:38] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:24:38] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:24:38] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:24:38] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 11:24:44] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:24:45] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:24:45] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 11:24:45] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:24:45] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:24:45] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:24:45] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:24:45] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:24:45] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:24:45] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:24:45] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:24:45] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:24:45] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:24:45] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1282384) [2026-01-26 11:24:46] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1282384) [2026-01-26 11:24:47] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1282384) [2026-01-26 11:24:47] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1282384) [2026-01-26 11:24:47] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=1282384) [2026-01-26 11:24:47] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=1282384) [2026-01-26 11:24:47] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=1282384) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1282384) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.92it/s]
(EngineCore_DP0 pid=1282384) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.39it/s]
(EngineCore_DP0 pid=1282384) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.45it/s]
(EngineCore_DP0 pid=1282384) 
(EngineCore_DP0 pid=1282384) [2026-01-26 11:24:48] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5376] -> 1D uint8
(EngineCore_DP0 pid=1282384) [2026-01-26 11:24:49] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15482880 bytes
(EngineCore_DP0 pid=1282384) [2026-01-26 11:24:49] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5376] -> 1D uint8
(EngineCore_DP0 pid=1282384) [2026-01-26 11:24:49] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12042240 bytes
(EngineCore_DP0 pid=1282384) [2026-01-26 11:24:49] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5376] -> 1D uint8
(EngineCore_DP0 pid=1282384) [2026-01-26 11:24:49] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 127303680 bytes
(EngineCore_DP0 pid=1282384) [2026-01-26 11:24:49] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 28416] -> 1D uint8
(EngineCore_DP0 pid=1282384) [2026-01-26 11:24:49] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 63651840 bytes
(EngineCore_DP0 pid=1282384) 2026-01-26 11:25:06,010 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1282384) 2026-01-26 11:25:06,033 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1282384) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:00<00:00, 14.92it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 14.72it/s]
(EngineCore_DP0 pid=1282384) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 19.03it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 19.00it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   8%|▊         | 21/256 [00:00<00:01, 204.41it/s]
Adding requests:  24%|██▍       | 62/256 [00:00<00:00, 322.39it/s]
Adding requests:  38%|███▊      | 98/256 [00:00<00:00, 337.99it/s]
Adding requests:  53%|█████▎    | 136/256 [00:00<00:00, 353.63it/s]
Adding requests:  69%|██████▉   | 176/256 [00:00<00:00, 367.60it/s]
Adding requests:  84%|████████▍ | 216/256 [00:00<00:00, 376.47it/s]
Adding requests: 100%|█████████▉| 255/256 [00:00<00:00, 379.01it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 358.96it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   8%|▊         | 20/256 [00:00<00:01, 145.12it/s, est. speed input: 148641.79 toks/s, output: 145.13 toks/s]
Processed prompts:  14%|█▎        | 35/256 [00:00<00:03, 63.63it/s, est. speed input: 72098.78 toks/s, output: 70.41 toks/s]   
Processed prompts:  17%|█▋        | 44/256 [00:00<00:04, 50.88it/s, est. speed input: 59769.11 toks/s, output: 58.37 toks/s]
Processed prompts:  20%|█▉        | 51/256 [00:00<00:04, 48.89it/s, est. speed input: 57169.78 toks/s, output: 55.83 toks/s]
Processed prompts:  22%|██▏       | 57/256 [00:01<00:04, 45.73it/s, est. speed input: 54453.52 toks/s, output: 53.18 toks/s]
Processed prompts:  24%|██▍       | 62/256 [00:01<00:04, 41.85it/s, est. speed input: 51732.26 toks/s, output: 50.52 toks/s]
Processed prompts:  26%|██▌       | 67/256 [00:01<00:04, 43.25it/s, est. speed input: 51565.10 toks/s, output: 50.36 toks/s]
Processed prompts:  28%|██▊       | 72/256 [00:01<00:04, 39.87it/s, est. speed input: 49663.02 toks/s, output: 48.50 toks/s]
Processed prompts:  30%|███       | 77/256 [00:01<00:04, 41.82it/s, est. speed input: 49648.20 toks/s, output: 48.48 toks/s]
Processed prompts:  32%|███▏      | 82/256 [00:01<00:04, 38.72it/s, est. speed input: 48191.77 toks/s, output: 47.06 toks/s]
Processed prompts:  34%|███▎      | 86/256 [00:01<00:04, 38.54it/s, est. speed input: 47663.49 toks/s, output: 46.55 toks/s]
Processed prompts:  35%|███▌      | 90/256 [00:01<00:04, 38.46it/s, est. speed input: 47206.35 toks/s, output: 46.10 toks/s]
Processed prompts:  37%|███▋      | 94/256 [00:02<00:04, 38.37it/s, est. speed input: 46790.19 toks/s, output: 45.69 toks/s]
Processed prompts:  38%|███▊      | 98/256 [00:02<00:04, 38.35it/s, est. speed input: 46423.19 toks/s, output: 45.33 toks/s]
Processed prompts:  40%|███▉      | 102/256 [00:02<00:04, 38.39it/s, est. speed input: 46102.84 toks/s, output: 45.02 toks/s]
Processed prompts:  41%|████▏     | 106/256 [00:02<00:03, 38.54it/s, est. speed input: 45831.15 toks/s, output: 44.76 toks/s]
Processed prompts:  43%|████▎     | 110/256 [00:02<00:03, 38.57it/s, est. speed input: 45568.41 toks/s, output: 44.50 toks/s]
Processed prompts:  45%|████▍     | 114/256 [00:02<00:03, 38.56it/s, est. speed input: 45323.03 toks/s, output: 44.26 toks/s]
Processed prompts:  46%|████▌     | 118/256 [00:02<00:03, 38.68it/s, est. speed input: 45114.62 toks/s, output: 44.06 toks/s]
Processed prompts:  48%|████▊     | 122/256 [00:02<00:03, 38.60it/s, est. speed input: 44899.12 toks/s, output: 43.85 toks/s]
Processed prompts:  49%|████▉     | 126/256 [00:02<00:03, 38.48it/s, est. speed input: 44688.44 toks/s, output: 43.64 toks/s]
Processed prompts:  51%|█████     | 130/256 [00:02<00:03, 38.27it/s, est. speed input: 44476.91 toks/s, output: 43.43 toks/s]
Processed prompts:  52%|█████▏    | 134/256 [00:03<00:03, 38.19it/s, est. speed input: 44287.79 toks/s, output: 43.25 toks/s]
Processed prompts:  54%|█████▍    | 138/256 [00:03<00:03, 38.20it/s, est. speed input: 44119.68 toks/s, output: 43.09 toks/s]
Processed prompts:  55%|█████▌    | 142/256 [00:03<00:02, 38.27it/s, est. speed input: 43970.09 toks/s, output: 42.94 toks/s]
Processed prompts:  57%|█████▋    | 146/256 [00:03<00:02, 38.34it/s, est. speed input: 43831.75 toks/s, output: 42.80 toks/s]
Processed prompts:  59%|█████▊    | 150/256 [00:03<00:02, 38.47it/s, est. speed input: 43710.79 toks/s, output: 42.69 toks/s]
Processed prompts:  60%|██████    | 154/256 [00:03<00:02, 38.44it/s, est. speed input: 43583.73 toks/s, output: 42.56 toks/s]
Processed prompts:  62%|██████▏   | 158/256 [00:03<00:02, 38.45it/s, est. speed input: 43466.41 toks/s, output: 42.45 toks/s]
Processed prompts:  63%|██████▎   | 162/256 [00:03<00:02, 38.38it/s, est. speed input: 43347.73 toks/s, output: 42.33 toks/s]
Processed prompts:  65%|██████▍   | 166/256 [00:03<00:02, 38.33it/s, est. speed input: 43235.55 toks/s, output: 42.22 toks/s]
Processed prompts:  66%|██████▋   | 170/256 [00:04<00:02, 38.22it/s, est. speed input: 43121.94 toks/s, output: 42.11 toks/s]
Processed prompts:  68%|██████▊   | 174/256 [00:04<00:02, 38.21it/s, est. speed input: 43020.44 toks/s, output: 42.01 toks/s]
Processed prompts:  70%|██████▉   | 178/256 [00:04<00:02, 38.29it/s, est. speed input: 42931.99 toks/s, output: 41.93 toks/s]
Processed prompts:  71%|███████   | 182/256 [00:04<00:01, 38.32it/s, est. speed input: 42844.64 toks/s, output: 41.84 toks/s]
Processed prompts:  73%|███████▎  | 186/256 [00:04<00:01, 38.39it/s, est. speed input: 42766.75 toks/s, output: 41.76 toks/s]
Processed prompts:  74%|███████▍  | 190/256 [00:04<00:01, 38.44it/s, est. speed input: 42691.61 toks/s, output: 41.69 toks/s]
Processed prompts:  76%|███████▌  | 194/256 [00:04<00:01, 38.53it/s, est. speed input: 42624.74 toks/s, output: 41.63 toks/s]
Processed prompts:  77%|███████▋  | 198/256 [00:04<00:01, 38.56it/s, est. speed input: 42558.41 toks/s, output: 41.56 toks/s]
Processed prompts:  80%|███████▉  | 204/256 [00:04<00:01, 40.16it/s, est. speed input: 42597.96 toks/s, output: 41.60 toks/s]
Processed prompts:  81%|████████▏ | 208/256 [00:05<00:01, 39.53it/s, est. speed input: 42519.02 toks/s, output: 41.52 toks/s]
Processed prompts:  83%|████████▎ | 212/256 [00:05<00:01, 39.13it/s, est. speed input: 42447.58 toks/s, output: 41.45 toks/s]
Processed prompts:  84%|████████▍ | 216/256 [00:05<00:01, 38.83it/s, est. speed input: 42379.08 toks/s, output: 41.39 toks/s]
Processed prompts:  86%|████████▌ | 220/256 [00:05<00:00, 38.74it/s, est. speed input: 42321.96 toks/s, output: 41.33 toks/s]
Processed prompts:  88%|████████▊ | 224/256 [00:05<00:00, 38.68it/s, est. speed input: 42266.76 toks/s, output: 41.28 toks/s]
Processed prompts:  89%|████████▉ | 228/256 [00:05<00:00, 38.71it/s, est. speed input: 42219.51 toks/s, output: 41.23 toks/s]
Processed prompts:  91%|█████████ | 232/256 [00:05<00:00, 38.64it/s, est. speed input: 42167.14 toks/s, output: 41.18 toks/s]
Processed prompts:  92%|█████████▏| 236/256 [00:05<00:00, 38.53it/s, est. speed input: 42113.03 toks/s, output: 41.13 toks/s]
Processed prompts:  94%|█████████▍| 240/256 [00:05<00:00, 38.50it/s, est. speed input: 42063.65 toks/s, output: 41.08 toks/s]
Processed prompts:  95%|█████████▌| 244/256 [00:05<00:00, 38.39it/s, est. speed input: 42010.74 toks/s, output: 41.03 toks/s]
Processed prompts:  97%|█████████▋| 248/256 [00:06<00:00, 38.40it/s, est. speed input: 41965.01 toks/s, output: 40.98 toks/s]
Processed prompts:  98%|█████████▊| 252/256 [00:06<00:00, 38.31it/s, est. speed input: 41914.31 toks/s, output: 40.93 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:06<00:00, 38.31it/s, est. speed input: 42024.66 toks/s, output: 41.04 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:06<00:00, 41.04it/s, est. speed input: 42024.66 toks/s, output: 41.04 toks/s]
[rank0]:[W126 11:25:15.074709123 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 47.0s

测试结果:
  Requests/s:   36.82
  Tokens/s:     37739.68
  Total Reqs:   256
  Elapsed:      6.95s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     37702.86

============================================================
[4/8] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 11:25:26 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1283541) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1283541) WARNING 01-26 11:25:43 [backends.py:609] Failed to read file <frozen os>
Throughput: 39.13 requests/s, 40110.02 total tokens/s, 39.13 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-26 11:25:25] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:25:26] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:25:26] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 11:25:26] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:25:26] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:25:26] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:25:26] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:25:26] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:25:26] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:25:26] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:25:26] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:25:26] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:25:26] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:25:26] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 11:25:33] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:25:33] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:25:33] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 11:25:33] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:25:33] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:25:33] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:25:33] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:25:33] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:25:33] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:25:33] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:25:33] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:25:33] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:25:33] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:25:33] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1283541) [2026-01-26 11:25:34] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1283541) [2026-01-26 11:25:35] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1283541) [2026-01-26 11:25:35] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1283541) [2026-01-26 11:25:35] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=1283541) [2026-01-26 11:25:35] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=1283541) [2026-01-26 11:25:35] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=1283541) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1283541) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.94it/s]
(EngineCore_DP0 pid=1283541) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.39it/s]
(EngineCore_DP0 pid=1283541) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.45it/s]
(EngineCore_DP0 pid=1283541) 
(EngineCore_DP0 pid=1283541) [2026-01-26 11:25:36] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5376] -> 1D uint8
(EngineCore_DP0 pid=1283541) [2026-01-26 11:25:36] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15482880 bytes
(EngineCore_DP0 pid=1283541) [2026-01-26 11:25:36] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5376] -> 1D uint8
(EngineCore_DP0 pid=1283541) [2026-01-26 11:25:36] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12042240 bytes
(EngineCore_DP0 pid=1283541) [2026-01-26 11:25:36] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5376] -> 1D uint8
(EngineCore_DP0 pid=1283541) [2026-01-26 11:25:36] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 127303680 bytes
(EngineCore_DP0 pid=1283541) [2026-01-26 11:25:36] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 28416] -> 1D uint8
(EngineCore_DP0 pid=1283541) [2026-01-26 11:25:36] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 63651840 bytes
(EngineCore_DP0 pid=1283541) 2026-01-26 11:25:54,365 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1283541) 2026-01-26 11:25:54,405 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1283541) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 1/4 [00:00<00:00,  8.65it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 14.87it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 14.10it/s]
(EngineCore_DP0 pid=1283541) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00, 18.64it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 19.25it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   4%|▍         | 21/512 [00:00<00:02, 207.98it/s]
Adding requests:  12%|█▏        | 62/512 [00:00<00:01, 324.56it/s]
Adding requests:  19%|█▉        | 98/512 [00:00<00:01, 339.07it/s]
Adding requests:  27%|██▋       | 136/512 [00:00<00:01, 354.13it/s]
Adding requests:  34%|███▍      | 176/512 [00:00<00:00, 367.35it/s]
Adding requests:  42%|████▏     | 217/512 [00:00<00:00, 380.16it/s]
Adding requests:  50%|█████     | 256/512 [00:00<00:00, 380.05it/s]
Adding requests:  58%|█████▊    | 296/512 [00:00<00:00, 384.35it/s]
Adding requests:  66%|██████▌   | 337/512 [00:00<00:00, 392.20it/s]
Adding requests:  74%|███████▎  | 377/512 [00:01<00:00, 393.54it/s]
Adding requests:  82%|████████▏ | 419/512 [00:01<00:00, 399.97it/s]
Adding requests:  90%|████████▉ | 459/512 [00:01<00:00, 395.67it/s]
Adding requests:  98%|█████████▊| 501/512 [00:01<00:00, 402.06it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 380.54it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  10%|▉         | 50/512 [00:00<00:01, 359.00it/s, est. speed input: 367676.99 toks/s, output: 359.02 toks/s]
Processed prompts:  17%|█▋        | 86/512 [00:01<00:06, 70.37it/s, est. speed input: 83813.71 toks/s, output: 81.85 toks/s]   
Processed prompts:  20%|██        | 104/512 [00:01<00:06, 61.24it/s, est. speed input: 73443.75 toks/s, output: 71.72 toks/s]
Processed prompts:  23%|██▎       | 116/512 [00:01<00:07, 55.15it/s, est. speed input: 67762.24 toks/s, output: 66.17 toks/s]
Processed prompts:  24%|██▍       | 125/512 [00:01<00:07, 52.65it/s, est. speed input: 65312.18 toks/s, output: 63.78 toks/s]
Processed prompts:  26%|██▌       | 132/512 [00:02<00:07, 48.47it/s, est. speed input: 62516.54 toks/s, output: 61.05 toks/s]
Processed prompts:  27%|██▋       | 138/512 [00:02<00:08, 43.88it/s, est. speed input: 59804.38 toks/s, output: 58.40 toks/s]
Processed prompts:  28%|██▊       | 143/512 [00:02<00:08, 44.75it/s, est. speed input: 59438.89 toks/s, output: 58.05 toks/s]
Processed prompts:  29%|██▉       | 148/512 [00:02<00:07, 45.58it/s, est. speed input: 59089.22 toks/s, output: 57.70 toks/s]
Processed prompts:  30%|██▉       | 153/512 [00:02<00:07, 46.30it/s, est. speed input: 58753.85 toks/s, output: 57.38 toks/s]
Processed prompts:  31%|███       | 158/512 [00:02<00:09, 38.22it/s, est. speed input: 56346.05 toks/s, output: 55.02 toks/s]
Processed prompts:  32%|███▏      | 163/512 [00:02<00:08, 40.52it/s, est. speed input: 56145.65 toks/s, output: 54.83 toks/s]
Processed prompts:  33%|███▎      | 168/512 [00:03<00:08, 42.43it/s, est. speed input: 55941.47 toks/s, output: 54.63 toks/s]
Processed prompts:  34%|███▍      | 173/512 [00:03<00:07, 44.08it/s, est. speed input: 55760.55 toks/s, output: 54.45 toks/s]
Processed prompts:  35%|███▍      | 178/512 [00:03<00:09, 36.17it/s, est. speed input: 53949.67 toks/s, output: 52.68 toks/s]
Processed prompts:  36%|███▌      | 182/512 [00:03<00:08, 36.93it/s, est. speed input: 53557.93 toks/s, output: 52.30 toks/s]
Processed prompts:  36%|███▋      | 186/512 [00:03<00:08, 37.52it/s, est. speed input: 53179.81 toks/s, output: 51.93 toks/s]
Processed prompts:  37%|███▋      | 190/512 [00:03<00:08, 37.94it/s, est. speed input: 52816.98 toks/s, output: 51.58 toks/s]
Processed prompts:  38%|███▊      | 194/512 [00:03<00:08, 38.20it/s, est. speed input: 52463.51 toks/s, output: 51.23 toks/s]
Processed prompts:  39%|███▊      | 198/512 [00:03<00:08, 38.48it/s, est. speed input: 52140.93 toks/s, output: 50.92 toks/s]
Processed prompts:  40%|████      | 206/512 [00:04<00:07, 40.21it/s, est. speed input: 51747.50 toks/s, output: 50.53 toks/s]
Processed prompts:  41%|████      | 211/512 [00:04<00:07, 42.47it/s, est. speed input: 51718.22 toks/s, output: 50.51 toks/s]
Processed prompts:  42%|████▏     | 216/512 [00:04<00:06, 44.37it/s, est. speed input: 51701.84 toks/s, output: 50.49 toks/s]
Processed prompts:  43%|████▎     | 221/512 [00:04<00:06, 45.79it/s, est. speed input: 51681.21 toks/s, output: 50.47 toks/s]
Processed prompts:  44%|████▍     | 226/512 [00:04<00:07, 36.55it/s, est. speed input: 50512.41 toks/s, output: 49.33 toks/s]
Processed prompts:  45%|████▍     | 230/512 [00:04<00:07, 37.22it/s, est. speed input: 50289.95 toks/s, output: 49.11 toks/s]
Processed prompts:  46%|████▌     | 234/512 [00:04<00:07, 37.68it/s, est. speed input: 50069.51 toks/s, output: 48.90 toks/s]
Processed prompts:  46%|████▋     | 238/512 [00:04<00:07, 38.05it/s, est. speed input: 49858.74 toks/s, output: 48.69 toks/s]
Processed prompts:  47%|████▋     | 242/512 [00:04<00:07, 38.46it/s, est. speed input: 49668.22 toks/s, output: 48.50 toks/s]
Processed prompts:  48%|████▊     | 246/512 [00:05<00:06, 38.73it/s, est. speed input: 49483.16 toks/s, output: 48.32 toks/s]
Processed prompts:  49%|████▉     | 250/512 [00:05<00:06, 39.01it/s, est. speed input: 49312.17 toks/s, output: 48.16 toks/s]
Processed prompts:  50%|████▉     | 254/512 [00:05<00:06, 39.20it/s, est. speed input: 49145.93 toks/s, output: 47.99 toks/s]
Processed prompts:  50%|█████     | 258/512 [00:05<00:06, 39.36it/s, est. speed input: 48988.26 toks/s, output: 47.84 toks/s]
Processed prompts:  51%|█████     | 262/512 [00:05<00:06, 39.43it/s, est. speed input: 48833.01 toks/s, output: 47.69 toks/s]
Processed prompts:  52%|█████▏    | 266/512 [00:05<00:06, 39.42it/s, est. speed input: 48678.73 toks/s, output: 47.54 toks/s]
Processed prompts:  53%|█████▎    | 270/512 [00:05<00:06, 39.27it/s, est. speed input: 48520.06 toks/s, output: 47.38 toks/s]
Processed prompts:  54%|█████▎    | 274/512 [00:05<00:06, 39.18it/s, est. speed input: 48367.73 toks/s, output: 47.23 toks/s]
Processed prompts:  54%|█████▍    | 278/512 [00:05<00:05, 39.21it/s, est. speed input: 48227.38 toks/s, output: 47.10 toks/s]
Processed prompts:  55%|█████▌    | 282/512 [00:06<00:05, 39.25it/s, est. speed input: 48092.75 toks/s, output: 46.97 toks/s]
Processed prompts:  56%|█████▌    | 286/512 [00:06<00:05, 39.23it/s, est. speed input: 47959.41 toks/s, output: 46.84 toks/s]
Processed prompts:  57%|█████▋    | 290/512 [00:06<00:05, 39.29it/s, est. speed input: 47835.19 toks/s, output: 46.71 toks/s]
Processed prompts:  57%|█████▋    | 294/512 [00:06<00:05, 39.42it/s, est. speed input: 47720.98 toks/s, output: 46.60 toks/s]
Processed prompts:  58%|█████▊    | 298/512 [00:06<00:05, 39.54it/s, est. speed input: 47612.57 toks/s, output: 46.50 toks/s]
Processed prompts:  59%|█████▉    | 302/512 [00:06<00:05, 39.56it/s, est. speed input: 47503.03 toks/s, output: 46.39 toks/s]
Processed prompts:  61%|██████    | 310/512 [00:06<00:04, 41.21it/s, est. speed input: 47416.23 toks/s, output: 46.30 toks/s]
Processed prompts:  62%|██████▏   | 315/512 [00:06<00:04, 43.19it/s, est. speed input: 47455.70 toks/s, output: 46.34 toks/s]
Processed prompts:  62%|██████▎   | 320/512 [00:06<00:04, 44.77it/s, est. speed input: 47496.90 toks/s, output: 46.38 toks/s]
Processed prompts:  63%|██████▎   | 325/512 [00:07<00:04, 45.94it/s, est. speed input: 47535.64 toks/s, output: 46.42 toks/s]
Processed prompts:  64%|██████▍   | 330/512 [00:07<00:04, 36.48it/s, est. speed input: 46902.33 toks/s, output: 45.80 toks/s]
Processed prompts:  65%|██████▌   | 334/512 [00:07<00:04, 37.22it/s, est. speed input: 46814.10 toks/s, output: 45.72 toks/s]
Processed prompts:  66%|██████▌   | 338/512 [00:07<00:04, 37.77it/s, est. speed input: 46725.27 toks/s, output: 45.63 toks/s]
Processed prompts:  67%|██████▋   | 342/512 [00:07<00:04, 38.23it/s, est. speed input: 46640.23 toks/s, output: 45.55 toks/s]
Processed prompts:  68%|██████▊   | 346/512 [00:07<00:04, 38.61it/s, est. speed input: 46559.70 toks/s, output: 45.47 toks/s]
Processed prompts:  68%|██████▊   | 350/512 [00:07<00:04, 38.90it/s, est. speed input: 46481.54 toks/s, output: 45.39 toks/s]
Processed prompts:  69%|██████▉   | 354/512 [00:07<00:04, 39.09it/s, est. speed input: 46404.26 toks/s, output: 45.32 toks/s]
Processed prompts:  70%|██████▉   | 358/512 [00:07<00:03, 39.20it/s, est. speed input: 46327.48 toks/s, output: 45.24 toks/s]
Processed prompts:  71%|███████   | 362/512 [00:08<00:03, 39.16it/s, est. speed input: 46246.60 toks/s, output: 45.16 toks/s]
Processed prompts:  71%|███████▏  | 366/512 [00:08<00:03, 39.12it/s, est. speed input: 46167.08 toks/s, output: 45.08 toks/s]
Processed prompts:  72%|███████▏  | 370/512 [00:08<00:03, 39.16it/s, est. speed input: 46093.22 toks/s, output: 45.01 toks/s]
Processed prompts:  73%|███████▎  | 374/512 [00:08<00:03, 39.22it/s, est. speed input: 46022.48 toks/s, output: 44.94 toks/s]
Processed prompts:  74%|███████▍  | 378/512 [00:08<00:03, 39.24it/s, est. speed input: 45952.53 toks/s, output: 44.88 toks/s]
Processed prompts:  75%|███████▍  | 382/512 [00:08<00:03, 39.25it/s, est. speed input: 45883.87 toks/s, output: 44.81 toks/s]
Processed prompts:  75%|███████▌  | 386/512 [00:08<00:03, 39.24it/s, est. speed input: 45816.32 toks/s, output: 44.74 toks/s]
Processed prompts:  76%|███████▌  | 390/512 [00:08<00:03, 39.27it/s, est. speed input: 45751.66 toks/s, output: 44.68 toks/s]
Processed prompts:  77%|███████▋  | 394/512 [00:08<00:02, 39.36it/s, est. speed input: 45691.87 toks/s, output: 44.62 toks/s]
Processed prompts:  78%|███████▊  | 398/512 [00:08<00:02, 39.47it/s, est. speed input: 45635.41 toks/s, output: 44.57 toks/s]
Processed prompts:  79%|███████▊  | 402/512 [00:09<00:02, 39.47it/s, est. speed input: 45576.96 toks/s, output: 44.51 toks/s]
Processed prompts:  79%|███████▉  | 406/512 [00:09<00:02, 39.47it/s, est. speed input: 45519.44 toks/s, output: 44.45 toks/s]
Processed prompts:  80%|████████  | 410/512 [00:09<00:02, 39.37it/s, est. speed input: 45459.28 toks/s, output: 44.39 toks/s]
Processed prompts:  81%|████████  | 414/512 [00:09<00:02, 39.34it/s, est. speed input: 45402.02 toks/s, output: 44.34 toks/s]
Processed prompts:  82%|████████▏ | 418/512 [00:09<00:02, 39.31it/s, est. speed input: 45345.69 toks/s, output: 44.28 toks/s]
Processed prompts:  82%|████████▏ | 422/512 [00:09<00:02, 39.26it/s, est. speed input: 45289.64 toks/s, output: 44.23 toks/s]
Processed prompts:  83%|████████▎ | 426/512 [00:09<00:02, 39.25it/s, est. speed input: 45235.03 toks/s, output: 44.17 toks/s]
Processed prompts:  84%|████████▍ | 430/512 [00:09<00:02, 39.26it/s, est. speed input: 45182.86 toks/s, output: 44.12 toks/s]
Processed prompts:  86%|████████▌ | 438/512 [00:09<00:01, 41.04it/s, est. speed input: 45167.83 toks/s, output: 44.11 toks/s]
Processed prompts:  87%|████████▋ | 443/512 [00:10<00:01, 43.17it/s, est. speed input: 45222.14 toks/s, output: 44.16 toks/s]
Processed prompts:  88%|████████▊ | 448/512 [00:10<00:01, 44.80it/s, est. speed input: 45274.51 toks/s, output: 44.21 toks/s]
Processed prompts:  88%|████████▊ | 453/512 [00:10<00:01, 45.98it/s, est. speed input: 45323.72 toks/s, output: 44.26 toks/s]
Processed prompts:  89%|████████▉ | 458/512 [00:10<00:01, 36.47it/s, est. speed input: 44927.75 toks/s, output: 43.87 toks/s]
Processed prompts:  90%|█████████ | 462/512 [00:10<00:01, 37.15it/s, est. speed input: 44882.68 toks/s, output: 43.83 toks/s]
Processed prompts:  91%|█████████ | 466/512 [00:10<00:01, 37.71it/s, est. speed input: 44838.80 toks/s, output: 43.79 toks/s]
Processed prompts:  92%|█████████▏| 470/512 [00:10<00:01, 38.13it/s, est. speed input: 44794.96 toks/s, output: 43.74 toks/s]
Processed prompts:  93%|█████████▎| 474/512 [00:10<00:00, 38.44it/s, est. speed input: 44752.00 toks/s, output: 43.70 toks/s]
Processed prompts:  93%|█████████▎| 478/512 [00:10<00:00, 38.67it/s, est. speed input: 44709.48 toks/s, output: 43.66 toks/s]
Processed prompts:  94%|█████████▍| 482/512 [00:11<00:00, 38.92it/s, est. speed input: 44670.88 toks/s, output: 43.62 toks/s]
Processed prompts:  95%|█████████▍| 486/512 [00:11<00:00, 39.09it/s, est. speed input: 44632.66 toks/s, output: 43.59 toks/s]
Processed prompts:  96%|█████████▌| 490/512 [00:11<00:00, 39.27it/s, est. speed input: 44597.15 toks/s, output: 43.55 toks/s]
Processed prompts:  96%|█████████▋| 494/512 [00:11<00:00, 39.35it/s, est. speed input: 44560.50 toks/s, output: 43.52 toks/s]
Processed prompts:  97%|█████████▋| 498/512 [00:11<00:00, 39.38it/s, est. speed input: 44523.62 toks/s, output: 43.48 toks/s]
Processed prompts:  98%|█████████▊| 502/512 [00:11<00:00, 39.28it/s, est. speed input: 44483.34 toks/s, output: 43.44 toks/s]
Processed prompts:  99%|█████████▉| 506/512 [00:11<00:00, 39.29it/s, est. speed input: 44446.60 toks/s, output: 43.40 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:11<00:00, 39.29it/s, est. speed input: 44670.45 toks/s, output: 43.62 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:11<00:00, 43.62it/s, est. speed input: 44670.45 toks/s, output: 43.62 toks/s]
[rank0]:[W126 11:26:09.708425838 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 54.7s

测试结果:
  Requests/s:   39.13
  Tokens/s:     40110.02
  Total Reqs:   512
  Elapsed:      13.08s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     40070.89

============================================================
[5/8] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 11:26:24 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1284829) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1284829) WARNING 01-26 11:26:41 [backends.py:609] Failed to read file <frozen os>
Throughput: 40.27 requests/s, 41280.25 total tokens/s, 40.27 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-26 11:26:23] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:26:24] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:26:24] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 11:26:24] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:26:24] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:26:24] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:26:24] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:26:24] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:26:24] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:26:24] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:26:24] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:26:24] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:26:24] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:26:24] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 11:26:30] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:26:31] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:26:31] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 11:26:31] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:26:31] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:26:31] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:26:31] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:26:31] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:26:31] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:26:31] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:26:31] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:26:31] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:26:31] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:26:31] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1284829) [2026-01-26 11:26:33] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1284829) [2026-01-26 11:26:33] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1284829) [2026-01-26 11:26:33] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1284829) [2026-01-26 11:26:33] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=1284829) [2026-01-26 11:26:33] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=1284829) [2026-01-26 11:26:33] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=1284829) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1284829) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.95it/s]
(EngineCore_DP0 pid=1284829) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.40it/s]
(EngineCore_DP0 pid=1284829) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.46it/s]
(EngineCore_DP0 pid=1284829) 
(EngineCore_DP0 pid=1284829) [2026-01-26 11:26:35] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5376] -> 1D uint8
(EngineCore_DP0 pid=1284829) [2026-01-26 11:26:35] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15482880 bytes
(EngineCore_DP0 pid=1284829) [2026-01-26 11:26:35] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5376] -> 1D uint8
(EngineCore_DP0 pid=1284829) [2026-01-26 11:26:35] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12042240 bytes
(EngineCore_DP0 pid=1284829) [2026-01-26 11:26:35] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5376] -> 1D uint8
(EngineCore_DP0 pid=1284829) [2026-01-26 11:26:35] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 127303680 bytes
(EngineCore_DP0 pid=1284829) [2026-01-26 11:26:35] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 28416] -> 1D uint8
(EngineCore_DP0 pid=1284829) [2026-01-26 11:26:35] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 63651840 bytes
(EngineCore_DP0 pid=1284829) 2026-01-26 11:26:52,174 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1284829) 2026-01-26 11:26:52,198 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1284829) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 1/5 [00:00<00:01,  2.54it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:01,  2.14it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:01<00:00,  4.58it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:01<00:00,  4.55it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:01<00:00,  3.92it/s]
(EngineCore_DP0 pid=1284829) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 1/4 [00:00<00:00,  9.47it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 16.79it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 15.86it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 23/1024 [00:00<00:04, 227.22it/s]
Adding requests:   6%|▋         | 64/1024 [00:00<00:02, 328.79it/s]
Adding requests:  10%|▉         | 99/1024 [00:00<00:02, 333.48it/s]
Adding requests:  13%|█▎        | 137/1024 [00:00<00:02, 349.71it/s]
Adding requests:  17%|█▋        | 177/1024 [00:00<00:02, 364.22it/s]
Adding requests:  21%|██▏       | 219/1024 [00:00<00:02, 381.21it/s]
Adding requests:  25%|██▌       | 258/1024 [00:00<00:02, 378.37it/s]
Adding requests:  29%|██▉       | 298/1024 [00:00<00:01, 383.41it/s]
Adding requests:  33%|███▎      | 338/1024 [00:00<00:01, 387.30it/s]
Adding requests:  37%|███▋      | 378/1024 [00:01<00:01, 389.09it/s]
Adding requests:  41%|████      | 420/1024 [00:01<00:01, 396.36it/s]
Adding requests:  45%|████▍     | 460/1024 [00:01<00:01, 390.56it/s]
Adding requests:  49%|████▉     | 503/1024 [00:01<00:01, 399.86it/s]
Adding requests:  53%|█████▎    | 544/1024 [00:01<00:01, 401.25it/s]
Adding requests:  57%|█████▋    | 585/1024 [00:01<00:01, 396.64it/s]
Adding requests:  61%|██████    | 625/1024 [00:01<00:01, 392.26it/s]
Adding requests:  65%|██████▍   | 665/1024 [00:01<00:00, 386.48it/s]
Adding requests:  69%|██████▉   | 705/1024 [00:01<00:00, 390.00it/s]
Adding requests:  73%|███████▎  | 745/1024 [00:01<00:00, 385.10it/s]
Adding requests:  77%|███████▋  | 785/1024 [00:02<00:00, 388.11it/s]
Adding requests:  80%|████████  | 824/1024 [00:02<00:00, 387.70it/s]
Adding requests:  84%|████████▍ | 864/1024 [00:02<00:00, 390.27it/s]
Adding requests:  88%|████████▊ | 906/1024 [00:02<00:00, 397.31it/s]
Adding requests:  92%|█████████▏| 946/1024 [00:02<00:00, 386.08it/s]
Adding requests:  96%|█████████▋| 986/1024 [00:02<00:00, 387.76it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 382.66it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  10%|█         | 106/1024 [00:00<00:01, 502.82it/s, est. speed input: 514992.83 toks/s, output: 502.85 toks/s]
Processed prompts:  15%|█▌        | 157/1024 [00:01<00:09, 93.37it/s, est. speed input: 114495.61 toks/s, output: 111.81 toks/s] 
Processed prompts:  18%|█▊        | 181/1024 [00:01<00:11, 72.78it/s, est. speed input: 92701.80 toks/s, output: 90.53 toks/s]  
Processed prompts:  19%|█▉        | 196/1024 [00:02<00:13, 63.43it/s, est. speed input: 83830.05 toks/s, output: 81.86 toks/s]
Processed prompts:  20%|██        | 207/1024 [00:02<00:13, 62.80it/s, est. speed input: 82168.04 toks/s, output: 80.24 toks/s]
Processed prompts:  21%|██        | 216/1024 [00:02<00:13, 59.50it/s, est. speed input: 79620.17 toks/s, output: 77.75 toks/s]
Processed prompts:  22%|██▏       | 224/1024 [00:02<00:14, 55.54it/s, est. speed input: 77118.05 toks/s, output: 75.31 toks/s]
Processed prompts:  23%|██▎       | 231/1024 [00:03<00:15, 50.91it/s, est. speed input: 74596.78 toks/s, output: 72.85 toks/s]
Processed prompts:  23%|██▎       | 237/1024 [00:03<00:17, 45.65it/s, est. speed input: 71997.24 toks/s, output: 70.31 toks/s]
Processed prompts:  24%|██▎       | 242/1024 [00:03<00:19, 40.11it/s, est. speed input: 69372.35 toks/s, output: 67.75 toks/s]
Processed prompts:  24%|██▍       | 250/1024 [00:03<00:19, 40.10it/s, est. speed input: 67871.65 toks/s, output: 66.28 toks/s]
Processed prompts:  25%|██▌       | 258/1024 [00:03<00:19, 40.25it/s, est. speed input: 66568.90 toks/s, output: 65.01 toks/s]
Processed prompts:  26%|██▌       | 266/1024 [00:04<00:18, 40.33it/s, est. speed input: 65383.14 toks/s, output: 63.85 toks/s]
Processed prompts:  27%|██▋       | 274/1024 [00:04<00:18, 40.26it/s, est. speed input: 64271.18 toks/s, output: 62.76 toks/s]
Processed prompts:  28%|██▊       | 282/1024 [00:04<00:18, 40.17it/s, est. speed input: 63245.44 toks/s, output: 61.76 toks/s]
Processed prompts:  28%|██▊       | 290/1024 [00:04<00:18, 40.15it/s, est. speed input: 62317.20 toks/s, output: 60.86 toks/s]
Processed prompts:  29%|██▉       | 298/1024 [00:04<00:18, 40.21it/s, est. speed input: 61478.91 toks/s, output: 60.04 toks/s]
Processed prompts:  30%|██▉       | 306/1024 [00:05<00:17, 41.40it/s, est. speed input: 60923.73 toks/s, output: 59.50 toks/s]
Processed prompts:  31%|███       | 314/1024 [00:05<00:17, 41.13it/s, est. speed input: 60204.08 toks/s, output: 58.79 toks/s]
Processed prompts:  31%|███▏      | 322/1024 [00:05<00:17, 40.86it/s, est. speed input: 59523.26 toks/s, output: 58.13 toks/s]
Processed prompts:  32%|███▏      | 330/1024 [00:05<00:17, 40.69it/s, est. speed input: 58891.79 toks/s, output: 57.51 toks/s]
Processed prompts:  33%|███▎      | 338/1024 [00:05<00:16, 40.54it/s, est. speed input: 58296.72 toks/s, output: 56.93 toks/s]
Processed prompts:  34%|███▍      | 346/1024 [00:06<00:16, 40.51it/s, est. speed input: 57752.47 toks/s, output: 56.40 toks/s]
Processed prompts:  35%|███▍      | 354/1024 [00:06<00:16, 40.45it/s, est. speed input: 57236.56 toks/s, output: 55.89 toks/s]
Processed prompts:  35%|███▌      | 362/1024 [00:06<00:16, 40.36it/s, est. speed input: 56744.33 toks/s, output: 55.41 toks/s]
Processed prompts:  36%|███▌      | 370/1024 [00:06<00:16, 40.28it/s, est. speed input: 56279.35 toks/s, output: 54.96 toks/s]
Processed prompts:  37%|███▋      | 378/1024 [00:06<00:16, 40.29it/s, est. speed input: 55850.29 toks/s, output: 54.54 toks/s]
Processed prompts:  38%|███▊      | 386/1024 [00:07<00:15, 40.32it/s, est. speed input: 55447.03 toks/s, output: 54.15 toks/s]
Processed prompts:  38%|███▊      | 394/1024 [00:07<00:15, 40.22it/s, est. speed input: 55052.07 toks/s, output: 53.76 toks/s]
Processed prompts:  39%|███▉      | 402/1024 [00:07<00:15, 40.14it/s, est. speed input: 54676.10 toks/s, output: 53.39 toks/s]
Processed prompts:  40%|████      | 410/1024 [00:07<00:15, 40.17it/s, est. speed input: 54329.18 toks/s, output: 53.06 toks/s]
Processed prompts:  41%|████      | 418/1024 [00:07<00:15, 40.15it/s, est. speed input: 53995.45 toks/s, output: 52.73 toks/s]
Processed prompts:  42%|████▏     | 426/1024 [00:08<00:14, 40.23it/s, est. speed input: 53688.69 toks/s, output: 52.43 toks/s]
Processed prompts:  42%|████▏     | 434/1024 [00:08<00:14, 41.35it/s, est. speed input: 53505.47 toks/s, output: 52.25 toks/s]
Processed prompts:  43%|████▎     | 442/1024 [00:08<00:14, 40.91it/s, est. speed input: 53208.11 toks/s, output: 51.96 toks/s]
Processed prompts:  44%|████▍     | 450/1024 [00:08<00:14, 40.64it/s, est. speed input: 52927.33 toks/s, output: 51.69 toks/s]
Processed prompts:  45%|████▍     | 458/1024 [00:08<00:13, 40.55it/s, est. speed input: 52668.31 toks/s, output: 51.43 toks/s]
Processed prompts:  46%|████▌     | 466/1024 [00:09<00:13, 40.43it/s, est. speed input: 52415.72 toks/s, output: 51.19 toks/s]
Processed prompts:  46%|████▋     | 474/1024 [00:09<00:13, 40.37it/s, est. speed input: 52175.78 toks/s, output: 50.95 toks/s]
Processed prompts:  47%|████▋     | 482/1024 [00:09<00:13, 40.44it/s, est. speed input: 51955.90 toks/s, output: 50.74 toks/s]
Processed prompts:  48%|████▊     | 490/1024 [00:09<00:13, 40.38it/s, est. speed input: 51735.45 toks/s, output: 50.52 toks/s]
Processed prompts:  49%|████▊     | 498/1024 [00:09<00:13, 40.34it/s, est. speed input: 51524.19 toks/s, output: 50.32 toks/s]
Processed prompts:  49%|████▉     | 506/1024 [00:10<00:12, 40.28it/s, est. speed input: 51318.74 toks/s, output: 50.12 toks/s]
Processed prompts:  50%|█████     | 514/1024 [00:10<00:12, 40.36it/s, est. speed input: 51131.06 toks/s, output: 49.93 toks/s]
Processed prompts:  51%|█████     | 522/1024 [00:10<00:12, 40.42it/s, est. speed input: 50950.23 toks/s, output: 49.76 toks/s]
Processed prompts:  52%|█████▏    | 530/1024 [00:10<00:12, 40.45it/s, est. speed input: 50775.60 toks/s, output: 49.59 toks/s]
Processed prompts:  53%|█████▎    | 538/1024 [00:10<00:12, 40.45it/s, est. speed input: 50605.54 toks/s, output: 49.42 toks/s]
Processed prompts:  53%|█████▎    | 546/1024 [00:11<00:11, 40.37it/s, est. speed input: 50435.75 toks/s, output: 49.25 toks/s]
Processed prompts:  54%|█████▍    | 554/1024 [00:11<00:11, 40.36it/s, est. speed input: 50275.66 toks/s, output: 49.10 toks/s]
Processed prompts:  55%|█████▍    | 562/1024 [00:11<00:11, 40.44it/s, est. speed input: 50126.31 toks/s, output: 48.95 toks/s]
Processed prompts:  56%|█████▌    | 570/1024 [00:11<00:11, 40.50it/s, est. speed input: 49982.92 toks/s, output: 48.81 toks/s]
Processed prompts:  56%|█████▋    | 578/1024 [00:11<00:11, 40.50it/s, est. speed input: 49841.58 toks/s, output: 48.67 toks/s]
Processed prompts:  57%|█████▋    | 586/1024 [00:12<00:10, 40.45it/s, est. speed input: 49701.12 toks/s, output: 48.54 toks/s]
Processed prompts:  58%|█████▊    | 594/1024 [00:12<00:10, 40.39it/s, est. speed input: 49563.88 toks/s, output: 48.40 toks/s]
Processed prompts:  59%|█████▉    | 602/1024 [00:12<00:10, 40.36it/s, est. speed input: 49431.29 toks/s, output: 48.27 toks/s]
Processed prompts:  60%|█████▉    | 610/1024 [00:12<00:10, 40.40it/s, est. speed input: 49307.38 toks/s, output: 48.15 toks/s]
Processed prompts:  60%|██████    | 618/1024 [00:12<00:10, 40.40it/s, est. speed input: 49184.87 toks/s, output: 48.03 toks/s]
Processed prompts:  61%|██████    | 626/1024 [00:13<00:09, 40.34it/s, est. speed input: 49062.68 toks/s, output: 47.91 toks/s]
Processed prompts:  62%|██████▏   | 634/1024 [00:13<00:09, 40.28it/s, est. speed input: 48943.27 toks/s, output: 47.80 toks/s]
Processed prompts:  63%|██████▎   | 642/1024 [00:13<00:09, 40.27it/s, est. speed input: 48828.97 toks/s, output: 47.68 toks/s]
Processed prompts:  63%|██████▎   | 650/1024 [00:13<00:09, 40.37it/s, est. speed input: 48724.42 toks/s, output: 47.58 toks/s]
Processed prompts:  64%|██████▍   | 658/1024 [00:13<00:09, 40.45it/s, est. speed input: 48623.44 toks/s, output: 47.48 toks/s]
Processed prompts:  65%|██████▌   | 666/1024 [00:14<00:08, 40.42it/s, est. speed input: 48520.38 toks/s, output: 47.38 toks/s]
Processed prompts:  66%|██████▌   | 674/1024 [00:14<00:08, 40.34it/s, est. speed input: 48416.90 toks/s, output: 47.28 toks/s]
Processed prompts:  67%|██████▋   | 682/1024 [00:14<00:08, 40.34it/s, est. speed input: 48319.27 toks/s, output: 47.19 toks/s]
Processed prompts:  67%|██████▋   | 690/1024 [00:14<00:08, 40.41it/s, est. speed input: 48228.06 toks/s, output: 47.10 toks/s]
Processed prompts:  68%|██████▊   | 698/1024 [00:14<00:08, 40.40it/s, est. speed input: 48136.17 toks/s, output: 47.01 toks/s]
Processed prompts:  69%|██████▉   | 706/1024 [00:15<00:07, 40.39it/s, est. speed input: 48046.83 toks/s, output: 46.92 toks/s]
Processed prompts:  70%|██████▉   | 714/1024 [00:15<00:07, 40.25it/s, est. speed input: 47952.78 toks/s, output: 46.83 toks/s]
Processed prompts:  71%|███████   | 722/1024 [00:15<00:07, 40.29it/s, est. speed input: 47868.22 toks/s, output: 46.75 toks/s]
Processed prompts:  71%|███████▏  | 730/1024 [00:15<00:07, 40.35it/s, est. speed input: 47787.24 toks/s, output: 46.67 toks/s]
Processed prompts:  72%|███████▏  | 738/1024 [00:15<00:07, 40.43it/s, est. speed input: 47710.21 toks/s, output: 46.59 toks/s]
Processed prompts:  73%|███████▎  | 746/1024 [00:16<00:06, 40.40it/s, est. speed input: 47631.19 toks/s, output: 46.51 toks/s]
Processed prompts:  74%|███████▎  | 754/1024 [00:16<00:06, 40.36it/s, est. speed input: 47552.52 toks/s, output: 46.44 toks/s]
Processed prompts:  74%|███████▍  | 762/1024 [00:16<00:06, 40.28it/s, est. speed input: 47473.57 toks/s, output: 46.36 toks/s]
Processed prompts:  75%|███████▌  | 770/1024 [00:16<00:06, 40.36it/s, est. speed input: 47402.93 toks/s, output: 46.29 toks/s]
Processed prompts:  76%|███████▌  | 778/1024 [00:16<00:06, 40.39it/s, est. speed input: 47332.74 toks/s, output: 46.22 toks/s]
Processed prompts:  77%|███████▋  | 786/1024 [00:17<00:05, 41.51it/s, est. speed input: 47312.67 toks/s, output: 46.20 toks/s]
Processed prompts:  78%|███████▊  | 794/1024 [00:17<00:05, 41.15it/s, est. speed input: 47243.35 toks/s, output: 46.14 toks/s]
Processed prompts:  78%|███████▊  | 802/1024 [00:17<00:05, 40.85it/s, est. speed input: 47173.67 toks/s, output: 46.07 toks/s]
Processed prompts:  79%|███████▉  | 810/1024 [00:17<00:05, 40.63it/s, est. speed input: 47104.72 toks/s, output: 46.00 toks/s]
Processed prompts:  80%|███████▉  | 818/1024 [00:17<00:05, 40.55it/s, est. speed input: 47040.30 toks/s, output: 45.94 toks/s]
Processed prompts:  81%|████████  | 826/1024 [00:18<00:04, 40.48it/s, est. speed input: 46977.13 toks/s, output: 45.88 toks/s]
Processed prompts:  81%|████████▏ | 834/1024 [00:18<00:04, 40.41it/s, est. speed input: 46914.23 toks/s, output: 45.81 toks/s]
Processed prompts:  82%|████████▏ | 842/1024 [00:18<00:04, 40.39it/s, est. speed input: 46853.80 toks/s, output: 45.76 toks/s]
Processed prompts:  83%|████████▎ | 850/1024 [00:18<00:04, 40.42it/s, est. speed input: 46796.43 toks/s, output: 45.70 toks/s]
Processed prompts:  84%|████████▍ | 858/1024 [00:18<00:04, 40.47it/s, est. speed input: 46741.60 toks/s, output: 45.65 toks/s]
Processed prompts:  85%|████████▍ | 866/1024 [00:18<00:03, 40.44it/s, est. speed input: 46685.20 toks/s, output: 45.59 toks/s]
Processed prompts:  85%|████████▌ | 874/1024 [00:19<00:03, 40.39it/s, est. speed input: 46628.93 toks/s, output: 45.54 toks/s]
Processed prompts:  86%|████████▌ | 882/1024 [00:19<00:03, 40.36it/s, est. speed input: 46573.85 toks/s, output: 45.48 toks/s]
Processed prompts:  87%|████████▋ | 890/1024 [00:19<00:03, 40.33it/s, est. speed input: 46519.70 toks/s, output: 45.43 toks/s]
Processed prompts:  88%|████████▊ | 898/1024 [00:19<00:03, 40.30it/s, est. speed input: 46466.28 toks/s, output: 45.38 toks/s]
Processed prompts:  88%|████████▊ | 906/1024 [00:19<00:02, 40.30it/s, est. speed input: 46414.46 toks/s, output: 45.33 toks/s]
Processed prompts:  89%|████████▉ | 914/1024 [00:20<00:02, 40.22it/s, est. speed input: 46361.06 toks/s, output: 45.27 toks/s]
Processed prompts:  90%|█████████ | 922/1024 [00:20<00:02, 40.20it/s, est. speed input: 46309.53 toks/s, output: 45.22 toks/s]
Processed prompts:  91%|█████████ | 930/1024 [00:20<00:02, 40.21it/s, est. speed input: 46260.36 toks/s, output: 45.18 toks/s]
Processed prompts:  92%|█████████▏| 938/1024 [00:20<00:02, 40.20it/s, est. speed input: 46211.21 toks/s, output: 45.13 toks/s]
Processed prompts:  92%|█████████▏| 946/1024 [00:20<00:01, 40.24it/s, est. speed input: 46164.98 toks/s, output: 45.08 toks/s]
Processed prompts:  93%|█████████▎| 954/1024 [00:21<00:01, 40.25it/s, est. speed input: 46118.54 toks/s, output: 45.04 toks/s]
Processed prompts:  94%|█████████▍| 962/1024 [00:21<00:01, 40.32it/s, est. speed input: 46075.47 toks/s, output: 45.00 toks/s]
Processed prompts:  95%|█████████▍| 970/1024 [00:21<00:01, 40.36it/s, est. speed input: 46032.74 toks/s, output: 44.95 toks/s]
Processed prompts:  96%|█████████▌| 978/1024 [00:21<00:01, 40.32it/s, est. speed input: 45988.61 toks/s, output: 44.91 toks/s]
Processed prompts:  96%|█████████▋| 986/1024 [00:21<00:00, 40.29it/s, est. speed input: 45944.95 toks/s, output: 44.87 toks/s]
Processed prompts:  97%|█████████▋| 994/1024 [00:22<00:00, 40.25it/s, est. speed input: 45901.67 toks/s, output: 44.83 toks/s]
Processed prompts:  98%|█████████▊| 1002/1024 [00:22<00:00, 40.26it/s, est. speed input: 45860.27 toks/s, output: 44.79 toks/s]
Processed prompts:  99%|█████████▊| 1010/1024 [00:22<00:00, 40.23it/s, est. speed input: 45818.73 toks/s, output: 44.74 toks/s]
Processed prompts:  99%|█████████▉| 1018/1024 [00:22<00:00, 41.70it/s, est. speed input: 45825.21 toks/s, output: 44.75 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:22<00:00, 41.70it/s, est. speed input: 46094.75 toks/s, output: 45.01 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:22<00:00, 45.01it/s, est. speed input: 46094.75 toks/s, output: 45.01 toks/s]
[rank0]:[W126 11:27:20.961972683 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 71.5s

测试结果:
  Requests/s:   40.27
  Tokens/s:     41280.25
  Total Reqs:   1024
  Elapsed:      25.43s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     41239.97

============================================================
[6/8] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 11:27:41 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1286359) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1286359) WARNING 01-26 11:27:58 [backends.py:609] Failed to read file <frozen os>
Throughput: 41.12 requests/s, 42152.79 total tokens/s, 41.12 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-26 11:27:40] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:27:41] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:27:41] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 11:27:41] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:27:41] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:27:41] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:27:41] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:27:41] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:27:41] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:27:41] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:27:41] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:27:41] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:27:41] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:27:41] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 11:27:47] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:27:48] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:27:48] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 11:27:48] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:27:48] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:27:48] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:27:48] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:27:48] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:27:48] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:27:48] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:27:48] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:27:48] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:27:48] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:27:48] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1286359) [2026-01-26 11:27:50] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1286359) [2026-01-26 11:27:50] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1286359) [2026-01-26 11:27:50] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1286359) [2026-01-26 11:27:50] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=1286359) [2026-01-26 11:27:50] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=1286359) [2026-01-26 11:27:50] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=1286359) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1286359) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.93it/s]
(EngineCore_DP0 pid=1286359) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.39it/s]
(EngineCore_DP0 pid=1286359) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.45it/s]
(EngineCore_DP0 pid=1286359) 
(EngineCore_DP0 pid=1286359) [2026-01-26 11:27:52] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5376] -> 1D uint8
(EngineCore_DP0 pid=1286359) [2026-01-26 11:27:52] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15482880 bytes
(EngineCore_DP0 pid=1286359) [2026-01-26 11:27:52] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5376] -> 1D uint8
(EngineCore_DP0 pid=1286359) [2026-01-26 11:27:52] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12042240 bytes
(EngineCore_DP0 pid=1286359) [2026-01-26 11:27:52] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5376] -> 1D uint8
(EngineCore_DP0 pid=1286359) [2026-01-26 11:27:52] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 127303680 bytes
(EngineCore_DP0 pid=1286359) [2026-01-26 11:27:52] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 28416] -> 1D uint8
(EngineCore_DP0 pid=1286359) [2026-01-26 11:27:52] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 63651840 bytes
(EngineCore_DP0 pid=1286359) [rank0]:W0126 11:28:04.331000 1286359 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1286359) [rank0]:W0126 11:28:04.407000 1286359 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1286359) [rank0]:W0126 11:28:05.364000 1286359 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1286359) [rank0]:W0126 11:28:05.486000 1286359 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1286359) 2026-01-26 11:28:09,228 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1286359) 2026-01-26 11:28:09,254 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1286359) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00, 13.82it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00, 10.67it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:01<00:00,  4.70it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:01<00:00,  5.25it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:01<00:00,  5.87it/s]
(EngineCore_DP0 pid=1286359) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00, 19.37it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 20.25it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 20.13it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   1%|          | 22/2048 [00:00<00:09, 217.21it/s]
Adding requests:   3%|▎         | 62/2048 [00:00<00:06, 323.91it/s]
Adding requests:   5%|▍         | 98/2048 [00:00<00:05, 339.01it/s]
Adding requests:   7%|▋         | 136/2048 [00:00<00:05, 355.10it/s]
Adding requests:   9%|▊         | 176/2048 [00:00<00:05, 367.23it/s]
Adding requests:  11%|█         | 216/2048 [00:00<00:04, 377.39it/s]
Adding requests:  12%|█▏        | 254/2048 [00:00<00:04, 377.78it/s]
Adding requests:  14%|█▍        | 293/2048 [00:00<00:04, 380.30it/s]
Adding requests:  16%|█▋        | 335/2048 [00:00<00:04, 390.64it/s]
Adding requests:  18%|█▊        | 375/2048 [00:01<00:04, 392.75it/s]
Adding requests:  20%|██        | 417/2048 [00:01<00:04, 399.95it/s]
Adding requests:  22%|██▏       | 457/2048 [00:01<00:04, 395.32it/s]
Adding requests:  24%|██▍       | 500/2048 [00:01<00:03, 403.97it/s]
Adding requests:  27%|██▋       | 543/2048 [00:01<00:03, 408.89it/s]
Adding requests:  29%|██▊       | 584/2048 [00:01<00:03, 403.98it/s]
Adding requests:  31%|███       | 625/2048 [00:01<00:03, 395.94it/s]
Adding requests:  32%|███▏      | 665/2048 [00:01<00:03, 388.54it/s]
Adding requests:  34%|███▍      | 705/2048 [00:01<00:03, 390.81it/s]
Adding requests:  36%|███▋      | 745/2048 [00:01<00:03, 385.55it/s]
Adding requests:  38%|███▊      | 784/2048 [00:02<00:03, 380.60it/s]
Adding requests:  40%|████      | 823/2048 [00:02<00:03, 379.97it/s]
Adding requests:  42%|████▏     | 863/2048 [00:02<00:03, 383.53it/s]
Adding requests:  44%|████▍     | 903/2048 [00:02<00:02, 387.87it/s]
Adding requests:  46%|████▌     | 942/2048 [00:02<00:02, 379.64it/s]
Adding requests:  48%|████▊     | 981/2048 [00:02<00:02, 378.90it/s]
Adding requests:  50%|████▉     | 1019/2048 [00:02<00:02, 374.12it/s]
Adding requests:  52%|█████▏    | 1057/2048 [00:02<00:02, 372.86it/s]
Adding requests:  53%|█████▎    | 1095/2048 [00:02<00:02, 370.76it/s]
Adding requests:  55%|█████▌    | 1135/2048 [00:02<00:02, 378.28it/s]
Adding requests:  57%|█████▋    | 1173/2048 [00:03<00:02, 376.09it/s]
Adding requests:  59%|█████▉    | 1212/2048 [00:03<00:02, 378.42it/s]
Adding requests:  61%|██████    | 1250/2048 [00:03<00:02, 378.64it/s]
Adding requests:  63%|██████▎   | 1288/2048 [00:03<00:02, 374.93it/s]
Adding requests:  65%|██████▍   | 1328/2048 [00:03<00:01, 378.57it/s]
Adding requests:  67%|██████▋   | 1368/2048 [00:03<00:01, 381.79it/s]
Adding requests:  69%|██████▊   | 1407/2048 [00:03<00:01, 382.00it/s]
Adding requests:  71%|███████   | 1446/2048 [00:03<00:01, 381.44it/s]
Adding requests:  73%|███████▎  | 1487/2048 [00:03<00:01, 388.27it/s]
Adding requests:  75%|███████▍  | 1527/2048 [00:04<00:01, 388.97it/s]
Adding requests:  76%|███████▋  | 1566/2048 [00:04<00:01, 381.86it/s]
Adding requests:  78%|███████▊  | 1605/2048 [00:04<00:01, 377.07it/s]
Adding requests:  80%|████████  | 1643/2048 [00:04<00:01, 368.82it/s]
Adding requests:  82%|████████▏ | 1680/2048 [00:04<00:01, 366.30it/s]
Adding requests:  84%|████████▍ | 1719/2048 [00:04<00:00, 371.71it/s]
Adding requests:  86%|████████▌ | 1758/2048 [00:04<00:00, 374.55it/s]
Adding requests:  88%|████████▊ | 1797/2048 [00:04<00:00, 376.47it/s]
Adding requests:  90%|████████▉ | 1835/2048 [00:04<00:00, 377.31it/s]
Adding requests:  92%|█████████▏| 1874/2048 [00:04<00:00, 378.74it/s]
Adding requests:  93%|█████████▎| 1912/2048 [00:05<00:00, 374.40it/s]
Adding requests:  95%|█████████▌| 1952/2048 [00:05<00:00, 380.23it/s]
Adding requests:  97%|█████████▋| 1991/2048 [00:05<00:00, 376.56it/s]
Adding requests:  99%|█████████▉| 2029/2048 [00:05<00:00, 368.64it/s]
Adding requests: 100%|██████████| 2048/2048 [00:05<00:00, 378.88it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  10%|█         | 210/2048 [00:00<00:01, 1793.61it/s, est. speed input: 1837303.13 toks/s, output: 1793.76 toks/s]
Processed prompts:  19%|█▉        | 390/2048 [00:04<00:21, 75.57it/s, est. speed input: 91546.33 toks/s, output: 89.40 toks/s]      
Processed prompts:  23%|██▎       | 467/2048 [00:06<00:25, 61.79it/s, est. speed input: 76098.62 toks/s, output: 74.31 toks/s]
Processed prompts:  25%|██▍       | 511/2048 [00:07<00:25, 60.78it/s, est. speed input: 74061.99 toks/s, output: 72.33 toks/s]
Processed prompts:  26%|██▋       | 540/2048 [00:07<00:26, 55.89it/s, est. speed input: 70491.68 toks/s, output: 68.84 toks/s]
Processed prompts:  27%|██▋       | 560/2048 [00:08<00:26, 55.29it/s, est. speed input: 69636.68 toks/s, output: 68.00 toks/s]
Processed prompts:  28%|██▊       | 575/2048 [00:08<00:27, 52.69it/s, est. speed input: 68277.98 toks/s, output: 66.68 toks/s]
Processed prompts:  29%|██▊       | 587/2048 [00:09<00:30, 48.69it/s, est. speed input: 66683.79 toks/s, output: 65.12 toks/s]
Processed prompts:  29%|██▉       | 596/2048 [00:09<00:33, 43.45it/s, est. speed input: 64915.63 toks/s, output: 63.39 toks/s]
Processed prompts:  30%|██▉       | 610/2048 [00:09<00:34, 41.74it/s, est. speed input: 63796.19 toks/s, output: 62.30 toks/s]
Processed prompts:  31%|███       | 626/2048 [00:10<00:34, 41.53it/s, est. speed input: 62954.58 toks/s, output: 61.48 toks/s]
Processed prompts:  31%|███▏      | 642/2048 [00:10<00:33, 41.45it/s, est. speed input: 62192.39 toks/s, output: 60.73 toks/s]
Processed prompts:  32%|███▏      | 658/2048 [00:10<00:33, 41.29it/s, est. speed input: 61465.35 toks/s, output: 60.02 toks/s]
Processed prompts:  33%|███▎      | 674/2048 [00:11<00:33, 41.30it/s, est. speed input: 60812.72 toks/s, output: 59.39 toks/s]
Processed prompts:  34%|███▎      | 690/2048 [00:11<00:32, 41.23it/s, est. speed input: 60189.16 toks/s, output: 58.78 toks/s]
Processed prompts:  34%|███▍      | 706/2048 [00:12<00:32, 41.16it/s, est. speed input: 59603.08 toks/s, output: 58.21 toks/s]
Processed prompts:  35%|███▌      | 722/2048 [00:12<00:32, 41.20it/s, est. speed input: 59067.33 toks/s, output: 57.68 toks/s]
Processed prompts:  36%|███▌      | 738/2048 [00:12<00:31, 41.10it/s, est. speed input: 58544.71 toks/s, output: 57.17 toks/s]
Processed prompts:  37%|███▋      | 754/2048 [00:13<00:31, 41.11it/s, est. speed input: 58063.65 toks/s, output: 56.70 toks/s]
Processed prompts:  38%|███▊      | 770/2048 [00:13<00:31, 41.14it/s, est. speed input: 57614.36 toks/s, output: 56.26 toks/s]
Processed prompts:  38%|███▊      | 786/2048 [00:14<00:30, 41.73it/s, est. speed input: 57260.97 toks/s, output: 55.92 toks/s]
Processed prompts:  39%|███▉      | 802/2048 [00:14<00:29, 41.60it/s, est. speed input: 56859.19 toks/s, output: 55.53 toks/s]
Processed prompts:  40%|███▉      | 818/2048 [00:14<00:29, 41.37it/s, est. speed input: 56462.01 toks/s, output: 55.14 toks/s]
Processed prompts:  41%|████      | 834/2048 [00:15<00:29, 41.28it/s, est. speed input: 56093.80 toks/s, output: 54.78 toks/s]
Processed prompts:  42%|████▏     | 850/2048 [00:15<00:29, 41.19it/s, est. speed input: 55740.47 toks/s, output: 54.43 toks/s]
Processed prompts:  42%|████▏     | 866/2048 [00:16<00:28, 41.15it/s, est. speed input: 55406.80 toks/s, output: 54.11 toks/s]
Processed prompts:  43%|████▎     | 882/2048 [00:16<00:28, 41.08it/s, est. speed input: 55084.45 toks/s, output: 53.79 toks/s]
Processed prompts:  44%|████▍     | 898/2048 [00:16<00:28, 41.03it/s, est. speed input: 54777.56 toks/s, output: 53.49 toks/s]
Processed prompts:  45%|████▍     | 914/2048 [00:17<00:27, 41.05it/s, est. speed input: 54489.48 toks/s, output: 53.21 toks/s]
Processed prompts:  45%|████▌     | 930/2048 [00:17<00:27, 41.03it/s, est. speed input: 54211.61 toks/s, output: 52.94 toks/s]
Processed prompts:  46%|████▌     | 946/2048 [00:17<00:26, 40.99it/s, est. speed input: 53942.76 toks/s, output: 52.68 toks/s]
Processed prompts:  47%|████▋     | 962/2048 [00:18<00:26, 40.98it/s, est. speed input: 53686.90 toks/s, output: 52.43 toks/s]
Processed prompts:  48%|████▊     | 978/2048 [00:18<00:26, 41.00it/s, est. speed input: 53444.69 toks/s, output: 52.19 toks/s]
Processed prompts:  49%|████▊     | 994/2048 [00:19<00:25, 40.94it/s, est. speed input: 53205.46 toks/s, output: 51.96 toks/s]
Processed prompts:  49%|████▉     | 1010/2048 [00:19<00:25, 40.94it/s, est. speed input: 52980.00 toks/s, output: 51.74 toks/s]
Processed prompts:  50%|█████     | 1026/2048 [00:19<00:24, 40.93it/s, est. speed input: 52761.69 toks/s, output: 51.52 toks/s]
Processed prompts:  51%|█████     | 1042/2048 [00:20<00:24, 40.92it/s, est. speed input: 52552.26 toks/s, output: 51.32 toks/s]
Processed prompts:  52%|█████▏    | 1058/2048 [00:20<00:24, 40.92it/s, est. speed input: 52350.64 toks/s, output: 51.12 toks/s]
Processed prompts:  52%|█████▏    | 1074/2048 [00:21<00:23, 40.87it/s, est. speed input: 52152.85 toks/s, output: 50.93 toks/s]
Processed prompts:  53%|█████▎    | 1090/2048 [00:21<00:23, 40.88it/s, est. speed input: 51965.78 toks/s, output: 50.75 toks/s]
Processed prompts:  54%|█████▍    | 1106/2048 [00:21<00:23, 40.87it/s, est. speed input: 51784.54 toks/s, output: 50.57 toks/s]
Processed prompts:  55%|█████▍    | 1122/2048 [00:22<00:22, 40.87it/s, est. speed input: 51609.96 toks/s, output: 50.40 toks/s]
Processed prompts:  56%|█████▌    | 1138/2048 [00:22<00:22, 40.90it/s, est. speed input: 51443.13 toks/s, output: 50.24 toks/s]
Processed prompts:  56%|█████▋    | 1154/2048 [00:23<00:21, 40.86it/s, est. speed input: 51278.07 toks/s, output: 50.08 toks/s]
Processed prompts:  57%|█████▋    | 1170/2048 [00:23<00:21, 40.83it/s, est. speed input: 51118.46 toks/s, output: 49.92 toks/s]
Processed prompts:  58%|█████▊    | 1186/2048 [00:23<00:21, 40.83it/s, est. speed input: 50965.10 toks/s, output: 49.77 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [00:24<00:20, 41.50it/s, est. speed input: 50861.18 toks/s, output: 49.67 toks/s]
Processed prompts:  59%|█████▉    | 1218/2048 [00:24<00:20, 41.34it/s, est. speed input: 50719.78 toks/s, output: 49.53 toks/s]
Processed prompts:  60%|██████    | 1234/2048 [00:24<00:19, 41.80it/s, est. speed input: 50618.91 toks/s, output: 49.43 toks/s]
Processed prompts:  61%|██████    | 1250/2048 [00:25<00:19, 41.47it/s, est. speed input: 50480.54 toks/s, output: 49.30 toks/s]
Processed prompts:  62%|██████▏   | 1266/2048 [00:25<00:18, 41.36it/s, est. speed input: 50353.70 toks/s, output: 49.17 toks/s]
Processed prompts:  63%|██████▎   | 1282/2048 [00:26<00:18, 41.13it/s, est. speed input: 50221.45 toks/s, output: 49.04 toks/s]
Processed prompts:  63%|██████▎   | 1298/2048 [00:26<00:18, 41.10it/s, est. speed input: 50101.06 toks/s, output: 48.93 toks/s]
Processed prompts:  64%|██████▍   | 1314/2048 [00:26<00:17, 40.96it/s, est. speed input: 49976.76 toks/s, output: 48.81 toks/s]
Processed prompts:  65%|██████▍   | 1330/2048 [00:27<00:17, 41.51it/s, est. speed input: 49893.56 toks/s, output: 48.72 toks/s]
Processed prompts:  66%|██████▌   | 1346/2048 [00:27<00:16, 41.38it/s, est. speed input: 49783.17 toks/s, output: 48.62 toks/s]
Processed prompts:  67%|██████▋   | 1362/2048 [00:28<00:16, 41.14it/s, est. speed input: 49667.85 toks/s, output: 48.50 toks/s]
Processed prompts:  67%|██████▋   | 1378/2048 [00:28<00:16, 41.09it/s, est. speed input: 49562.08 toks/s, output: 48.40 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [00:28<00:15, 40.95it/s, est. speed input: 49453.66 toks/s, output: 48.29 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [00:29<00:15, 40.81it/s, est. speed input: 49345.80 toks/s, output: 48.19 toks/s]
Processed prompts:  70%|██████▉   | 1426/2048 [00:29<00:15, 40.86it/s, est. speed input: 49248.24 toks/s, output: 48.09 toks/s]
Processed prompts:  70%|███████   | 1442/2048 [00:30<00:14, 40.81it/s, est. speed input: 49149.40 toks/s, output: 48.00 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [00:30<00:14, 41.48it/s, est. speed input: 49088.66 toks/s, output: 47.94 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [00:30<00:13, 41.30it/s, est. speed input: 48996.73 toks/s, output: 47.85 toks/s]
Processed prompts:  73%|███████▎  | 1490/2048 [00:31<00:13, 41.10it/s, est. speed input: 48903.57 toks/s, output: 47.76 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [00:31<00:13, 41.02it/s, est. speed input: 48815.47 toks/s, output: 47.67 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [00:31<00:12, 41.59it/s, est. speed input: 48759.63 toks/s, output: 47.62 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [00:32<00:12, 41.31it/s, est. speed input: 48673.27 toks/s, output: 47.53 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [00:32<00:11, 41.88it/s, est. speed input: 48623.77 toks/s, output: 47.48 toks/s]
Processed prompts:  77%|███████▋  | 1570/2048 [00:33<00:11, 41.48it/s, est. speed input: 48539.64 toks/s, output: 47.40 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [00:33<00:11, 41.28it/s, est. speed input: 48460.84 toks/s, output: 47.32 toks/s]
Processed prompts:  78%|███████▊  | 1602/2048 [00:33<00:10, 41.12it/s, est. speed input: 48383.05 toks/s, output: 47.25 toks/s]
Processed prompts:  79%|███████▉  | 1618/2048 [00:34<00:10, 41.64it/s, est. speed input: 48334.47 toks/s, output: 47.20 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [00:34<00:10, 41.38it/s, est. speed input: 48260.35 toks/s, output: 47.13 toks/s]
Processed prompts:  81%|████████  | 1650/2048 [00:35<00:09, 41.15it/s, est. speed input: 48185.30 toks/s, output: 47.06 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [00:35<00:09, 40.99it/s, est. speed input: 48112.13 toks/s, output: 46.98 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [00:35<00:08, 40.93it/s, est. speed input: 48042.89 toks/s, output: 46.92 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [00:36<00:08, 40.84it/s, est. speed input: 47972.97 toks/s, output: 46.85 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [00:36<00:08, 40.80it/s, est. speed input: 47905.45 toks/s, output: 46.78 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [00:37<00:07, 41.42it/s, est. speed input: 47866.01 toks/s, output: 46.74 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [00:37<00:07, 41.85it/s, est. speed input: 47826.49 toks/s, output: 46.71 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [00:37<00:06, 41.61it/s, est. speed input: 47766.67 toks/s, output: 46.65 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [00:38<00:06, 41.28it/s, est. speed input: 47701.86 toks/s, output: 46.58 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [00:38<00:06, 41.09it/s, est. speed input: 47640.04 toks/s, output: 46.52 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [00:38<00:05, 41.03it/s, est. speed input: 47581.93 toks/s, output: 46.47 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [00:39<00:05, 40.89it/s, est. speed input: 47521.46 toks/s, output: 46.41 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [00:39<00:05, 40.90it/s, est. speed input: 47466.12 toks/s, output: 46.35 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [00:40<00:04, 40.82it/s, est. speed input: 47408.78 toks/s, output: 46.30 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [00:40<00:04, 40.73it/s, est. speed input: 47351.11 toks/s, output: 46.24 toks/s]
Processed prompts:  92%|█████████▏| 1890/2048 [00:40<00:03, 41.38it/s, est. speed input: 47320.73 toks/s, output: 46.21 toks/s]
Processed prompts:  93%|█████████▎| 1906/2048 [00:41<00:03, 41.17it/s, est. speed input: 47266.71 toks/s, output: 46.16 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [00:41<00:03, 41.03it/s, est. speed input: 47214.14 toks/s, output: 46.11 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [00:42<00:02, 40.88it/s, est. speed input: 47160.63 toks/s, output: 46.06 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [00:42<00:02, 40.83it/s, est. speed input: 47110.06 toks/s, output: 46.01 toks/s]
Processed prompts:  96%|█████████▌| 1970/2048 [00:42<00:01, 40.77it/s, est. speed input: 47059.53 toks/s, output: 45.96 toks/s]
Processed prompts:  97%|█████████▋| 1986/2048 [00:43<00:01, 41.44it/s, est. speed input: 47034.26 toks/s, output: 45.93 toks/s]
Processed prompts:  98%|█████████▊| 2002/2048 [00:43<00:01, 41.24it/s, est. speed input: 46986.69 toks/s, output: 45.89 toks/s]
Processed prompts:  99%|█████████▊| 2018/2048 [00:44<00:00, 41.04it/s, est. speed input: 46938.09 toks/s, output: 45.84 toks/s]
Processed prompts:  99%|█████████▉| 2034/2048 [00:44<00:00, 41.78it/s, est. speed input: 46919.45 toks/s, output: 45.82 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:44<00:00, 41.78it/s, est. speed input: 47241.80 toks/s, output: 46.13 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:44<00:00, 46.13it/s, est. speed input: 47241.80 toks/s, output: 46.13 toks/s]
[rank0]:[W126 11:29:02.456730879 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 101.2s

测试结果:
  Requests/s:   41.12
  Tokens/s:     42152.79
  Total Reqs:   2048
  Elapsed:      49.80s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     42111.67

============================================================
[7/8] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 11:29:32 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1288312) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1288312) WARNING 01-26 11:29:49 [backends.py:609] Failed to read file <frozen os>
Throughput: 41.07 requests/s, 42101.12 total tokens/s, 41.07 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-26 11:29:32] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:29:32] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:29:32] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 11:29:32] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:29:32] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:29:32] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:29:32] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:29:32] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:29:32] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:29:32] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:29:32] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:29:32] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:29:32] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:29:32] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 11:29:39] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:29:39] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:29:39] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 11:29:39] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:29:40] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:29:40] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:29:40] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:29:40] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:29:40] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:29:40] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:29:40] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:29:40] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:29:40] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:29:40] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1288312) [2026-01-26 11:29:41] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1288312) [2026-01-26 11:29:41] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1288312) [2026-01-26 11:29:41] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1288312) [2026-01-26 11:29:41] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=1288312) [2026-01-26 11:29:41] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=1288312) [2026-01-26 11:29:41] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=1288312) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1288312) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.94it/s]
(EngineCore_DP0 pid=1288312) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.39it/s]
(EngineCore_DP0 pid=1288312) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.45it/s]
(EngineCore_DP0 pid=1288312) 
(EngineCore_DP0 pid=1288312) [2026-01-26 11:29:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5376] -> 1D uint8
(EngineCore_DP0 pid=1288312) [2026-01-26 11:29:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15482880 bytes
(EngineCore_DP0 pid=1288312) [2026-01-26 11:29:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5376] -> 1D uint8
(EngineCore_DP0 pid=1288312) [2026-01-26 11:29:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12042240 bytes
(EngineCore_DP0 pid=1288312) [2026-01-26 11:29:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5376] -> 1D uint8
(EngineCore_DP0 pid=1288312) [2026-01-26 11:29:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 127303680 bytes
(EngineCore_DP0 pid=1288312) [2026-01-26 11:29:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 28416] -> 1D uint8
(EngineCore_DP0 pid=1288312) [2026-01-26 11:29:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 63651840 bytes
(EngineCore_DP0 pid=1288312) [rank0]:W0126 11:29:54.941000 1288312 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1288312) [rank0]:W0126 11:29:55.020000 1288312 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1288312) [rank0]:W0126 11:29:56.266000 1288312 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1288312) [rank0]:W0126 11:29:56.393000 1288312 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1288312) 2026-01-26 11:30:00,170 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1288312) 2026-01-26 11:30:00,330 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1288312) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▉         | 1/11 [00:00<00:03,  2.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:02,  4.39it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:00<00:01,  6.58it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 6/11 [00:00<00:00,  9.24it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:00<00:00, 11.80it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 10/11 [00:01<00:00, 13.84it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00,  8.43it/s]
(EngineCore_DP0 pid=1288312) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 1/7 [00:00<00:03,  1.95it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:01,  3.49it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00,  7.06it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00,  8.13it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:01<00:00,  6.96it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 25/4096 [00:00<00:16, 245.65it/s]
Adding requests:   2%|▏         | 65/4096 [00:00<00:12, 334.34it/s]
Adding requests:   2%|▏         | 101/4096 [00:00<00:11, 343.83it/s]
Adding requests:   3%|▎         | 139/4096 [00:00<00:11, 357.26it/s]
Adding requests:   4%|▍         | 178/4096 [00:00<00:10, 368.50it/s]
Adding requests:   5%|▌         | 220/4096 [00:00<00:10, 384.40it/s]
Adding requests:   6%|▋         | 259/4096 [00:00<00:10, 379.41it/s]
Adding requests:   7%|▋         | 299/4096 [00:00<00:09, 384.44it/s]
Adding requests:   8%|▊         | 340/4096 [00:00<00:09, 389.95it/s]
Adding requests:   9%|▉         | 380/4096 [00:01<00:09, 389.74it/s]
Adding requests:  10%|█         | 422/4096 [00:01<00:09, 397.38it/s]
Adding requests:  11%|█▏        | 462/4096 [00:01<00:09, 394.20it/s]
Adding requests:  12%|█▏        | 504/4096 [00:01<00:08, 401.88it/s]
Adding requests:  13%|█▎        | 546/4096 [00:01<00:08, 406.38it/s]
Adding requests:  14%|█▍        | 587/4096 [00:01<00:08, 401.36it/s]
Adding requests:  15%|█▌        | 628/4096 [00:01<00:08, 396.33it/s]
Adding requests:  16%|█▋        | 668/4096 [00:01<00:08, 385.67it/s]
Adding requests:  17%|█▋        | 709/4096 [00:01<00:08, 390.06it/s]
Adding requests:  18%|█▊        | 749/4096 [00:01<00:08, 382.13it/s]
Adding requests:  19%|█▉        | 789/4096 [00:02<00:08, 385.42it/s]
Adding requests:  20%|██        | 828/4096 [00:02<00:08, 383.84it/s]
Adding requests:  21%|██        | 868/4096 [00:02<00:08, 387.16it/s]
Adding requests:  22%|██▏       | 909/4096 [00:02<00:08, 391.46it/s]
Adding requests:  23%|██▎       | 949/4096 [00:02<00:08, 382.68it/s]
Adding requests:  24%|██▍       | 989/4096 [00:02<00:08, 385.51it/s]
Adding requests:  25%|██▌       | 1028/4096 [00:02<00:08, 382.08it/s]
Adding requests:  26%|██▌       | 1067/4096 [00:02<00:07, 380.01it/s]
Adding requests:  27%|██▋       | 1106/4096 [00:02<00:08, 369.95it/s]
Adding requests:  28%|██▊       | 1146/4096 [00:02<00:07, 377.61it/s]
Adding requests:  29%|██▉       | 1184/4096 [00:03<00:07, 377.67it/s]
Adding requests:  30%|██▉       | 1225/4096 [00:03<00:07, 383.88it/s]
Adding requests:  31%|███       | 1264/4096 [00:03<00:07, 379.69it/s]
Adding requests:  32%|███▏      | 1303/4096 [00:03<00:07, 378.68it/s]
Adding requests:  33%|███▎      | 1341/4096 [00:03<00:07, 377.96it/s]
Adding requests:  34%|███▎      | 1382/4096 [00:03<00:07, 384.34it/s]
Adding requests:  35%|███▍      | 1421/4096 [00:03<00:07, 381.10it/s]
Adding requests:  36%|███▌      | 1460/4096 [00:03<00:06, 382.44it/s]
Adding requests:  37%|███▋      | 1501/4096 [00:03<00:06, 388.39it/s]
Adding requests:  38%|███▊      | 1540/4096 [00:04<00:06, 388.31it/s]
Adding requests:  39%|███▊      | 1579/4096 [00:04<00:06, 376.03it/s]
Adding requests:  39%|███▉      | 1617/4096 [00:04<00:06, 375.34it/s]
Adding requests:  40%|████      | 1655/4096 [00:04<00:06, 369.73it/s]
Adding requests:  41%|████▏     | 1693/4096 [00:04<00:06, 371.89it/s]
Adding requests:  42%|████▏     | 1733/4096 [00:04<00:06, 378.21it/s]
Adding requests:  43%|████▎     | 1774/4096 [00:04<00:06, 385.83it/s]
Adding requests:  44%|████▍     | 1813/4096 [00:04<00:05, 383.68it/s]
Adding requests:  45%|████▌     | 1852/4096 [00:04<00:05, 383.91it/s]
Adding requests:  46%|████▌     | 1892/4096 [00:04<00:05, 388.35it/s]
Adding requests:  47%|████▋     | 1933/4096 [00:05<00:05, 392.67it/s]
Adding requests:  48%|████▊     | 1973/4096 [00:05<00:05, 392.95it/s]
Adding requests:  49%|████▉     | 2013/4096 [00:05<00:05, 388.80it/s]
Adding requests:  50%|█████     | 2052/4096 [00:05<00:05, 382.30it/s]
Adding requests:  51%|█████     | 2091/4096 [00:05<00:05, 374.49it/s]
Adding requests:  52%|█████▏    | 2131/4096 [00:05<00:05, 380.44it/s]
Adding requests:  53%|█████▎    | 2170/4096 [00:05<00:05, 376.14it/s]
Adding requests:  54%|█████▍    | 2208/4096 [00:05<00:05, 371.89it/s]
Adding requests:  55%|█████▍    | 2247/4096 [00:05<00:04, 376.46it/s]
Adding requests:  56%|█████▌    | 2288/4096 [00:05<00:04, 385.76it/s]
Adding requests:  57%|█████▋    | 2327/4096 [00:06<00:04, 380.74it/s]
Adding requests:  58%|█████▊    | 2367/4096 [00:06<00:04, 385.14it/s]
Adding requests:  59%|█████▉    | 2408/4096 [00:06<00:04, 390.99it/s]
Adding requests:  60%|█████▉    | 2448/4096 [00:06<00:04, 392.14it/s]
Adding requests:  61%|██████    | 2488/4096 [00:06<00:04, 392.90it/s]
Adding requests:  62%|██████▏   | 2529/4096 [00:06<00:03, 396.33it/s]
Adding requests:  63%|██████▎   | 2572/4096 [00:06<00:03, 405.71it/s]
Adding requests:  64%|██████▍   | 2613/4096 [00:06<00:03, 402.47it/s]
Adding requests:  65%|██████▍   | 2654/4096 [00:06<00:03, 392.95it/s]
Adding requests:  66%|██████▌   | 2694/4096 [00:07<00:03, 389.87it/s]
Adding requests:  67%|██████▋   | 2734/4096 [00:07<00:03, 387.38it/s]
Adding requests:  68%|██████▊   | 2775/4096 [00:07<00:03, 393.40it/s]
Adding requests:  69%|██████▉   | 2818/4096 [00:07<00:03, 401.34it/s]
Adding requests:  70%|██████▉   | 2859/4096 [00:07<00:03, 399.41it/s]
Adding requests:  71%|███████   | 2899/4096 [00:07<00:03, 396.93it/s]
Adding requests:  72%|███████▏  | 2940/4096 [00:07<00:02, 399.39it/s]
Adding requests:  73%|███████▎  | 2980/4096 [00:07<00:02, 398.52it/s]
Adding requests:  74%|███████▍  | 3021/4096 [00:07<00:02, 400.33it/s]
Adding requests:  75%|███████▍  | 3063/4096 [00:07<00:02, 404.32it/s]
Adding requests:  76%|███████▌  | 3104/4096 [00:08<00:02, 403.36it/s]
Adding requests:  77%|███████▋  | 3146/4096 [00:08<00:02, 404.86it/s]
Adding requests:  78%|███████▊  | 3187/4096 [00:08<00:02, 397.49it/s]
Adding requests:  79%|███████▉  | 3227/4096 [00:08<00:02, 396.10it/s]
Adding requests:  80%|███████▉  | 3267/4096 [00:08<00:02, 392.10it/s]
Adding requests:  81%|████████  | 3307/4096 [00:08<00:02, 382.69it/s]
Adding requests:  82%|████████▏ | 3347/4096 [00:08<00:01, 385.43it/s]
Adding requests:  83%|████████▎ | 3388/4096 [00:08<00:01, 390.61it/s]
Adding requests:  84%|████████▎ | 3428/4096 [00:08<00:01, 390.47it/s]
Adding requests:  85%|████████▍ | 3469/4096 [00:08<00:01, 393.99it/s]
Adding requests:  86%|████████▌ | 3509/4096 [00:09<00:01, 395.44it/s]
Adding requests:  87%|████████▋ | 3550/4096 [00:09<00:01, 399.24it/s]
Adding requests:  88%|████████▊ | 3590/4096 [00:09<00:01, 396.34it/s]
Adding requests:  89%|████████▊ | 3632/4096 [00:09<00:01, 401.27it/s]
Adding requests:  90%|████████▉ | 3673/4096 [00:09<00:01, 379.76it/s]
Adding requests:  91%|█████████ | 3712/4096 [00:09<00:01, 381.18it/s]
Adding requests:  92%|█████████▏| 3751/4096 [00:09<00:00, 380.65it/s]
Adding requests:  93%|█████████▎| 3790/4096 [00:09<00:00, 371.44it/s]
Adding requests:  93%|█████████▎| 3828/4096 [00:09<00:00, 370.20it/s]
Adding requests:  94%|█████████▍| 3867/4096 [00:10<00:00, 375.74it/s]
Adding requests:  95%|█████████▌| 3905/4096 [00:10<00:00, 372.99it/s]
Adding requests:  96%|█████████▋| 3943/4096 [00:10<00:00, 372.38it/s]
Adding requests:  97%|█████████▋| 3981/4096 [00:10<00:00, 374.06it/s]
Adding requests:  98%|█████████▊| 4020/4096 [00:10<00:00, 376.14it/s]
Adding requests:  99%|█████████▉| 4058/4096 [00:10<00:00, 375.08it/s]
Adding requests: 100%|██████████| 4096/4096 [00:10<00:00, 375.28it/s]
Adding requests: 100%|██████████| 4096/4096 [00:10<00:00, 385.23it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  10%|█         | 424/4096 [00:00<00:03, 1174.16it/s, est. speed input: 1202481.83 toks/s, output: 1174.20 toks/s]
Processed prompts:  13%|█▎        | 542/4096 [00:02<00:22, 161.02it/s, est. speed input: 206750.81 toks/s, output: 201.90 toks/s]   
Processed prompts:  15%|█▍        | 594/4096 [00:04<00:34, 102.08it/s, est. speed input: 143628.58 toks/s, output: 140.26 toks/s]
Processed prompts:  15%|█▌        | 624/4096 [00:05<00:40, 86.39it/s, est. speed input: 127573.18 toks/s, output: 124.58 toks/s] 
Processed prompts:  16%|█▌        | 648/4096 [00:05<00:47, 71.89it/s, est. speed input: 114736.71 toks/s, output: 112.05 toks/s]
Processed prompts:  17%|█▋        | 680/4096 [00:06<00:53, 63.52it/s, est. speed input: 106146.09 toks/s, output: 103.66 toks/s]
Processed prompts:  17%|█▋        | 712/4096 [00:07<00:59, 57.30it/s, est. speed input: 99402.83 toks/s, output: 97.07 toks/s]  
Processed prompts:  18%|█▊        | 744/4096 [00:08<01:03, 52.71it/s, est. speed input: 93929.88 toks/s, output: 91.73 toks/s]
Processed prompts:  19%|█▉        | 776/4096 [00:08<01:06, 49.75it/s, est. speed input: 89613.36 toks/s, output: 87.51 toks/s]
Processed prompts:  20%|█▉        | 808/4096 [00:09<01:09, 47.25it/s, est. speed input: 85803.15 toks/s, output: 83.79 toks/s]
Processed prompts:  21%|██        | 840/4096 [00:10<01:11, 45.47it/s, est. speed input: 82560.67 toks/s, output: 80.63 toks/s]
Processed prompts:  21%|██▏       | 872/4096 [00:11<01:12, 44.20it/s, est. speed input: 79757.52 toks/s, output: 77.89 toks/s]
Processed prompts:  22%|██▏       | 904/4096 [00:11<01:13, 43.28it/s, est. speed input: 77312.15 toks/s, output: 75.50 toks/s]
Processed prompts:  23%|██▎       | 936/4096 [00:12<01:14, 42.65it/s, est. speed input: 75169.68 toks/s, output: 73.41 toks/s]
Processed prompts:  24%|██▎       | 968/4096 [00:13<01:14, 42.20it/s, est. speed input: 73270.42 toks/s, output: 71.55 toks/s]
Processed prompts:  24%|██▍       | 1000/4096 [00:14<01:13, 41.84it/s, est. speed input: 71563.72 toks/s, output: 69.89 toks/s]
Processed prompts:  25%|██▌       | 1032/4096 [00:15<01:13, 41.61it/s, est. speed input: 70040.26 toks/s, output: 68.40 toks/s]
Processed prompts:  26%|██▌       | 1064/4096 [00:15<01:13, 41.45it/s, est. speed input: 68665.63 toks/s, output: 67.06 toks/s]
Processed prompts:  27%|██▋       | 1096/4096 [00:16<01:12, 41.31it/s, est. speed input: 67414.07 toks/s, output: 65.83 toks/s]
Processed prompts:  28%|██▊       | 1128/4096 [00:17<01:11, 41.24it/s, est. speed input: 66280.60 toks/s, output: 64.73 toks/s]
Processed prompts:  28%|██▊       | 1160/4096 [00:18<01:11, 41.19it/s, est. speed input: 65244.43 toks/s, output: 63.72 toks/s]
Processed prompts:  29%|██▉       | 1192/4096 [00:18<01:10, 41.47it/s, est. speed input: 64359.99 toks/s, output: 62.85 toks/s]
Processed prompts:  30%|██▉       | 1224/4096 [00:19<01:08, 41.69it/s, est. speed input: 63548.11 toks/s, output: 62.06 toks/s]
Processed prompts:  31%|███       | 1256/4096 [00:20<01:08, 41.49it/s, est. speed input: 62728.02 toks/s, output: 61.26 toks/s]
Processed prompts:  31%|███▏      | 1288/4096 [00:21<01:08, 41.29it/s, est. speed input: 61957.52 toks/s, output: 60.51 toks/s]
Processed prompts:  32%|███▏      | 1320/4096 [00:22<01:06, 41.49it/s, est. speed input: 61301.62 toks/s, output: 59.86 toks/s]
Processed prompts:  33%|███▎      | 1352/4096 [00:22<01:06, 41.36it/s, est. speed input: 60644.05 toks/s, output: 59.22 toks/s]
Processed prompts:  34%|███▍      | 1384/4096 [00:23<01:05, 41.24it/s, est. speed input: 60024.70 toks/s, output: 58.62 toks/s]
Processed prompts:  35%|███▍      | 1416/4096 [00:24<01:05, 41.14it/s, est. speed input: 59444.00 toks/s, output: 58.05 toks/s]
Processed prompts:  35%|███▌      | 1448/4096 [00:25<01:03, 41.40it/s, est. speed input: 58946.42 toks/s, output: 57.56 toks/s]
Processed prompts:  36%|███▌      | 1480/4096 [00:25<01:03, 41.26it/s, est. speed input: 58433.53 toks/s, output: 57.06 toks/s]
Processed prompts:  37%|███▋      | 1512/4096 [00:26<01:02, 41.50it/s, est. speed input: 57996.32 toks/s, output: 56.64 toks/s]
Processed prompts:  38%|███▊      | 1544/4096 [00:27<01:01, 41.68it/s, est. speed input: 57584.01 toks/s, output: 56.23 toks/s]
Processed prompts:  38%|███▊      | 1576/4096 [00:28<01:00, 41.45it/s, est. speed input: 57149.48 toks/s, output: 55.81 toks/s]
Processed prompts:  39%|███▉      | 1608/4096 [00:28<00:59, 41.65it/s, est. speed input: 56782.71 toks/s, output: 55.45 toks/s]
Processed prompts:  40%|████      | 1640/4096 [00:29<00:59, 41.44it/s, est. speed input: 56393.77 toks/s, output: 55.07 toks/s]
Processed prompts:  41%|████      | 1672/4096 [00:30<00:58, 41.32it/s, est. speed input: 56026.55 toks/s, output: 54.71 toks/s]
Processed prompts:  42%|████▏     | 1704/4096 [00:31<00:58, 41.17it/s, est. speed input: 55671.13 toks/s, output: 54.37 toks/s]
Processed prompts:  42%|████▏     | 1736/4096 [00:32<00:56, 41.83it/s, est. speed input: 55415.04 toks/s, output: 54.12 toks/s]
Processed prompts:  43%|████▎     | 1768/4096 [00:32<00:56, 41.53it/s, est. speed input: 55091.20 toks/s, output: 53.80 toks/s]
Processed prompts:  44%|████▍     | 1800/4096 [00:33<00:55, 41.34it/s, est. speed input: 54784.26 toks/s, output: 53.50 toks/s]
Processed prompts:  45%|████▍     | 1832/4096 [00:34<00:54, 41.19it/s, est. speed input: 54488.82 toks/s, output: 53.21 toks/s]
Processed prompts:  46%|████▌     | 1864/4096 [00:35<00:53, 41.42it/s, est. speed input: 54239.57 toks/s, output: 52.97 toks/s]
Processed prompts:  46%|████▋     | 1896/4096 [00:35<00:53, 41.23it/s, est. speed input: 53968.00 toks/s, output: 52.70 toks/s]
Processed prompts:  47%|████▋     | 1928/4096 [00:36<00:52, 41.13it/s, est. speed input: 53710.83 toks/s, output: 52.45 toks/s]
Processed prompts:  48%|████▊     | 1960/4096 [00:37<00:51, 41.38it/s, est. speed input: 53492.59 toks/s, output: 52.24 toks/s]
Processed prompts:  49%|████▊     | 1992/4096 [00:38<00:51, 41.19it/s, est. speed input: 53251.43 toks/s, output: 52.00 toks/s]
Processed prompts:  49%|████▉     | 2024/4096 [00:39<00:50, 41.04it/s, est. speed input: 53018.65 toks/s, output: 51.78 toks/s]
Processed prompts:  50%|█████     | 2056/4096 [00:39<00:49, 41.30it/s, est. speed input: 52825.07 toks/s, output: 51.59 toks/s]
Processed prompts:  51%|█████     | 2088/4096 [00:40<00:48, 41.15it/s, est. speed input: 52612.11 toks/s, output: 51.38 toks/s]
Processed prompts:  52%|█████▏    | 2120/4096 [00:41<00:48, 41.06it/s, est. speed input: 52408.18 toks/s, output: 51.18 toks/s]
Processed prompts:  53%|█████▎    | 2152/4096 [00:42<00:47, 40.96it/s, est. speed input: 52208.72 toks/s, output: 50.99 toks/s]
Processed prompts:  53%|█████▎    | 2184/4096 [00:42<00:46, 41.20it/s, est. speed input: 52040.36 toks/s, output: 50.82 toks/s]
Processed prompts:  54%|█████▍    | 2216/4096 [00:43<00:45, 41.05it/s, est. speed input: 51854.26 toks/s, output: 50.64 toks/s]
Processed prompts:  55%|█████▍    | 2248/4096 [00:44<00:45, 40.94it/s, est. speed input: 51674.14 toks/s, output: 50.46 toks/s]
Processed prompts:  56%|█████▌    | 2280/4096 [00:45<00:44, 40.90it/s, est. speed input: 51503.07 toks/s, output: 50.30 toks/s]
Processed prompts:  56%|█████▋    | 2312/4096 [00:46<00:43, 40.83it/s, est. speed input: 51335.31 toks/s, output: 50.13 toks/s]
Processed prompts:  57%|█████▋    | 2344/4096 [00:46<00:42, 40.80it/s, est. speed input: 51174.20 toks/s, output: 49.97 toks/s]
Processed prompts:  58%|█████▊    | 2376/4096 [00:47<00:42, 40.73it/s, est. speed input: 51014.78 toks/s, output: 49.82 toks/s]
Processed prompts:  59%|█████▉    | 2408/4096 [00:48<00:41, 40.72it/s, est. speed input: 50862.92 toks/s, output: 49.67 toks/s]
Processed prompts:  60%|█████▉    | 2440/4096 [00:49<00:40, 40.70it/s, est. speed input: 50715.73 toks/s, output: 49.53 toks/s]
Processed prompts:  60%|██████    | 2472/4096 [00:50<00:39, 40.70it/s, est. speed input: 50573.40 toks/s, output: 49.39 toks/s]
Processed prompts:  61%|██████    | 2504/4096 [00:50<00:39, 40.70it/s, est. speed input: 50435.94 toks/s, output: 49.25 toks/s]
Processed prompts:  62%|██████▏   | 2536/4096 [00:51<00:38, 41.03it/s, est. speed input: 50322.90 toks/s, output: 49.14 toks/s]
Processed prompts:  63%|██████▎   | 2568/4096 [00:52<00:37, 40.90it/s, est. speed input: 50191.18 toks/s, output: 49.01 toks/s]
Processed prompts:  63%|██████▎   | 2600/4096 [00:53<00:36, 41.13it/s, est. speed input: 50082.54 toks/s, output: 48.91 toks/s]
Processed prompts:  64%|██████▍   | 2632/4096 [00:53<00:35, 40.99it/s, est. speed input: 49959.49 toks/s, output: 48.79 toks/s]
Processed prompts:  65%|██████▌   | 2664/4096 [00:54<00:35, 40.89it/s, est. speed input: 49839.85 toks/s, output: 48.67 toks/s]
Processed prompts:  66%|██████▌   | 2696/4096 [00:55<00:34, 40.80it/s, est. speed input: 49722.48 toks/s, output: 48.56 toks/s]
Processed prompts:  67%|██████▋   | 2728/4096 [00:56<00:33, 41.07it/s, est. speed input: 49627.14 toks/s, output: 48.46 toks/s]
Processed prompts:  67%|██████▋   | 2760/4096 [00:57<00:32, 40.92it/s, est. speed input: 49515.32 toks/s, output: 48.35 toks/s]
Processed prompts:  68%|██████▊   | 2792/4096 [00:57<00:31, 40.86it/s, est. speed input: 49408.98 toks/s, output: 48.25 toks/s]
Processed prompts:  69%|██████▉   | 2824/4096 [00:58<00:31, 40.78it/s, est. speed input: 49303.93 toks/s, output: 48.15 toks/s]
Processed prompts:  70%|██████▉   | 2856/4096 [00:59<00:30, 40.73it/s, est. speed input: 49201.65 toks/s, output: 48.05 toks/s]
Processed prompts:  71%|███████   | 2888/4096 [01:00<00:29, 41.42it/s, est. speed input: 49139.36 toks/s, output: 47.99 toks/s]
Processed prompts:  71%|███████▏  | 2920/4096 [01:00<00:28, 41.54it/s, est. speed input: 49060.27 toks/s, output: 47.91 toks/s]
Processed prompts:  72%|███████▏  | 2952/4096 [01:01<00:27, 41.21it/s, est. speed input: 48962.25 toks/s, output: 47.81 toks/s]
Processed prompts:  73%|███████▎  | 2984/4096 [01:02<00:27, 41.03it/s, est. speed input: 48869.54 toks/s, output: 47.72 toks/s]
Processed prompts:  74%|███████▎  | 3016/4096 [01:03<00:26, 40.85it/s, est. speed input: 48776.47 toks/s, output: 47.63 toks/s]
Processed prompts:  74%|███████▍  | 3048/4096 [01:04<00:25, 40.80it/s, est. speed input: 48688.81 toks/s, output: 47.55 toks/s]
Processed prompts:  75%|███████▌  | 3080/4096 [01:04<00:24, 40.73it/s, est. speed input: 48601.98 toks/s, output: 47.46 toks/s]
Processed prompts:  76%|███████▌  | 3112/4096 [01:05<00:24, 40.62it/s, est. speed input: 48514.45 toks/s, output: 47.38 toks/s]
Processed prompts:  77%|███████▋  | 3144/4096 [01:06<00:23, 40.57it/s, est. speed input: 48430.02 toks/s, output: 47.29 toks/s]
Processed prompts:  78%|███████▊  | 3176/4096 [01:07<00:22, 40.55it/s, est. speed input: 48348.38 toks/s, output: 47.22 toks/s]
Processed prompts:  78%|███████▊  | 3208/4096 [01:08<00:21, 40.54it/s, est. speed input: 48268.51 toks/s, output: 47.14 toks/s]
Processed prompts:  79%|███████▉  | 3240/4096 [01:08<00:21, 40.52it/s, est. speed input: 48190.50 toks/s, output: 47.06 toks/s]
Processed prompts:  80%|███████▉  | 3272/4096 [01:09<00:20, 40.51it/s, est. speed input: 48114.18 toks/s, output: 46.99 toks/s]
Processed prompts:  81%|████████  | 3304/4096 [01:10<00:19, 40.51it/s, est. speed input: 48039.61 toks/s, output: 46.91 toks/s]
Processed prompts:  81%|████████▏ | 3336/4096 [01:11<00:18, 40.47it/s, est. speed input: 47965.10 toks/s, output: 46.84 toks/s]
Processed prompts:  82%|████████▏ | 3368/4096 [01:12<00:17, 40.47it/s, est. speed input: 47893.56 toks/s, output: 46.77 toks/s]
Processed prompts:  83%|████████▎ | 3400/4096 [01:12<00:17, 40.47it/s, est. speed input: 47823.41 toks/s, output: 46.70 toks/s]
Processed prompts:  84%|████████▍ | 3432/4096 [01:13<00:16, 40.46it/s, est. speed input: 47754.44 toks/s, output: 46.64 toks/s]
Processed prompts:  85%|████████▍ | 3464/4096 [01:14<00:15, 40.47it/s, est. speed input: 47687.55 toks/s, output: 46.57 toks/s]
Processed prompts:  85%|████████▌ | 3496/4096 [01:15<00:14, 40.50it/s, est. speed input: 47623.00 toks/s, output: 46.51 toks/s]
Processed prompts:  86%|████████▌ | 3528/4096 [01:15<00:14, 40.52it/s, est. speed input: 47559.88 toks/s, output: 46.45 toks/s]
Processed prompts:  87%|████████▋ | 3560/4096 [01:16<00:13, 40.86it/s, est. speed input: 47511.02 toks/s, output: 46.40 toks/s]
Processed prompts:  88%|████████▊ | 3592/4096 [01:17<00:12, 40.75it/s, est. speed input: 47449.41 toks/s, output: 46.34 toks/s]
Processed prompts:  88%|████████▊ | 3624/4096 [01:18<00:11, 40.68it/s, est. speed input: 47389.22 toks/s, output: 46.28 toks/s]
Processed prompts:  89%|████████▉ | 3656/4096 [01:19<00:10, 40.59it/s, est. speed input: 47328.65 toks/s, output: 46.22 toks/s]
Processed prompts:  90%|█████████ | 3688/4096 [01:19<00:09, 40.90it/s, est. speed input: 47283.79 toks/s, output: 46.18 toks/s]
Processed prompts:  91%|█████████ | 3720/4096 [01:20<00:09, 40.77it/s, est. speed input: 47226.58 toks/s, output: 46.12 toks/s]
Processed prompts:  92%|█████████▏| 3752/4096 [01:21<00:08, 40.68it/s, est. speed input: 47170.21 toks/s, output: 46.06 toks/s]
Processed prompts:  92%|█████████▏| 3784/4096 [01:22<00:07, 40.58it/s, est. speed input: 47113.96 toks/s, output: 46.01 toks/s]
Processed prompts:  93%|█████████▎| 3816/4096 [01:23<00:06, 40.56it/s, est. speed input: 47060.26 toks/s, output: 45.96 toks/s]
Processed prompts:  94%|█████████▍| 3848/4096 [01:23<00:06, 40.53it/s, est. speed input: 47007.24 toks/s, output: 45.91 toks/s]
Processed prompts:  95%|█████████▍| 3880/4096 [01:24<00:05, 40.49it/s, est. speed input: 46954.42 toks/s, output: 45.85 toks/s]
Processed prompts:  96%|█████████▌| 3912/4096 [01:25<00:04, 40.83it/s, est. speed input: 46915.63 toks/s, output: 45.82 toks/s]
Processed prompts:  96%|█████████▋| 3944/4096 [01:26<00:03, 41.03it/s, est. speed input: 46876.22 toks/s, output: 45.78 toks/s]
Processed prompts:  97%|█████████▋| 3976/4096 [01:26<00:02, 40.84it/s, est. speed input: 46825.96 toks/s, output: 45.73 toks/s]
Processed prompts:  98%|█████████▊| 4008/4096 [01:27<00:02, 41.06it/s, est. speed input: 46788.82 toks/s, output: 45.69 toks/s]
Processed prompts:  99%|█████████▊| 4040/4096 [01:28<00:01, 41.21it/s, est. speed input: 46752.09 toks/s, output: 45.66 toks/s]
Processed prompts:  99%|█████████▉| 4072/4096 [01:29<00:00, 44.28it/s, est. speed input: 46806.45 toks/s, output: 45.71 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [01:29<00:00, 44.28it/s, est. speed input: 47081.84 toks/s, output: 45.98 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [01:29<00:00, 45.98it/s, est. speed input: 47081.84 toks/s, output: 45.98 toks/s]
[rank0]:[W126 11:31:44.306171132 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 162.1s

测试结果:
  Requests/s:   41.07
  Tokens/s:     42101.12
  Total Reqs:   4096
  Elapsed:      99.72s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     42060.05

============================================================
[8/8] 测试 M=65536
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 65536
│   M_prefill     = 65536 (= 64 x 1024)
│   M_decode      = 64
│   batched_tokens = 65536 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 8192
│   --max-num-seqs           = 64
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 65536
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 11:32:37 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1291221) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1291221) WARNING 01-26 11:32:54 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]     self.model_runner.profile_run()
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]     hidden_states = self.model(
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]                     ^^^^^^^^^^^
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]     def forward(
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]     raise e
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]   File "<eval_with_key>.58", line 325, in forward
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]     return range_entry.runnable(*args)
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]     return self._compiled_fn(*args)
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]     return compiled_fn(full_args)
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]                             ^^^^^^^
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 724, in inner_fn
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]     outs = compiled_fn(args)
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]            ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]     return self.current_callable(inputs)
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]     out = model(new_inputs)
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]   File "/tmp/torchinductor_root/bq/cbqmruv4oxk6jb5mn7bgysrw4nuowq772awe77lv4scsgx64bxce.py", line 1093, in call
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]     buf17 = torch.ops.slidesparse.quant_slide_int8.default(buf16, 'Qwen2.5-7B-INT8', 8)
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 564, in _quant_slide_int8_impl
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]     return fn(input, L)
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]            ^^^^^^^^^^^^
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]   File "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/H100_cc90_py312_cu129_x86_64/quant_slide_tuned_Qwen2.5-7B.py", line 339, in quant_slide_int8_triton
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]     _quant_slide_int8_kernel[(M,)](
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 756, in run
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]     launch_metadata = kernel.launch_metadata(grid, stream, *bound_args.values())
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 490, in launch_metadata
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]     self._init_handles()
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 473, in _init_handles
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]     self.module, self.function, self.n_regs, self.n_spills, self.n_max_threads = driver.active.utils.load_binary(
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866]                                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1291221) ERROR 01-26 11:33:02 [core.py:866] RuntimeError: Triton Error [CUDA]: an illegal memory access was encountered


─── STDERR ───
[2026-01-26 11:32:36] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:32:36] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:32:36] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 11:32:36] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:32:36] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:32:36] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:32:36] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:32:36] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:32:36] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:32:36] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:32:36] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:32:36] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:32:36] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:32:36] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 11:32:43] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:32:44] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:32:44] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 11:32:44] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:32:44] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:32:44] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:32:44] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:32:44] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:32:44] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:32:44] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:32:44] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:32:44] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:32:44] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:32:44] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1291221) [2026-01-26 11:32:46] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1291221) [2026-01-26 11:32:46] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1291221) [2026-01-26 11:32:46] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1291221) [2026-01-26 11:32:46] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=1291221) [2026-01-26 11:32:46] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=1291221) [2026-01-26 11:32:46] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=1291221) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1291221) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.95it/s]
(EngineCore_DP0 pid=1291221) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.39it/s]
(EngineCore_DP0 pid=1291221) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.45it/s]
(EngineCore_DP0 pid=1291221) 
(EngineCore_DP0 pid=1291221) [2026-01-26 11:32:48] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5376] -> 1D uint8
(EngineCore_DP0 pid=1291221) [2026-01-26 11:32:48] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15482880 bytes
(EngineCore_DP0 pid=1291221) [2026-01-26 11:32:48] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5376] -> 1D uint8
(EngineCore_DP0 pid=1291221) [2026-01-26 11:32:48] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12042240 bytes
(EngineCore_DP0 pid=1291221) [2026-01-26 11:32:48] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5376] -> 1D uint8
(EngineCore_DP0 pid=1291221) [2026-01-26 11:32:48] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 127303680 bytes
(EngineCore_DP0 pid=1291221) [2026-01-26 11:32:48] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 28416] -> 1D uint8
(EngineCore_DP0 pid=1291221) [2026-01-26 11:32:48] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 63651840 bytes
(EngineCore_DP0 pid=1291221) [rank0]:W0126 11:32:59.696000 1291221 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1291221) [rank0]:W0126 11:32:59.774000 1291221 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1291221) [rank0]:W0126 11:33:01.125000 1291221 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1291221) [rank0]:W0126 11:33:01.247000 1291221 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1291221) Process EngineCore_DP0:
(EngineCore_DP0 pid=1291221) Traceback (most recent call last):
(EngineCore_DP0 pid=1291221)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=1291221)     self.run()
(EngineCore_DP0 pid=1291221)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=1291221)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=1291221)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=1291221)     raise e
(EngineCore_DP0 pid=1291221)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1291221)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1291221)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1291221)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1291221)     super().__init__(
(EngineCore_DP0 pid=1291221)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=1291221)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=1291221)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1291221)   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=1291221)     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=1291221)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1291221)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=1291221)     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=1291221)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1291221)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=1291221)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=1291221)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1291221)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=1291221)     return func(*args, **kwargs)
(EngineCore_DP0 pid=1291221)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1291221)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=1291221)     return func(*args, **kwargs)
(EngineCore_DP0 pid=1291221)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1291221)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=1291221)     self.model_runner.profile_run()
(EngineCore_DP0 pid=1291221)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=1291221)     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=1291221)                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1291221)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=1291221)     return func(*args, **kwargs)
(EngineCore_DP0 pid=1291221)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1291221)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=1291221)     outputs = self.model(
(EngineCore_DP0 pid=1291221)               ^^^^^^^^^^^
(EngineCore_DP0 pid=1291221)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=1291221)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=1291221)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1291221)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=1291221)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=1291221)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1291221)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=1291221)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=1291221)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1291221)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=1291221)     hidden_states = self.model(
(EngineCore_DP0 pid=1291221)                     ^^^^^^^^^^^
(EngineCore_DP0 pid=1291221)   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=1291221)     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=1291221)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1291221)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=1291221)     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=1291221)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1291221)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=1291221)     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=1291221)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1291221)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
(EngineCore_DP0 pid=1291221)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=1291221)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1291221)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
(EngineCore_DP0 pid=1291221)     def forward(
(EngineCore_DP0 pid=1291221)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=1291221)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=1291221)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1291221)   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=1291221)     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=1291221)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1291221)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=1291221)     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=1291221)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1291221)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=1291221)     raise e
(EngineCore_DP0 pid=1291221)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=1291221)     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=1291221)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1291221)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=1291221)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=1291221)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1291221)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=1291221)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=1291221)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1291221)   File "<eval_with_key>.58", line 325, in forward
(EngineCore_DP0 pid=1291221)     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
(EngineCore_DP0 pid=1291221)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1291221)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=1291221)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=1291221)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1291221)   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=1291221)     return range_entry.runnable(*args)
(EngineCore_DP0 pid=1291221)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1291221)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=1291221)     return self._compiled_fn(*args)
(EngineCore_DP0 pid=1291221)            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1291221)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=1291221)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=1291221)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1291221)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=1291221)     return compiled_fn(full_args)
(EngineCore_DP0 pid=1291221)            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1291221)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=1291221)     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=1291221)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1291221)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=1291221)     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=1291221)                             ^^^^^^^
(EngineCore_DP0 pid=1291221)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 724, in inner_fn
(EngineCore_DP0 pid=1291221)     outs = compiled_fn(args)
(EngineCore_DP0 pid=1291221)            ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1291221)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=1291221)     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=1291221)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1291221)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=1291221)     return self.current_callable(inputs)
(EngineCore_DP0 pid=1291221)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1291221)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=1291221)     out = model(new_inputs)
(EngineCore_DP0 pid=1291221)           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1291221)   File "/tmp/torchinductor_root/bq/cbqmruv4oxk6jb5mn7bgysrw4nuowq772awe77lv4scsgx64bxce.py", line 1093, in call
(EngineCore_DP0 pid=1291221)     buf17 = torch.ops.slidesparse.quant_slide_int8.default(buf16, 'Qwen2.5-7B-INT8', 8)
(EngineCore_DP0 pid=1291221)             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1291221)   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
(EngineCore_DP0 pid=1291221)     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=1291221)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1291221)   File "/root/vllmbench/slidesparse/core/kernels.py", line 564, in _quant_slide_int8_impl
(EngineCore_DP0 pid=1291221)     return fn(input, L)
(EngineCore_DP0 pid=1291221)            ^^^^^^^^^^^^
(EngineCore_DP0 pid=1291221)   File "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/H100_cc90_py312_cu129_x86_64/quant_slide_tuned_Qwen2.5-7B.py", line 339, in quant_slide_int8_triton
(EngineCore_DP0 pid=1291221)     _quant_slide_int8_kernel[(M,)](
(EngineCore_DP0 pid=1291221)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=1291221)     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=1291221)                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1291221)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 756, in run
(EngineCore_DP0 pid=1291221)     launch_metadata = kernel.launch_metadata(grid, stream, *bound_args.values())
(EngineCore_DP0 pid=1291221)                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1291221)   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 490, in launch_metadata
(EngineCore_DP0 pid=1291221)     self._init_handles()
(EngineCore_DP0 pid=1291221)   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 473, in _init_handles
(EngineCore_DP0 pid=1291221)     self.module, self.function, self.n_regs, self.n_spills, self.n_max_threads = driver.active.utils.load_binary(
(EngineCore_DP0 pid=1291221)                                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1291221) RuntimeError: Triton Error [CUDA]: an illegal memory access was encountered
[rank0]:[W126 11:33:02.968643628 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[ERROR] 测试失败: M=65536 (exit code: 1)


------------------------------------------------------------
  生成 CSV: Qwen2.5-7B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_INT8_py312_cu129_x86_64/cusparselt/2_8/Qwen2.5-7B-INT8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,35.2349,18075.4965,3.6328
1024,1024,1,128,128,32.7336,33551.9710,3.9104
2048,1024,2,256,128,36.8192,37739.6754,6.9529
4096,1024,4,512,128,39.1317,40110.0221,13.0840
8192,1024,8,1024,128,40.2734,41280.2472,25.4262
16384,1024,16,2048,128,41.1247,42152.7920,49.7998
32768,1024,32,4096,128,41.0743,42101.1205,99.7218
65536,1024,64,8192,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[INFO] 完成: 7 成功, 1 失败

============================================================
  Qwen2.5-7B-INT8 | cuSPARSELt (2_10) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_10
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_INT8_py312_cu129_x86_64/cusparselt/2_10

============================================================
[1/8] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 11:33:13 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1292207) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1292207) WARNING 01-26 11:33:29 [backends.py:609] Failed to read file <frozen os>
Throughput: 35.41 requests/s, 18163.89 total tokens/s, 35.41 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-26 11:33:12] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:33:13] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:33:13] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 11:33:13] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:33:13] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:33:13] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:33:13] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:33:13] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:33:13] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:33:13] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:33:13] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:33:13] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:33:13] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:33:13] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 11:33:19] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:33:20] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:33:20] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 11:33:20] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:33:20] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:33:20] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:33:20] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:33:20] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:33:20] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:33:20] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:33:20] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:33:20] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:33:20] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:33:20] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1292207) [2026-01-26 11:33:21] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1292207) [2026-01-26 11:33:21] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1292207) [2026-01-26 11:33:21] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1292207) [2026-01-26 11:33:21] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=1292207) [2026-01-26 11:33:21] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=1292207) [2026-01-26 11:33:21] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=1292207) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1292207) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.86it/s]
(EngineCore_DP0 pid=1292207) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.33it/s]
(EngineCore_DP0 pid=1292207) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.39it/s]
(EngineCore_DP0 pid=1292207) 
(EngineCore_DP0 pid=1292207) [2026-01-26 11:33:23] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=1292207) [2026-01-26 11:33:23] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16588800 bytes
(EngineCore_DP0 pid=1292207) [2026-01-26 11:33:23] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=1292207) [2026-01-26 11:33:23] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12902400 bytes
(EngineCore_DP0 pid=1292207) [2026-01-26 11:33:23] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=1292207) [2026-01-26 11:33:23] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 136396800 bytes
(EngineCore_DP0 pid=1292207) [2026-01-26 11:33:23] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=1292207) [2026-01-26 11:33:23] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 67952640 bytes
(EngineCore_DP0 pid=1292207) 2026-01-26 11:33:41,269 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1292207) 2026-01-26 11:33:41,321 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1292207) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  4.92it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  2.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  2.96it/s]
(EngineCore_DP0 pid=1292207) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  5.77it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  5.77it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  28%|██▊       | 36/128 [00:00<00:00, 355.68it/s]
Adding requests:  84%|████████▎ | 107/128 [00:00<00:00, 560.63it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 551.38it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:26,  4.75it/s, est. speed input: 2430.13 toks/s, output: 4.75 toks/s]
Processed prompts:   5%|▍         | 6/128 [00:00<00:05, 21.17it/s, est. speed input: 9242.11 toks/s, output: 18.05 toks/s]
Processed prompts:   9%|▊         | 11/128 [00:00<00:04, 29.07it/s, est. speed input: 12426.80 toks/s, output: 24.27 toks/s]
Processed prompts:  12%|█▎        | 16/128 [00:00<00:03, 33.38it/s, est. speed input: 14246.01 toks/s, output: 27.82 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:02, 35.98it/s, est. speed input: 15427.54 toks/s, output: 30.13 toks/s]
Processed prompts:  20%|██        | 26/128 [00:00<00:02, 37.36it/s, est. speed input: 16205.49 toks/s, output: 31.65 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:00<00:02, 38.20it/s, est. speed input: 16767.73 toks/s, output: 32.75 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:01<00:02, 38.66it/s, est. speed input: 17118.95 toks/s, output: 33.43 toks/s]
Processed prompts:  30%|███       | 39/128 [00:01<00:02, 38.96it/s, est. speed input: 17401.59 toks/s, output: 33.99 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:01<00:02, 39.22it/s, est. speed input: 17643.96 toks/s, output: 34.46 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:01<00:02, 39.43it/s, est. speed input: 17852.58 toks/s, output: 34.87 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:01<00:01, 39.48it/s, est. speed input: 18021.08 toks/s, output: 35.20 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:01<00:01, 39.57it/s, est. speed input: 18173.87 toks/s, output: 35.50 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:01<00:01, 39.64it/s, est. speed input: 18308.68 toks/s, output: 35.76 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:01<00:01, 39.72it/s, est. speed input: 18429.84 toks/s, output: 36.00 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:01<00:01, 39.77it/s, est. speed input: 18538.35 toks/s, output: 36.21 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:01<00:01, 39.80it/s, est. speed input: 18634.74 toks/s, output: 36.40 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:02<00:01, 39.82it/s, est. speed input: 18721.84 toks/s, output: 36.57 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:02<00:01, 39.79it/s, est. speed input: 18796.86 toks/s, output: 36.71 toks/s]
Processed prompts:  66%|██████▌   | 84/128 [00:02<00:01, 39.88it/s, est. speed input: 18890.74 toks/s, output: 36.90 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:02<00:00, 39.93it/s, est. speed input: 18974.40 toks/s, output: 37.06 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:02<00:00, 39.92it/s, est. speed input: 19032.41 toks/s, output: 37.17 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:02<00:00, 39.82it/s, est. speed input: 19080.06 toks/s, output: 37.27 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:02<00:00, 39.80it/s, est. speed input: 19127.52 toks/s, output: 37.36 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:02<00:00, 39.83it/s, est. speed input: 19173.94 toks/s, output: 37.45 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:02<00:00, 39.81it/s, est. speed input: 19215.27 toks/s, output: 37.53 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:03<00:00, 39.76it/s, est. speed input: 19251.16 toks/s, output: 37.60 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:03<00:00, 39.82it/s, est. speed input: 19290.23 toks/s, output: 37.68 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:03<00:00, 39.78it/s, est. speed input: 19322.67 toks/s, output: 37.74 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:03<00:00, 39.83it/s, est. speed input: 19356.84 toks/s, output: 37.81 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 39.83it/s, est. speed input: 19382.86 toks/s, output: 37.86 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 37.85it/s, est. speed input: 19382.86 toks/s, output: 37.86 toks/s]
[rank0]:[W126 11:33:47.622440410 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 43.5s

测试结果:
  Requests/s:   35.41
  Tokens/s:     18163.89
  Total Reqs:   128
  Elapsed:      3.62s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     18128.48

============================================================
[2/8] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 11:33:57 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1293376) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1293376) WARNING 01-26 11:34:13 [backends.py:609] Failed to read file <frozen os>
Throughput: 31.64 requests/s, 32428.30 total tokens/s, 31.64 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-26 11:33:56] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:33:57] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:33:57] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 11:33:57] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:33:57] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:33:57] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:33:57] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:33:57] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:33:57] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:33:57] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:33:57] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:33:57] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:33:57] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:33:57] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 11:34:03] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:34:03] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:34:03] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 11:34:03] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:34:04] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:34:04] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:34:04] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:34:04] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:34:04] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:34:04] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:34:04] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:34:04] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:34:04] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:34:04] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1293376) [2026-01-26 11:34:05] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1293376) [2026-01-26 11:34:05] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1293376) [2026-01-26 11:34:05] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1293376) [2026-01-26 11:34:05] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=1293376) [2026-01-26 11:34:05] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=1293376) [2026-01-26 11:34:05] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=1293376) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1293376) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.84it/s]
(EngineCore_DP0 pid=1293376) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.32it/s]
(EngineCore_DP0 pid=1293376) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.38it/s]
(EngineCore_DP0 pid=1293376) 
(EngineCore_DP0 pid=1293376) [2026-01-26 11:34:07] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=1293376) [2026-01-26 11:34:07] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16588800 bytes
(EngineCore_DP0 pid=1293376) [2026-01-26 11:34:07] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=1293376) [2026-01-26 11:34:07] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12902400 bytes
(EngineCore_DP0 pid=1293376) [2026-01-26 11:34:07] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=1293376) [2026-01-26 11:34:07] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 136396800 bytes
(EngineCore_DP0 pid=1293376) [2026-01-26 11:34:07] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=1293376) [2026-01-26 11:34:07] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 67952640 bytes
(EngineCore_DP0 pid=1293376) 2026-01-26 11:34:24,203 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1293376) 2026-01-26 11:34:24,227 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1293376) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  3.26it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  5.27it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  4.82it/s]
(EngineCore_DP0 pid=1293376) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 17.60it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  15%|█▍        | 19/128 [00:00<00:00, 186.16it/s]
Adding requests:  45%|████▍     | 57/128 [00:00<00:00, 297.13it/s]
Adding requests:  73%|███████▎  | 94/128 [00:00<00:00, 327.39it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 322.69it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   5%|▌         | 7/128 [00:00<00:01, 68.02it/s, est. speed input: 69663.40 toks/s, output: 68.02 toks/s]
Processed prompts:  11%|█         | 14/128 [00:00<00:02, 43.22it/s, est. speed input: 46825.41 toks/s, output: 45.72 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:00<00:02, 39.40it/s, est. speed input: 43067.84 toks/s, output: 42.06 toks/s]
Processed prompts:  19%|█▉        | 24/128 [00:00<00:02, 37.47it/s, est. speed input: 41141.98 toks/s, output: 40.18 toks/s]
Processed prompts:  22%|██▏       | 28/128 [00:00<00:02, 36.47it/s, est. speed input: 40132.49 toks/s, output: 39.19 toks/s]
Processed prompts:  25%|██▌       | 32/128 [00:00<00:02, 35.83it/s, est. speed input: 39439.77 toks/s, output: 38.51 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:00<00:02, 35.23it/s, est. speed input: 38846.49 toks/s, output: 37.93 toks/s]
Processed prompts:  31%|███▏      | 40/128 [00:01<00:02, 34.66it/s, est. speed input: 38322.74 toks/s, output: 37.42 toks/s]
Processed prompts:  34%|███▍      | 44/128 [00:01<00:02, 34.53it/s, est. speed input: 38000.38 toks/s, output: 37.11 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:01<00:02, 34.44it/s, est. speed input: 37735.34 toks/s, output: 36.85 toks/s]
Processed prompts:  41%|████      | 52/128 [00:01<00:02, 34.35it/s, est. speed input: 37504.68 toks/s, output: 36.62 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:01<00:02, 34.38it/s, est. speed input: 37336.75 toks/s, output: 36.46 toks/s]
Processed prompts:  47%|████▋     | 60/128 [00:01<00:01, 34.32it/s, est. speed input: 37172.71 toks/s, output: 36.30 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:01<00:01, 34.26it/s, est. speed input: 37023.99 toks/s, output: 36.16 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:01<00:01, 34.24it/s, est. speed input: 36899.06 toks/s, output: 36.03 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:02<00:01, 34.07it/s, est. speed input: 36756.56 toks/s, output: 35.89 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:02<00:01, 34.03it/s, est. speed input: 36644.72 toks/s, output: 35.79 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:02<00:01, 34.07it/s, est. speed input: 36557.32 toks/s, output: 35.70 toks/s]
Processed prompts:  66%|██████▌   | 84/128 [00:02<00:01, 34.12it/s, est. speed input: 36484.23 toks/s, output: 35.63 toks/s]
Processed prompts:  69%|██████▉   | 88/128 [00:02<00:01, 34.15it/s, est. speed input: 36415.08 toks/s, output: 35.56 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:02<00:01, 34.15it/s, est. speed input: 36349.65 toks/s, output: 35.50 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:02<00:00, 34.12it/s, est. speed input: 36285.57 toks/s, output: 35.43 toks/s]
Processed prompts:  78%|███████▊  | 100/128 [00:02<00:00, 34.13it/s, est. speed input: 36230.96 toks/s, output: 35.38 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:02<00:00, 34.05it/s, est. speed input: 36168.56 toks/s, output: 35.32 toks/s]
Processed prompts:  84%|████████▍ | 108/128 [00:03<00:00, 34.06it/s, est. speed input: 36119.85 toks/s, output: 35.27 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:03<00:00, 34.05it/s, est. speed input: 36072.43 toks/s, output: 35.23 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:03<00:00, 34.09it/s, est. speed input: 36035.21 toks/s, output: 35.19 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:03<00:00, 34.10it/s, est. speed input: 35997.12 toks/s, output: 35.15 toks/s]
Processed prompts:  97%|█████████▋| 124/128 [00:03<00:00, 34.13it/s, est. speed input: 35965.05 toks/s, output: 35.12 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 34.15it/s, est. speed input: 35934.77 toks/s, output: 35.09 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 34.15it/s, est. speed input: 35934.77 toks/s, output: 35.09 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 35.09it/s, est. speed input: 35934.77 toks/s, output: 35.09 toks/s]
[rank0]:[W126 11:34:30.557277335 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 42.9s

测试结果:
  Requests/s:   31.64
  Tokens/s:     32428.30
  Total Reqs:   128
  Elapsed:      4.05s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     32396.66

============================================================
[3/8] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 11:34:40 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1294490) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1294490) WARNING 01-26 11:34:58 [backends.py:609] Failed to read file <frozen os>
Throughput: 35.68 requests/s, 36571.99 total tokens/s, 35.68 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-26 11:34:39] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:34:40] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:34:40] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 11:34:40] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:34:40] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:34:40] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:34:40] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:34:40] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:34:40] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:34:40] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:34:40] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:34:40] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:34:40] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:34:40] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 11:34:47] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:34:47] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:34:47] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 11:34:47] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:34:47] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:34:47] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:34:47] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:34:47] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:34:47] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:34:47] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:34:47] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:34:47] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:34:47] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:34:47] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1294490) [2026-01-26 11:34:49] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1294490) [2026-01-26 11:34:49] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1294490) [2026-01-26 11:34:49] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1294490) [2026-01-26 11:34:49] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=1294490) [2026-01-26 11:34:49] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=1294490) [2026-01-26 11:34:49] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=1294490) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1294490) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.85it/s]
(EngineCore_DP0 pid=1294490) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.32it/s]
(EngineCore_DP0 pid=1294490) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.38it/s]
(EngineCore_DP0 pid=1294490) 
(EngineCore_DP0 pid=1294490) [2026-01-26 11:34:52] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=1294490) [2026-01-26 11:34:52] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16588800 bytes
(EngineCore_DP0 pid=1294490) [2026-01-26 11:34:52] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=1294490) [2026-01-26 11:34:52] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12902400 bytes
(EngineCore_DP0 pid=1294490) [2026-01-26 11:34:52] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=1294490) [2026-01-26 11:34:52] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 136396800 bytes
(EngineCore_DP0 pid=1294490) [2026-01-26 11:34:52] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=1294490) [2026-01-26 11:34:52] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 67952640 bytes
(EngineCore_DP0 pid=1294490) 2026-01-26 11:35:08,877 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1294490) 2026-01-26 11:35:08,921 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1294490) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:00<00:00, 16.03it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 15.49it/s]
(EngineCore_DP0 pid=1294490) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 1/2 [00:00<00:00,  6.51it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00,  8.82it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   8%|▊         | 20/256 [00:00<00:01, 199.84it/s]
Adding requests:  24%|██▍       | 61/256 [00:00<00:00, 320.22it/s]
Adding requests:  38%|███▊      | 98/256 [00:00<00:00, 339.32it/s]
Adding requests:  53%|█████▎    | 136/256 [00:00<00:00, 355.09it/s]
Adding requests:  69%|██████▉   | 176/256 [00:00<00:00, 368.66it/s]
Adding requests:  84%|████████▍ | 216/256 [00:00<00:00, 378.19it/s]
Adding requests: 100%|█████████▉| 255/256 [00:00<00:00, 380.47it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 360.03it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   8%|▊         | 20/256 [00:00<00:01, 180.91it/s, est. speed input: 185292.43 toks/s, output: 180.92 toks/s]
Processed prompts:  15%|█▌        | 39/256 [00:00<00:03, 58.96it/s, est. speed input: 67368.22 toks/s, output: 65.79 toks/s]   
Processed prompts:  19%|█▉        | 49/256 [00:00<00:04, 49.76it/s, est. speed input: 57927.10 toks/s, output: 56.57 toks/s]
Processed prompts:  22%|██▏       | 56/256 [00:01<00:04, 44.09it/s, est. speed input: 52856.48 toks/s, output: 51.62 toks/s]
Processed prompts:  24%|██▍       | 62/256 [00:01<00:04, 42.36it/s, est. speed input: 50950.75 toks/s, output: 49.76 toks/s]
Processed prompts:  26%|██▌       | 67/256 [00:01<00:04, 43.21it/s, est. speed input: 50710.24 toks/s, output: 49.52 toks/s]
Processed prompts:  28%|██▊       | 72/256 [00:01<00:04, 39.72it/s, est. speed input: 48728.55 toks/s, output: 47.59 toks/s]
Processed prompts:  30%|███       | 77/256 [00:01<00:04, 41.19it/s, est. speed input: 48650.36 toks/s, output: 47.51 toks/s]
Processed prompts:  32%|███▏      | 82/256 [00:01<00:04, 37.79it/s, est. speed input: 47066.95 toks/s, output: 45.96 toks/s]
Processed prompts:  34%|███▎      | 86/256 [00:01<00:04, 37.42it/s, est. speed input: 46487.36 toks/s, output: 45.40 toks/s]
Processed prompts:  35%|███▌      | 90/256 [00:02<00:04, 37.20it/s, est. speed input: 45990.91 toks/s, output: 44.91 toks/s]
Processed prompts:  37%|███▋      | 94/256 [00:02<00:04, 37.12it/s, est. speed input: 45569.26 toks/s, output: 44.50 toks/s]
Processed prompts:  38%|███▊      | 98/256 [00:02<00:04, 37.12it/s, est. speed input: 45202.86 toks/s, output: 44.14 toks/s]
Processed prompts:  40%|███▉      | 102/256 [00:02<00:04, 37.14it/s, est. speed input: 44873.88 toks/s, output: 43.82 toks/s]
Processed prompts:  41%|████▏     | 106/256 [00:02<00:04, 37.19it/s, est. speed input: 44581.35 toks/s, output: 43.54 toks/s]
Processed prompts:  43%|████▎     | 110/256 [00:02<00:03, 37.17it/s, est. speed input: 44302.55 toks/s, output: 43.26 toks/s]
Processed prompts:  45%|████▍     | 114/256 [00:02<00:03, 37.09it/s, est. speed input: 44036.42 toks/s, output: 43.00 toks/s]
Processed prompts:  46%|████▌     | 118/256 [00:02<00:03, 36.98it/s, est. speed input: 43782.53 toks/s, output: 42.76 toks/s]
Processed prompts:  48%|████▊     | 122/256 [00:02<00:03, 36.88it/s, est. speed input: 43543.59 toks/s, output: 42.52 toks/s]
Processed prompts:  49%|████▉     | 126/256 [00:02<00:03, 36.78it/s, est. speed input: 43319.24 toks/s, output: 42.30 toks/s]
Processed prompts:  51%|█████     | 130/256 [00:03<00:03, 36.74it/s, est. speed input: 43114.17 toks/s, output: 42.10 toks/s]
Processed prompts:  52%|█████▏    | 134/256 [00:03<00:03, 36.83it/s, est. speed input: 42939.10 toks/s, output: 41.93 toks/s]
Processed prompts:  54%|█████▍    | 138/256 [00:03<00:03, 36.90it/s, est. speed input: 42776.41 toks/s, output: 41.77 toks/s]
Processed prompts:  55%|█████▌    | 142/256 [00:03<00:03, 36.98it/s, est. speed input: 42628.09 toks/s, output: 41.63 toks/s]
Processed prompts:  57%|█████▋    | 146/256 [00:03<00:02, 37.05it/s, est. speed input: 42489.57 toks/s, output: 41.49 toks/s]
Processed prompts:  59%|█████▊    | 150/256 [00:03<00:02, 37.05it/s, est. speed input: 42354.10 toks/s, output: 41.36 toks/s]
Processed prompts:  60%|██████    | 154/256 [00:03<00:02, 37.05it/s, est. speed input: 42225.88 toks/s, output: 41.24 toks/s]
Processed prompts:  62%|██████▏   | 158/256 [00:03<00:02, 36.88it/s, est. speed input: 42087.42 toks/s, output: 41.10 toks/s]
Processed prompts:  63%|██████▎   | 162/256 [00:03<00:02, 36.79it/s, est. speed input: 41959.96 toks/s, output: 40.98 toks/s]
Processed prompts:  65%|██████▍   | 166/256 [00:04<00:02, 36.63it/s, est. speed input: 41828.89 toks/s, output: 40.85 toks/s]
Processed prompts:  66%|██████▋   | 170/256 [00:04<00:02, 36.75it/s, est. speed input: 41727.32 toks/s, output: 40.75 toks/s]
Processed prompts:  68%|██████▊   | 174/256 [00:04<00:02, 36.89it/s, est. speed input: 41636.18 toks/s, output: 40.66 toks/s]
Processed prompts:  70%|██████▉   | 178/256 [00:04<00:02, 36.87it/s, est. speed input: 41539.14 toks/s, output: 40.57 toks/s]
Processed prompts:  71%|███████   | 182/256 [00:04<00:02, 36.93it/s, est. speed input: 41453.81 toks/s, output: 40.48 toks/s]
Processed prompts:  73%|███████▎  | 186/256 [00:04<00:01, 36.98it/s, est. speed input: 41372.81 toks/s, output: 40.40 toks/s]
Processed prompts:  74%|███████▍  | 190/256 [00:04<00:01, 36.97it/s, est. speed input: 41291.37 toks/s, output: 40.32 toks/s]
Processed prompts:  76%|███████▌  | 194/256 [00:04<00:01, 36.86it/s, est. speed input: 41204.66 toks/s, output: 40.24 toks/s]
Processed prompts:  77%|███████▋  | 198/256 [00:04<00:01, 36.82it/s, est. speed input: 41125.22 toks/s, output: 40.16 toks/s]
Processed prompts:  80%|███████▉  | 204/256 [00:05<00:01, 38.43it/s, est. speed input: 41156.96 toks/s, output: 40.19 toks/s]
Processed prompts:  81%|████████▏ | 208/256 [00:05<00:01, 37.91it/s, est. speed input: 41078.62 toks/s, output: 40.12 toks/s]
Processed prompts:  83%|████████▎ | 212/256 [00:05<00:01, 37.68it/s, est. speed input: 41015.51 toks/s, output: 40.05 toks/s]
Processed prompts:  84%|████████▍ | 216/256 [00:05<00:01, 37.48it/s, est. speed input: 40953.26 toks/s, output: 39.99 toks/s]
Processed prompts:  86%|████████▌ | 220/256 [00:05<00:00, 37.41it/s, est. speed input: 40898.22 toks/s, output: 39.94 toks/s]
Processed prompts:  88%|████████▊ | 224/256 [00:05<00:00, 37.29it/s, est. speed input: 40840.38 toks/s, output: 39.88 toks/s]
Processed prompts:  89%|████████▉ | 228/256 [00:05<00:00, 37.13it/s, est. speed input: 40779.45 toks/s, output: 39.82 toks/s]
Processed prompts:  91%|█████████ | 232/256 [00:05<00:00, 37.08it/s, est. speed input: 40724.82 toks/s, output: 39.77 toks/s]
Processed prompts:  92%|█████████▏| 236/256 [00:05<00:00, 36.87it/s, est. speed input: 40660.60 toks/s, output: 39.71 toks/s]
Processed prompts:  94%|█████████▍| 240/256 [00:06<00:00, 36.88it/s, est. speed input: 40609.45 toks/s, output: 39.66 toks/s]
Processed prompts:  95%|█████████▌| 244/256 [00:06<00:00, 36.88it/s, est. speed input: 40559.47 toks/s, output: 39.61 toks/s]
Processed prompts:  97%|█████████▋| 248/256 [00:06<00:00, 36.87it/s, est. speed input: 40510.25 toks/s, output: 39.56 toks/s]
Processed prompts:  98%|█████████▊| 252/256 [00:06<00:00, 36.93it/s, est. speed input: 40466.85 toks/s, output: 39.52 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:06<00:00, 36.93it/s, est. speed input: 40566.11 toks/s, output: 39.62 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:06<00:00, 39.61it/s, est. speed input: 40566.11 toks/s, output: 39.62 toks/s]
[rank0]:[W126 11:35:18.296562959 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 47.7s

测试结果:
  Requests/s:   35.68
  Tokens/s:     36571.99
  Total Reqs:   256
  Elapsed:      7.17s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     36536.31

============================================================
[4/8] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 11:35:29 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1295654) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1295654) WARNING 01-26 11:35:46 [backends.py:609] Failed to read file <frozen os>
Throughput: 37.35 requests/s, 38278.80 total tokens/s, 37.35 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-26 11:35:29] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:35:29] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:35:29] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 11:35:29] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:35:29] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:35:29] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:35:29] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:35:29] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:35:29] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:35:29] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:35:29] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:35:29] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:35:29] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:35:29] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 11:35:36] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:35:36] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:35:36] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 11:35:36] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:35:36] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:35:36] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:35:36] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:35:36] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:35:36] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:35:36] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:35:36] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:35:36] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:35:36] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:35:36] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1295654) [2026-01-26 11:35:38] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1295654) [2026-01-26 11:35:38] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1295654) [2026-01-26 11:35:38] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1295654) [2026-01-26 11:35:38] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=1295654) [2026-01-26 11:35:38] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=1295654) [2026-01-26 11:35:38] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=1295654) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1295654) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.85it/s]
(EngineCore_DP0 pid=1295654) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.32it/s]
(EngineCore_DP0 pid=1295654) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.38it/s]
(EngineCore_DP0 pid=1295654) 
(EngineCore_DP0 pid=1295654) [2026-01-26 11:35:40] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=1295654) [2026-01-26 11:35:40] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16588800 bytes
(EngineCore_DP0 pid=1295654) [2026-01-26 11:35:40] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=1295654) [2026-01-26 11:35:40] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12902400 bytes
(EngineCore_DP0 pid=1295654) [2026-01-26 11:35:40] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=1295654) [2026-01-26 11:35:40] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 136396800 bytes
(EngineCore_DP0 pid=1295654) [2026-01-26 11:35:40] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=1295654) [2026-01-26 11:35:40] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 67952640 bytes
(EngineCore_DP0 pid=1295654) 2026-01-26 11:35:56,398 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1295654) 2026-01-26 11:35:56,421 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1295654) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 1/4 [00:00<00:00,  7.00it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00,  2.29it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 3/4 [00:00<00:00,  3.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00,  4.07it/s]
(EngineCore_DP0 pid=1295654) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00, 18.84it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 19.00it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   4%|▍         | 21/512 [00:00<00:02, 208.94it/s]
Adding requests:  12%|█▏        | 62/512 [00:00<00:01, 324.10it/s]
Adding requests:  19%|█▉        | 98/512 [00:00<00:01, 337.99it/s]
Adding requests:  27%|██▋       | 136/512 [00:00<00:01, 353.04it/s]
Adding requests:  34%|███▍      | 175/512 [00:00<00:00, 364.57it/s]
Adding requests:  42%|████▏     | 215/512 [00:00<00:00, 374.35it/s]
Adding requests:  49%|████▉     | 253/512 [00:00<00:00, 374.48it/s]
Adding requests:  57%|█████▋    | 291/512 [00:00<00:00, 375.73it/s]
Adding requests:  65%|██████▍   | 331/512 [00:00<00:00, 381.40it/s]
Adding requests:  72%|███████▏  | 371/512 [00:01<00:00, 386.21it/s]
Adding requests:  80%|████████  | 411/512 [00:01<00:00, 389.52it/s]
Adding requests:  88%|████████▊ | 450/512 [00:01<00:00, 385.13it/s]
Adding requests:  96%|█████████▋| 493/512 [00:01<00:00, 396.28it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 374.37it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  10%|▉         | 50/512 [00:00<00:01, 272.57it/s, est. speed input: 279145.94 toks/s, output: 272.58 toks/s]
Processed prompts:  15%|█▌        | 78/512 [00:00<00:05, 72.41it/s, est. speed input: 86350.98 toks/s, output: 84.32 toks/s]   
Processed prompts:  18%|█▊        | 92/512 [00:01<00:06, 62.74it/s, est. speed input: 75848.89 toks/s, output: 74.07 toks/s]
Processed prompts:  20%|█▉        | 102/512 [00:01<00:07, 52.76it/s, est. speed input: 67203.74 toks/s, output: 65.63 toks/s]
Processed prompts:  21%|██▏       | 109/512 [00:01<00:07, 54.52it/s, est. speed input: 67200.59 toks/s, output: 65.63 toks/s]
Processed prompts:  23%|██▎       | 116/512 [00:01<00:08, 48.24it/s, est. speed input: 63285.42 toks/s, output: 61.80 toks/s]
Processed prompts:  24%|██▍       | 122/512 [00:02<00:09, 42.50it/s, est. speed input: 59770.75 toks/s, output: 58.37 toks/s]
Processed prompts:  25%|██▍       | 127/512 [00:02<00:08, 43.30it/s, est. speed input: 59221.09 toks/s, output: 57.83 toks/s]
Processed prompts:  26%|██▌       | 132/512 [00:02<00:08, 44.16it/s, est. speed input: 58751.54 toks/s, output: 57.37 toks/s]
Processed prompts:  27%|██▋       | 137/512 [00:02<00:08, 45.05it/s, est. speed input: 58352.47 toks/s, output: 56.98 toks/s]
Processed prompts:  28%|██▊       | 142/512 [00:02<00:10, 36.84it/s, est. speed input: 55598.24 toks/s, output: 54.29 toks/s]
Processed prompts:  29%|██▊       | 147/512 [00:02<00:09, 39.04it/s, est. speed input: 55306.39 toks/s, output: 54.01 toks/s]
Processed prompts:  30%|██▉       | 152/512 [00:02<00:08, 40.80it/s, est. speed input: 55009.18 toks/s, output: 53.72 toks/s]
Processed prompts:  31%|███       | 157/512 [00:02<00:08, 42.25it/s, est. speed input: 54739.65 toks/s, output: 53.46 toks/s]
Processed prompts:  32%|███▏      | 162/512 [00:03<00:10, 34.44it/s, est. speed input: 52672.44 toks/s, output: 51.44 toks/s]
Processed prompts:  32%|███▏      | 166/512 [00:03<00:09, 35.19it/s, est. speed input: 52215.17 toks/s, output: 50.99 toks/s]
Processed prompts:  33%|███▎      | 170/512 [00:03<00:09, 35.98it/s, est. speed input: 51816.50 toks/s, output: 50.60 toks/s]
Processed prompts:  34%|███▍      | 174/512 [00:03<00:09, 36.57it/s, est. speed input: 51434.51 toks/s, output: 50.23 toks/s]
Processed prompts:  35%|███▍      | 178/512 [00:03<00:09, 36.92it/s, est. speed input: 51059.66 toks/s, output: 49.86 toks/s]
Processed prompts:  36%|███▌      | 182/512 [00:03<00:08, 37.06it/s, est. speed input: 50688.98 toks/s, output: 49.50 toks/s]
Processed prompts:  36%|███▋      | 186/512 [00:03<00:08, 37.20it/s, est. speed input: 50344.32 toks/s, output: 49.16 toks/s]
Processed prompts:  37%|███▋      | 190/512 [00:03<00:08, 37.35it/s, est. speed input: 50024.36 toks/s, output: 48.85 toks/s]
Processed prompts:  38%|███▊      | 194/512 [00:03<00:08, 37.31it/s, est. speed input: 49703.89 toks/s, output: 48.54 toks/s]
Processed prompts:  39%|███▊      | 198/512 [00:04<00:08, 37.36it/s, est. speed input: 49409.81 toks/s, output: 48.25 toks/s]
Processed prompts:  40%|████      | 206/512 [00:04<00:07, 38.31it/s, est. speed input: 48987.55 toks/s, output: 47.84 toks/s]
Processed prompts:  41%|████      | 210/512 [00:04<00:07, 38.27it/s, est. speed input: 48751.49 toks/s, output: 47.61 toks/s]
Processed prompts:  42%|████▏     | 214/512 [00:04<00:07, 38.14it/s, est. speed input: 48515.76 toks/s, output: 47.38 toks/s]
Processed prompts:  43%|████▎     | 218/512 [00:04<00:07, 38.00it/s, est. speed input: 48285.73 toks/s, output: 47.15 toks/s]
Processed prompts:  43%|████▎     | 222/512 [00:04<00:07, 37.94it/s, est. speed input: 48070.76 toks/s, output: 46.94 toks/s]
Processed prompts:  44%|████▍     | 226/512 [00:04<00:07, 37.77it/s, est. speed input: 47853.70 toks/s, output: 46.73 toks/s]
Processed prompts:  45%|████▍     | 230/512 [00:04<00:07, 37.64it/s, est. speed input: 47644.48 toks/s, output: 46.53 toks/s]
Processed prompts:  46%|████▌     | 234/512 [00:05<00:07, 37.61it/s, est. speed input: 47450.34 toks/s, output: 46.34 toks/s]
Processed prompts:  46%|████▋     | 238/512 [00:05<00:07, 37.54it/s, est. speed input: 47259.65 toks/s, output: 46.15 toks/s]
Processed prompts:  47%|████▋     | 242/512 [00:05<00:07, 37.66it/s, est. speed input: 47091.39 toks/s, output: 45.99 toks/s]
Processed prompts:  48%|████▊     | 246/512 [00:05<00:07, 37.78it/s, est. speed input: 46932.17 toks/s, output: 45.83 toks/s]
Processed prompts:  49%|████▉     | 250/512 [00:05<00:06, 37.82it/s, est. speed input: 46775.89 toks/s, output: 45.68 toks/s]
Processed prompts:  50%|████▉     | 254/512 [00:05<00:06, 37.75it/s, est. speed input: 46617.85 toks/s, output: 45.52 toks/s]
Processed prompts:  50%|█████     | 258/512 [00:05<00:06, 37.78it/s, est. speed input: 46471.57 toks/s, output: 45.38 toks/s]
Processed prompts:  51%|█████     | 262/512 [00:05<00:06, 37.75it/s, est. speed input: 46327.19 toks/s, output: 45.24 toks/s]
Processed prompts:  52%|█████▏    | 266/512 [00:05<00:06, 37.64it/s, est. speed input: 46181.20 toks/s, output: 45.10 toks/s]
Processed prompts:  53%|█████▎    | 270/512 [00:06<00:06, 37.50it/s, est. speed input: 46035.72 toks/s, output: 44.96 toks/s]
Processed prompts:  54%|█████▎    | 274/512 [00:06<00:06, 37.60it/s, est. speed input: 45909.96 toks/s, output: 44.83 toks/s]
Processed prompts:  54%|█████▍    | 278/512 [00:06<00:06, 37.69it/s, est. speed input: 45788.96 toks/s, output: 44.72 toks/s]
Processed prompts:  55%|█████▌    | 282/512 [00:06<00:06, 37.73it/s, est. speed input: 45671.42 toks/s, output: 44.60 toks/s]
Processed prompts:  56%|█████▌    | 286/512 [00:06<00:05, 37.82it/s, est. speed input: 45561.11 toks/s, output: 44.49 toks/s]
Processed prompts:  57%|█████▋    | 290/512 [00:06<00:05, 37.80it/s, est. speed input: 45449.48 toks/s, output: 44.38 toks/s]
Processed prompts:  57%|█████▋    | 294/512 [00:06<00:05, 37.78it/s, est. speed input: 45340.69 toks/s, output: 44.28 toks/s]
Processed prompts:  58%|█████▊    | 298/512 [00:06<00:05, 37.77it/s, est. speed input: 45235.52 toks/s, output: 44.18 toks/s]
Processed prompts:  59%|█████▉    | 302/512 [00:06<00:05, 37.71it/s, est. speed input: 45130.17 toks/s, output: 44.07 toks/s]
Processed prompts:  61%|██████    | 310/512 [00:07<00:05, 38.41it/s, est. speed input: 44987.71 toks/s, output: 43.93 toks/s]
Processed prompts:  61%|██████▏   | 314/512 [00:07<00:05, 38.19it/s, est. speed input: 44890.42 toks/s, output: 43.84 toks/s]
Processed prompts:  62%|██████▏   | 318/512 [00:07<00:05, 38.12it/s, est. speed input: 44802.52 toks/s, output: 43.75 toks/s]
Processed prompts:  63%|██████▎   | 322/512 [00:07<00:04, 38.03it/s, est. speed input: 44715.14 toks/s, output: 43.67 toks/s]
Processed prompts:  64%|██████▎   | 326/512 [00:07<00:04, 37.96it/s, est. speed input: 44629.75 toks/s, output: 43.58 toks/s]
Processed prompts:  64%|██████▍   | 330/512 [00:07<00:04, 37.94it/s, est. speed input: 44548.52 toks/s, output: 43.50 toks/s]
Processed prompts:  65%|██████▌   | 334/512 [00:07<00:04, 37.95it/s, est. speed input: 44470.71 toks/s, output: 43.43 toks/s]
Processed prompts:  66%|██████▌   | 338/512 [00:07<00:04, 37.71it/s, est. speed input: 44381.88 toks/s, output: 43.34 toks/s]
Processed prompts:  67%|██████▋   | 342/512 [00:07<00:04, 37.68it/s, est. speed input: 44302.82 toks/s, output: 43.26 toks/s]
Processed prompts:  68%|██████▊   | 346/512 [00:08<00:04, 37.76it/s, est. speed input: 44231.39 toks/s, output: 43.19 toks/s]
Processed prompts:  68%|██████▊   | 350/512 [00:08<00:04, 37.57it/s, est. speed input: 44149.18 toks/s, output: 43.11 toks/s]
Processed prompts:  69%|██████▉   | 354/512 [00:08<00:04, 37.60it/s, est. speed input: 44077.11 toks/s, output: 43.04 toks/s]
Processed prompts:  70%|██████▉   | 358/512 [00:08<00:04, 37.76it/s, est. speed input: 44013.85 toks/s, output: 42.98 toks/s]
Processed prompts:  71%|███████   | 362/512 [00:08<00:03, 37.75it/s, est. speed input: 43946.15 toks/s, output: 42.92 toks/s]
Processed prompts:  71%|███████▏  | 366/512 [00:08<00:03, 37.68it/s, est. speed input: 43877.13 toks/s, output: 42.85 toks/s]
Processed prompts:  72%|███████▏  | 370/512 [00:08<00:03, 37.74it/s, est. speed input: 43814.96 toks/s, output: 42.79 toks/s]
Processed prompts:  73%|███████▎  | 374/512 [00:08<00:03, 37.68it/s, est. speed input: 43749.46 toks/s, output: 42.72 toks/s]
Processed prompts:  74%|███████▍  | 378/512 [00:08<00:03, 37.64it/s, est. speed input: 43685.67 toks/s, output: 42.66 toks/s]
Processed prompts:  75%|███████▍  | 382/512 [00:08<00:03, 37.63it/s, est. speed input: 43624.56 toks/s, output: 42.60 toks/s]
Processed prompts:  75%|███████▌  | 386/512 [00:09<00:03, 37.61it/s, est. speed input: 43563.78 toks/s, output: 42.54 toks/s]
Processed prompts:  76%|███████▌  | 390/512 [00:09<00:03, 37.63it/s, est. speed input: 43506.05 toks/s, output: 42.49 toks/s]
Processed prompts:  77%|███████▋  | 394/512 [00:09<00:03, 37.68it/s, est. speed input: 43451.62 toks/s, output: 42.43 toks/s]
Processed prompts:  78%|███████▊  | 398/512 [00:09<00:03, 37.79it/s, est. speed input: 43401.32 toks/s, output: 42.38 toks/s]
Processed prompts:  79%|███████▊  | 402/512 [00:09<00:02, 37.76it/s, est. speed input: 43347.66 toks/s, output: 42.33 toks/s]
Processed prompts:  79%|███████▉  | 406/512 [00:09<00:02, 37.74it/s, est. speed input: 43295.10 toks/s, output: 42.28 toks/s]
Processed prompts:  80%|████████  | 410/512 [00:09<00:02, 37.72it/s, est. speed input: 43243.50 toks/s, output: 42.23 toks/s]
Processed prompts:  81%|████████  | 414/512 [00:09<00:02, 37.63it/s, est. speed input: 43190.05 toks/s, output: 42.18 toks/s]
Processed prompts:  82%|████████▏ | 418/512 [00:09<00:02, 37.59it/s, est. speed input: 43138.10 toks/s, output: 42.13 toks/s]
Processed prompts:  82%|████████▏ | 422/512 [00:10<00:02, 37.57it/s, est. speed input: 43088.22 toks/s, output: 42.08 toks/s]
Processed prompts:  83%|████████▎ | 426/512 [00:10<00:02, 37.55it/s, est. speed input: 43038.92 toks/s, output: 42.03 toks/s]
Processed prompts:  84%|████████▍ | 430/512 [00:10<00:02, 37.59it/s, est. speed input: 42992.59 toks/s, output: 41.98 toks/s]
Processed prompts:  86%|████████▌ | 438/512 [00:10<00:01, 38.51it/s, est. speed input: 42946.33 toks/s, output: 41.94 toks/s]
Processed prompts:  86%|████████▋ | 442/512 [00:10<00:01, 38.34it/s, est. speed input: 42903.96 toks/s, output: 41.90 toks/s]
Processed prompts:  87%|████████▋ | 446/512 [00:10<00:01, 38.13it/s, est. speed input: 42859.52 toks/s, output: 41.85 toks/s]
Processed prompts:  88%|████████▊ | 450/512 [00:10<00:01, 37.97it/s, est. speed input: 42815.79 toks/s, output: 41.81 toks/s]
Processed prompts:  89%|████████▊ | 454/512 [00:10<00:01, 37.85it/s, est. speed input: 42772.87 toks/s, output: 41.77 toks/s]
Processed prompts:  89%|████████▉ | 458/512 [00:10<00:01, 37.76it/s, est. speed input: 42731.00 toks/s, output: 41.73 toks/s]
Processed prompts:  90%|█████████ | 462/512 [00:11<00:01, 37.67it/s, est. speed input: 42688.56 toks/s, output: 41.69 toks/s]
Processed prompts:  91%|█████████ | 466/512 [00:11<00:01, 37.67it/s, est. speed input: 42649.49 toks/s, output: 41.65 toks/s]
Processed prompts:  92%|█████████▏| 470/512 [00:11<00:01, 37.64it/s, est. speed input: 42610.25 toks/s, output: 41.61 toks/s]
Processed prompts:  93%|█████████▎| 474/512 [00:11<00:01, 37.63it/s, est. speed input: 42571.84 toks/s, output: 41.57 toks/s]
Processed prompts:  93%|█████████▎| 478/512 [00:11<00:00, 37.68it/s, est. speed input: 42536.40 toks/s, output: 41.54 toks/s]
Processed prompts:  94%|█████████▍| 482/512 [00:11<00:00, 37.70it/s, est. speed input: 42500.85 toks/s, output: 41.50 toks/s]
Processed prompts:  95%|█████████▍| 486/512 [00:11<00:00, 37.63it/s, est. speed input: 42463.22 toks/s, output: 41.47 toks/s]
Processed prompts:  96%|█████████▌| 490/512 [00:11<00:00, 37.60it/s, est. speed input: 42426.90 toks/s, output: 41.43 toks/s]
Processed prompts:  96%|█████████▋| 494/512 [00:11<00:00, 37.64it/s, est. speed input: 42393.19 toks/s, output: 41.40 toks/s]
Processed prompts:  97%|█████████▋| 498/512 [00:12<00:00, 37.59it/s, est. speed input: 42357.54 toks/s, output: 41.36 toks/s]
Processed prompts:  98%|█████████▊| 502/512 [00:12<00:00, 37.61it/s, est. speed input: 42324.25 toks/s, output: 41.33 toks/s]
Processed prompts:  99%|█████████▉| 506/512 [00:12<00:00, 37.66it/s, est. speed input: 42292.83 toks/s, output: 41.30 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:12<00:00, 37.66it/s, est. speed input: 42486.12 toks/s, output: 41.49 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:12<00:00, 41.49it/s, est. speed input: 42486.12 toks/s, output: 41.49 toks/s]
[rank0]:[W126 11:36:13.101652464 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 55.0s

测试结果:
  Requests/s:   37.35
  Tokens/s:     38278.80
  Total Reqs:   512
  Elapsed:      13.71s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     38241.45

============================================================
[5/8] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 11:36:27 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1296949) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1296949) WARNING 01-26 11:36:44 [backends.py:609] Failed to read file <frozen os>
Throughput: 38.36 requests/s, 39318.67 total tokens/s, 38.36 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-26 11:36:26] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:36:27] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:36:27] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 11:36:27] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:36:27] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:36:27] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:36:27] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:36:27] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:36:27] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:36:27] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:36:27] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:36:27] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:36:27] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:36:27] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 11:36:34] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:36:35] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:36:35] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 11:36:35] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:36:35] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:36:35] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:36:35] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:36:35] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:36:35] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:36:35] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:36:35] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:36:35] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:36:35] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:36:35] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1296949) [2026-01-26 11:36:36] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1296949) [2026-01-26 11:36:36] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1296949) [2026-01-26 11:36:36] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1296949) [2026-01-26 11:36:36] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=1296949) [2026-01-26 11:36:36] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=1296949) [2026-01-26 11:36:36] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=1296949) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1296949) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.85it/s]
(EngineCore_DP0 pid=1296949) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.33it/s]
(EngineCore_DP0 pid=1296949) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.38it/s]
(EngineCore_DP0 pid=1296949) 
(EngineCore_DP0 pid=1296949) [2026-01-26 11:36:38] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=1296949) [2026-01-26 11:36:38] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16588800 bytes
(EngineCore_DP0 pid=1296949) [2026-01-26 11:36:38] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=1296949) [2026-01-26 11:36:38] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12902400 bytes
(EngineCore_DP0 pid=1296949) [2026-01-26 11:36:38] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=1296949) [2026-01-26 11:36:38] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 136396800 bytes
(EngineCore_DP0 pid=1296949) [2026-01-26 11:36:38] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=1296949) [2026-01-26 11:36:38] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 67952640 bytes
(EngineCore_DP0 pid=1296949) 2026-01-26 11:36:55,079 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1296949) 2026-01-26 11:36:55,122 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1296949) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 1/5 [00:00<00:03,  1.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 3/5 [00:00<00:00,  3.94it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:01<00:00,  6.45it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:01<00:00,  4.81it/s]
(EngineCore_DP0 pid=1296949) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00, 18.74it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 19.64it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   3%|▎         | 28/1024 [00:00<00:03, 278.97it/s]
Adding requests:   7%|▋         | 68/1024 [00:00<00:02, 349.10it/s]
Adding requests:  10%|█         | 105/1024 [00:00<00:02, 357.04it/s]
Adding requests:  14%|█▍        | 142/1024 [00:00<00:02, 360.68it/s]
Adding requests:  18%|█▊        | 182/1024 [00:00<00:02, 371.97it/s]
Adding requests:  22%|██▏       | 223/1024 [00:00<00:02, 384.65it/s]
Adding requests:  26%|██▌       | 262/1024 [00:00<00:01, 381.24it/s]
Adding requests:  29%|██▉       | 302/1024 [00:00<00:01, 386.11it/s]
Adding requests:  33%|███▎      | 343/1024 [00:00<00:01, 390.00it/s]
Adding requests:  38%|███▊      | 384/1024 [00:01<00:01, 394.40it/s]
Adding requests:  42%|████▏     | 425/1024 [00:01<00:01, 397.14it/s]
Adding requests:  45%|████▌     | 465/1024 [00:01<00:01, 392.78it/s]
Adding requests:  50%|████▉     | 508/1024 [00:01<00:01, 403.10it/s]
Adding requests:  54%|█████▎    | 549/1024 [00:01<00:01, 400.44it/s]
Adding requests:  58%|█████▊    | 590/1024 [00:01<00:01, 395.77it/s]
Adding requests:  62%|██████▏   | 630/1024 [00:01<00:01, 391.48it/s]
Adding requests:  65%|██████▌   | 670/1024 [00:01<00:00, 379.45it/s]
Adding requests:  69%|██████▉   | 710/1024 [00:01<00:00, 384.22it/s]
Adding requests:  73%|███████▎  | 749/1024 [00:01<00:00, 378.51it/s]
Adding requests:  77%|███████▋  | 789/1024 [00:02<00:00, 383.04it/s]
Adding requests:  81%|████████  | 829/1024 [00:02<00:00, 387.25it/s]
Adding requests:  85%|████████▍ | 868/1024 [00:02<00:00, 387.81it/s]
Adding requests:  89%|████████▉ | 909/1024 [00:02<00:00, 392.70it/s]
Adding requests:  93%|█████████▎| 949/1024 [00:02<00:00, 383.95it/s]
Adding requests:  97%|█████████▋| 989/1024 [00:02<00:00, 386.50it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 384.04it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  10%|▉         | 98/1024 [00:00<00:01, 571.67it/s, est. speed input: 585554.06 toks/s, output: 571.71 toks/s]
Processed prompts:  15%|█▌        | 156/1024 [00:01<00:10, 80.13it/s, est. speed input: 97921.84 toks/s, output: 95.63 toks/s]  
Processed prompts:  18%|█▊        | 182/1024 [00:02<00:12, 66.43it/s, est. speed input: 82803.03 toks/s, output: 80.86 toks/s]
Processed prompts:  19%|█▉        | 198/1024 [00:02<00:13, 59.16it/s, est. speed input: 75944.05 toks/s, output: 74.16 toks/s]
Processed prompts:  21%|██        | 210/1024 [00:03<00:15, 51.76it/s, est. speed input: 70179.10 toks/s, output: 68.53 toks/s]
Processed prompts:  21%|██▏       | 219/1024 [00:03<00:15, 50.51it/s, est. speed input: 68599.59 toks/s, output: 66.99 toks/s]
Processed prompts:  22%|██▏       | 226/1024 [00:03<00:16, 47.34it/s, est. speed input: 66561.25 toks/s, output: 65.00 toks/s]
Processed prompts:  23%|██▎       | 234/1024 [00:03<00:17, 45.27it/s, est. speed input: 64956.99 toks/s, output: 63.43 toks/s]
Processed prompts:  24%|██▎       | 242/1024 [00:03<00:17, 43.68it/s, est. speed input: 63589.96 toks/s, output: 62.10 toks/s]
Processed prompts:  24%|██▍       | 250/1024 [00:04<00:18, 42.50it/s, est. speed input: 62397.73 toks/s, output: 60.93 toks/s]
Processed prompts:  25%|██▌       | 258/1024 [00:04<00:18, 41.52it/s, est. speed input: 61310.12 toks/s, output: 59.87 toks/s]
Processed prompts:  26%|██▌       | 266/1024 [00:04<00:18, 40.65it/s, est. speed input: 60291.45 toks/s, output: 58.88 toks/s]
Processed prompts:  27%|██▋       | 274/1024 [00:04<00:18, 39.83it/s, est. speed input: 59323.19 toks/s, output: 57.93 toks/s]
Processed prompts:  28%|██▊       | 282/1024 [00:04<00:18, 39.41it/s, est. speed input: 58477.62 toks/s, output: 57.11 toks/s]
Processed prompts:  28%|██▊       | 290/1024 [00:05<00:18, 39.24it/s, est. speed input: 57725.94 toks/s, output: 56.37 toks/s]
Processed prompts:  29%|██▉       | 298/1024 [00:05<00:18, 39.08it/s, est. speed input: 57026.72 toks/s, output: 55.69 toks/s]
Processed prompts:  30%|██▉       | 306/1024 [00:05<00:17, 40.01it/s, est. speed input: 56565.56 toks/s, output: 55.24 toks/s]
Processed prompts:  31%|███       | 314/1024 [00:05<00:18, 39.43it/s, est. speed input: 55924.61 toks/s, output: 54.61 toks/s]
Processed prompts:  31%|███▏      | 322/1024 [00:05<00:17, 39.19it/s, est. speed input: 55356.71 toks/s, output: 54.06 toks/s]
Processed prompts:  32%|███▏      | 330/1024 [00:06<00:17, 39.07it/s, est. speed input: 54832.48 toks/s, output: 53.55 toks/s]
Processed prompts:  33%|███▎      | 338/1024 [00:06<00:17, 38.89it/s, est. speed input: 54328.90 toks/s, output: 53.06 toks/s]
Processed prompts:  34%|███▍      | 346/1024 [00:06<00:17, 38.63it/s, est. speed input: 53836.88 toks/s, output: 52.57 toks/s]
Processed prompts:  35%|███▍      | 354/1024 [00:06<00:17, 38.49it/s, est. speed input: 53381.50 toks/s, output: 52.13 toks/s]
Processed prompts:  35%|███▌      | 362/1024 [00:06<00:17, 38.47it/s, est. speed input: 52963.88 toks/s, output: 51.72 toks/s]
Processed prompts:  36%|███▌      | 370/1024 [00:07<00:16, 38.49it/s, est. speed input: 52574.91 toks/s, output: 51.34 toks/s]
Processed prompts:  37%|███▋      | 378/1024 [00:07<00:16, 38.56it/s, est. speed input: 52214.92 toks/s, output: 50.99 toks/s]
Processed prompts:  38%|███▊      | 386/1024 [00:07<00:16, 38.38it/s, est. speed input: 51846.76 toks/s, output: 50.63 toks/s]
Processed prompts:  38%|███▊      | 394/1024 [00:07<00:16, 38.35it/s, est. speed input: 51509.28 toks/s, output: 50.30 toks/s]
Processed prompts:  39%|███▉      | 402/1024 [00:08<00:16, 38.42it/s, est. speed input: 51200.14 toks/s, output: 50.00 toks/s]
Processed prompts:  40%|████      | 410/1024 [00:08<00:15, 38.43it/s, est. speed input: 50901.96 toks/s, output: 49.71 toks/s]
Processed prompts:  41%|████      | 418/1024 [00:08<00:15, 38.46it/s, est. speed input: 50620.67 toks/s, output: 49.43 toks/s]
Processed prompts:  42%|████▏     | 426/1024 [00:08<00:15, 38.35it/s, est. speed input: 50339.54 toks/s, output: 49.16 toks/s]
Processed prompts:  42%|████▏     | 434/1024 [00:08<00:14, 39.65it/s, est. speed input: 50208.18 toks/s, output: 49.03 toks/s]
Processed prompts:  43%|████▎     | 442/1024 [00:09<00:14, 39.26it/s, est. speed input: 49956.71 toks/s, output: 48.79 toks/s]
Processed prompts:  44%|████▍     | 450/1024 [00:09<00:14, 38.96it/s, est. speed input: 49714.50 toks/s, output: 48.55 toks/s]
Processed prompts:  45%|████▍     | 458/1024 [00:09<00:14, 38.77it/s, est. speed input: 49484.25 toks/s, output: 48.32 toks/s]
Processed prompts:  46%|████▌     | 466/1024 [00:09<00:14, 38.50it/s, est. speed input: 49251.19 toks/s, output: 48.10 toks/s]
Processed prompts:  46%|████▋     | 474/1024 [00:09<00:14, 38.53it/s, est. speed input: 49047.21 toks/s, output: 47.90 toks/s]
Processed prompts:  47%|████▋     | 482/1024 [00:10<00:14, 38.58it/s, est. speed input: 48854.47 toks/s, output: 47.71 toks/s]
Processed prompts:  48%|████▊     | 490/1024 [00:10<00:13, 38.49it/s, est. speed input: 48658.53 toks/s, output: 47.52 toks/s]
Processed prompts:  49%|████▊     | 498/1024 [00:10<00:13, 38.36it/s, est. speed input: 48465.60 toks/s, output: 47.33 toks/s]
Processed prompts:  49%|████▉     | 506/1024 [00:10<00:13, 38.30it/s, est. speed input: 48282.02 toks/s, output: 47.15 toks/s]
Processed prompts:  50%|█████     | 514/1024 [00:10<00:13, 38.27it/s, est. speed input: 48106.65 toks/s, output: 46.98 toks/s]
Processed prompts:  51%|█████     | 522/1024 [00:11<00:13, 38.36it/s, est. speed input: 47946.34 toks/s, output: 46.82 toks/s]
Processed prompts:  52%|█████▏    | 530/1024 [00:11<00:12, 38.39it/s, est. speed input: 47789.25 toks/s, output: 46.67 toks/s]
Processed prompts:  53%|█████▎    | 538/1024 [00:11<00:12, 38.24it/s, est. speed input: 47625.71 toks/s, output: 46.51 toks/s]
Processed prompts:  53%|█████▎    | 546/1024 [00:11<00:12, 38.34it/s, est. speed input: 47482.26 toks/s, output: 46.37 toks/s]
Processed prompts:  54%|█████▍    | 554/1024 [00:11<00:12, 38.27it/s, est. speed input: 47334.55 toks/s, output: 46.23 toks/s]
Processed prompts:  55%|█████▍    | 562/1024 [00:12<00:12, 38.30it/s, est. speed input: 47196.79 toks/s, output: 46.09 toks/s]
Processed prompts:  56%|█████▌    | 570/1024 [00:12<00:11, 38.31it/s, est. speed input: 47063.23 toks/s, output: 45.96 toks/s]
Processed prompts:  56%|█████▋    | 578/1024 [00:12<00:11, 38.26it/s, est. speed input: 46930.12 toks/s, output: 45.83 toks/s]
Processed prompts:  57%|█████▋    | 586/1024 [00:12<00:11, 38.29it/s, est. speed input: 46805.57 toks/s, output: 45.71 toks/s]
Processed prompts:  58%|█████▊    | 594/1024 [00:13<00:11, 38.24it/s, est. speed input: 46680.78 toks/s, output: 45.59 toks/s]
Processed prompts:  59%|█████▉    | 602/1024 [00:13<00:11, 38.28it/s, est. speed input: 46564.24 toks/s, output: 45.47 toks/s]
Processed prompts:  60%|█████▉    | 610/1024 [00:13<00:10, 38.23it/s, est. speed input: 46446.34 toks/s, output: 45.36 toks/s]
Processed prompts:  60%|██████    | 618/1024 [00:13<00:10, 38.33it/s, est. speed input: 46340.60 toks/s, output: 45.25 toks/s]
Processed prompts:  61%|██████    | 626/1024 [00:13<00:10, 38.24it/s, est. speed input: 46228.44 toks/s, output: 45.14 toks/s]
Processed prompts:  62%|██████▏   | 634/1024 [00:14<00:10, 38.35it/s, est. speed input: 46129.89 toks/s, output: 45.05 toks/s]
Processed prompts:  63%|██████▎   | 642/1024 [00:14<00:09, 38.36it/s, est. speed input: 46030.15 toks/s, output: 44.95 toks/s]
Processed prompts:  63%|██████▎   | 650/1024 [00:14<00:09, 38.40it/s, est. speed input: 45935.49 toks/s, output: 44.86 toks/s]
Processed prompts:  64%|██████▍   | 658/1024 [00:14<00:09, 38.41it/s, est. speed input: 45842.48 toks/s, output: 44.77 toks/s]
Processed prompts:  65%|██████▌   | 666/1024 [00:14<00:09, 38.33it/s, est. speed input: 45747.15 toks/s, output: 44.67 toks/s]
Processed prompts:  66%|██████▌   | 674/1024 [00:15<00:09, 38.46it/s, est. speed input: 45664.28 toks/s, output: 44.59 toks/s]
Processed prompts:  67%|██████▋   | 682/1024 [00:15<00:08, 38.45it/s, est. speed input: 45578.60 toks/s, output: 44.51 toks/s]
Processed prompts:  67%|██████▋   | 690/1024 [00:15<00:08, 38.50it/s, est. speed input: 45498.27 toks/s, output: 44.43 toks/s]
Processed prompts:  68%|██████▊   | 698/1024 [00:15<00:08, 38.47it/s, est. speed input: 45416.22 toks/s, output: 44.35 toks/s]
Processed prompts:  69%|██████▉   | 706/1024 [00:15<00:08, 38.35it/s, est. speed input: 45331.59 toks/s, output: 44.27 toks/s]
Processed prompts:  70%|██████▉   | 714/1024 [00:16<00:08, 38.43it/s, est. speed input: 45257.34 toks/s, output: 44.20 toks/s]
Processed prompts:  71%|███████   | 722/1024 [00:16<00:07, 38.41it/s, est. speed input: 45181.06 toks/s, output: 44.12 toks/s]
Processed prompts:  71%|███████▏  | 730/1024 [00:16<00:07, 38.49it/s, est. speed input: 45111.72 toks/s, output: 44.05 toks/s]
Processed prompts:  72%|███████▏  | 738/1024 [00:16<00:07, 38.41it/s, est. speed input: 45037.26 toks/s, output: 43.98 toks/s]
Processed prompts:  73%|███████▎  | 746/1024 [00:16<00:07, 38.30it/s, est. speed input: 44962.18 toks/s, output: 43.91 toks/s]
Processed prompts:  74%|███████▎  | 754/1024 [00:17<00:07, 38.38it/s, est. speed input: 44896.08 toks/s, output: 43.84 toks/s]
Processed prompts:  74%|███████▍  | 762/1024 [00:17<00:06, 38.35it/s, est. speed input: 44827.92 toks/s, output: 43.78 toks/s]
Processed prompts:  75%|███████▌  | 770/1024 [00:17<00:06, 38.53it/s, est. speed input: 44770.19 toks/s, output: 43.72 toks/s]
Processed prompts:  76%|███████▌  | 778/1024 [00:17<00:06, 38.44it/s, est. speed input: 44704.17 toks/s, output: 43.66 toks/s]
Processed prompts:  77%|███████▋  | 786/1024 [00:18<00:06, 39.63it/s, est. speed input: 44694.19 toks/s, output: 43.65 toks/s]
Processed prompts:  78%|███████▊  | 794/1024 [00:18<00:05, 39.33it/s, est. speed input: 44636.01 toks/s, output: 43.59 toks/s]
Processed prompts:  78%|███████▊  | 802/1024 [00:18<00:05, 39.11it/s, est. speed input: 44578.59 toks/s, output: 43.53 toks/s]
Processed prompts:  79%|███████▉  | 810/1024 [00:18<00:05, 38.93it/s, est. speed input: 44521.19 toks/s, output: 43.48 toks/s]
Processed prompts:  80%|███████▉  | 818/1024 [00:18<00:05, 38.81it/s, est. speed input: 44465.51 toks/s, output: 43.42 toks/s]
Processed prompts:  81%|████████  | 826/1024 [00:19<00:05, 38.67it/s, est. speed input: 44408.57 toks/s, output: 43.37 toks/s]
Processed prompts:  81%|████████▏ | 834/1024 [00:19<00:04, 38.58it/s, est. speed input: 44353.30 toks/s, output: 43.31 toks/s]
Processed prompts:  82%|████████▏ | 842/1024 [00:19<00:04, 38.47it/s, est. speed input: 44296.95 toks/s, output: 43.26 toks/s]
Processed prompts:  83%|████████▎ | 850/1024 [00:19<00:04, 38.49it/s, est. speed input: 44246.01 toks/s, output: 43.21 toks/s]
Processed prompts:  84%|████████▍ | 858/1024 [00:19<00:04, 38.35it/s, est. speed input: 44189.81 toks/s, output: 43.15 toks/s]
Processed prompts:  85%|████████▍ | 866/1024 [00:20<00:04, 38.42it/s, est. speed input: 44141.44 toks/s, output: 43.11 toks/s]
Processed prompts:  85%|████████▌ | 874/1024 [00:20<00:03, 38.48it/s, est. speed input: 44094.77 toks/s, output: 43.06 toks/s]
Processed prompts:  86%|████████▌ | 882/1024 [00:20<00:03, 38.45it/s, est. speed input: 44045.94 toks/s, output: 43.01 toks/s]
Processed prompts:  87%|████████▋ | 890/1024 [00:20<00:03, 38.49it/s, est. speed input: 44000.41 toks/s, output: 42.97 toks/s]
Processed prompts:  88%|████████▊ | 898/1024 [00:20<00:03, 38.39it/s, est. speed input: 43951.03 toks/s, output: 42.92 toks/s]
Processed prompts:  88%|████████▊ | 906/1024 [00:21<00:03, 38.36it/s, est. speed input: 43904.28 toks/s, output: 42.88 toks/s]
Processed prompts:  89%|████████▉ | 914/1024 [00:21<00:02, 38.35it/s, est. speed input: 43858.87 toks/s, output: 42.83 toks/s]
Processed prompts:  90%|█████████ | 922/1024 [00:21<00:02, 38.41it/s, est. speed input: 43816.61 toks/s, output: 42.79 toks/s]
Processed prompts:  91%|█████████ | 930/1024 [00:21<00:02, 38.39it/s, est. speed input: 43772.78 toks/s, output: 42.75 toks/s]
Processed prompts:  92%|█████████▏| 938/1024 [00:21<00:02, 38.28it/s, est. speed input: 43726.54 toks/s, output: 42.70 toks/s]
Processed prompts:  92%|█████████▏| 946/1024 [00:22<00:02, 38.36it/s, est. speed input: 43686.66 toks/s, output: 42.66 toks/s]
Processed prompts:  93%|█████████▎| 954/1024 [00:22<00:01, 38.37it/s, est. speed input: 43645.88 toks/s, output: 42.62 toks/s]
Processed prompts:  94%|█████████▍| 962/1024 [00:22<00:01, 38.41it/s, est. speed input: 43607.00 toks/s, output: 42.58 toks/s]
Processed prompts:  95%|█████████▍| 970/1024 [00:22<00:01, 38.45it/s, est. speed input: 43569.30 toks/s, output: 42.55 toks/s]
Processed prompts:  96%|█████████▌| 978/1024 [00:23<00:01, 38.34it/s, est. speed input: 43527.68 toks/s, output: 42.51 toks/s]
Processed prompts:  96%|█████████▋| 986/1024 [00:23<00:00, 38.40it/s, est. speed input: 43491.29 toks/s, output: 42.47 toks/s]
Processed prompts:  97%|█████████▋| 994/1024 [00:23<00:00, 38.36it/s, est. speed input: 43452.83 toks/s, output: 42.43 toks/s]
Processed prompts:  98%|█████████▊| 1002/1024 [00:23<00:00, 38.37it/s, est. speed input: 43416.38 toks/s, output: 42.40 toks/s]
Processed prompts:  99%|█████████▊| 1010/1024 [00:23<00:00, 38.30it/s, est. speed input: 43377.99 toks/s, output: 42.36 toks/s]
Processed prompts:  99%|█████████▉| 1018/1024 [00:24<00:00, 39.74it/s, est. speed input: 43387.44 toks/s, output: 42.37 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:24<00:00, 39.74it/s, est. speed input: 43642.40 toks/s, output: 42.62 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:24<00:00, 42.62it/s, est. speed input: 43642.40 toks/s, output: 42.62 toks/s]
[rank0]:[W126 11:37:24.858716353 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 71.8s

测试结果:
  Requests/s:   38.36
  Tokens/s:     39318.67
  Total Reqs:   1024
  Elapsed:      26.69s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     39280.31

============================================================
[6/8] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 11:37:44 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1298480) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1298480) WARNING 01-26 11:38:02 [backends.py:609] Failed to read file <frozen os>
Throughput: 39.53 requests/s, 40520.23 total tokens/s, 39.53 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-26 11:37:44] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:37:44] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:37:44] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 11:37:44] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:37:44] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:37:44] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:37:44] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:37:44] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:37:44] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:37:44] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:37:44] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:37:44] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:37:44] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:37:44] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 11:37:51] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:37:52] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:37:52] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 11:37:52] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:37:52] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:37:52] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:37:52] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:37:52] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:37:52] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:37:52] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:37:52] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:37:52] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:37:52] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:37:52] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1298480) [2026-01-26 11:37:54] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1298480) [2026-01-26 11:37:54] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1298480) [2026-01-26 11:37:54] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1298480) [2026-01-26 11:37:54] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=1298480) [2026-01-26 11:37:54] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=1298480) [2026-01-26 11:37:54] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=1298480) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1298480) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.86it/s]
(EngineCore_DP0 pid=1298480) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.33it/s]
(EngineCore_DP0 pid=1298480) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.39it/s]
(EngineCore_DP0 pid=1298480) 
(EngineCore_DP0 pid=1298480) [2026-01-26 11:37:56] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=1298480) [2026-01-26 11:37:56] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16588800 bytes
(EngineCore_DP0 pid=1298480) [2026-01-26 11:37:56] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=1298480) [2026-01-26 11:37:56] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12902400 bytes
(EngineCore_DP0 pid=1298480) [2026-01-26 11:37:56] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=1298480) [2026-01-26 11:37:56] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 136396800 bytes
(EngineCore_DP0 pid=1298480) [2026-01-26 11:37:56] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=1298480) [2026-01-26 11:37:56] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 67952640 bytes
(EngineCore_DP0 pid=1298480) [rank0]:W0126 11:38:08.078000 1298480 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1298480) [rank0]:W0126 11:38:08.156000 1298480 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1298480) [rank0]:W0126 11:38:09.106000 1298480 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1298480) [rank0]:W0126 11:38:09.233000 1298480 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1298480) 2026-01-26 11:38:13,189 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1298480) 2026-01-26 11:38:13,216 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1298480) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00, 14.41it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00, 10.37it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:01<00:00,  4.45it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:01<00:00,  5.77it/s]
(EngineCore_DP0 pid=1298480) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00, 18.84it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00, 19.47it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 19.51it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   1%|          | 24/2048 [00:00<00:08, 237.14it/s]
Adding requests:   3%|▎         | 64/2048 [00:00<00:06, 329.88it/s]
Adding requests:   5%|▍         | 100/2048 [00:00<00:05, 341.25it/s]
Adding requests:   7%|▋         | 138/2048 [00:00<00:05, 354.60it/s]
Adding requests:   9%|▊         | 177/2048 [00:00<00:05, 366.83it/s]
Adding requests:  11%|█         | 218/2048 [00:00<00:04, 381.07it/s]
Adding requests:  13%|█▎        | 257/2048 [00:00<00:04, 379.42it/s]
Adding requests:  15%|█▍        | 297/2048 [00:00<00:04, 385.65it/s]
Adding requests:  17%|█▋        | 338/2048 [00:00<00:04, 390.59it/s]
Adding requests:  18%|█▊        | 378/2048 [00:01<00:04, 390.60it/s]
Adding requests:  21%|██        | 420/2048 [00:01<00:04, 398.16it/s]
Adding requests:  22%|██▏       | 460/2048 [00:01<00:04, 393.73it/s]
Adding requests:  25%|██▍       | 503/2048 [00:01<00:03, 402.45it/s]
Adding requests:  27%|██▋       | 546/2048 [00:01<00:03, 407.48it/s]
Adding requests:  29%|██▊       | 587/2048 [00:01<00:03, 402.43it/s]
Adding requests:  31%|███       | 628/2048 [00:01<00:03, 396.91it/s]
Adding requests:  33%|███▎      | 668/2048 [00:01<00:03, 385.21it/s]
Adding requests:  35%|███▍      | 709/2048 [00:01<00:03, 390.81it/s]
Adding requests:  37%|███▋      | 749/2048 [00:01<00:03, 379.05it/s]
Adding requests:  39%|███▊      | 789/2048 [00:02<00:03, 382.47it/s]
Adding requests:  41%|████      | 830/2048 [00:02<00:03, 389.18it/s]
Adding requests:  42%|████▏     | 870/2048 [00:02<00:03, 390.44it/s]
Adding requests:  44%|████▍     | 910/2048 [00:02<00:02, 392.18it/s]
Adding requests:  46%|████▋     | 950/2048 [00:02<00:02, 384.94it/s]
Adding requests:  48%|████▊     | 990/2048 [00:02<00:02, 386.61it/s]
Adding requests:  50%|█████     | 1029/2048 [00:02<00:02, 385.24it/s]
Adding requests:  52%|█████▏    | 1068/2048 [00:02<00:02, 381.58it/s]
Adding requests:  54%|█████▍    | 1107/2048 [00:02<00:02, 380.14it/s]
Adding requests:  56%|█████▌    | 1148/2048 [00:02<00:02, 385.80it/s]
Adding requests:  58%|█████▊    | 1187/2048 [00:03<00:02, 384.92it/s]
Adding requests:  60%|█████▉    | 1228/2048 [00:03<00:02, 390.91it/s]
Adding requests:  62%|██████▏   | 1268/2048 [00:03<00:02, 387.92it/s]
Adding requests:  64%|██████▍   | 1307/2048 [00:03<00:01, 386.79it/s]
Adding requests:  66%|██████▌   | 1347/2048 [00:03<00:01, 388.23it/s]
Adding requests:  68%|██████▊   | 1387/2048 [00:03<00:01, 391.07it/s]
Adding requests:  70%|██████▉   | 1427/2048 [00:03<00:01, 389.34it/s]
Adding requests:  72%|███████▏  | 1467/2048 [00:03<00:01, 391.76it/s]
Adding requests:  74%|███████▎  | 1508/2048 [00:03<00:01, 395.24it/s]
Adding requests:  76%|███████▌  | 1548/2048 [00:04<00:01, 392.95it/s]
Adding requests:  78%|███████▊  | 1588/2048 [00:04<00:01, 385.60it/s]
Adding requests:  79%|███████▉  | 1627/2048 [00:04<00:01, 379.85it/s]
Adding requests:  81%|████████▏ | 1666/2048 [00:04<00:01, 372.43it/s]
Adding requests:  83%|████████▎ | 1706/2048 [00:04<00:00, 380.01it/s]
Adding requests:  85%|████████▌ | 1746/2048 [00:04<00:00, 383.69it/s]
Adding requests:  87%|████████▋ | 1787/2048 [00:04<00:00, 388.17it/s]
Adding requests:  89%|████████▉ | 1826/2048 [00:04<00:00, 387.35it/s]
Adding requests:  91%|█████████ | 1866/2048 [00:04<00:00, 388.33it/s]
Adding requests:  93%|█████████▎| 1906/2048 [00:04<00:00, 390.10it/s]
Adding requests:  95%|█████████▌| 1946/2048 [00:05<00:00, 386.02it/s]
Adding requests:  97%|█████████▋| 1986/2048 [00:05<00:00, 386.74it/s]
Adding requests:  99%|█████████▉| 2025/2048 [00:05<00:00, 378.86it/s]
Adding requests: 100%|██████████| 2048/2048 [00:05<00:00, 384.77it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  10%|█         | 210/2048 [00:00<00:03, 472.80it/s, est. speed input: 484199.26 toks/s, output: 472.82 toks/s]
Processed prompts:  13%|█▎        | 258/2048 [00:01<00:14, 127.59it/s, est. speed input: 159009.36 toks/s, output: 155.28 toks/s]
Processed prompts:  14%|█▎        | 280/2048 [00:02<00:16, 107.23it/s, est. speed input: 138524.24 toks/s, output: 135.28 toks/s]
Processed prompts:  14%|█▍        | 295/2048 [00:02<00:20, 87.53it/s, est. speed input: 122096.58 toks/s, output: 119.23 toks/s] 
Processed prompts:  15%|█▍        | 306/2048 [00:02<00:24, 71.51it/s, est. speed input: 109807.40 toks/s, output: 107.23 toks/s]
Processed prompts:  16%|█▌        | 322/2048 [00:03<00:27, 61.93it/s, est. speed input: 101079.30 toks/s, output: 98.71 toks/s] 
Processed prompts:  17%|█▋        | 338/2048 [00:03<00:30, 55.33it/s, est. speed input: 94400.88 toks/s, output: 92.19 toks/s] 
Processed prompts:  17%|█▋        | 354/2048 [00:04<00:33, 50.49it/s, est. speed input: 88949.83 toks/s, output: 86.86 toks/s]
Processed prompts:  18%|█▊        | 370/2048 [00:04<00:35, 47.26it/s, est. speed input: 84586.50 toks/s, output: 82.60 toks/s]
Processed prompts:  19%|█▉        | 386/2048 [00:04<00:37, 44.91it/s, est. speed input: 80906.23 toks/s, output: 79.01 toks/s]
Processed prompts:  20%|█▉        | 402/2048 [00:05<00:38, 43.26it/s, est. speed input: 77793.23 toks/s, output: 75.97 toks/s]
Processed prompts:  20%|██        | 418/2048 [00:05<00:38, 42.15it/s, est. speed input: 75145.15 toks/s, output: 73.38 toks/s]
Processed prompts:  21%|██        | 434/2048 [00:06<00:38, 42.02it/s, est. speed input: 73094.79 toks/s, output: 71.38 toks/s]
Processed prompts:  22%|██▏       | 450/2048 [00:06<00:38, 41.27it/s, est. speed input: 71063.02 toks/s, output: 69.40 toks/s]
Processed prompts:  23%|██▎       | 466/2048 [00:06<00:38, 40.70it/s, est. speed input: 69251.63 toks/s, output: 67.63 toks/s]
Processed prompts:  24%|██▎       | 482/2048 [00:07<00:38, 40.34it/s, est. speed input: 67651.99 toks/s, output: 66.07 toks/s]
Processed prompts:  24%|██▍       | 498/2048 [00:07<00:38, 40.14it/s, est. speed input: 66237.17 toks/s, output: 64.68 toks/s]
Processed prompts:  25%|██▌       | 514/2048 [00:08<00:38, 39.99it/s, est. speed input: 64960.26 toks/s, output: 63.44 toks/s]
Processed prompts:  26%|██▌       | 530/2048 [00:08<00:38, 39.88it/s, est. speed input: 63802.30 toks/s, output: 62.31 toks/s]
Processed prompts:  27%|██▋       | 546/2048 [00:08<00:37, 39.77it/s, est. speed input: 62740.41 toks/s, output: 61.27 toks/s]
Processed prompts:  27%|██▋       | 562/2048 [00:09<00:37, 39.72it/s, est. speed input: 61778.53 toks/s, output: 60.33 toks/s]
Processed prompts:  28%|██▊       | 578/2048 [00:09<00:37, 39.70it/s, est. speed input: 60900.44 toks/s, output: 59.47 toks/s]
Processed prompts:  29%|██▉       | 594/2048 [00:10<00:36, 39.68it/s, est. speed input: 60089.32 toks/s, output: 58.68 toks/s]
Processed prompts:  30%|██▉       | 610/2048 [00:10<00:36, 39.64it/s, est. speed input: 59336.32 toks/s, output: 57.95 toks/s]
Processed prompts:  31%|███       | 626/2048 [00:10<00:35, 39.62it/s, est. speed input: 58639.93 toks/s, output: 57.27 toks/s]
Processed prompts:  31%|███▏      | 642/2048 [00:11<00:35, 39.57it/s, est. speed input: 57987.58 toks/s, output: 56.63 toks/s]
Processed prompts:  32%|███▏      | 658/2048 [00:11<00:35, 39.56it/s, est. speed input: 57385.03 toks/s, output: 56.04 toks/s]
Processed prompts:  33%|███▎      | 674/2048 [00:12<00:34, 39.59it/s, est. speed input: 56826.99 toks/s, output: 55.50 toks/s]
Processed prompts:  34%|███▎      | 690/2048 [00:12<00:34, 39.53it/s, est. speed input: 56293.70 toks/s, output: 54.97 toks/s]
Processed prompts:  34%|███▍      | 706/2048 [00:12<00:33, 39.52it/s, est. speed input: 55797.85 toks/s, output: 54.49 toks/s]
Processed prompts:  35%|███▌      | 722/2048 [00:13<00:33, 39.52it/s, est. speed input: 55333.81 toks/s, output: 54.04 toks/s]
Processed prompts:  36%|███▌      | 738/2048 [00:13<00:33, 39.50it/s, est. speed input: 54894.32 toks/s, output: 53.61 toks/s]
Processed prompts:  37%|███▋      | 754/2048 [00:14<00:32, 39.47it/s, est. speed input: 54477.54 toks/s, output: 53.20 toks/s]
Processed prompts:  38%|███▊      | 770/2048 [00:14<00:32, 39.52it/s, est. speed input: 54092.52 toks/s, output: 52.82 toks/s]
Processed prompts:  38%|███▊      | 786/2048 [00:14<00:31, 40.17it/s, est. speed input: 53803.15 toks/s, output: 52.54 toks/s]
Processed prompts:  39%|███▉      | 802/2048 [00:15<00:31, 39.98it/s, est. speed input: 53452.25 toks/s, output: 52.20 toks/s]
Processed prompts:  40%|███▉      | 818/2048 [00:15<00:30, 39.82it/s, est. speed input: 53116.65 toks/s, output: 51.87 toks/s]
Processed prompts:  41%|████      | 834/2048 [00:16<00:30, 39.69it/s, est. speed input: 52796.06 toks/s, output: 51.56 toks/s]
Processed prompts:  42%|████▏     | 850/2048 [00:16<00:30, 39.64it/s, est. speed input: 52495.18 toks/s, output: 51.26 toks/s]
Processed prompts:  42%|████▏     | 866/2048 [00:16<00:29, 39.60it/s, est. speed input: 52208.03 toks/s, output: 50.98 toks/s]
Processed prompts:  43%|████▎     | 882/2048 [00:17<00:29, 39.56it/s, est. speed input: 51932.55 toks/s, output: 50.72 toks/s]
Processed prompts:  44%|████▍     | 898/2048 [00:17<00:29, 39.53it/s, est. speed input: 51670.00 toks/s, output: 50.46 toks/s]
Processed prompts:  45%|████▍     | 914/2048 [00:18<00:28, 39.52it/s, est. speed input: 51420.45 toks/s, output: 50.22 toks/s]
Processed prompts:  45%|████▌     | 930/2048 [00:18<00:28, 39.54it/s, est. speed input: 51183.82 toks/s, output: 49.98 toks/s]
Processed prompts:  46%|████▌     | 946/2048 [00:19<00:27, 39.39it/s, est. speed input: 50942.40 toks/s, output: 49.75 toks/s]
Processed prompts:  47%|████▋     | 962/2048 [00:19<00:27, 39.45it/s, est. speed input: 50726.36 toks/s, output: 49.54 toks/s]
Processed prompts:  48%|████▊     | 978/2048 [00:19<00:27, 39.38it/s, est. speed input: 50508.36 toks/s, output: 49.32 toks/s]
Processed prompts:  49%|████▊     | 994/2048 [00:20<00:26, 39.37it/s, est. speed input: 50303.03 toks/s, output: 49.12 toks/s]
Processed prompts:  49%|████▉     | 1010/2048 [00:20<00:26, 39.41it/s, est. speed input: 50109.97 toks/s, output: 48.94 toks/s]
Processed prompts:  50%|█████     | 1026/2048 [00:21<00:25, 39.39it/s, est. speed input: 49919.82 toks/s, output: 48.75 toks/s]
Processed prompts:  51%|█████     | 1042/2048 [00:21<00:25, 39.34it/s, est. speed input: 49734.50 toks/s, output: 48.57 toks/s]
Processed prompts:  52%|█████▏    | 1058/2048 [00:21<00:25, 39.35it/s, est. speed input: 49559.35 toks/s, output: 48.40 toks/s]
Processed prompts:  52%|█████▏    | 1074/2048 [00:22<00:24, 39.40it/s, est. speed input: 49393.72 toks/s, output: 48.24 toks/s]
Processed prompts:  53%|█████▎    | 1090/2048 [00:22<00:24, 39.36it/s, est. speed input: 49228.60 toks/s, output: 48.07 toks/s]
Processed prompts:  54%|█████▍    | 1106/2048 [00:23<00:23, 39.32it/s, est. speed input: 49068.41 toks/s, output: 47.92 toks/s]
Processed prompts:  55%|█████▍    | 1122/2048 [00:23<00:23, 39.37it/s, est. speed input: 48919.60 toks/s, output: 47.77 toks/s]
Processed prompts:  56%|█████▌    | 1138/2048 [00:23<00:23, 39.31it/s, est. speed input: 48768.83 toks/s, output: 47.63 toks/s]
Processed prompts:  56%|█████▋    | 1154/2048 [00:24<00:22, 39.33it/s, est. speed input: 48627.77 toks/s, output: 47.49 toks/s]
Processed prompts:  57%|█████▋    | 1170/2048 [00:24<00:22, 39.35it/s, est. speed input: 48491.32 toks/s, output: 47.35 toks/s]
Processed prompts:  58%|█████▊    | 1186/2048 [00:25<00:21, 39.35it/s, est. speed input: 48358.73 toks/s, output: 47.23 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [00:25<00:21, 40.07it/s, est. speed input: 48276.35 toks/s, output: 47.14 toks/s]
Processed prompts:  59%|█████▉    | 1218/2048 [00:25<00:20, 39.82it/s, est. speed input: 48149.18 toks/s, output: 47.02 toks/s]
Processed prompts:  60%|██████    | 1234/2048 [00:26<00:20, 40.33it/s, est. speed input: 48067.22 toks/s, output: 46.94 toks/s]
Processed prompts:  61%|██████    | 1250/2048 [00:26<00:19, 39.94it/s, est. speed input: 47943.68 toks/s, output: 46.82 toks/s]
Processed prompts:  62%|██████▏   | 1266/2048 [00:27<00:19, 39.74it/s, est. speed input: 47827.49 toks/s, output: 46.71 toks/s]
Processed prompts:  63%|██████▎   | 1282/2048 [00:27<00:19, 39.66it/s, est. speed input: 47718.39 toks/s, output: 46.60 toks/s]
Processed prompts:  63%|██████▎   | 1298/2048 [00:27<00:18, 39.52it/s, est. speed input: 47607.47 toks/s, output: 46.49 toks/s]
Processed prompts:  64%|██████▍   | 1314/2048 [00:28<00:18, 39.51it/s, est. speed input: 47504.81 toks/s, output: 46.39 toks/s]
Processed prompts:  65%|██████▍   | 1330/2048 [00:28<00:17, 40.06it/s, est. speed input: 47436.20 toks/s, output: 46.32 toks/s]
Processed prompts:  66%|██████▌   | 1346/2048 [00:29<00:17, 39.76it/s, est. speed input: 47331.61 toks/s, output: 46.22 toks/s]
Processed prompts:  67%|██████▋   | 1362/2048 [00:29<00:17, 39.68it/s, est. speed input: 47237.43 toks/s, output: 46.13 toks/s]
Processed prompts:  67%|██████▋   | 1378/2048 [00:29<00:16, 39.50it/s, est. speed input: 47138.83 toks/s, output: 46.03 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [00:30<00:16, 39.49it/s, est. speed input: 47048.59 toks/s, output: 45.95 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [00:30<00:16, 39.42it/s, est. speed input: 46957.75 toks/s, output: 45.86 toks/s]
Processed prompts:  70%|██████▉   | 1426/2048 [00:31<00:15, 39.35it/s, est. speed input: 46868.37 toks/s, output: 45.77 toks/s]
Processed prompts:  70%|███████   | 1442/2048 [00:31<00:15, 39.35it/s, est. speed input: 46783.57 toks/s, output: 45.69 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [00:31<00:14, 39.94it/s, est. speed input: 46730.49 toks/s, output: 45.64 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [00:32<00:14, 39.74it/s, est. speed input: 46648.44 toks/s, output: 45.56 toks/s]
Processed prompts:  73%|███████▎  | 1490/2048 [00:32<00:14, 39.60it/s, est. speed input: 46568.66 toks/s, output: 45.48 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [00:33<00:13, 39.45it/s, est. speed input: 46488.34 toks/s, output: 45.40 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [00:33<00:13, 40.06it/s, est. speed input: 46442.98 toks/s, output: 45.35 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [00:33<00:12, 39.85it/s, est. speed input: 46369.55 toks/s, output: 45.28 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [00:34<00:12, 40.25it/s, est. speed input: 46322.53 toks/s, output: 45.24 toks/s]
Processed prompts:  77%|███████▋  | 1570/2048 [00:34<00:11, 39.95it/s, est. speed input: 46250.88 toks/s, output: 45.17 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [00:35<00:11, 39.74it/s, est. speed input: 46181.00 toks/s, output: 45.10 toks/s]
Processed prompts:  78%|███████▊  | 1602/2048 [00:35<00:11, 39.55it/s, est. speed input: 46110.59 toks/s, output: 45.03 toks/s]
Processed prompts:  79%|███████▉  | 1618/2048 [00:35<00:10, 40.15it/s, est. speed input: 46073.10 toks/s, output: 44.99 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [00:36<00:10, 39.84it/s, est. speed input: 46005.54 toks/s, output: 44.93 toks/s]
Processed prompts:  81%|████████  | 1650/2048 [00:36<00:10, 39.65it/s, est. speed input: 45940.67 toks/s, output: 44.86 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [00:37<00:09, 39.50it/s, est. speed input: 45876.40 toks/s, output: 44.80 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [00:37<00:09, 39.40it/s, est. speed input: 45813.93 toks/s, output: 44.74 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [00:38<00:08, 39.37it/s, est. speed input: 45753.96 toks/s, output: 44.68 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [00:38<00:08, 39.24it/s, est. speed input: 45691.40 toks/s, output: 44.62 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [00:38<00:07, 39.89it/s, est. speed input: 45659.58 toks/s, output: 44.59 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [00:39<00:07, 40.33it/s, est. speed input: 45627.35 toks/s, output: 44.56 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [00:39<00:07, 39.95it/s, est. speed input: 45569.26 toks/s, output: 44.50 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [00:40<00:06, 39.72it/s, est. speed input: 45513.71 toks/s, output: 44.45 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [00:40<00:06, 39.48it/s, est. speed input: 45456.53 toks/s, output: 44.39 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [00:40<00:06, 39.39it/s, est. speed input: 45403.25 toks/s, output: 44.34 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [00:41<00:05, 39.30it/s, est. speed input: 45349.76 toks/s, output: 44.29 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [00:41<00:05, 39.21it/s, est. speed input: 45296.33 toks/s, output: 44.23 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [00:42<00:04, 39.25it/s, est. speed input: 45248.11 toks/s, output: 44.19 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [00:42<00:04, 39.15it/s, est. speed input: 45195.88 toks/s, output: 44.14 toks/s]
Processed prompts:  92%|█████████▏| 1890/2048 [00:42<00:03, 39.84it/s, est. speed input: 45172.15 toks/s, output: 44.11 toks/s]
Processed prompts:  93%|█████████▎| 1906/2048 [00:43<00:03, 39.62it/s, est. speed input: 45123.66 toks/s, output: 44.07 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [00:43<00:03, 39.43it/s, est. speed input: 45074.83 toks/s, output: 44.02 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [00:44<00:02, 39.38it/s, est. speed input: 45029.74 toks/s, output: 43.97 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [00:44<00:02, 39.21it/s, est. speed input: 44980.84 toks/s, output: 43.93 toks/s]
Processed prompts:  96%|█████████▌| 1970/2048 [00:44<00:01, 39.19it/s, est. speed input: 44936.26 toks/s, output: 43.88 toks/s]
Processed prompts:  97%|█████████▋| 1986/2048 [00:45<00:01, 39.80it/s, est. speed input: 44913.57 toks/s, output: 43.86 toks/s]
Processed prompts:  98%|█████████▊| 2002/2048 [00:45<00:01, 39.56it/s, est. speed input: 44868.98 toks/s, output: 43.82 toks/s]
Processed prompts:  99%|█████████▊| 2018/2048 [00:46<00:00, 39.42it/s, est. speed input: 44826.13 toks/s, output: 43.78 toks/s]
Processed prompts:  99%|█████████▉| 2034/2048 [00:46<00:00, 40.11it/s, est. speed input: 44809.75 toks/s, output: 43.76 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:46<00:00, 40.11it/s, est. speed input: 45117.93 toks/s, output: 44.06 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:46<00:00, 44.06it/s, est. speed input: 45117.93 toks/s, output: 44.06 toks/s]
[rank0]:[W126 11:39:08.370224119 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 103.4s

测试结果:
  Requests/s:   39.53
  Tokens/s:     40520.23
  Total Reqs:   2048
  Elapsed:      51.81s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     40480.70

============================================================
[7/8] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 11:39:38 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1300495) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1300495) WARNING 01-26 11:39:56 [backends.py:609] Failed to read file <frozen os>
Throughput: 38.95 requests/s, 39928.40 total tokens/s, 38.95 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-26 11:39:38] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:39:38] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:39:38] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 11:39:38] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:39:38] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:39:38] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:39:38] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:39:38] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:39:38] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:39:38] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:39:38] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:39:38] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:39:38] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:39:38] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 11:39:45] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:39:46] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:39:46] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 11:39:46] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:39:46] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:39:46] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:39:46] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:39:46] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:39:46] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:39:46] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:39:46] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:39:46] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:39:46] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:39:46] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1300495) [2026-01-26 11:39:47] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1300495) [2026-01-26 11:39:47] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1300495) [2026-01-26 11:39:47] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1300495) [2026-01-26 11:39:47] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=1300495) [2026-01-26 11:39:47] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=1300495) [2026-01-26 11:39:47] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=1300495) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1300495) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.85it/s]
(EngineCore_DP0 pid=1300495) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.32it/s]
(EngineCore_DP0 pid=1300495) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.38it/s]
(EngineCore_DP0 pid=1300495) 
(EngineCore_DP0 pid=1300495) [2026-01-26 11:39:50] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=1300495) [2026-01-26 11:39:50] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16588800 bytes
(EngineCore_DP0 pid=1300495) [2026-01-26 11:39:50] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=1300495) [2026-01-26 11:39:50] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12902400 bytes
(EngineCore_DP0 pid=1300495) [2026-01-26 11:39:50] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=1300495) [2026-01-26 11:39:50] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 136396800 bytes
(EngineCore_DP0 pid=1300495) [2026-01-26 11:39:50] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=1300495) [2026-01-26 11:39:50] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 67952640 bytes
(EngineCore_DP0 pid=1300495) [rank0]:W0126 11:40:01.989000 1300495 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1300495) [rank0]:W0126 11:40:02.068000 1300495 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1300495) [rank0]:W0126 11:40:03.017000 1300495 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1300495) [rank0]:W0126 11:40:03.146000 1300495 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1300495) 2026-01-26 11:40:07,086 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1300495) 2026-01-26 11:40:07,112 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1300495) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▉         | 1/11 [00:00<00:08,  1.17it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:01<00:04,  2.01it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:01<00:01,  4.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 6/11 [00:01<00:00,  7.01it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:01<00:00,  9.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 10/11 [00:01<00:00, 11.67it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00,  6.87it/s]
(EngineCore_DP0 pid=1300495) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00, 18.92it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 5/7 [00:00<00:00, 19.96it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00,  7.93it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00,  9.47it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 26/4096 [00:00<00:16, 253.29it/s]
Adding requests:   2%|▏         | 66/4096 [00:00<00:11, 338.23it/s]
Adding requests:   3%|▎         | 103/4096 [00:00<00:11, 349.41it/s]
Adding requests:   3%|▎         | 141/4096 [00:00<00:10, 359.59it/s]
Adding requests:   4%|▍         | 180/4096 [00:00<00:10, 370.24it/s]
Adding requests:   5%|▌         | 222/4096 [00:00<00:10, 384.40it/s]
Adding requests:   6%|▋         | 261/4096 [00:00<00:10, 382.72it/s]
Adding requests:   7%|▋         | 301/4096 [00:00<00:09, 386.04it/s]
Adding requests:   8%|▊         | 342/4096 [00:00<00:09, 391.61it/s]
Adding requests:   9%|▉         | 382/4096 [00:01<00:09, 394.14it/s]
Adding requests:  10%|█         | 424/4096 [00:01<00:09, 400.96it/s]
Adding requests:  11%|█▏        | 465/4096 [00:01<00:09, 396.46it/s]
Adding requests:  12%|█▏        | 509/4096 [00:01<00:08, 406.13it/s]
Adding requests:  13%|█▎        | 551/4096 [00:01<00:08, 408.47it/s]
Adding requests:  14%|█▍        | 592/4096 [00:01<00:08, 403.16it/s]
Adding requests:  15%|█▌        | 633/4096 [00:01<00:08, 398.14it/s]
Adding requests:  16%|█▋        | 673/4096 [00:01<00:08, 388.19it/s]
Adding requests:  17%|█▋        | 715/4096 [00:01<00:08, 394.39it/s]
Adding requests:  18%|█▊        | 755/4096 [00:01<00:08, 386.92it/s]
Adding requests:  19%|█▉        | 794/4096 [00:02<00:08, 387.13it/s]
Adding requests:  20%|██        | 835/4096 [00:02<00:08, 393.12it/s]
Adding requests:  21%|██▏       | 875/4096 [00:02<00:08, 393.12it/s]
Adding requests:  22%|██▏       | 915/4096 [00:02<00:08, 391.81it/s]
Adding requests:  23%|██▎       | 955/4096 [00:02<00:08, 390.88it/s]
Adding requests:  24%|██▍       | 995/4096 [00:02<00:08, 383.97it/s]
Adding requests:  25%|██▌       | 1034/4096 [00:02<00:07, 384.45it/s]
Adding requests:  26%|██▌       | 1073/4096 [00:02<00:07, 383.32it/s]
Adding requests:  27%|██▋       | 1112/4096 [00:02<00:08, 372.61it/s]
Adding requests:  28%|██▊       | 1151/4096 [00:02<00:07, 376.53it/s]
Adding requests:  29%|██▉       | 1189/4096 [00:03<00:07, 377.29it/s]
Adding requests:  30%|███       | 1230/4096 [00:03<00:07, 385.68it/s]
Adding requests:  31%|███       | 1269/4096 [00:03<00:07, 383.68it/s]
Adding requests:  32%|███▏      | 1308/4096 [00:03<00:07, 383.06it/s]
Adding requests:  33%|███▎      | 1348/4096 [00:03<00:07, 386.62it/s]
Adding requests:  34%|███▍      | 1388/4096 [00:03<00:06, 388.63it/s]
Adding requests:  35%|███▍      | 1427/4096 [00:03<00:06, 386.48it/s]
Adding requests:  36%|███▌      | 1467/4096 [00:03<00:06, 389.78it/s]
Adding requests:  37%|███▋      | 1508/4096 [00:03<00:06, 392.77it/s]
Adding requests:  38%|███▊      | 1548/4096 [00:04<00:06, 391.96it/s]
Adding requests:  39%|███▉      | 1588/4096 [00:04<00:06, 383.58it/s]
Adding requests:  40%|███▉      | 1627/4096 [00:04<00:06, 378.41it/s]
Adding requests:  41%|████      | 1665/4096 [00:04<00:06, 370.98it/s]
Adding requests:  42%|████▏     | 1705/4096 [00:04<00:06, 376.93it/s]
Adding requests:  43%|████▎     | 1745/4096 [00:04<00:06, 382.66it/s]
Adding requests:  44%|████▎     | 1785/4096 [00:04<00:05, 387.24it/s]
Adding requests:  45%|████▍     | 1824/4096 [00:04<00:05, 384.65it/s]
Adding requests:  45%|████▌     | 1863/4096 [00:04<00:05, 384.54it/s]
Adding requests:  46%|████▋     | 1902/4096 [00:04<00:05, 385.42it/s]
Adding requests:  47%|████▋     | 1944/4096 [00:05<00:05, 392.94it/s]
Adding requests:  48%|████▊     | 1984/4096 [00:05<00:05, 391.71it/s]
Adding requests:  49%|████▉     | 2024/4096 [00:05<00:05, 381.27it/s]
Adding requests:  50%|█████     | 2063/4096 [00:05<00:05, 378.71it/s]
Adding requests:  51%|█████▏    | 2101/4096 [00:05<00:05, 374.67it/s]
Adding requests:  52%|█████▏    | 2140/4096 [00:05<00:05, 377.06it/s]
Adding requests:  53%|█████▎    | 2178/4096 [00:05<00:05, 371.28it/s]
Adding requests:  54%|█████▍    | 2216/4096 [00:05<00:05, 372.36it/s]
Adding requests:  55%|█████▌    | 2256/4096 [00:05<00:04, 380.17it/s]
Adding requests:  56%|█████▌    | 2296/4096 [00:05<00:04, 385.16it/s]
Adding requests:  57%|█████▋    | 2335/4096 [00:06<00:04, 377.23it/s]
Adding requests:  58%|█████▊    | 2375/4096 [00:06<00:04, 383.02it/s]
Adding requests:  59%|█████▉    | 2417/4096 [00:06<00:04, 391.81it/s]
Adding requests:  60%|█████▉    | 2457/4096 [00:06<00:04, 389.29it/s]
Adding requests:  61%|██████    | 2498/4096 [00:06<00:04, 392.76it/s]
Adding requests:  62%|██████▏   | 2538/4096 [00:06<00:03, 393.93it/s]
Adding requests:  63%|██████▎   | 2582/4096 [00:06<00:03, 405.04it/s]
Adding requests:  64%|██████▍   | 2623/4096 [00:06<00:03, 398.10it/s]
Adding requests:  65%|██████▌   | 2663/4096 [00:06<00:03, 390.47it/s]
Adding requests:  66%|██████▌   | 2703/4096 [00:07<00:03, 386.90it/s]
Adding requests:  67%|██████▋   | 2742/4096 [00:07<00:03, 383.19it/s]
Adding requests:  68%|██████▊   | 2783/4096 [00:07<00:03, 389.85it/s]
Adding requests:  69%|██████▉   | 2825/4096 [00:07<00:03, 395.71it/s]
Adding requests:  70%|██████▉   | 2865/4096 [00:07<00:03, 395.02it/s]
Adding requests:  71%|███████   | 2905/4096 [00:07<00:03, 394.98it/s]
Adding requests:  72%|███████▏  | 2946/4096 [00:07<00:02, 397.71it/s]
Adding requests:  73%|███████▎  | 2986/4096 [00:07<00:02, 394.89it/s]
Adding requests:  74%|███████▍  | 3028/4096 [00:07<00:02, 398.89it/s]
Adding requests:  75%|███████▍  | 3070/4096 [00:07<00:02, 403.22it/s]
Adding requests:  76%|███████▌  | 3111/4096 [00:08<00:02, 401.80it/s]
Adding requests:  77%|███████▋  | 3152/4096 [00:08<00:02, 401.83it/s]
Adding requests:  78%|███████▊  | 3193/4096 [00:08<00:02, 396.94it/s]
Adding requests:  79%|███████▉  | 3234/4096 [00:08<00:02, 398.22it/s]
Adding requests:  80%|███████▉  | 3274/4096 [00:08<00:02, 386.52it/s]
Adding requests:  81%|████████  | 3313/4096 [00:08<00:02, 379.43it/s]
Adding requests:  82%|████████▏ | 3352/4096 [00:08<00:01, 380.71it/s]
Adding requests:  83%|████████▎ | 3392/4096 [00:08<00:01, 385.58it/s]
Adding requests:  84%|████████▍ | 3432/4096 [00:08<00:01, 389.38it/s]
Adding requests:  85%|████████▍ | 3472/4096 [00:08<00:01, 392.04it/s]
Adding requests:  86%|████████▌ | 3512/4096 [00:09<00:01, 392.78it/s]
Adding requests:  87%|████████▋ | 3554/4096 [00:09<00:01, 398.85it/s]
Adding requests:  88%|████████▊ | 3594/4096 [00:09<00:01, 398.20it/s]
Adding requests:  89%|████████▊ | 3635/4096 [00:09<00:01, 399.18it/s]
Adding requests:  90%|████████▉ | 3675/4096 [00:09<00:01, 378.95it/s]
Adding requests:  91%|█████████ | 3714/4096 [00:09<00:01, 380.92it/s]
Adding requests:  92%|█████████▏| 3753/4096 [00:09<00:00, 379.11it/s]
Adding requests:  93%|█████████▎| 3792/4096 [00:09<00:00, 368.93it/s]
Adding requests:  94%|█████████▎| 3830/4096 [00:09<00:00, 367.87it/s]
Adding requests:  94%|█████████▍| 3870/4096 [00:10<00:00, 375.84it/s]
Adding requests:  95%|█████████▌| 3908/4096 [00:10<00:00, 371.83it/s]
Adding requests:  96%|█████████▋| 3946/4096 [00:10<00:00, 372.79it/s]
Adding requests:  97%|█████████▋| 3984/4096 [00:10<00:00, 372.60it/s]
Adding requests:  98%|█████████▊| 4022/4096 [00:10<00:00, 374.43it/s]
Adding requests:  99%|█████████▉| 4060/4096 [00:10<00:00, 375.15it/s]
Adding requests: 100%|██████████| 4096/4096 [00:10<00:00, 385.61it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  10%|▉         | 396/4096 [00:00<00:05, 621.89it/s, est. speed input: 636839.19 toks/s, output: 621.90 toks/s]
Processed prompts:  11%|█         | 459/4096 [00:01<00:13, 274.09it/s, est. speed input: 328176.22 toks/s, output: 320.48 toks/s]
Processed prompts:  12%|█▏        | 490/4096 [00:02<00:22, 160.20it/s, est. speed input: 223664.64 toks/s, output: 218.42 toks/s]
Processed prompts:  12%|█▏        | 509/4096 [00:03<00:34, 104.31it/s, est. speed input: 170571.86 toks/s, output: 166.57 toks/s]
Processed prompts:  13%|█▎        | 524/4096 [00:03<00:49, 72.64it/s, est. speed input: 138689.71 toks/s, output: 135.44 toks/s] 
Processed prompts:  14%|█▎        | 556/4096 [00:04<00:57, 61.13it/s, est. speed input: 121571.36 toks/s, output: 118.72 toks/s]
Processed prompts:  14%|█▍        | 588/4096 [00:05<01:05, 53.93it/s, est. speed input: 109540.84 toks/s, output: 106.97 toks/s]
Processed prompts:  15%|█▌        | 620/4096 [00:06<01:10, 49.23it/s, est. speed input: 100590.66 toks/s, output: 98.23 toks/s] 
Processed prompts:  16%|█▌        | 652/4096 [00:07<01:14, 46.09it/s, est. speed input: 93678.48 toks/s, output: 91.48 toks/s] 
Processed prompts:  17%|█▋        | 684/4096 [00:07<01:17, 44.01it/s, est. speed input: 88213.67 toks/s, output: 86.15 toks/s]
Processed prompts:  17%|█▋        | 716/4096 [00:08<01:19, 42.55it/s, est. speed input: 83744.08 toks/s, output: 81.78 toks/s]
Processed prompts:  18%|█▊        | 748/4096 [00:09<01:20, 41.52it/s, est. speed input: 80022.64 toks/s, output: 78.15 toks/s]
Processed prompts:  19%|█▉        | 780/4096 [00:10<01:20, 41.12it/s, est. speed input: 77035.59 toks/s, output: 75.23 toks/s]
Processed prompts:  20%|█▉        | 812/4096 [00:11<01:20, 40.54it/s, est. speed input: 74346.28 toks/s, output: 72.60 toks/s]
Processed prompts:  21%|██        | 844/4096 [00:11<01:20, 40.15it/s, est. speed input: 72024.40 toks/s, output: 70.34 toks/s]
Processed prompts:  21%|██▏       | 876/4096 [00:12<01:20, 39.88it/s, est. speed input: 69999.96 toks/s, output: 68.36 toks/s]
Processed prompts:  22%|██▏       | 908/4096 [00:13<01:20, 39.66it/s, est. speed input: 68207.91 toks/s, output: 66.61 toks/s]
Processed prompts:  23%|██▎       | 940/4096 [00:14<01:19, 39.52it/s, est. speed input: 66619.74 toks/s, output: 65.06 toks/s]
Processed prompts:  24%|██▎       | 972/4096 [00:15<01:19, 39.42it/s, est. speed input: 65202.20 toks/s, output: 63.67 toks/s]
Processed prompts:  25%|██▍       | 1004/4096 [00:16<01:18, 39.35it/s, est. speed input: 63929.07 toks/s, output: 62.43 toks/s]
Processed prompts:  25%|██▌       | 1036/4096 [00:16<01:17, 39.30it/s, est. speed input: 62779.04 toks/s, output: 61.31 toks/s]
Processed prompts:  26%|██▌       | 1068/4096 [00:17<01:17, 39.30it/s, est. speed input: 61743.41 toks/s, output: 60.30 toks/s]
Processed prompts:  27%|██▋       | 1100/4096 [00:18<01:16, 39.25it/s, est. speed input: 60786.82 toks/s, output: 59.36 toks/s]
Processed prompts:  28%|██▊       | 1132/4096 [00:19<01:15, 39.17it/s, est. speed input: 59902.36 toks/s, output: 58.50 toks/s]
Processed prompts:  28%|██▊       | 1164/4096 [00:20<01:14, 39.13it/s, est. speed input: 59092.98 toks/s, output: 57.71 toks/s]
Processed prompts:  29%|██▉       | 1196/4096 [00:20<01:13, 39.41it/s, est. speed input: 58405.53 toks/s, output: 57.04 toks/s]
Processed prompts:  30%|██▉       | 1228/4096 [00:21<01:12, 39.60it/s, est. speed input: 57765.88 toks/s, output: 56.41 toks/s]
Processed prompts:  31%|███       | 1260/4096 [00:22<01:11, 39.45it/s, est. speed input: 57123.90 toks/s, output: 55.78 toks/s]
Processed prompts:  32%|███▏      | 1292/4096 [00:23<01:11, 39.33it/s, est. speed input: 56524.82 toks/s, output: 55.20 toks/s]
Processed prompts:  32%|███▏      | 1324/4096 [00:24<01:10, 39.54it/s, est. speed input: 56011.38 toks/s, output: 54.70 toks/s]
Processed prompts:  33%|███▎      | 1356/4096 [00:25<01:09, 39.39it/s, est. speed input: 55487.19 toks/s, output: 54.19 toks/s]
Processed prompts:  34%|███▍      | 1388/4096 [00:25<01:08, 39.28it/s, est. speed input: 54994.89 toks/s, output: 53.71 toks/s]
Processed prompts:  35%|███▍      | 1420/4096 [00:26<01:08, 39.23it/s, est. speed input: 54535.41 toks/s, output: 53.26 toks/s]
Processed prompts:  35%|███▌      | 1452/4096 [00:27<01:07, 39.44it/s, est. speed input: 54137.97 toks/s, output: 52.87 toks/s]
Processed prompts:  36%|███▌      | 1484/4096 [00:28<01:06, 39.30it/s, est. speed input: 53725.29 toks/s, output: 52.47 toks/s]
Processed prompts:  37%|███▋      | 1516/4096 [00:29<01:05, 39.51it/s, est. speed input: 53374.45 toks/s, output: 52.12 toks/s]
Processed prompts:  38%|███▊      | 1548/4096 [00:29<01:04, 39.63it/s, est. speed input: 53038.76 toks/s, output: 51.80 toks/s]
Processed prompts:  39%|███▊      | 1580/4096 [00:30<01:03, 39.45it/s, est. speed input: 52689.51 toks/s, output: 51.45 toks/s]
Processed prompts:  39%|███▉      | 1612/4096 [00:31<01:02, 39.59it/s, est. speed input: 52389.58 toks/s, output: 51.16 toks/s]
Processed prompts:  40%|████      | 1644/4096 [00:32<01:02, 39.41it/s, est. speed input: 52072.76 toks/s, output: 50.85 toks/s]
Processed prompts:  41%|████      | 1676/4096 [00:33<01:01, 39.26it/s, est. speed input: 51769.69 toks/s, output: 50.56 toks/s]
Processed prompts:  42%|████▏     | 1708/4096 [00:33<01:00, 39.16it/s, est. speed input: 51481.60 toks/s, output: 50.27 toks/s]
Processed prompts:  42%|████▏     | 1740/4096 [00:34<00:59, 39.77it/s, est. speed input: 51276.40 toks/s, output: 50.07 toks/s]
Processed prompts:  43%|████▎     | 1772/4096 [00:35<00:58, 39.51it/s, est. speed input: 51011.82 toks/s, output: 49.82 toks/s]
Processed prompts:  44%|████▍     | 1804/4096 [00:36<00:58, 39.32it/s, est. speed input: 50758.56 toks/s, output: 49.57 toks/s]
Processed prompts:  45%|████▍     | 1836/4096 [00:37<00:57, 39.18it/s, est. speed input: 50516.33 toks/s, output: 49.33 toks/s]
Processed prompts:  46%|████▌     | 1868/4096 [00:38<00:56, 39.38it/s, est. speed input: 50311.29 toks/s, output: 49.13 toks/s]
Processed prompts:  46%|████▋     | 1900/4096 [00:38<00:55, 39.23it/s, est. speed input: 50088.83 toks/s, output: 48.91 toks/s]
Processed prompts:  47%|████▋     | 1932/4096 [00:39<00:55, 39.14it/s, est. speed input: 49877.26 toks/s, output: 48.71 toks/s]
Processed prompts:  48%|████▊     | 1964/4096 [00:40<00:54, 39.36it/s, est. speed input: 49697.51 toks/s, output: 48.53 toks/s]
Processed prompts:  49%|████▊     | 1996/4096 [00:41<00:53, 39.17it/s, est. speed input: 49497.31 toks/s, output: 48.34 toks/s]
Processed prompts:  50%|████▉     | 2028/4096 [00:42<00:52, 39.08it/s, est. speed input: 49307.88 toks/s, output: 48.15 toks/s]
Processed prompts:  50%|█████     | 2060/4096 [00:42<00:51, 39.26it/s, est. speed input: 49144.77 toks/s, output: 47.99 toks/s]
Processed prompts:  51%|█████     | 2092/4096 [00:43<00:51, 39.15it/s, est. speed input: 48969.66 toks/s, output: 47.82 toks/s]
Processed prompts:  52%|█████▏    | 2124/4096 [00:44<00:50, 39.07it/s, est. speed input: 48800.93 toks/s, output: 47.66 toks/s]
Processed prompts:  53%|█████▎    | 2156/4096 [00:45<00:49, 38.99it/s, est. speed input: 48635.96 toks/s, output: 47.50 toks/s]
Processed prompts:  53%|█████▎    | 2188/4096 [00:46<00:48, 39.20it/s, est. speed input: 48496.76 toks/s, output: 47.36 toks/s]
Processed prompts:  54%|█████▍    | 2220/4096 [00:47<00:48, 39.07it/s, est. speed input: 48342.13 toks/s, output: 47.21 toks/s]
Processed prompts:  55%|█████▍    | 2252/4096 [00:47<00:47, 39.01it/s, est. speed input: 48195.47 toks/s, output: 47.07 toks/s]
Processed prompts:  56%|█████▌    | 2284/4096 [00:48<00:46, 38.96it/s, est. speed input: 48052.73 toks/s, output: 46.93 toks/s]
Processed prompts:  57%|█████▋    | 2316/4096 [00:49<00:45, 38.90it/s, est. speed input: 47913.32 toks/s, output: 46.79 toks/s]
Processed prompts:  57%|█████▋    | 2348/4096 [00:50<00:44, 38.90it/s, est. speed input: 47781.24 toks/s, output: 46.66 toks/s]
Processed prompts:  58%|█████▊    | 2380/4096 [00:51<00:44, 38.84it/s, est. speed input: 47649.50 toks/s, output: 46.53 toks/s]
Processed prompts:  59%|█████▉    | 2412/4096 [00:51<00:43, 38.83it/s, est. speed input: 47523.99 toks/s, output: 46.41 toks/s]
Processed prompts:  60%|█████▉    | 2444/4096 [00:52<00:42, 38.82it/s, est. speed input: 47402.25 toks/s, output: 46.29 toks/s]
Processed prompts:  60%|██████    | 2476/4096 [00:53<00:41, 38.81it/s, est. speed input: 47284.02 toks/s, output: 46.18 toks/s]
Processed prompts:  61%|██████    | 2508/4096 [00:54<00:40, 38.82it/s, est. speed input: 47170.36 toks/s, output: 46.06 toks/s]
Processed prompts:  62%|██████▏   | 2540/4096 [00:55<00:39, 39.06it/s, est. speed input: 47074.34 toks/s, output: 45.97 toks/s]
Processed prompts:  63%|██████▎   | 2572/4096 [00:56<00:39, 38.95it/s, est. speed input: 46964.40 toks/s, output: 45.86 toks/s]
Processed prompts:  64%|██████▎   | 2604/4096 [00:56<00:38, 39.16it/s, est. speed input: 46874.34 toks/s, output: 45.78 toks/s]
Processed prompts:  64%|██████▍   | 2636/4096 [00:57<00:37, 39.02it/s, est. speed input: 46770.52 toks/s, output: 45.67 toks/s]
Processed prompts:  65%|██████▌   | 2668/4096 [00:58<00:36, 38.93it/s, est. speed input: 46670.11 toks/s, output: 45.58 toks/s]
Processed prompts:  66%|██████▌   | 2700/4096 [00:59<00:35, 38.87it/s, est. speed input: 46572.47 toks/s, output: 45.48 toks/s]
Processed prompts:  67%|██████▋   | 2732/4096 [01:00<00:34, 39.11it/s, est. speed input: 46493.12 toks/s, output: 45.40 toks/s]
Processed prompts:  67%|██████▋   | 2764/4096 [01:00<00:34, 38.98it/s, est. speed input: 46399.59 toks/s, output: 45.31 toks/s]
Processed prompts:  68%|██████▊   | 2796/4096 [01:01<00:33, 38.88it/s, est. speed input: 46308.19 toks/s, output: 45.22 toks/s]
Processed prompts:  69%|██████▉   | 2828/4096 [01:02<00:32, 38.81it/s, est. speed input: 46218.96 toks/s, output: 45.14 toks/s]
Processed prompts:  70%|██████▉   | 2860/4096 [01:03<00:31, 38.75it/s, est. speed input: 46131.91 toks/s, output: 45.05 toks/s]
Processed prompts:  71%|███████   | 2892/4096 [01:04<00:30, 39.43it/s, est. speed input: 46082.87 toks/s, output: 45.00 toks/s]
Processed prompts:  71%|███████▏  | 2924/4096 [01:05<00:29, 39.50it/s, est. speed input: 46015.19 toks/s, output: 44.94 toks/s]
Processed prompts:  72%|███████▏  | 2956/4096 [01:05<00:29, 39.24it/s, est. speed input: 45934.27 toks/s, output: 44.86 toks/s]
Processed prompts:  73%|███████▎  | 2988/4096 [01:06<00:28, 39.05it/s, est. speed input: 45854.73 toks/s, output: 44.78 toks/s]
Processed prompts:  74%|███████▎  | 3020/4096 [01:07<00:27, 38.93it/s, est. speed input: 45777.77 toks/s, output: 44.70 toks/s]
Processed prompts:  75%|███████▍  | 3052/4096 [01:08<00:26, 38.81it/s, est. speed input: 45700.99 toks/s, output: 44.63 toks/s]
Processed prompts:  75%|███████▌  | 3084/4096 [01:09<00:26, 38.77it/s, est. speed input: 45628.18 toks/s, output: 44.56 toks/s]
Processed prompts:  76%|███████▌  | 3116/4096 [01:10<00:25, 38.73it/s, est. speed input: 45556.54 toks/s, output: 44.49 toks/s]
Processed prompts:  77%|███████▋  | 3148/4096 [01:10<00:24, 38.70it/s, est. speed input: 45486.22 toks/s, output: 44.42 toks/s]
Processed prompts:  78%|███████▊  | 3180/4096 [01:11<00:23, 38.63it/s, est. speed input: 45415.66 toks/s, output: 44.35 toks/s]
Processed prompts:  78%|███████▊  | 3212/4096 [01:12<00:22, 38.63it/s, est. speed input: 45348.87 toks/s, output: 44.29 toks/s]
Processed prompts:  79%|███████▉  | 3244/4096 [01:13<00:22, 38.66it/s, est. speed input: 45284.88 toks/s, output: 44.22 toks/s]
Processed prompts:  80%|███████▉  | 3276/4096 [01:14<00:21, 38.62it/s, est. speed input: 45219.23 toks/s, output: 44.16 toks/s]
Processed prompts:  81%|████████  | 3308/4096 [01:15<00:20, 38.59it/s, est. speed input: 45155.28 toks/s, output: 44.10 toks/s]
Processed prompts:  82%|████████▏ | 3340/4096 [01:15<00:19, 38.55it/s, est. speed input: 45091.90 toks/s, output: 44.04 toks/s]
Processed prompts:  82%|████████▏ | 3372/4096 [01:16<00:18, 38.55it/s, est. speed input: 45031.35 toks/s, output: 43.98 toks/s]
Processed prompts:  83%|████████▎ | 3404/4096 [01:17<00:17, 38.56it/s, est. speed input: 44972.00 toks/s, output: 43.92 toks/s]
Processed prompts:  84%|████████▍ | 3436/4096 [01:18<00:17, 38.59it/s, est. speed input: 44915.28 toks/s, output: 43.86 toks/s]
Processed prompts:  85%|████████▍ | 3468/4096 [01:19<00:16, 38.58it/s, est. speed input: 44858.44 toks/s, output: 43.81 toks/s]
Processed prompts:  85%|████████▌ | 3500/4096 [01:19<00:15, 38.58it/s, est. speed input: 44802.93 toks/s, output: 43.75 toks/s]
Processed prompts:  86%|████████▌ | 3532/4096 [01:20<00:14, 38.53it/s, est. speed input: 44746.36 toks/s, output: 43.70 toks/s]
Processed prompts:  87%|████████▋ | 3564/4096 [01:21<00:13, 38.83it/s, est. speed input: 44704.18 toks/s, output: 43.66 toks/s]
Processed prompts:  88%|████████▊ | 3596/4096 [01:22<00:12, 38.75it/s, est. speed input: 44651.88 toks/s, output: 43.61 toks/s]
Processed prompts:  89%|████████▊ | 3628/4096 [01:23<00:12, 38.70it/s, est. speed input: 44600.49 toks/s, output: 43.56 toks/s]
Processed prompts:  89%|████████▉ | 3660/4096 [01:24<00:11, 38.93it/s, est. speed input: 44560.19 toks/s, output: 43.52 toks/s]
Processed prompts:  90%|█████████ | 3692/4096 [01:24<00:10, 38.79it/s, est. speed input: 44509.53 toks/s, output: 43.47 toks/s]
Processed prompts:  91%|█████████ | 3724/4096 [01:25<00:09, 38.69it/s, est. speed input: 44459.81 toks/s, output: 43.42 toks/s]
Processed prompts:  92%|█████████▏| 3756/4096 [01:26<00:08, 38.63it/s, est. speed input: 44411.28 toks/s, output: 43.37 toks/s]
Processed prompts:  92%|█████████▏| 3788/4096 [01:27<00:07, 38.59it/s, est. speed input: 44364.02 toks/s, output: 43.32 toks/s]
Processed prompts:  93%|█████████▎| 3820/4096 [01:28<00:07, 38.54it/s, est. speed input: 44316.70 toks/s, output: 43.28 toks/s]
Processed prompts:  94%|█████████▍| 3852/4096 [01:29<00:06, 38.51it/s, est. speed input: 44270.46 toks/s, output: 43.23 toks/s]
Processed prompts:  95%|█████████▍| 3884/4096 [01:29<00:05, 38.50it/s, est. speed input: 44225.43 toks/s, output: 43.19 toks/s]
Processed prompts:  96%|█████████▌| 3916/4096 [01:30<00:04, 39.18it/s, est. speed input: 44204.98 toks/s, output: 43.17 toks/s]
Processed prompts:  96%|█████████▋| 3948/4096 [01:31<00:03, 38.94it/s, est. speed input: 44160.22 toks/s, output: 43.13 toks/s]
Processed prompts:  97%|█████████▋| 3980/4096 [01:32<00:02, 39.12it/s, est. speed input: 44128.02 toks/s, output: 43.09 toks/s]
Processed prompts:  98%|█████████▊| 4012/4096 [01:33<00:02, 38.92it/s, est. speed input: 44085.86 toks/s, output: 43.05 toks/s]
Processed prompts:  99%|█████████▊| 4044/4096 [01:33<00:01, 39.09it/s, est. speed input: 44054.41 toks/s, output: 43.02 toks/s]
Processed prompts: 100%|█████████▉| 4076/4096 [01:34<00:00, 43.84it/s, est. speed input: 44157.40 toks/s, output: 43.12 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [01:34<00:00, 43.84it/s, est. speed input: 44373.67 toks/s, output: 43.33 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [01:34<00:00, 43.33it/s, est. speed input: 44373.67 toks/s, output: 43.33 toks/s]
[rank0]:[W126 11:41:56.538679793 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 168.4s

测试结果:
  Requests/s:   38.95
  Tokens/s:     39928.40
  Total Reqs:   4096
  Elapsed:      105.15s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     39889.44

============================================================
[8/8] 测试 M=65536
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 65536
│   M_prefill     = 65536 (= 64 x 1024)
│   M_decode      = 64
│   batched_tokens = 65536 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 8192
│   --max-num-seqs           = 64
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 65536
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 11:42:49 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1303467) [INFO] Loading compress extension: cusparselt_compress_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1303467) WARNING 01-26 11:43:06 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]     self.model_runner.profile_run()
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]     hidden_states = self.model(
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]                     ^^^^^^^^^^^
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]     def forward(
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]     raise e
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]   File "<eval_with_key>.58", line 325, in forward
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]     return range_entry.runnable(*args)
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]     return self._compiled_fn(*args)
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]     return compiled_fn(full_args)
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]                             ^^^^^^^
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 724, in inner_fn
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]     outs = compiled_fn(args)
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]            ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]     return self.current_callable(inputs)
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]     out = model(new_inputs)
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]   File "/tmp/torchinductor_root/h2/ch2f2fl72d437f7nhefu4ztoxpftxl7zqo6tizg6tnqnowdjohtd.py", line 1093, in call
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]     buf17 = torch.ops.slidesparse.quant_slide_int8.default(buf16, 'Qwen2.5-7B-INT8', 10)
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 564, in _quant_slide_int8_impl
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]     return fn(input, L)
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]            ^^^^^^^^^^^^
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]   File "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/H100_cc90_py312_cu129_x86_64/quant_slide_tuned_Qwen2.5-7B.py", line 339, in quant_slide_int8_triton
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]     _quant_slide_int8_kernel[(M,)](
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 756, in run
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]     launch_metadata = kernel.launch_metadata(grid, stream, *bound_args.values())
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 490, in launch_metadata
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]     self._init_handles()
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 473, in _init_handles
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]     self.module, self.function, self.n_regs, self.n_spills, self.n_max_threads = driver.active.utils.load_binary(
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866]                                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1303467) ERROR 01-26 11:43:14 [core.py:866] RuntimeError: Triton Error [CUDA]: an illegal memory access was encountered


─── STDERR ───
[2026-01-26 11:42:48] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:42:49] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:42:49] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 11:42:49] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:42:49] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:42:49] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:42:49] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:42:49] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:42:49] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:42:49] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:42:49] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:42:49] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:42:49] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:42:49] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 11:42:55] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 11:42:56] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:42:56] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-26 11:42:56] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:42:56] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:42:56] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:42:56] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:42:56] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-26 11:42:56] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-26 11:42:56] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 11:42:56] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 11:42:56] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 11:42:56] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 11:42:56] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1303467) [2026-01-26 11:42:58] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1303467) [2026-01-26 11:42:58] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_H100_cc90_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1303467) [2026-01-26 11:42:58] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1303467) [2026-01-26 11:42:58] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=1303467) [2026-01-26 11:42:58] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=1303467) [2026-01-26 11:42:58] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=1303467) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1303467) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.85it/s]
(EngineCore_DP0 pid=1303467) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.33it/s]
(EngineCore_DP0 pid=1303467) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.39it/s]
(EngineCore_DP0 pid=1303467) 
(EngineCore_DP0 pid=1303467) [2026-01-26 11:43:00] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=1303467) [2026-01-26 11:43:00] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16588800 bytes
(EngineCore_DP0 pid=1303467) [2026-01-26 11:43:00] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=1303467) [2026-01-26 11:43:00] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12902400 bytes
(EngineCore_DP0 pid=1303467) [2026-01-26 11:43:00] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=1303467) [2026-01-26 11:43:00] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 136396800 bytes
(EngineCore_DP0 pid=1303467) [2026-01-26 11:43:00] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=1303467) [2026-01-26 11:43:00] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 67952640 bytes
(EngineCore_DP0 pid=1303467) [rank0]:W0126 11:43:12.167000 1303467 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1303467) [rank0]:W0126 11:43:12.245000 1303467 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1303467) [rank0]:W0126 11:43:13.635000 1303467 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1303467) [rank0]:W0126 11:43:13.759000 1303467 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=1303467) Process EngineCore_DP0:
(EngineCore_DP0 pid=1303467) Traceback (most recent call last):
(EngineCore_DP0 pid=1303467)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=1303467)     self.run()
(EngineCore_DP0 pid=1303467)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=1303467)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=1303467)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=1303467)     raise e
(EngineCore_DP0 pid=1303467)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=1303467)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=1303467)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1303467)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=1303467)     super().__init__(
(EngineCore_DP0 pid=1303467)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=1303467)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=1303467)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1303467)   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=1303467)     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=1303467)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1303467)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=1303467)     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=1303467)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1303467)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=1303467)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=1303467)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1303467)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=1303467)     return func(*args, **kwargs)
(EngineCore_DP0 pid=1303467)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1303467)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=1303467)     return func(*args, **kwargs)
(EngineCore_DP0 pid=1303467)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1303467)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=1303467)     self.model_runner.profile_run()
(EngineCore_DP0 pid=1303467)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4474, in profile_run
(EngineCore_DP0 pid=1303467)     hidden_states, last_hidden_states = self._dummy_run(
(EngineCore_DP0 pid=1303467)                                         ^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1303467)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=1303467)     return func(*args, **kwargs)
(EngineCore_DP0 pid=1303467)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1303467)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=1303467)     outputs = self.model(
(EngineCore_DP0 pid=1303467)               ^^^^^^^^^^^
(EngineCore_DP0 pid=1303467)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=1303467)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=1303467)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1303467)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=1303467)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=1303467)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1303467)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=1303467)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=1303467)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1303467)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=1303467)     hidden_states = self.model(
(EngineCore_DP0 pid=1303467)                     ^^^^^^^^^^^
(EngineCore_DP0 pid=1303467)   File "/root/vllmbench/vllm/compilation/decorators.py", line 526, in __call__
(EngineCore_DP0 pid=1303467)     output = TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=1303467)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1303467)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 218, in __call__
(EngineCore_DP0 pid=1303467)     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=1303467)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1303467)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=1303467)     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=1303467)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1303467)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 832, in compile_wrapper
(EngineCore_DP0 pid=1303467)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=1303467)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1303467)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
(EngineCore_DP0 pid=1303467)     def forward(
(EngineCore_DP0 pid=1303467)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=1303467)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=1303467)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1303467)   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=1303467)     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=1303467)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1303467)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=1303467)     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=1303467)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1303467)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=1303467)     raise e
(EngineCore_DP0 pid=1303467)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=1303467)     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=1303467)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1303467)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=1303467)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=1303467)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1303467)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=1303467)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=1303467)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1303467)   File "<eval_with_key>.58", line 325, in forward
(EngineCore_DP0 pid=1303467)     submod_2 = self.submod_2(getitem_3, s72, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_, getitem_4, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_3 = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = getitem_4 = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
(EngineCore_DP0 pid=1303467)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1303467)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=1303467)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=1303467)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1303467)   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=1303467)     return range_entry.runnable(*args)
(EngineCore_DP0 pid=1303467)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1303467)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=1303467)     return self._compiled_fn(*args)
(EngineCore_DP0 pid=1303467)            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1303467)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=1303467)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=1303467)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1303467)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=1303467)     return compiled_fn(full_args)
(EngineCore_DP0 pid=1303467)            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1303467)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=1303467)     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=1303467)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1303467)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=1303467)     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=1303467)                             ^^^^^^^
(EngineCore_DP0 pid=1303467)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 724, in inner_fn
(EngineCore_DP0 pid=1303467)     outs = compiled_fn(args)
(EngineCore_DP0 pid=1303467)            ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1303467)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=1303467)     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=1303467)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1303467)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=1303467)     return self.current_callable(inputs)
(EngineCore_DP0 pid=1303467)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1303467)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=1303467)     out = model(new_inputs)
(EngineCore_DP0 pid=1303467)           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1303467)   File "/tmp/torchinductor_root/h2/ch2f2fl72d437f7nhefu4ztoxpftxl7zqo6tizg6tnqnowdjohtd.py", line 1093, in call
(EngineCore_DP0 pid=1303467)     buf17 = torch.ops.slidesparse.quant_slide_int8.default(buf16, 'Qwen2.5-7B-INT8', 10)
(EngineCore_DP0 pid=1303467)             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1303467)   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
(EngineCore_DP0 pid=1303467)     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=1303467)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1303467)   File "/root/vllmbench/slidesparse/core/kernels.py", line 564, in _quant_slide_int8_impl
(EngineCore_DP0 pid=1303467)     return fn(input, L)
(EngineCore_DP0 pid=1303467)            ^^^^^^^^^^^^
(EngineCore_DP0 pid=1303467)   File "/root/vllmbench/slidesparse/csrc/fused_quant_slide_triton/build/H100_cc90_py312_cu129_x86_64/quant_slide_tuned_Qwen2.5-7B.py", line 339, in quant_slide_int8_triton
(EngineCore_DP0 pid=1303467)     _quant_slide_int8_kernel[(M,)](
(EngineCore_DP0 pid=1303467)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 419, in <lambda>
(EngineCore_DP0 pid=1303467)     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
(EngineCore_DP0 pid=1303467)                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1303467)   File "/usr/local/lib/python3.12/dist-packages/triton/runtime/jit.py", line 756, in run
(EngineCore_DP0 pid=1303467)     launch_metadata = kernel.launch_metadata(grid, stream, *bound_args.values())
(EngineCore_DP0 pid=1303467)                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1303467)   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 490, in launch_metadata
(EngineCore_DP0 pid=1303467)     self._init_handles()
(EngineCore_DP0 pid=1303467)   File "/usr/local/lib/python3.12/dist-packages/triton/compiler/compiler.py", line 473, in _init_handles
(EngineCore_DP0 pid=1303467)     self.module, self.function, self.n_regs, self.n_spills, self.n_max_threads = driver.active.utils.load_binary(
(EngineCore_DP0 pid=1303467)                                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=1303467) RuntimeError: Triton Error [CUDA]: an illegal memory access was encountered
[rank0]:[W126 11:43:15.599334067 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[ERROR] 测试失败: M=65536 (exit code: 1)


------------------------------------------------------------
  生成 CSV: Qwen2.5-7B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/H100_cc90_INT8_py312_cu129_x86_64/cusparselt/2_10/Qwen2.5-7B-INT8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,35.4072,18163.8859,3.6151
1024,1024,1,128,128,31.6374,32428.2969,4.0458
2048,1024,2,256,128,35.6800,36571.9876,7.1749
4096,1024,4,512,128,37.3452,38278.7981,13.7099
8192,1024,8,1024,128,38.3597,39318.6735,26.6947
16384,1024,16,2048,128,39.5319,40520.2286,51.8062
32768,1024,32,4096,128,38.9545,39928.3975,105.1482
65536,1024,64,8192,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[INFO] 完成: 7 成功, 1 失败


============================================================
  Benchmark 完成!
============================================================


总计: 35 成功, 5 失败
============================================================
