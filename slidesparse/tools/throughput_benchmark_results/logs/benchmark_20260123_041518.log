======================================================================
SlideSparse vLLM Throughput Benchmark Log
Created: 2026-01-23 04:15:18
======================================================================

原始命令:
  throughput_benchmark.py --model qwen2.5-0.5b-int8 --backend all --stage prefill --M quick

命令行参数:
  --model: qwen2.5-0.5b-int8
  --backend: all
  --sparsity: None
  --stage: prefill
  --M: quick
  --N: None
  --inner-32: False
  --eager: False
  --gpu-id: 0
  --gpu-mem: 0.8
  --dry-run: False
  --list-models: False

硬件信息:
  GPU: RTX5080
  Compute Capability: cc120
  VRAM: 15.5 GB
  CUDA: 12.9
  Python: py312

Backend 环境变量 (初始状态):
  DISABLE_SLIDESPARSE: 未设置
  USE_CUBLASLT: 未设置
  USE_CUSPARSELT: 未设置
  SPARSITY: 未设置
  INNER_DTYPE_32: 未设置

======================================================================

[WARNING] Backend 不支持，跳过: Qwen2.5-0.5B-INT8 + cutlass
[WARNING]   原因: vLLM CUTLASS INT8 不支持: sm_120 >= sm_100

============================================================
  Qwen2.5-0.5B-INT8 | cuBLASLt | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints/Qwen2.5-0.5B-INT8
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cublaslt

============================================================
[1/3] 测试 M=16
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-0.5B-INT8                               │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度:
│   M_prefill = 16 (= 1 x 16)
│   M_decode  = 1
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len       = 16
│   --output-len      = 1
│   --num-prompts     = 128
│   --max-num-seqs    = 1
│   --max-model-len   = 145
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
INFO 01-23 04:15:22 [datasets.py:612] Sampling input_len from [16, 16] and output_len from [1, 1]
INFO 01-23 04:15:22 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints/Qwen2.5-0.5B-INT8', 'max_model_len': 145, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 145, 'max_num_seqs': 1, 'disable_log_stats': True, 'enable_lora': None, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints/Qwen2.5-0.5B-INT8'}
INFO 01-23 04:15:22 [model.py:514] Resolved architecture: Qwen2ForCausalLM
INFO 01-23 04:15:22 [model.py:1661] Using max model len 145
INFO 01-23 04:15:22 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=145.
(EngineCore_DP0 pid=41305) INFO 01-23 04:15:26 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints/Qwen2.5-0.5B-INT8', speculative_config=None, tokenizer='/root/vllmbench/checkpoints/Qwen2.5-0.5B-INT8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=145, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints/Qwen2.5-0.5B-INT8, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [145], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 2, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=41305) INFO 01-23 04:15:26 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:55073 backend=nccl
(EngineCore_DP0 pid=41305) INFO 01-23 04:15:27 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=41305) INFO 01-23 04:15:27 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints/Qwen2.5-0.5B-INT8...
(EngineCore_DP0 pid=41305) INFO 01-23 04:15:27 [compressed_tensors_w8a8_int8.py:62] Using CutlassScaledMMLinearKernel for CompressedTensorsW8A8Int8
(EngineCore_DP0 pid=41305) INFO 01-23 04:15:27 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=41305) INFO 01-23 04:15:27 [default_loader.py:308] Loading weights took 0.08 seconds
(EngineCore_DP0 pid=41305) INFO 01-23 04:15:28 [gpu_model_runner.py:3659] Model loading took 0.5924 GiB memory and 0.254821 seconds
(EngineCore_DP0 pid=41305) INFO 01-23 04:15:32 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/90ee5c37e8/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=41305) INFO 01-23 04:15:32 [backends.py:703] Dynamo bytecode transform time: 3.92 s
(EngineCore_DP0 pid=41305) INFO 01-23 04:15:35 [backends.py:261] Cache the graph of compile range (1, 145) for later use
(EngineCore_DP0 pid=41305) INFO 01-23 04:15:49 [backends.py:278] Compiling a graph for compile range (1, 145) takes 15.90 s
(EngineCore_DP0 pid=41305) INFO 01-23 04:15:49 [monitor.py:34] torch.compile takes 19.82 s in total
(EngineCore_DP0 pid=41305) INFO 01-23 04:15:50 [gpu_worker.py:375] Available KV cache memory: 11.41 GiB
(EngineCore_DP0 pid=41305) INFO 01-23 04:15:50 [kv_cache_utils.py:1291] GPU KV cache size: 996,976 tokens
(EngineCore_DP0 pid=41305) INFO 01-23 04:15:50 [kv_cache_utils.py:1296] Maximum concurrency for 145 tokens per request: 6231.10x
(EngineCore_DP0 pid=41305) INFO 01-23 04:15:51 [gpu_model_runner.py:4587] Graph capturing finished in 1 secs, took 0.01 GiB
(EngineCore_DP0 pid=41305) INFO 01-23 04:15:51 [core.py:259] init engine (profile, create kv cache, warmup model) took 23.11 seconds
INFO 01-23 04:15:51 [llm.py:360] Supported tasks: ['generate']
Throughput: 124.35 requests/s, 2113.99 total tokens/s, 124.35 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  128


─── STDERR ───
[2026-01-23 04:15:22] INFO gemm_wrapper.py:362: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-23 04:15:22] INFO gemm_wrapper.py:408: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-23 04:15:22] INFO gemm_wrapper.py:455: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-23 04:15:22] INFO gemm_wrapper.py:502: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-23 04:15:26] INFO gemm_wrapper.py:362: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-23 04:15:26] INFO gemm_wrapper.py:408: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-23 04:15:26] INFO gemm_wrapper.py:455: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-23 04:15:26] INFO gemm_wrapper.py:502: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=41305) [2026-01-23 04:15:27] INFO SlideSparseLinearMethod_INT8.py:767: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=41305) [2026-01-23 04:15:27] INFO gemm_wrapper.py:307: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=41305) [2026-01-23 04:15:27] INFO kernels.py:278: INT8 quant kernel loaded
(EngineCore_DP0 pid=41305) [2026-01-23 04:15:27] INFO kernels.py:115: Dequant+bias kernel loaded
(EngineCore_DP0 pid=41305) [2026-01-23 04:15:27] INFO SlideSparseLinearMethod_INT8.py:437: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=41305) [2026-01-23 04:15:27] INFO SlideSparseLinearMethod_INT8.py:587: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=41305) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=41305) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 13.63it/s]
(EngineCore_DP0 pid=41305) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=41305) 2026-01-23 04:15:50,629 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=41305) 2026-01-23 04:15:50,637 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=41305) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 50.49it/s]
(EngineCore_DP0 pid=41305) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.70it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.70it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 6100.88it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:01, 85.06it/s, est. speed input: 1361.14 toks/s, output: 85.07 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:00<00:00, 113.86it/s, est. speed input: 1752.22 toks/s, output: 109.51 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:00<00:00, 121.92it/s, est. speed input: 1869.91 toks/s, output: 116.87 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:00<00:00, 126.10it/s, est. speed input: 1932.42 toks/s, output: 120.78 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:00<00:00, 128.04it/s, est. speed input: 1966.87 toks/s, output: 122.93 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:00<00:00, 129.60it/s, est. speed input: 1992.68 toks/s, output: 124.54 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:00<00:00, 130.32it/s, est. speed input: 2009.36 toks/s, output: 125.58 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:00<00:00, 130.57it/s, est. speed input: 2020.48 toks/s, output: 126.28 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:00<00:00, 130.97it/s, est. speed input: 2030.41 toks/s, output: 126.90 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 130.97it/s, est. speed input: 2034.56 toks/s, output: 127.16 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 127.15it/s, est. speed input: 2034.56 toks/s, output: 127.16 toks/s]
[rank0]:[W123 04:15:53.825384761 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 35.3s

测试结果:
  Requests/s:   124.35
  Tokens/s:     2113.99
  Total Reqs:   128
  Elapsed:      1.03s

  [Prefill 分析]
  Total Prefill Tokens: 2048
  Prefill Tokens/s:     1989.63

============================================================
[2/3] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-0.5B-INT8                               │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度:
│   M_prefill = 128 (= 1 x 128)
│   M_decode  = 1
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len       = 128
│   --output-len      = 1
│   --num-prompts     = 128
│   --max-num-seqs    = 1
│   --max-model-len   = 257
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
INFO 01-23 04:15:58 [datasets.py:612] Sampling input_len from [128, 128] and output_len from [1, 1]
INFO 01-23 04:15:58 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints/Qwen2.5-0.5B-INT8', 'max_model_len': 257, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 257, 'max_num_seqs': 1, 'disable_log_stats': True, 'enable_lora': None, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints/Qwen2.5-0.5B-INT8'}
INFO 01-23 04:15:58 [model.py:514] Resolved architecture: Qwen2ForCausalLM
INFO 01-23 04:15:58 [model.py:1661] Using max model len 257
INFO 01-23 04:15:58 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=257.
(EngineCore_DP0 pid=42858) INFO 01-23 04:16:02 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints/Qwen2.5-0.5B-INT8', speculative_config=None, tokenizer='/root/vllmbench/checkpoints/Qwen2.5-0.5B-INT8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=257, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints/Qwen2.5-0.5B-INT8, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [257], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 2, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=42858) INFO 01-23 04:16:02 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:60927 backend=nccl
(EngineCore_DP0 pid=42858) INFO 01-23 04:16:02 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=42858) INFO 01-23 04:16:02 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints/Qwen2.5-0.5B-INT8...
(EngineCore_DP0 pid=42858) INFO 01-23 04:16:02 [compressed_tensors_w8a8_int8.py:62] Using CutlassScaledMMLinearKernel for CompressedTensorsW8A8Int8
(EngineCore_DP0 pid=42858) INFO 01-23 04:16:02 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=42858) INFO 01-23 04:16:02 [default_loader.py:308] Loading weights took 0.08 seconds
(EngineCore_DP0 pid=42858) INFO 01-23 04:16:03 [gpu_model_runner.py:3659] Model loading took 0.5924 GiB memory and 0.260811 seconds
(EngineCore_DP0 pid=42858) INFO 01-23 04:16:07 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/1039dc10af/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=42858) INFO 01-23 04:16:07 [backends.py:703] Dynamo bytecode transform time: 3.84 s
(EngineCore_DP0 pid=42858) INFO 01-23 04:16:09 [backends.py:261] Cache the graph of compile range (1, 257) for later use
(EngineCore_DP0 pid=42858) INFO 01-23 04:16:23 [backends.py:278] Compiling a graph for compile range (1, 257) takes 14.66 s
(EngineCore_DP0 pid=42858) INFO 01-23 04:16:23 [monitor.py:34] torch.compile takes 18.50 s in total
(EngineCore_DP0 pid=42858) INFO 01-23 04:16:24 [gpu_worker.py:375] Available KV cache memory: 11.41 GiB
(EngineCore_DP0 pid=42858) INFO 01-23 04:16:24 [kv_cache_utils.py:1291] GPU KV cache size: 996,944 tokens
(EngineCore_DP0 pid=42858) INFO 01-23 04:16:24 [kv_cache_utils.py:1296] Maximum concurrency for 257 tokens per request: 3665.24x
(EngineCore_DP0 pid=42858) INFO 01-23 04:16:25 [gpu_model_runner.py:4587] Graph capturing finished in 1 secs, took 0.00 GiB
(EngineCore_DP0 pid=42858) INFO 01-23 04:16:25 [core.py:259] init engine (profile, create kv cache, warmup model) took 21.97 seconds
INFO 01-23 04:16:25 [llm.py:360] Supported tasks: ['generate']
Throughput: 120.72 requests/s, 15572.24 total tokens/s, 120.72 output tokens/s
Total num prompt tokens:  16384
Total num output tokens:  128


─── STDERR ───
[2026-01-23 04:15:58] INFO gemm_wrapper.py:362: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-23 04:15:58] INFO gemm_wrapper.py:408: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-23 04:15:58] INFO gemm_wrapper.py:455: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-23 04:15:58] INFO gemm_wrapper.py:502: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-23 04:16:02] INFO gemm_wrapper.py:362: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-23 04:16:02] INFO gemm_wrapper.py:408: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-23 04:16:02] INFO gemm_wrapper.py:455: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-23 04:16:02] INFO gemm_wrapper.py:502: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=42858) [2026-01-23 04:16:02] INFO SlideSparseLinearMethod_INT8.py:767: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=42858) [2026-01-23 04:16:02] INFO gemm_wrapper.py:307: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=42858) [2026-01-23 04:16:02] INFO kernels.py:278: INT8 quant kernel loaded
(EngineCore_DP0 pid=42858) [2026-01-23 04:16:02] INFO kernels.py:115: Dequant+bias kernel loaded
(EngineCore_DP0 pid=42858) [2026-01-23 04:16:02] INFO SlideSparseLinearMethod_INT8.py:437: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=42858) [2026-01-23 04:16:02] INFO SlideSparseLinearMethod_INT8.py:587: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=42858) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=42858) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 13.51it/s]
(EngineCore_DP0 pid=42858) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=42858) 2026-01-23 04:16:24,345 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=42858) 2026-01-23 04:16:24,372 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=42858) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 50.92it/s]
(EngineCore_DP0 pid=42858) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.46it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.46it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 2482.88it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:00, 125.25it/s, est. speed input: 16033.58 toks/s, output: 125.25 toks/s]
Processed prompts:  20%|██        | 26/128 [00:00<00:00, 126.68it/s, est. speed input: 16188.70 toks/s, output: 126.47 toks/s]
Processed prompts:  30%|███       | 39/128 [00:00<00:00, 127.41it/s, est. speed input: 16265.26 toks/s, output: 127.07 toks/s]
Processed prompts:  41%|████      | 52/128 [00:00<00:00, 126.95it/s, est. speed input: 16239.16 toks/s, output: 126.87 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:00<00:00, 126.62it/s, est. speed input: 16217.98 toks/s, output: 126.70 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:00<00:00, 125.66it/s, est. speed input: 16155.21 toks/s, output: 126.21 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:00<00:00, 126.46it/s, est. speed input: 16190.13 toks/s, output: 126.48 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:00<00:00, 126.70it/s, est. speed input: 16201.72 toks/s, output: 126.57 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:00<00:00, 127.32it/s, est. speed input: 16231.53 toks/s, output: 126.81 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 127.32it/s, est. speed input: 16260.15 toks/s, output: 127.03 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 127.02it/s, est. speed input: 16260.15 toks/s, output: 127.03 toks/s]
[rank0]:[W123 04:16:26.583095970 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 33.8s

测试结果:
  Requests/s:   120.72
  Tokens/s:     15572.24
  Total Reqs:   128
  Elapsed:      1.06s

  [Prefill 分析]
  Total Prefill Tokens: 16384
  Prefill Tokens/s:     15451.53

============================================================
[3/3] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-0.5B-INT8                               │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度:
│   M_prefill = 256 (= 1 x 256)
│   M_decode  = 1
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len       = 256
│   --output-len      = 1
│   --num-prompts     = 128
│   --max-num-seqs    = 1
│   --max-model-len   = 385
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
INFO 01-23 04:16:31 [datasets.py:612] Sampling input_len from [256, 256] and output_len from [1, 1]
INFO 01-23 04:16:32 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints/Qwen2.5-0.5B-INT8', 'max_model_len': 385, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 385, 'max_num_seqs': 1, 'disable_log_stats': True, 'enable_lora': None, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints/Qwen2.5-0.5B-INT8'}
INFO 01-23 04:16:32 [model.py:514] Resolved architecture: Qwen2ForCausalLM
INFO 01-23 04:16:32 [model.py:1661] Using max model len 385
INFO 01-23 04:16:32 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=385.
(EngineCore_DP0 pid=44216) INFO 01-23 04:16:35 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints/Qwen2.5-0.5B-INT8', speculative_config=None, tokenizer='/root/vllmbench/checkpoints/Qwen2.5-0.5B-INT8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=385, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints/Qwen2.5-0.5B-INT8, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [385], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 2, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=44216) INFO 01-23 04:16:36 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:37483 backend=nccl
(EngineCore_DP0 pid=44216) INFO 01-23 04:16:36 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=44216) INFO 01-23 04:16:36 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints/Qwen2.5-0.5B-INT8...
(EngineCore_DP0 pid=44216) INFO 01-23 04:16:36 [compressed_tensors_w8a8_int8.py:62] Using CutlassScaledMMLinearKernel for CompressedTensorsW8A8Int8
(EngineCore_DP0 pid=44216) INFO 01-23 04:16:36 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=44216) INFO 01-23 04:16:37 [default_loader.py:308] Loading weights took 0.08 seconds
(EngineCore_DP0 pid=44216) INFO 01-23 04:16:37 [gpu_model_runner.py:3659] Model loading took 0.5924 GiB memory and 0.258797 seconds
(EngineCore_DP0 pid=44216) INFO 01-23 04:16:41 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/c092e4da1e/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=44216) INFO 01-23 04:16:41 [backends.py:703] Dynamo bytecode transform time: 3.82 s
(EngineCore_DP0 pid=44216) INFO 01-23 04:16:43 [backends.py:261] Cache the graph of compile range (1, 385) for later use
(EngineCore_DP0 pid=44216) INFO 01-23 04:16:57 [backends.py:278] Compiling a graph for compile range (1, 385) takes 14.58 s
(EngineCore_DP0 pid=44216) INFO 01-23 04:16:57 [monitor.py:34] torch.compile takes 18.40 s in total
(EngineCore_DP0 pid=44216) INFO 01-23 04:16:58 [gpu_worker.py:375] Available KV cache memory: 11.41 GiB
(EngineCore_DP0 pid=44216) INFO 01-23 04:16:58 [kv_cache_utils.py:1291] GPU KV cache size: 996,880 tokens
(EngineCore_DP0 pid=44216) INFO 01-23 04:16:58 [kv_cache_utils.py:1296] Maximum concurrency for 385 tokens per request: 2492.20x
(EngineCore_DP0 pid=44216) INFO 01-23 04:16:59 [gpu_model_runner.py:4587] Graph capturing finished in 1 secs, took -0.01 GiB
(EngineCore_DP0 pid=44216) INFO 01-23 04:16:59 [core.py:259] init engine (profile, create kv cache, warmup model) took 21.80 seconds
INFO 01-23 04:16:59 [llm.py:360] Supported tasks: ['generate']
Throughput: 120.33 requests/s, 30925.37 total tokens/s, 120.33 output tokens/s
Total num prompt tokens:  32768
Total num output tokens:  128


─── STDERR ───
[2026-01-23 04:16:32] INFO gemm_wrapper.py:362: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-23 04:16:32] INFO gemm_wrapper.py:408: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-23 04:16:32] INFO gemm_wrapper.py:455: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-23 04:16:32] INFO gemm_wrapper.py:502: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-23 04:16:35] INFO gemm_wrapper.py:362: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-23 04:16:35] INFO gemm_wrapper.py:408: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-23 04:16:35] INFO gemm_wrapper.py:455: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-23 04:16:35] INFO gemm_wrapper.py:502: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=44216) [2026-01-23 04:16:36] INFO SlideSparseLinearMethod_INT8.py:767: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=44216) [2026-01-23 04:16:36] INFO gemm_wrapper.py:307: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=44216) [2026-01-23 04:16:36] INFO kernels.py:278: INT8 quant kernel loaded
(EngineCore_DP0 pid=44216) [2026-01-23 04:16:36] INFO kernels.py:115: Dequant+bias kernel loaded
(EngineCore_DP0 pid=44216) [2026-01-23 04:16:36] INFO SlideSparseLinearMethod_INT8.py:437: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=44216) [2026-01-23 04:16:36] INFO SlideSparseLinearMethod_INT8.py:587: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=44216) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=44216) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 13.63it/s]
(EngineCore_DP0 pid=44216) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=44216) 2026-01-23 04:16:58,490 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=44216) 2026-01-23 04:16:58,498 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=44216) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 53.72it/s]
(EngineCore_DP0 pid=44216) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.54it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.53it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 1428.96it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  12%|█▎        | 16/128 [00:00<00:00, 158.20it/s, est. speed input: 40504.33 toks/s, output: 158.20 toks/s]
Processed prompts:  25%|██▌       | 32/128 [00:00<00:00, 139.18it/s, est. speed input: 36284.52 toks/s, output: 141.73 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:00<00:00, 135.87it/s, est. speed input: 35439.39 toks/s, output: 138.43 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:00<00:00, 134.25it/s, est. speed input: 35020.11 toks/s, output: 136.80 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:00<00:00, 128.70it/s, est. speed input: 34091.14 toks/s, output: 133.17 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:00<00:00, 128.80it/s, est. speed input: 33918.85 toks/s, output: 132.49 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:00<00:00, 128.40it/s, est. speed input: 33749.95 toks/s, output: 131.83 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:00<00:00, 128.81it/s, est. speed input: 33689.33 toks/s, output: 131.60 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 128.81it/s, est. speed input: 33699.61 toks/s, output: 131.64 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 131.63it/s, est. speed input: 33699.61 toks/s, output: 131.64 toks/s]
[rank0]:[W123 04:17:00.709339107 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 34.1s

测试结果:
  Requests/s:   120.33
  Tokens/s:     30925.37
  Total Reqs:   128
  Elapsed:      1.06s

  [Prefill 分析]
  Total Prefill Tokens: 32768
  Prefill Tokens/s:     30805.04


------------------------------------------------------------
  生成 CSV: Qwen2.5-0.5B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cublaslt/Qwen2.5-0.5B-INT8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
16,16,1,128,128,124.3521,2113.9851,1.0293
128,128,1,128,128,120.7150,15572.2404,1.0603
256,256,1,128,128,120.3322,30925.3737,1.0637

------------------------------------------------------------

[INFO] 完成: 3 成功, 0 失败

============================================================
  Qwen2.5-0.5B-INT8 | cuSPARSELt (2_4) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_4
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_4

============================================================
[1/3] 测试 M=16
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-0.5B-INT8                               │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度:
│   M_prefill = 16 (= 1 x 16)
│   M_decode  = 1
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len       = 16
│   --output-len      = 1
│   --num-prompts     = 128
│   --max-num-seqs    = 1
│   --max-model-len   = 145
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
INFO 01-23 04:17:06 [datasets.py:612] Sampling input_len from [16, 16] and output_len from [1, 1]
INFO 01-23 04:17:06 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_4', 'max_model_len': 145, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 145, 'max_num_seqs': 1, 'disable_log_stats': True, 'enable_lora': None, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_4'}
INFO 01-23 04:17:06 [model.py:514] Resolved architecture: Qwen2ForCausalLM
INFO 01-23 04:17:06 [model.py:1661] Using max model len 145
INFO 01-23 04:17:06 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=145.
(EngineCore_DP0 pid=45513) INFO 01-23 04:17:10 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_4', speculative_config=None, tokenizer='/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_4', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=145, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_4, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [145], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 2, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=45513) INFO 01-23 04:17:10 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:56283 backend=nccl
(EngineCore_DP0 pid=45513) INFO 01-23 04:17:10 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=45513) INFO 01-23 04:17:10 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_4...
(EngineCore_DP0 pid=45513) INFO 01-23 04:17:10 [compressed_tensors_w8a8_int8.py:62] Using CutlassScaledMMLinearKernel for CompressedTensorsW8A8Int8
(EngineCore_DP0 pid=45513) INFO 01-23 04:17:10 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=45513) INFO 01-23 04:17:11 [default_loader.py:308] Loading weights took 0.08 seconds
(EngineCore_DP0 pid=45513) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=45513) INFO 01-23 04:17:11 [gpu_model_runner.py:3659] Model loading took 0.5119 GiB memory and 0.306726 seconds
(EngineCore_DP0 pid=45513) INFO 01-23 04:17:15 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/dad9a5cd20/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=45513) INFO 01-23 04:17:15 [backends.py:703] Dynamo bytecode transform time: 3.91 s
(EngineCore_DP0 pid=45513) INFO 01-23 04:17:17 [backends.py:261] Cache the graph of compile range (1, 145) for later use
(EngineCore_DP0 pid=45513) INFO 01-23 04:17:33 [backends.py:278] Compiling a graph for compile range (1, 145) takes 16.56 s
(EngineCore_DP0 pid=45513) INFO 01-23 04:17:33 [monitor.py:34] torch.compile takes 20.46 s in total
(EngineCore_DP0 pid=45513) INFO 01-23 04:17:34 [gpu_worker.py:375] Available KV cache memory: 11.49 GiB
(EngineCore_DP0 pid=45513) INFO 01-23 04:17:34 [kv_cache_utils.py:1291] GPU KV cache size: 1,004,000 tokens
(EngineCore_DP0 pid=45513) INFO 01-23 04:17:34 [kv_cache_utils.py:1296] Maximum concurrency for 145 tokens per request: 6275.00x
(EngineCore_DP0 pid=45513) INFO 01-23 04:17:35 [gpu_model_runner.py:4587] Graph capturing finished in 1 secs, took 0.01 GiB
(EngineCore_DP0 pid=45513) INFO 01-23 04:17:35 [core.py:259] init engine (profile, create kv cache, warmup model) took 23.71 seconds
INFO 01-23 04:17:35 [llm.py:360] Supported tasks: ['generate']
Throughput: 123.32 requests/s, 2096.41 total tokens/s, 123.32 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  128


─── STDERR ───
[2026-01-23 04:17:06] INFO gemm_wrapper.py:362: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-23 04:17:06] INFO gemm_wrapper.py:408: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-23 04:17:06] INFO gemm_wrapper.py:455: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-23 04:17:06] INFO gemm_wrapper.py:502: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-23 04:17:10] INFO gemm_wrapper.py:362: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-23 04:17:10] INFO gemm_wrapper.py:408: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-23 04:17:10] INFO gemm_wrapper.py:455: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-23 04:17:10] INFO gemm_wrapper.py:502: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=45513) [2026-01-23 04:17:10] INFO SlideSparseLinearMethod_INT8.py:767: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=45513) [2026-01-23 04:17:10] INFO gemm_wrapper.py:307: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=45513) [2026-01-23 04:17:10] INFO kernels.py:329: INT8 quant+slide kernel loaded
(EngineCore_DP0 pid=45513) [2026-01-23 04:17:10] INFO kernels.py:115: Dequant+bias kernel loaded
(EngineCore_DP0 pid=45513) [2026-01-23 04:17:10] INFO SlideSparseLinearMethod_INT8.py:437: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=45513) [2026-01-23 04:17:10] INFO SlideSparseLinearMethod_INT8.py:582: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=45513) [2026-01-23 04:17:10] INFO SlideSparseLinearMethod_INT8.py:587: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=45513) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=45513) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 13.92it/s]
(EngineCore_DP0 pid=45513) 
(EngineCore_DP0 pid=45513) [2026-01-23 04:17:11] INFO SlideSparseLinearMethod_INT8.py:681: cuSPARSELt INT8 compression: [1152, 896] -> 1D uint8
(EngineCore_DP0 pid=45513) [2026-01-23 04:17:11] INFO SlideSparseLinearMethod_INT8.py:691: cuSPARSELt INT8 compression done: 774144 bytes
(EngineCore_DP0 pid=45513) [2026-01-23 04:17:11] INFO SlideSparseLinearMethod_INT8.py:681: cuSPARSELt INT8 compression: [896, 896] -> 1D uint8
(EngineCore_DP0 pid=45513) [2026-01-23 04:17:11] INFO SlideSparseLinearMethod_INT8.py:691: cuSPARSELt INT8 compression done: 602112 bytes
(EngineCore_DP0 pid=45513) [2026-01-23 04:17:11] INFO SlideSparseLinearMethod_INT8.py:681: cuSPARSELt INT8 compression: [9728, 896] -> 1D uint8
(EngineCore_DP0 pid=45513) [2026-01-23 04:17:11] INFO SlideSparseLinearMethod_INT8.py:691: cuSPARSELt INT8 compression done: 6537216 bytes
(EngineCore_DP0 pid=45513) [2026-01-23 04:17:11] INFO SlideSparseLinearMethod_INT8.py:681: cuSPARSELt INT8 compression: [896, 4864] -> 1D uint8
(EngineCore_DP0 pid=45513) [2026-01-23 04:17:11] INFO SlideSparseLinearMethod_INT8.py:691: cuSPARSELt INT8 compression done: 3268608 bytes
(EngineCore_DP0 pid=45513) 2026-01-23 04:17:34,467 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=45513) 2026-01-23 04:17:34,475 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=45513) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 46.01it/s]
(EngineCore_DP0 pid=45513) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.49it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.49it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 4345.73it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:01, 89.59it/s, est. speed input: 1433.59 toks/s, output: 89.60 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:00<00:00, 111.40it/s, est. speed input: 1730.80 toks/s, output: 108.17 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:00<00:00, 121.32it/s, est. speed input: 1864.98 toks/s, output: 116.56 toks/s]
Processed prompts:  39%|███▉      | 50/128 [00:00<00:00, 126.26it/s, est. speed input: 1934.57 toks/s, output: 120.91 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:00<00:00, 128.33it/s, est. speed input: 1970.58 toks/s, output: 123.16 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:00<00:00, 130.08it/s, est. speed input: 1998.26 toks/s, output: 124.89 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:00<00:00, 130.15it/s, est. speed input: 2010.95 toks/s, output: 125.68 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:00<00:00, 130.00it/s, est. speed input: 2019.15 toks/s, output: 126.20 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:00<00:00, 130.87it/s, est. speed input: 2030.96 toks/s, output: 126.93 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 130.87it/s, est. speed input: 2034.44 toks/s, output: 127.15 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 127.14it/s, est. speed input: 2034.44 toks/s, output: 127.15 toks/s]
[rank0]:[W123 04:17:36.667790527 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 35.9s

测试结果:
  Requests/s:   123.32
  Tokens/s:     2096.41
  Total Reqs:   128
  Elapsed:      1.04s

  [Prefill 分析]
  Total Prefill Tokens: 2048
  Prefill Tokens/s:     1973.09

============================================================
[2/3] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-0.5B-INT8                               │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度:
│   M_prefill = 128 (= 1 x 128)
│   M_decode  = 1
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len       = 128
│   --output-len      = 1
│   --num-prompts     = 128
│   --max-num-seqs    = 1
│   --max-model-len   = 257
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
INFO 01-23 04:17:42 [datasets.py:612] Sampling input_len from [128, 128] and output_len from [1, 1]
INFO 01-23 04:17:42 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_4', 'max_model_len': 257, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 257, 'max_num_seqs': 1, 'disable_log_stats': True, 'enable_lora': None, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_4'}
INFO 01-23 04:17:42 [model.py:514] Resolved architecture: Qwen2ForCausalLM
INFO 01-23 04:17:42 [model.py:1661] Using max model len 257
INFO 01-23 04:17:42 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=257.
(EngineCore_DP0 pid=46850) INFO 01-23 04:17:46 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_4', speculative_config=None, tokenizer='/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_4', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=257, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_4, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [257], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 2, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=46850) INFO 01-23 04:17:46 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:59887 backend=nccl
(EngineCore_DP0 pid=46850) INFO 01-23 04:17:46 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=46850) INFO 01-23 04:17:46 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_4...
(EngineCore_DP0 pid=46850) INFO 01-23 04:17:47 [compressed_tensors_w8a8_int8.py:62] Using CutlassScaledMMLinearKernel for CompressedTensorsW8A8Int8
(EngineCore_DP0 pid=46850) INFO 01-23 04:17:47 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=46850) INFO 01-23 04:17:47 [default_loader.py:308] Loading weights took 0.08 seconds
(EngineCore_DP0 pid=46850) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=46850) INFO 01-23 04:17:47 [gpu_model_runner.py:3659] Model loading took 0.5119 GiB memory and 0.300657 seconds
(EngineCore_DP0 pid=46850) INFO 01-23 04:17:52 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/ca4db33fc8/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=46850) INFO 01-23 04:17:52 [backends.py:703] Dynamo bytecode transform time: 4.18 s
(EngineCore_DP0 pid=46850) INFO 01-23 04:17:54 [backends.py:261] Cache the graph of compile range (1, 257) for later use
(EngineCore_DP0 pid=46850) INFO 01-23 04:18:10 [backends.py:278] Compiling a graph for compile range (1, 257) takes 16.57 s
(EngineCore_DP0 pid=46850) INFO 01-23 04:18:10 [monitor.py:34] torch.compile takes 20.76 s in total
(EngineCore_DP0 pid=46850) INFO 01-23 04:18:10 [gpu_worker.py:375] Available KV cache memory: 11.49 GiB
(EngineCore_DP0 pid=46850) INFO 01-23 04:18:11 [kv_cache_utils.py:1291] GPU KV cache size: 1,003,968 tokens
(EngineCore_DP0 pid=46850) INFO 01-23 04:18:11 [kv_cache_utils.py:1296] Maximum concurrency for 257 tokens per request: 3691.06x
(EngineCore_DP0 pid=46850) INFO 01-23 04:18:11 [gpu_model_runner.py:4587] Graph capturing finished in 1 secs, took 0.01 GiB
(EngineCore_DP0 pid=46850) INFO 01-23 04:18:11 [core.py:259] init engine (profile, create kv cache, warmup model) took 24.07 seconds
INFO 01-23 04:18:12 [llm.py:360] Supported tasks: ['generate']
Throughput: 118.11 requests/s, 15236.20 total tokens/s, 118.11 output tokens/s
Total num prompt tokens:  16384
Total num output tokens:  128


─── STDERR ───
[2026-01-23 04:17:42] INFO gemm_wrapper.py:362: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-23 04:17:42] INFO gemm_wrapper.py:408: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-23 04:17:42] INFO gemm_wrapper.py:455: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-23 04:17:42] INFO gemm_wrapper.py:502: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-23 04:17:46] INFO gemm_wrapper.py:362: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-23 04:17:46] INFO gemm_wrapper.py:408: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-23 04:17:46] INFO gemm_wrapper.py:455: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-23 04:17:46] INFO gemm_wrapper.py:502: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=46850) [2026-01-23 04:17:47] INFO SlideSparseLinearMethod_INT8.py:767: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=46850) [2026-01-23 04:17:47] INFO gemm_wrapper.py:307: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=46850) [2026-01-23 04:17:47] INFO kernels.py:329: INT8 quant+slide kernel loaded
(EngineCore_DP0 pid=46850) [2026-01-23 04:17:47] INFO kernels.py:115: Dequant+bias kernel loaded
(EngineCore_DP0 pid=46850) [2026-01-23 04:17:47] INFO SlideSparseLinearMethod_INT8.py:437: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=46850) [2026-01-23 04:17:47] INFO SlideSparseLinearMethod_INT8.py:582: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=46850) [2026-01-23 04:17:47] INFO SlideSparseLinearMethod_INT8.py:587: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=46850) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=46850) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 14.02it/s]
(EngineCore_DP0 pid=46850) 
(EngineCore_DP0 pid=46850) [2026-01-23 04:17:47] INFO SlideSparseLinearMethod_INT8.py:681: cuSPARSELt INT8 compression: [1152, 896] -> 1D uint8
(EngineCore_DP0 pid=46850) [2026-01-23 04:17:47] INFO SlideSparseLinearMethod_INT8.py:691: cuSPARSELt INT8 compression done: 774144 bytes
(EngineCore_DP0 pid=46850) [2026-01-23 04:17:47] INFO SlideSparseLinearMethod_INT8.py:681: cuSPARSELt INT8 compression: [896, 896] -> 1D uint8
(EngineCore_DP0 pid=46850) [2026-01-23 04:17:47] INFO SlideSparseLinearMethod_INT8.py:691: cuSPARSELt INT8 compression done: 602112 bytes
(EngineCore_DP0 pid=46850) [2026-01-23 04:17:47] INFO SlideSparseLinearMethod_INT8.py:681: cuSPARSELt INT8 compression: [9728, 896] -> 1D uint8
(EngineCore_DP0 pid=46850) [2026-01-23 04:17:47] INFO SlideSparseLinearMethod_INT8.py:691: cuSPARSELt INT8 compression done: 6537216 bytes
(EngineCore_DP0 pid=46850) [2026-01-23 04:17:47] INFO SlideSparseLinearMethod_INT8.py:681: cuSPARSELt INT8 compression: [896, 4864] -> 1D uint8
(EngineCore_DP0 pid=46850) [2026-01-23 04:17:47] INFO SlideSparseLinearMethod_INT8.py:691: cuSPARSELt INT8 compression done: 3268608 bytes
(EngineCore_DP0 pid=46850) 2026-01-23 04:18:11,084 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=46850) 2026-01-23 04:18:11,092 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=46850) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 46.00it/s]
(EngineCore_DP0 pid=46850) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.46it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.46it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 2150.84it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   5%|▍         | 6/128 [00:00<00:02, 55.91it/s, est. speed input: 7157.85 toks/s, output: 55.91 toks/s]
Processed prompts:  16%|█▌        | 20/128 [00:00<00:01, 99.83it/s, est. speed input: 11935.97 toks/s, output: 93.24 toks/s]
Processed prompts:  27%|██▋       | 34/128 [00:00<00:00, 114.43it/s, est. speed input: 13577.11 toks/s, output: 106.07 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:00<00:00, 122.11it/s, est. speed input: 14459.44 toks/s, output: 112.96 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:00<00:00, 126.47it/s, est. speed input: 14996.54 toks/s, output: 117.16 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:00<00:00, 128.65it/s, est. speed input: 15330.52 toks/s, output: 119.77 toks/s]
Processed prompts:  70%|███████   | 90/128 [00:00<00:00, 130.45it/s, est. speed input: 15591.08 toks/s, output: 121.80 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:00<00:00, 131.29it/s, est. speed input: 15770.95 toks/s, output: 123.21 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:00<00:00, 132.03it/s, est. speed input: 15918.83 toks/s, output: 124.37 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 132.03it/s, est. speed input: 16022.20 toks/s, output: 125.17 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 125.16it/s, est. speed input: 16022.20 toks/s, output: 125.17 toks/s]
[rank0]:[W123 04:18:13.349597298 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 36.7s

测试结果:
  Requests/s:   118.11
  Tokens/s:     15236.20
  Total Reqs:   128
  Elapsed:      1.08s

  [Prefill 分析]
  Total Prefill Tokens: 16384
  Prefill Tokens/s:     15118.09

============================================================
[3/3] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-0.5B-INT8                               │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度:
│   M_prefill = 256 (= 1 x 256)
│   M_decode  = 1
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len       = 256
│   --output-len      = 1
│   --num-prompts     = 128
│   --max-num-seqs    = 1
│   --max-model-len   = 385
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
INFO 01-23 04:18:18 [datasets.py:612] Sampling input_len from [256, 256] and output_len from [1, 1]
INFO 01-23 04:18:18 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_4', 'max_model_len': 385, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 385, 'max_num_seqs': 1, 'disable_log_stats': True, 'enable_lora': None, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_4'}
INFO 01-23 04:18:18 [model.py:514] Resolved architecture: Qwen2ForCausalLM
INFO 01-23 04:18:18 [model.py:1661] Using max model len 385
INFO 01-23 04:18:19 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=385.
(EngineCore_DP0 pid=48188) INFO 01-23 04:18:22 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_4', speculative_config=None, tokenizer='/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_4', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=385, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_4, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [385], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 2, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=48188) INFO 01-23 04:18:22 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:38121 backend=nccl
(EngineCore_DP0 pid=48188) INFO 01-23 04:18:22 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=48188) INFO 01-23 04:18:23 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_4...
(EngineCore_DP0 pid=48188) INFO 01-23 04:18:23 [compressed_tensors_w8a8_int8.py:62] Using CutlassScaledMMLinearKernel for CompressedTensorsW8A8Int8
(EngineCore_DP0 pid=48188) INFO 01-23 04:18:23 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=48188) INFO 01-23 04:18:23 [default_loader.py:308] Loading weights took 0.08 seconds
(EngineCore_DP0 pid=48188) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=48188) INFO 01-23 04:18:23 [gpu_model_runner.py:3659] Model loading took 0.5119 GiB memory and 0.298469 seconds
(EngineCore_DP0 pid=48188) INFO 01-23 04:18:27 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/0a3938aa0b/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=48188) INFO 01-23 04:18:27 [backends.py:703] Dynamo bytecode transform time: 3.93 s
(EngineCore_DP0 pid=48188) INFO 01-23 04:18:30 [backends.py:261] Cache the graph of compile range (1, 385) for later use
(EngineCore_DP0 pid=48188) INFO 01-23 04:18:45 [backends.py:278] Compiling a graph for compile range (1, 385) takes 16.35 s
(EngineCore_DP0 pid=48188) INFO 01-23 04:18:45 [monitor.py:34] torch.compile takes 20.27 s in total
(EngineCore_DP0 pid=48188) INFO 01-23 04:18:46 [gpu_worker.py:375] Available KV cache memory: 11.49 GiB
(EngineCore_DP0 pid=48188) INFO 01-23 04:18:46 [kv_cache_utils.py:1291] GPU KV cache size: 1,003,920 tokens
(EngineCore_DP0 pid=48188) INFO 01-23 04:18:46 [kv_cache_utils.py:1296] Maximum concurrency for 385 tokens per request: 2509.80x
(EngineCore_DP0 pid=48188) INFO 01-23 04:18:47 [gpu_model_runner.py:4587] Graph capturing finished in 1 secs, took 0.01 GiB
(EngineCore_DP0 pid=48188) INFO 01-23 04:18:47 [core.py:259] init engine (profile, create kv cache, warmup model) took 23.53 seconds
INFO 01-23 04:18:48 [llm.py:360] Supported tasks: ['generate']
Throughput: 114.12 requests/s, 29328.96 total tokens/s, 114.12 output tokens/s
Total num prompt tokens:  32768
Total num output tokens:  128


─── STDERR ───
[2026-01-23 04:18:18] INFO gemm_wrapper.py:362: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-23 04:18:18] INFO gemm_wrapper.py:408: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-23 04:18:18] INFO gemm_wrapper.py:455: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-23 04:18:18] INFO gemm_wrapper.py:502: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-23 04:18:22] INFO gemm_wrapper.py:362: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-23 04:18:22] INFO gemm_wrapper.py:408: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-23 04:18:22] INFO gemm_wrapper.py:455: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-23 04:18:22] INFO gemm_wrapper.py:502: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=48188) [2026-01-23 04:18:23] INFO SlideSparseLinearMethod_INT8.py:767: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=48188) [2026-01-23 04:18:23] INFO gemm_wrapper.py:307: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=48188) [2026-01-23 04:18:23] INFO kernels.py:329: INT8 quant+slide kernel loaded
(EngineCore_DP0 pid=48188) [2026-01-23 04:18:23] INFO kernels.py:115: Dequant+bias kernel loaded
(EngineCore_DP0 pid=48188) [2026-01-23 04:18:23] INFO SlideSparseLinearMethod_INT8.py:437: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=48188) [2026-01-23 04:18:23] INFO SlideSparseLinearMethod_INT8.py:582: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=48188) [2026-01-23 04:18:23] INFO SlideSparseLinearMethod_INT8.py:587: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=48188) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=48188) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 14.09it/s]
(EngineCore_DP0 pid=48188) 
(EngineCore_DP0 pid=48188) [2026-01-23 04:18:23] INFO SlideSparseLinearMethod_INT8.py:681: cuSPARSELt INT8 compression: [1152, 896] -> 1D uint8
(EngineCore_DP0 pid=48188) [2026-01-23 04:18:23] INFO SlideSparseLinearMethod_INT8.py:691: cuSPARSELt INT8 compression done: 774144 bytes
(EngineCore_DP0 pid=48188) [2026-01-23 04:18:23] INFO SlideSparseLinearMethod_INT8.py:681: cuSPARSELt INT8 compression: [896, 896] -> 1D uint8
(EngineCore_DP0 pid=48188) [2026-01-23 04:18:23] INFO SlideSparseLinearMethod_INT8.py:691: cuSPARSELt INT8 compression done: 602112 bytes
(EngineCore_DP0 pid=48188) [2026-01-23 04:18:23] INFO SlideSparseLinearMethod_INT8.py:681: cuSPARSELt INT8 compression: [9728, 896] -> 1D uint8
(EngineCore_DP0 pid=48188) [2026-01-23 04:18:23] INFO SlideSparseLinearMethod_INT8.py:691: cuSPARSELt INT8 compression done: 6537216 bytes
(EngineCore_DP0 pid=48188) [2026-01-23 04:18:23] INFO SlideSparseLinearMethod_INT8.py:681: cuSPARSELt INT8 compression: [896, 4864] -> 1D uint8
(EngineCore_DP0 pid=48188) [2026-01-23 04:18:23] INFO SlideSparseLinearMethod_INT8.py:691: cuSPARSELt INT8 compression done: 3268608 bytes
(EngineCore_DP0 pid=48188) 2026-01-23 04:18:46,711 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=48188) 2026-01-23 04:18:46,718 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=48188) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 46.08it/s]
(EngineCore_DP0 pid=48188) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.52it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.51it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  62%|██████▎   | 80/128 [00:00<00:00, 410.01it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 572.55it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:00<00:00, 279.66it/s, est. speed input: 71599.08 toks/s, output: 279.67 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:00<00:00, 147.80it/s, est. speed input: 40773.21 toks/s, output: 159.26 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:00<00:00, 138.74it/s, est. speed input: 38205.40 toks/s, output: 149.23 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:00<00:00, 137.55it/s, est. speed input: 37505.64 toks/s, output: 146.50 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:00<00:00, 136.47it/s, est. speed input: 37012.70 toks/s, output: 144.58 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:00<00:00, 135.88it/s, est. speed input: 36671.11 toks/s, output: 143.25 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 135.88it/s, est. speed input: 36536.64 toks/s, output: 142.72 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 142.71it/s, est. speed input: 36536.64 toks/s, output: 142.72 toks/s]
[rank0]:[W123 04:18:49.000838695 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 35.5s

测试结果:
  Requests/s:   114.12
  Tokens/s:     29328.96
  Total Reqs:   128
  Elapsed:      1.12s

  [Prefill 分析]
  Total Prefill Tokens: 32768
  Prefill Tokens/s:     29214.84


------------------------------------------------------------
  生成 CSV: Qwen2.5-0.5B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_4/Qwen2.5-0.5B-INT8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
16,16,1,128,128,123.3179,2096.4051,1.0380
128,128,1,128,128,118.1101,15236.2011,1.0837
256,256,1,128,128,114.1205,29328.9621,1.1216

------------------------------------------------------------

[INFO] 完成: 3 成功, 0 失败

============================================================
  Qwen2.5-0.5B-INT8 | cuSPARSELt (2_6) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_6
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_6

============================================================
[1/3] 测试 M=16
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-0.5B-INT8                               │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度:
│   M_prefill = 16 (= 1 x 16)
│   M_decode  = 1
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len       = 16
│   --output-len      = 1
│   --num-prompts     = 128
│   --max-num-seqs    = 1
│   --max-model-len   = 145
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
INFO 01-23 04:18:54 [datasets.py:612] Sampling input_len from [16, 16] and output_len from [1, 1]
INFO 01-23 04:18:54 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_6', 'max_model_len': 145, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 145, 'max_num_seqs': 1, 'disable_log_stats': True, 'enable_lora': None, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_6'}
INFO 01-23 04:18:54 [model.py:514] Resolved architecture: Qwen2ForCausalLM
INFO 01-23 04:18:54 [model.py:1661] Using max model len 145
INFO 01-23 04:18:54 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=145.
(EngineCore_DP0 pid=49487) INFO 01-23 04:18:58 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_6', speculative_config=None, tokenizer='/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_6', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=145, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_6, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [145], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 2, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=49487) INFO 01-23 04:18:58 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:39939 backend=nccl
(EngineCore_DP0 pid=49487) INFO 01-23 04:18:58 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=49487) INFO 01-23 04:18:58 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_6...
(EngineCore_DP0 pid=49487) INFO 01-23 04:18:58 [compressed_tensors_w8a8_int8.py:62] Using CutlassScaledMMLinearKernel for CompressedTensorsW8A8Int8
(EngineCore_DP0 pid=49487) INFO 01-23 04:18:58 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=49487) INFO 01-23 04:18:58 [default_loader.py:308] Loading weights took 0.09 seconds
(EngineCore_DP0 pid=49487) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=49487) INFO 01-23 04:18:59 [gpu_model_runner.py:3659] Model loading took 0.6055 GiB memory and 0.322760 seconds
(EngineCore_DP0 pid=49487) INFO 01-23 04:19:03 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/a02f47cdb9/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=49487) INFO 01-23 04:19:03 [backends.py:703] Dynamo bytecode transform time: 4.00 s
(EngineCore_DP0 pid=49487) INFO 01-23 04:19:06 [backends.py:261] Cache the graph of compile range (1, 145) for later use
(EngineCore_DP0 pid=49487) INFO 01-23 04:19:24 [backends.py:278] Compiling a graph for compile range (1, 145) takes 19.06 s
(EngineCore_DP0 pid=49487) INFO 01-23 04:19:24 [monitor.py:34] torch.compile takes 23.06 s in total
(EngineCore_DP0 pid=49487) INFO 01-23 04:19:24 [gpu_worker.py:375] Available KV cache memory: 11.40 GiB
(EngineCore_DP0 pid=49487) INFO 01-23 04:19:25 [kv_cache_utils.py:1291] GPU KV cache size: 995,824 tokens
(EngineCore_DP0 pid=49487) INFO 01-23 04:19:25 [kv_cache_utils.py:1296] Maximum concurrency for 145 tokens per request: 6223.90x
(EngineCore_DP0 pid=49487) INFO 01-23 04:19:25 [gpu_model_runner.py:4587] Graph capturing finished in 1 secs, took 0.01 GiB
(EngineCore_DP0 pid=49487) INFO 01-23 04:19:25 [core.py:259] init engine (profile, create kv cache, warmup model) took 26.43 seconds
INFO 01-23 04:19:26 [llm.py:360] Supported tasks: ['generate']
Throughput: 128.76 requests/s, 2188.84 total tokens/s, 128.76 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  128


─── STDERR ───
[2026-01-23 04:18:54] INFO gemm_wrapper.py:362: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-23 04:18:54] INFO gemm_wrapper.py:408: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-23 04:18:54] INFO gemm_wrapper.py:455: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-23 04:18:54] INFO gemm_wrapper.py:502: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-23 04:18:58] INFO gemm_wrapper.py:362: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-23 04:18:58] INFO gemm_wrapper.py:408: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-23 04:18:58] INFO gemm_wrapper.py:455: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-23 04:18:58] INFO gemm_wrapper.py:502: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=49487) [2026-01-23 04:18:58] INFO SlideSparseLinearMethod_INT8.py:767: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=49487) [2026-01-23 04:18:58] INFO gemm_wrapper.py:307: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=49487) [2026-01-23 04:18:58] INFO kernels.py:329: INT8 quant+slide kernel loaded
(EngineCore_DP0 pid=49487) [2026-01-23 04:18:58] INFO kernels.py:115: Dequant+bias kernel loaded
(EngineCore_DP0 pid=49487) [2026-01-23 04:18:58] INFO SlideSparseLinearMethod_INT8.py:437: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=49487) [2026-01-23 04:18:58] INFO SlideSparseLinearMethod_INT8.py:582: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=49487) [2026-01-23 04:18:58] INFO SlideSparseLinearMethod_INT8.py:587: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=49487) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=49487) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 12.01it/s]
(EngineCore_DP0 pid=49487) 
(EngineCore_DP0 pid=49487) [2026-01-23 04:18:58] INFO SlideSparseLinearMethod_INT8.py:681: cuSPARSELt INT8 compression: [1152, 1216] -> 1D uint8
(EngineCore_DP0 pid=49487) [2026-01-23 04:18:58] INFO SlideSparseLinearMethod_INT8.py:691: cuSPARSELt INT8 compression done: 1069056 bytes
(EngineCore_DP0 pid=49487) [2026-01-23 04:18:58] INFO SlideSparseLinearMethod_INT8.py:681: cuSPARSELt INT8 compression: [896, 1216] -> 1D uint8
(EngineCore_DP0 pid=49487) [2026-01-23 04:18:58] INFO SlideSparseLinearMethod_INT8.py:691: cuSPARSELt INT8 compression done: 831488 bytes
(EngineCore_DP0 pid=49487) [2026-01-23 04:18:58] INFO SlideSparseLinearMethod_INT8.py:681: cuSPARSELt INT8 compression: [9728, 1216] -> 1D uint8
(EngineCore_DP0 pid=49487) [2026-01-23 04:18:58] INFO SlideSparseLinearMethod_INT8.py:691: cuSPARSELt INT8 compression done: 9027584 bytes
(EngineCore_DP0 pid=49487) [2026-01-23 04:18:58] INFO SlideSparseLinearMethod_INT8.py:681: cuSPARSELt INT8 compression: [896, 6496] -> 1D uint8
(EngineCore_DP0 pid=49487) [2026-01-23 04:18:58] INFO SlideSparseLinearMethod_INT8.py:691: cuSPARSELt INT8 compression done: 4372480 bytes
(EngineCore_DP0 pid=49487) 2026-01-23 04:19:25,075 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=49487) 2026-01-23 04:19:25,083 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=49487) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 46.63it/s]
(EngineCore_DP0 pid=49487) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.49it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.48it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  62%|██████▎   | 80/128 [00:00<00:00, 550.26it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 844.15it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  21%|██        | 27/128 [00:00<00:00, 257.72it/s, est. speed input: 4123.91 toks/s, output: 257.73 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:00<00:00, 171.13it/s, est. speed input: 2886.38 toks/s, output: 180.39 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:00<00:00, 157.70it/s, est. speed input: 2679.65 toks/s, output: 167.47 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:00<00:00, 148.99it/s, est. speed input: 2556.10 toks/s, output: 159.75 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:00<00:00, 145.58it/s, est. speed input: 2497.19 toks/s, output: 156.07 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:00<00:00, 142.42it/s, est. speed input: 2449.92 toks/s, output: 153.12 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 142.42it/s, est. speed input: 2433.10 toks/s, output: 152.07 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 152.04it/s, est. speed input: 2433.10 toks/s, output: 152.07 toks/s]
[rank0]:[W123 04:19:27.214593114 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 38.2s

测试结果:
  Requests/s:   128.76
  Tokens/s:     2188.84
  Total Reqs:   128
  Elapsed:      0.99s

  [Prefill 分析]
  Total Prefill Tokens: 2048
  Prefill Tokens/s:     2060.09

============================================================
[2/3] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-0.5B-INT8                               │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度:
│   M_prefill = 128 (= 1 x 128)
│   M_decode  = 1
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len       = 128
│   --output-len      = 1
│   --num-prompts     = 128
│   --max-num-seqs    = 1
│   --max-model-len   = 257
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
INFO 01-23 04:19:32 [datasets.py:612] Sampling input_len from [128, 128] and output_len from [1, 1]
INFO 01-23 04:19:32 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_6', 'max_model_len': 257, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 257, 'max_num_seqs': 1, 'disable_log_stats': True, 'enable_lora': None, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_6'}
INFO 01-23 04:19:32 [model.py:514] Resolved architecture: Qwen2ForCausalLM
INFO 01-23 04:19:32 [model.py:1661] Using max model len 257
INFO 01-23 04:19:32 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=257.
(EngineCore_DP0 pid=50870) INFO 01-23 04:19:36 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_6', speculative_config=None, tokenizer='/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_6', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=257, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_6, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [257], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 2, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=50870) INFO 01-23 04:19:36 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:33695 backend=nccl
(EngineCore_DP0 pid=50870) INFO 01-23 04:19:36 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=50870) INFO 01-23 04:19:37 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_6...
(EngineCore_DP0 pid=50870) INFO 01-23 04:19:37 [compressed_tensors_w8a8_int8.py:62] Using CutlassScaledMMLinearKernel for CompressedTensorsW8A8Int8
(EngineCore_DP0 pid=50870) INFO 01-23 04:19:37 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=50870) INFO 01-23 04:19:37 [default_loader.py:308] Loading weights took 0.10 seconds
(EngineCore_DP0 pid=50870) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=50870) INFO 01-23 04:19:37 [gpu_model_runner.py:3659] Model loading took 0.6055 GiB memory and 0.321760 seconds
(EngineCore_DP0 pid=50870) INFO 01-23 04:19:41 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/e0a558d030/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=50870) INFO 01-23 04:19:41 [backends.py:703] Dynamo bytecode transform time: 3.96 s
(EngineCore_DP0 pid=50870) INFO 01-23 04:19:44 [backends.py:261] Cache the graph of compile range (1, 257) for later use
(EngineCore_DP0 pid=50870) INFO 01-23 04:20:02 [backends.py:278] Compiling a graph for compile range (1, 257) takes 19.34 s
(EngineCore_DP0 pid=50870) INFO 01-23 04:20:02 [monitor.py:34] torch.compile takes 23.30 s in total
(EngineCore_DP0 pid=50870) INFO 01-23 04:20:03 [gpu_worker.py:375] Available KV cache memory: 11.40 GiB
(EngineCore_DP0 pid=50870) INFO 01-23 04:20:03 [kv_cache_utils.py:1291] GPU KV cache size: 995,776 tokens
(EngineCore_DP0 pid=50870) INFO 01-23 04:20:03 [kv_cache_utils.py:1296] Maximum concurrency for 257 tokens per request: 3660.94x
(EngineCore_DP0 pid=50870) INFO 01-23 04:20:04 [gpu_model_runner.py:4587] Graph capturing finished in 1 secs, took -0.01 GiB
(EngineCore_DP0 pid=50870) INFO 01-23 04:20:04 [core.py:259] init engine (profile, create kv cache, warmup model) took 26.68 seconds
INFO 01-23 04:20:05 [llm.py:360] Supported tasks: ['generate']
Throughput: 124.18 requests/s, 16019.03 total tokens/s, 124.18 output tokens/s
Total num prompt tokens:  16384
Total num output tokens:  128


─── STDERR ───
[2026-01-23 04:19:32] INFO gemm_wrapper.py:362: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-23 04:19:32] INFO gemm_wrapper.py:408: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-23 04:19:32] INFO gemm_wrapper.py:455: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-23 04:19:32] INFO gemm_wrapper.py:502: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-23 04:19:36] INFO gemm_wrapper.py:362: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-23 04:19:36] INFO gemm_wrapper.py:408: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-23 04:19:36] INFO gemm_wrapper.py:455: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-23 04:19:36] INFO gemm_wrapper.py:502: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=50870) [2026-01-23 04:19:37] INFO SlideSparseLinearMethod_INT8.py:767: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=50870) [2026-01-23 04:19:37] INFO gemm_wrapper.py:307: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=50870) [2026-01-23 04:19:37] INFO kernels.py:329: INT8 quant+slide kernel loaded
(EngineCore_DP0 pid=50870) [2026-01-23 04:19:37] INFO kernels.py:115: Dequant+bias kernel loaded
(EngineCore_DP0 pid=50870) [2026-01-23 04:19:37] INFO SlideSparseLinearMethod_INT8.py:437: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=50870) [2026-01-23 04:19:37] INFO SlideSparseLinearMethod_INT8.py:582: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=50870) [2026-01-23 04:19:37] INFO SlideSparseLinearMethod_INT8.py:587: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=50870) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=50870) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 11.85it/s]
(EngineCore_DP0 pid=50870) 
(EngineCore_DP0 pid=50870) [2026-01-23 04:19:37] INFO SlideSparseLinearMethod_INT8.py:681: cuSPARSELt INT8 compression: [1152, 1216] -> 1D uint8
(EngineCore_DP0 pid=50870) [2026-01-23 04:19:37] INFO SlideSparseLinearMethod_INT8.py:691: cuSPARSELt INT8 compression done: 1069056 bytes
(EngineCore_DP0 pid=50870) [2026-01-23 04:19:37] INFO SlideSparseLinearMethod_INT8.py:681: cuSPARSELt INT8 compression: [896, 1216] -> 1D uint8
(EngineCore_DP0 pid=50870) [2026-01-23 04:19:37] INFO SlideSparseLinearMethod_INT8.py:691: cuSPARSELt INT8 compression done: 831488 bytes
(EngineCore_DP0 pid=50870) [2026-01-23 04:19:37] INFO SlideSparseLinearMethod_INT8.py:681: cuSPARSELt INT8 compression: [9728, 1216] -> 1D uint8
(EngineCore_DP0 pid=50870) [2026-01-23 04:19:37] INFO SlideSparseLinearMethod_INT8.py:691: cuSPARSELt INT8 compression done: 9027584 bytes
(EngineCore_DP0 pid=50870) [2026-01-23 04:19:37] INFO SlideSparseLinearMethod_INT8.py:681: cuSPARSELt INT8 compression: [896, 6496] -> 1D uint8
(EngineCore_DP0 pid=50870) [2026-01-23 04:19:37] INFO SlideSparseLinearMethod_INT8.py:691: cuSPARSELt INT8 compression done: 4372480 bytes
(EngineCore_DP0 pid=50870) 2026-01-23 04:20:03,829 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=50870) 2026-01-23 04:20:03,837 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=50870) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 45.46it/s]
(EngineCore_DP0 pid=50870) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.48it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.48it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 2051.91it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:00, 129.91it/s, est. speed input: 16629.27 toks/s, output: 129.91 toks/s]
Processed prompts:  21%|██        | 27/128 [00:00<00:00, 131.65it/s, est. speed input: 16818.83 toks/s, output: 131.39 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:00<00:00, 131.49it/s, est. speed input: 16814.99 toks/s, output: 131.37 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:00<00:00, 131.93it/s, est. speed input: 16854.51 toks/s, output: 131.67 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:00<00:00, 132.83it/s, est. speed input: 16925.13 toks/s, output: 132.23 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:00<00:00, 132.88it/s, est. speed input: 16941.45 toks/s, output: 132.35 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:00<00:00, 133.51it/s, est. speed input: 16986.26 toks/s, output: 132.70 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:00<00:00, 131.77it/s, est. speed input: 16911.71 toks/s, output: 132.12 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:00<00:00, 132.65it/s, est. speed input: 16946.72 toks/s, output: 132.39 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 132.65it/s, est. speed input: 16956.34 toks/s, output: 132.47 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 132.45it/s, est. speed input: 16956.34 toks/s, output: 132.47 toks/s]
[rank0]:[W123 04:20:06.021230307 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 39.1s

测试结果:
  Requests/s:   124.18
  Tokens/s:     16019.03
  Total Reqs:   128
  Elapsed:      1.03s

  [Prefill 分析]
  Total Prefill Tokens: 16384
  Prefill Tokens/s:     15894.86

============================================================
[3/3] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-0.5B-INT8                               │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度:
│   M_prefill = 256 (= 1 x 256)
│   M_decode  = 1
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len       = 256
│   --output-len      = 1
│   --num-prompts     = 128
│   --max-num-seqs    = 1
│   --max-model-len   = 385
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
INFO 01-23 04:20:11 [datasets.py:612] Sampling input_len from [256, 256] and output_len from [1, 1]
INFO 01-23 04:20:11 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_6', 'max_model_len': 385, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 385, 'max_num_seqs': 1, 'disable_log_stats': True, 'enable_lora': None, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_6'}
INFO 01-23 04:20:11 [model.py:514] Resolved architecture: Qwen2ForCausalLM
INFO 01-23 04:20:11 [model.py:1661] Using max model len 385
INFO 01-23 04:20:11 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=385.
(EngineCore_DP0 pid=52213) INFO 01-23 04:20:15 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_6', speculative_config=None, tokenizer='/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_6', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=385, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_6, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [385], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 2, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=52213) INFO 01-23 04:20:15 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:41891 backend=nccl
(EngineCore_DP0 pid=52213) INFO 01-23 04:20:15 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=52213) INFO 01-23 04:20:15 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_6...
(EngineCore_DP0 pid=52213) INFO 01-23 04:20:16 [compressed_tensors_w8a8_int8.py:62] Using CutlassScaledMMLinearKernel for CompressedTensorsW8A8Int8
(EngineCore_DP0 pid=52213) INFO 01-23 04:20:16 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=52213) INFO 01-23 04:20:16 [default_loader.py:308] Loading weights took 0.10 seconds
(EngineCore_DP0 pid=52213) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=52213) INFO 01-23 04:20:16 [gpu_model_runner.py:3659] Model loading took 0.6055 GiB memory and 0.320101 seconds
(EngineCore_DP0 pid=52213) INFO 01-23 04:20:20 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/656ce6a373/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=52213) INFO 01-23 04:20:20 [backends.py:703] Dynamo bytecode transform time: 4.06 s
(EngineCore_DP0 pid=52213) INFO 01-23 04:20:23 [backends.py:261] Cache the graph of compile range (1, 385) for later use
(EngineCore_DP0 pid=52213) INFO 01-23 04:20:41 [backends.py:278] Compiling a graph for compile range (1, 385) takes 18.70 s
(EngineCore_DP0 pid=52213) INFO 01-23 04:20:41 [monitor.py:34] torch.compile takes 22.76 s in total
(EngineCore_DP0 pid=52213) INFO 01-23 04:20:41 [gpu_worker.py:375] Available KV cache memory: 11.40 GiB
(EngineCore_DP0 pid=52213) INFO 01-23 04:20:42 [kv_cache_utils.py:1291] GPU KV cache size: 995,728 tokens
(EngineCore_DP0 pid=52213) INFO 01-23 04:20:42 [kv_cache_utils.py:1296] Maximum concurrency for 385 tokens per request: 2489.32x
(EngineCore_DP0 pid=52213) INFO 01-23 04:20:42 [gpu_model_runner.py:4587] Graph capturing finished in 1 secs, took -0.01 GiB
(EngineCore_DP0 pid=52213) INFO 01-23 04:20:42 [core.py:259] init engine (profile, create kv cache, warmup model) took 26.22 seconds
INFO 01-23 04:20:43 [llm.py:360] Supported tasks: ['generate']
Throughput: 120.63 requests/s, 31001.04 total tokens/s, 120.63 output tokens/s
Total num prompt tokens:  32768
Total num output tokens:  128


─── STDERR ───
[2026-01-23 04:20:11] INFO gemm_wrapper.py:362: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-23 04:20:11] INFO gemm_wrapper.py:408: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-23 04:20:11] INFO gemm_wrapper.py:455: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-23 04:20:11] INFO gemm_wrapper.py:502: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-23 04:20:15] INFO gemm_wrapper.py:362: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-23 04:20:15] INFO gemm_wrapper.py:408: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-23 04:20:15] INFO gemm_wrapper.py:455: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-23 04:20:15] INFO gemm_wrapper.py:502: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=52213) [2026-01-23 04:20:16] INFO SlideSparseLinearMethod_INT8.py:767: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=52213) [2026-01-23 04:20:16] INFO gemm_wrapper.py:307: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=52213) [2026-01-23 04:20:16] INFO kernels.py:329: INT8 quant+slide kernel loaded
(EngineCore_DP0 pid=52213) [2026-01-23 04:20:16] INFO kernels.py:115: Dequant+bias kernel loaded
(EngineCore_DP0 pid=52213) [2026-01-23 04:20:16] INFO SlideSparseLinearMethod_INT8.py:437: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=52213) [2026-01-23 04:20:16] INFO SlideSparseLinearMethod_INT8.py:582: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=52213) [2026-01-23 04:20:16] INFO SlideSparseLinearMethod_INT8.py:587: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=52213) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=52213) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 11.94it/s]
(EngineCore_DP0 pid=52213) 
(EngineCore_DP0 pid=52213) [2026-01-23 04:20:16] INFO SlideSparseLinearMethod_INT8.py:681: cuSPARSELt INT8 compression: [1152, 1216] -> 1D uint8
(EngineCore_DP0 pid=52213) [2026-01-23 04:20:16] INFO SlideSparseLinearMethod_INT8.py:691: cuSPARSELt INT8 compression done: 1069056 bytes
(EngineCore_DP0 pid=52213) [2026-01-23 04:20:16] INFO SlideSparseLinearMethod_INT8.py:681: cuSPARSELt INT8 compression: [896, 1216] -> 1D uint8
(EngineCore_DP0 pid=52213) [2026-01-23 04:20:16] INFO SlideSparseLinearMethod_INT8.py:691: cuSPARSELt INT8 compression done: 831488 bytes
(EngineCore_DP0 pid=52213) [2026-01-23 04:20:16] INFO SlideSparseLinearMethod_INT8.py:681: cuSPARSELt INT8 compression: [9728, 1216] -> 1D uint8
(EngineCore_DP0 pid=52213) [2026-01-23 04:20:16] INFO SlideSparseLinearMethod_INT8.py:691: cuSPARSELt INT8 compression done: 9027584 bytes
(EngineCore_DP0 pid=52213) [2026-01-23 04:20:16] INFO SlideSparseLinearMethod_INT8.py:681: cuSPARSELt INT8 compression: [896, 6496] -> 1D uint8
(EngineCore_DP0 pid=52213) [2026-01-23 04:20:16] INFO SlideSparseLinearMethod_INT8.py:691: cuSPARSELt INT8 compression done: 4372480 bytes
(EngineCore_DP0 pid=52213) 2026-01-23 04:20:42,120 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=52213) 2026-01-23 04:20:42,128 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=52213) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  9.42it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 11.84it/s]
(EngineCore_DP0 pid=52213) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.44it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.44it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 1561.18it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  12%|█▎        | 16/128 [00:00<00:00, 149.86it/s, est. speed input: 38368.56 toks/s, output: 149.87 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:00<00:00, 136.20it/s, est. speed input: 35368.16 toks/s, output: 138.15 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:00<00:00, 132.43it/s, est. speed input: 34497.83 toks/s, output: 134.75 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:00<00:00, 131.27it/s, est. speed input: 34163.92 toks/s, output: 133.45 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:00<00:00, 130.61it/s, est. speed input: 33961.67 toks/s, output: 132.66 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:00<00:00, 129.95it/s, est. speed input: 33792.03 toks/s, output: 132.00 toks/s]
Processed prompts:  78%|███████▊  | 100/128 [00:00<00:00, 129.61it/s, est. speed input: 33685.72 toks/s, output: 131.58 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:00<00:00, 129.16it/s, est. speed input: 33581.60 toks/s, output: 131.18 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:00<00:00, 129.23it/s, est. speed input: 33534.06 toks/s, output: 130.99 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 129.23it/s, est. speed input: 33525.92 toks/s, output: 130.96 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 130.95it/s, est. speed input: 33525.92 toks/s, output: 130.96 toks/s]
[rank0]:[W123 04:20:44.472283804 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 38.2s

测试结果:
  Requests/s:   120.63
  Tokens/s:     31001.04
  Total Reqs:   128
  Elapsed:      1.06s

  [Prefill 分析]
  Total Prefill Tokens: 32768
  Prefill Tokens/s:     30880.42


------------------------------------------------------------
  生成 CSV: Qwen2.5-0.5B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_6/Qwen2.5-0.5B-INT8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
16,16,1,128,128,128.7555,2188.8429,0.9941
128,128,1,128,128,124.1786,16019.0338,1.0308
256,256,1,128,128,120.6266,31001.0442,1.0611

------------------------------------------------------------

[INFO] 完成: 3 成功, 0 失败
[INFO] 尝试自动转换: Qwen2.5-0.5B-INT8 -> SlideSparse-2_8

======================================================================
[SlideSparse] 自动转换: Qwen2.5-0.5B-INT8 -> SlideSparse-2_8
======================================================================
[SUCCESS] 转换成功: Qwen2.5-0.5B-INT8-SlideSparse-2_8
======================================================================


============================================================
  Qwen2.5-0.5B-INT8 | cuSPARSELt (2_8) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_8
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_8

============================================================
[1/3] 测试 M=16
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-0.5B-INT8                               │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度:
│   M_prefill = 16 (= 1 x 16)
│   M_decode  = 1
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len       = 16
│   --output-len      = 1
│   --num-prompts     = 128
│   --max-num-seqs    = 1
│   --max-model-len   = 145
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
INFO 01-23 04:21:17 [datasets.py:612] Sampling input_len from [16, 16] and output_len from [1, 1]
INFO 01-23 04:21:17 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_8', 'max_model_len': 145, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 145, 'max_num_seqs': 1, 'disable_log_stats': True, 'enable_lora': None, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_8'}
INFO 01-23 04:21:17 [model.py:514] Resolved architecture: Qwen2ForCausalLM
INFO 01-23 04:21:17 [model.py:1661] Using max model len 145
INFO 01-23 04:21:17 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=145.
(EngineCore_DP0 pid=53832) INFO 01-23 04:21:21 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_8', speculative_config=None, tokenizer='/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=145, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_8, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [145], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 2, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=53832) INFO 01-23 04:21:21 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:44433 backend=nccl
(EngineCore_DP0 pid=53832) INFO 01-23 04:21:21 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=53832) INFO 01-23 04:21:22 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_8...
(EngineCore_DP0 pid=53832) INFO 01-23 04:21:22 [compressed_tensors_w8a8_int8.py:62] Using CutlassScaledMMLinearKernel for CompressedTensorsW8A8Int8
(EngineCore_DP0 pid=53832) INFO 01-23 04:21:22 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=53832) INFO 01-23 04:21:22 [default_loader.py:308] Loading weights took 0.10 seconds
(EngineCore_DP0 pid=53832) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=53832) INFO 01-23 04:21:22 [gpu_model_runner.py:3659] Model loading took 0.6436 GiB memory and 0.328629 seconds
(EngineCore_DP0 pid=53832) INFO 01-23 04:21:26 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/8ad0b7ca9e/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=53832) INFO 01-23 04:21:26 [backends.py:703] Dynamo bytecode transform time: 3.92 s
(EngineCore_DP0 pid=53832) INFO 01-23 04:21:29 [backends.py:261] Cache the graph of compile range (1, 145) for later use
(EngineCore_DP0 pid=53832) INFO 01-23 04:21:49 [backends.py:278] Compiling a graph for compile range (1, 145) takes 21.27 s
(EngineCore_DP0 pid=53832) INFO 01-23 04:21:49 [monitor.py:34] torch.compile takes 25.19 s in total
(EngineCore_DP0 pid=53832) INFO 01-23 04:21:50 [gpu_worker.py:375] Available KV cache memory: 11.36 GiB
(EngineCore_DP0 pid=53832) INFO 01-23 04:21:50 [kv_cache_utils.py:1291] GPU KV cache size: 992,496 tokens
(EngineCore_DP0 pid=53832) INFO 01-23 04:21:50 [kv_cache_utils.py:1296] Maximum concurrency for 145 tokens per request: 6203.10x
(EngineCore_DP0 pid=53832) INFO 01-23 04:21:51 [gpu_model_runner.py:4587] Graph capturing finished in 1 secs, took 0.01 GiB
(EngineCore_DP0 pid=53832) INFO 01-23 04:21:51 [core.py:259] init engine (profile, create kv cache, warmup model) took 28.53 seconds
INFO 01-23 04:21:52 [llm.py:360] Supported tasks: ['generate']
Throughput: 125.92 requests/s, 2140.56 total tokens/s, 125.92 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  128


─── STDERR ───
[2026-01-23 04:21:17] INFO gemm_wrapper.py:362: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-23 04:21:17] INFO gemm_wrapper.py:408: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-23 04:21:17] INFO gemm_wrapper.py:455: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-23 04:21:17] INFO gemm_wrapper.py:502: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-23 04:21:21] INFO gemm_wrapper.py:362: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-23 04:21:21] INFO gemm_wrapper.py:408: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-23 04:21:21] INFO gemm_wrapper.py:455: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-23 04:21:21] INFO gemm_wrapper.py:502: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=53832) [2026-01-23 04:21:22] INFO SlideSparseLinearMethod_INT8.py:767: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=53832) [2026-01-23 04:21:22] INFO gemm_wrapper.py:307: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=53832) [2026-01-23 04:21:22] INFO kernels.py:329: INT8 quant+slide kernel loaded
(EngineCore_DP0 pid=53832) [2026-01-23 04:21:22] INFO kernels.py:115: Dequant+bias kernel loaded
(EngineCore_DP0 pid=53832) [2026-01-23 04:21:22] INFO SlideSparseLinearMethod_INT8.py:437: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=53832) [2026-01-23 04:21:22] INFO SlideSparseLinearMethod_INT8.py:582: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=53832) [2026-01-23 04:21:22] INFO SlideSparseLinearMethod_INT8.py:587: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=53832) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=53832) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 11.27it/s]
(EngineCore_DP0 pid=53832) 
(EngineCore_DP0 pid=53832) [2026-01-23 04:21:22] INFO SlideSparseLinearMethod_INT8.py:681: cuSPARSELt INT8 compression: [1152, 1344] -> 1D uint8
(EngineCore_DP0 pid=53832) [2026-01-23 04:21:22] INFO SlideSparseLinearMethod_INT8.py:691: cuSPARSELt INT8 compression done: 1179648 bytes
(EngineCore_DP0 pid=53832) [2026-01-23 04:21:22] INFO SlideSparseLinearMethod_INT8.py:681: cuSPARSELt INT8 compression: [896, 1344] -> 1D uint8
(EngineCore_DP0 pid=53832) [2026-01-23 04:21:22] INFO SlideSparseLinearMethod_INT8.py:691: cuSPARSELt INT8 compression done: 917504 bytes
(EngineCore_DP0 pid=53832) [2026-01-23 04:21:22] INFO SlideSparseLinearMethod_INT8.py:681: cuSPARSELt INT8 compression: [9728, 1344] -> 1D uint8
(EngineCore_DP0 pid=53832) [2026-01-23 04:21:22] INFO SlideSparseLinearMethod_INT8.py:691: cuSPARSELt INT8 compression done: 9961472 bytes
(EngineCore_DP0 pid=53832) [2026-01-23 04:21:22] INFO SlideSparseLinearMethod_INT8.py:681: cuSPARSELt INT8 compression: [896, 7296] -> 1D uint8
(EngineCore_DP0 pid=53832) [2026-01-23 04:21:22] INFO SlideSparseLinearMethod_INT8.py:691: cuSPARSELt INT8 compression done: 4902912 bytes
(EngineCore_DP0 pid=53832) 2026-01-23 04:21:50,725 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=53832) 2026-01-23 04:21:50,733 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=53832) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 45.77it/s]
(EngineCore_DP0 pid=53832) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.33it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.33it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 4221.28it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   8%|▊         | 10/128 [00:00<00:01, 93.51it/s, est. speed input: 1496.36 toks/s, output: 93.52 toks/s]
Processed prompts:  19%|█▉        | 24/128 [00:00<00:00, 117.08it/s, est. speed input: 1816.12 toks/s, output: 113.50 toks/s]
Processed prompts:  30%|██▉       | 38/128 [00:00<00:00, 125.14it/s, est. speed input: 1928.77 toks/s, output: 120.55 toks/s]
Processed prompts:  41%|████      | 52/128 [00:00<00:00, 129.05it/s, est. speed input: 1986.36 toks/s, output: 124.15 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:00<00:00, 131.35it/s, est. speed input: 2022.22 toks/s, output: 126.39 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:00<00:00, 132.38it/s, est. speed input: 2043.44 toks/s, output: 127.71 toks/s]
Processed prompts:  73%|███████▎  | 94/128 [00:00<00:00, 132.76it/s, est. speed input: 2056.85 toks/s, output: 128.55 toks/s]
Processed prompts:  84%|████████▍ | 108/128 [00:00<00:00, 133.16it/s, est. speed input: 2067.73 toks/s, output: 129.23 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:00<00:00, 133.45it/s, est. speed input: 2076.34 toks/s, output: 129.77 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 133.45it/s, est. speed input: 2080.19 toks/s, output: 130.01 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 130.00it/s, est. speed input: 2080.19 toks/s, output: 130.01 toks/s]
[rank0]:[W123 04:21:53.915221188 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 40.8s

测试结果:
  Requests/s:   125.92
  Tokens/s:     2140.56
  Total Reqs:   128
  Elapsed:      1.02s

  [Prefill 分析]
  Total Prefill Tokens: 2048
  Prefill Tokens/s:     2014.65

============================================================
[2/3] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-0.5B-INT8                               │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度:
│   M_prefill = 128 (= 1 x 128)
│   M_decode  = 1
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len       = 128
│   --output-len      = 1
│   --num-prompts     = 128
│   --max-num-seqs    = 1
│   --max-model-len   = 257
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
INFO 01-23 04:21:58 [datasets.py:612] Sampling input_len from [128, 128] and output_len from [1, 1]
INFO 01-23 04:21:58 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_8', 'max_model_len': 257, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 257, 'max_num_seqs': 1, 'disable_log_stats': True, 'enable_lora': None, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_8'}
INFO 01-23 04:21:58 [model.py:514] Resolved architecture: Qwen2ForCausalLM
INFO 01-23 04:21:58 [model.py:1661] Using max model len 257
INFO 01-23 04:21:58 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=257.
(EngineCore_DP0 pid=55183) INFO 01-23 04:22:02 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_8', speculative_config=None, tokenizer='/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=257, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_8, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [257], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 2, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=55183) INFO 01-23 04:22:02 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:40109 backend=nccl
(EngineCore_DP0 pid=55183) INFO 01-23 04:22:02 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=55183) INFO 01-23 04:22:02 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_8...
(EngineCore_DP0 pid=55183) INFO 01-23 04:22:02 [compressed_tensors_w8a8_int8.py:62] Using CutlassScaledMMLinearKernel for CompressedTensorsW8A8Int8
(EngineCore_DP0 pid=55183) INFO 01-23 04:22:02 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=55183) INFO 01-23 04:22:02 [default_loader.py:308] Loading weights took 0.10 seconds
(EngineCore_DP0 pid=55183) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=55183) INFO 01-23 04:22:03 [gpu_model_runner.py:3659] Model loading took 0.6436 GiB memory and 0.324161 seconds
(EngineCore_DP0 pid=55183) INFO 01-23 04:22:07 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/c0e0dace29/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=55183) INFO 01-23 04:22:07 [backends.py:703] Dynamo bytecode transform time: 3.91 s
(EngineCore_DP0 pid=55183) INFO 01-23 04:22:09 [backends.py:261] Cache the graph of compile range (1, 257) for later use
(EngineCore_DP0 pid=55183) INFO 01-23 04:22:30 [backends.py:278] Compiling a graph for compile range (1, 257) takes 21.41 s
(EngineCore_DP0 pid=55183) INFO 01-23 04:22:30 [monitor.py:34] torch.compile takes 25.32 s in total
(EngineCore_DP0 pid=55183) INFO 01-23 04:22:30 [gpu_worker.py:375] Available KV cache memory: 11.36 GiB
(EngineCore_DP0 pid=55183) INFO 01-23 04:22:31 [kv_cache_utils.py:1291] GPU KV cache size: 992,448 tokens
(EngineCore_DP0 pid=55183) INFO 01-23 04:22:31 [kv_cache_utils.py:1296] Maximum concurrency for 257 tokens per request: 3648.71x
(EngineCore_DP0 pid=55183) INFO 01-23 04:22:31 [gpu_model_runner.py:4587] Graph capturing finished in 1 secs, took 0.01 GiB
(EngineCore_DP0 pid=55183) INFO 01-23 04:22:31 [core.py:259] init engine (profile, create kv cache, warmup model) took 28.71 seconds
INFO 01-23 04:22:32 [llm.py:360] Supported tasks: ['generate']
Throughput: 125.01 requests/s, 16126.72 total tokens/s, 125.01 output tokens/s
Total num prompt tokens:  16384
Total num output tokens:  128


─── STDERR ───
[2026-01-23 04:21:58] INFO gemm_wrapper.py:362: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-23 04:21:58] INFO gemm_wrapper.py:408: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-23 04:21:58] INFO gemm_wrapper.py:455: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-23 04:21:58] INFO gemm_wrapper.py:502: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-23 04:22:02] INFO gemm_wrapper.py:362: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-23 04:22:02] INFO gemm_wrapper.py:408: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-23 04:22:02] INFO gemm_wrapper.py:455: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-23 04:22:02] INFO gemm_wrapper.py:502: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=55183) [2026-01-23 04:22:02] INFO SlideSparseLinearMethod_INT8.py:767: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=55183) [2026-01-23 04:22:02] INFO gemm_wrapper.py:307: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=55183) [2026-01-23 04:22:02] INFO kernels.py:329: INT8 quant+slide kernel loaded
(EngineCore_DP0 pid=55183) [2026-01-23 04:22:02] INFO kernels.py:115: Dequant+bias kernel loaded
(EngineCore_DP0 pid=55183) [2026-01-23 04:22:02] INFO SlideSparseLinearMethod_INT8.py:437: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=55183) [2026-01-23 04:22:02] INFO SlideSparseLinearMethod_INT8.py:582: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=55183) [2026-01-23 04:22:02] INFO SlideSparseLinearMethod_INT8.py:587: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=55183) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=55183) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 11.36it/s]
(EngineCore_DP0 pid=55183) 
(EngineCore_DP0 pid=55183) [2026-01-23 04:22:02] INFO SlideSparseLinearMethod_INT8.py:681: cuSPARSELt INT8 compression: [1152, 1344] -> 1D uint8
(EngineCore_DP0 pid=55183) [2026-01-23 04:22:02] INFO SlideSparseLinearMethod_INT8.py:691: cuSPARSELt INT8 compression done: 1179648 bytes
(EngineCore_DP0 pid=55183) [2026-01-23 04:22:02] INFO SlideSparseLinearMethod_INT8.py:681: cuSPARSELt INT8 compression: [896, 1344] -> 1D uint8
(EngineCore_DP0 pid=55183) [2026-01-23 04:22:02] INFO SlideSparseLinearMethod_INT8.py:691: cuSPARSELt INT8 compression done: 917504 bytes
(EngineCore_DP0 pid=55183) [2026-01-23 04:22:02] INFO SlideSparseLinearMethod_INT8.py:681: cuSPARSELt INT8 compression: [9728, 1344] -> 1D uint8
(EngineCore_DP0 pid=55183) [2026-01-23 04:22:02] INFO SlideSparseLinearMethod_INT8.py:691: cuSPARSELt INT8 compression done: 9961472 bytes
(EngineCore_DP0 pid=55183) [2026-01-23 04:22:02] INFO SlideSparseLinearMethod_INT8.py:681: cuSPARSELt INT8 compression: [896, 7296] -> 1D uint8
(EngineCore_DP0 pid=55183) [2026-01-23 04:22:02] INFO SlideSparseLinearMethod_INT8.py:691: cuSPARSELt INT8 compression done: 4902912 bytes
(EngineCore_DP0 pid=55183) 2026-01-23 04:22:31,203 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=55183) 2026-01-23 04:22:31,211 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=55183) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 44.88it/s]
(EngineCore_DP0 pid=55183) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.42it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.42it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 2220.01it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  11%|█         | 14/128 [00:00<00:00, 130.26it/s, est. speed input: 16676.15 toks/s, output: 130.26 toks/s]
Processed prompts:  22%|██▏       | 28/128 [00:00<00:00, 132.27it/s, est. speed input: 16892.22 toks/s, output: 131.97 toks/s]
Processed prompts:  33%|███▎      | 42/128 [00:00<00:00, 133.16it/s, est. speed input: 16987.26 toks/s, output: 132.71 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:00<00:00, 132.06it/s, est. speed input: 16913.06 toks/s, output: 132.13 toks/s]
Processed prompts:  55%|█████▍    | 70/128 [00:00<00:00, 132.35it/s, est. speed input: 16931.68 toks/s, output: 132.27 toks/s]
Processed prompts:  66%|██████▌   | 84/128 [00:00<00:00, 132.46it/s, est. speed input: 16940.36 toks/s, output: 132.34 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:00<00:00, 133.04it/s, est. speed input: 16974.71 toks/s, output: 132.61 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:00<00:00, 132.64it/s, est. speed input: 16961.06 toks/s, output: 132.51 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:00<00:00, 132.98it/s, est. speed input: 16978.57 toks/s, output: 132.64 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 132.98it/s, est. speed input: 16973.36 toks/s, output: 132.60 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 132.59it/s, est. speed input: 16973.36 toks/s, output: 132.60 toks/s]
[rank0]:[W123 04:22:33.388202186 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 40.4s

测试结果:
  Requests/s:   125.01
  Tokens/s:     16126.72
  Total Reqs:   128
  Elapsed:      1.02s

  [Prefill 分析]
  Total Prefill Tokens: 16384
  Prefill Tokens/s:     16001.71

============================================================
[3/3] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-0.5B-INT8                               │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度:
│   M_prefill = 256 (= 1 x 256)
│   M_decode  = 1
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len       = 256
│   --output-len      = 1
│   --num-prompts     = 128
│   --max-num-seqs    = 1
│   --max-model-len   = 385
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
INFO 01-23 04:22:38 [datasets.py:612] Sampling input_len from [256, 256] and output_len from [1, 1]
INFO 01-23 04:22:38 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_8', 'max_model_len': 385, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 385, 'max_num_seqs': 1, 'disable_log_stats': True, 'enable_lora': None, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_8'}
INFO 01-23 04:22:38 [model.py:514] Resolved architecture: Qwen2ForCausalLM
INFO 01-23 04:22:38 [model.py:1661] Using max model len 385
INFO 01-23 04:22:38 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=385.
(EngineCore_DP0 pid=56584) INFO 01-23 04:22:42 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_8', speculative_config=None, tokenizer='/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=385, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_8, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [385], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 2, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=56584) INFO 01-23 04:22:42 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:53235 backend=nccl
(EngineCore_DP0 pid=56584) INFO 01-23 04:22:42 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=56584) INFO 01-23 04:22:43 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_8...
(EngineCore_DP0 pid=56584) INFO 01-23 04:22:43 [compressed_tensors_w8a8_int8.py:62] Using CutlassScaledMMLinearKernel for CompressedTensorsW8A8Int8
(EngineCore_DP0 pid=56584) INFO 01-23 04:22:43 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=56584) INFO 01-23 04:22:43 [default_loader.py:308] Loading weights took 0.10 seconds
(EngineCore_DP0 pid=56584) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=56584) INFO 01-23 04:22:43 [gpu_model_runner.py:3659] Model loading took 0.6436 GiB memory and 0.336445 seconds
(EngineCore_DP0 pid=56584) INFO 01-23 04:22:47 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/925e105152/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=56584) INFO 01-23 04:22:47 [backends.py:703] Dynamo bytecode transform time: 3.93 s
(EngineCore_DP0 pid=56584) INFO 01-23 04:22:50 [backends.py:261] Cache the graph of compile range (1, 385) for later use
(EngineCore_DP0 pid=56584) INFO 01-23 04:23:10 [backends.py:278] Compiling a graph for compile range (1, 385) takes 21.01 s
(EngineCore_DP0 pid=56584) INFO 01-23 04:23:10 [monitor.py:34] torch.compile takes 24.94 s in total
(EngineCore_DP0 pid=56584) INFO 01-23 04:23:11 [gpu_worker.py:375] Available KV cache memory: 11.36 GiB
(EngineCore_DP0 pid=56584) INFO 01-23 04:23:11 [kv_cache_utils.py:1291] GPU KV cache size: 992,400 tokens
(EngineCore_DP0 pid=56584) INFO 01-23 04:23:11 [kv_cache_utils.py:1296] Maximum concurrency for 385 tokens per request: 2481.00x
(EngineCore_DP0 pid=56584) INFO 01-23 04:23:12 [gpu_model_runner.py:4587] Graph capturing finished in 1 secs, took -0.01 GiB
(EngineCore_DP0 pid=56584) INFO 01-23 04:23:12 [core.py:259] init engine (profile, create kv cache, warmup model) took 28.26 seconds
INFO 01-23 04:23:12 [llm.py:360] Supported tasks: ['generate']
Throughput: 122.37 requests/s, 31449.38 total tokens/s, 122.37 output tokens/s
Total num prompt tokens:  32768
Total num output tokens:  128


─── STDERR ───
[2026-01-23 04:22:38] INFO gemm_wrapper.py:362: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-23 04:22:38] INFO gemm_wrapper.py:408: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-23 04:22:38] INFO gemm_wrapper.py:455: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-23 04:22:38] INFO gemm_wrapper.py:502: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-23 04:22:42] INFO gemm_wrapper.py:362: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-23 04:22:42] INFO gemm_wrapper.py:408: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-23 04:22:42] INFO gemm_wrapper.py:455: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-23 04:22:42] INFO gemm_wrapper.py:502: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=56584) [2026-01-23 04:22:43] INFO SlideSparseLinearMethod_INT8.py:767: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=56584) [2026-01-23 04:22:43] INFO gemm_wrapper.py:307: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=56584) [2026-01-23 04:22:43] INFO kernels.py:329: INT8 quant+slide kernel loaded
(EngineCore_DP0 pid=56584) [2026-01-23 04:22:43] INFO kernels.py:115: Dequant+bias kernel loaded
(EngineCore_DP0 pid=56584) [2026-01-23 04:22:43] INFO SlideSparseLinearMethod_INT8.py:437: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=56584) [2026-01-23 04:22:43] INFO SlideSparseLinearMethod_INT8.py:582: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=56584) [2026-01-23 04:22:43] INFO SlideSparseLinearMethod_INT8.py:587: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=56584) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=56584) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 11.20it/s]
(EngineCore_DP0 pid=56584) 
(EngineCore_DP0 pid=56584) [2026-01-23 04:22:43] INFO SlideSparseLinearMethod_INT8.py:681: cuSPARSELt INT8 compression: [1152, 1344] -> 1D uint8
(EngineCore_DP0 pid=56584) [2026-01-23 04:22:43] INFO SlideSparseLinearMethod_INT8.py:691: cuSPARSELt INT8 compression done: 1179648 bytes
(EngineCore_DP0 pid=56584) [2026-01-23 04:22:43] INFO SlideSparseLinearMethod_INT8.py:681: cuSPARSELt INT8 compression: [896, 1344] -> 1D uint8
(EngineCore_DP0 pid=56584) [2026-01-23 04:22:43] INFO SlideSparseLinearMethod_INT8.py:691: cuSPARSELt INT8 compression done: 917504 bytes
(EngineCore_DP0 pid=56584) [2026-01-23 04:22:43] INFO SlideSparseLinearMethod_INT8.py:681: cuSPARSELt INT8 compression: [9728, 1344] -> 1D uint8
(EngineCore_DP0 pid=56584) [2026-01-23 04:22:43] INFO SlideSparseLinearMethod_INT8.py:691: cuSPARSELt INT8 compression done: 9961472 bytes
(EngineCore_DP0 pid=56584) [2026-01-23 04:22:43] INFO SlideSparseLinearMethod_INT8.py:681: cuSPARSELt INT8 compression: [896, 7296] -> 1D uint8
(EngineCore_DP0 pid=56584) [2026-01-23 04:22:43] INFO SlideSparseLinearMethod_INT8.py:691: cuSPARSELt INT8 compression done: 4902912 bytes
(EngineCore_DP0 pid=56584) 2026-01-23 04:23:11,482 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=56584) 2026-01-23 04:23:11,490 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=56584) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 45.90it/s]
(EngineCore_DP0 pid=56584) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.48it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.48it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  62%|██████▎   | 80/128 [00:00<00:00, 414.45it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 577.23it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  25%|██▌       | 32/128 [00:00<00:00, 311.76it/s, est. speed input: 79818.20 toks/s, output: 311.77 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:00<00:00, 174.81it/s, est. speed input: 47908.44 toks/s, output: 187.14 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:00<00:00, 156.89it/s, est. speed input: 43458.18 toks/s, output: 169.76 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:00<00:00, 149.32it/s, est. speed input: 41549.14 toks/s, output: 162.30 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:00<00:00, 144.53it/s, est. speed input: 40373.30 toks/s, output: 157.71 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 144.53it/s, est. speed input: 39846.70 toks/s, output: 155.65 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 155.63it/s, est. speed input: 39846.70 toks/s, output: 155.65 toks/s]
[rank0]:[W123 04:23:13.699092437 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 40.2s

测试结果:
  Requests/s:   122.37
  Tokens/s:     31449.38
  Total Reqs:   128
  Elapsed:      1.05s

  [Prefill 分析]
  Total Prefill Tokens: 32768
  Prefill Tokens/s:     31327.01


------------------------------------------------------------
  生成 CSV: Qwen2.5-0.5B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_8/Qwen2.5-0.5B-INT8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
16,16,1,128,128,125.9156,2140.5644,1.0166
128,128,1,128,128,125.0134,16126.7222,1.0239
256,256,1,128,128,122.3711,31449.3782,1.0460

------------------------------------------------------------

[INFO] 完成: 3 成功, 0 失败
[INFO] 尝试自动转换: Qwen2.5-0.5B-INT8 -> SlideSparse-2_10

======================================================================
[SlideSparse] 自动转换: Qwen2.5-0.5B-INT8 -> SlideSparse-2_10
======================================================================
[SUCCESS] 转换成功: Qwen2.5-0.5B-INT8-SlideSparse-2_10
======================================================================


============================================================
  Qwen2.5-0.5B-INT8 | cuSPARSELt (2_10) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_10
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_10

============================================================
[1/3] 测试 M=16
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-0.5B-INT8                               │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度:
│   M_prefill = 16 (= 1 x 16)
│   M_decode  = 1
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len       = 16
│   --output-len      = 1
│   --num-prompts     = 128
│   --max-num-seqs    = 1
│   --max-model-len   = 145
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
INFO 01-23 04:23:46 [datasets.py:612] Sampling input_len from [16, 16] and output_len from [1, 1]
INFO 01-23 04:23:46 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_10', 'max_model_len': 145, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 145, 'max_num_seqs': 1, 'disable_log_stats': True, 'enable_lora': None, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_10'}
INFO 01-23 04:23:46 [model.py:514] Resolved architecture: Qwen2ForCausalLM
INFO 01-23 04:23:46 [model.py:1661] Using max model len 145
INFO 01-23 04:23:46 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=145.
(EngineCore_DP0 pid=58200) INFO 01-23 04:23:50 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_10', speculative_config=None, tokenizer='/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_10', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=145, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_10, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [145], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 2, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=58200) INFO 01-23 04:23:50 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:35465 backend=nccl
(EngineCore_DP0 pid=58200) INFO 01-23 04:23:50 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=58200) INFO 01-23 04:23:50 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_10...
(EngineCore_DP0 pid=58200) INFO 01-23 04:23:50 [compressed_tensors_w8a8_int8.py:62] Using CutlassScaledMMLinearKernel for CompressedTensorsW8A8Int8
(EngineCore_DP0 pid=58200) INFO 01-23 04:23:50 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=58200) INFO 01-23 04:23:51 [default_loader.py:308] Loading weights took 0.11 seconds
(EngineCore_DP0 pid=58200) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=58200) INFO 01-23 04:23:51 [gpu_model_runner.py:3659] Model loading took 0.6694 GiB memory and 0.645189 seconds
(EngineCore_DP0 pid=58200) INFO 01-23 04:23:55 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/64d7a3740e/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=58200) INFO 01-23 04:23:55 [backends.py:703] Dynamo bytecode transform time: 3.93 s
(EngineCore_DP0 pid=58200) INFO 01-23 04:23:58 [backends.py:261] Cache the graph of compile range (1, 145) for later use
(EngineCore_DP0 pid=58200) INFO 01-23 04:24:21 [backends.py:278] Compiling a graph for compile range (1, 145) takes 23.85 s
(EngineCore_DP0 pid=58200) INFO 01-23 04:24:21 [monitor.py:34] torch.compile takes 27.79 s in total
(EngineCore_DP0 pid=58200) INFO 01-23 04:24:22 [gpu_worker.py:375] Available KV cache memory: 11.33 GiB
(EngineCore_DP0 pid=58200) INFO 01-23 04:24:22 [kv_cache_utils.py:1291] GPU KV cache size: 990,240 tokens
(EngineCore_DP0 pid=58200) INFO 01-23 04:24:22 [kv_cache_utils.py:1296] Maximum concurrency for 145 tokens per request: 6189.00x
(EngineCore_DP0 pid=58200) INFO 01-23 04:24:22 [gpu_model_runner.py:4587] Graph capturing finished in 1 secs, took 0.01 GiB
(EngineCore_DP0 pid=58200) INFO 01-23 04:24:22 [core.py:259] init engine (profile, create kv cache, warmup model) took 31.14 seconds
INFO 01-23 04:24:23 [llm.py:360] Supported tasks: ['generate']
Throughput: 122.22 requests/s, 2077.79 total tokens/s, 122.22 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  128


─── STDERR ───
[2026-01-23 04:23:46] INFO gemm_wrapper.py:362: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-23 04:23:46] INFO gemm_wrapper.py:408: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-23 04:23:46] INFO gemm_wrapper.py:455: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-23 04:23:46] INFO gemm_wrapper.py:502: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-23 04:23:50] INFO gemm_wrapper.py:362: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-23 04:23:50] INFO gemm_wrapper.py:408: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-23 04:23:50] INFO gemm_wrapper.py:455: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-23 04:23:50] INFO gemm_wrapper.py:502: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=58200) [2026-01-23 04:23:50] INFO SlideSparseLinearMethod_INT8.py:767: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=58200) [2026-01-23 04:23:50] INFO gemm_wrapper.py:307: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=58200) [2026-01-23 04:23:50] INFO kernels.py:329: INT8 quant+slide kernel loaded
(EngineCore_DP0 pid=58200) [2026-01-23 04:23:50] INFO kernels.py:115: Dequant+bias kernel loaded
(EngineCore_DP0 pid=58200) [2026-01-23 04:23:50] INFO SlideSparseLinearMethod_INT8.py:437: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=58200) [2026-01-23 04:23:50] INFO SlideSparseLinearMethod_INT8.py:582: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=58200) [2026-01-23 04:23:50] INFO SlideSparseLinearMethod_INT8.py:587: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=58200) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=58200) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 10.94it/s]
(EngineCore_DP0 pid=58200) 
(EngineCore_DP0 pid=58200) [2026-01-23 04:23:51] INFO SlideSparseLinearMethod_INT8.py:681: cuSPARSELt INT8 compression: [1152, 1440] -> 1D uint8
(EngineCore_DP0 pid=58200) [2026-01-23 04:23:51] INFO SlideSparseLinearMethod_INT8.py:691: cuSPARSELt INT8 compression done: 1271808 bytes
(EngineCore_DP0 pid=58200) [2026-01-23 04:23:51] INFO SlideSparseLinearMethod_INT8.py:681: cuSPARSELt INT8 compression: [896, 1440] -> 1D uint8
(EngineCore_DP0 pid=58200) [2026-01-23 04:23:51] INFO SlideSparseLinearMethod_INT8.py:691: cuSPARSELt INT8 compression done: 989184 bytes
(EngineCore_DP0 pid=58200) [2026-01-23 04:23:51] INFO SlideSparseLinearMethod_INT8.py:681: cuSPARSELt INT8 compression: [9728, 1440] -> 1D uint8
(EngineCore_DP0 pid=58200) [2026-01-23 04:23:51] INFO SlideSparseLinearMethod_INT8.py:691: cuSPARSELt INT8 compression done: 10739712 bytes
(EngineCore_DP0 pid=58200) [2026-01-23 04:23:51] INFO SlideSparseLinearMethod_INT8.py:681: cuSPARSELt INT8 compression: [896, 7808] -> 1D uint8
(EngineCore_DP0 pid=58200) [2026-01-23 04:23:51] INFO SlideSparseLinearMethod_INT8.py:691: cuSPARSELt INT8 compression done: 5246976 bytes
(EngineCore_DP0 pid=58200) 2026-01-23 04:24:22,278 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=58200) 2026-01-23 04:24:22,292 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=58200) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 46.35it/s]
(EngineCore_DP0 pid=58200) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.33it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.32it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  62%|██████▎   | 80/128 [00:00<00:00, 513.93it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 789.03it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  20%|██        | 26/128 [00:00<00:00, 244.90it/s, est. speed input: 3919.03 toks/s, output: 244.91 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:00<00:00, 162.98it/s, est. speed input: 2748.43 toks/s, output: 171.77 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:00<00:00, 151.05it/s, est. speed input: 2563.08 toks/s, output: 160.19 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:00<00:00, 145.03it/s, est. speed input: 2471.30 toks/s, output: 154.45 toks/s]
Processed prompts:  78%|███████▊  | 100/128 [00:00<00:00, 141.37it/s, est. speed input: 2414.54 toks/s, output: 150.91 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:00<00:00, 136.18it/s, est. speed input: 2352.35 toks/s, output: 147.02 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 136.18it/s, est. speed input: 2319.24 toks/s, output: 144.95 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 144.93it/s, est. speed input: 2319.24 toks/s, output: 144.95 toks/s]
[rank0]:[W123 04:24:24.487815976 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 43.0s

测试结果:
  Requests/s:   122.22
  Tokens/s:     2077.79
  Total Reqs:   128
  Elapsed:      1.05s

  [Prefill 分析]
  Total Prefill Tokens: 2048
  Prefill Tokens/s:     1955.57

============================================================
[2/3] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-0.5B-INT8                               │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度:
│   M_prefill = 128 (= 1 x 128)
│   M_decode  = 1
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len       = 128
│   --output-len      = 1
│   --num-prompts     = 128
│   --max-num-seqs    = 1
│   --max-model-len   = 257
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
INFO 01-23 04:24:29 [datasets.py:612] Sampling input_len from [128, 128] and output_len from [1, 1]
INFO 01-23 04:24:29 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_10', 'max_model_len': 257, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 257, 'max_num_seqs': 1, 'disable_log_stats': True, 'enable_lora': None, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_10'}
INFO 01-23 04:24:29 [model.py:514] Resolved architecture: Qwen2ForCausalLM
INFO 01-23 04:24:29 [model.py:1661] Using max model len 257
INFO 01-23 04:24:29 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=257.
(EngineCore_DP0 pid=59555) INFO 01-23 04:24:33 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_10', speculative_config=None, tokenizer='/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_10', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=257, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_10, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [257], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 2, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=59555) INFO 01-23 04:24:33 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:36249 backend=nccl
(EngineCore_DP0 pid=59555) INFO 01-23 04:24:33 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=59555) INFO 01-23 04:24:33 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_10...
(EngineCore_DP0 pid=59555) INFO 01-23 04:24:34 [compressed_tensors_w8a8_int8.py:62] Using CutlassScaledMMLinearKernel for CompressedTensorsW8A8Int8
(EngineCore_DP0 pid=59555) INFO 01-23 04:24:34 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=59555) INFO 01-23 04:24:34 [default_loader.py:308] Loading weights took 0.11 seconds
(EngineCore_DP0 pid=59555) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=59555) INFO 01-23 04:24:34 [gpu_model_runner.py:3659] Model loading took 0.6694 GiB memory and 0.334455 seconds
(EngineCore_DP0 pid=59555) INFO 01-23 04:24:38 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/5c98280058/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=59555) INFO 01-23 04:24:38 [backends.py:703] Dynamo bytecode transform time: 3.91 s
(EngineCore_DP0 pid=59555) INFO 01-23 04:24:41 [backends.py:261] Cache the graph of compile range (1, 257) for later use
(EngineCore_DP0 pid=59555) INFO 01-23 04:25:04 [backends.py:278] Compiling a graph for compile range (1, 257) takes 23.70 s
(EngineCore_DP0 pid=59555) INFO 01-23 04:25:04 [monitor.py:34] torch.compile takes 27.61 s in total
(EngineCore_DP0 pid=59555) INFO 01-23 04:25:04 [gpu_worker.py:375] Available KV cache memory: 11.33 GiB
(EngineCore_DP0 pid=59555) INFO 01-23 04:25:04 [kv_cache_utils.py:1291] GPU KV cache size: 990,192 tokens
(EngineCore_DP0 pid=59555) INFO 01-23 04:25:04 [kv_cache_utils.py:1296] Maximum concurrency for 257 tokens per request: 3640.41x
(EngineCore_DP0 pid=59555) INFO 01-23 04:25:05 [gpu_model_runner.py:4587] Graph capturing finished in 1 secs, took -0.01 GiB
(EngineCore_DP0 pid=59555) INFO 01-23 04:25:05 [core.py:259] init engine (profile, create kv cache, warmup model) took 30.92 seconds
INFO 01-23 04:25:06 [llm.py:360] Supported tasks: ['generate']
Throughput: 122.74 requests/s, 15833.20 total tokens/s, 122.74 output tokens/s
Total num prompt tokens:  16384
Total num output tokens:  128


─── STDERR ───
[2026-01-23 04:24:29] INFO gemm_wrapper.py:362: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-23 04:24:29] INFO gemm_wrapper.py:408: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-23 04:24:29] INFO gemm_wrapper.py:455: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-23 04:24:29] INFO gemm_wrapper.py:502: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-23 04:24:33] INFO gemm_wrapper.py:362: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-23 04:24:33] INFO gemm_wrapper.py:408: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-23 04:24:33] INFO gemm_wrapper.py:455: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-23 04:24:33] INFO gemm_wrapper.py:502: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=59555) [2026-01-23 04:24:34] INFO SlideSparseLinearMethod_INT8.py:767: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=59555) [2026-01-23 04:24:34] INFO gemm_wrapper.py:307: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=59555) [2026-01-23 04:24:34] INFO kernels.py:329: INT8 quant+slide kernel loaded
(EngineCore_DP0 pid=59555) [2026-01-23 04:24:34] INFO kernels.py:115: Dequant+bias kernel loaded
(EngineCore_DP0 pid=59555) [2026-01-23 04:24:34] INFO SlideSparseLinearMethod_INT8.py:437: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=59555) [2026-01-23 04:24:34] INFO SlideSparseLinearMethod_INT8.py:582: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=59555) [2026-01-23 04:24:34] INFO SlideSparseLinearMethod_INT8.py:587: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=59555) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=59555) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 10.88it/s]
(EngineCore_DP0 pid=59555) 
(EngineCore_DP0 pid=59555) [2026-01-23 04:24:34] INFO SlideSparseLinearMethod_INT8.py:681: cuSPARSELt INT8 compression: [1152, 1440] -> 1D uint8
(EngineCore_DP0 pid=59555) [2026-01-23 04:24:34] INFO SlideSparseLinearMethod_INT8.py:691: cuSPARSELt INT8 compression done: 1271808 bytes
(EngineCore_DP0 pid=59555) [2026-01-23 04:24:34] INFO SlideSparseLinearMethod_INT8.py:681: cuSPARSELt INT8 compression: [896, 1440] -> 1D uint8
(EngineCore_DP0 pid=59555) [2026-01-23 04:24:34] INFO SlideSparseLinearMethod_INT8.py:691: cuSPARSELt INT8 compression done: 989184 bytes
(EngineCore_DP0 pid=59555) [2026-01-23 04:24:34] INFO SlideSparseLinearMethod_INT8.py:681: cuSPARSELt INT8 compression: [9728, 1440] -> 1D uint8
(EngineCore_DP0 pid=59555) [2026-01-23 04:24:34] INFO SlideSparseLinearMethod_INT8.py:691: cuSPARSELt INT8 compression done: 10739712 bytes
(EngineCore_DP0 pid=59555) [2026-01-23 04:24:34] INFO SlideSparseLinearMethod_INT8.py:681: cuSPARSELt INT8 compression: [896, 7808] -> 1D uint8
(EngineCore_DP0 pid=59555) [2026-01-23 04:24:34] INFO SlideSparseLinearMethod_INT8.py:691: cuSPARSELt INT8 compression done: 5246976 bytes
(EngineCore_DP0 pid=59555) 2026-01-23 04:25:04,961 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=59555) 2026-01-23 04:25:04,969 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=59555) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 45.96it/s]
(EngineCore_DP0 pid=59555) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.51it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.50it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 2533.68it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:01, 88.18it/s, est. speed input: 11289.13 toks/s, output: 88.18 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:00<00:00, 114.04it/s, est. speed input: 14112.63 toks/s, output: 110.25 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:00<00:00, 119.88it/s, est. speed input: 14817.76 toks/s, output: 115.76 toks/s]
Processed prompts:  39%|███▉      | 50/128 [00:00<00:00, 125.31it/s, est. speed input: 15390.41 toks/s, output: 120.24 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:00<00:00, 128.98it/s, est. speed input: 15780.69 toks/s, output: 123.28 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:00<00:00, 131.11it/s, est. speed input: 16034.91 toks/s, output: 125.27 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:00<00:00, 132.59it/s, est. speed input: 16224.31 toks/s, output: 126.75 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:00<00:00, 133.49it/s, est. speed input: 16362.30 toks/s, output: 127.83 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:00<00:00, 134.24it/s, est. speed input: 16476.34 toks/s, output: 128.72 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 134.24it/s, est. speed input: 16528.02 toks/s, output: 129.12 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 129.11it/s, est. speed input: 16528.02 toks/s, output: 129.12 toks/s]
[rank0]:[W123 04:25:07.147516118 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 42.8s

测试结果:
  Requests/s:   122.74
  Tokens/s:     15833.20
  Total Reqs:   128
  Elapsed:      1.04s

  [Prefill 分析]
  Total Prefill Tokens: 16384
  Prefill Tokens/s:     15710.46

============================================================
[3/3] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-0.5B-INT8                               │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度:
│   M_prefill = 256 (= 1 x 256)
│   M_decode  = 1
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len       = 256
│   --output-len      = 1
│   --num-prompts     = 128
│   --max-num-seqs    = 1
│   --max-model-len   = 385
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
INFO 01-23 04:25:12 [datasets.py:612] Sampling input_len from [256, 256] and output_len from [1, 1]
INFO 01-23 04:25:12 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_10', 'max_model_len': 385, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 385, 'max_num_seqs': 1, 'disable_log_stats': True, 'enable_lora': None, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_10'}
INFO 01-23 04:25:12 [model.py:514] Resolved architecture: Qwen2ForCausalLM
INFO 01-23 04:25:12 [model.py:1661] Using max model len 385
INFO 01-23 04:25:12 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=385.
(EngineCore_DP0 pid=60929) INFO 01-23 04:25:16 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_10', speculative_config=None, tokenizer='/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_10', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=385, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_10, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [385], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 2, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=60929) INFO 01-23 04:25:16 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:39551 backend=nccl
(EngineCore_DP0 pid=60929) INFO 01-23 04:25:16 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=60929) INFO 01-23 04:25:16 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_10...
(EngineCore_DP0 pid=60929) INFO 01-23 04:25:16 [compressed_tensors_w8a8_int8.py:62] Using CutlassScaledMMLinearKernel for CompressedTensorsW8A8Int8
(EngineCore_DP0 pid=60929) INFO 01-23 04:25:16 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=60929) INFO 01-23 04:25:17 [default_loader.py:308] Loading weights took 0.11 seconds
(EngineCore_DP0 pid=60929) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=60929) INFO 01-23 04:25:17 [gpu_model_runner.py:3659] Model loading took 0.6694 GiB memory and 0.343579 seconds
(EngineCore_DP0 pid=60929) INFO 01-23 04:25:21 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/d25496b8ce/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=60929) INFO 01-23 04:25:21 [backends.py:703] Dynamo bytecode transform time: 3.83 s
(EngineCore_DP0 pid=60929) INFO 01-23 04:25:24 [backends.py:261] Cache the graph of compile range (1, 385) for later use
(EngineCore_DP0 pid=60929) INFO 01-23 04:25:46 [backends.py:278] Compiling a graph for compile range (1, 385) takes 23.22 s
(EngineCore_DP0 pid=60929) INFO 01-23 04:25:46 [monitor.py:34] torch.compile takes 27.05 s in total
(EngineCore_DP0 pid=60929) INFO 01-23 04:25:46 [gpu_worker.py:375] Available KV cache memory: 11.33 GiB
(EngineCore_DP0 pid=60929) INFO 01-23 04:25:47 [kv_cache_utils.py:1291] GPU KV cache size: 990,144 tokens
(EngineCore_DP0 pid=60929) INFO 01-23 04:25:47 [kv_cache_utils.py:1296] Maximum concurrency for 385 tokens per request: 2475.36x
(EngineCore_DP0 pid=60929) INFO 01-23 04:25:47 [gpu_model_runner.py:4587] Graph capturing finished in 1 secs, took -0.01 GiB
(EngineCore_DP0 pid=60929) INFO 01-23 04:25:47 [core.py:259] init engine (profile, create kv cache, warmup model) took 30.30 seconds
INFO 01-23 04:25:48 [llm.py:360] Supported tasks: ['generate']
Throughput: 125.61 requests/s, 32280.66 total tokens/s, 125.61 output tokens/s
Total num prompt tokens:  32768
Total num output tokens:  128


─── STDERR ───
[2026-01-23 04:25:12] INFO gemm_wrapper.py:362: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-23 04:25:12] INFO gemm_wrapper.py:408: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-23 04:25:12] INFO gemm_wrapper.py:455: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-23 04:25:12] INFO gemm_wrapper.py:502: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-23 04:25:16] INFO gemm_wrapper.py:362: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-23 04:25:16] INFO gemm_wrapper.py:408: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-23 04:25:16] INFO gemm_wrapper.py:455: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-23 04:25:16] INFO gemm_wrapper.py:502: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=60929) [2026-01-23 04:25:16] INFO SlideSparseLinearMethod_INT8.py:767: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=60929) [2026-01-23 04:25:16] INFO gemm_wrapper.py:307: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=60929) [2026-01-23 04:25:16] INFO kernels.py:329: INT8 quant+slide kernel loaded
(EngineCore_DP0 pid=60929) [2026-01-23 04:25:16] INFO kernels.py:115: Dequant+bias kernel loaded
(EngineCore_DP0 pid=60929) [2026-01-23 04:25:16] INFO SlideSparseLinearMethod_INT8.py:437: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=60929) [2026-01-23 04:25:16] INFO SlideSparseLinearMethod_INT8.py:582: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=60929) [2026-01-23 04:25:16] INFO SlideSparseLinearMethod_INT8.py:587: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=60929) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=60929) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 10.66it/s]
(EngineCore_DP0 pid=60929) 
(EngineCore_DP0 pid=60929) [2026-01-23 04:25:17] INFO SlideSparseLinearMethod_INT8.py:681: cuSPARSELt INT8 compression: [1152, 1440] -> 1D uint8
(EngineCore_DP0 pid=60929) [2026-01-23 04:25:17] INFO SlideSparseLinearMethod_INT8.py:691: cuSPARSELt INT8 compression done: 1271808 bytes
(EngineCore_DP0 pid=60929) [2026-01-23 04:25:17] INFO SlideSparseLinearMethod_INT8.py:681: cuSPARSELt INT8 compression: [896, 1440] -> 1D uint8
(EngineCore_DP0 pid=60929) [2026-01-23 04:25:17] INFO SlideSparseLinearMethod_INT8.py:691: cuSPARSELt INT8 compression done: 989184 bytes
(EngineCore_DP0 pid=60929) [2026-01-23 04:25:17] INFO SlideSparseLinearMethod_INT8.py:681: cuSPARSELt INT8 compression: [9728, 1440] -> 1D uint8
(EngineCore_DP0 pid=60929) [2026-01-23 04:25:17] INFO SlideSparseLinearMethod_INT8.py:691: cuSPARSELt INT8 compression done: 10739712 bytes
(EngineCore_DP0 pid=60929) [2026-01-23 04:25:17] INFO SlideSparseLinearMethod_INT8.py:681: cuSPARSELt INT8 compression: [896, 7808] -> 1D uint8
(EngineCore_DP0 pid=60929) [2026-01-23 04:25:17] INFO SlideSparseLinearMethod_INT8.py:691: cuSPARSELt INT8 compression done: 5246976 bytes
(EngineCore_DP0 pid=60929) 2026-01-23 04:25:47,106 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=60929) 2026-01-23 04:25:47,114 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=60929) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 47.24it/s]
(EngineCore_DP0 pid=60929) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.46it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.45it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  62%|██████▎   | 80/128 [00:00<00:00, 422.77it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 587.67it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:00<00:00, 312.01it/s, est. speed input: 79883.22 toks/s, output: 312.02 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:00<00:00, 179.23it/s, est. speed input: 49064.02 toks/s, output: 191.65 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:00<00:00, 161.48it/s, est. speed input: 44670.55 toks/s, output: 174.49 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:00<00:00, 153.10it/s, est. speed input: 42629.93 toks/s, output: 166.52 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:00<00:00, 148.17it/s, est. speed input: 41391.86 toks/s, output: 161.68 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 148.17it/s, est. speed input: 40930.36 toks/s, output: 159.88 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 159.86it/s, est. speed input: 40930.36 toks/s, output: 159.88 toks/s]
[rank0]:[W123 04:25:49.259652292 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 42.0s

测试结果:
  Requests/s:   125.61
  Tokens/s:     32280.66
  Total Reqs:   128
  Elapsed:      1.02s

  [Prefill 分析]
  Total Prefill Tokens: 32768
  Prefill Tokens/s:     32155.05


------------------------------------------------------------
  生成 CSV: Qwen2.5-0.5B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_10/Qwen2.5-0.5B-INT8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
16,16,1,128,128,122.2231,2077.7928,1.0473
128,128,1,128,128,122.7379,15833.1954,1.0429
256,256,1,128,128,125.6057,32280.6564,1.0191

------------------------------------------------------------

[INFO] 完成: 3 成功, 0 失败
[INFO] 尝试自动转换: Qwen2.5-0.5B-INT8 -> SlideSparse-2_12

======================================================================
[SlideSparse] 自动转换: Qwen2.5-0.5B-INT8 -> SlideSparse-2_12
======================================================================
[SUCCESS] 转换成功: Qwen2.5-0.5B-INT8-SlideSparse-2_12
======================================================================


============================================================
  Qwen2.5-0.5B-INT8 | cuSPARSELt (2_12) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_12
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_12

============================================================
[1/3] 测试 M=16
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-0.5B-INT8                               │
│ Backend:  cuSPARSELt (2:12)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度:
│   M_prefill = 16 (= 1 x 16)
│   M_decode  = 1
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len       = 16
│   --output-len      = 1
│   --num-prompts     = 128
│   --max-num-seqs    = 1
│   --max-model-len   = 145
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
INFO 01-23 04:26:27 [datasets.py:612] Sampling input_len from [16, 16] and output_len from [1, 1]
INFO 01-23 04:26:27 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_12', 'max_model_len': 145, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 145, 'max_num_seqs': 1, 'disable_log_stats': True, 'enable_lora': None, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_12'}
INFO 01-23 04:26:27 [model.py:514] Resolved architecture: Qwen2ForCausalLM
INFO 01-23 04:26:27 [model.py:1661] Using max model len 145
INFO 01-23 04:26:27 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=145.
(EngineCore_DP0 pid=62622) INFO 01-23 04:26:30 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_12', speculative_config=None, tokenizer='/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_12', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=145, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_12, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [145], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 2, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=62622) INFO 01-23 04:26:31 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:57967 backend=nccl
(EngineCore_DP0 pid=62622) INFO 01-23 04:26:31 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=62622) INFO 01-23 04:26:31 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_12...
(EngineCore_DP0 pid=62622) INFO 01-23 04:26:31 [compressed_tensors_w8a8_int8.py:62] Using CutlassScaledMMLinearKernel for CompressedTensorsW8A8Int8
(EngineCore_DP0 pid=62622) INFO 01-23 04:26:31 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=62622) INFO 01-23 04:26:31 [default_loader.py:308] Loading weights took 0.11 seconds
(EngineCore_DP0 pid=62622) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=62622) INFO 01-23 04:26:32 [gpu_model_runner.py:3659] Model loading took 0.6898 GiB memory and 0.340171 seconds
(EngineCore_DP0 pid=62622) INFO 01-23 04:26:36 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/cd011b4f90/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=62622) INFO 01-23 04:26:36 [backends.py:703] Dynamo bytecode transform time: 3.90 s
(EngineCore_DP0 pid=62622) INFO 01-23 04:26:38 [backends.py:261] Cache the graph of compile range (1, 145) for later use
(EngineCore_DP0 pid=62622) INFO 01-23 04:27:03 [backends.py:278] Compiling a graph for compile range (1, 145) takes 25.85 s
(EngineCore_DP0 pid=62622) INFO 01-23 04:27:03 [monitor.py:34] torch.compile takes 29.75 s in total
(EngineCore_DP0 pid=62622) INFO 01-23 04:27:04 [gpu_worker.py:375] Available KV cache memory: 11.31 GiB
(EngineCore_DP0 pid=62622) INFO 01-23 04:27:04 [kv_cache_utils.py:1291] GPU KV cache size: 988,448 tokens
(EngineCore_DP0 pid=62622) INFO 01-23 04:27:04 [kv_cache_utils.py:1296] Maximum concurrency for 145 tokens per request: 6177.80x
(EngineCore_DP0 pid=62622) INFO 01-23 04:27:05 [gpu_model_runner.py:4587] Graph capturing finished in 1 secs, took 0.01 GiB
(EngineCore_DP0 pid=62622) INFO 01-23 04:27:05 [core.py:259] init engine (profile, create kv cache, warmup model) took 33.05 seconds
INFO 01-23 04:27:05 [llm.py:360] Supported tasks: ['generate']
Throughput: 123.28 requests/s, 2095.75 total tokens/s, 123.28 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  128


─── STDERR ───
[2026-01-23 04:26:27] INFO gemm_wrapper.py:362: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-23 04:26:27] INFO gemm_wrapper.py:408: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-23 04:26:27] INFO gemm_wrapper.py:455: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-23 04:26:27] INFO gemm_wrapper.py:502: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-23 04:26:30] INFO gemm_wrapper.py:362: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-23 04:26:30] INFO gemm_wrapper.py:408: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-23 04:26:30] INFO gemm_wrapper.py:455: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-23 04:26:30] INFO gemm_wrapper.py:502: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=62622) [2026-01-23 04:26:31] INFO SlideSparseLinearMethod_INT8.py:767: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=62622) [2026-01-23 04:26:31] INFO gemm_wrapper.py:307: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=62622) [2026-01-23 04:26:31] INFO kernels.py:329: INT8 quant+slide kernel loaded
(EngineCore_DP0 pid=62622) [2026-01-23 04:26:31] INFO kernels.py:115: Dequant+bias kernel loaded
(EngineCore_DP0 pid=62622) [2026-01-23 04:26:31] INFO SlideSparseLinearMethod_INT8.py:437: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=62622) [2026-01-23 04:26:31] INFO SlideSparseLinearMethod_INT8.py:582: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:12, expand_ratio=1.200
(EngineCore_DP0 pid=62622) [2026-01-23 04:26:31] INFO SlideSparseLinearMethod_INT8.py:587: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=62622) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=62622) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 10.30it/s]
(EngineCore_DP0 pid=62622) 
(EngineCore_DP0 pid=62622) [2026-01-23 04:26:31] INFO SlideSparseLinearMethod_INT8.py:681: cuSPARSELt INT8 compression: [1152, 1504] -> 1D uint8
(EngineCore_DP0 pid=62622) [2026-01-23 04:26:31] INFO SlideSparseLinearMethod_INT8.py:691: cuSPARSELt INT8 compression done: 1308672 bytes
(EngineCore_DP0 pid=62622) [2026-01-23 04:26:31] INFO SlideSparseLinearMethod_INT8.py:681: cuSPARSELt INT8 compression: [896, 1504] -> 1D uint8
(EngineCore_DP0 pid=62622) [2026-01-23 04:26:31] INFO SlideSparseLinearMethod_INT8.py:691: cuSPARSELt INT8 compression done: 1017856 bytes
(EngineCore_DP0 pid=62622) [2026-01-23 04:26:31] INFO SlideSparseLinearMethod_INT8.py:681: cuSPARSELt INT8 compression: [9728, 1504] -> 1D uint8
(EngineCore_DP0 pid=62622) [2026-01-23 04:26:31] INFO SlideSparseLinearMethod_INT8.py:691: cuSPARSELt INT8 compression done: 11051008 bytes
(EngineCore_DP0 pid=62622) [2026-01-23 04:26:31] INFO SlideSparseLinearMethod_INT8.py:681: cuSPARSELt INT8 compression: [896, 8128] -> 1D uint8
(EngineCore_DP0 pid=62622) [2026-01-23 04:26:31] INFO SlideSparseLinearMethod_INT8.py:691: cuSPARSELt INT8 compression done: 5476352 bytes
(EngineCore_DP0 pid=62622) 2026-01-23 04:27:04,419 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=62622) 2026-01-23 04:27:04,427 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=62622) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 39.07it/s]
(EngineCore_DP0 pid=62622) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.38it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.37it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  62%|██████▎   | 80/128 [00:00<00:00, 524.38it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 804.91it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  20%|██        | 26/128 [00:00<00:00, 250.47it/s, est. speed input: 4007.98 toks/s, output: 250.48 toks/s]
Processed prompts:  41%|████      | 52/128 [00:00<00:00, 167.81it/s, est. speed input: 2824.77 toks/s, output: 176.55 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:00<00:00, 154.72it/s, est. speed input: 2624.08 toks/s, output: 164.00 toks/s]
Processed prompts:  69%|██████▉   | 88/128 [00:00<00:00, 148.72it/s, est. speed input: 2530.86 toks/s, output: 158.17 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:00<00:00, 143.75it/s, est. speed input: 2461.12 toks/s, output: 153.82 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:00<00:00, 141.08it/s, est. speed input: 2418.47 toks/s, output: 151.15 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 141.08it/s, est. speed input: 2333.88 toks/s, output: 145.87 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 145.85it/s, est. speed input: 2333.88 toks/s, output: 145.87 toks/s]
[rank0]:[W123 04:27:06.623665240 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 44.6s

测试结果:
  Requests/s:   123.28
  Tokens/s:     2095.75
  Total Reqs:   128
  Elapsed:      1.04s

  [Prefill 分析]
  Total Prefill Tokens: 2048
  Prefill Tokens/s:     1972.47

============================================================
[2/3] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-0.5B-INT8                               │
│ Backend:  cuSPARSELt (2:12)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度:
│   M_prefill = 128 (= 1 x 128)
│   M_decode  = 1
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len       = 128
│   --output-len      = 1
│   --num-prompts     = 128
│   --max-num-seqs    = 1
│   --max-model-len   = 257
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
INFO 01-23 04:27:11 [datasets.py:612] Sampling input_len from [128, 128] and output_len from [1, 1]
INFO 01-23 04:27:11 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_12', 'max_model_len': 257, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 257, 'max_num_seqs': 1, 'disable_log_stats': True, 'enable_lora': None, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_12'}
INFO 01-23 04:27:11 [model.py:514] Resolved architecture: Qwen2ForCausalLM
INFO 01-23 04:27:11 [model.py:1661] Using max model len 257
INFO 01-23 04:27:11 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=257.
(EngineCore_DP0 pid=63990) INFO 01-23 04:27:15 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_12', speculative_config=None, tokenizer='/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_12', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=257, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_12, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [257], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 2, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=63990) INFO 01-23 04:27:15 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:51147 backend=nccl
(EngineCore_DP0 pid=63990) INFO 01-23 04:27:15 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=63990) INFO 01-23 04:27:16 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_12...
(EngineCore_DP0 pid=63990) INFO 01-23 04:27:16 [compressed_tensors_w8a8_int8.py:62] Using CutlassScaledMMLinearKernel for CompressedTensorsW8A8Int8
(EngineCore_DP0 pid=63990) INFO 01-23 04:27:16 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=63990) INFO 01-23 04:27:16 [default_loader.py:308] Loading weights took 0.11 seconds
(EngineCore_DP0 pid=63990) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=63990) INFO 01-23 04:27:16 [gpu_model_runner.py:3659] Model loading took 0.6898 GiB memory and 0.342775 seconds
(EngineCore_DP0 pid=63990) INFO 01-23 04:27:20 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/8d5e68c92a/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=63990) INFO 01-23 04:27:20 [backends.py:703] Dynamo bytecode transform time: 3.92 s
(EngineCore_DP0 pid=63990) INFO 01-23 04:27:23 [backends.py:261] Cache the graph of compile range (1, 257) for later use
(EngineCore_DP0 pid=63990) INFO 01-23 04:27:48 [backends.py:278] Compiling a graph for compile range (1, 257) takes 26.05 s
(EngineCore_DP0 pid=63990) INFO 01-23 04:27:48 [monitor.py:34] torch.compile takes 29.97 s in total
(EngineCore_DP0 pid=63990) INFO 01-23 04:27:49 [gpu_worker.py:375] Available KV cache memory: 11.31 GiB
(EngineCore_DP0 pid=63990) INFO 01-23 04:27:49 [kv_cache_utils.py:1291] GPU KV cache size: 988,400 tokens
(EngineCore_DP0 pid=63990) INFO 01-23 04:27:49 [kv_cache_utils.py:1296] Maximum concurrency for 257 tokens per request: 3633.82x
(EngineCore_DP0 pid=63990) INFO 01-23 04:27:50 [gpu_model_runner.py:4587] Graph capturing finished in 1 secs, took 0.01 GiB
(EngineCore_DP0 pid=63990) INFO 01-23 04:27:50 [core.py:259] init engine (profile, create kv cache, warmup model) took 33.29 seconds
INFO 01-23 04:27:50 [llm.py:360] Supported tasks: ['generate']
Throughput: 122.49 requests/s, 15801.38 total tokens/s, 122.49 output tokens/s
Total num prompt tokens:  16384
Total num output tokens:  128


─── STDERR ───
[2026-01-23 04:27:11] INFO gemm_wrapper.py:362: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-23 04:27:11] INFO gemm_wrapper.py:408: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-23 04:27:11] INFO gemm_wrapper.py:455: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-23 04:27:11] INFO gemm_wrapper.py:502: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-23 04:27:15] INFO gemm_wrapper.py:362: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-23 04:27:15] INFO gemm_wrapper.py:408: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-23 04:27:15] INFO gemm_wrapper.py:455: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-23 04:27:15] INFO gemm_wrapper.py:502: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=63990) [2026-01-23 04:27:16] INFO SlideSparseLinearMethod_INT8.py:767: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=63990) [2026-01-23 04:27:16] INFO gemm_wrapper.py:307: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=63990) [2026-01-23 04:27:16] INFO kernels.py:329: INT8 quant+slide kernel loaded
(EngineCore_DP0 pid=63990) [2026-01-23 04:27:16] INFO kernels.py:115: Dequant+bias kernel loaded
(EngineCore_DP0 pid=63990) [2026-01-23 04:27:16] INFO SlideSparseLinearMethod_INT8.py:437: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=63990) [2026-01-23 04:27:16] INFO SlideSparseLinearMethod_INT8.py:582: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:12, expand_ratio=1.200
(EngineCore_DP0 pid=63990) [2026-01-23 04:27:16] INFO SlideSparseLinearMethod_INT8.py:587: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=63990) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=63990) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 10.30it/s]
(EngineCore_DP0 pid=63990) 
(EngineCore_DP0 pid=63990) [2026-01-23 04:27:16] INFO SlideSparseLinearMethod_INT8.py:681: cuSPARSELt INT8 compression: [1152, 1504] -> 1D uint8
(EngineCore_DP0 pid=63990) [2026-01-23 04:27:16] INFO SlideSparseLinearMethod_INT8.py:691: cuSPARSELt INT8 compression done: 1308672 bytes
(EngineCore_DP0 pid=63990) [2026-01-23 04:27:16] INFO SlideSparseLinearMethod_INT8.py:681: cuSPARSELt INT8 compression: [896, 1504] -> 1D uint8
(EngineCore_DP0 pid=63990) [2026-01-23 04:27:16] INFO SlideSparseLinearMethod_INT8.py:691: cuSPARSELt INT8 compression done: 1017856 bytes
(EngineCore_DP0 pid=63990) [2026-01-23 04:27:16] INFO SlideSparseLinearMethod_INT8.py:681: cuSPARSELt INT8 compression: [9728, 1504] -> 1D uint8
(EngineCore_DP0 pid=63990) [2026-01-23 04:27:16] INFO SlideSparseLinearMethod_INT8.py:691: cuSPARSELt INT8 compression done: 11051008 bytes
(EngineCore_DP0 pid=63990) [2026-01-23 04:27:16] INFO SlideSparseLinearMethod_INT8.py:681: cuSPARSELt INT8 compression: [896, 8128] -> 1D uint8
(EngineCore_DP0 pid=63990) [2026-01-23 04:27:16] INFO SlideSparseLinearMethod_INT8.py:691: cuSPARSELt INT8 compression done: 5476352 bytes
(EngineCore_DP0 pid=63990) 2026-01-23 04:27:49,489 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=63990) 2026-01-23 04:27:49,497 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=63990) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 44.54it/s]
(EngineCore_DP0 pid=63990) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.47it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.46it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 2088.84it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:00, 124.39it/s, est. speed input: 15923.95 toks/s, output: 124.40 toks/s]
Processed prompts:  20%|██        | 26/128 [00:00<00:00, 127.21it/s, est. speed input: 16229.04 toks/s, output: 126.78 toks/s]
Processed prompts:  31%|███▏      | 40/128 [00:00<00:00, 131.15it/s, est. speed input: 16614.11 toks/s, output: 129.79 toks/s]
Processed prompts:  42%|████▏     | 54/128 [00:00<00:00, 133.12it/s, est. speed input: 16816.99 toks/s, output: 131.38 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:00<00:00, 134.23it/s, est. speed input: 16940.46 toks/s, output: 132.35 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:00<00:00, 134.05it/s, est. speed input: 16969.98 toks/s, output: 132.58 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:00<00:00, 132.99it/s, est. speed input: 16937.48 toks/s, output: 132.32 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:00<00:00, 126.70it/s, est. speed input: 16622.00 toks/s, output: 129.86 toks/s]
Processed prompts:  97%|█████████▋| 124/128 [00:00<00:00, 129.11it/s, est. speed input: 16689.56 toks/s, output: 130.39 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 129.11it/s, est. speed input: 16688.34 toks/s, output: 130.38 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 130.36it/s, est. speed input: 16688.34 toks/s, output: 130.38 toks/s]
[rank0]:[W123 04:27:51.690409710 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 45.2s

测试结果:
  Requests/s:   122.49
  Tokens/s:     15801.38
  Total Reqs:   128
  Elapsed:      1.04s

  [Prefill 分析]
  Total Prefill Tokens: 16384
  Prefill Tokens/s:     15678.89

============================================================
[3/3] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-0.5B-INT8                               │
│ Backend:  cuSPARSELt (2:12)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度:
│   M_prefill = 256 (= 1 x 256)
│   M_decode  = 1
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len       = 256
│   --output-len      = 1
│   --num-prompts     = 128
│   --max-num-seqs    = 1
│   --max-model-len   = 385
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
INFO 01-23 04:27:56 [datasets.py:612] Sampling input_len from [256, 256] and output_len from [1, 1]
INFO 01-23 04:27:57 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_12', 'max_model_len': 385, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 385, 'max_num_seqs': 1, 'disable_log_stats': True, 'enable_lora': None, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_12'}
INFO 01-23 04:27:57 [model.py:514] Resolved architecture: Qwen2ForCausalLM
INFO 01-23 04:27:57 [model.py:1661] Using max model len 385
INFO 01-23 04:27:57 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=385.
(EngineCore_DP0 pid=65360) INFO 01-23 04:28:00 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_12', speculative_config=None, tokenizer='/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_12', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=385, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_12, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [385], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 2, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=65360) INFO 01-23 04:28:01 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:47595 backend=nccl
(EngineCore_DP0 pid=65360) INFO 01-23 04:28:01 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=65360) INFO 01-23 04:28:01 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-INT8-SlideSparse-2_12...
(EngineCore_DP0 pid=65360) INFO 01-23 04:28:01 [compressed_tensors_w8a8_int8.py:62] Using CutlassScaledMMLinearKernel for CompressedTensorsW8A8Int8
(EngineCore_DP0 pid=65360) INFO 01-23 04:28:01 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=65360) INFO 01-23 04:28:01 [default_loader.py:308] Loading weights took 0.12 seconds
(EngineCore_DP0 pid=65360) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=65360) INFO 01-23 04:28:02 [gpu_model_runner.py:3659] Model loading took 0.6898 GiB memory and 0.340610 seconds
(EngineCore_DP0 pid=65360) INFO 01-23 04:28:06 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/12d56d7551/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=65360) INFO 01-23 04:28:06 [backends.py:703] Dynamo bytecode transform time: 3.89 s
(EngineCore_DP0 pid=65360) INFO 01-23 04:28:08 [backends.py:261] Cache the graph of compile range (1, 385) for later use
(EngineCore_DP0 pid=65360) INFO 01-23 04:28:33 [backends.py:278] Compiling a graph for compile range (1, 385) takes 25.49 s
(EngineCore_DP0 pid=65360) INFO 01-23 04:28:33 [monitor.py:34] torch.compile takes 29.38 s in total
(EngineCore_DP0 pid=65360) INFO 01-23 04:28:33 [gpu_worker.py:375] Available KV cache memory: 11.31 GiB
(EngineCore_DP0 pid=65360) INFO 01-23 04:28:34 [kv_cache_utils.py:1291] GPU KV cache size: 988,352 tokens
(EngineCore_DP0 pid=65360) INFO 01-23 04:28:34 [kv_cache_utils.py:1296] Maximum concurrency for 385 tokens per request: 2470.88x
(EngineCore_DP0 pid=65360) INFO 01-23 04:28:34 [gpu_model_runner.py:4587] Graph capturing finished in 1 secs, took -0.01 GiB
(EngineCore_DP0 pid=65360) INFO 01-23 04:28:34 [core.py:259] init engine (profile, create kv cache, warmup model) took 32.65 seconds
INFO 01-23 04:28:35 [llm.py:360] Supported tasks: ['generate']
Throughput: 119.92 requests/s, 30819.29 total tokens/s, 119.92 output tokens/s
Total num prompt tokens:  32768
Total num output tokens:  128


─── STDERR ───
[2026-01-23 04:27:57] INFO gemm_wrapper.py:362: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-23 04:27:57] INFO gemm_wrapper.py:408: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-23 04:27:57] INFO gemm_wrapper.py:455: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-23 04:27:57] INFO gemm_wrapper.py:502: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-23 04:28:00] INFO gemm_wrapper.py:362: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-23 04:28:00] INFO gemm_wrapper.py:408: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-23 04:28:00] INFO gemm_wrapper.py:455: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-23 04:28:00] INFO gemm_wrapper.py:502: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=65360) [2026-01-23 04:28:01] INFO SlideSparseLinearMethod_INT8.py:767: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=65360) [2026-01-23 04:28:01] INFO gemm_wrapper.py:307: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=65360) [2026-01-23 04:28:01] INFO kernels.py:329: INT8 quant+slide kernel loaded
(EngineCore_DP0 pid=65360) [2026-01-23 04:28:01] INFO kernels.py:115: Dequant+bias kernel loaded
(EngineCore_DP0 pid=65360) [2026-01-23 04:28:01] INFO SlideSparseLinearMethod_INT8.py:437: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=65360) [2026-01-23 04:28:01] INFO SlideSparseLinearMethod_INT8.py:582: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:12, expand_ratio=1.200
(EngineCore_DP0 pid=65360) [2026-01-23 04:28:01] INFO SlideSparseLinearMethod_INT8.py:587: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=65360) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=65360) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 10.21it/s]
(EngineCore_DP0 pid=65360) 
(EngineCore_DP0 pid=65360) [2026-01-23 04:28:01] INFO SlideSparseLinearMethod_INT8.py:681: cuSPARSELt INT8 compression: [1152, 1504] -> 1D uint8
(EngineCore_DP0 pid=65360) [2026-01-23 04:28:01] INFO SlideSparseLinearMethod_INT8.py:691: cuSPARSELt INT8 compression done: 1308672 bytes
(EngineCore_DP0 pid=65360) [2026-01-23 04:28:01] INFO SlideSparseLinearMethod_INT8.py:681: cuSPARSELt INT8 compression: [896, 1504] -> 1D uint8
(EngineCore_DP0 pid=65360) [2026-01-23 04:28:01] INFO SlideSparseLinearMethod_INT8.py:691: cuSPARSELt INT8 compression done: 1017856 bytes
(EngineCore_DP0 pid=65360) [2026-01-23 04:28:01] INFO SlideSparseLinearMethod_INT8.py:681: cuSPARSELt INT8 compression: [9728, 1504] -> 1D uint8
(EngineCore_DP0 pid=65360) [2026-01-23 04:28:01] INFO SlideSparseLinearMethod_INT8.py:691: cuSPARSELt INT8 compression done: 11051008 bytes
(EngineCore_DP0 pid=65360) [2026-01-23 04:28:01] INFO SlideSparseLinearMethod_INT8.py:681: cuSPARSELt INT8 compression: [896, 8128] -> 1D uint8
(EngineCore_DP0 pid=65360) [2026-01-23 04:28:01] INFO SlideSparseLinearMethod_INT8.py:691: cuSPARSELt INT8 compression done: 5476352 bytes
(EngineCore_DP0 pid=65360) 2026-01-23 04:28:34,008 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=65360) 2026-01-23 04:28:34,016 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=65360) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 45.99it/s]
(EngineCore_DP0 pid=65360) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.50it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.49it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 1497.70it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:00<00:00, 142.13it/s, est. speed input: 36387.67 toks/s, output: 142.13 toks/s]
Processed prompts:  23%|██▎       | 30/128 [00:00<00:00, 130.82it/s, est. speed input: 33894.83 toks/s, output: 132.40 toks/s]
Processed prompts:  34%|███▍      | 44/128 [00:00<00:00, 125.22it/s, est. speed input: 32698.81 toks/s, output: 127.73 toks/s]
Processed prompts:  45%|████▌     | 58/128 [00:00<00:00, 127.79it/s, est. speed input: 32962.63 toks/s, output: 128.76 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:00<00:00, 129.33it/s, est. speed input: 33131.37 toks/s, output: 129.42 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:00<00:00, 128.27it/s, est. speed input: 32997.37 toks/s, output: 128.90 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:00<00:00, 130.26it/s, est. speed input: 33192.04 toks/s, output: 129.66 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:00<00:00, 131.65it/s, est. speed input: 33347.02 toks/s, output: 130.26 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:00<00:00, 132.10it/s, est. speed input: 33425.79 toks/s, output: 130.57 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 132.10it/s, est. speed input: 33436.54 toks/s, output: 130.61 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 130.60it/s, est. speed input: 33436.54 toks/s, output: 130.61 toks/s]
[rank0]:[W123 04:28:36.234741756 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 44.6s

测试结果:
  Requests/s:   119.92
  Tokens/s:     30819.29
  Total Reqs:   128
  Elapsed:      1.07s

  [Prefill 分析]
  Total Prefill Tokens: 32768
  Prefill Tokens/s:     30699.37


------------------------------------------------------------
  生成 CSV: Qwen2.5-0.5B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_12/Qwen2.5-0.5B-INT8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
16,16,1,128,128,123.2792,2095.7462,1.0383
128,128,1,128,128,122.4913,15801.3829,1.0450
256,256,1,128,128,119.9194,30819.2869,1.0674

------------------------------------------------------------

[INFO] 完成: 3 成功, 0 失败


============================================================
  Benchmark 完成!
============================================================


总计: 18 成功, 0 失败
============================================================
