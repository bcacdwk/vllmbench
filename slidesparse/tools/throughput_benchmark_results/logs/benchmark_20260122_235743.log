======================================================================
SlideSparse vLLM Throughput Benchmark Log
Created: 2026-01-22 23:57:43
======================================================================

原始命令:
  throughput_benchmark.py --model qwen2.5-1.5b-fp8 --backend all --sparsity 2_4,2_10 --M quick

命令行参数:
  --model: qwen2.5-1.5b-fp8
  --backend: all
  --sparsity: 2_4,2_10
  --stage: None
  --M: quick
  --N: None
  --inner-32: False
  --eager: False
  --gpu-id: 0
  --gpu-mem: 0.8
  --dry-run: False
  --list-models: False

硬件信息:
  GPU: RTX5080
  Compute Capability: cc120
  VRAM: 15.5 GB
  CUDA: 12.9
  Python: py312

======================================================================


============================================================
  Qwen2.5-1.5B-FP8 | cuBLASLt | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints/Qwen2.5-1.5B-FP8
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cutlass

============================================================
[1/3] 测试 M=16
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-1.5B-FP8
│ Backend:  CUTLASS (vLLM native)
│ 阶段:     prefill
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度:
│   M_prefill = 16 (= 1 x 16)
│   M_decode  = 1
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len       = 16
│   --output-len      = 1
│   --num-prompts     = 128
│   --max-num-seqs    = 1
│   --max-model-len   = 145
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
INFO 01-22 23:57:46 [datasets.py:612] Sampling input_len from [16, 16] and output_len from [1, 1]
INFO 01-22 23:57:46 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints/Qwen2.5-1.5B-FP8', 'max_model_len': 145, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 145, 'max_num_seqs': 1, 'disable_log_stats': True, 'enable_lora': None, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints/Qwen2.5-1.5B-FP8'}
INFO 01-22 23:57:46 [model.py:514] Resolved architecture: Qwen2ForCausalLM
INFO 01-22 23:57:46 [model.py:1661] Using max model len 145
INFO 01-22 23:57:46 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=145.
(EngineCore_DP0 pid=4123821) INFO 01-22 23:57:50 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints/Qwen2.5-1.5B-FP8', speculative_config=None, tokenizer='/root/vllmbench/checkpoints/Qwen2.5-1.5B-FP8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=145, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints/Qwen2.5-1.5B-FP8, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [145], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 2, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=4123821) INFO 01-22 23:57:50 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:57833 backend=nccl
(EngineCore_DP0 pid=4123821) INFO 01-22 23:57:51 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=4123821) INFO 01-22 23:57:51 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints/Qwen2.5-1.5B-FP8...
(EngineCore_DP0 pid=4123821) INFO 01-22 23:57:51 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=4123821) INFO 01-22 23:57:51 [default_loader.py:308] Loading weights took 0.20 seconds
(EngineCore_DP0 pid=4123821) INFO 01-22 23:57:52 [gpu_model_runner.py:3659] Model loading took 1.6685 GiB memory and 0.353024 seconds
(EngineCore_DP0 pid=4123821) INFO 01-22 23:57:56 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/4234aa9e36/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=4123821) INFO 01-22 23:57:56 [backends.py:703] Dynamo bytecode transform time: 4.40 s
(EngineCore_DP0 pid=4123821) INFO 01-22 23:57:58 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 145) from the cache, took 0.381 s
(EngineCore_DP0 pid=4123821) INFO 01-22 23:57:58 [monitor.py:34] torch.compile takes 4.78 s in total
(EngineCore_DP0 pid=4123821) INFO 01-22 23:57:59 [gpu_worker.py:375] Available KV cache memory: 10.60 GiB
(EngineCore_DP0 pid=4123821) INFO 01-22 23:57:59 [kv_cache_utils.py:1291] GPU KV cache size: 397,120 tokens
(EngineCore_DP0 pid=4123821) INFO 01-22 23:57:59 [kv_cache_utils.py:1296] Maximum concurrency for 145 tokens per request: 2482.00x
(EngineCore_DP0 pid=4123821) INFO 01-22 23:58:00 [gpu_model_runner.py:4587] Graph capturing finished in 1 secs, took 0.01 GiB
(EngineCore_DP0 pid=4123821) INFO 01-22 23:58:00 [core.py:259] init engine (profile, create kv cache, warmup model) took 8.11 seconds
INFO 01-22 23:58:00 [llm.py:360] Supported tasks: ['generate']
Throughput: 129.47 requests/s, 2200.91 total tokens/s, 129.47 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  128


─── STDERR ───
[2026-01-22 23:57:46] INFO gemm_wrapper.py:362: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-22 23:57:46] INFO gemm_wrapper.py:408: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-22 23:57:46] INFO gemm_wrapper.py:455: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-22 23:57:46] INFO gemm_wrapper.py:502: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-22 23:57:50] INFO gemm_wrapper.py:362: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-22 23:57:50] INFO gemm_wrapper.py:408: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-22 23:57:50] INFO gemm_wrapper.py:455: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-22 23:57:50] INFO gemm_wrapper.py:502: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=4123821) [2026-01-22 23:57:51] INFO SlideSparseLinearMethod_FP8.py:691: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (CUTLASS)
(EngineCore_DP0 pid=4123821) [2026-01-22 23:57:51] INFO SlideSparseLinearMethod_FP8.py:372: SlideSparseFp8LinearOp initialized (kernel=CUTLASS)
(EngineCore_DP0 pid=4123821) [2026-01-22 23:57:51] INFO SlideSparseLinearMethod_FP8.py:517: SlideSparseFp8LinearMethod initialized, kernel=CUTLASS
(EngineCore_DP0 pid=4123821) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=4123821) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.71it/s]
(EngineCore_DP0 pid=4123821) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.71it/s]
(EngineCore_DP0 pid=4123821) 
(EngineCore_DP0 pid=4123821) 2026-01-22 23:57:59,726 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=4123821) 2026-01-22 23:57:59,731 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=4123821) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 71.44it/s]
(EngineCore_DP0 pid=4123821) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  8.11it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  8.11it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  62%|██████▎   | 80/128 [00:00<00:00, 468.34it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 722.50it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  25%|██▌       | 32/128 [00:00<00:00, 304.44it/s, est. speed input: 4871.54 toks/s, output: 304.45 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:00<00:00, 176.97it/s, est. speed input: 3024.52 toks/s, output: 189.03 toks/s]
Processed prompts:  66%|██████▌   | 84/128 [00:00<00:00, 160.09it/s, est. speed input: 2761.37 toks/s, output: 172.58 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:00<00:00, 151.72it/s, est. speed input: 2634.62 toks/s, output: 164.66 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:00<00:00, 147.19it/s, est. speed input: 2563.44 toks/s, output: 160.21 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 147.19it/s, est. speed input: 2529.44 toks/s, output: 158.09 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 158.08it/s, est. speed input: 2529.44 toks/s, output: 158.09 toks/s]
[rank0]:[W122 23:58:01.729710849 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 20.0s

测试结果:
  Requests/s:   129.47
  Tokens/s:     2200.91
  Total Reqs:   128
  Elapsed:      0.99s

  [Prefill 分析]
  Total Prefill Tokens: 2048
  Prefill Tokens/s:     2071.44

============================================================
[2/3] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-1.5B-FP8
│ Backend:  CUTLASS (vLLM native)
│ 阶段:     prefill
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度:
│   M_prefill = 128 (= 1 x 128)
│   M_decode  = 1
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len       = 128
│   --output-len      = 1
│   --num-prompts     = 128
│   --max-num-seqs    = 1
│   --max-model-len   = 257
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
INFO 01-22 23:58:06 [datasets.py:612] Sampling input_len from [128, 128] and output_len from [1, 1]
INFO 01-22 23:58:06 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints/Qwen2.5-1.5B-FP8', 'max_model_len': 257, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 257, 'max_num_seqs': 1, 'disable_log_stats': True, 'enable_lora': None, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints/Qwen2.5-1.5B-FP8'}
INFO 01-22 23:58:06 [model.py:514] Resolved architecture: Qwen2ForCausalLM
INFO 01-22 23:58:06 [model.py:1661] Using max model len 257
INFO 01-22 23:58:06 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=257.
(EngineCore_DP0 pid=4124203) INFO 01-22 23:58:10 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints/Qwen2.5-1.5B-FP8', speculative_config=None, tokenizer='/root/vllmbench/checkpoints/Qwen2.5-1.5B-FP8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=257, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints/Qwen2.5-1.5B-FP8, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [257], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 2, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=4124203) INFO 01-22 23:58:10 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:56719 backend=nccl
(EngineCore_DP0 pid=4124203) INFO 01-22 23:58:10 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=4124203) INFO 01-22 23:58:11 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints/Qwen2.5-1.5B-FP8...
(EngineCore_DP0 pid=4124203) INFO 01-22 23:58:11 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=4124203) INFO 01-22 23:58:11 [default_loader.py:308] Loading weights took 0.20 seconds
(EngineCore_DP0 pid=4124203) INFO 01-22 23:58:11 [gpu_model_runner.py:3659] Model loading took 1.6685 GiB memory and 0.352581 seconds
(EngineCore_DP0 pid=4124203) INFO 01-22 23:58:16 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/5b35444d51/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=4124203) INFO 01-22 23:58:16 [backends.py:703] Dynamo bytecode transform time: 4.37 s
(EngineCore_DP0 pid=4124203) INFO 01-22 23:58:18 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 257) from the cache, took 0.427 s
(EngineCore_DP0 pid=4124203) INFO 01-22 23:58:18 [monitor.py:34] torch.compile takes 4.80 s in total
(EngineCore_DP0 pid=4124203) INFO 01-22 23:58:19 [gpu_worker.py:375] Available KV cache memory: 10.60 GiB
(EngineCore_DP0 pid=4124203) INFO 01-22 23:58:19 [kv_cache_utils.py:1291] GPU KV cache size: 397,104 tokens
(EngineCore_DP0 pid=4124203) INFO 01-22 23:58:19 [kv_cache_utils.py:1296] Maximum concurrency for 257 tokens per request: 1459.94x
(EngineCore_DP0 pid=4124203) INFO 01-22 23:58:19 [gpu_model_runner.py:4587] Graph capturing finished in 1 secs, took 0.01 GiB
(EngineCore_DP0 pid=4124203) INFO 01-22 23:58:19 [core.py:259] init engine (profile, create kv cache, warmup model) took 8.08 seconds
INFO 01-22 23:58:20 [llm.py:360] Supported tasks: ['generate']
Throughput: 127.52 requests/s, 16449.51 total tokens/s, 127.52 output tokens/s
Total num prompt tokens:  16384
Total num output tokens:  128


─── STDERR ───
[2026-01-22 23:58:06] INFO gemm_wrapper.py:362: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-22 23:58:06] INFO gemm_wrapper.py:408: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-22 23:58:06] INFO gemm_wrapper.py:455: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-22 23:58:06] INFO gemm_wrapper.py:502: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-22 23:58:10] INFO gemm_wrapper.py:362: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-22 23:58:10] INFO gemm_wrapper.py:408: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-22 23:58:10] INFO gemm_wrapper.py:455: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-22 23:58:10] INFO gemm_wrapper.py:502: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=4124203) [2026-01-22 23:58:11] INFO SlideSparseLinearMethod_FP8.py:691: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (CUTLASS)
(EngineCore_DP0 pid=4124203) [2026-01-22 23:58:11] INFO SlideSparseLinearMethod_FP8.py:372: SlideSparseFp8LinearOp initialized (kernel=CUTLASS)
(EngineCore_DP0 pid=4124203) [2026-01-22 23:58:11] INFO SlideSparseLinearMethod_FP8.py:517: SlideSparseFp8LinearMethod initialized, kernel=CUTLASS
(EngineCore_DP0 pid=4124203) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=4124203) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.70it/s]
(EngineCore_DP0 pid=4124203) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.70it/s]
(EngineCore_DP0 pid=4124203) 
(EngineCore_DP0 pid=4124203) 2026-01-22 23:58:19,419 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=4124203) 2026-01-22 23:58:19,424 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=4124203) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 75.35it/s]
(EngineCore_DP0 pid=4124203) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  9.11it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  9.10it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  62%|██████▎   | 80/128 [00:00<00:00, 455.49it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 658.74it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  27%|██▋       | 34/128 [00:00<00:00, 325.87it/s, est. speed input: 41712.62 toks/s, output: 325.87 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:00<00:00, 176.57it/s, est. speed input: 24296.03 toks/s, output: 189.81 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:00<00:00, 158.56it/s, est. speed input: 22021.83 toks/s, output: 172.04 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:00<00:00, 150.58it/s, est. speed input: 21029.20 toks/s, output: 164.29 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:00<00:00, 145.77it/s, est. speed input: 20434.87 toks/s, output: 159.65 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 145.77it/s, est. speed input: 20287.55 toks/s, output: 158.50 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 158.48it/s, est. speed input: 20287.55 toks/s, output: 158.50 toks/s]
[rank0]:[W122 23:58:21.395680184 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 19.7s

测试结果:
  Requests/s:   127.52
  Tokens/s:     16449.51
  Total Reqs:   128
  Elapsed:      1.00s

  [Prefill 分析]
  Total Prefill Tokens: 16384
  Prefill Tokens/s:     16321.99

============================================================
[3/3] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-1.5B-FP8
│ Backend:  CUTLASS (vLLM native)
│ 阶段:     prefill
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度:
│   M_prefill = 256 (= 1 x 256)
│   M_decode  = 1
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len       = 256
│   --output-len      = 1
│   --num-prompts     = 128
│   --max-num-seqs    = 1
│   --max-model-len   = 385
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
INFO 01-22 23:58:26 [datasets.py:612] Sampling input_len from [256, 256] and output_len from [1, 1]
INFO 01-22 23:58:26 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints/Qwen2.5-1.5B-FP8', 'max_model_len': 385, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 385, 'max_num_seqs': 1, 'disable_log_stats': True, 'enable_lora': None, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints/Qwen2.5-1.5B-FP8'}
INFO 01-22 23:58:26 [model.py:514] Resolved architecture: Qwen2ForCausalLM
INFO 01-22 23:58:26 [model.py:1661] Using max model len 385
INFO 01-22 23:58:26 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=385.
(EngineCore_DP0 pid=4124608) INFO 01-22 23:58:30 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints/Qwen2.5-1.5B-FP8', speculative_config=None, tokenizer='/root/vllmbench/checkpoints/Qwen2.5-1.5B-FP8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=385, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints/Qwen2.5-1.5B-FP8, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [385], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 2, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=4124608) INFO 01-22 23:58:30 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:46843 backend=nccl
(EngineCore_DP0 pid=4124608) INFO 01-22 23:58:30 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=4124608) INFO 01-22 23:58:31 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints/Qwen2.5-1.5B-FP8...
(EngineCore_DP0 pid=4124608) INFO 01-22 23:58:31 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=4124608) INFO 01-22 23:58:31 [default_loader.py:308] Loading weights took 0.20 seconds
(EngineCore_DP0 pid=4124608) INFO 01-22 23:58:31 [gpu_model_runner.py:3659] Model loading took 1.6690 GiB memory and 0.352177 seconds
(EngineCore_DP0 pid=4124608) INFO 01-22 23:58:36 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/40821f0644/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=4124608) INFO 01-22 23:58:36 [backends.py:703] Dynamo bytecode transform time: 4.42 s
(EngineCore_DP0 pid=4124608) INFO 01-22 23:58:38 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 385) from the cache, took 0.339 s
(EngineCore_DP0 pid=4124608) INFO 01-22 23:58:38 [monitor.py:34] torch.compile takes 4.76 s in total
(EngineCore_DP0 pid=4124608) INFO 01-22 23:58:39 [gpu_worker.py:375] Available KV cache memory: 10.60 GiB
(EngineCore_DP0 pid=4124608) INFO 01-22 23:58:39 [kv_cache_utils.py:1291] GPU KV cache size: 396,848 tokens
(EngineCore_DP0 pid=4124608) INFO 01-22 23:58:39 [kv_cache_utils.py:1296] Maximum concurrency for 385 tokens per request: 992.12x
(EngineCore_DP0 pid=4124608) INFO 01-22 23:58:39 [gpu_model_runner.py:4587] Graph capturing finished in 1 secs, took -0.00 GiB
(EngineCore_DP0 pid=4124608) INFO 01-22 23:58:39 [core.py:259] init engine (profile, create kv cache, warmup model) took 8.09 seconds
INFO 01-22 23:58:40 [llm.py:360] Supported tasks: ['generate']
Throughput: 108.88 requests/s, 27982.42 total tokens/s, 108.88 output tokens/s
Total num prompt tokens:  32768
Total num output tokens:  128


─── STDERR ───
[2026-01-22 23:58:26] INFO gemm_wrapper.py:362: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-22 23:58:26] INFO gemm_wrapper.py:408: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-22 23:58:26] INFO gemm_wrapper.py:455: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-22 23:58:26] INFO gemm_wrapper.py:502: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-22 23:58:30] INFO gemm_wrapper.py:362: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-22 23:58:30] INFO gemm_wrapper.py:408: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-22 23:58:30] INFO gemm_wrapper.py:455: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-22 23:58:30] INFO gemm_wrapper.py:502: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=4124608) [2026-01-22 23:58:31] INFO SlideSparseLinearMethod_FP8.py:691: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (CUTLASS)
(EngineCore_DP0 pid=4124608) [2026-01-22 23:58:31] INFO SlideSparseLinearMethod_FP8.py:372: SlideSparseFp8LinearOp initialized (kernel=CUTLASS)
(EngineCore_DP0 pid=4124608) [2026-01-22 23:58:31] INFO SlideSparseLinearMethod_FP8.py:517: SlideSparseFp8LinearMethod initialized, kernel=CUTLASS
(EngineCore_DP0 pid=4124608) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=4124608) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.69it/s]
(EngineCore_DP0 pid=4124608) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.69it/s]
(EngineCore_DP0 pid=4124608) 
(EngineCore_DP0 pid=4124608) 2026-01-22 23:58:39,316 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=4124608) 2026-01-22 23:58:39,320 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=4124608) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 25.66it/s]
(EngineCore_DP0 pid=4124608) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  9.00it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  8.99it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 1523.09it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:00, 158.22it/s, est. speed input: 40508.13 toks/s, output: 158.22 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:00<00:00, 128.42it/s, est. speed input: 33862.08 toks/s, output: 132.27 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:00<00:00, 121.29it/s, est. speed input: 32185.07 toks/s, output: 125.72 toks/s]
Processed prompts:  47%|████▋     | 60/128 [00:00<00:00, 118.04it/s, est. speed input: 31392.33 toks/s, output: 122.62 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:00<00:00, 116.47it/s, est. speed input: 30964.96 toks/s, output: 120.96 toks/s]
Processed prompts:  66%|██████▌   | 84/128 [00:00<00:00, 115.38it/s, est. speed input: 30658.80 toks/s, output: 119.76 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:00<00:00, 114.75it/s, est. speed input: 30444.30 toks/s, output: 118.92 toks/s]
Processed prompts:  84%|████████▍ | 108/128 [00:00<00:00, 114.29it/s, est. speed input: 30276.37 toks/s, output: 118.27 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:01<00:00, 113.96it/s, est. speed input: 30141.86 toks/s, output: 117.74 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 113.96it/s, est. speed input: 30069.14 toks/s, output: 117.46 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 117.45it/s, est. speed input: 30069.14 toks/s, output: 117.46 toks/s]
[rank0]:[W122 23:58:41.503212761 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 20.3s

测试结果:
  Requests/s:   108.88
  Tokens/s:     27982.42
  Total Reqs:   128
  Elapsed:      1.18s

  [Prefill 分析]
  Total Prefill Tokens: 32768
  Prefill Tokens/s:     27873.54


------------------------------------------------------------
  生成 CSV: Qwen2.5-1.5B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cutlass/Qwen2.5-1.5B-FP8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
16,16,1,128,128,129.4653,2200.9099,0.9887
128,128,1,128,128,127.5156,16449.5097,1.0038
256,256,1,128,128,108.8810,27982.4242,1.1756

------------------------------------------------------------

[INFO] 完成: 3 成功, 0 失败

============================================================
  Qwen2.5-1.5B-FP8 | cuBLASLt | decode
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints/Qwen2.5-1.5B-FP8
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cutlass

============================================================
[1/3] 测试 M=16
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-1.5B-FP8
│ Backend:  CUTLASS (vLLM native)
│ 阶段:     decode
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度:
│   M_prefill = 256 (= 16 x 16)
│   M_decode  = 16
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len       = 16
│   --output-len      = 256
│   --num-prompts     = 16
│   --max-num-seqs    = 16
│   --max-model-len   = 400
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
INFO 01-22 23:58:46 [datasets.py:612] Sampling input_len from [16, 16] and output_len from [256, 256]
INFO 01-22 23:58:46 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints/Qwen2.5-1.5B-FP8', 'max_model_len': 400, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 6400, 'max_num_seqs': 16, 'disable_log_stats': True, 'enable_lora': None, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints/Qwen2.5-1.5B-FP8'}
INFO 01-22 23:58:46 [model.py:514] Resolved architecture: Qwen2ForCausalLM
INFO 01-22 23:58:46 [model.py:1661] Using max model len 400
INFO 01-22 23:58:46 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=6400.
(EngineCore_DP0 pid=4125001) INFO 01-22 23:58:50 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints/Qwen2.5-1.5B-FP8', speculative_config=None, tokenizer='/root/vllmbench/checkpoints/Qwen2.5-1.5B-FP8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=400, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints/Qwen2.5-1.5B-FP8, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [6400], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 32, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=4125001) INFO 01-22 23:58:50 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:50263 backend=nccl
(EngineCore_DP0 pid=4125001) INFO 01-22 23:58:50 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=4125001) INFO 01-22 23:58:51 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints/Qwen2.5-1.5B-FP8...
(EngineCore_DP0 pid=4125001) INFO 01-22 23:58:51 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=4125001) INFO 01-22 23:58:51 [default_loader.py:308] Loading weights took 0.21 seconds
(EngineCore_DP0 pid=4125001) INFO 01-22 23:58:52 [gpu_model_runner.py:3659] Model loading took 1.6685 GiB memory and 0.660376 seconds
(EngineCore_DP0 pid=4125001) INFO 01-22 23:58:56 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/e8ba5fea07/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=4125001) INFO 01-22 23:58:56 [backends.py:703] Dynamo bytecode transform time: 4.40 s
(EngineCore_DP0 pid=4125001) INFO 01-22 23:58:59 [backends.py:261] Cache the graph of compile range (1, 6400) for later use
(EngineCore_DP0 pid=4125001) INFO 01-22 23:59:03 [backends.py:278] Compiling a graph for compile range (1, 6400) takes 4.79 s
(EngineCore_DP0 pid=4125001) INFO 01-22 23:59:03 [monitor.py:34] torch.compile takes 9.19 s in total
(EngineCore_DP0 pid=4125001) INFO 01-22 23:59:04 [gpu_worker.py:375] Available KV cache memory: 10.14 GiB
(EngineCore_DP0 pid=4125001) INFO 01-22 23:59:04 [kv_cache_utils.py:1291] GPU KV cache size: 379,648 tokens
(EngineCore_DP0 pid=4125001) INFO 01-22 23:59:04 [kv_cache_utils.py:1296] Maximum concurrency for 400 tokens per request: 949.12x
(EngineCore_DP0 pid=4125001) INFO 01-22 23:59:05 [gpu_model_runner.py:4587] Graph capturing finished in 1 secs, took -0.35 GiB
(EngineCore_DP0 pid=4125001) INFO 01-22 23:59:05 [core.py:259] init engine (profile, create kv cache, warmup model) took 13.08 seconds
INFO 01-22 23:59:05 [llm.py:360] Supported tasks: ['generate']
Throughput: 8.78 requests/s, 2388.78 total tokens/s, 2248.26 output tokens/s
Total num prompt tokens:  256
Total num output tokens:  4096


─── STDERR ───
[2026-01-22 23:58:46] INFO gemm_wrapper.py:362: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-22 23:58:46] INFO gemm_wrapper.py:408: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-22 23:58:46] INFO gemm_wrapper.py:455: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-22 23:58:46] INFO gemm_wrapper.py:502: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-22 23:58:50] INFO gemm_wrapper.py:362: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-22 23:58:50] INFO gemm_wrapper.py:408: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-22 23:58:50] INFO gemm_wrapper.py:455: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-22 23:58:50] INFO gemm_wrapper.py:502: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=4125001) [2026-01-22 23:58:51] INFO SlideSparseLinearMethod_FP8.py:691: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (CUTLASS)
(EngineCore_DP0 pid=4125001) [2026-01-22 23:58:51] INFO SlideSparseLinearMethod_FP8.py:372: SlideSparseFp8LinearOp initialized (kernel=CUTLASS)
(EngineCore_DP0 pid=4125001) [2026-01-22 23:58:51] INFO SlideSparseLinearMethod_FP8.py:517: SlideSparseFp8LinearMethod initialized, kernel=CUTLASS
(EngineCore_DP0 pid=4125001) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=4125001) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.38it/s]
(EngineCore_DP0 pid=4125001) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.38it/s]
(EngineCore_DP0 pid=4125001) 
(EngineCore_DP0 pid=4125001) 2026-01-22 23:59:04,430 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=4125001) 2026-01-22 23:59:04,434 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=4125001) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 69.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 69.31it/s]
(EngineCore_DP0 pid=4125001) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 1/5 [00:00<00:00,  9.05it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 31.87it/s]

Adding requests:   0%|          | 0/16 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 16/16 [00:00<00:00, 1496.26it/s]

Processed prompts:   0%|          | 0/16 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   6%|▋         | 1/16 [00:01<00:27,  1.80s/it, est. speed input: 8.88 toks/s, output: 142.08 toks/s]
Processed prompts: 100%|██████████| 16/16 [00:01<00:00,  1.80s/it, est. speed input: 141.51 toks/s, output: 2264.08 toks/s]
Processed prompts: 100%|██████████| 16/16 [00:01<00:00,  8.84it/s, est. speed input: 141.51 toks/s, output: 2264.08 toks/s]
[rank0]:[W122 23:59:07.461870128 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 25.9s

测试结果:
  Requests/s:   8.78
  Tokens/s:     2388.78
  Total Reqs:   16
  Elapsed:      1.82s

  [Decode 分析]
  Total Decode Tokens:  4096
  Decode Tokens/s:      2248.26

============================================================
[2/3] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-1.5B-FP8
│ Backend:  CUTLASS (vLLM native)
│ 阶段:     decode
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度:
│   M_prefill = 2048 (= 128 x 16)
│   M_decode  = 128
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len       = 16
│   --output-len      = 256
│   --num-prompts     = 128
│   --max-num-seqs    = 128
│   --max-model-len   = 400
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
INFO 01-22 23:59:12 [datasets.py:612] Sampling input_len from [16, 16] and output_len from [256, 256]
INFO 01-22 23:59:12 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints/Qwen2.5-1.5B-FP8', 'max_model_len': 400, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 51200, 'max_num_seqs': 128, 'disable_log_stats': True, 'enable_lora': None, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints/Qwen2.5-1.5B-FP8'}
INFO 01-22 23:59:12 [model.py:514] Resolved architecture: Qwen2ForCausalLM
INFO 01-22 23:59:12 [model.py:1661] Using max model len 400
INFO 01-22 23:59:12 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=51200.
(EngineCore_DP0 pid=4125476) INFO 01-22 23:59:16 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints/Qwen2.5-1.5B-FP8', speculative_config=None, tokenizer='/root/vllmbench/checkpoints/Qwen2.5-1.5B-FP8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=400, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints/Qwen2.5-1.5B-FP8, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [51200], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 256, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=4125476) INFO 01-22 23:59:16 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:51481 backend=nccl
(EngineCore_DP0 pid=4125476) INFO 01-22 23:59:16 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=4125476) INFO 01-22 23:59:16 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints/Qwen2.5-1.5B-FP8...
(EngineCore_DP0 pid=4125476) INFO 01-22 23:59:17 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=4125476) INFO 01-22 23:59:17 [default_loader.py:308] Loading weights took 0.21 seconds
(EngineCore_DP0 pid=4125476) INFO 01-22 23:59:17 [gpu_model_runner.py:3659] Model loading took 1.6685 GiB memory and 0.368265 seconds
(EngineCore_DP0 pid=4125476) INFO 01-22 23:59:22 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/43cfadf3f6/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=4125476) INFO 01-22 23:59:22 [backends.py:703] Dynamo bytecode transform time: 4.39 s
(EngineCore_DP0 pid=4125476) INFO 01-22 23:59:26 [backends.py:261] Cache the graph of compile range (1, 51200) for later use
(EngineCore_DP0 pid=4125476) INFO 01-22 23:59:30 [backends.py:278] Compiling a graph for compile range (1, 51200) takes 6.78 s
(EngineCore_DP0 pid=4125476) INFO 01-22 23:59:30 [monitor.py:34] torch.compile takes 11.17 s in total
(EngineCore_DP0 pid=4125476) INFO 01-22 23:59:31 [gpu_worker.py:375] Available KV cache memory: 7.04 GiB
(EngineCore_DP0 pid=4125476) INFO 01-22 23:59:32 [kv_cache_utils.py:1291] GPU KV cache size: 263,680 tokens
(EngineCore_DP0 pid=4125476) INFO 01-22 23:59:32 [kv_cache_utils.py:1296] Maximum concurrency for 400 tokens per request: 659.20x
(EngineCore_DP0 pid=4125476) INFO 01-22 23:59:34 [gpu_model_runner.py:4587] Graph capturing finished in 2 secs, took -3.09 GiB
(EngineCore_DP0 pid=4125476) INFO 01-22 23:59:34 [core.py:259] init engine (profile, create kv cache, warmup model) took 16.68 seconds
INFO 01-22 23:59:35 [llm.py:360] Supported tasks: ['generate']
Throughput: 50.22 requests/s, 13659.96 total tokens/s, 12856.44 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768


─── STDERR ───
[2026-01-22 23:59:12] INFO gemm_wrapper.py:362: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-22 23:59:12] INFO gemm_wrapper.py:408: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-22 23:59:12] INFO gemm_wrapper.py:455: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-22 23:59:12] INFO gemm_wrapper.py:502: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-22 23:59:16] INFO gemm_wrapper.py:362: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-22 23:59:16] INFO gemm_wrapper.py:408: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-22 23:59:16] INFO gemm_wrapper.py:455: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-22 23:59:16] INFO gemm_wrapper.py:502: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=4125476) [2026-01-22 23:59:17] INFO SlideSparseLinearMethod_FP8.py:691: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (CUTLASS)
(EngineCore_DP0 pid=4125476) [2026-01-22 23:59:17] INFO SlideSparseLinearMethod_FP8.py:372: SlideSparseFp8LinearOp initialized (kernel=CUTLASS)
(EngineCore_DP0 pid=4125476) [2026-01-22 23:59:17] INFO SlideSparseLinearMethod_FP8.py:517: SlideSparseFp8LinearMethod initialized, kernel=CUTLASS
(EngineCore_DP0 pid=4125476) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=4125476) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.25it/s]
(EngineCore_DP0 pid=4125476) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.24it/s]
(EngineCore_DP0 pid=4125476) 
(EngineCore_DP0 pid=4125476) [rank0]:W0122 23:59:25.042000 4125476 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=4125476) [rank0]:W0122 23:59:25.686000 4125476 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=4125476) [rank0]:W0122 23:59:27.310000 4125476 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=4125476) [rank0]:W0122 23:59:27.403000 4125476 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=4125476) 2026-01-22 23:59:32,174 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=4125476) 2026-01-22 23:59:32,179 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=4125476) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|▎         | 1/35 [00:00<00:17,  1.96it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  23%|██▎       | 8/35 [00:00<00:01, 16.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  34%|███▍      | 12/35 [00:00<00:01, 20.00it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  46%|████▌     | 16/35 [00:00<00:00, 19.70it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  54%|█████▍    | 19/35 [00:01<00:00, 19.27it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 25/35 [00:01<00:00, 27.02it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 33/35 [00:01<00:00, 37.99it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:01<00:00, 25.08it/s]
(EngineCore_DP0 pid=4125476) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   5%|▌         | 1/19 [00:00<00:02,  8.86it/s]
Capturing CUDA graphs (decode, FULL):  47%|████▋     | 9/19 [00:00<00:00, 48.19it/s]
Capturing CUDA graphs (decode, FULL):  95%|█████████▍| 18/19 [00:00<00:00, 64.47it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:00<00:00, 56.92it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  62%|██████▎   | 80/128 [00:00<00:00, 524.57it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 803.43it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:02<04:47,  2.27s/it, est. speed input: 7.06 toks/s, output: 112.90 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:02<00:01, 46.88it/s, est. speed input: 537.73 toks/s, output: 8603.54 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 46.88it/s, est. speed input: 857.76 toks/s, output: 13724.07 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 53.61it/s, est. speed input: 857.76 toks/s, output: 13724.07 toks/s]
[rank0]:[W122 23:59:37.399978676 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 29.8s

测试结果:
  Requests/s:   50.22
  Tokens/s:     13659.96
  Total Reqs:   128
  Elapsed:      2.55s

  [Decode 分析]
  Total Decode Tokens:  32768
  Decode Tokens/s:      12856.44

============================================================
[3/3] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-1.5B-FP8
│ Backend:  CUTLASS (vLLM native)
│ 阶段:     decode
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度:
│   M_prefill = 4096 (= 256 x 16)
│   M_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len       = 16
│   --output-len      = 256
│   --num-prompts     = 256
│   --max-num-seqs    = 256
│   --max-model-len   = 400
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
INFO 01-22 23:59:42 [datasets.py:612] Sampling input_len from [16, 16] and output_len from [256, 256]
INFO 01-22 23:59:42 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints/Qwen2.5-1.5B-FP8', 'max_model_len': 400, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 102400, 'max_num_seqs': 256, 'disable_log_stats': True, 'enable_lora': None, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints/Qwen2.5-1.5B-FP8'}
INFO 01-22 23:59:42 [model.py:514] Resolved architecture: Qwen2ForCausalLM
INFO 01-22 23:59:42 [model.py:1661] Using max model len 400
INFO 01-22 23:59:42 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=102400.
(EngineCore_DP0 pid=4126113) INFO 01-22 23:59:46 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints/Qwen2.5-1.5B-FP8', speculative_config=None, tokenizer='/root/vllmbench/checkpoints/Qwen2.5-1.5B-FP8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=400, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints/Qwen2.5-1.5B-FP8, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [102400], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=4126113) INFO 01-22 23:59:46 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:44641 backend=nccl
(EngineCore_DP0 pid=4126113) INFO 01-22 23:59:46 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=4126113) INFO 01-22 23:59:47 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints/Qwen2.5-1.5B-FP8...
(EngineCore_DP0 pid=4126113) INFO 01-22 23:59:47 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=4126113) INFO 01-22 23:59:47 [default_loader.py:308] Loading weights took 0.20 seconds
(EngineCore_DP0 pid=4126113) INFO 01-22 23:59:47 [gpu_model_runner.py:3659] Model loading took 1.6685 GiB memory and 0.357168 seconds
(EngineCore_DP0 pid=4126113) INFO 01-22 23:59:52 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/1f24ca00cd/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=4126113) INFO 01-22 23:59:52 [backends.py:703] Dynamo bytecode transform time: 4.41 s
(EngineCore_DP0 pid=4126113) INFO 01-22 23:59:55 [backends.py:261] Cache the graph of compile range (1, 102400) for later use
(EngineCore_DP0 pid=4126113) INFO 01-23 00:00:01 [backends.py:278] Compiling a graph for compile range (1, 102400) takes 6.94 s
(EngineCore_DP0 pid=4126113) INFO 01-23 00:00:01 [monitor.py:34] torch.compile takes 11.35 s in total
(EngineCore_DP0 pid=4126113) INFO 01-23 00:00:02 [gpu_worker.py:375] Available KV cache memory: 3.46 GiB
(EngineCore_DP0 pid=4126113) INFO 01-23 00:00:02 [kv_cache_utils.py:1291] GPU KV cache size: 129,744 tokens
(EngineCore_DP0 pid=4126113) INFO 01-23 00:00:02 [kv_cache_utils.py:1296] Maximum concurrency for 400 tokens per request: 324.36x
(EngineCore_DP0 pid=4126113) INFO 01-23 00:00:05 [gpu_model_runner.py:4587] Graph capturing finished in 3 secs, took -6.26 GiB
(EngineCore_DP0 pid=4126113) INFO 01-23 00:00:05 [core.py:259] init engine (profile, create kv cache, warmup model) took 17.93 seconds
INFO 01-23 00:00:06 [llm.py:360] Supported tasks: ['generate']
Throughput: 71.91 requests/s, 19559.57 total tokens/s, 18409.01 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536


─── STDERR ───
[2026-01-22 23:59:42] INFO gemm_wrapper.py:362: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-22 23:59:42] INFO gemm_wrapper.py:408: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-22 23:59:42] INFO gemm_wrapper.py:455: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-22 23:59:42] INFO gemm_wrapper.py:502: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-22 23:59:46] INFO gemm_wrapper.py:362: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-22 23:59:46] INFO gemm_wrapper.py:408: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-22 23:59:46] INFO gemm_wrapper.py:455: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-22 23:59:46] INFO gemm_wrapper.py:502: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=4126113) [2026-01-22 23:59:47] INFO SlideSparseLinearMethod_FP8.py:691: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (CUTLASS)
(EngineCore_DP0 pid=4126113) [2026-01-22 23:59:47] INFO SlideSparseLinearMethod_FP8.py:372: SlideSparseFp8LinearOp initialized (kernel=CUTLASS)
(EngineCore_DP0 pid=4126113) [2026-01-22 23:59:47] INFO SlideSparseLinearMethod_FP8.py:517: SlideSparseFp8LinearMethod initialized, kernel=CUTLASS
(EngineCore_DP0 pid=4126113) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=4126113) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.63it/s]
(EngineCore_DP0 pid=4126113) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.63it/s]
(EngineCore_DP0 pid=4126113) 
(EngineCore_DP0 pid=4126113) [rank0]:W0122 23:59:55.172000 4126113 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=4126113) [rank0]:W0122 23:59:55.477000 4126113 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=4126113) [rank0]:W0122 23:59:56.760000 4126113 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=4126113) [rank0]:W0122 23:59:56.853000 4126113 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=4126113) 2026-01-23 00:00:02,690 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=4126113) 2026-01-23 00:00:02,695 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=4126113) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   2%|▏         | 1/51 [00:01<01:01,  1.23s/it]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▎        | 7/51 [00:01<00:06,  6.93it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 14/51 [00:01<00:02, 14.87it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|████      | 21/51 [00:01<00:01, 23.14it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 28/51 [00:01<00:00, 31.07it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 35/51 [00:01<00:00, 38.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 43/51 [00:01<00:00, 46.58it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:01<00:00, 52.06it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:01<00:00, 25.57it/s]
(EngineCore_DP0 pid=4126113) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   3%|▎         | 1/35 [00:00<00:03,  8.90it/s]
Capturing CUDA graphs (decode, FULL):  26%|██▌       | 9/35 [00:00<00:00, 45.28it/s]
Capturing CUDA graphs (decode, FULL):  49%|████▊     | 17/35 [00:00<00:00, 57.78it/s]
Capturing CUDA graphs (decode, FULL):  74%|███████▍  | 26/35 [00:00<00:00, 66.75it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:00<00:00, 72.44it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:00<00:00, 62.96it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 7447.13it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:03<14:54,  3.51s/it, est. speed input: 4.56 toks/s, output: 72.99 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00,  3.51s/it, est. speed input: 1162.00 toks/s, output: 18591.99 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 72.62it/s, est. speed input: 1162.00 toks/s, output: 18591.99 toks/s]
[rank0]:[W123 00:00:09.747114920 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 32.4s

测试结果:
  Requests/s:   71.91
  Tokens/s:     19559.57
  Total Reqs:   256
  Elapsed:      3.56s

  [Decode 分析]
  Total Decode Tokens:  65536
  Decode Tokens/s:      18409.01


------------------------------------------------------------
  生成 CSV: Qwen2.5-1.5B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cutlass/Qwen2.5-1.5B-FP8_decode.csv

预览:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
16,16,16,16,256,256,8.7823,2388.7775,1.8219
128,16,128,128,256,256,50.2205,13659.9642,2.5488
256,16,256,256,256,256,71.9102,19559.5745,3.5600

------------------------------------------------------------

[INFO] 完成: 3 成功, 0 失败

============================================================
  Qwen2.5-1.5B-FP8 | cuBLASLt | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints/Qwen2.5-1.5B-FP8
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cublaslt

============================================================
[1/3] 测试 M=16
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-1.5B-FP8
│ Backend:  cuBLASLt
│ 阶段:     prefill
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度:
│   M_prefill = 16 (= 1 x 16)
│   M_decode  = 1
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len       = 16
│   --output-len      = 1
│   --num-prompts     = 128
│   --max-num-seqs    = 1
│   --max-model-len   = 145
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
INFO 01-23 00:00:14 [datasets.py:612] Sampling input_len from [16, 16] and output_len from [1, 1]
INFO 01-23 00:00:14 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints/Qwen2.5-1.5B-FP8', 'max_model_len': 145, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 145, 'max_num_seqs': 1, 'disable_log_stats': True, 'enable_lora': None, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints/Qwen2.5-1.5B-FP8'}
INFO 01-23 00:00:14 [model.py:514] Resolved architecture: Qwen2ForCausalLM
INFO 01-23 00:00:14 [model.py:1661] Using max model len 145
INFO 01-23 00:00:15 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=145.
(EngineCore_DP0 pid=4126551) INFO 01-23 00:00:18 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints/Qwen2.5-1.5B-FP8', speculative_config=None, tokenizer='/root/vllmbench/checkpoints/Qwen2.5-1.5B-FP8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=145, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints/Qwen2.5-1.5B-FP8, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [145], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 2, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=4126551) INFO 01-23 00:00:18 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:47131 backend=nccl
(EngineCore_DP0 pid=4126551) INFO 01-23 00:00:18 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=4126551) INFO 01-23 00:00:19 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints/Qwen2.5-1.5B-FP8...
(EngineCore_DP0 pid=4126551) INFO 01-23 00:00:19 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=4126551) INFO 01-23 00:00:19 [default_loader.py:308] Loading weights took 0.20 seconds
(EngineCore_DP0 pid=4126551) INFO 01-23 00:00:19 [gpu_model_runner.py:3659] Model loading took 1.6685 GiB memory and 0.393268 seconds
(EngineCore_DP0 pid=4126551) INFO 01-23 00:00:24 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/36b5066762/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=4126551) INFO 01-23 00:00:24 [backends.py:703] Dynamo bytecode transform time: 4.22 s
(EngineCore_DP0 pid=4126551) INFO 01-23 00:00:27 [backends.py:261] Cache the graph of compile range (1, 145) for later use
(EngineCore_DP0 pid=4126551) INFO 01-23 00:00:45 [backends.py:278] Compiling a graph for compile range (1, 145) takes 19.77 s
(EngineCore_DP0 pid=4126551) INFO 01-23 00:00:45 [monitor.py:34] torch.compile takes 23.99 s in total
(EngineCore_DP0 pid=4126551) INFO 01-23 00:00:46 [gpu_worker.py:375] Available KV cache memory: 10.12 GiB
(EngineCore_DP0 pid=4126551) INFO 01-23 00:00:46 [kv_cache_utils.py:1291] GPU KV cache size: 378,992 tokens
(EngineCore_DP0 pid=4126551) INFO 01-23 00:00:46 [kv_cache_utils.py:1296] Maximum concurrency for 145 tokens per request: 2368.70x
(EngineCore_DP0 pid=4126551) INFO 01-23 00:00:47 [gpu_model_runner.py:4587] Graph capturing finished in 1 secs, took 0.04 GiB
(EngineCore_DP0 pid=4126551) INFO 01-23 00:00:47 [core.py:259] init engine (profile, create kv cache, warmup model) took 27.63 seconds
INFO 01-23 00:00:48 [llm.py:360] Supported tasks: ['generate']
Throughput: 109.73 requests/s, 1865.46 total tokens/s, 109.73 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  128


─── STDERR ───
[2026-01-23 00:00:15] INFO gemm_wrapper.py:362: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-23 00:00:15] INFO gemm_wrapper.py:408: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-23 00:00:15] INFO gemm_wrapper.py:455: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-23 00:00:15] INFO gemm_wrapper.py:502: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-23 00:00:18] INFO gemm_wrapper.py:362: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-23 00:00:18] INFO gemm_wrapper.py:408: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-23 00:00:18] INFO gemm_wrapper.py:455: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-23 00:00:18] INFO gemm_wrapper.py:502: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=4126551) [2026-01-23 00:00:19] INFO SlideSparseLinearMethod_FP8.py:691: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=4126551) [2026-01-23 00:00:19] INFO gemm_wrapper.py:307: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=4126551) [2026-01-23 00:00:19] INFO kernels.py:172: FP8 quant kernel loaded
(EngineCore_DP0 pid=4126551) [2026-01-23 00:00:19] INFO kernels.py:115: Dequant+bias kernel loaded
(EngineCore_DP0 pid=4126551) [2026-01-23 00:00:19] INFO SlideSparseLinearMethod_FP8.py:372: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=4126551) [2026-01-23 00:00:19] INFO SlideSparseLinearMethod_FP8.py:517: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=4126551) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=4126551) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.56it/s]
(EngineCore_DP0 pid=4126551) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.56it/s]
(EngineCore_DP0 pid=4126551) 
(EngineCore_DP0 pid=4126551) 2026-01-23 00:00:46,964 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=4126551) 2026-01-23 00:00:46,973 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=4126551) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 47.48it/s]
(EngineCore_DP0 pid=4126551) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  8.44it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  8.43it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  62%|██████▎   | 80/128 [00:00<00:00, 527.93it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 810.69it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  19%|█▉        | 24/128 [00:00<00:00, 227.18it/s, est. speed input: 3635.63 toks/s, output: 227.19 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:00<00:00, 143.70it/s, est. speed input: 2436.42 toks/s, output: 152.27 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:00<00:00, 131.86it/s, est. speed input: 2250.58 toks/s, output: 140.66 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:00<00:00, 126.47it/s, est. speed input: 2165.37 toks/s, output: 135.33 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:00<00:00, 122.92it/s, est. speed input: 2110.43 toks/s, output: 131.90 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:00<00:00, 121.10it/s, est. speed input: 2077.46 toks/s, output: 129.84 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:00<00:00, 119.46it/s, est. speed input: 2049.94 toks/s, output: 128.12 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 119.46it/s, est. speed input: 2032.20 toks/s, output: 127.01 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 127.00it/s, est. speed input: 2032.20 toks/s, output: 127.01 toks/s]
[rank0]:[W123 00:00:49.281215812 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 39.5s

测试结果:
  Requests/s:   109.73
  Tokens/s:     1865.46
  Total Reqs:   128
  Elapsed:      1.17s

  [Prefill 分析]
  Total Prefill Tokens: 2048
  Prefill Tokens/s:     1755.73

============================================================
[2/3] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-1.5B-FP8
│ Backend:  cuBLASLt
│ 阶段:     prefill
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度:
│   M_prefill = 128 (= 1 x 128)
│   M_decode  = 1
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len       = 128
│   --output-len      = 1
│   --num-prompts     = 128
│   --max-num-seqs    = 1
│   --max-model-len   = 257
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
INFO 01-23 00:00:54 [datasets.py:612] Sampling input_len from [128, 128] and output_len from [1, 1]
INFO 01-23 00:00:54 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints/Qwen2.5-1.5B-FP8', 'max_model_len': 257, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 257, 'max_num_seqs': 1, 'disable_log_stats': True, 'enable_lora': None, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints/Qwen2.5-1.5B-FP8'}
INFO 01-23 00:00:54 [model.py:514] Resolved architecture: Qwen2ForCausalLM
INFO 01-23 00:00:54 [model.py:1661] Using max model len 257
INFO 01-23 00:00:54 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=257.
(EngineCore_DP0 pid=4128242) INFO 01-23 00:00:58 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints/Qwen2.5-1.5B-FP8', speculative_config=None, tokenizer='/root/vllmbench/checkpoints/Qwen2.5-1.5B-FP8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=257, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints/Qwen2.5-1.5B-FP8, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [257], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 2, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=4128242) INFO 01-23 00:00:58 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:36351 backend=nccl
(EngineCore_DP0 pid=4128242) INFO 01-23 00:00:58 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=4128242) INFO 01-23 00:00:58 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints/Qwen2.5-1.5B-FP8...
(EngineCore_DP0 pid=4128242) INFO 01-23 00:00:58 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=4128242) INFO 01-23 00:00:59 [default_loader.py:308] Loading weights took 0.20 seconds
(EngineCore_DP0 pid=4128242) INFO 01-23 00:00:59 [gpu_model_runner.py:3659] Model loading took 1.6685 GiB memory and 0.390946 seconds
(EngineCore_DP0 pid=4128242) INFO 01-23 00:01:03 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/d2be8a846b/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=4128242) INFO 01-23 00:01:03 [backends.py:703] Dynamo bytecode transform time: 4.28 s
(EngineCore_DP0 pid=4128242) INFO 01-23 00:01:06 [backends.py:261] Cache the graph of compile range (1, 257) for later use
(EngineCore_DP0 pid=4128242) INFO 01-23 00:01:23 [backends.py:278] Compiling a graph for compile range (1, 257) takes 17.59 s
(EngineCore_DP0 pid=4128242) INFO 01-23 00:01:23 [monitor.py:34] torch.compile takes 21.86 s in total
(EngineCore_DP0 pid=4128242) INFO 01-23 00:01:24 [gpu_worker.py:375] Available KV cache memory: 10.09 GiB
(EngineCore_DP0 pid=4128242) INFO 01-23 00:01:24 [kv_cache_utils.py:1291] GPU KV cache size: 377,792 tokens
(EngineCore_DP0 pid=4128242) INFO 01-23 00:01:24 [kv_cache_utils.py:1296] Maximum concurrency for 257 tokens per request: 1388.94x
(EngineCore_DP0 pid=4128242) INFO 01-23 00:01:24 [gpu_model_runner.py:4587] Graph capturing finished in 1 secs, took 0.01 GiB
(EngineCore_DP0 pid=4128242) INFO 01-23 00:01:24 [core.py:259] init engine (profile, create kv cache, warmup model) took 25.49 seconds
INFO 01-23 00:01:25 [llm.py:360] Supported tasks: ['generate']
Throughput: 110.70 requests/s, 14279.71 total tokens/s, 110.70 output tokens/s
Total num prompt tokens:  16384
Total num output tokens:  128


─── STDERR ───
[2026-01-23 00:00:54] INFO gemm_wrapper.py:362: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-23 00:00:54] INFO gemm_wrapper.py:408: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-23 00:00:54] INFO gemm_wrapper.py:455: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-23 00:00:54] INFO gemm_wrapper.py:502: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-23 00:00:58] INFO gemm_wrapper.py:362: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-23 00:00:58] INFO gemm_wrapper.py:408: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-23 00:00:58] INFO gemm_wrapper.py:455: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-23 00:00:58] INFO gemm_wrapper.py:502: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=4128242) [2026-01-23 00:00:58] INFO SlideSparseLinearMethod_FP8.py:691: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=4128242) [2026-01-23 00:00:58] INFO gemm_wrapper.py:307: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=4128242) [2026-01-23 00:00:58] INFO kernels.py:172: FP8 quant kernel loaded
(EngineCore_DP0 pid=4128242) [2026-01-23 00:00:58] INFO kernels.py:115: Dequant+bias kernel loaded
(EngineCore_DP0 pid=4128242) [2026-01-23 00:00:58] INFO SlideSparseLinearMethod_FP8.py:372: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=4128242) [2026-01-23 00:00:58] INFO SlideSparseLinearMethod_FP8.py:517: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=4128242) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=4128242) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.54it/s]
(EngineCore_DP0 pid=4128242) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.54it/s]
(EngineCore_DP0 pid=4128242) 
(EngineCore_DP0 pid=4128242) 2026-01-23 00:01:24,289 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=4128242) 2026-01-23 00:01:24,302 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=4128242) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 36.85it/s]
(EngineCore_DP0 pid=4128242) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  8.32it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  8.32it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 2745.92it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:00, 121.91it/s, est. speed input: 15605.28 toks/s, output: 121.91 toks/s]
Processed prompts:  20%|██        | 26/128 [00:00<00:00, 117.57it/s, est. speed input: 15130.35 toks/s, output: 118.20 toks/s]
Processed prompts:  30%|██▉       | 38/128 [00:00<00:00, 115.62it/s, est. speed input: 14922.64 toks/s, output: 116.58 toks/s]
Processed prompts:  39%|███▉      | 50/128 [00:00<00:00, 113.38it/s, est. speed input: 14710.27 toks/s, output: 114.92 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:00<00:00, 113.98it/s, est. speed input: 14714.55 toks/s, output: 114.95 toks/s]
Processed prompts:  58%|█████▊    | 74/128 [00:00<00:00, 114.25it/s, est. speed input: 14710.81 toks/s, output: 114.93 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:00<00:00, 114.74it/s, est. speed input: 14725.89 toks/s, output: 115.04 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:00<00:00, 114.78it/s, est. speed input: 14723.09 toks/s, output: 115.02 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:00<00:00, 115.12it/s, est. speed input: 14735.05 toks/s, output: 115.12 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:01<00:00, 115.70it/s, est. speed input: 14758.42 toks/s, output: 115.30 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 115.70it/s, est. speed input: 14773.99 toks/s, output: 115.42 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 115.41it/s, est. speed input: 14773.99 toks/s, output: 115.42 toks/s]
[rank0]:[W123 00:01:26.615253849 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 37.5s

测试结果:
  Requests/s:   110.70
  Tokens/s:     14279.71
  Total Reqs:   128
  Elapsed:      1.16s

  [Prefill 分析]
  Total Prefill Tokens: 16384
  Prefill Tokens/s:     14169.02

============================================================
[3/3] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-1.5B-FP8
│ Backend:  cuBLASLt
│ 阶段:     prefill
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度:
│   M_prefill = 256 (= 1 x 256)
│   M_decode  = 1
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len       = 256
│   --output-len      = 1
│   --num-prompts     = 128
│   --max-num-seqs    = 1
│   --max-model-len   = 385
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
INFO 01-23 00:01:31 [datasets.py:612] Sampling input_len from [256, 256] and output_len from [1, 1]
INFO 01-23 00:01:31 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints/Qwen2.5-1.5B-FP8', 'max_model_len': 385, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 385, 'max_num_seqs': 1, 'disable_log_stats': True, 'enable_lora': None, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints/Qwen2.5-1.5B-FP8'}
INFO 01-23 00:01:31 [model.py:514] Resolved architecture: Qwen2ForCausalLM
INFO 01-23 00:01:31 [model.py:1661] Using max model len 385
INFO 01-23 00:01:32 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=385.
(EngineCore_DP0 pid=4129656) INFO 01-23 00:01:35 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints/Qwen2.5-1.5B-FP8', speculative_config=None, tokenizer='/root/vllmbench/checkpoints/Qwen2.5-1.5B-FP8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=385, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints/Qwen2.5-1.5B-FP8, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [385], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 2, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=4129656) INFO 01-23 00:01:35 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:54165 backend=nccl
(EngineCore_DP0 pid=4129656) INFO 01-23 00:01:35 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=4129656) INFO 01-23 00:01:36 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints/Qwen2.5-1.5B-FP8...
(EngineCore_DP0 pid=4129656) INFO 01-23 00:01:36 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=4129656) INFO 01-23 00:01:36 [default_loader.py:308] Loading weights took 0.20 seconds
(EngineCore_DP0 pid=4129656) INFO 01-23 00:01:37 [gpu_model_runner.py:3659] Model loading took 1.6690 GiB memory and 0.407585 seconds
(EngineCore_DP0 pid=4129656) INFO 01-23 00:01:41 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/0610d4563b/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=4129656) INFO 01-23 00:01:41 [backends.py:703] Dynamo bytecode transform time: 4.29 s
(EngineCore_DP0 pid=4129656) INFO 01-23 00:01:44 [backends.py:261] Cache the graph of compile range (1, 385) for later use
(EngineCore_DP0 pid=4129656) INFO 01-23 00:02:00 [backends.py:278] Compiling a graph for compile range (1, 385) takes 17.48 s
(EngineCore_DP0 pid=4129656) INFO 01-23 00:02:00 [monitor.py:34] torch.compile takes 21.77 s in total
(EngineCore_DP0 pid=4129656) INFO 01-23 00:02:01 [gpu_worker.py:375] Available KV cache memory: 10.12 GiB
(EngineCore_DP0 pid=4129656) INFO 01-23 00:02:01 [kv_cache_utils.py:1291] GPU KV cache size: 378,880 tokens
(EngineCore_DP0 pid=4129656) INFO 01-23 00:02:01 [kv_cache_utils.py:1296] Maximum concurrency for 385 tokens per request: 947.20x
(EngineCore_DP0 pid=4129656) INFO 01-23 00:02:02 [gpu_model_runner.py:4587] Graph capturing finished in 1 secs, took 0.04 GiB
(EngineCore_DP0 pid=4129656) INFO 01-23 00:02:02 [core.py:259] init engine (profile, create kv cache, warmup model) took 25.34 seconds
INFO 01-23 00:02:03 [llm.py:360] Supported tasks: ['generate']
Throughput: 107.27 requests/s, 27567.74 total tokens/s, 107.27 output tokens/s
Total num prompt tokens:  32768
Total num output tokens:  128


─── STDERR ───
[2026-01-23 00:01:31] INFO gemm_wrapper.py:362: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-23 00:01:31] INFO gemm_wrapper.py:408: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-23 00:01:31] INFO gemm_wrapper.py:455: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-23 00:01:31] INFO gemm_wrapper.py:502: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-23 00:01:35] INFO gemm_wrapper.py:362: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-23 00:01:35] INFO gemm_wrapper.py:408: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-23 00:01:35] INFO gemm_wrapper.py:455: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-23 00:01:35] INFO gemm_wrapper.py:502: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=4129656) [2026-01-23 00:01:36] INFO SlideSparseLinearMethod_FP8.py:691: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=4129656) [2026-01-23 00:01:36] INFO gemm_wrapper.py:307: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=4129656) [2026-01-23 00:01:36] INFO kernels.py:172: FP8 quant kernel loaded
(EngineCore_DP0 pid=4129656) [2026-01-23 00:01:36] INFO kernels.py:115: Dequant+bias kernel loaded
(EngineCore_DP0 pid=4129656) [2026-01-23 00:01:36] INFO SlideSparseLinearMethod_FP8.py:372: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=4129656) [2026-01-23 00:01:36] INFO SlideSparseLinearMethod_FP8.py:517: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=4129656) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=4129656) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.53it/s]
(EngineCore_DP0 pid=4129656) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.52it/s]
(EngineCore_DP0 pid=4129656) 
(EngineCore_DP0 pid=4129656) 2026-01-23 00:02:01,812 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=4129656) 2026-01-23 00:02:01,821 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=4129656) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 47.90it/s]
(EngineCore_DP0 pid=4129656) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  8.39it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  8.38it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 1462.19it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:00<00:00, 140.14it/s, est. speed input: 35878.46 toks/s, output: 140.14 toks/s]
Processed prompts:  23%|██▎       | 30/128 [00:00<00:00, 123.61it/s, est. speed input: 32216.99 toks/s, output: 125.84 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:00<00:00, 119.55it/s, est. speed input: 31270.40 toks/s, output: 122.14 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:00<00:00, 116.57it/s, est. speed input: 30621.17 toks/s, output: 119.61 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:00<00:00, 113.79it/s, est. speed input: 30080.42 toks/s, output: 117.50 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:00<00:00, 113.97it/s, est. speed input: 29957.31 toks/s, output: 117.02 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:00<00:00, 114.01it/s, est. speed input: 29857.17 toks/s, output: 116.63 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:00<00:00, 113.75it/s, est. speed input: 29753.00 toks/s, output: 116.22 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:00<00:00, 114.21it/s, est. speed input: 29726.94 toks/s, output: 116.12 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 114.24it/s, est. speed input: 29682.71 toks/s, output: 115.95 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 114.24it/s, est. speed input: 29682.71 toks/s, output: 115.95 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 115.94it/s, est. speed input: 29682.71 toks/s, output: 115.95 toks/s]
[rank0]:[W123 00:02:04.166348481 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 37.6s

测试结果:
  Requests/s:   107.27
  Tokens/s:     27567.74
  Total Reqs:   128
  Elapsed:      1.19s

  [Prefill 分析]
  Total Prefill Tokens: 32768
  Prefill Tokens/s:     27460.48


------------------------------------------------------------
  生成 CSV: Qwen2.5-1.5B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cublaslt/Qwen2.5-1.5B-FP8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
16,16,1,128,128,109.7332,1865.4647,1.1665
128,128,1,128,128,110.6954,14279.7125,1.1563
256,256,1,128,128,107.2675,27567.7439,1.1933

------------------------------------------------------------

[INFO] 完成: 3 成功, 0 失败

============================================================
  Qwen2.5-1.5B-FP8 | cuBLASLt | decode
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints/Qwen2.5-1.5B-FP8
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cublaslt

============================================================
[1/3] 测试 M=16
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-1.5B-FP8
│ Backend:  cuBLASLt
│ 阶段:     decode
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度:
│   M_prefill = 256 (= 16 x 16)
│   M_decode  = 16
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len       = 16
│   --output-len      = 256
│   --num-prompts     = 16
│   --max-num-seqs    = 16
│   --max-model-len   = 400
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
INFO 01-23 00:02:09 [datasets.py:612] Sampling input_len from [16, 16] and output_len from [256, 256]
INFO 01-23 00:02:09 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints/Qwen2.5-1.5B-FP8', 'max_model_len': 400, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 6400, 'max_num_seqs': 16, 'disable_log_stats': True, 'enable_lora': None, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints/Qwen2.5-1.5B-FP8'}
INFO 01-23 00:02:09 [model.py:514] Resolved architecture: Qwen2ForCausalLM
INFO 01-23 00:02:09 [model.py:1661] Using max model len 400
INFO 01-23 00:02:09 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=6400.
(EngineCore_DP0 pid=4131081) INFO 01-23 00:02:13 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints/Qwen2.5-1.5B-FP8', speculative_config=None, tokenizer='/root/vllmbench/checkpoints/Qwen2.5-1.5B-FP8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=400, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints/Qwen2.5-1.5B-FP8, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [6400], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 32, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=4131081) INFO 01-23 00:02:13 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:55575 backend=nccl
(EngineCore_DP0 pid=4131081) INFO 01-23 00:02:13 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=4131081) INFO 01-23 00:02:13 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints/Qwen2.5-1.5B-FP8...
(EngineCore_DP0 pid=4131081) INFO 01-23 00:02:13 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=4131081) INFO 01-23 00:02:14 [default_loader.py:308] Loading weights took 0.20 seconds
(EngineCore_DP0 pid=4131081) INFO 01-23 00:02:14 [gpu_model_runner.py:3659] Model loading took 1.6685 GiB memory and 0.390277 seconds
(EngineCore_DP0 pid=4131081) INFO 01-23 00:02:18 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/bb77f0fb49/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=4131081) INFO 01-23 00:02:18 [backends.py:703] Dynamo bytecode transform time: 4.31 s
(EngineCore_DP0 pid=4131081) INFO 01-23 00:02:21 [backends.py:261] Cache the graph of compile range (1, 6400) for later use
(EngineCore_DP0 pid=4131081) INFO 01-23 00:02:39 [backends.py:278] Compiling a graph for compile range (1, 6400) takes 18.58 s
(EngineCore_DP0 pid=4131081) INFO 01-23 00:02:39 [monitor.py:34] torch.compile takes 22.89 s in total
(EngineCore_DP0 pid=4131081) INFO 01-23 00:02:40 [gpu_worker.py:375] Available KV cache memory: 10.08 GiB
(EngineCore_DP0 pid=4131081) INFO 01-23 00:02:40 [kv_cache_utils.py:1291] GPU KV cache size: 377,648 tokens
(EngineCore_DP0 pid=4131081) INFO 01-23 00:02:40 [kv_cache_utils.py:1296] Maximum concurrency for 400 tokens per request: 944.12x
(EngineCore_DP0 pid=4131081) INFO 01-23 00:02:41 [gpu_model_runner.py:4587] Graph capturing finished in 1 secs, took -0.66 GiB
(EngineCore_DP0 pid=4131081) INFO 01-23 00:02:41 [core.py:259] init engine (profile, create kv cache, warmup model) took 26.80 seconds
INFO 01-23 00:02:41 [llm.py:360] Supported tasks: ['generate']
Throughput: 15.59 requests/s, 4240.72 total tokens/s, 3991.26 output tokens/s
Total num prompt tokens:  256
Total num output tokens:  4096


─── STDERR ───
[2026-01-23 00:02:09] INFO gemm_wrapper.py:362: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-23 00:02:09] INFO gemm_wrapper.py:408: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-23 00:02:09] INFO gemm_wrapper.py:455: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-23 00:02:09] INFO gemm_wrapper.py:502: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-23 00:02:13] INFO gemm_wrapper.py:362: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-23 00:02:13] INFO gemm_wrapper.py:408: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-23 00:02:13] INFO gemm_wrapper.py:455: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-23 00:02:13] INFO gemm_wrapper.py:502: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=4131081) [2026-01-23 00:02:13] INFO SlideSparseLinearMethod_FP8.py:691: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=4131081) [2026-01-23 00:02:13] INFO gemm_wrapper.py:307: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=4131081) [2026-01-23 00:02:13] INFO kernels.py:172: FP8 quant kernel loaded
(EngineCore_DP0 pid=4131081) [2026-01-23 00:02:13] INFO kernels.py:115: Dequant+bias kernel loaded
(EngineCore_DP0 pid=4131081) [2026-01-23 00:02:13] INFO SlideSparseLinearMethod_FP8.py:372: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=4131081) [2026-01-23 00:02:13] INFO SlideSparseLinearMethod_FP8.py:517: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=4131081) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=4131081) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.61it/s]
(EngineCore_DP0 pid=4131081) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.61it/s]
(EngineCore_DP0 pid=4131081) 
(EngineCore_DP0 pid=4131081) [rank0]:W0123 00:02:23.065000 4131081 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=4131081) [rank0]:W0123 00:02:23.675000 4131081 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=4131081) [rank0]:W0123 00:02:38.759000 4131081 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=4131081) [rank0]:W0123 00:02:38.913000 4131081 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=4131081) 2026-01-23 00:02:40,385 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=4131081) 2026-01-23 00:02:40,395 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=4131081) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 5/7 [00:00<00:00, 48.63it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 45.69it/s]
(EngineCore_DP0 pid=4131081) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 1/5 [00:00<00:00,  8.42it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 26.01it/s]

Adding requests:   0%|          | 0/16 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 16/16 [00:00<00:00, 1528.40it/s]

Processed prompts:   0%|          | 0/16 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   6%|▋         | 1/16 [00:01<00:15,  1.01s/it, est. speed input: 15.86 toks/s, output: 253.69 toks/s]
Processed prompts: 100%|██████████| 16/16 [00:01<00:00,  1.01s/it, est. speed input: 252.60 toks/s, output: 4041.57 toks/s]
Processed prompts: 100%|██████████| 16/16 [00:01<00:00, 15.79it/s, est. speed input: 252.60 toks/s, output: 4041.57 toks/s]
[rank0]:[W123 00:02:42.794178459 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 38.6s

测试结果:
  Requests/s:   15.59
  Tokens/s:     4240.72
  Total Reqs:   16
  Elapsed:      1.03s

  [Decode 分析]
  Total Decode Tokens:  4096
  Decode Tokens/s:      3991.26

============================================================
[2/3] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-1.5B-FP8
│ Backend:  cuBLASLt
│ 阶段:     decode
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度:
│   M_prefill = 2048 (= 128 x 16)
│   M_decode  = 128
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len       = 16
│   --output-len      = 256
│   --num-prompts     = 128
│   --max-num-seqs    = 128
│   --max-model-len   = 400
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
INFO 01-23 00:02:47 [datasets.py:612] Sampling input_len from [16, 16] and output_len from [256, 256]
INFO 01-23 00:02:47 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints/Qwen2.5-1.5B-FP8', 'max_model_len': 400, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 51200, 'max_num_seqs': 128, 'disable_log_stats': True, 'enable_lora': None, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints/Qwen2.5-1.5B-FP8'}
INFO 01-23 00:02:47 [model.py:514] Resolved architecture: Qwen2ForCausalLM
INFO 01-23 00:02:47 [model.py:1661] Using max model len 400
INFO 01-23 00:02:48 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=51200.
(EngineCore_DP0 pid=4132571) INFO 01-23 00:02:51 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints/Qwen2.5-1.5B-FP8', speculative_config=None, tokenizer='/root/vllmbench/checkpoints/Qwen2.5-1.5B-FP8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=400, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints/Qwen2.5-1.5B-FP8, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [51200], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 256, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=4132571) INFO 01-23 00:02:51 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:47119 backend=nccl
(EngineCore_DP0 pid=4132571) INFO 01-23 00:02:52 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=4132571) INFO 01-23 00:02:52 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints/Qwen2.5-1.5B-FP8...
(EngineCore_DP0 pid=4132571) INFO 01-23 00:02:52 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=4132571) INFO 01-23 00:02:52 [default_loader.py:308] Loading weights took 0.20 seconds
(EngineCore_DP0 pid=4132571) INFO 01-23 00:02:53 [gpu_model_runner.py:3659] Model loading took 1.6685 GiB memory and 0.391003 seconds
(EngineCore_DP0 pid=4132571) INFO 01-23 00:02:57 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/77049d67f1/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=4132571) INFO 01-23 00:02:57 [backends.py:703] Dynamo bytecode transform time: 4.22 s
(EngineCore_DP0 pid=4132571) INFO 01-23 00:03:01 [backends.py:261] Cache the graph of compile range (1, 51200) for later use
(EngineCore_DP0 pid=4132571) INFO 01-23 00:03:19 [backends.py:278] Compiling a graph for compile range (1, 51200) takes 20.46 s
(EngineCore_DP0 pid=4132571) INFO 01-23 00:03:19 [monitor.py:34] torch.compile takes 24.68 s in total
(EngineCore_DP0 pid=4132571) INFO 01-23 00:03:20 [gpu_worker.py:375] Available KV cache memory: 6.61 GiB
(EngineCore_DP0 pid=4132571) INFO 01-23 00:03:20 [kv_cache_utils.py:1291] GPU KV cache size: 247,712 tokens
(EngineCore_DP0 pid=4132571) INFO 01-23 00:03:20 [kv_cache_utils.py:1296] Maximum concurrency for 400 tokens per request: 619.28x
(EngineCore_DP0 pid=4132571) INFO 01-23 00:03:23 [gpu_model_runner.py:4587] Graph capturing finished in 2 secs, took -5.76 GiB
(EngineCore_DP0 pid=4132571) INFO 01-23 00:03:23 [core.py:259] init engine (profile, create kv cache, warmup model) took 30.35 seconds
INFO 01-23 00:03:24 [llm.py:360] Supported tasks: ['generate']
Throughput: 70.52 requests/s, 19181.53 total tokens/s, 18053.21 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768


─── STDERR ───
[2026-01-23 00:02:48] INFO gemm_wrapper.py:362: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-23 00:02:48] INFO gemm_wrapper.py:408: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-23 00:02:48] INFO gemm_wrapper.py:455: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-23 00:02:48] INFO gemm_wrapper.py:502: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-23 00:02:51] INFO gemm_wrapper.py:362: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-23 00:02:51] INFO gemm_wrapper.py:408: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-23 00:02:51] INFO gemm_wrapper.py:455: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-23 00:02:51] INFO gemm_wrapper.py:502: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=4132571) [2026-01-23 00:02:52] INFO SlideSparseLinearMethod_FP8.py:691: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=4132571) [2026-01-23 00:02:52] INFO gemm_wrapper.py:307: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=4132571) [2026-01-23 00:02:52] INFO kernels.py:172: FP8 quant kernel loaded
(EngineCore_DP0 pid=4132571) [2026-01-23 00:02:52] INFO kernels.py:115: Dequant+bias kernel loaded
(EngineCore_DP0 pid=4132571) [2026-01-23 00:02:52] INFO SlideSparseLinearMethod_FP8.py:372: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=4132571) [2026-01-23 00:02:52] INFO SlideSparseLinearMethod_FP8.py:517: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=4132571) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=4132571) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.58it/s]
(EngineCore_DP0 pid=4132571) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.57it/s]
(EngineCore_DP0 pid=4132571) 
(EngineCore_DP0 pid=4132571) [rank0]:W0123 00:03:00.029000 4132571 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=4132571) [rank0]:W0123 00:03:00.274000 4132571 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=4132571) [rank0]:W0123 00:03:00.334000 4132571 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=4132571) [rank0]:W0123 00:03:02.260000 4132571 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=4132571) [rank0]:W0123 00:03:02.556000 4132571 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=4132571) [rank0]:W0123 00:03:02.901000 4132571 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=4132571) [rank0]:W0123 00:03:02.930000 4132571 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=4132571) [rank0]:W0123 00:03:02.976000 4132571 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=4132571) [rank0]:W0123 00:03:03.002000 4132571 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=4132571) [rank0]:W0123 00:03:18.993000 4132571 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=4132571) [rank0]:W0123 00:03:19.004000 4132571 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=4132571) [rank0]:W0123 00:03:19.108000 4132571 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=4132571) [rank0]:W0123 00:03:19.147000 4132571 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=4132571) 2026-01-23 00:03:20,946 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=4132571) 2026-01-23 00:03:20,957 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=4132571) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|▎         | 1/35 [00:00<00:26,  1.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 6/35 [00:00<00:03,  8.79it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 11/35 [00:00<00:01, 15.93it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  46%|████▌     | 16/35 [00:01<00:00, 22.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 21/35 [00:01<00:00, 28.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▍  | 26/35 [00:01<00:00, 33.25it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▊ | 31/35 [00:01<00:00, 37.03it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:01<00:00, 23.28it/s]
(EngineCore_DP0 pid=4132571) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   5%|▌         | 1/19 [00:00<00:02,  8.14it/s]
Capturing CUDA graphs (decode, FULL):  32%|███▏      | 6/19 [00:00<00:00, 29.97it/s]
Capturing CUDA graphs (decode, FULL):  58%|█████▊    | 11/19 [00:00<00:00, 37.75it/s]
Capturing CUDA graphs (decode, FULL):  84%|████████▍ | 16/19 [00:00<00:00, 42.15it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:00<00:00, 38.62it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 4408.78it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:01<03:45,  1.78s/it, est. speed input: 9.01 toks/s, output: 144.20 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00,  1.78s/it, est. speed input: 1147.80 toks/s, output: 18364.83 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 71.74it/s, est. speed input: 1147.80 toks/s, output: 18364.83 toks/s]
[rank0]:[W123 00:03:25.767207360 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 43.0s

测试结果:
  Requests/s:   70.52
  Tokens/s:     19181.53
  Total Reqs:   128
  Elapsed:      1.82s

  [Decode 分析]
  Total Decode Tokens:  32768
  Decode Tokens/s:      18053.21

============================================================
[3/3] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-1.5B-FP8
│ Backend:  cuBLASLt
│ 阶段:     decode
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度:
│   M_prefill = 4096 (= 256 x 16)
│   M_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len       = 16
│   --output-len      = 256
│   --num-prompts     = 256
│   --max-num-seqs    = 256
│   --max-model-len   = 400
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
INFO 01-23 00:03:30 [datasets.py:612] Sampling input_len from [16, 16] and output_len from [256, 256]
INFO 01-23 00:03:30 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints/Qwen2.5-1.5B-FP8', 'max_model_len': 400, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 102400, 'max_num_seqs': 256, 'disable_log_stats': True, 'enable_lora': None, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints/Qwen2.5-1.5B-FP8'}
INFO 01-23 00:03:31 [model.py:514] Resolved architecture: Qwen2ForCausalLM
INFO 01-23 00:03:31 [model.py:1661] Using max model len 400
INFO 01-23 00:03:31 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=102400.
(EngineCore_DP0 pid=4134253) INFO 01-23 00:03:34 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints/Qwen2.5-1.5B-FP8', speculative_config=None, tokenizer='/root/vllmbench/checkpoints/Qwen2.5-1.5B-FP8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=400, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints/Qwen2.5-1.5B-FP8, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [102400], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=4134253) INFO 01-23 00:03:35 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:59703 backend=nccl
(EngineCore_DP0 pid=4134253) INFO 01-23 00:03:35 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=4134253) INFO 01-23 00:03:35 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints/Qwen2.5-1.5B-FP8...
(EngineCore_DP0 pid=4134253) INFO 01-23 00:03:35 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=4134253) INFO 01-23 00:03:35 [default_loader.py:308] Loading weights took 0.20 seconds
(EngineCore_DP0 pid=4134253) INFO 01-23 00:03:36 [gpu_model_runner.py:3659] Model loading took 1.6685 GiB memory and 0.395685 seconds
(EngineCore_DP0 pid=4134253) INFO 01-23 00:03:40 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/0cfccf434b/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=4134253) INFO 01-23 00:03:40 [backends.py:703] Dynamo bytecode transform time: 4.31 s
(EngineCore_DP0 pid=4134253) INFO 01-23 00:03:44 [backends.py:261] Cache the graph of compile range (1, 102400) for later use
(EngineCore_DP0 pid=4134253) INFO 01-23 00:04:02 [backends.py:278] Compiling a graph for compile range (1, 102400) takes 20.09 s
(EngineCore_DP0 pid=4134253) INFO 01-23 00:04:02 [monitor.py:34] torch.compile takes 24.40 s in total
(EngineCore_DP0 pid=4134253) INFO 01-23 00:04:03 [gpu_worker.py:375] Available KV cache memory: 2.61 GiB
(EngineCore_DP0 pid=4134253) INFO 01-23 00:04:03 [kv_cache_utils.py:1291] GPU KV cache size: 97,760 tokens
(EngineCore_DP0 pid=4134253) INFO 01-23 00:04:03 [kv_cache_utils.py:1296] Maximum concurrency for 400 tokens per request: 244.40x
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 256, in _initialize_kv_caches
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]     self.model_executor.initialize_from_config(kv_cache_configs)
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 116, in initialize_from_config
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]     self.collective_rpc("compile_or_warm_up_model")
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 455, in compile_or_warm_up_model
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]     kernel_warmup(self)
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]   File "/root/vllmbench/vllm/model_executor/warmup/kernel_warmup.py", line 41, in kernel_warmup
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]     flashinfer_autotune(worker.model_runner)
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]   File "/root/vllmbench/vllm/model_executor/warmup/kernel_warmup.py", line 94, in flashinfer_autotune
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]     runner._dummy_run(
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]     hidden_states = self.model(
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]                     ^^^^^^^^^^^
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 439, in __call__
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]     return TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 223, in __call__
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]     def forward(
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]     raise e
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]   File "<eval_with_key>.58", line 334, in forward
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]     submod_4 = self.submod_4(getitem_10, s72, getitem_4, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_, getitem_5, l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_, getitem_11, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_10 = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_ = getitem_11 = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]     return range_entry.runnable(*args)
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]     return self._compiled_fn(*args)
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]     return compiled_fn(full_args)
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]                             ^^^^^^^
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 724, in inner_fn
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]     outs = compiled_fn(args)
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]            ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]     return self.current_callable(inputs)
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]     out = model(new_inputs)
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]   File "/tmp/torchinductor_root/o4/co4fywmxgo5nwegifc3gqwxr5hcq5it42u2t7doy3lr7ff3elr7o.py", line 2304, in call
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]     buf17 = empty_strided_cuda((s72, 17920), (17920, 1), torch.bfloat16)
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4134253) ERROR 01-23 00:04:04 [core.py:866] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.42 GiB. GPU 0 has a total capacity of 15.46 GiB of which 2.58 GiB is free. Including non-PyTorch memory, this process has 12.64 GiB memory in use. Of the allocated memory 8.89 GiB is allocated by PyTorch, and 3.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


─── STDERR ───
[2026-01-23 00:03:31] INFO gemm_wrapper.py:362: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-23 00:03:31] INFO gemm_wrapper.py:408: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-23 00:03:31] INFO gemm_wrapper.py:455: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-23 00:03:31] INFO gemm_wrapper.py:502: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-23 00:03:34] INFO gemm_wrapper.py:362: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-23 00:03:34] INFO gemm_wrapper.py:408: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-23 00:03:34] INFO gemm_wrapper.py:455: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-23 00:03:34] INFO gemm_wrapper.py:502: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=4134253) [2026-01-23 00:03:35] INFO SlideSparseLinearMethod_FP8.py:691: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=4134253) [2026-01-23 00:03:35] INFO gemm_wrapper.py:307: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=4134253) [2026-01-23 00:03:35] INFO kernels.py:172: FP8 quant kernel loaded
(EngineCore_DP0 pid=4134253) [2026-01-23 00:03:35] INFO kernels.py:115: Dequant+bias kernel loaded
(EngineCore_DP0 pid=4134253) [2026-01-23 00:03:35] INFO SlideSparseLinearMethod_FP8.py:372: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=4134253) [2026-01-23 00:03:35] INFO SlideSparseLinearMethod_FP8.py:517: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=4134253) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=4134253) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.56it/s]
(EngineCore_DP0 pid=4134253) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.56it/s]
(EngineCore_DP0 pid=4134253) 
(EngineCore_DP0 pid=4134253) [rank0]:W0123 00:03:43.352000 4134253 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=4134253) [rank0]:W0123 00:03:43.610000 4134253 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=4134253) [rank0]:W0123 00:03:43.638000 4134253 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=4134253) [rank0]:W0123 00:03:43.653000 4134253 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=4134253) [rank0]:W0123 00:03:45.206000 4134253 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=4134253) [rank0]:W0123 00:03:45.521000 4134253 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=4134253) [rank0]:W0123 00:03:45.875000 4134253 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=4134253) [rank0]:W0123 00:03:45.904000 4134253 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=4134253) [rank0]:W0123 00:03:45.952000 4134253 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=4134253) [rank0]:W0123 00:03:45.980000 4134253 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=4134253) [rank0]:W0123 00:04:01.856000 4134253 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=4134253) [rank0]:W0123 00:04:01.866000 4134253 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=4134253) [rank0]:W0123 00:04:02.002000 4134253 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=4134253) [rank0]:W0123 00:04:02.042000 4134253 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=4134253) 2026-01-23 00:04:03,919 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=4134253) 2026-01-23 00:04:04,033 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=4134253) Process EngineCore_DP0:
(EngineCore_DP0 pid=4134253) Traceback (most recent call last):
(EngineCore_DP0 pid=4134253)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=4134253)     self.run()
(EngineCore_DP0 pid=4134253)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=4134253)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=4134253)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=4134253)     raise e
(EngineCore_DP0 pid=4134253)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=4134253)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=4134253)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4134253)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=4134253)     super().__init__(
(EngineCore_DP0 pid=4134253)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=4134253)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=4134253)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4134253)   File "/root/vllmbench/vllm/v1/engine/core.py", line 256, in _initialize_kv_caches
(EngineCore_DP0 pid=4134253)     self.model_executor.initialize_from_config(kv_cache_configs)
(EngineCore_DP0 pid=4134253)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 116, in initialize_from_config
(EngineCore_DP0 pid=4134253)     self.collective_rpc("compile_or_warm_up_model")
(EngineCore_DP0 pid=4134253)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=4134253)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=4134253)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4134253)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=4134253)     return func(*args, **kwargs)
(EngineCore_DP0 pid=4134253)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4134253)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 455, in compile_or_warm_up_model
(EngineCore_DP0 pid=4134253)     kernel_warmup(self)
(EngineCore_DP0 pid=4134253)   File "/root/vllmbench/vllm/model_executor/warmup/kernel_warmup.py", line 41, in kernel_warmup
(EngineCore_DP0 pid=4134253)     flashinfer_autotune(worker.model_runner)
(EngineCore_DP0 pid=4134253)   File "/root/vllmbench/vllm/model_executor/warmup/kernel_warmup.py", line 94, in flashinfer_autotune
(EngineCore_DP0 pid=4134253)     runner._dummy_run(
(EngineCore_DP0 pid=4134253)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=4134253)     return func(*args, **kwargs)
(EngineCore_DP0 pid=4134253)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4134253)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=4134253)     outputs = self.model(
(EngineCore_DP0 pid=4134253)               ^^^^^^^^^^^
(EngineCore_DP0 pid=4134253)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=4134253)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=4134253)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4134253)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=4134253)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=4134253)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4134253)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=4134253)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=4134253)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4134253)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=4134253)     hidden_states = self.model(
(EngineCore_DP0 pid=4134253)                     ^^^^^^^^^^^
(EngineCore_DP0 pid=4134253)   File "/root/vllmbench/vllm/compilation/decorators.py", line 439, in __call__
(EngineCore_DP0 pid=4134253)     return TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=4134253)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4134253)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 223, in __call__
(EngineCore_DP0 pid=4134253)     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=4134253)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4134253)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=4134253)     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=4134253)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4134253)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
(EngineCore_DP0 pid=4134253)     def forward(
(EngineCore_DP0 pid=4134253)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=4134253)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=4134253)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4134253)   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=4134253)     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=4134253)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4134253)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=4134253)     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=4134253)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4134253)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=4134253)     raise e
(EngineCore_DP0 pid=4134253)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=4134253)     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=4134253)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4134253)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=4134253)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=4134253)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4134253)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=4134253)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=4134253)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4134253)   File "<eval_with_key>.58", line 334, in forward
(EngineCore_DP0 pid=4134253)     submod_4 = self.submod_4(getitem_10, s72, getitem_4, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_, getitem_5, l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_, getitem_11, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_10 = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_ = getitem_11 = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
(EngineCore_DP0 pid=4134253)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4134253)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=4134253)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=4134253)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4134253)   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=4134253)     return range_entry.runnable(*args)
(EngineCore_DP0 pid=4134253)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4134253)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=4134253)     return self._compiled_fn(*args)
(EngineCore_DP0 pid=4134253)            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4134253)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=4134253)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=4134253)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4134253)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=4134253)     return compiled_fn(full_args)
(EngineCore_DP0 pid=4134253)            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4134253)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=4134253)     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=4134253)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4134253)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=4134253)     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=4134253)                             ^^^^^^^
(EngineCore_DP0 pid=4134253)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 724, in inner_fn
(EngineCore_DP0 pid=4134253)     outs = compiled_fn(args)
(EngineCore_DP0 pid=4134253)            ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4134253)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=4134253)     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=4134253)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4134253)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=4134253)     return self.current_callable(inputs)
(EngineCore_DP0 pid=4134253)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4134253)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=4134253)     out = model(new_inputs)
(EngineCore_DP0 pid=4134253)           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4134253)   File "/tmp/torchinductor_root/o4/co4fywmxgo5nwegifc3gqwxr5hcq5it42u2t7doy3lr7ff3elr7o.py", line 2304, in call
(EngineCore_DP0 pid=4134253)     buf17 = empty_strided_cuda((s72, 17920), (17920, 1), torch.bfloat16)
(EngineCore_DP0 pid=4134253)             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4134253) torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.42 GiB. GPU 0 has a total capacity of 15.46 GiB of which 2.58 GiB is free. Including non-PyTorch memory, this process has 12.64 GiB memory in use. Of the allocated memory 8.89 GiB is allocated by PyTorch, and 3.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W123 00:04:04.373111029 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[ERROR] 测试失败: M=256 (exit code: 1)


------------------------------------------------------------
  生成 CSV: Qwen2.5-1.5B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cublaslt/Qwen2.5-1.5B-FP8_decode.csv

预览:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
16,16,16,16,256,256,15.5909,4240.7181,1.0262
128,16,128,128,256,256,70.5203,19181.5325,1.8151

------------------------------------------------------------

[INFO] 完成: 2 成功, 1 失败
[INFO] 尝试自动转换: Qwen2.5-1.5B-FP8 -> SlideSparse-2_4

======================================================================
[SlideSparse] 自动转换: Qwen2.5-1.5B-FP8 -> SlideSparse-2_4
======================================================================
[SUCCESS] 转换成功: Qwen2.5-1.5B-FP8-SlideSparse-2_4
======================================================================


============================================================
  Qwen2.5-1.5B-FP8 | cuSPARSELt (2_4) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-1.5B-FP8-SlideSparse-2_4
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_4

============================================================
[1/3] 测试 M=16
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-1.5B-FP8
│ Backend:  cuSPARSELt (2:4)
│ 阶段:     prefill
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度:
│   M_prefill = 16 (= 1 x 16)
│   M_decode  = 1
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len       = 16
│   --output-len      = 1
│   --num-prompts     = 128
│   --max-num-seqs    = 1
│   --max-model-len   = 145
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
INFO 01-23 00:04:52 [datasets.py:612] Sampling input_len from [16, 16] and output_len from [1, 1]
INFO 01-23 00:04:52 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints_slidesparse/Qwen2.5-1.5B-FP8-SlideSparse-2_4', 'max_model_len': 145, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 145, 'max_num_seqs': 1, 'disable_log_stats': True, 'enable_lora': None, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints_slidesparse/Qwen2.5-1.5B-FP8-SlideSparse-2_4'}
INFO 01-23 00:04:52 [model.py:514] Resolved architecture: Qwen2ForCausalLM
INFO 01-23 00:04:52 [model.py:1661] Using max model len 145
INFO 01-23 00:04:52 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=145.
(EngineCore_DP0 pid=4136162) INFO 01-23 00:04:55 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints_slidesparse/Qwen2.5-1.5B-FP8-SlideSparse-2_4', speculative_config=None, tokenizer='/root/vllmbench/checkpoints_slidesparse/Qwen2.5-1.5B-FP8-SlideSparse-2_4', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=145, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints_slidesparse/Qwen2.5-1.5B-FP8-SlideSparse-2_4, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [145], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 2, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=4136162) INFO 01-23 00:04:56 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:34989 backend=nccl
(EngineCore_DP0 pid=4136162) INFO 01-23 00:04:56 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=4136162) INFO 01-23 00:04:56 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-1.5B-FP8-SlideSparse-2_4...
(EngineCore_DP0 pid=4136162) INFO 01-23 00:04:56 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=4136162) INFO 01-23 00:04:56 [default_loader.py:308] Loading weights took 0.20 seconds
(EngineCore_DP0 pid=4136162) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=4136162) INFO 01-23 00:04:57 [gpu_model_runner.py:3659] Model loading took 1.2166 GiB memory and 0.441469 seconds
(EngineCore_DP0 pid=4136162) INFO 01-23 00:05:01 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/c2378e2653/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=4136162) INFO 01-23 00:05:01 [backends.py:703] Dynamo bytecode transform time: 4.42 s
(EngineCore_DP0 pid=4136162) INFO 01-23 00:05:04 [backends.py:261] Cache the graph of compile range (1, 145) for later use
(EngineCore_DP0 pid=4136162) INFO 01-23 00:05:22 [backends.py:278] Compiling a graph for compile range (1, 145) takes 19.06 s
(EngineCore_DP0 pid=4136162) INFO 01-23 00:05:22 [monitor.py:34] torch.compile takes 23.49 s in total
(EngineCore_DP0 pid=4136162) INFO 01-23 00:05:23 [gpu_worker.py:375] Available KV cache memory: 10.63 GiB
(EngineCore_DP0 pid=4136162) INFO 01-23 00:05:23 [kv_cache_utils.py:1291] GPU KV cache size: 398,032 tokens
(EngineCore_DP0 pid=4136162) INFO 01-23 00:05:23 [kv_cache_utils.py:1296] Maximum concurrency for 145 tokens per request: 2487.70x
(EngineCore_DP0 pid=4136162) INFO 01-23 00:05:24 [gpu_model_runner.py:4587] Graph capturing finished in 1 secs, took 0.01 GiB
(EngineCore_DP0 pid=4136162) INFO 01-23 00:05:24 [core.py:259] init engine (profile, create kv cache, warmup model) took 27.12 seconds
INFO 01-23 00:05:25 [llm.py:360] Supported tasks: ['generate']
Throughput: 124.40 requests/s, 2114.77 total tokens/s, 124.40 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  128


─── STDERR ───
[2026-01-23 00:04:52] INFO gemm_wrapper.py:362: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-23 00:04:52] INFO gemm_wrapper.py:408: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-23 00:04:52] INFO gemm_wrapper.py:455: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-23 00:04:52] INFO gemm_wrapper.py:502: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-23 00:04:55] INFO gemm_wrapper.py:362: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-23 00:04:55] INFO gemm_wrapper.py:408: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-23 00:04:55] INFO gemm_wrapper.py:455: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-23 00:04:55] INFO gemm_wrapper.py:502: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=4136162) [2026-01-23 00:04:56] INFO SlideSparseLinearMethod_FP8.py:691: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=4136162) [2026-01-23 00:04:56] INFO gemm_wrapper.py:307: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=4136162) [2026-01-23 00:04:56] INFO kernels.py:223: FP8 quant+slide kernel loaded
(EngineCore_DP0 pid=4136162) [2026-01-23 00:04:56] INFO kernels.py:115: Dequant+bias kernel loaded
(EngineCore_DP0 pid=4136162) [2026-01-23 00:04:56] INFO SlideSparseLinearMethod_FP8.py:372: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=4136162) [2026-01-23 00:04:56] INFO SlideSparseLinearMethod_FP8.py:512: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=4136162) [2026-01-23 00:04:56] INFO SlideSparseLinearMethod_FP8.py:517: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=4136162) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=4136162) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.64it/s]
(EngineCore_DP0 pid=4136162) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.63it/s]
(EngineCore_DP0 pid=4136162) 
(EngineCore_DP0 pid=4136162) [2026-01-23 00:04:56] INFO SlideSparseLinearMethod_FP8.py:617: cuSPARSELt compression: [2048, 1536] -> 1D uint8
(EngineCore_DP0 pid=4136162) [2026-01-23 00:04:56] INFO SlideSparseLinearMethod_FP8.py:627: cuSPARSELt compression done: 1966080 bytes
(EngineCore_DP0 pid=4136162) [2026-01-23 00:04:56] INFO SlideSparseLinearMethod_FP8.py:617: cuSPARSELt compression: [1536, 1536] -> 1D uint8
(EngineCore_DP0 pid=4136162) [2026-01-23 00:04:56] INFO SlideSparseLinearMethod_FP8.py:627: cuSPARSELt compression done: 1474560 bytes
(EngineCore_DP0 pid=4136162) [2026-01-23 00:04:56] INFO SlideSparseLinearMethod_FP8.py:617: cuSPARSELt compression: [17920, 1536] -> 1D uint8
(EngineCore_DP0 pid=4136162) [2026-01-23 00:04:56] INFO SlideSparseLinearMethod_FP8.py:627: cuSPARSELt compression done: 17203200 bytes
(EngineCore_DP0 pid=4136162) [2026-01-23 00:04:56] INFO SlideSparseLinearMethod_FP8.py:617: cuSPARSELt compression: [1536, 8960] -> 1D uint8
(EngineCore_DP0 pid=4136162) [2026-01-23 00:04:56] INFO SlideSparseLinearMethod_FP8.py:627: cuSPARSELt compression done: 8601600 bytes
(EngineCore_DP0 pid=4136162) 2026-01-23 00:05:23,690 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=4136162) 2026-01-23 00:05:23,697 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=4136162) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 45.50it/s]
(EngineCore_DP0 pid=4136162) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.73it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.73it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  62%|██████▎   | 80/128 [00:00<00:00, 539.79it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 828.14it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  21%|██        | 27/128 [00:00<00:00, 260.24it/s, est. speed input: 4164.12 toks/s, output: 260.24 toks/s]
Processed prompts:  42%|████▏     | 54/128 [00:00<00:00, 166.04it/s, est. speed input: 2809.21 toks/s, output: 175.57 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:00<00:00, 151.98it/s, est. speed input: 2592.85 toks/s, output: 162.05 toks/s]
Processed prompts:  70%|███████   | 90/128 [00:00<00:00, 144.72it/s, est. speed input: 2483.80 toks/s, output: 155.24 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:00<00:00, 140.42it/s, est. speed input: 2419.36 toks/s, output: 151.21 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:00<00:00, 136.79it/s, est. speed input: 2367.65 toks/s, output: 147.98 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 136.79it/s, est. speed input: 2347.28 toks/s, output: 146.70 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 146.69it/s, est. speed input: 2347.28 toks/s, output: 146.70 toks/s]
[rank0]:[W123 00:05:26.947279308 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 39.0s

测试结果:
  Requests/s:   124.40
  Tokens/s:     2114.77
  Total Reqs:   128
  Elapsed:      1.03s

  [Prefill 分析]
  Total Prefill Tokens: 2048
  Prefill Tokens/s:     1990.37

============================================================
[2/3] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-1.5B-FP8
│ Backend:  cuSPARSELt (2:4)
│ 阶段:     prefill
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度:
│   M_prefill = 128 (= 1 x 128)
│   M_decode  = 1
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len       = 128
│   --output-len      = 1
│   --num-prompts     = 128
│   --max-num-seqs    = 1
│   --max-model-len   = 257
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
INFO 01-23 00:05:31 [datasets.py:612] Sampling input_len from [128, 128] and output_len from [1, 1]
INFO 01-23 00:05:31 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints_slidesparse/Qwen2.5-1.5B-FP8-SlideSparse-2_4', 'max_model_len': 257, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 257, 'max_num_seqs': 1, 'disable_log_stats': True, 'enable_lora': None, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints_slidesparse/Qwen2.5-1.5B-FP8-SlideSparse-2_4'}
INFO 01-23 00:05:31 [model.py:514] Resolved architecture: Qwen2ForCausalLM
INFO 01-23 00:05:31 [model.py:1661] Using max model len 257
INFO 01-23 00:05:31 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=257.
(EngineCore_DP0 pid=4137589) INFO 01-23 00:05:34 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints_slidesparse/Qwen2.5-1.5B-FP8-SlideSparse-2_4', speculative_config=None, tokenizer='/root/vllmbench/checkpoints_slidesparse/Qwen2.5-1.5B-FP8-SlideSparse-2_4', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=257, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints_slidesparse/Qwen2.5-1.5B-FP8-SlideSparse-2_4, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [257], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 2, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=4137589) INFO 01-23 00:05:35 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:44689 backend=nccl
(EngineCore_DP0 pid=4137589) INFO 01-23 00:05:35 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=4137589) INFO 01-23 00:05:35 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-1.5B-FP8-SlideSparse-2_4...
(EngineCore_DP0 pid=4137589) INFO 01-23 00:05:35 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=4137589) INFO 01-23 00:05:35 [default_loader.py:308] Loading weights took 0.20 seconds
(EngineCore_DP0 pid=4137589) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=4137589) INFO 01-23 00:05:36 [gpu_model_runner.py:3659] Model loading took 1.2166 GiB memory and 0.439737 seconds
(EngineCore_DP0 pid=4137589) INFO 01-23 00:05:40 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/0466aeca23/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=4137589) INFO 01-23 00:05:40 [backends.py:703] Dynamo bytecode transform time: 4.43 s
(EngineCore_DP0 pid=4137589) INFO 01-23 00:05:43 [backends.py:261] Cache the graph of compile range (1, 257) for later use
(EngineCore_DP0 pid=4137589) INFO 01-23 00:06:01 [backends.py:278] Compiling a graph for compile range (1, 257) takes 18.70 s
(EngineCore_DP0 pid=4137589) INFO 01-23 00:06:01 [monitor.py:34] torch.compile takes 23.13 s in total
(EngineCore_DP0 pid=4137589) INFO 01-23 00:06:02 [gpu_worker.py:375] Available KV cache memory: 10.63 GiB
(EngineCore_DP0 pid=4137589) INFO 01-23 00:06:02 [kv_cache_utils.py:1291] GPU KV cache size: 398,000 tokens
(EngineCore_DP0 pid=4137589) INFO 01-23 00:06:02 [kv_cache_utils.py:1296] Maximum concurrency for 257 tokens per request: 1463.24x
(EngineCore_DP0 pid=4137589) INFO 01-23 00:06:02 [gpu_model_runner.py:4587] Graph capturing finished in 1 secs, took 0.01 GiB
(EngineCore_DP0 pid=4137589) INFO 01-23 00:06:02 [core.py:259] init engine (profile, create kv cache, warmup model) took 26.70 seconds
INFO 01-23 00:06:03 [llm.py:360] Supported tasks: ['generate']
Throughput: 121.97 requests/s, 15734.58 total tokens/s, 121.97 output tokens/s
Total num prompt tokens:  16384
Total num output tokens:  128


─── STDERR ───
[2026-01-23 00:05:31] INFO gemm_wrapper.py:362: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-23 00:05:31] INFO gemm_wrapper.py:408: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-23 00:05:31] INFO gemm_wrapper.py:455: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-23 00:05:31] INFO gemm_wrapper.py:502: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-23 00:05:34] INFO gemm_wrapper.py:362: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-23 00:05:34] INFO gemm_wrapper.py:408: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-23 00:05:34] INFO gemm_wrapper.py:455: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-23 00:05:34] INFO gemm_wrapper.py:502: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=4137589) [2026-01-23 00:05:35] INFO SlideSparseLinearMethod_FP8.py:691: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=4137589) [2026-01-23 00:05:35] INFO gemm_wrapper.py:307: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=4137589) [2026-01-23 00:05:35] INFO kernels.py:223: FP8 quant+slide kernel loaded
(EngineCore_DP0 pid=4137589) [2026-01-23 00:05:35] INFO kernels.py:115: Dequant+bias kernel loaded
(EngineCore_DP0 pid=4137589) [2026-01-23 00:05:35] INFO SlideSparseLinearMethod_FP8.py:372: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=4137589) [2026-01-23 00:05:35] INFO SlideSparseLinearMethod_FP8.py:512: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=4137589) [2026-01-23 00:05:35] INFO SlideSparseLinearMethod_FP8.py:517: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=4137589) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=4137589) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.72it/s]
(EngineCore_DP0 pid=4137589) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.72it/s]
(EngineCore_DP0 pid=4137589) 
(EngineCore_DP0 pid=4137589) [2026-01-23 00:05:35] INFO SlideSparseLinearMethod_FP8.py:617: cuSPARSELt compression: [2048, 1536] -> 1D uint8
(EngineCore_DP0 pid=4137589) [2026-01-23 00:05:35] INFO SlideSparseLinearMethod_FP8.py:627: cuSPARSELt compression done: 1966080 bytes
(EngineCore_DP0 pid=4137589) [2026-01-23 00:05:35] INFO SlideSparseLinearMethod_FP8.py:617: cuSPARSELt compression: [1536, 1536] -> 1D uint8
(EngineCore_DP0 pid=4137589) [2026-01-23 00:05:35] INFO SlideSparseLinearMethod_FP8.py:627: cuSPARSELt compression done: 1474560 bytes
(EngineCore_DP0 pid=4137589) [2026-01-23 00:05:35] INFO SlideSparseLinearMethod_FP8.py:617: cuSPARSELt compression: [17920, 1536] -> 1D uint8
(EngineCore_DP0 pid=4137589) [2026-01-23 00:05:35] INFO SlideSparseLinearMethod_FP8.py:627: cuSPARSELt compression done: 17203200 bytes
(EngineCore_DP0 pid=4137589) [2026-01-23 00:05:35] INFO SlideSparseLinearMethod_FP8.py:617: cuSPARSELt compression: [1536, 8960] -> 1D uint8
(EngineCore_DP0 pid=4137589) [2026-01-23 00:05:35] INFO SlideSparseLinearMethod_FP8.py:627: cuSPARSELt compression done: 8601600 bytes
(EngineCore_DP0 pid=4137589) 2026-01-23 00:06:02,286 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=4137589) 2026-01-23 00:06:02,294 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=4137589) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 46.11it/s]
(EngineCore_DP0 pid=4137589) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  8.52it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  8.51it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  62%|██████▎   | 80/128 [00:00<00:00, 482.36it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 695.34it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  23%|██▎       | 30/128 [00:00<00:00, 283.47it/s, est. speed input: 36290.59 toks/s, output: 283.49 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:00<00:00, 167.30it/s, est. speed input: 22844.24 toks/s, output: 178.46 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:00<00:00, 151.34it/s, est. speed input: 20857.38 toks/s, output: 162.95 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:00<00:00, 143.53it/s, est. speed input: 19912.52 toks/s, output: 155.56 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:00<00:00, 139.33it/s, est. speed input: 19385.32 toks/s, output: 151.45 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:00<00:00, 136.27it/s, est. speed input: 18995.03 toks/s, output: 148.40 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 136.27it/s, est. speed input: 18950.92 toks/s, output: 148.05 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 148.03it/s, est. speed input: 18950.92 toks/s, output: 148.05 toks/s]
[rank0]:[W123 00:06:04.485122779 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 38.5s

测试结果:
  Requests/s:   121.97
  Tokens/s:     15734.58
  Total Reqs:   128
  Elapsed:      1.05s

  [Prefill 分析]
  Total Prefill Tokens: 16384
  Prefill Tokens/s:     15612.61

============================================================
[3/3] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-1.5B-FP8
│ Backend:  cuSPARSELt (2:4)
│ 阶段:     prefill
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度:
│   M_prefill = 256 (= 1 x 256)
│   M_decode  = 1
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len       = 256
│   --output-len      = 1
│   --num-prompts     = 128
│   --max-num-seqs    = 1
│   --max-model-len   = 385
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
INFO 01-23 00:06:09 [datasets.py:612] Sampling input_len from [256, 256] and output_len from [1, 1]
INFO 01-23 00:06:09 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints_slidesparse/Qwen2.5-1.5B-FP8-SlideSparse-2_4', 'max_model_len': 385, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 385, 'max_num_seqs': 1, 'disable_log_stats': True, 'enable_lora': None, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints_slidesparse/Qwen2.5-1.5B-FP8-SlideSparse-2_4'}
INFO 01-23 00:06:09 [model.py:514] Resolved architecture: Qwen2ForCausalLM
INFO 01-23 00:06:09 [model.py:1661] Using max model len 385
INFO 01-23 00:06:09 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=385.
(EngineCore_DP0 pid=4139020) INFO 01-23 00:06:13 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints_slidesparse/Qwen2.5-1.5B-FP8-SlideSparse-2_4', speculative_config=None, tokenizer='/root/vllmbench/checkpoints_slidesparse/Qwen2.5-1.5B-FP8-SlideSparse-2_4', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=385, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints_slidesparse/Qwen2.5-1.5B-FP8-SlideSparse-2_4, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [385], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 2, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=4139020) INFO 01-23 00:06:13 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:51535 backend=nccl
(EngineCore_DP0 pid=4139020) INFO 01-23 00:06:13 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=4139020) INFO 01-23 00:06:14 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-1.5B-FP8-SlideSparse-2_4...
(EngineCore_DP0 pid=4139020) INFO 01-23 00:06:14 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=4139020) INFO 01-23 00:06:14 [default_loader.py:308] Loading weights took 0.20 seconds
(EngineCore_DP0 pid=4139020) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=4139020) INFO 01-23 00:06:14 [gpu_model_runner.py:3659] Model loading took 1.2166 GiB memory and 0.439459 seconds
(EngineCore_DP0 pid=4139020) INFO 01-23 00:06:19 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/a42a74eaec/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=4139020) INFO 01-23 00:06:19 [backends.py:703] Dynamo bytecode transform time: 4.45 s
(EngineCore_DP0 pid=4139020) INFO 01-23 00:06:22 [backends.py:261] Cache the graph of compile range (1, 385) for later use
(EngineCore_DP0 pid=4139020) INFO 01-23 00:06:40 [backends.py:278] Compiling a graph for compile range (1, 385) takes 18.86 s
(EngineCore_DP0 pid=4139020) INFO 01-23 00:06:40 [monitor.py:34] torch.compile takes 23.31 s in total
(EngineCore_DP0 pid=4139020) INFO 01-23 00:06:40 [gpu_worker.py:375] Available KV cache memory: 10.63 GiB
(EngineCore_DP0 pid=4139020) INFO 01-23 00:06:41 [kv_cache_utils.py:1291] GPU KV cache size: 397,936 tokens
(EngineCore_DP0 pid=4139020) INFO 01-23 00:06:41 [kv_cache_utils.py:1296] Maximum concurrency for 385 tokens per request: 994.84x
(EngineCore_DP0 pid=4139020) INFO 01-23 00:06:41 [gpu_model_runner.py:4587] Graph capturing finished in 1 secs, took -0.01 GiB
(EngineCore_DP0 pid=4139020) INFO 01-23 00:06:41 [core.py:259] init engine (profile, create kv cache, warmup model) took 26.90 seconds
INFO 01-23 00:06:42 [llm.py:360] Supported tasks: ['generate']
Throughput: 112.05 requests/s, 28797.38 total tokens/s, 112.05 output tokens/s
Total num prompt tokens:  32768
Total num output tokens:  128


─── STDERR ───
[2026-01-23 00:06:09] INFO gemm_wrapper.py:362: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-23 00:06:09] INFO gemm_wrapper.py:408: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-23 00:06:09] INFO gemm_wrapper.py:455: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-23 00:06:09] INFO gemm_wrapper.py:502: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-23 00:06:13] INFO gemm_wrapper.py:362: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-23 00:06:13] INFO gemm_wrapper.py:408: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-23 00:06:13] INFO gemm_wrapper.py:455: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-23 00:06:13] INFO gemm_wrapper.py:502: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=4139020) [2026-01-23 00:06:14] INFO SlideSparseLinearMethod_FP8.py:691: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=4139020) [2026-01-23 00:06:14] INFO gemm_wrapper.py:307: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=4139020) [2026-01-23 00:06:14] INFO kernels.py:223: FP8 quant+slide kernel loaded
(EngineCore_DP0 pid=4139020) [2026-01-23 00:06:14] INFO kernels.py:115: Dequant+bias kernel loaded
(EngineCore_DP0 pid=4139020) [2026-01-23 00:06:14] INFO SlideSparseLinearMethod_FP8.py:372: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=4139020) [2026-01-23 00:06:14] INFO SlideSparseLinearMethod_FP8.py:512: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=4139020) [2026-01-23 00:06:14] INFO SlideSparseLinearMethod_FP8.py:517: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=4139020) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=4139020) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.69it/s]
(EngineCore_DP0 pid=4139020) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.69it/s]
(EngineCore_DP0 pid=4139020) 
(EngineCore_DP0 pid=4139020) [2026-01-23 00:06:14] INFO SlideSparseLinearMethod_FP8.py:617: cuSPARSELt compression: [2048, 1536] -> 1D uint8
(EngineCore_DP0 pid=4139020) [2026-01-23 00:06:14] INFO SlideSparseLinearMethod_FP8.py:627: cuSPARSELt compression done: 1966080 bytes
(EngineCore_DP0 pid=4139020) [2026-01-23 00:06:14] INFO SlideSparseLinearMethod_FP8.py:617: cuSPARSELt compression: [1536, 1536] -> 1D uint8
(EngineCore_DP0 pid=4139020) [2026-01-23 00:06:14] INFO SlideSparseLinearMethod_FP8.py:627: cuSPARSELt compression done: 1474560 bytes
(EngineCore_DP0 pid=4139020) [2026-01-23 00:06:14] INFO SlideSparseLinearMethod_FP8.py:617: cuSPARSELt compression: [17920, 1536] -> 1D uint8
(EngineCore_DP0 pid=4139020) [2026-01-23 00:06:14] INFO SlideSparseLinearMethod_FP8.py:627: cuSPARSELt compression done: 17203200 bytes
(EngineCore_DP0 pid=4139020) [2026-01-23 00:06:14] INFO SlideSparseLinearMethod_FP8.py:617: cuSPARSELt compression: [1536, 8960] -> 1D uint8
(EngineCore_DP0 pid=4139020) [2026-01-23 00:06:14] INFO SlideSparseLinearMethod_FP8.py:627: cuSPARSELt compression done: 8601600 bytes
(EngineCore_DP0 pid=4139020) 2026-01-23 00:06:41,085 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=4139020) 2026-01-23 00:06:41,093 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=4139020) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 46.78it/s]
(EngineCore_DP0 pid=4139020) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  8.45it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  8.44it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 1495.64it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:02, 49.47it/s, est. speed input: 12665.63 toks/s, output: 49.47 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:00<00:01, 98.87it/s, est. speed input: 23462.96 toks/s, output: 91.65 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:00<00:00, 113.53it/s, est. speed input: 26851.34 toks/s, output: 104.89 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:00<00:00, 120.18it/s, est. speed input: 28501.34 toks/s, output: 111.33 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:00<00:00, 119.85it/s, est. speed input: 28891.64 toks/s, output: 112.85 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:00<00:00, 121.42it/s, est. speed input: 29385.87 toks/s, output: 114.79 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:00<00:00, 124.47it/s, est. speed input: 29975.69 toks/s, output: 117.09 toks/s]
Processed prompts:  78%|███████▊  | 100/128 [00:00<00:00, 126.76it/s, est. speed input: 30445.24 toks/s, output: 118.93 toks/s]
Processed prompts:  89%|████████▉ | 114/128 [00:00<00:00, 128.46it/s, est. speed input: 30824.95 toks/s, output: 120.41 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:01<00:00, 128.72it/s, est. speed input: 31044.47 toks/s, output: 121.27 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 128.72it/s, est. speed input: 31063.75 toks/s, output: 121.34 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 121.32it/s, est. speed input: 31063.75 toks/s, output: 121.34 toks/s]
[rank0]:[W123 00:06:43.396705914 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 39.0s

测试结果:
  Requests/s:   112.05
  Tokens/s:     28797.38
  Total Reqs:   128
  Elapsed:      1.14s

  [Prefill 分析]
  Total Prefill Tokens: 32768
  Prefill Tokens/s:     28685.33


------------------------------------------------------------
  生成 CSV: Qwen2.5-1.5B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_4/Qwen2.5-1.5B-FP8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
16,16,1,128,128,124.3984,2114.7728,1.0290
128,128,1,128,128,121.9735,15734.5807,1.0494
256,256,1,128,128,112.0521,28797.3797,1.1423

------------------------------------------------------------

[INFO] 完成: 3 成功, 0 失败

============================================================
  Qwen2.5-1.5B-FP8 | cuSPARSELt (2_4) | decode
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-1.5B-FP8-SlideSparse-2_4
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_4

============================================================
[1/3] 测试 M=16
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-1.5B-FP8
│ Backend:  cuSPARSELt (2:4)
│ 阶段:     decode
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度:
│   M_prefill = 256 (= 16 x 16)
│   M_decode  = 16
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len       = 16
│   --output-len      = 256
│   --num-prompts     = 16
│   --max-num-seqs    = 16
│   --max-model-len   = 400
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
INFO 01-23 00:06:48 [datasets.py:612] Sampling input_len from [16, 16] and output_len from [256, 256]
INFO 01-23 00:06:48 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints_slidesparse/Qwen2.5-1.5B-FP8-SlideSparse-2_4', 'max_model_len': 400, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 6400, 'max_num_seqs': 16, 'disable_log_stats': True, 'enable_lora': None, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints_slidesparse/Qwen2.5-1.5B-FP8-SlideSparse-2_4'}
INFO 01-23 00:06:48 [model.py:514] Resolved architecture: Qwen2ForCausalLM
INFO 01-23 00:06:48 [model.py:1661] Using max model len 400
INFO 01-23 00:06:48 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=6400.
(EngineCore_DP0 pid=4140449) INFO 01-23 00:06:52 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints_slidesparse/Qwen2.5-1.5B-FP8-SlideSparse-2_4', speculative_config=None, tokenizer='/root/vllmbench/checkpoints_slidesparse/Qwen2.5-1.5B-FP8-SlideSparse-2_4', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=400, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints_slidesparse/Qwen2.5-1.5B-FP8-SlideSparse-2_4, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [6400], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 32, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=4140449) INFO 01-23 00:06:52 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:60389 backend=nccl
(EngineCore_DP0 pid=4140449) INFO 01-23 00:06:52 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=4140449) INFO 01-23 00:06:52 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-1.5B-FP8-SlideSparse-2_4...
(EngineCore_DP0 pid=4140449) INFO 01-23 00:06:53 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=4140449) INFO 01-23 00:06:53 [default_loader.py:308] Loading weights took 0.21 seconds
(EngineCore_DP0 pid=4140449) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=4140449) INFO 01-23 00:06:53 [gpu_model_runner.py:3659] Model loading took 1.2166 GiB memory and 0.439826 seconds
(EngineCore_DP0 pid=4140449) INFO 01-23 00:06:58 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/856a447cb2/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=4140449) INFO 01-23 00:06:58 [backends.py:703] Dynamo bytecode transform time: 4.41 s
(EngineCore_DP0 pid=4140449) INFO 01-23 00:07:01 [backends.py:261] Cache the graph of compile range (1, 6400) for later use
(EngineCore_DP0 pid=4140449) INFO 01-23 00:07:18 [backends.py:278] Compiling a graph for compile range (1, 6400) takes 18.85 s
(EngineCore_DP0 pid=4140449) INFO 01-23 00:07:18 [monitor.py:34] torch.compile takes 23.26 s in total
(EngineCore_DP0 pid=4140449) INFO 01-23 00:07:19 [gpu_worker.py:375] Available KV cache memory: 10.56 GiB
(EngineCore_DP0 pid=4140449) INFO 01-23 00:07:19 [kv_cache_utils.py:1291] GPU KV cache size: 395,632 tokens
(EngineCore_DP0 pid=4140449) INFO 01-23 00:07:19 [kv_cache_utils.py:1296] Maximum concurrency for 400 tokens per request: 989.08x
(EngineCore_DP0 pid=4140449) INFO 01-23 00:07:20 [gpu_model_runner.py:4587] Graph capturing finished in 1 secs, took -0.65 GiB
(EngineCore_DP0 pid=4140449) INFO 01-23 00:07:20 [core.py:259] init engine (profile, create kv cache, warmup model) took 26.99 seconds
INFO 01-23 00:07:21 [llm.py:360] Supported tasks: ['generate']
Throughput: 11.85 requests/s, 3222.83 total tokens/s, 3033.25 output tokens/s
Total num prompt tokens:  256
Total num output tokens:  4096


─── STDERR ───
[2026-01-23 00:06:48] INFO gemm_wrapper.py:362: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-23 00:06:48] INFO gemm_wrapper.py:408: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-23 00:06:48] INFO gemm_wrapper.py:455: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-23 00:06:48] INFO gemm_wrapper.py:502: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-23 00:06:52] INFO gemm_wrapper.py:362: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-23 00:06:52] INFO gemm_wrapper.py:408: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-23 00:06:52] INFO gemm_wrapper.py:455: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-23 00:06:52] INFO gemm_wrapper.py:502: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=4140449) [2026-01-23 00:06:53] INFO SlideSparseLinearMethod_FP8.py:691: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=4140449) [2026-01-23 00:06:53] INFO gemm_wrapper.py:307: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=4140449) [2026-01-23 00:06:53] INFO kernels.py:223: FP8 quant+slide kernel loaded
(EngineCore_DP0 pid=4140449) [2026-01-23 00:06:53] INFO kernels.py:115: Dequant+bias kernel loaded
(EngineCore_DP0 pid=4140449) [2026-01-23 00:06:53] INFO SlideSparseLinearMethod_FP8.py:372: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=4140449) [2026-01-23 00:06:53] INFO SlideSparseLinearMethod_FP8.py:512: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=4140449) [2026-01-23 00:06:53] INFO SlideSparseLinearMethod_FP8.py:517: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=4140449) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=4140449) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.64it/s]
(EngineCore_DP0 pid=4140449) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.63it/s]
(EngineCore_DP0 pid=4140449) 
(EngineCore_DP0 pid=4140449) [2026-01-23 00:06:53] INFO SlideSparseLinearMethod_FP8.py:617: cuSPARSELt compression: [2048, 1536] -> 1D uint8
(EngineCore_DP0 pid=4140449) [2026-01-23 00:06:53] INFO SlideSparseLinearMethod_FP8.py:627: cuSPARSELt compression done: 1966080 bytes
(EngineCore_DP0 pid=4140449) [2026-01-23 00:06:53] INFO SlideSparseLinearMethod_FP8.py:617: cuSPARSELt compression: [1536, 1536] -> 1D uint8
(EngineCore_DP0 pid=4140449) [2026-01-23 00:06:53] INFO SlideSparseLinearMethod_FP8.py:627: cuSPARSELt compression done: 1474560 bytes
(EngineCore_DP0 pid=4140449) [2026-01-23 00:06:53] INFO SlideSparseLinearMethod_FP8.py:617: cuSPARSELt compression: [17920, 1536] -> 1D uint8
(EngineCore_DP0 pid=4140449) [2026-01-23 00:06:53] INFO SlideSparseLinearMethod_FP8.py:627: cuSPARSELt compression done: 17203200 bytes
(EngineCore_DP0 pid=4140449) [2026-01-23 00:06:53] INFO SlideSparseLinearMethod_FP8.py:617: cuSPARSELt compression: [1536, 8960] -> 1D uint8
(EngineCore_DP0 pid=4140449) [2026-01-23 00:06:53] INFO SlideSparseLinearMethod_FP8.py:627: cuSPARSELt compression done: 8601600 bytes
(EngineCore_DP0 pid=4140449) [rank0]:W0123 00:07:02.469000 4140449 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=4140449) [rank0]:W0123 00:07:02.606000 4140449 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=4140449) [rank0]:W0123 00:07:18.870000 4140449 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=4140449) [rank0]:W0123 00:07:18.925000 4140449 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=4140449) 2026-01-23 00:07:19,950 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=4140449) 2026-01-23 00:07:19,958 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=4140449) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 5/7 [00:00<00:00, 47.33it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 45.40it/s]
(EngineCore_DP0 pid=4140449) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 1/5 [00:00<00:00,  8.39it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 26.73it/s]

Adding requests:   0%|          | 0/16 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 16/16 [00:00<00:00, 6042.58it/s]

Processed prompts:   0%|          | 0/16 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   6%|▋         | 1/16 [00:01<00:20,  1.34s/it, est. speed input: 11.93 toks/s, output: 190.81 toks/s]
Processed prompts: 100%|██████████| 16/16 [00:01<00:00,  1.34s/it, est. speed input: 190.05 toks/s, output: 3040.76 toks/s]
Processed prompts: 100%|██████████| 16/16 [00:01<00:00, 11.88it/s, est. speed input: 190.05 toks/s, output: 3040.76 toks/s]
[rank0]:[W123 00:07:22.650700048 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 39.3s

测试结果:
  Requests/s:   11.85
  Tokens/s:     3222.83
  Total Reqs:   16
  Elapsed:      1.35s

  [Decode 分析]
  Total Decode Tokens:  4096
  Decode Tokens/s:      3033.25

============================================================
[2/3] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-1.5B-FP8
│ Backend:  cuSPARSELt (2:4)
│ 阶段:     decode
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度:
│   M_prefill = 2048 (= 128 x 16)
│   M_decode  = 128
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len       = 16
│   --output-len      = 256
│   --num-prompts     = 128
│   --max-num-seqs    = 128
│   --max-model-len   = 400
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
INFO 01-23 00:07:27 [datasets.py:612] Sampling input_len from [16, 16] and output_len from [256, 256]
INFO 01-23 00:07:27 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints_slidesparse/Qwen2.5-1.5B-FP8-SlideSparse-2_4', 'max_model_len': 400, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 51200, 'max_num_seqs': 128, 'disable_log_stats': True, 'enable_lora': None, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints_slidesparse/Qwen2.5-1.5B-FP8-SlideSparse-2_4'}
INFO 01-23 00:07:27 [model.py:514] Resolved architecture: Qwen2ForCausalLM
INFO 01-23 00:07:27 [model.py:1661] Using max model len 400
INFO 01-23 00:07:28 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=51200.
(EngineCore_DP0 pid=4141920) INFO 01-23 00:07:31 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints_slidesparse/Qwen2.5-1.5B-FP8-SlideSparse-2_4', speculative_config=None, tokenizer='/root/vllmbench/checkpoints_slidesparse/Qwen2.5-1.5B-FP8-SlideSparse-2_4', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=400, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints_slidesparse/Qwen2.5-1.5B-FP8-SlideSparse-2_4, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [51200], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 256, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=4141920) INFO 01-23 00:07:31 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:59591 backend=nccl
(EngineCore_DP0 pid=4141920) INFO 01-23 00:07:31 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=4141920) INFO 01-23 00:07:32 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-1.5B-FP8-SlideSparse-2_4...
(EngineCore_DP0 pid=4141920) INFO 01-23 00:07:32 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=4141920) INFO 01-23 00:07:32 [default_loader.py:308] Loading weights took 0.21 seconds
(EngineCore_DP0 pid=4141920) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=4141920) INFO 01-23 00:07:33 [gpu_model_runner.py:3659] Model loading took 1.2166 GiB memory and 0.472189 seconds
(EngineCore_DP0 pid=4141920) INFO 01-23 00:07:37 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/a8ee922c3f/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=4141920) INFO 01-23 00:07:37 [backends.py:703] Dynamo bytecode transform time: 4.43 s
(EngineCore_DP0 pid=4141920) INFO 01-23 00:07:40 [backends.py:261] Cache the graph of compile range (1, 51200) for later use
(EngineCore_DP0 pid=4141920) INFO 01-23 00:07:58 [backends.py:278] Compiling a graph for compile range (1, 51200) takes 18.93 s
(EngineCore_DP0 pid=4141920) INFO 01-23 00:07:58 [monitor.py:34] torch.compile takes 23.36 s in total
(EngineCore_DP0 pid=4141920) INFO 01-23 00:07:59 [gpu_worker.py:375] Available KV cache memory: 7.06 GiB
(EngineCore_DP0 pid=4141920) INFO 01-23 00:07:59 [kv_cache_utils.py:1291] GPU KV cache size: 264,352 tokens
(EngineCore_DP0 pid=4141920) INFO 01-23 00:07:59 [kv_cache_utils.py:1296] Maximum concurrency for 400 tokens per request: 660.88x
(EngineCore_DP0 pid=4141920) INFO 01-23 00:08:02 [gpu_model_runner.py:4587] Graph capturing finished in 3 secs, took -5.75 GiB
(EngineCore_DP0 pid=4141920) INFO 01-23 00:08:02 [core.py:259] init engine (profile, create kv cache, warmup model) took 28.90 seconds
INFO 01-23 00:08:02 [llm.py:360] Supported tasks: ['generate']
Throughput: 64.23 requests/s, 17470.24 total tokens/s, 16442.58 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768


─── STDERR ───
[2026-01-23 00:07:27] INFO gemm_wrapper.py:362: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-23 00:07:27] INFO gemm_wrapper.py:408: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-23 00:07:27] INFO gemm_wrapper.py:455: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-23 00:07:27] INFO gemm_wrapper.py:502: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-23 00:07:31] INFO gemm_wrapper.py:362: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-23 00:07:31] INFO gemm_wrapper.py:408: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-23 00:07:31] INFO gemm_wrapper.py:455: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-23 00:07:31] INFO gemm_wrapper.py:502: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=4141920) [2026-01-23 00:07:32] INFO SlideSparseLinearMethod_FP8.py:691: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=4141920) [2026-01-23 00:07:32] INFO gemm_wrapper.py:307: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=4141920) [2026-01-23 00:07:32] INFO kernels.py:223: FP8 quant+slide kernel loaded
(EngineCore_DP0 pid=4141920) [2026-01-23 00:07:32] INFO kernels.py:115: Dequant+bias kernel loaded
(EngineCore_DP0 pid=4141920) [2026-01-23 00:07:32] INFO SlideSparseLinearMethod_FP8.py:372: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=4141920) [2026-01-23 00:07:32] INFO SlideSparseLinearMethod_FP8.py:512: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=4141920) [2026-01-23 00:07:32] INFO SlideSparseLinearMethod_FP8.py:517: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=4141920) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=4141920) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.59it/s]
(EngineCore_DP0 pid=4141920) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.59it/s]
(EngineCore_DP0 pid=4141920) 
(EngineCore_DP0 pid=4141920) [2026-01-23 00:07:32] INFO SlideSparseLinearMethod_FP8.py:617: cuSPARSELt compression: [2048, 1536] -> 1D uint8
(EngineCore_DP0 pid=4141920) [2026-01-23 00:07:32] INFO SlideSparseLinearMethod_FP8.py:627: cuSPARSELt compression done: 1966080 bytes
(EngineCore_DP0 pid=4141920) [2026-01-23 00:07:32] INFO SlideSparseLinearMethod_FP8.py:617: cuSPARSELt compression: [1536, 1536] -> 1D uint8
(EngineCore_DP0 pid=4141920) [2026-01-23 00:07:32] INFO SlideSparseLinearMethod_FP8.py:627: cuSPARSELt compression done: 1474560 bytes
(EngineCore_DP0 pid=4141920) [2026-01-23 00:07:32] INFO SlideSparseLinearMethod_FP8.py:617: cuSPARSELt compression: [17920, 1536] -> 1D uint8
(EngineCore_DP0 pid=4141920) [2026-01-23 00:07:32] INFO SlideSparseLinearMethod_FP8.py:627: cuSPARSELt compression done: 17203200 bytes
(EngineCore_DP0 pid=4141920) [2026-01-23 00:07:32] INFO SlideSparseLinearMethod_FP8.py:617: cuSPARSELt compression: [1536, 8960] -> 1D uint8
(EngineCore_DP0 pid=4141920) [2026-01-23 00:07:32] INFO SlideSparseLinearMethod_FP8.py:627: cuSPARSELt compression done: 8601600 bytes
(EngineCore_DP0 pid=4141920) [rank0]:W0123 00:07:40.284000 4141920 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=4141920) [rank0]:W0123 00:07:40.364000 4141920 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=4141920) [rank0]:W0123 00:07:40.424000 4141920 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=4141920) [rank0]:W0123 00:07:41.836000 4141920 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=4141920) [rank0]:W0123 00:07:41.862000 4141920 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=4141920) [rank0]:W0123 00:07:41.918000 4141920 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=4141920) [rank0]:W0123 00:07:41.947000 4141920 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=4141920) [rank0]:W0123 00:07:41.997000 4141920 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=4141920) [rank0]:W0123 00:07:42.032000 4141920 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=4141920) [rank0]:W0123 00:07:58.317000 4141920 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=4141920) [rank0]:W0123 00:07:58.328000 4141920 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=4141920) [rank0]:W0123 00:07:58.340000 4141920 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=4141920) [rank0]:W0123 00:07:58.383000 4141920 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=4141920) 2026-01-23 00:07:59,455 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=4141920) 2026-01-23 00:07:59,465 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=4141920) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|▎         | 1/35 [00:00<00:20,  1.68it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▊         | 3/35 [00:00<00:06,  4.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█▏        | 4/35 [00:00<00:05,  5.88it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 5/35 [00:00<00:04,  6.67it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 10/35 [00:01<00:01, 16.72it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 15/35 [00:01<00:00, 24.13it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 20/35 [00:01<00:00, 29.88it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 25/35 [00:01<00:00, 33.60it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 30/35 [00:01<00:00, 36.99it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:01<00:00, 39.08it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:01<00:00, 21.59it/s]
(EngineCore_DP0 pid=4141920) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   5%|▌         | 1/19 [00:00<00:02,  8.42it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 7/19 [00:00<00:00, 34.75it/s]
Capturing CUDA graphs (decode, FULL):  68%|██████▊   | 13/19 [00:00<00:00, 43.93it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:00<00:00, 48.93it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:00<00:00, 42.98it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 4241.99it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:01<04:07,  1.95s/it, est. speed input: 8.20 toks/s, output: 131.22 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00,  1.95s/it, est. speed input: 1044.43 toks/s, output: 16710.88 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 65.27it/s, est. speed input: 1044.43 toks/s, output: 16710.88 toks/s]
[rank0]:[W123 00:08:04.544139244 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 41.9s

测试结果:
  Requests/s:   64.23
  Tokens/s:     17470.24
  Total Reqs:   128
  Elapsed:      1.99s

  [Decode 分析]
  Total Decode Tokens:  32768
  Decode Tokens/s:      16442.58

============================================================
[3/3] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-1.5B-FP8
│ Backend:  cuSPARSELt (2:4)
│ 阶段:     decode
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度:
│   M_prefill = 4096 (= 256 x 16)
│   M_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len       = 16
│   --output-len      = 256
│   --num-prompts     = 256
│   --max-num-seqs    = 256
│   --max-model-len   = 400
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
INFO 01-23 00:08:09 [datasets.py:612] Sampling input_len from [16, 16] and output_len from [256, 256]
INFO 01-23 00:08:09 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints_slidesparse/Qwen2.5-1.5B-FP8-SlideSparse-2_4', 'max_model_len': 400, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 102400, 'max_num_seqs': 256, 'disable_log_stats': True, 'enable_lora': None, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints_slidesparse/Qwen2.5-1.5B-FP8-SlideSparse-2_4'}
INFO 01-23 00:08:09 [model.py:514] Resolved architecture: Qwen2ForCausalLM
INFO 01-23 00:08:09 [model.py:1661] Using max model len 400
INFO 01-23 00:08:10 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=102400.
(EngineCore_DP0 pid=4143377) INFO 01-23 00:08:13 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints_slidesparse/Qwen2.5-1.5B-FP8-SlideSparse-2_4', speculative_config=None, tokenizer='/root/vllmbench/checkpoints_slidesparse/Qwen2.5-1.5B-FP8-SlideSparse-2_4', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=400, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints_slidesparse/Qwen2.5-1.5B-FP8-SlideSparse-2_4, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [102400], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=4143377) INFO 01-23 00:08:13 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:33205 backend=nccl
(EngineCore_DP0 pid=4143377) INFO 01-23 00:08:13 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=4143377) INFO 01-23 00:08:14 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-1.5B-FP8-SlideSparse-2_4...
(EngineCore_DP0 pid=4143377) INFO 01-23 00:08:14 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=4143377) INFO 01-23 00:08:14 [default_loader.py:308] Loading weights took 0.20 seconds
(EngineCore_DP0 pid=4143377) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=4143377) INFO 01-23 00:08:15 [gpu_model_runner.py:3659] Model loading took 1.2166 GiB memory and 0.437394 seconds
(EngineCore_DP0 pid=4143377) INFO 01-23 00:08:19 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/0d9fd14f1c/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=4143377) INFO 01-23 00:08:19 [backends.py:703] Dynamo bytecode transform time: 4.43 s
(EngineCore_DP0 pid=4143377) INFO 01-23 00:08:22 [backends.py:261] Cache the graph of compile range (1, 102400) for later use
(EngineCore_DP0 pid=4143377) INFO 01-23 00:08:40 [backends.py:278] Compiling a graph for compile range (1, 102400) takes 19.03 s
(EngineCore_DP0 pid=4143377) INFO 01-23 00:08:40 [monitor.py:34] torch.compile takes 23.46 s in total
(EngineCore_DP0 pid=4143377) INFO 01-23 00:08:41 [gpu_worker.py:375] Available KV cache memory: 3.06 GiB
(EngineCore_DP0 pid=4143377) INFO 01-23 00:08:41 [kv_cache_utils.py:1291] GPU KV cache size: 114,400 tokens
(EngineCore_DP0 pid=4143377) INFO 01-23 00:08:41 [kv_cache_utils.py:1296] Maximum concurrency for 400 tokens per request: 286.00x
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 256, in _initialize_kv_caches
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]     self.model_executor.initialize_from_config(kv_cache_configs)
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 116, in initialize_from_config
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]     self.collective_rpc("compile_or_warm_up_model")
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 455, in compile_or_warm_up_model
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]     kernel_warmup(self)
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]   File "/root/vllmbench/vllm/model_executor/warmup/kernel_warmup.py", line 41, in kernel_warmup
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]     flashinfer_autotune(worker.model_runner)
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]   File "/root/vllmbench/vllm/model_executor/warmup/kernel_warmup.py", line 94, in flashinfer_autotune
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]     runner._dummy_run(
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]     hidden_states = self.model(
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]                     ^^^^^^^^^^^
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 439, in __call__
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]     return TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 223, in __call__
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]     def forward(
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]     raise e
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]   File "<eval_with_key>.58", line 334, in forward
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]     submod_4 = self.submod_4(getitem_10, s72, getitem_4, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_, getitem_5, l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_, getitem_11, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_10 = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_ = getitem_11 = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]     return range_entry.runnable(*args)
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]     return self._compiled_fn(*args)
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]     return compiled_fn(full_args)
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]                             ^^^^^^^
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 724, in inner_fn
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]     outs = compiled_fn(args)
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]            ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]     return self.current_callable(inputs)
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]     out = model(new_inputs)
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]   File "/tmp/torchinductor_root/lx/clxcnleawpqn5cck5buutm2y6ptizgofto4zdzrpxrmcpk6dyudu.py", line 2404, in call
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]     buf17 = empty_strided_cuda((s72, 17920), (17920, 1), torch.bfloat16)
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4143377) ERROR 01-23 00:08:41 [core.py:866] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.42 GiB. GPU 0 has a total capacity of 15.46 GiB of which 2.65 GiB is free. Including non-PyTorch memory, this process has 12.57 GiB memory in use. Of the allocated memory 8.88 GiB is allocated by PyTorch, and 3.31 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


─── STDERR ───
[2026-01-23 00:08:09] INFO gemm_wrapper.py:362: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-23 00:08:09] INFO gemm_wrapper.py:408: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-23 00:08:09] INFO gemm_wrapper.py:455: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-23 00:08:09] INFO gemm_wrapper.py:502: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-23 00:08:13] INFO gemm_wrapper.py:362: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-23 00:08:13] INFO gemm_wrapper.py:408: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-23 00:08:13] INFO gemm_wrapper.py:455: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-23 00:08:13] INFO gemm_wrapper.py:502: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=4143377) [2026-01-23 00:08:14] INFO SlideSparseLinearMethod_FP8.py:691: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=4143377) [2026-01-23 00:08:14] INFO gemm_wrapper.py:307: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=4143377) [2026-01-23 00:08:14] INFO kernels.py:223: FP8 quant+slide kernel loaded
(EngineCore_DP0 pid=4143377) [2026-01-23 00:08:14] INFO kernels.py:115: Dequant+bias kernel loaded
(EngineCore_DP0 pid=4143377) [2026-01-23 00:08:14] INFO SlideSparseLinearMethod_FP8.py:372: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=4143377) [2026-01-23 00:08:14] INFO SlideSparseLinearMethod_FP8.py:512: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=4143377) [2026-01-23 00:08:14] INFO SlideSparseLinearMethod_FP8.py:517: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=4143377) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=4143377) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.73it/s]
(EngineCore_DP0 pid=4143377) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.72it/s]
(EngineCore_DP0 pid=4143377) 
(EngineCore_DP0 pid=4143377) [2026-01-23 00:08:14] INFO SlideSparseLinearMethod_FP8.py:617: cuSPARSELt compression: [2048, 1536] -> 1D uint8
(EngineCore_DP0 pid=4143377) [2026-01-23 00:08:14] INFO SlideSparseLinearMethod_FP8.py:627: cuSPARSELt compression done: 1966080 bytes
(EngineCore_DP0 pid=4143377) [2026-01-23 00:08:14] INFO SlideSparseLinearMethod_FP8.py:617: cuSPARSELt compression: [1536, 1536] -> 1D uint8
(EngineCore_DP0 pid=4143377) [2026-01-23 00:08:14] INFO SlideSparseLinearMethod_FP8.py:627: cuSPARSELt compression done: 1474560 bytes
(EngineCore_DP0 pid=4143377) [2026-01-23 00:08:14] INFO SlideSparseLinearMethod_FP8.py:617: cuSPARSELt compression: [17920, 1536] -> 1D uint8
(EngineCore_DP0 pid=4143377) [2026-01-23 00:08:14] INFO SlideSparseLinearMethod_FP8.py:627: cuSPARSELt compression done: 17203200 bytes
(EngineCore_DP0 pid=4143377) [2026-01-23 00:08:14] INFO SlideSparseLinearMethod_FP8.py:617: cuSPARSELt compression: [1536, 8960] -> 1D uint8
(EngineCore_DP0 pid=4143377) [2026-01-23 00:08:14] INFO SlideSparseLinearMethod_FP8.py:627: cuSPARSELt compression done: 8601600 bytes
(EngineCore_DP0 pid=4143377) [rank0]:W0123 00:08:22.306000 4143377 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=4143377) [rank0]:W0123 00:08:22.385000 4143377 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=4143377) [rank0]:W0123 00:08:22.414000 4143377 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=4143377) [rank0]:W0123 00:08:22.431000 4143377 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=4143377) [rank0]:W0123 00:08:23.858000 4143377 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=4143377) [rank0]:W0123 00:08:23.883000 4143377 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=4143377) [rank0]:W0123 00:08:23.937000 4143377 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=4143377) [rank0]:W0123 00:08:23.967000 4143377 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=4143377) [rank0]:W0123 00:08:24.019000 4143377 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=4143377) [rank0]:W0123 00:08:24.050000 4143377 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=4143377) [rank0]:W0123 00:08:40.441000 4143377 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=4143377) [rank0]:W0123 00:08:40.452000 4143377 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=4143377) [rank0]:W0123 00:08:40.465000 4143377 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=4143377) [rank0]:W0123 00:08:40.510000 4143377 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=4143377) 2026-01-23 00:08:41,623 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=4143377) 2026-01-23 00:08:41,708 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=4143377) Process EngineCore_DP0:
(EngineCore_DP0 pid=4143377) Traceback (most recent call last):
(EngineCore_DP0 pid=4143377)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=4143377)     self.run()
(EngineCore_DP0 pid=4143377)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=4143377)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=4143377)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=4143377)     raise e
(EngineCore_DP0 pid=4143377)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=4143377)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=4143377)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4143377)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=4143377)     super().__init__(
(EngineCore_DP0 pid=4143377)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=4143377)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=4143377)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4143377)   File "/root/vllmbench/vllm/v1/engine/core.py", line 256, in _initialize_kv_caches
(EngineCore_DP0 pid=4143377)     self.model_executor.initialize_from_config(kv_cache_configs)
(EngineCore_DP0 pid=4143377)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 116, in initialize_from_config
(EngineCore_DP0 pid=4143377)     self.collective_rpc("compile_or_warm_up_model")
(EngineCore_DP0 pid=4143377)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=4143377)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=4143377)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4143377)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=4143377)     return func(*args, **kwargs)
(EngineCore_DP0 pid=4143377)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4143377)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 455, in compile_or_warm_up_model
(EngineCore_DP0 pid=4143377)     kernel_warmup(self)
(EngineCore_DP0 pid=4143377)   File "/root/vllmbench/vllm/model_executor/warmup/kernel_warmup.py", line 41, in kernel_warmup
(EngineCore_DP0 pid=4143377)     flashinfer_autotune(worker.model_runner)
(EngineCore_DP0 pid=4143377)   File "/root/vllmbench/vllm/model_executor/warmup/kernel_warmup.py", line 94, in flashinfer_autotune
(EngineCore_DP0 pid=4143377)     runner._dummy_run(
(EngineCore_DP0 pid=4143377)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=4143377)     return func(*args, **kwargs)
(EngineCore_DP0 pid=4143377)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4143377)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=4143377)     outputs = self.model(
(EngineCore_DP0 pid=4143377)               ^^^^^^^^^^^
(EngineCore_DP0 pid=4143377)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=4143377)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=4143377)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4143377)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=4143377)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=4143377)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4143377)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=4143377)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=4143377)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4143377)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 583, in forward
(EngineCore_DP0 pid=4143377)     hidden_states = self.model(
(EngineCore_DP0 pid=4143377)                     ^^^^^^^^^^^
(EngineCore_DP0 pid=4143377)   File "/root/vllmbench/vllm/compilation/decorators.py", line 439, in __call__
(EngineCore_DP0 pid=4143377)     return TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=4143377)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4143377)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 223, in __call__
(EngineCore_DP0 pid=4143377)     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=4143377)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4143377)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=4143377)     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=4143377)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4143377)   File "/root/vllmbench/vllm/model_executor/models/qwen2.py", line 418, in forward
(EngineCore_DP0 pid=4143377)     def forward(
(EngineCore_DP0 pid=4143377)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=4143377)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=4143377)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4143377)   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=4143377)     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=4143377)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4143377)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=4143377)     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=4143377)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4143377)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=4143377)     raise e
(EngineCore_DP0 pid=4143377)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=4143377)     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=4143377)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4143377)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=4143377)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=4143377)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4143377)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=4143377)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=4143377)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4143377)   File "<eval_with_key>.58", line 334, in forward
(EngineCore_DP0 pid=4143377)     submod_4 = self.submod_4(getitem_10, s72, getitem_4, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_, getitem_5, l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_, getitem_11, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_10 = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_ = getitem_11 = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_bias_ = None
(EngineCore_DP0 pid=4143377)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4143377)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=4143377)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=4143377)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4143377)   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=4143377)     return range_entry.runnable(*args)
(EngineCore_DP0 pid=4143377)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4143377)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=4143377)     return self._compiled_fn(*args)
(EngineCore_DP0 pid=4143377)            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4143377)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=4143377)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=4143377)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4143377)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=4143377)     return compiled_fn(full_args)
(EngineCore_DP0 pid=4143377)            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4143377)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=4143377)     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=4143377)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4143377)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=4143377)     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=4143377)                             ^^^^^^^
(EngineCore_DP0 pid=4143377)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 724, in inner_fn
(EngineCore_DP0 pid=4143377)     outs = compiled_fn(args)
(EngineCore_DP0 pid=4143377)            ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4143377)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=4143377)     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=4143377)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4143377)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=4143377)     return self.current_callable(inputs)
(EngineCore_DP0 pid=4143377)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4143377)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=4143377)     out = model(new_inputs)
(EngineCore_DP0 pid=4143377)           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4143377)   File "/tmp/torchinductor_root/lx/clxcnleawpqn5cck5buutm2y6ptizgofto4zdzrpxrmcpk6dyudu.py", line 2404, in call
(EngineCore_DP0 pid=4143377)     buf17 = empty_strided_cuda((s72, 17920), (17920, 1), torch.bfloat16)
(EngineCore_DP0 pid=4143377)             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=4143377) torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.42 GiB. GPU 0 has a total capacity of 15.46 GiB of which 2.65 GiB is free. Including non-PyTorch memory, this process has 12.57 GiB memory in use. Of the allocated memory 8.88 GiB is allocated by PyTorch, and 3.31 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W123 00:08:42.053769120 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[ERROR] 测试失败: M=256 (exit code: 1)


------------------------------------------------------------
  生成 CSV: Qwen2.5-1.5B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_4/Qwen2.5-1.5B-FP8_decode.csv

预览:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
16,16,16,16,256,256,11.8486,3222.8263,1.3504
128,16,128,128,256,256,64.2288,17470.2399,1.9929

------------------------------------------------------------

[INFO] 完成: 2 成功, 1 失败
[INFO] 尝试自动转换: Qwen2.5-1.5B-FP8 -> SlideSparse-2_10

======================================================================
[SlideSparse] 自动转换: Qwen2.5-1.5B-FP8 -> SlideSparse-2_10
======================================================================
[SUCCESS] 转换成功: Qwen2.5-1.5B-FP8-SlideSparse-2_10
======================================================================


============================================================
  Qwen2.5-1.5B-FP8 | cuSPARSELt (2_10) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-1.5B-FP8-SlideSparse-2_10
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10

============================================================
[1/3] 测试 M=16
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-1.5B-FP8
│ Backend:  cuSPARSELt (2:10)
│ 阶段:     prefill
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度:
│   M_prefill = 16 (= 1 x 16)
│   M_decode  = 1
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len       = 16
│   --output-len      = 1
│   --num-prompts     = 128
│   --max-num-seqs    = 1
│   --max-model-len   = 145
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
INFO 01-23 00:09:34 [datasets.py:612] Sampling input_len from [16, 16] and output_len from [1, 1]
INFO 01-23 00:09:34 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints_slidesparse/Qwen2.5-1.5B-FP8-SlideSparse-2_10', 'max_model_len': 145, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 145, 'max_num_seqs': 1, 'disable_log_stats': True, 'enable_lora': None, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints_slidesparse/Qwen2.5-1.5B-FP8-SlideSparse-2_10'}
INFO 01-23 00:09:34 [model.py:514] Resolved architecture: Qwen2ForCausalLM
INFO 01-23 00:09:34 [model.py:1661] Using max model len 145
INFO 01-23 00:09:35 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=145.
(EngineCore_DP0 pid=4145301) INFO 01-23 00:09:38 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints_slidesparse/Qwen2.5-1.5B-FP8-SlideSparse-2_10', speculative_config=None, tokenizer='/root/vllmbench/checkpoints_slidesparse/Qwen2.5-1.5B-FP8-SlideSparse-2_10', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=145, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints_slidesparse/Qwen2.5-1.5B-FP8-SlideSparse-2_10, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [145], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 2, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=4145301) INFO 01-23 00:09:39 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:55773 backend=nccl
(EngineCore_DP0 pid=4145301) INFO 01-23 00:09:39 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=4145301) INFO 01-23 00:09:39 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints_slidesparse/Qwen2.5-1.5B-FP8-SlideSparse-2_10...
(EngineCore_DP0 pid=4145301) INFO 01-23 00:09:39 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=4145301) INFO 01-23 00:09:40 [default_loader.py:308] Loading weights took 0.34 seconds
(EngineCore_DP0 pid=4145301) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=4145301) INFO 01-23 00:09:40 [gpu_model_runner.py:3659] Model loading took 1.6881 GiB memory and 0.584368 seconds
(EngineCore_DP0 pid=4145301) INFO 01-23 00:09:45 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/dd79870017/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=4145301) INFO 01-23 00:09:45 [backends.py:703] Dynamo bytecode transform time: 4.36 s
(EngineCore_DP0 pid=4145301) INFO 01-23 00:09:48 [backends.py:261] Cache the graph of compile range (1, 145) for later use
(EngineCore_DP0 pid=4145301) INFO 01-23 00:10:13 [backends.py:278] Compiling a graph for compile range (1, 145) takes 27.06 s
(EngineCore_DP0 pid=4145301) INFO 01-23 00:10:13 [monitor.py:34] torch.compile takes 31.42 s in total
(EngineCore_DP0 pid=4145301) INFO 01-23 00:10:14 [gpu_worker.py:375] Available KV cache memory: 10.16 GiB
(EngineCore_DP0 pid=4145301) INFO 01-23 00:10:14 [kv_cache_utils.py:1291] GPU KV cache size: 380,368 tokens
(EngineCore_DP0 pid=4145301) INFO 01-23 00:10:14 [kv_cache_utils.py:1296] Maximum concurrency for 145 tokens per request: 2377.30x
(EngineCore_DP0 pid=4145301) INFO 01-23 00:10:15 [gpu_model_runner.py:4587] Graph capturing finished in 1 secs, took 0.01 GiB
(EngineCore_DP0 pid=4145301) INFO 01-23 00:10:15 [core.py:259] init engine (profile, create kv cache, warmup model) took 35.14 seconds
INFO 01-23 00:10:16 [llm.py:360] Supported tasks: ['generate']
Throughput: 120.89 requests/s, 2055.16 total tokens/s, 120.89 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  128


─── STDERR ───
[2026-01-23 00:09:35] INFO gemm_wrapper.py:362: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-23 00:09:35] INFO gemm_wrapper.py:408: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-23 00:09:35] INFO gemm_wrapper.py:455: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-23 00:09:35] INFO gemm_wrapper.py:502: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-23 00:09:38] INFO gemm_wrapper.py:362: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-23 00:09:38] INFO gemm_wrapper.py:408: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-23 00:09:38] INFO gemm_wrapper.py:455: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-23 00:09:38] INFO gemm_wrapper.py:502: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=4145301) [2026-01-23 00:09:39] INFO SlideSparseLinearMethod_FP8.py:691: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=4145301) [2026-01-23 00:09:39] INFO gemm_wrapper.py:307: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=4145301) [2026-01-23 00:09:39] INFO kernels.py:223: FP8 quant+slide kernel loaded
(EngineCore_DP0 pid=4145301) [2026-01-23 00:09:39] INFO kernels.py:115: Dequant+bias kernel loaded
(EngineCore_DP0 pid=4145301) [2026-01-23 00:09:39] INFO SlideSparseLinearMethod_FP8.py:372: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=4145301) [2026-01-23 00:09:39] INFO SlideSparseLinearMethod_FP8.py:512: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=4145301) [2026-01-23 00:09:39] INFO SlideSparseLinearMethod_FP8.py:517: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=4145301) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=4145301) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.40it/s]
(EngineCore_DP0 pid=4145301) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.40it/s]
(EngineCore_DP0 pid=4145301) 
(EngineCore_DP0 pid=4145301) [2026-01-23 00:09:40] INFO SlideSparseLinearMethod_FP8.py:617: cuSPARSELt compression: [2048, 2464] -> 1D uint8
(EngineCore_DP0 pid=4145301) [2026-01-23 00:09:40] INFO SlideSparseLinearMethod_FP8.py:627: cuSPARSELt compression done: 3178496 bytes
(EngineCore_DP0 pid=4145301) [2026-01-23 00:09:40] INFO SlideSparseLinearMethod_FP8.py:617: cuSPARSELt compression: [1536, 2464] -> 1D uint8
(EngineCore_DP0 pid=4145301) [2026-01-23 00:09:40] INFO SlideSparseLinearMethod_FP8.py:627: cuSPARSELt compression done: 2383872 bytes
(EngineCore_DP0 pid=4145301) [2026-01-23 00:09:40] INFO SlideSparseLinearMethod_FP8.py:617: cuSPARSELt compression: [17920, 2464] -> 1D uint8
(EngineCore_DP0 pid=4145301) [2026-01-23 00:09:40] INFO SlideSparseLinearMethod_FP8.py:627: cuSPARSELt compression done: 27811840 bytes
(EngineCore_DP0 pid=4145301) [2026-01-23 00:09:40] INFO SlideSparseLinearMethod_FP8.py:617: cuSPARSELt compression: [1536, 14336] -> 1D uint8
(EngineCore_DP0 pid=4145301) [2026-01-23 00:09:40] INFO SlideSparseLinearMethod_FP8.py:627: cuSPARSELt compression done: 13762560 bytes
(EngineCore_DP0 pid=4145301) 2026-01-23 00:10:14,971 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=4145301) 2026-01-23 00:10:14,985 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=4145301) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 20.10it/s]
(EngineCore_DP0 pid=4145301) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.54it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.53it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  62%|██████▎   | 80/128 [00:00<00:00, 511.36it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 785.28it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  21%|██        | 27/128 [00:00<00:00, 261.84it/s, est. speed input: 4189.58 toks/s, output: 261.84 toks/s]
Processed prompts:  42%|████▏     | 54/128 [00:00<00:00, 164.23it/s, est. speed input: 2783.44 toks/s, output: 173.96 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:00<00:00, 149.41it/s, est. speed input: 2555.86 toks/s, output: 159.74 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:00<00:00, 141.77it/s, est. speed input: 2443.63 toks/s, output: 152.72 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:00<00:00, 134.49it/s, est. speed input: 2349.85 toks/s, output: 146.86 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:00<00:00, 132.90it/s, est. speed input: 2311.82 toks/s, output: 144.49 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 132.90it/s, est. speed input: 2291.03 toks/s, output: 143.19 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 143.18it/s, est. speed input: 2291.03 toks/s, output: 143.19 toks/s]
[rank0]:[W123 00:10:17.314056006 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 47.5s

测试结果:
  Requests/s:   120.89
  Tokens/s:     2055.16
  Total Reqs:   128
  Elapsed:      1.06s

  [Prefill 分析]
  Total Prefill Tokens: 2048
  Prefill Tokens/s:     1934.27

============================================================
[2/3] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-1.5B-FP8
│ Backend:  cuSPARSELt (2:10)
│ 阶段:     prefill
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度:
│   M_prefill = 128 (= 1 x 128)
│   M_decode  = 1
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len       = 128
│   --output-len      = 1
│   --num-prompts     = 128
│   --max-num-seqs    = 1
│   --max-model-len   = 257
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

============================================================
测试被中断!
当前结果目录: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10
============================================================
