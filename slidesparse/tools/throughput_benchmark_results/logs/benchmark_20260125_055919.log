======================================================================
SlideSparse vLLM Throughput Benchmark Log
Created: 2026-01-25 05:59:19
======================================================================

原始命令:
  slidesparse/tools/throughput_benchmark.py --backend cusparselt --sparsity 2_4,2_6,2_10 --M quick

命令行参数:
  --model: None
  --backend: cusparselt
  --sparsity: 2_4,2_6,2_10
  --stage: None
  --M: quick
  --N: None
  --inner-32: False
  --eager: False
  --gpu-id: 0
  --gpu-mem: 0.8
  --dry-run: False
  --list-models: False

硬件信息:
  GPU: RTX5080
  Compute Capability: cc120
  VRAM: 15.5 GB
  CUDA: 12.9
  Python: py312

Backend 环境变量 (初始状态):
  DISABLE_SLIDESPARSE: 未设置
  USE_CUBLASLT: 未设置
  USE_CUSPARSELT: 未设置
  SPARSITY: 未设置
  INNER_DTYPE_32: 未设置

======================================================================


======================================================================
[SlideSparse] 未找到 Llama3.2-1B-INT8-SlideSparse-2_4 checkpoint
[SlideSparse] 发现基础模型: Llama3.2-1B-INT8
[SlideSparse] 自动转换中...
[SlideSparse] 开始转换: Llama3.2-1B-INT8 -> SlideSparse-2_4
======================================================================

[SlideSparse] 转换成功: Llama3.2-1B-INT8-SlideSparse-2_4
======================================================================


============================================================
  Llama3.2-1B-INT8 | cuSPARSELt (2_4) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_4

============================================================
[1/3] 测试 M=16
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16
│   M_prefill     = 16 (= 1 x 16)
│   M_decode      = 1
│   batched_tokens = 17 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 17
│   --max-num-batched-tokens = 17
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
INFO 01-25 05:59:52 [datasets.py:612] Sampling input_len from [15, 15] and output_len from [1, 1]
INFO 01-25 05:59:52 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4', 'max_model_len': 17, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 17, 'max_num_seqs': 1, 'disable_log_stats': True, 'enable_lora': None, 'enable_chunked_prefill': False, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4'}
INFO 01-25 05:59:52 [model.py:514] Resolved architecture: LlamaForCausalLM
INFO 01-25 05:59:52 [model.py:1661] Using max model len 17
WARNING 01-25 05:59:52 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1078740) INFO 01-25 05:59:56 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4', speculative_config=None, tokenizer='/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=17, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4, enable_prefix_caching=True, enable_chunked_prefill=False, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [17], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 2, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=1078740) INFO 01-25 05:59:56 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:41689 backend=nccl
(EngineCore_DP0 pid=1078740) INFO 01-25 05:59:56 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=1078740) INFO 01-25 05:59:56 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4...
(EngineCore_DP0 pid=1078740) INFO 01-25 05:59:56 [compressed_tensors_w8a8_int8.py:62] Using CutlassScaledMMLinearKernel for CompressedTensorsW8A8Int8
(EngineCore_DP0 pid=1078740) INFO 01-25 05:59:56 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=1078740) INFO 01-25 05:59:56 [default_loader.py:308] Loading weights took 0.16 seconds
(EngineCore_DP0 pid=1078740) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1078740) INFO 01-25 05:59:57 [gpu_model_runner.py:3659] Model loading took 1.1954 GiB memory and 0.431909 seconds
(EngineCore_DP0 pid=1078740) WARNING 01-25 05:59:59 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=1078740) INFO 01-25 05:59:59 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/d13503b51f/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=1078740) INFO 01-25 05:59:59 [backends.py:703] Dynamo bytecode transform time: 2.08 s
(EngineCore_DP0 pid=1078740) INFO 01-25 06:00:01 [backends.py:261] Cache the graph of compile range (1, 17) for later use
(EngineCore_DP0 pid=1078740) INFO 01-25 06:00:03 [backends.py:278] Compiling a graph for compile range (1, 17) takes 2.67 s
(EngineCore_DP0 pid=1078740) INFO 01-25 06:00:03 [monitor.py:34] torch.compile takes 4.74 s in total
(EngineCore_DP0 pid=1078740) INFO 01-25 06:00:04 [gpu_worker.py:375] Available KV cache memory: 10.60 GiB
(EngineCore_DP0 pid=1078740) INFO 01-25 06:00:04 [kv_cache_utils.py:1291] GPU KV cache size: 347,392 tokens
(EngineCore_DP0 pid=1078740) INFO 01-25 06:00:04 [kv_cache_utils.py:1296] Maximum concurrency for 17 tokens per request: 10856.00x
(EngineCore_DP0 pid=1078740) INFO 01-25 06:00:05 [gpu_model_runner.py:4587] Graph capturing finished in 1 secs, took 0.01 GiB
(EngineCore_DP0 pid=1078740) INFO 01-25 06:00:05 [core.py:259] init engine (profile, create kv cache, warmup model) took 8.05 seconds
INFO 01-25 06:00:05 [llm.py:360] Supported tasks: ['generate']
Throughput: 62.42 requests/s, 1061.17 total tokens/s, 62.42 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  128


─── STDERR ───
[2026-01-25 05:59:52] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 05:59:52] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-INT8'
[2026-01-25 05:59:52] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 05:59:52] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 05:59:52] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 05:59:52] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 05:59:52] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 05:59:52] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 05:59:52] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-INT8'
[2026-01-25 05:59:52] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 05:59:52] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 05:59:52] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 05:59:52] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 05:59:52] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 05:59:56] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 05:59:56] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-INT8'
[2026-01-25 05:59:56] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 05:59:56] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 05:59:56] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 05:59:56] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 05:59:56] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 05:59:56] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 05:59:56] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-INT8'
[2026-01-25 05:59:56] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 05:59:56] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 05:59:56] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 05:59:56] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 05:59:56] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1078740) [2026-01-25 05:59:56] INFO SlideSparseLinearMethod_INT8.py:801: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1078740) [2026-01-25 05:59:56] INFO gemm_wrapper.py:872: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1078740) [2026-01-25 05:59:56] INFO SlideSparseLinearMethod_INT8.py:448: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1078740) [2026-01-25 05:59:56] INFO SlideSparseLinearMethod_INT8.py:600: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=1078740) [2026-01-25 05:59:56] INFO kernels.py:132: Loaded tuned kernel for base model: Llama3.2-1B-INT8 (from Llama3.2-1B-INT8-SlideSparse-2_4)
(EngineCore_DP0 pid=1078740) [2026-01-25 05:59:56] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8-SlideSparse-2_4
(EngineCore_DP0 pid=1078740) [2026-01-25 05:59:56] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8-SlideSparse-2_4
(EngineCore_DP0 pid=1078740) [2026-01-25 05:59:56] INFO SlideSparseLinearMethod_INT8.py:617: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8-SlideSparse-2_4
(EngineCore_DP0 pid=1078740) [2026-01-25 05:59:56] INFO SlideSparseLinearMethod_INT8.py:621: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=1078740) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1078740) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  7.07it/s]
(EngineCore_DP0 pid=1078740) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  7.06it/s]
(EngineCore_DP0 pid=1078740) 
(EngineCore_DP0 pid=1078740) [2026-01-25 05:59:56] INFO SlideSparseLinearMethod_INT8.py:715: cuSPARSELt INT8 compression: [3072, 2048] -> 1D uint8
(EngineCore_DP0 pid=1078740) [2026-01-25 05:59:56] INFO SlideSparseLinearMethod_INT8.py:725: cuSPARSELt INT8 compression done: 4718592 bytes
(EngineCore_DP0 pid=1078740) [2026-01-25 05:59:56] INFO SlideSparseLinearMethod_INT8.py:715: cuSPARSELt INT8 compression: [2048, 2048] -> 1D uint8
(EngineCore_DP0 pid=1078740) [2026-01-25 05:59:56] INFO SlideSparseLinearMethod_INT8.py:725: cuSPARSELt INT8 compression done: 3145728 bytes
(EngineCore_DP0 pid=1078740) [2026-01-25 05:59:56] INFO SlideSparseLinearMethod_INT8.py:715: cuSPARSELt INT8 compression: [16384, 2048] -> 1D uint8
(EngineCore_DP0 pid=1078740) [2026-01-25 05:59:56] INFO SlideSparseLinearMethod_INT8.py:725: cuSPARSELt INT8 compression done: 25165824 bytes
(EngineCore_DP0 pid=1078740) [2026-01-25 05:59:56] INFO SlideSparseLinearMethod_INT8.py:715: cuSPARSELt INT8 compression: [2048, 8192] -> 1D uint8
(EngineCore_DP0 pid=1078740) [2026-01-25 05:59:56] INFO SlideSparseLinearMethod_INT8.py:725: cuSPARSELt INT8 compression done: 12582912 bytes
(EngineCore_DP0 pid=1078740) 2026-01-25 06:00:04,353 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1078740) 2026-01-25 06:00:04,366 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1078740) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  7.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  4.68it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  4.98it/s]
(EngineCore_DP0 pid=1078740) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.96it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.95it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:   5%|▌         | 7/128 [00:00<00:02, 45.95it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 772.82it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:14,  8.87it/s, est. speed input: 141.98 toks/s, output: 8.87 toks/s]
Processed prompts:   6%|▋         | 8/128 [00:00<00:02, 42.72it/s, est. speed input: 598.05 toks/s, output: 37.38 toks/s]
Processed prompts:  12%|█▎        | 16/128 [00:00<00:01, 56.98it/s, est. speed input: 792.91 toks/s, output: 49.56 toks/s]
Processed prompts:  19%|█▉        | 24/128 [00:00<00:01, 62.67it/s, est. speed input: 881.60 toks/s, output: 55.10 toks/s]
Processed prompts:  25%|██▌       | 32/128 [00:00<00:01, 66.56it/s, est. speed input: 940.39 toks/s, output: 58.77 toks/s]
Processed prompts:  31%|███▏      | 40/128 [00:00<00:01, 68.27it/s, est. speed input: 975.15 toks/s, output: 60.95 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:00<00:01, 69.98it/s, est. speed input: 1003.84 toks/s, output: 62.74 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:00<00:01, 70.98it/s, est. speed input: 1024.59 toks/s, output: 64.04 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:00<00:00, 71.19it/s, est. speed input: 1038.39 toks/s, output: 64.90 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:01<00:00, 71.93it/s, est. speed input: 1052.21 toks/s, output: 65.76 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:01<00:00, 71.44it/s, est. speed input: 1059.14 toks/s, output: 66.20 toks/s]
Processed prompts:  69%|██████▉   | 88/128 [00:01<00:00, 71.90it/s, est. speed input: 1068.14 toks/s, output: 66.76 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:01<00:00, 72.50it/s, est. speed input: 1076.82 toks/s, output: 67.30 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:01<00:00, 71.20it/s, est. speed input: 1078.13 toks/s, output: 67.38 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:01<00:00, 71.99it/s, est. speed input: 1084.96 toks/s, output: 67.81 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:01<00:00, 71.29it/s, est. speed input: 1086.94 toks/s, output: 67.93 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 70.25it/s, est. speed input: 1086.95 toks/s, output: 67.93 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 70.25it/s, est. speed input: 1086.95 toks/s, output: 67.93 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 67.93it/s, est. speed input: 1086.95 toks/s, output: 67.93 toks/s]
[rank0]:[W125 06:00:08.820739468 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 20.7s

测试结果:
  Requests/s:   62.42
  Tokens/s:     1061.17
  Total Reqs:   128
  Elapsed:      2.05s

  [Prefill 分析]
  Total Prefill Tokens: 2048
  Prefill Tokens/s:     998.74

============================================================
[2/3] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 128 (= 1 x 128)
│   M_decode      = 1
│   batched_tokens = 129 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 128
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 129
│   --max-num-batched-tokens = 129
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
INFO 01-25 06:00:12 [datasets.py:612] Sampling input_len from [127, 127] and output_len from [1, 1]
INFO 01-25 06:00:12 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4', 'max_model_len': 129, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 129, 'max_num_seqs': 1, 'disable_log_stats': True, 'enable_lora': None, 'enable_chunked_prefill': False, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4'}
INFO 01-25 06:00:12 [model.py:514] Resolved architecture: LlamaForCausalLM
INFO 01-25 06:00:12 [model.py:1661] Using max model len 129
WARNING 01-25 06:00:13 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1079239) INFO 01-25 06:00:17 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4', speculative_config=None, tokenizer='/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=129, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4, enable_prefix_caching=True, enable_chunked_prefill=False, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [129], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 2, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=1079239) INFO 01-25 06:00:17 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:51391 backend=nccl
(EngineCore_DP0 pid=1079239) INFO 01-25 06:00:17 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=1079239) INFO 01-25 06:00:17 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4...
(EngineCore_DP0 pid=1079239) INFO 01-25 06:00:17 [compressed_tensors_w8a8_int8.py:62] Using CutlassScaledMMLinearKernel for CompressedTensorsW8A8Int8
(EngineCore_DP0 pid=1079239) INFO 01-25 06:00:17 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=1079239) INFO 01-25 06:00:17 [default_loader.py:308] Loading weights took 0.16 seconds
(EngineCore_DP0 pid=1079239) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1079239) INFO 01-25 06:00:18 [gpu_model_runner.py:3659] Model loading took 1.1954 GiB memory and 0.426271 seconds
(EngineCore_DP0 pid=1079239) WARNING 01-25 06:00:20 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=1079239) INFO 01-25 06:00:20 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/c54a38687c/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=1079239) INFO 01-25 06:00:20 [backends.py:703] Dynamo bytecode transform time: 2.06 s
(EngineCore_DP0 pid=1079239) INFO 01-25 06:00:22 [backends.py:261] Cache the graph of compile range (1, 129) for later use
(EngineCore_DP0 pid=1079239) INFO 01-25 06:00:24 [backends.py:278] Compiling a graph for compile range (1, 129) takes 2.48 s
(EngineCore_DP0 pid=1079239) INFO 01-25 06:00:24 [monitor.py:34] torch.compile takes 4.53 s in total
(EngineCore_DP0 pid=1079239) INFO 01-25 06:00:24 [gpu_worker.py:375] Available KV cache memory: 10.60 GiB
(EngineCore_DP0 pid=1079239) INFO 01-25 06:00:25 [kv_cache_utils.py:1291] GPU KV cache size: 347,360 tokens
(EngineCore_DP0 pid=1079239) INFO 01-25 06:00:25 [kv_cache_utils.py:1296] Maximum concurrency for 129 tokens per request: 2412.22x
(EngineCore_DP0 pid=1079239) INFO 01-25 06:00:25 [gpu_model_runner.py:4587] Graph capturing finished in 1 secs, took 0.01 GiB
(EngineCore_DP0 pid=1079239) INFO 01-25 06:00:25 [core.py:259] init engine (profile, create kv cache, warmup model) took 7.42 seconds
INFO 01-25 06:00:26 [llm.py:360] Supported tasks: ['generate']
Throughput: 61.41 requests/s, 7922.13 total tokens/s, 61.41 output tokens/s
Total num prompt tokens:  16384
Total num output tokens:  128


─── STDERR ───
[2026-01-25 06:00:13] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:00:13] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-INT8'
[2026-01-25 06:00:13] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 06:00:13] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:00:13] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:00:13] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:00:13] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:00:13] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:00:13] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-INT8'
[2026-01-25 06:00:13] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 06:00:13] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:00:13] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:00:13] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:00:13] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 06:00:16] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:00:16] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-INT8'
[2026-01-25 06:00:16] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 06:00:16] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:00:17] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:00:17] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:00:17] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:00:17] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:00:17] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-INT8'
[2026-01-25 06:00:17] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 06:00:17] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:00:17] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:00:17] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:00:17] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1079239) [2026-01-25 06:00:17] INFO SlideSparseLinearMethod_INT8.py:801: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1079239) [2026-01-25 06:00:17] INFO gemm_wrapper.py:872: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1079239) [2026-01-25 06:00:17] INFO SlideSparseLinearMethod_INT8.py:448: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1079239) [2026-01-25 06:00:17] INFO SlideSparseLinearMethod_INT8.py:600: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=1079239) [2026-01-25 06:00:17] INFO kernels.py:132: Loaded tuned kernel for base model: Llama3.2-1B-INT8 (from Llama3.2-1B-INT8-SlideSparse-2_4)
(EngineCore_DP0 pid=1079239) [2026-01-25 06:00:17] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8-SlideSparse-2_4
(EngineCore_DP0 pid=1079239) [2026-01-25 06:00:17] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8-SlideSparse-2_4
(EngineCore_DP0 pid=1079239) [2026-01-25 06:00:17] INFO SlideSparseLinearMethod_INT8.py:617: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8-SlideSparse-2_4
(EngineCore_DP0 pid=1079239) [2026-01-25 06:00:17] INFO SlideSparseLinearMethod_INT8.py:621: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=1079239) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1079239) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  7.11it/s]
(EngineCore_DP0 pid=1079239) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  7.10it/s]
(EngineCore_DP0 pid=1079239) 
(EngineCore_DP0 pid=1079239) [2026-01-25 06:00:17] INFO SlideSparseLinearMethod_INT8.py:715: cuSPARSELt INT8 compression: [3072, 2048] -> 1D uint8
(EngineCore_DP0 pid=1079239) [2026-01-25 06:00:17] INFO SlideSparseLinearMethod_INT8.py:725: cuSPARSELt INT8 compression done: 4718592 bytes
(EngineCore_DP0 pid=1079239) [2026-01-25 06:00:17] INFO SlideSparseLinearMethod_INT8.py:715: cuSPARSELt INT8 compression: [2048, 2048] -> 1D uint8
(EngineCore_DP0 pid=1079239) [2026-01-25 06:00:17] INFO SlideSparseLinearMethod_INT8.py:725: cuSPARSELt INT8 compression done: 3145728 bytes
(EngineCore_DP0 pid=1079239) [2026-01-25 06:00:17] INFO SlideSparseLinearMethod_INT8.py:715: cuSPARSELt INT8 compression: [16384, 2048] -> 1D uint8
(EngineCore_DP0 pid=1079239) [2026-01-25 06:00:17] INFO SlideSparseLinearMethod_INT8.py:725: cuSPARSELt INT8 compression done: 25165824 bytes
(EngineCore_DP0 pid=1079239) [2026-01-25 06:00:17] INFO SlideSparseLinearMethod_INT8.py:715: cuSPARSELt INT8 compression: [2048, 8192] -> 1D uint8
(EngineCore_DP0 pid=1079239) [2026-01-25 06:00:17] INFO SlideSparseLinearMethod_INT8.py:725: cuSPARSELt INT8 compression done: 12582912 bytes
(EngineCore_DP0 pid=1079239) 2026-01-25 06:00:25,015 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1079239) 2026-01-25 06:00:25,028 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1079239) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 31.12it/s]
(EngineCore_DP0 pid=1079239) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.94it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.93it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 2477.15it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:34,  3.63it/s, est. speed input: 464.70 toks/s, output: 3.63 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:04, 28.70it/s, est. speed input: 2986.07 toks/s, output: 23.33 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:02, 43.78it/s, est. speed input: 4392.45 toks/s, output: 34.32 toks/s]
Processed prompts:  19%|█▉        | 24/128 [00:00<00:02, 51.79it/s, est. speed input: 5156.75 toks/s, output: 40.29 toks/s]
Processed prompts:  25%|██▌       | 32/128 [00:00<00:01, 58.34it/s, est. speed input: 5791.34 toks/s, output: 45.24 toks/s]
Processed prompts:  31%|███▏      | 40/128 [00:00<00:01, 62.64it/s, est. speed input: 6253.96 toks/s, output: 48.86 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:00<00:01, 66.06it/s, est. speed input: 6626.86 toks/s, output: 51.77 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:01<00:01, 68.05it/s, est. speed input: 6909.04 toks/s, output: 53.98 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:01<00:00, 69.79it/s, est. speed input: 7149.32 toks/s, output: 55.85 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:01<00:00, 69.87it/s, est. speed input: 7313.96 toks/s, output: 57.14 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:01<00:00, 70.90it/s, est. speed input: 7478.98 toks/s, output: 58.43 toks/s]
Processed prompts:  69%|██████▉   | 88/128 [00:01<00:00, 71.59it/s, est. speed input: 7618.62 toks/s, output: 59.52 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:01<00:00, 71.66it/s, est. speed input: 7728.86 toks/s, output: 60.38 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:01<00:00, 71.69it/s, est. speed input: 7824.34 toks/s, output: 61.13 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:01<00:00, 72.11it/s, est. speed input: 7917.01 toks/s, output: 61.85 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:01<00:00, 72.00it/s, est. speed input: 7990.53 toks/s, output: 62.43 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 72.51it/s, est. speed input: 8067.86 toks/s, output: 63.03 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 72.51it/s, est. speed input: 8067.86 toks/s, output: 63.03 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 63.03it/s, est. speed input: 8067.86 toks/s, output: 63.03 toks/s]
[rank0]:[W125 06:00:28.174369051 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 20.3s

测试结果:
  Requests/s:   61.41
  Tokens/s:     7922.13
  Total Reqs:   128
  Elapsed:      2.08s

  [Prefill 分析]
  Total Prefill Tokens: 16384
  Prefill Tokens/s:     7860.72

============================================================
[3/3] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 256 (= 1 x 256)
│   M_decode      = 1
│   batched_tokens = 257 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 256
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 257
│   --max-num-batched-tokens = 257
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
INFO 01-25 06:00:33 [datasets.py:612] Sampling input_len from [255, 255] and output_len from [1, 1]
INFO 01-25 06:00:33 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4', 'max_model_len': 257, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 257, 'max_num_seqs': 1, 'disable_log_stats': True, 'enable_lora': None, 'enable_chunked_prefill': False, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4'}
INFO 01-25 06:00:33 [model.py:514] Resolved architecture: LlamaForCausalLM
INFO 01-25 06:00:33 [model.py:1661] Using max model len 257
WARNING 01-25 06:00:33 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1079719) INFO 01-25 06:00:37 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4', speculative_config=None, tokenizer='/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=257, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4, enable_prefix_caching=True, enable_chunked_prefill=False, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [257], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 2, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=1079719) INFO 01-25 06:00:37 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:51661 backend=nccl
(EngineCore_DP0 pid=1079719) INFO 01-25 06:00:37 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=1079719) INFO 01-25 06:00:37 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4...
(EngineCore_DP0 pid=1079719) INFO 01-25 06:00:37 [compressed_tensors_w8a8_int8.py:62] Using CutlassScaledMMLinearKernel for CompressedTensorsW8A8Int8
(EngineCore_DP0 pid=1079719) INFO 01-25 06:00:37 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=1079719) INFO 01-25 06:00:38 [default_loader.py:308] Loading weights took 0.16 seconds
(EngineCore_DP0 pid=1079719) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1079719) INFO 01-25 06:00:38 [gpu_model_runner.py:3659] Model loading took 1.1949 GiB memory and 0.429394 seconds
(EngineCore_DP0 pid=1079719) WARNING 01-25 06:00:40 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=1079719) INFO 01-25 06:00:40 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/84b080c292/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=1079719) INFO 01-25 06:00:40 [backends.py:703] Dynamo bytecode transform time: 2.07 s
(EngineCore_DP0 pid=1079719) INFO 01-25 06:00:42 [backends.py:261] Cache the graph of compile range (1, 257) for later use
(EngineCore_DP0 pid=1079719) INFO 01-25 06:00:44 [backends.py:278] Compiling a graph for compile range (1, 257) takes 2.06 s
(EngineCore_DP0 pid=1079719) INFO 01-25 06:00:44 [monitor.py:34] torch.compile takes 4.13 s in total
(EngineCore_DP0 pid=1079719) INFO 01-25 06:00:44 [gpu_worker.py:375] Available KV cache memory: 10.60 GiB
(EngineCore_DP0 pid=1079719) INFO 01-25 06:00:44 [kv_cache_utils.py:1291] GPU KV cache size: 347,328 tokens
(EngineCore_DP0 pid=1079719) INFO 01-25 06:00:44 [kv_cache_utils.py:1296] Maximum concurrency for 257 tokens per request: 1276.94x
(EngineCore_DP0 pid=1079719) INFO 01-25 06:00:45 [gpu_model_runner.py:4587] Graph capturing finished in 1 secs, took 0.01 GiB
(EngineCore_DP0 pid=1079719) INFO 01-25 06:00:45 [core.py:259] init engine (profile, create kv cache, warmup model) took 6.97 seconds
INFO 01-25 06:00:46 [llm.py:360] Supported tasks: ['generate']
Throughput: 63.06 requests/s, 16205.88 total tokens/s, 63.06 output tokens/s
Total num prompt tokens:  32768
Total num output tokens:  128


─── STDERR ───
[2026-01-25 06:00:33] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:00:33] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-INT8'
[2026-01-25 06:00:33] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 06:00:33] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:00:33] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:00:33] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:00:33] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:00:33] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:00:33] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-INT8'
[2026-01-25 06:00:33] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 06:00:33] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:00:33] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:00:33] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:00:33] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 06:00:37] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:00:37] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-INT8'
[2026-01-25 06:00:37] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 06:00:37] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:00:37] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:00:37] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:00:37] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:00:37] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:00:37] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-INT8'
[2026-01-25 06:00:37] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 06:00:37] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:00:37] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:00:37] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:00:37] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1079719) [2026-01-25 06:00:37] INFO SlideSparseLinearMethod_INT8.py:801: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1079719) [2026-01-25 06:00:37] INFO gemm_wrapper.py:872: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1079719) [2026-01-25 06:00:37] INFO SlideSparseLinearMethod_INT8.py:448: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1079719) [2026-01-25 06:00:37] INFO SlideSparseLinearMethod_INT8.py:600: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=1079719) [2026-01-25 06:00:37] INFO kernels.py:132: Loaded tuned kernel for base model: Llama3.2-1B-INT8 (from Llama3.2-1B-INT8-SlideSparse-2_4)
(EngineCore_DP0 pid=1079719) [2026-01-25 06:00:37] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8-SlideSparse-2_4
(EngineCore_DP0 pid=1079719) [2026-01-25 06:00:37] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8-SlideSparse-2_4
(EngineCore_DP0 pid=1079719) [2026-01-25 06:00:37] INFO SlideSparseLinearMethod_INT8.py:617: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8-SlideSparse-2_4
(EngineCore_DP0 pid=1079719) [2026-01-25 06:00:37] INFO SlideSparseLinearMethod_INT8.py:621: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=1079719) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1079719) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  7.11it/s]
(EngineCore_DP0 pid=1079719) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  7.10it/s]
(EngineCore_DP0 pid=1079719) 
(EngineCore_DP0 pid=1079719) [2026-01-25 06:00:38] INFO SlideSparseLinearMethod_INT8.py:715: cuSPARSELt INT8 compression: [3072, 2048] -> 1D uint8
(EngineCore_DP0 pid=1079719) [2026-01-25 06:00:38] INFO SlideSparseLinearMethod_INT8.py:725: cuSPARSELt INT8 compression done: 4718592 bytes
(EngineCore_DP0 pid=1079719) [2026-01-25 06:00:38] INFO SlideSparseLinearMethod_INT8.py:715: cuSPARSELt INT8 compression: [2048, 2048] -> 1D uint8
(EngineCore_DP0 pid=1079719) [2026-01-25 06:00:38] INFO SlideSparseLinearMethod_INT8.py:725: cuSPARSELt INT8 compression done: 3145728 bytes
(EngineCore_DP0 pid=1079719) [2026-01-25 06:00:38] INFO SlideSparseLinearMethod_INT8.py:715: cuSPARSELt INT8 compression: [16384, 2048] -> 1D uint8
(EngineCore_DP0 pid=1079719) [2026-01-25 06:00:38] INFO SlideSparseLinearMethod_INT8.py:725: cuSPARSELt INT8 compression done: 25165824 bytes
(EngineCore_DP0 pid=1079719) [2026-01-25 06:00:38] INFO SlideSparseLinearMethod_INT8.py:715: cuSPARSELt INT8 compression: [2048, 8192] -> 1D uint8
(EngineCore_DP0 pid=1079719) [2026-01-25 06:00:38] INFO SlideSparseLinearMethod_INT8.py:725: cuSPARSELt INT8 compression done: 12582912 bytes
(EngineCore_DP0 pid=1079719) 2026-01-25 06:00:44,887 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1079719) 2026-01-25 06:00:44,900 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1079719) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 30.33it/s]
(EngineCore_DP0 pid=1079719) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.45it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.45it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:   5%|▌         | 7/128 [00:00<00:02, 45.40it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 608.66it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:02, 44.85it/s, est. speed input: 11483.60 toks/s, output: 44.85 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:01, 60.59it/s, est. speed input: 14908.00 toks/s, output: 58.23 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:01, 65.66it/s, est. speed input: 16057.05 toks/s, output: 62.72 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:00<00:01, 68.30it/s, est. speed input: 16670.26 toks/s, output: 65.12 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:00<00:01, 69.76it/s, est. speed input: 17039.68 toks/s, output: 66.56 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:00<00:01, 70.78it/s, est. speed input: 17302.34 toks/s, output: 67.59 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:00<00:01, 71.10it/s, est. speed input: 17455.18 toks/s, output: 68.18 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:00<00:00, 70.56it/s, est. speed input: 17497.29 toks/s, output: 68.35 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:01<00:00, 71.00it/s, est. speed input: 17599.62 toks/s, output: 68.75 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:01<00:00, 71.68it/s, est. speed input: 17712.72 toks/s, output: 69.19 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:01<00:00, 72.05it/s, est. speed input: 17797.78 toks/s, output: 69.52 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:01<00:00, 72.33it/s, est. speed input: 17870.36 toks/s, output: 69.81 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:01<00:00, 72.61it/s, est. speed input: 17937.52 toks/s, output: 70.07 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:01<00:00, 72.38it/s, est. speed input: 17970.20 toks/s, output: 70.20 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:01<00:00, 72.68it/s, est. speed input: 18024.05 toks/s, output: 70.41 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:01<00:00, 71.95it/s, est. speed input: 18022.59 toks/s, output: 70.40 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 71.95it/s, est. speed input: 18026.71 toks/s, output: 70.42 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 70.41it/s, est. speed input: 18026.71 toks/s, output: 70.42 toks/s]
[rank0]:[W125 06:00:48.989956561 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 19.9s

测试结果:
  Requests/s:   63.06
  Tokens/s:     16205.88
  Total Reqs:   128
  Elapsed:      2.03s

  [Prefill 分析]
  Total Prefill Tokens: 32768
  Prefill Tokens/s:     16142.82


------------------------------------------------------------
  生成 CSV: Llama3.2-1B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_4/Llama3.2-1B-INT8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
16,16,1,128,128,62.4215,1061.1654,2.0506
128,128,1,128,128,61.4118,7922.1286,2.0843
256,256,1,128,128,63.0579,16205.8777,2.0299

------------------------------------------------------------

[INFO] 完成: 3 成功, 0 失败

============================================================
  Llama3.2-1B-INT8 | cuSPARSELt (2_4) | decode
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_4

============================================================
[1/3] 测试 M=16
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16
│   M_prefill     = 256 (= 16 x 16)
│   M_decode      = 16
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 16
│   --max-num-seqs           = 16
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
INFO 01-25 06:00:53 [datasets.py:612] Sampling input_len from [15, 15] and output_len from [256, 256]
INFO 01-25 06:00:53 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4', 'max_model_len': 272, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 272, 'max_num_seqs': 16, 'disable_log_stats': True, 'enable_lora': None, 'enable_chunked_prefill': False, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4'}
INFO 01-25 06:00:53 [model.py:514] Resolved architecture: LlamaForCausalLM
INFO 01-25 06:00:53 [model.py:1661] Using max model len 272
WARNING 01-25 06:00:53 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1080123) INFO 01-25 06:00:57 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4', speculative_config=None, tokenizer='/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=272, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4, enable_prefix_caching=True, enable_chunked_prefill=False, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [272], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 32, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=1080123) INFO 01-25 06:00:57 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:35053 backend=nccl
(EngineCore_DP0 pid=1080123) INFO 01-25 06:00:57 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=1080123) INFO 01-25 06:00:57 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4...
(EngineCore_DP0 pid=1080123) INFO 01-25 06:00:58 [compressed_tensors_w8a8_int8.py:62] Using CutlassScaledMMLinearKernel for CompressedTensorsW8A8Int8
(EngineCore_DP0 pid=1080123) INFO 01-25 06:00:58 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=1080123) INFO 01-25 06:00:58 [default_loader.py:308] Loading weights took 0.16 seconds
(EngineCore_DP0 pid=1080123) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1080123) INFO 01-25 06:00:58 [gpu_model_runner.py:3659] Model loading took 1.1949 GiB memory and 0.433470 seconds
(EngineCore_DP0 pid=1080123) WARNING 01-25 06:01:00 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=1080123) INFO 01-25 06:01:00 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/f1ab1b7c4d/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=1080123) INFO 01-25 06:01:00 [backends.py:703] Dynamo bytecode transform time: 2.05 s
(EngineCore_DP0 pid=1080123) INFO 01-25 06:01:03 [backends.py:261] Cache the graph of compile range (1, 272) for later use
(EngineCore_DP0 pid=1080123) INFO 01-25 06:01:04 [backends.py:278] Compiling a graph for compile range (1, 272) takes 2.30 s
(EngineCore_DP0 pid=1080123) INFO 01-25 06:01:04 [monitor.py:34] torch.compile takes 4.35 s in total
(EngineCore_DP0 pid=1080123) INFO 01-25 06:01:05 [gpu_worker.py:375] Available KV cache memory: 10.60 GiB
(EngineCore_DP0 pid=1080123) INFO 01-25 06:01:05 [kv_cache_utils.py:1291] GPU KV cache size: 347,344 tokens
(EngineCore_DP0 pid=1080123) INFO 01-25 06:01:05 [kv_cache_utils.py:1296] Maximum concurrency for 272 tokens per request: 1277.00x
(EngineCore_DP0 pid=1080123) INFO 01-25 06:01:06 [gpu_model_runner.py:4587] Graph capturing finished in 1 secs, took 0.03 GiB
(EngineCore_DP0 pid=1080123) INFO 01-25 06:01:06 [core.py:259] init engine (profile, create kv cache, warmup model) took 7.40 seconds
INFO 01-25 06:01:06 [llm.py:360] Supported tasks: ['generate']
Throughput: 20.91 requests/s, 5687.37 total tokens/s, 5352.82 output tokens/s
Total num prompt tokens:  256
Total num output tokens:  4096


─── STDERR ───
[2026-01-25 06:00:53] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:00:53] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-INT8'
[2026-01-25 06:00:53] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 06:00:53] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:00:53] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:00:53] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:00:53] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:00:53] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:00:53] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-INT8'
[2026-01-25 06:00:53] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 06:00:53] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:00:53] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:00:53] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:00:53] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 06:00:57] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:00:57] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-INT8'
[2026-01-25 06:00:57] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 06:00:57] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:00:57] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:00:57] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:00:57] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:00:57] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:00:57] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-INT8'
[2026-01-25 06:00:57] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 06:00:57] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:00:57] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:00:57] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:00:57] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1080123) [2026-01-25 06:00:58] INFO SlideSparseLinearMethod_INT8.py:801: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1080123) [2026-01-25 06:00:58] INFO gemm_wrapper.py:872: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1080123) [2026-01-25 06:00:58] INFO SlideSparseLinearMethod_INT8.py:448: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1080123) [2026-01-25 06:00:58] INFO SlideSparseLinearMethod_INT8.py:600: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=1080123) [2026-01-25 06:00:58] INFO kernels.py:132: Loaded tuned kernel for base model: Llama3.2-1B-INT8 (from Llama3.2-1B-INT8-SlideSparse-2_4)
(EngineCore_DP0 pid=1080123) [2026-01-25 06:00:58] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8-SlideSparse-2_4
(EngineCore_DP0 pid=1080123) [2026-01-25 06:00:58] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8-SlideSparse-2_4
(EngineCore_DP0 pid=1080123) [2026-01-25 06:00:58] INFO SlideSparseLinearMethod_INT8.py:617: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8-SlideSparse-2_4
(EngineCore_DP0 pid=1080123) [2026-01-25 06:00:58] INFO SlideSparseLinearMethod_INT8.py:621: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=1080123) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1080123) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  7.04it/s]
(EngineCore_DP0 pid=1080123) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  7.03it/s]
(EngineCore_DP0 pid=1080123) 
(EngineCore_DP0 pid=1080123) [2026-01-25 06:00:58] INFO SlideSparseLinearMethod_INT8.py:715: cuSPARSELt INT8 compression: [3072, 2048] -> 1D uint8
(EngineCore_DP0 pid=1080123) [2026-01-25 06:00:58] INFO SlideSparseLinearMethod_INT8.py:725: cuSPARSELt INT8 compression done: 4718592 bytes
(EngineCore_DP0 pid=1080123) [2026-01-25 06:00:58] INFO SlideSparseLinearMethod_INT8.py:715: cuSPARSELt INT8 compression: [2048, 2048] -> 1D uint8
(EngineCore_DP0 pid=1080123) [2026-01-25 06:00:58] INFO SlideSparseLinearMethod_INT8.py:725: cuSPARSELt INT8 compression done: 3145728 bytes
(EngineCore_DP0 pid=1080123) [2026-01-25 06:00:58] INFO SlideSparseLinearMethod_INT8.py:715: cuSPARSELt INT8 compression: [16384, 2048] -> 1D uint8
(EngineCore_DP0 pid=1080123) [2026-01-25 06:00:58] INFO SlideSparseLinearMethod_INT8.py:725: cuSPARSELt INT8 compression done: 25165824 bytes
(EngineCore_DP0 pid=1080123) [2026-01-25 06:00:58] INFO SlideSparseLinearMethod_INT8.py:715: cuSPARSELt INT8 compression: [2048, 8192] -> 1D uint8
(EngineCore_DP0 pid=1080123) [2026-01-25 06:00:58] INFO SlideSparseLinearMethod_INT8.py:725: cuSPARSELt INT8 compression done: 12582912 bytes
(EngineCore_DP0 pid=1080123) 2026-01-25 06:01:05,353 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1080123) 2026-01-25 06:01:05,367 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1080123) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00, 33.97it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 34.25it/s]
(EngineCore_DP0 pid=1080123) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 1/5 [00:00<00:00,  6.92it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 20.84it/s]

Adding requests:   0%|          | 0/16 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 16/16 [00:00<00:00, 2520.43it/s]

Processed prompts:   0%|          | 0/16 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   6%|▋         | 1/16 [00:00<00:11,  1.33it/s, est. speed input: 21.25 toks/s, output: 339.93 toks/s]
Processed prompts: 100%|██████████| 16/16 [00:00<00:00,  1.33it/s, est. speed input: 338.52 toks/s, output: 5416.31 toks/s]
Processed prompts: 100%|██████████| 16/16 [00:00<00:00, 21.16it/s, est. speed input: 338.52 toks/s, output: 5416.31 toks/s]
[rank0]:[W125 06:01:07.415227376 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 19.3s

测试结果:
  Requests/s:   20.91
  Tokens/s:     5687.37
  Total Reqs:   16
  Elapsed:      0.77s

  [Decode 分析]
  Total Decode Tokens:  4096
  Decode Tokens/s:      5352.82

============================================================
[2/3] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 2048 (= 128 x 16)
│   M_decode      = 128
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 128
│   --max-num-seqs           = 128
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
INFO 01-25 06:01:12 [datasets.py:612] Sampling input_len from [15, 15] and output_len from [256, 256]
INFO 01-25 06:01:12 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4', 'max_model_len': 272, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 272, 'max_num_seqs': 128, 'disable_log_stats': True, 'enable_lora': None, 'enable_chunked_prefill': False, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4'}
INFO 01-25 06:01:12 [model.py:514] Resolved architecture: LlamaForCausalLM
INFO 01-25 06:01:12 [model.py:1661] Using max model len 272
WARNING 01-25 06:01:12 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1080490) INFO 01-25 06:01:16 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4', speculative_config=None, tokenizer='/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=272, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4, enable_prefix_caching=True, enable_chunked_prefill=False, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [272], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 256, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=1080490) INFO 01-25 06:01:16 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:33787 backend=nccl
(EngineCore_DP0 pid=1080490) INFO 01-25 06:01:16 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=1080490) INFO 01-25 06:01:16 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4...
(EngineCore_DP0 pid=1080490) INFO 01-25 06:01:17 [compressed_tensors_w8a8_int8.py:62] Using CutlassScaledMMLinearKernel for CompressedTensorsW8A8Int8
(EngineCore_DP0 pid=1080490) INFO 01-25 06:01:17 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=1080490) INFO 01-25 06:01:17 [default_loader.py:308] Loading weights took 0.16 seconds
(EngineCore_DP0 pid=1080490) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1080490) INFO 01-25 06:01:17 [gpu_model_runner.py:3659] Model loading took 1.1949 GiB memory and 0.426865 seconds
(EngineCore_DP0 pid=1080490) WARNING 01-25 06:01:19 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=1080490) INFO 01-25 06:01:19 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/2531e9d93b/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=1080490) INFO 01-25 06:01:19 [backends.py:703] Dynamo bytecode transform time: 2.06 s
(EngineCore_DP0 pid=1080490) INFO 01-25 06:01:21 [backends.py:261] Cache the graph of compile range (1, 272) for later use
(EngineCore_DP0 pid=1080490) INFO 01-25 06:01:22 [backends.py:278] Compiling a graph for compile range (1, 272) takes 0.87 s
(EngineCore_DP0 pid=1080490) INFO 01-25 06:01:22 [monitor.py:34] torch.compile takes 2.93 s in total
(EngineCore_DP0 pid=1080490) INFO 01-25 06:01:22 [gpu_worker.py:375] Available KV cache memory: 10.44 GiB
(EngineCore_DP0 pid=1080490) INFO 01-25 06:01:22 [kv_cache_utils.py:1291] GPU KV cache size: 342,032 tokens
(EngineCore_DP0 pid=1080490) INFO 01-25 06:01:22 [kv_cache_utils.py:1296] Maximum concurrency for 272 tokens per request: 1257.47x
(EngineCore_DP0 pid=1080490) INFO 01-25 06:01:24 [gpu_model_runner.py:4587] Graph capturing finished in 2 secs, took 0.16 GiB
(EngineCore_DP0 pid=1080490) INFO 01-25 06:01:24 [core.py:259] init engine (profile, create kv cache, warmup model) took 7.27 seconds
INFO 01-25 06:01:25 [llm.py:360] Supported tasks: ['generate']
Throughput: 85.50 requests/s, 23256.30 total tokens/s, 21888.28 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768


─── STDERR ───
[2026-01-25 06:01:12] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:01:12] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-INT8'
[2026-01-25 06:01:12] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 06:01:12] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:01:12] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:01:12] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:01:12] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:01:12] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:01:12] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-INT8'
[2026-01-25 06:01:12] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 06:01:12] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:01:12] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:01:12] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:01:12] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 06:01:16] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:01:16] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-INT8'
[2026-01-25 06:01:16] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 06:01:16] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:01:16] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:01:16] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:01:16] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:01:16] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:01:16] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-INT8'
[2026-01-25 06:01:16] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 06:01:16] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:01:16] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:01:16] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:01:16] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1080490) [2026-01-25 06:01:17] INFO SlideSparseLinearMethod_INT8.py:801: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1080490) [2026-01-25 06:01:17] INFO gemm_wrapper.py:872: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1080490) [2026-01-25 06:01:17] INFO SlideSparseLinearMethod_INT8.py:448: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1080490) [2026-01-25 06:01:17] INFO SlideSparseLinearMethod_INT8.py:600: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=1080490) [2026-01-25 06:01:17] INFO kernels.py:132: Loaded tuned kernel for base model: Llama3.2-1B-INT8 (from Llama3.2-1B-INT8-SlideSparse-2_4)
(EngineCore_DP0 pid=1080490) [2026-01-25 06:01:17] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8-SlideSparse-2_4
(EngineCore_DP0 pid=1080490) [2026-01-25 06:01:17] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8-SlideSparse-2_4
(EngineCore_DP0 pid=1080490) [2026-01-25 06:01:17] INFO SlideSparseLinearMethod_INT8.py:617: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8-SlideSparse-2_4
(EngineCore_DP0 pid=1080490) [2026-01-25 06:01:17] INFO SlideSparseLinearMethod_INT8.py:621: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=1080490) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1080490) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  7.03it/s]
(EngineCore_DP0 pid=1080490) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  7.02it/s]
(EngineCore_DP0 pid=1080490) 
(EngineCore_DP0 pid=1080490) [2026-01-25 06:01:17] INFO SlideSparseLinearMethod_INT8.py:715: cuSPARSELt INT8 compression: [3072, 2048] -> 1D uint8
(EngineCore_DP0 pid=1080490) [2026-01-25 06:01:17] INFO SlideSparseLinearMethod_INT8.py:725: cuSPARSELt INT8 compression done: 4718592 bytes
(EngineCore_DP0 pid=1080490) [2026-01-25 06:01:17] INFO SlideSparseLinearMethod_INT8.py:715: cuSPARSELt INT8 compression: [2048, 2048] -> 1D uint8
(EngineCore_DP0 pid=1080490) [2026-01-25 06:01:17] INFO SlideSparseLinearMethod_INT8.py:725: cuSPARSELt INT8 compression done: 3145728 bytes
(EngineCore_DP0 pid=1080490) [2026-01-25 06:01:17] INFO SlideSparseLinearMethod_INT8.py:715: cuSPARSELt INT8 compression: [16384, 2048] -> 1D uint8
(EngineCore_DP0 pid=1080490) [2026-01-25 06:01:17] INFO SlideSparseLinearMethod_INT8.py:725: cuSPARSELt INT8 compression done: 25165824 bytes
(EngineCore_DP0 pid=1080490) [2026-01-25 06:01:17] INFO SlideSparseLinearMethod_INT8.py:715: cuSPARSELt INT8 compression: [2048, 8192] -> 1D uint8
(EngineCore_DP0 pid=1080490) [2026-01-25 06:01:17] INFO SlideSparseLinearMethod_INT8.py:725: cuSPARSELt INT8 compression done: 12582912 bytes
(EngineCore_DP0 pid=1080490) 2026-01-25 06:01:22,752 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1080490) 2026-01-25 06:01:22,766 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1080490) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▊         | 3/35 [00:00<00:01, 28.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 7/35 [00:00<00:00, 33.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 11/35 [00:00<00:00, 34.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 15/35 [00:00<00:00, 35.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  54%|█████▍    | 19/35 [00:00<00:00, 34.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  66%|██████▌   | 23/35 [00:00<00:00, 35.12it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  77%|███████▋  | 27/35 [00:00<00:00, 35.66it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▊ | 31/35 [00:00<00:00, 36.02it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:00<00:00, 35.63it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:00<00:00, 35.05it/s]
(EngineCore_DP0 pid=1080490) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   5%|▌         | 1/19 [00:00<00:03,  5.98it/s]
Capturing CUDA graphs (decode, FULL):  21%|██        | 4/19 [00:00<00:00, 16.13it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 7/19 [00:00<00:00, 18.38it/s]
Capturing CUDA graphs (decode, FULL):  47%|████▋     | 9/19 [00:00<00:00, 15.71it/s]
Capturing CUDA graphs (decode, FULL):  74%|███████▎  | 14/19 [00:00<00:00, 23.75it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:00<00:00, 29.26it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:00<00:00, 22.94it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 7134.02it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:01<03:01,  1.43s/it, est. speed input: 11.17 toks/s, output: 178.74 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00,  1.43s/it, est. speed input: 1386.35 toks/s, output: 22181.60 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 86.64it/s, est. speed input: 1386.35 toks/s, output: 22181.60 toks/s]
[rank0]:[W125 06:01:27.889384236 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 19.5s

测试结果:
  Requests/s:   85.50
  Tokens/s:     23256.30
  Total Reqs:   128
  Elapsed:      1.50s

  [Decode 分析]
  Total Decode Tokens:  32768
  Decode Tokens/s:      21888.28

============================================================
[3/3] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 4096 (= 256 x 16)
│   M_decode      = 256
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 256
│   --max-num-seqs           = 256
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
INFO 01-25 06:01:31 [datasets.py:612] Sampling input_len from [15, 15] and output_len from [256, 256]
INFO 01-25 06:01:32 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4', 'max_model_len': 272, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 272, 'max_num_seqs': 256, 'disable_log_stats': True, 'enable_lora': None, 'enable_chunked_prefill': False, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4'}
INFO 01-25 06:01:32 [model.py:514] Resolved architecture: LlamaForCausalLM
INFO 01-25 06:01:32 [model.py:1661] Using max model len 272
WARNING 01-25 06:01:32 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1080851) INFO 01-25 06:01:35 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4', speculative_config=None, tokenizer='/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=272, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4, enable_prefix_caching=True, enable_chunked_prefill=False, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [272], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 272, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=1080851) INFO 01-25 06:01:36 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:45843 backend=nccl
(EngineCore_DP0 pid=1080851) INFO 01-25 06:01:36 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=1080851) INFO 01-25 06:01:36 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4...
(EngineCore_DP0 pid=1080851) INFO 01-25 06:01:36 [compressed_tensors_w8a8_int8.py:62] Using CutlassScaledMMLinearKernel for CompressedTensorsW8A8Int8
(EngineCore_DP0 pid=1080851) INFO 01-25 06:01:36 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=1080851) INFO 01-25 06:01:36 [default_loader.py:308] Loading weights took 0.16 seconds
(EngineCore_DP0 pid=1080851) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1080851) INFO 01-25 06:01:37 [gpu_model_runner.py:3659] Model loading took 1.1949 GiB memory and 0.426391 seconds
(EngineCore_DP0 pid=1080851) WARNING 01-25 06:01:39 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=1080851) INFO 01-25 06:01:39 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/41c0e195af/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=1080851) INFO 01-25 06:01:39 [backends.py:703] Dynamo bytecode transform time: 2.07 s
(EngineCore_DP0 pid=1080851) INFO 01-25 06:01:40 [backends.py:261] Cache the graph of compile range (1, 272) for later use
(EngineCore_DP0 pid=1080851) INFO 01-25 06:01:41 [backends.py:278] Compiling a graph for compile range (1, 272) takes 0.86 s
(EngineCore_DP0 pid=1080851) INFO 01-25 06:01:41 [monitor.py:34] torch.compile takes 2.93 s in total
(EngineCore_DP0 pid=1080851) INFO 01-25 06:01:42 [gpu_worker.py:375] Available KV cache memory: 9.86 GiB
(EngineCore_DP0 pid=1080851) INFO 01-25 06:01:42 [kv_cache_utils.py:1291] GPU KV cache size: 322,976 tokens
(EngineCore_DP0 pid=1080851) INFO 01-25 06:01:42 [kv_cache_utils.py:1296] Maximum concurrency for 272 tokens per request: 1187.41x
(EngineCore_DP0 pid=1080851) INFO 01-25 06:01:44 [gpu_model_runner.py:4587] Graph capturing finished in 2 secs, took 0.21 GiB
(EngineCore_DP0 pid=1080851) INFO 01-25 06:01:44 [core.py:259] init engine (profile, create kv cache, warmup model) took 7.52 seconds
INFO 01-25 06:01:45 [llm.py:360] Supported tasks: ['generate']
Throughput: 92.53 requests/s, 25169.03 total tokens/s, 23688.50 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536


─── STDERR ───
[2026-01-25 06:01:32] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:01:32] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-INT8'
[2026-01-25 06:01:32] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 06:01:32] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:01:32] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:01:32] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:01:32] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:01:32] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:01:32] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-INT8'
[2026-01-25 06:01:32] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 06:01:32] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:01:32] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:01:32] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:01:32] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 06:01:35] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:01:35] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-INT8'
[2026-01-25 06:01:35] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 06:01:35] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:01:35] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:01:35] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:01:35] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:01:35] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:01:35] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-INT8'
[2026-01-25 06:01:35] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 06:01:35] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:01:35] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:01:35] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:01:35] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1080851) [2026-01-25 06:01:36] INFO SlideSparseLinearMethod_INT8.py:801: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1080851) [2026-01-25 06:01:36] INFO gemm_wrapper.py:872: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1080851) [2026-01-25 06:01:36] INFO SlideSparseLinearMethod_INT8.py:448: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1080851) [2026-01-25 06:01:36] INFO SlideSparseLinearMethod_INT8.py:600: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=1080851) [2026-01-25 06:01:36] INFO kernels.py:132: Loaded tuned kernel for base model: Llama3.2-1B-INT8 (from Llama3.2-1B-INT8-SlideSparse-2_4)
(EngineCore_DP0 pid=1080851) [2026-01-25 06:01:36] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8-SlideSparse-2_4
(EngineCore_DP0 pid=1080851) [2026-01-25 06:01:36] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8-SlideSparse-2_4
(EngineCore_DP0 pid=1080851) [2026-01-25 06:01:36] INFO SlideSparseLinearMethod_INT8.py:617: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8-SlideSparse-2_4
(EngineCore_DP0 pid=1080851) [2026-01-25 06:01:36] INFO SlideSparseLinearMethod_INT8.py:621: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=1080851) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1080851) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  7.06it/s]
(EngineCore_DP0 pid=1080851) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  7.06it/s]
(EngineCore_DP0 pid=1080851) 
(EngineCore_DP0 pid=1080851) [2026-01-25 06:01:36] INFO SlideSparseLinearMethod_INT8.py:715: cuSPARSELt INT8 compression: [3072, 2048] -> 1D uint8
(EngineCore_DP0 pid=1080851) [2026-01-25 06:01:37] INFO SlideSparseLinearMethod_INT8.py:725: cuSPARSELt INT8 compression done: 4718592 bytes
(EngineCore_DP0 pid=1080851) [2026-01-25 06:01:37] INFO SlideSparseLinearMethod_INT8.py:715: cuSPARSELt INT8 compression: [2048, 2048] -> 1D uint8
(EngineCore_DP0 pid=1080851) [2026-01-25 06:01:37] INFO SlideSparseLinearMethod_INT8.py:725: cuSPARSELt INT8 compression done: 3145728 bytes
(EngineCore_DP0 pid=1080851) [2026-01-25 06:01:37] INFO SlideSparseLinearMethod_INT8.py:715: cuSPARSELt INT8 compression: [16384, 2048] -> 1D uint8
(EngineCore_DP0 pid=1080851) [2026-01-25 06:01:37] INFO SlideSparseLinearMethod_INT8.py:725: cuSPARSELt INT8 compression done: 25165824 bytes
(EngineCore_DP0 pid=1080851) [2026-01-25 06:01:37] INFO SlideSparseLinearMethod_INT8.py:715: cuSPARSELt INT8 compression: [2048, 8192] -> 1D uint8
(EngineCore_DP0 pid=1080851) [2026-01-25 06:01:37] INFO SlideSparseLinearMethod_INT8.py:725: cuSPARSELt INT8 compression done: 12582912 bytes
(EngineCore_DP0 pid=1080851) 2026-01-25 06:01:42,487 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1080851) 2026-01-25 06:01:42,501 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1080851) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/36 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 4/36 [00:00<00:00, 34.34it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 8/36 [00:00<00:00, 35.55it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 12/36 [00:00<00:00, 36.12it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  44%|████▍     | 16/36 [00:00<00:00, 36.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  56%|█████▌    | 20/36 [00:00<00:00, 35.63it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 24/36 [00:00<00:00, 36.03it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 28/36 [00:00<00:00, 36.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 32/36 [00:00<00:00, 36.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:01<00:00, 35.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:01<00:00, 35.94it/s]
(EngineCore_DP0 pid=1080851) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   3%|▎         | 1/35 [00:00<00:04,  6.86it/s]
Capturing CUDA graphs (decode, FULL):  17%|█▋        | 6/35 [00:00<00:01, 25.49it/s]
Capturing CUDA graphs (decode, FULL):  31%|███▏      | 11/35 [00:00<00:00, 32.32it/s]
Capturing CUDA graphs (decode, FULL):  46%|████▌     | 16/35 [00:00<00:00, 35.63it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 21/35 [00:00<00:00, 37.32it/s]
Capturing CUDA graphs (decode, FULL):  74%|███████▍  | 26/35 [00:00<00:00, 38.52it/s]
Capturing CUDA graphs (decode, FULL):  89%|████████▊ | 31/35 [00:00<00:00, 39.16it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:00<00:00, 39.34it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:00<00:00, 35.66it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 9447.38it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:02<10:38,  2.50s/it, est. speed input: 6.39 toks/s, output: 102.20 toks/s]
Processed prompts:  57%|█████▋    | 146/256 [00:02<00:01, 78.56it/s, est. speed input: 896.38 toks/s, output: 14341.99 toks/s]
Processed prompts:  96%|█████████▌| 245/256 [00:02<00:00, 143.70it/s, est. speed input: 1448.38 toks/s, output: 23174.00 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 143.70it/s, est. speed input: 1495.69 toks/s, output: 23930.98 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 93.48it/s, est. speed input: 1495.69 toks/s, output: 23930.98 toks/s] 
[rank0]:[W125 06:01:48.184360806 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 21.3s

测试结果:
  Requests/s:   92.53
  Tokens/s:     25169.03
  Total Reqs:   256
  Elapsed:      2.77s

  [Decode 分析]
  Total Decode Tokens:  65536
  Decode Tokens/s:      23688.50


------------------------------------------------------------
  生成 CSV: Llama3.2-1B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_4/Llama3.2-1B-INT8_decode.csv

预览:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
16,16,16,16,256,256,20.9094,5687.3678,0.7652
128,16,128,128,256,256,85.5011,23256.3019,1.4971
256,16,256,256,256,256,92.5332,25169.0318,2.7666

------------------------------------------------------------

[INFO] 完成: 3 成功, 0 失败

======================================================================
[SlideSparse] 未找到 Llama3.2-1B-INT8-SlideSparse-2_6 checkpoint
[SlideSparse] 发现基础模型: Llama3.2-1B-INT8
[SlideSparse] 自动转换中...
[SlideSparse] 开始转换: Llama3.2-1B-INT8 -> SlideSparse-2_6
======================================================================

[SlideSparse] 转换成功: Llama3.2-1B-INT8-SlideSparse-2_6
======================================================================


============================================================
  Llama3.2-1B-INT8 | cuSPARSELt (2_6) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_6

============================================================
[1/3] 测试 M=16
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16
│   M_prefill     = 16 (= 1 x 16)
│   M_decode      = 1
│   batched_tokens = 17 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 17
│   --max-num-batched-tokens = 17
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
INFO 01-25 06:02:23 [datasets.py:612] Sampling input_len from [15, 15] and output_len from [1, 1]
INFO 01-25 06:02:23 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6', 'max_model_len': 17, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 17, 'max_num_seqs': 1, 'disable_log_stats': True, 'enable_lora': None, 'enable_chunked_prefill': False, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6'}
INFO 01-25 06:02:23 [model.py:514] Resolved architecture: LlamaForCausalLM
INFO 01-25 06:02:23 [model.py:1661] Using max model len 17
WARNING 01-25 06:02:23 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1081501) INFO 01-25 06:02:27 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6', speculative_config=None, tokenizer='/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=17, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6, enable_prefix_caching=True, enable_chunked_prefill=False, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [17], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 2, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=1081501) INFO 01-25 06:02:27 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:54921 backend=nccl
(EngineCore_DP0 pid=1081501) INFO 01-25 06:02:27 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=1081501) INFO 01-25 06:02:27 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6...
(EngineCore_DP0 pid=1081501) INFO 01-25 06:02:28 [compressed_tensors_w8a8_int8.py:62] Using CutlassScaledMMLinearKernel for CompressedTensorsW8A8Int8
(EngineCore_DP0 pid=1081501) INFO 01-25 06:02:28 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=1081501) INFO 01-25 06:02:28 [default_loader.py:308] Loading weights took 0.19 seconds
(EngineCore_DP0 pid=1081501) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1081501) INFO 01-25 06:02:28 [gpu_model_runner.py:3659] Model loading took 1.4249 GiB memory and 0.475195 seconds
(EngineCore_DP0 pid=1081501) WARNING 01-25 06:02:30 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=1081501) INFO 01-25 06:02:30 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/f9e33fe43a/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=1081501) INFO 01-25 06:02:30 [backends.py:703] Dynamo bytecode transform time: 2.05 s
(EngineCore_DP0 pid=1081501) INFO 01-25 06:02:33 [backends.py:261] Cache the graph of compile range (1, 17) for later use
(EngineCore_DP0 pid=1081501) INFO 01-25 06:02:34 [backends.py:278] Compiling a graph for compile range (1, 17) takes 2.18 s
(EngineCore_DP0 pid=1081501) INFO 01-25 06:02:34 [monitor.py:34] torch.compile takes 4.23 s in total
(EngineCore_DP0 pid=1081501) INFO 01-25 06:02:35 [gpu_worker.py:375] Available KV cache memory: 10.37 GiB
(EngineCore_DP0 pid=1081501) INFO 01-25 06:02:35 [kv_cache_utils.py:1291] GPU KV cache size: 339,872 tokens
(EngineCore_DP0 pid=1081501) INFO 01-25 06:02:35 [kv_cache_utils.py:1296] Maximum concurrency for 17 tokens per request: 10621.00x
(EngineCore_DP0 pid=1081501) INFO 01-25 06:02:36 [gpu_model_runner.py:4587] Graph capturing finished in 1 secs, took 0.01 GiB
(EngineCore_DP0 pid=1081501) INFO 01-25 06:02:36 [core.py:259] init engine (profile, create kv cache, warmup model) took 7.37 seconds
INFO 01-25 06:02:36 [llm.py:360] Supported tasks: ['generate']
Throughput: 63.08 requests/s, 1072.38 total tokens/s, 63.08 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  128


─── STDERR ───
[2026-01-25 06:02:23] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:02:23] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-INT8'
[2026-01-25 06:02:23] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 06:02:23] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:02:23] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:02:23] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:02:23] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:02:23] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:02:23] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-INT8'
[2026-01-25 06:02:23] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 06:02:23] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:02:23] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:02:23] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:02:23] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 06:02:27] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:02:27] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-INT8'
[2026-01-25 06:02:27] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 06:02:27] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:02:27] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:02:27] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:02:27] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:02:27] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:02:27] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-INT8'
[2026-01-25 06:02:27] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 06:02:27] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:02:27] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:02:27] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:02:27] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1081501) [2026-01-25 06:02:28] INFO SlideSparseLinearMethod_INT8.py:801: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1081501) [2026-01-25 06:02:28] INFO gemm_wrapper.py:872: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1081501) [2026-01-25 06:02:28] INFO SlideSparseLinearMethod_INT8.py:448: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1081501) [2026-01-25 06:02:28] INFO SlideSparseLinearMethod_INT8.py:600: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=1081501) [2026-01-25 06:02:28] INFO kernels.py:132: Loaded tuned kernel for base model: Llama3.2-1B-INT8 (from Llama3.2-1B-INT8-SlideSparse-2_6)
(EngineCore_DP0 pid=1081501) [2026-01-25 06:02:28] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8-SlideSparse-2_6
(EngineCore_DP0 pid=1081501) [2026-01-25 06:02:28] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8-SlideSparse-2_6
(EngineCore_DP0 pid=1081501) [2026-01-25 06:02:28] INFO SlideSparseLinearMethod_INT8.py:617: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8-SlideSparse-2_6
(EngineCore_DP0 pid=1081501) [2026-01-25 06:02:28] INFO SlideSparseLinearMethod_INT8.py:621: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=1081501) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1081501) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.97it/s]
(EngineCore_DP0 pid=1081501) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.96it/s]
(EngineCore_DP0 pid=1081501) 
(EngineCore_DP0 pid=1081501) [2026-01-25 06:02:28] INFO SlideSparseLinearMethod_INT8.py:715: cuSPARSELt INT8 compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=1081501) [2026-01-25 06:02:28] INFO SlideSparseLinearMethod_INT8.py:725: cuSPARSELt INT8 compression done: 6389760 bytes
(EngineCore_DP0 pid=1081501) [2026-01-25 06:02:28] INFO SlideSparseLinearMethod_INT8.py:715: cuSPARSELt INT8 compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=1081501) [2026-01-25 06:02:28] INFO SlideSparseLinearMethod_INT8.py:725: cuSPARSELt INT8 compression done: 4259840 bytes
(EngineCore_DP0 pid=1081501) [2026-01-25 06:02:28] INFO SlideSparseLinearMethod_INT8.py:715: cuSPARSELt INT8 compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=1081501) [2026-01-25 06:02:28] INFO SlideSparseLinearMethod_INT8.py:725: cuSPARSELt INT8 compression done: 34078720 bytes
(EngineCore_DP0 pid=1081501) [2026-01-25 06:02:28] INFO SlideSparseLinearMethod_INT8.py:715: cuSPARSELt INT8 compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=1081501) [2026-01-25 06:02:28] INFO SlideSparseLinearMethod_INT8.py:725: cuSPARSELt INT8 compression done: 16842752 bytes
(EngineCore_DP0 pid=1081501) 2026-01-25 06:02:35,219 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1081501) 2026-01-25 06:02:35,233 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1081501) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  7.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  5.15it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  5.39it/s]
(EngineCore_DP0 pid=1081501) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.92it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.91it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 6094.23it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:29,  4.27it/s, est. speed input: 68.39 toks/s, output: 4.27 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:03, 31.41it/s, est. speed input: 414.83 toks/s, output: 25.93 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:02, 46.18it/s, est. speed input: 593.41 toks/s, output: 37.09 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:00<00:01, 55.08it/s, est. speed input: 702.89 toks/s, output: 43.93 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:00<00:01, 60.31it/s, est. speed input: 774.15 toks/s, output: 48.38 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:00<00:01, 63.99it/s, est. speed input: 826.80 toks/s, output: 51.67 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:00<00:01, 66.58it/s, est. speed input: 867.24 toks/s, output: 54.20 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:01<00:01, 68.83it/s, est. speed input: 901.08 toks/s, output: 56.32 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:01<00:00, 69.92it/s, est. speed input: 926.37 toks/s, output: 57.90 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:01<00:00, 70.74it/s, est. speed input: 947.40 toks/s, output: 59.21 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:01<00:00, 70.25it/s, est. speed input: 961.09 toks/s, output: 60.07 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:01<00:00, 70.54it/s, est. speed input: 974.80 toks/s, output: 60.92 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:01<00:00, 70.74it/s, est. speed input: 986.52 toks/s, output: 61.66 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:01<00:00, 71.04it/s, est. speed input: 997.22 toks/s, output: 62.33 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:01<00:00, 71.14it/s, est. speed input: 1006.24 toks/s, output: 62.89 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:01<00:00, 71.09it/s, est. speed input: 1013.87 toks/s, output: 63.37 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 71.09it/s, est. speed input: 1020.76 toks/s, output: 63.80 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 63.79it/s, est. speed input: 1020.76 toks/s, output: 63.80 toks/s]
[rank0]:[W125 06:02:38.624093371 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 20.4s

测试结果:
  Requests/s:   63.08
  Tokens/s:     1072.38
  Total Reqs:   128
  Elapsed:      2.03s

  [Prefill 分析]
  Total Prefill Tokens: 2048
  Prefill Tokens/s:     1009.29

============================================================
[2/3] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 128 (= 1 x 128)
│   M_decode      = 1
│   batched_tokens = 129 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 128
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 129
│   --max-num-batched-tokens = 129
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
INFO 01-25 06:02:43 [datasets.py:612] Sampling input_len from [127, 127] and output_len from [1, 1]
INFO 01-25 06:02:43 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6', 'max_model_len': 129, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 129, 'max_num_seqs': 1, 'disable_log_stats': True, 'enable_lora': None, 'enable_chunked_prefill': False, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6'}
INFO 01-25 06:02:43 [model.py:514] Resolved architecture: LlamaForCausalLM
INFO 01-25 06:02:43 [model.py:1661] Using max model len 129
WARNING 01-25 06:02:43 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1081925) INFO 01-25 06:02:47 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6', speculative_config=None, tokenizer='/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=129, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6, enable_prefix_caching=True, enable_chunked_prefill=False, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [129], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 2, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=1081925) INFO 01-25 06:02:47 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:41733 backend=nccl
(EngineCore_DP0 pid=1081925) INFO 01-25 06:02:47 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=1081925) INFO 01-25 06:02:48 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6...
(EngineCore_DP0 pid=1081925) INFO 01-25 06:02:48 [compressed_tensors_w8a8_int8.py:62] Using CutlassScaledMMLinearKernel for CompressedTensorsW8A8Int8
(EngineCore_DP0 pid=1081925) INFO 01-25 06:02:48 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=1081925) INFO 01-25 06:02:48 [default_loader.py:308] Loading weights took 0.19 seconds
(EngineCore_DP0 pid=1081925) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1081925) INFO 01-25 06:02:49 [gpu_model_runner.py:3659] Model loading took 1.4249 GiB memory and 0.491890 seconds
(EngineCore_DP0 pid=1081925) WARNING 01-25 06:02:51 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=1081925) INFO 01-25 06:02:51 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/589cee5511/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=1081925) INFO 01-25 06:02:51 [backends.py:703] Dynamo bytecode transform time: 2.07 s
(EngineCore_DP0 pid=1081925) INFO 01-25 06:02:53 [backends.py:261] Cache the graph of compile range (1, 129) for later use
(EngineCore_DP0 pid=1081925) INFO 01-25 06:02:54 [backends.py:278] Compiling a graph for compile range (1, 129) takes 2.19 s
(EngineCore_DP0 pid=1081925) INFO 01-25 06:02:54 [monitor.py:34] torch.compile takes 4.26 s in total
(EngineCore_DP0 pid=1081925) INFO 01-25 06:02:55 [gpu_worker.py:375] Available KV cache memory: 10.37 GiB
(EngineCore_DP0 pid=1081925) INFO 01-25 06:02:55 [kv_cache_utils.py:1291] GPU KV cache size: 339,840 tokens
(EngineCore_DP0 pid=1081925) INFO 01-25 06:02:55 [kv_cache_utils.py:1296] Maximum concurrency for 129 tokens per request: 2360.00x
(EngineCore_DP0 pid=1081925) INFO 01-25 06:02:56 [gpu_model_runner.py:4587] Graph capturing finished in 1 secs, took 0.01 GiB
(EngineCore_DP0 pid=1081925) INFO 01-25 06:02:56 [core.py:259] init engine (profile, create kv cache, warmup model) took 7.12 seconds
INFO 01-25 06:02:56 [llm.py:360] Supported tasks: ['generate']
Throughput: 64.44 requests/s, 8312.19 total tokens/s, 64.44 output tokens/s
Total num prompt tokens:  16384
Total num output tokens:  128


─── STDERR ───
[2026-01-25 06:02:43] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:02:43] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-INT8'
[2026-01-25 06:02:43] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 06:02:43] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:02:43] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:02:43] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:02:43] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:02:43] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:02:43] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-INT8'
[2026-01-25 06:02:43] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 06:02:43] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:02:43] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:02:43] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:02:43] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 06:02:47] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:02:47] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-INT8'
[2026-01-25 06:02:47] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 06:02:47] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:02:47] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:02:47] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:02:47] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:02:47] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:02:47] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-INT8'
[2026-01-25 06:02:47] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 06:02:47] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:02:47] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:02:47] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:02:47] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1081925) [2026-01-25 06:02:48] INFO SlideSparseLinearMethod_INT8.py:801: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1081925) [2026-01-25 06:02:48] INFO gemm_wrapper.py:872: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1081925) [2026-01-25 06:02:48] INFO SlideSparseLinearMethod_INT8.py:448: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1081925) [2026-01-25 06:02:48] INFO SlideSparseLinearMethod_INT8.py:600: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=1081925) [2026-01-25 06:02:48] INFO kernels.py:132: Loaded tuned kernel for base model: Llama3.2-1B-INT8 (from Llama3.2-1B-INT8-SlideSparse-2_6)
(EngineCore_DP0 pid=1081925) [2026-01-25 06:02:48] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8-SlideSparse-2_6
(EngineCore_DP0 pid=1081925) [2026-01-25 06:02:48] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8-SlideSparse-2_6
(EngineCore_DP0 pid=1081925) [2026-01-25 06:02:48] INFO SlideSparseLinearMethod_INT8.py:617: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8-SlideSparse-2_6
(EngineCore_DP0 pid=1081925) [2026-01-25 06:02:48] INFO SlideSparseLinearMethod_INT8.py:621: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=1081925) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1081925) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.16it/s]
(EngineCore_DP0 pid=1081925) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.15it/s]
(EngineCore_DP0 pid=1081925) 
(EngineCore_DP0 pid=1081925) [2026-01-25 06:02:48] INFO SlideSparseLinearMethod_INT8.py:715: cuSPARSELt INT8 compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=1081925) [2026-01-25 06:02:48] INFO SlideSparseLinearMethod_INT8.py:725: cuSPARSELt INT8 compression done: 6389760 bytes
(EngineCore_DP0 pid=1081925) [2026-01-25 06:02:48] INFO SlideSparseLinearMethod_INT8.py:715: cuSPARSELt INT8 compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=1081925) [2026-01-25 06:02:48] INFO SlideSparseLinearMethod_INT8.py:725: cuSPARSELt INT8 compression done: 4259840 bytes
(EngineCore_DP0 pid=1081925) [2026-01-25 06:02:48] INFO SlideSparseLinearMethod_INT8.py:715: cuSPARSELt INT8 compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=1081925) [2026-01-25 06:02:48] INFO SlideSparseLinearMethod_INT8.py:725: cuSPARSELt INT8 compression done: 34078720 bytes
(EngineCore_DP0 pid=1081925) [2026-01-25 06:02:48] INFO SlideSparseLinearMethod_INT8.py:715: cuSPARSELt INT8 compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=1081925) [2026-01-25 06:02:48] INFO SlideSparseLinearMethod_INT8.py:725: cuSPARSELt INT8 compression done: 16842752 bytes
(EngineCore_DP0 pid=1081925) 2026-01-25 06:02:55,638 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1081925) 2026-01-25 06:02:55,652 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1081925) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 32.55it/s]
(EngineCore_DP0 pid=1081925) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.95it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.95it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 2502.01it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:21,  6.03it/s, est. speed input: 771.38 toks/s, output: 6.03 toks/s]
Processed prompts:   6%|▋         | 8/128 [00:00<00:03, 35.54it/s, est. speed input: 3843.38 toks/s, output: 30.02 toks/s]
Processed prompts:  12%|█▎        | 16/128 [00:00<00:02, 50.50it/s, est. speed input: 5390.52 toks/s, output: 42.11 toks/s]
Processed prompts:  19%|█▉        | 24/128 [00:00<00:01, 58.69it/s, est. speed input: 6264.93 toks/s, output: 48.94 toks/s]
Processed prompts:  25%|██▌       | 32/128 [00:00<00:01, 62.94it/s, est. speed input: 6787.49 toks/s, output: 53.03 toks/s]
Processed prompts:  31%|███▏      | 40/128 [00:00<00:01, 65.82it/s, est. speed input: 7159.07 toks/s, output: 55.93 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:00<00:01, 67.53it/s, est. speed input: 7422.71 toks/s, output: 57.99 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:00<00:01, 68.80it/s, est. speed input: 7629.59 toks/s, output: 59.61 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:01<00:00, 70.02it/s, est. speed input: 7806.17 toks/s, output: 60.99 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:01<00:00, 70.85it/s, est. speed input: 7948.85 toks/s, output: 62.10 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:01<00:00, 71.29it/s, est. speed input: 8062.63 toks/s, output: 62.99 toks/s]
Processed prompts:  69%|██████▉   | 88/128 [00:01<00:00, 71.63it/s, est. speed input: 8159.18 toks/s, output: 63.74 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:01<00:00, 72.00it/s, est. speed input: 8245.02 toks/s, output: 64.41 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:01<00:00, 72.13it/s, est. speed input: 8315.91 toks/s, output: 64.97 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:01<00:00, 72.31it/s, est. speed input: 8379.91 toks/s, output: 65.47 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:01<00:00, 72.22it/s, est. speed input: 8430.74 toks/s, output: 65.86 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 72.05it/s, est. speed input: 8473.63 toks/s, output: 66.20 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 72.05it/s, est. speed input: 8473.63 toks/s, output: 66.20 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 66.20it/s, est. speed input: 8473.63 toks/s, output: 66.20 toks/s]
[rank0]:[W125 06:02:58.683114067 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 20.1s

测试结果:
  Requests/s:   64.44
  Tokens/s:     8312.19
  Total Reqs:   128
  Elapsed:      1.99s

  [Prefill 分析]
  Total Prefill Tokens: 16384
  Prefill Tokens/s:     8247.76

============================================================
[3/3] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 256 (= 1 x 256)
│   M_decode      = 1
│   batched_tokens = 257 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 256
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 257
│   --max-num-batched-tokens = 257
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
INFO 01-25 06:03:03 [datasets.py:612] Sampling input_len from [255, 255] and output_len from [1, 1]
INFO 01-25 06:03:03 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6', 'max_model_len': 257, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 257, 'max_num_seqs': 1, 'disable_log_stats': True, 'enable_lora': None, 'enable_chunked_prefill': False, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6'}
INFO 01-25 06:03:03 [model.py:514] Resolved architecture: LlamaForCausalLM
INFO 01-25 06:03:03 [model.py:1661] Using max model len 257
WARNING 01-25 06:03:04 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1082342) INFO 01-25 06:03:07 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6', speculative_config=None, tokenizer='/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=257, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6, enable_prefix_caching=True, enable_chunked_prefill=False, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [257], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 2, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=1082342) INFO 01-25 06:03:08 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:52619 backend=nccl
(EngineCore_DP0 pid=1082342) INFO 01-25 06:03:08 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=1082342) INFO 01-25 06:03:08 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6...
(EngineCore_DP0 pid=1082342) INFO 01-25 06:03:08 [compressed_tensors_w8a8_int8.py:62] Using CutlassScaledMMLinearKernel for CompressedTensorsW8A8Int8
(EngineCore_DP0 pid=1082342) INFO 01-25 06:03:08 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=1082342) INFO 01-25 06:03:08 [default_loader.py:308] Loading weights took 0.19 seconds
(EngineCore_DP0 pid=1082342) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1082342) INFO 01-25 06:03:09 [gpu_model_runner.py:3659] Model loading took 1.4249 GiB memory and 0.465270 seconds
(EngineCore_DP0 pid=1082342) WARNING 01-25 06:03:11 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=1082342) INFO 01-25 06:03:11 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/1aab3e659b/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=1082342) INFO 01-25 06:03:11 [backends.py:703] Dynamo bytecode transform time: 2.05 s
(EngineCore_DP0 pid=1082342) INFO 01-25 06:03:13 [backends.py:261] Cache the graph of compile range (1, 257) for later use
(EngineCore_DP0 pid=1082342) INFO 01-25 06:03:14 [backends.py:278] Compiling a graph for compile range (1, 257) takes 1.97 s
(EngineCore_DP0 pid=1082342) INFO 01-25 06:03:14 [monitor.py:34] torch.compile takes 4.02 s in total
(EngineCore_DP0 pid=1082342) INFO 01-25 06:03:15 [gpu_worker.py:375] Available KV cache memory: 10.37 GiB
(EngineCore_DP0 pid=1082342) INFO 01-25 06:03:15 [kv_cache_utils.py:1291] GPU KV cache size: 339,792 tokens
(EngineCore_DP0 pid=1082342) INFO 01-25 06:03:15 [kv_cache_utils.py:1296] Maximum concurrency for 257 tokens per request: 1249.24x
(EngineCore_DP0 pid=1082342) INFO 01-25 06:03:16 [gpu_model_runner.py:4587] Graph capturing finished in 1 secs, took 0.01 GiB
(EngineCore_DP0 pid=1082342) INFO 01-25 06:03:16 [core.py:259] init engine (profile, create kv cache, warmup model) took 6.88 seconds
INFO 01-25 06:03:16 [llm.py:360] Supported tasks: ['generate']
Throughput: 63.01 requests/s, 16194.14 total tokens/s, 63.01 output tokens/s
Total num prompt tokens:  32768
Total num output tokens:  128


─── STDERR ───
[2026-01-25 06:03:03] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:03:04] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-INT8'
[2026-01-25 06:03:04] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 06:03:04] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:03:04] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:03:04] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:03:04] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:03:04] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:03:04] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-INT8'
[2026-01-25 06:03:04] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 06:03:04] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:03:04] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:03:04] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:03:04] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 06:03:07] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:03:07] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-INT8'
[2026-01-25 06:03:07] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 06:03:07] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:03:07] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:03:07] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:03:07] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:03:07] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:03:07] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-INT8'
[2026-01-25 06:03:07] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 06:03:07] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:03:07] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:03:07] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:03:07] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1082342) [2026-01-25 06:03:08] INFO SlideSparseLinearMethod_INT8.py:801: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1082342) [2026-01-25 06:03:08] INFO gemm_wrapper.py:872: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1082342) [2026-01-25 06:03:08] INFO SlideSparseLinearMethod_INT8.py:448: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1082342) [2026-01-25 06:03:08] INFO SlideSparseLinearMethod_INT8.py:600: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=1082342) [2026-01-25 06:03:08] INFO kernels.py:132: Loaded tuned kernel for base model: Llama3.2-1B-INT8 (from Llama3.2-1B-INT8-SlideSparse-2_6)
(EngineCore_DP0 pid=1082342) [2026-01-25 06:03:08] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8-SlideSparse-2_6
(EngineCore_DP0 pid=1082342) [2026-01-25 06:03:08] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8-SlideSparse-2_6
(EngineCore_DP0 pid=1082342) [2026-01-25 06:03:08] INFO SlideSparseLinearMethod_INT8.py:617: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8-SlideSparse-2_6
(EngineCore_DP0 pid=1082342) [2026-01-25 06:03:08] INFO SlideSparseLinearMethod_INT8.py:621: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=1082342) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1082342) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.11it/s]
(EngineCore_DP0 pid=1082342) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.11it/s]
(EngineCore_DP0 pid=1082342) 
(EngineCore_DP0 pid=1082342) [2026-01-25 06:03:08] INFO SlideSparseLinearMethod_INT8.py:715: cuSPARSELt INT8 compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=1082342) [2026-01-25 06:03:08] INFO SlideSparseLinearMethod_INT8.py:725: cuSPARSELt INT8 compression done: 6389760 bytes
(EngineCore_DP0 pid=1082342) [2026-01-25 06:03:08] INFO SlideSparseLinearMethod_INT8.py:715: cuSPARSELt INT8 compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=1082342) [2026-01-25 06:03:08] INFO SlideSparseLinearMethod_INT8.py:725: cuSPARSELt INT8 compression done: 4259840 bytes
(EngineCore_DP0 pid=1082342) [2026-01-25 06:03:08] INFO SlideSparseLinearMethod_INT8.py:715: cuSPARSELt INT8 compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=1082342) [2026-01-25 06:03:08] INFO SlideSparseLinearMethod_INT8.py:725: cuSPARSELt INT8 compression done: 34078720 bytes
(EngineCore_DP0 pid=1082342) [2026-01-25 06:03:08] INFO SlideSparseLinearMethod_INT8.py:715: cuSPARSELt INT8 compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=1082342) [2026-01-25 06:03:08] INFO SlideSparseLinearMethod_INT8.py:725: cuSPARSELt INT8 compression done: 16842752 bytes
(EngineCore_DP0 pid=1082342) 2026-01-25 06:03:15,469 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1082342) 2026-01-25 06:03:15,483 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1082342) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 31.61it/s]
(EngineCore_DP0 pid=1082342) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.94it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.93it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 2005.07it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:24,  5.11it/s, est. speed input: 1309.11 toks/s, output: 5.11 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:03, 34.76it/s, est. speed input: 7458.46 toks/s, output: 29.13 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:02, 49.36it/s, est. speed input: 10381.43 toks/s, output: 40.55 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:00<00:01, 57.12it/s, est. speed input: 12027.08 toks/s, output: 46.98 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:00<00:01, 61.53it/s, est. speed input: 13068.85 toks/s, output: 51.05 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:00<00:01, 64.87it/s, est. speed input: 13851.05 toks/s, output: 54.11 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:00<00:01, 67.69it/s, est. speed input: 14486.94 toks/s, output: 56.59 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:00<00:01, 69.37it/s, est. speed input: 14961.56 toks/s, output: 58.44 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:01<00:00, 70.68it/s, est. speed input: 15351.76 toks/s, output: 59.97 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:01<00:00, 71.63it/s, est. speed input: 15674.08 toks/s, output: 61.23 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:01<00:00, 71.65it/s, est. speed input: 15903.47 toks/s, output: 62.12 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:01<00:00, 71.79it/s, est. speed input: 16104.02 toks/s, output: 62.91 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:01<00:00, 71.48it/s, est. speed input: 16253.36 toks/s, output: 63.49 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:01<00:00, 71.93it/s, est. speed input: 16416.16 toks/s, output: 64.13 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:01<00:00, 70.75it/s, est. speed input: 16485.01 toks/s, output: 64.39 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:01<00:00, 70.29it/s, est. speed input: 16562.02 toks/s, output: 64.70 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 70.29it/s, est. speed input: 16670.91 toks/s, output: 65.12 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 65.12it/s, est. speed input: 16670.91 toks/s, output: 65.12 toks/s]
[rank0]:[W125 06:03:18.553354242 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 19.8s

测试结果:
  Requests/s:   63.01
  Tokens/s:     16194.14
  Total Reqs:   128
  Elapsed:      2.03s

  [Prefill 分析]
  Total Prefill Tokens: 32768
  Prefill Tokens/s:     16131.13


------------------------------------------------------------
  生成 CSV: Llama3.2-1B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_6/Llama3.2-1B-INT8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
16,16,1,128,128,63.0809,1072.3754,2.0291
128,128,1,128,128,64.4356,8312.1946,1.9865
256,256,1,128,128,63.0122,16194.1390,2.0314

------------------------------------------------------------

[INFO] 完成: 3 成功, 0 失败

============================================================
  Llama3.2-1B-INT8 | cuSPARSELt (2_6) | decode
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_6

============================================================
[1/3] 测试 M=16
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16
│   M_prefill     = 256 (= 16 x 16)
│   M_decode      = 16
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 16
│   --max-num-seqs           = 16
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
INFO 01-25 06:03:23 [datasets.py:612] Sampling input_len from [15, 15] and output_len from [256, 256]
INFO 01-25 06:03:23 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6', 'max_model_len': 272, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 272, 'max_num_seqs': 16, 'disable_log_stats': True, 'enable_lora': None, 'enable_chunked_prefill': False, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6'}
INFO 01-25 06:03:23 [model.py:514] Resolved architecture: LlamaForCausalLM
INFO 01-25 06:03:23 [model.py:1661] Using max model len 272
WARNING 01-25 06:03:23 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1082742) INFO 01-25 06:03:27 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6', speculative_config=None, tokenizer='/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=272, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6, enable_prefix_caching=True, enable_chunked_prefill=False, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [272], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 32, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=1082742) INFO 01-25 06:03:27 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:58547 backend=nccl
(EngineCore_DP0 pid=1082742) INFO 01-25 06:03:27 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=1082742) INFO 01-25 06:03:27 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6...
(EngineCore_DP0 pid=1082742) INFO 01-25 06:03:28 [compressed_tensors_w8a8_int8.py:62] Using CutlassScaledMMLinearKernel for CompressedTensorsW8A8Int8
(EngineCore_DP0 pid=1082742) INFO 01-25 06:03:28 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=1082742) INFO 01-25 06:03:28 [default_loader.py:308] Loading weights took 0.19 seconds
(EngineCore_DP0 pid=1082742) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1082742) INFO 01-25 06:03:28 [gpu_model_runner.py:3659] Model loading took 1.4249 GiB memory and 0.463224 seconds
(EngineCore_DP0 pid=1082742) WARNING 01-25 06:03:30 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=1082742) INFO 01-25 06:03:30 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/18a72c088c/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=1082742) INFO 01-25 06:03:30 [backends.py:703] Dynamo bytecode transform time: 2.05 s
(EngineCore_DP0 pid=1082742) INFO 01-25 06:03:33 [backends.py:261] Cache the graph of compile range (1, 272) for later use
(EngineCore_DP0 pid=1082742) INFO 01-25 06:03:34 [backends.py:278] Compiling a graph for compile range (1, 272) takes 2.01 s
(EngineCore_DP0 pid=1082742) INFO 01-25 06:03:34 [monitor.py:34] torch.compile takes 4.06 s in total
(EngineCore_DP0 pid=1082742) INFO 01-25 06:03:34 [gpu_worker.py:375] Available KV cache memory: 10.37 GiB
(EngineCore_DP0 pid=1082742) INFO 01-25 06:03:35 [kv_cache_utils.py:1291] GPU KV cache size: 339,792 tokens
(EngineCore_DP0 pid=1082742) INFO 01-25 06:03:35 [kv_cache_utils.py:1296] Maximum concurrency for 272 tokens per request: 1249.24x
(EngineCore_DP0 pid=1082742) INFO 01-25 06:03:35 [gpu_model_runner.py:4587] Graph capturing finished in 1 secs, took 0.03 GiB
(EngineCore_DP0 pid=1082742) INFO 01-25 06:03:35 [core.py:259] init engine (profile, create kv cache, warmup model) took 7.12 seconds
INFO 01-25 06:03:36 [llm.py:360] Supported tasks: ['generate']
Throughput: 18.93 requests/s, 5149.13 total tokens/s, 4846.24 output tokens/s
Total num prompt tokens:  256
Total num output tokens:  4096


─── STDERR ───
[2026-01-25 06:03:23] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:03:23] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-INT8'
[2026-01-25 06:03:23] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 06:03:23] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:03:23] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:03:23] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:03:23] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:03:23] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:03:23] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-INT8'
[2026-01-25 06:03:23] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 06:03:23] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:03:23] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:03:23] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:03:23] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 06:03:27] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:03:27] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-INT8'
[2026-01-25 06:03:27] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 06:03:27] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:03:27] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:03:27] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:03:27] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:03:27] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:03:27] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-INT8'
[2026-01-25 06:03:27] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 06:03:27] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:03:27] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:03:27] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:03:27] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1082742) [2026-01-25 06:03:28] INFO SlideSparseLinearMethod_INT8.py:801: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1082742) [2026-01-25 06:03:28] INFO gemm_wrapper.py:872: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1082742) [2026-01-25 06:03:28] INFO SlideSparseLinearMethod_INT8.py:448: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1082742) [2026-01-25 06:03:28] INFO SlideSparseLinearMethod_INT8.py:600: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=1082742) [2026-01-25 06:03:28] INFO kernels.py:132: Loaded tuned kernel for base model: Llama3.2-1B-INT8 (from Llama3.2-1B-INT8-SlideSparse-2_6)
(EngineCore_DP0 pid=1082742) [2026-01-25 06:03:28] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8-SlideSparse-2_6
(EngineCore_DP0 pid=1082742) [2026-01-25 06:03:28] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8-SlideSparse-2_6
(EngineCore_DP0 pid=1082742) [2026-01-25 06:03:28] INFO SlideSparseLinearMethod_INT8.py:617: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8-SlideSparse-2_6
(EngineCore_DP0 pid=1082742) [2026-01-25 06:03:28] INFO SlideSparseLinearMethod_INT8.py:621: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=1082742) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1082742) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.10it/s]
(EngineCore_DP0 pid=1082742) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.09it/s]
(EngineCore_DP0 pid=1082742) 
(EngineCore_DP0 pid=1082742) [2026-01-25 06:03:28] INFO SlideSparseLinearMethod_INT8.py:715: cuSPARSELt INT8 compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=1082742) [2026-01-25 06:03:28] INFO SlideSparseLinearMethod_INT8.py:725: cuSPARSELt INT8 compression done: 6389760 bytes
(EngineCore_DP0 pid=1082742) [2026-01-25 06:03:28] INFO SlideSparseLinearMethod_INT8.py:715: cuSPARSELt INT8 compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=1082742) [2026-01-25 06:03:28] INFO SlideSparseLinearMethod_INT8.py:725: cuSPARSELt INT8 compression done: 4259840 bytes
(EngineCore_DP0 pid=1082742) [2026-01-25 06:03:28] INFO SlideSparseLinearMethod_INT8.py:715: cuSPARSELt INT8 compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=1082742) [2026-01-25 06:03:28] INFO SlideSparseLinearMethod_INT8.py:725: cuSPARSELt INT8 compression done: 34078720 bytes
(EngineCore_DP0 pid=1082742) [2026-01-25 06:03:28] INFO SlideSparseLinearMethod_INT8.py:715: cuSPARSELt INT8 compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=1082742) [2026-01-25 06:03:28] INFO SlideSparseLinearMethod_INT8.py:725: cuSPARSELt INT8 compression done: 16842752 bytes
(EngineCore_DP0 pid=1082742) 2026-01-25 06:03:35,036 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1082742) 2026-01-25 06:03:35,049 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1082742) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00, 34.03it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 34.02it/s]
(EngineCore_DP0 pid=1082742) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 1/5 [00:00<00:00,  6.85it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 20.70it/s]

Adding requests:   0%|          | 0/16 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 16/16 [00:00<00:00, 1455.91it/s]

Processed prompts:   0%|          | 0/16 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   6%|▋         | 1/16 [00:00<00:12,  1.21it/s, est. speed input: 19.30 toks/s, output: 308.81 toks/s]
Processed prompts: 100%|██████████| 16/16 [00:00<00:00,  1.21it/s, est. speed input: 307.55 toks/s, output: 4920.81 toks/s]
Processed prompts: 100%|██████████| 16/16 [00:00<00:00, 19.22it/s, est. speed input: 307.55 toks/s, output: 4920.81 toks/s]
[rank0]:[W125 06:03:37.183729628 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 18.8s

测试结果:
  Requests/s:   18.93
  Tokens/s:     5149.13
  Total Reqs:   16
  Elapsed:      0.85s

  [Decode 分析]
  Total Decode Tokens:  4096
  Decode Tokens/s:      4846.24

============================================================
[2/3] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 2048 (= 128 x 16)
│   M_decode      = 128
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 128
│   --max-num-seqs           = 128
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
INFO 01-25 06:03:42 [datasets.py:612] Sampling input_len from [15, 15] and output_len from [256, 256]
INFO 01-25 06:03:42 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6', 'max_model_len': 272, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 272, 'max_num_seqs': 128, 'disable_log_stats': True, 'enable_lora': None, 'enable_chunked_prefill': False, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6'}
INFO 01-25 06:03:42 [model.py:514] Resolved architecture: LlamaForCausalLM
INFO 01-25 06:03:42 [model.py:1661] Using max model len 272
WARNING 01-25 06:03:42 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1083161) INFO 01-25 06:03:46 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6', speculative_config=None, tokenizer='/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=272, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6, enable_prefix_caching=True, enable_chunked_prefill=False, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [272], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 256, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=1083161) INFO 01-25 06:03:46 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:38417 backend=nccl
(EngineCore_DP0 pid=1083161) INFO 01-25 06:03:46 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=1083161) INFO 01-25 06:03:46 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6...
(EngineCore_DP0 pid=1083161) INFO 01-25 06:03:46 [compressed_tensors_w8a8_int8.py:62] Using CutlassScaledMMLinearKernel for CompressedTensorsW8A8Int8
(EngineCore_DP0 pid=1083161) INFO 01-25 06:03:47 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=1083161) INFO 01-25 06:03:47 [default_loader.py:308] Loading weights took 0.19 seconds
(EngineCore_DP0 pid=1083161) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1083161) INFO 01-25 06:03:47 [gpu_model_runner.py:3659] Model loading took 1.4249 GiB memory and 0.470130 seconds
(EngineCore_DP0 pid=1083161) WARNING 01-25 06:03:49 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=1083161) INFO 01-25 06:03:49 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/7813840a88/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=1083161) INFO 01-25 06:03:49 [backends.py:703] Dynamo bytecode transform time: 2.08 s
(EngineCore_DP0 pid=1083161) INFO 01-25 06:03:51 [backends.py:261] Cache the graph of compile range (1, 272) for later use
(EngineCore_DP0 pid=1083161) INFO 01-25 06:03:52 [backends.py:278] Compiling a graph for compile range (1, 272) takes 0.91 s
(EngineCore_DP0 pid=1083161) INFO 01-25 06:03:52 [monitor.py:34] torch.compile takes 2.99 s in total
(EngineCore_DP0 pid=1083161) INFO 01-25 06:03:52 [gpu_worker.py:375] Available KV cache memory: 10.21 GiB
(EngineCore_DP0 pid=1083161) INFO 01-25 06:03:52 [kv_cache_utils.py:1291] GPU KV cache size: 334,496 tokens
(EngineCore_DP0 pid=1083161) INFO 01-25 06:03:52 [kv_cache_utils.py:1296] Maximum concurrency for 272 tokens per request: 1229.76x
(EngineCore_DP0 pid=1083161) INFO 01-25 06:03:54 [gpu_model_runner.py:4587] Graph capturing finished in 2 secs, took 0.16 GiB
(EngineCore_DP0 pid=1083161) INFO 01-25 06:03:54 [core.py:259] init engine (profile, create kv cache, warmup model) took 7.28 seconds
INFO 01-25 06:03:55 [llm.py:360] Supported tasks: ['generate']
Throughput: 80.02 requests/s, 21766.29 total tokens/s, 20485.92 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768


─── STDERR ───
[2026-01-25 06:03:42] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:03:42] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-INT8'
[2026-01-25 06:03:42] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 06:03:42] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:03:42] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:03:42] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:03:42] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:03:42] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:03:42] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-INT8'
[2026-01-25 06:03:42] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 06:03:42] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:03:42] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:03:42] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:03:42] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 06:03:46] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:03:46] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-INT8'
[2026-01-25 06:03:46] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 06:03:46] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:03:46] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:03:46] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:03:46] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:03:46] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:03:46] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-INT8'
[2026-01-25 06:03:46] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 06:03:46] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:03:46] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:03:46] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:03:46] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1083161) [2026-01-25 06:03:46] INFO SlideSparseLinearMethod_INT8.py:801: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1083161) [2026-01-25 06:03:46] INFO gemm_wrapper.py:872: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1083161) [2026-01-25 06:03:46] INFO SlideSparseLinearMethod_INT8.py:448: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1083161) [2026-01-25 06:03:46] INFO SlideSparseLinearMethod_INT8.py:600: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=1083161) [2026-01-25 06:03:46] INFO kernels.py:132: Loaded tuned kernel for base model: Llama3.2-1B-INT8 (from Llama3.2-1B-INT8-SlideSparse-2_6)
(EngineCore_DP0 pid=1083161) [2026-01-25 06:03:46] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8-SlideSparse-2_6
(EngineCore_DP0 pid=1083161) [2026-01-25 06:03:46] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8-SlideSparse-2_6
(EngineCore_DP0 pid=1083161) [2026-01-25 06:03:46] INFO SlideSparseLinearMethod_INT8.py:617: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8-SlideSparse-2_6
(EngineCore_DP0 pid=1083161) [2026-01-25 06:03:46] INFO SlideSparseLinearMethod_INT8.py:621: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=1083161) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1083161) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.10it/s]
(EngineCore_DP0 pid=1083161) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.10it/s]
(EngineCore_DP0 pid=1083161) 
(EngineCore_DP0 pid=1083161) [2026-01-25 06:03:47] INFO SlideSparseLinearMethod_INT8.py:715: cuSPARSELt INT8 compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=1083161) [2026-01-25 06:03:47] INFO SlideSparseLinearMethod_INT8.py:725: cuSPARSELt INT8 compression done: 6389760 bytes
(EngineCore_DP0 pid=1083161) [2026-01-25 06:03:47] INFO SlideSparseLinearMethod_INT8.py:715: cuSPARSELt INT8 compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=1083161) [2026-01-25 06:03:47] INFO SlideSparseLinearMethod_INT8.py:725: cuSPARSELt INT8 compression done: 4259840 bytes
(EngineCore_DP0 pid=1083161) [2026-01-25 06:03:47] INFO SlideSparseLinearMethod_INT8.py:715: cuSPARSELt INT8 compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=1083161) [2026-01-25 06:03:47] INFO SlideSparseLinearMethod_INT8.py:725: cuSPARSELt INT8 compression done: 34078720 bytes
(EngineCore_DP0 pid=1083161) [2026-01-25 06:03:47] INFO SlideSparseLinearMethod_INT8.py:715: cuSPARSELt INT8 compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=1083161) [2026-01-25 06:03:47] INFO SlideSparseLinearMethod_INT8.py:725: cuSPARSELt INT8 compression done: 16842752 bytes
(EngineCore_DP0 pid=1083161) 2026-01-25 06:03:52,833 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1083161) 2026-01-25 06:03:52,847 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1083161) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█▏        | 4/35 [00:00<00:00, 34.37it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  23%|██▎       | 8/35 [00:00<00:00, 35.66it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  34%|███▍      | 12/35 [00:00<00:00, 35.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  46%|████▌     | 16/35 [00:00<00:00, 35.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 20/35 [00:00<00:00, 34.44it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 24/35 [00:00<00:00, 34.92it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 28/35 [00:00<00:00, 35.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████▏| 32/35 [00:00<00:00, 35.63it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:01<00:00, 34.96it/s]
(EngineCore_DP0 pid=1083161) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   5%|▌         | 1/19 [00:00<00:03,  5.84it/s]
Capturing CUDA graphs (decode, FULL):  21%|██        | 4/19 [00:00<00:00, 15.12it/s]
Capturing CUDA graphs (decode, FULL):  32%|███▏      | 6/19 [00:00<00:00, 14.54it/s]
Capturing CUDA graphs (decode, FULL):  53%|█████▎    | 10/19 [00:00<00:00, 21.35it/s]
Capturing CUDA graphs (decode, FULL):  79%|███████▉  | 15/19 [00:00<00:00, 27.93it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:00<00:00, 24.36it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 4583.78it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:01<03:13,  1.52s/it, est. speed input: 10.52 toks/s, output: 168.31 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00,  1.52s/it, est. speed input: 1304.76 toks/s, output: 20876.05 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 81.54it/s, est. speed input: 1304.76 toks/s, output: 20876.05 toks/s]
[rank0]:[W125 06:03:57.006534440 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 19.7s

测试结果:
  Requests/s:   80.02
  Tokens/s:     21766.29
  Total Reqs:   128
  Elapsed:      1.60s

  [Decode 分析]
  Total Decode Tokens:  32768
  Decode Tokens/s:      20485.92

============================================================
[3/3] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 4096 (= 256 x 16)
│   M_decode      = 256
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 256
│   --max-num-seqs           = 256
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
INFO 01-25 06:04:02 [datasets.py:612] Sampling input_len from [15, 15] and output_len from [256, 256]
INFO 01-25 06:04:02 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6', 'max_model_len': 272, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 272, 'max_num_seqs': 256, 'disable_log_stats': True, 'enable_lora': None, 'enable_chunked_prefill': False, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6'}
INFO 01-25 06:04:02 [model.py:514] Resolved architecture: LlamaForCausalLM
INFO 01-25 06:04:02 [model.py:1661] Using max model len 272
WARNING 01-25 06:04:02 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1083519) INFO 01-25 06:04:06 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6', speculative_config=None, tokenizer='/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=272, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6, enable_prefix_caching=True, enable_chunked_prefill=False, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [272], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 272, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=1083519) INFO 01-25 06:04:06 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:40955 backend=nccl
(EngineCore_DP0 pid=1083519) INFO 01-25 06:04:06 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=1083519) INFO 01-25 06:04:06 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6...
(EngineCore_DP0 pid=1083519) INFO 01-25 06:04:07 [compressed_tensors_w8a8_int8.py:62] Using CutlassScaledMMLinearKernel for CompressedTensorsW8A8Int8
(EngineCore_DP0 pid=1083519) INFO 01-25 06:04:07 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=1083519) INFO 01-25 06:04:07 [default_loader.py:308] Loading weights took 0.18 seconds
(EngineCore_DP0 pid=1083519) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1083519) INFO 01-25 06:04:07 [gpu_model_runner.py:3659] Model loading took 1.4249 GiB memory and 0.466293 seconds
(EngineCore_DP0 pid=1083519) WARNING 01-25 06:04:10 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=1083519) INFO 01-25 06:04:10 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/ad8a7fd40b/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=1083519) INFO 01-25 06:04:10 [backends.py:703] Dynamo bytecode transform time: 2.07 s
(EngineCore_DP0 pid=1083519) INFO 01-25 06:04:11 [backends.py:261] Cache the graph of compile range (1, 272) for later use
(EngineCore_DP0 pid=1083519) INFO 01-25 06:04:12 [backends.py:278] Compiling a graph for compile range (1, 272) takes 0.84 s
(EngineCore_DP0 pid=1083519) INFO 01-25 06:04:12 [monitor.py:34] torch.compile takes 2.90 s in total
(EngineCore_DP0 pid=1083519) INFO 01-25 06:04:12 [gpu_worker.py:375] Available KV cache memory: 9.63 GiB
(EngineCore_DP0 pid=1083519) INFO 01-25 06:04:12 [kv_cache_utils.py:1291] GPU KV cache size: 315,440 tokens
(EngineCore_DP0 pid=1083519) INFO 01-25 06:04:12 [kv_cache_utils.py:1296] Maximum concurrency for 272 tokens per request: 1159.71x
(EngineCore_DP0 pid=1083519) INFO 01-25 06:04:15 [gpu_model_runner.py:4587] Graph capturing finished in 2 secs, took 0.21 GiB
(EngineCore_DP0 pid=1083519) INFO 01-25 06:04:15 [core.py:259] init engine (profile, create kv cache, warmup model) took 7.50 seconds
INFO 01-25 06:04:15 [llm.py:360] Supported tasks: ['generate']
Throughput: 87.80 requests/s, 23880.68 total tokens/s, 22475.94 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536


─── STDERR ───
[2026-01-25 06:04:02] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:04:02] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-INT8'
[2026-01-25 06:04:02] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 06:04:02] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:04:02] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:04:02] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:04:02] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:04:02] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:04:02] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-INT8'
[2026-01-25 06:04:02] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 06:04:02] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:04:02] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:04:02] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:04:02] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 06:04:06] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:04:06] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-INT8'
[2026-01-25 06:04:06] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 06:04:06] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:04:06] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:04:06] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:04:06] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:04:06] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:04:06] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-INT8'
[2026-01-25 06:04:06] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 06:04:06] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:04:06] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:04:06] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:04:06] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1083519) [2026-01-25 06:04:07] INFO SlideSparseLinearMethod_INT8.py:801: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1083519) [2026-01-25 06:04:07] INFO gemm_wrapper.py:872: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1083519) [2026-01-25 06:04:07] INFO SlideSparseLinearMethod_INT8.py:448: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1083519) [2026-01-25 06:04:07] INFO SlideSparseLinearMethod_INT8.py:600: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=1083519) [2026-01-25 06:04:07] INFO kernels.py:132: Loaded tuned kernel for base model: Llama3.2-1B-INT8 (from Llama3.2-1B-INT8-SlideSparse-2_6)
(EngineCore_DP0 pid=1083519) [2026-01-25 06:04:07] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8-SlideSparse-2_6
(EngineCore_DP0 pid=1083519) [2026-01-25 06:04:07] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8-SlideSparse-2_6
(EngineCore_DP0 pid=1083519) [2026-01-25 06:04:07] INFO SlideSparseLinearMethod_INT8.py:617: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8-SlideSparse-2_6
(EngineCore_DP0 pid=1083519) [2026-01-25 06:04:07] INFO SlideSparseLinearMethod_INT8.py:621: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=1083519) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1083519) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.14it/s]
(EngineCore_DP0 pid=1083519) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.14it/s]
(EngineCore_DP0 pid=1083519) 
(EngineCore_DP0 pid=1083519) [2026-01-25 06:04:07] INFO SlideSparseLinearMethod_INT8.py:715: cuSPARSELt INT8 compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=1083519) [2026-01-25 06:04:07] INFO SlideSparseLinearMethod_INT8.py:725: cuSPARSELt INT8 compression done: 6389760 bytes
(EngineCore_DP0 pid=1083519) [2026-01-25 06:04:07] INFO SlideSparseLinearMethod_INT8.py:715: cuSPARSELt INT8 compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=1083519) [2026-01-25 06:04:07] INFO SlideSparseLinearMethod_INT8.py:725: cuSPARSELt INT8 compression done: 4259840 bytes
(EngineCore_DP0 pid=1083519) [2026-01-25 06:04:07] INFO SlideSparseLinearMethod_INT8.py:715: cuSPARSELt INT8 compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=1083519) [2026-01-25 06:04:07] INFO SlideSparseLinearMethod_INT8.py:725: cuSPARSELt INT8 compression done: 34078720 bytes
(EngineCore_DP0 pid=1083519) [2026-01-25 06:04:07] INFO SlideSparseLinearMethod_INT8.py:715: cuSPARSELt INT8 compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=1083519) [2026-01-25 06:04:07] INFO SlideSparseLinearMethod_INT8.py:725: cuSPARSELt INT8 compression done: 16842752 bytes
(EngineCore_DP0 pid=1083519) 2026-01-25 06:04:12,881 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1083519) 2026-01-25 06:04:12,895 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1083519) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/36 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 4/36 [00:00<00:00, 34.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 8/36 [00:00<00:00, 35.94it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 12/36 [00:00<00:00, 35.97it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  44%|████▍     | 16/36 [00:00<00:00, 36.01it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  56%|█████▌    | 20/36 [00:00<00:00, 34.39it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 24/36 [00:00<00:00, 35.13it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 28/36 [00:00<00:00, 35.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 32/36 [00:00<00:00, 35.82it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:01<00:00, 35.27it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:01<00:00, 35.37it/s]
(EngineCore_DP0 pid=1083519) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   3%|▎         | 1/35 [00:00<00:04,  6.85it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 5/35 [00:00<00:01, 22.94it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 10/35 [00:00<00:00, 31.05it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 15/35 [00:00<00:00, 34.87it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 20/35 [00:00<00:00, 36.92it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 25/35 [00:00<00:00, 38.14it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 30/35 [00:00<00:00, 39.18it/s]
Capturing CUDA graphs (decode, FULL):  97%|█████████▋| 34/35 [00:00<00:00, 36.85it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:01<00:00, 34.53it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 9292.04it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:02<11:11,  2.63s/it, est. speed input: 6.07 toks/s, output: 97.19 toks/s]
Processed prompts:  54%|█████▎    | 137/256 [00:02<00:01, 70.22it/s, est. speed input: 800.89 toks/s, output: 12814.25 toks/s]
Processed prompts:  94%|█████████▍| 240/256 [00:02<00:00, 135.59it/s, est. speed input: 1352.31 toks/s, output: 21637.00 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 135.59it/s, est. speed input: 1418.69 toks/s, output: 22699.01 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 88.67it/s, est. speed input: 1418.69 toks/s, output: 22699.01 toks/s] 
[rank0]:[W125 06:04:19.808541102 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 21.7s

测试结果:
  Requests/s:   87.80
  Tokens/s:     23880.68
  Total Reqs:   256
  Elapsed:      2.92s

  [Decode 分析]
  Total Decode Tokens:  65536
  Decode Tokens/s:      22475.94


------------------------------------------------------------
  生成 CSV: Llama3.2-1B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_6/Llama3.2-1B-INT8_decode.csv

预览:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
16,16,16,16,256,256,18.9306,5149.1271,0.8452
128,16,128,128,256,256,80.0231,21766.2857,1.5995
256,16,256,256,256,256,87.7966,23880.6835,2.9158

------------------------------------------------------------

[INFO] 完成: 3 成功, 0 失败

======================================================================
[SlideSparse] 未找到 Llama3.2-1B-INT8-SlideSparse-2_10 checkpoint
[SlideSparse] 发现基础模型: Llama3.2-1B-INT8
[SlideSparse] 自动转换中...
[SlideSparse] 开始转换: Llama3.2-1B-INT8 -> SlideSparse-2_10
======================================================================

[SlideSparse] 转换成功: Llama3.2-1B-INT8-SlideSparse-2_10
======================================================================


============================================================
  Llama3.2-1B-INT8 | cuSPARSELt (2_10) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_10

============================================================
[1/3] 测试 M=16
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16
│   M_prefill     = 16 (= 1 x 16)
│   M_decode      = 1
│   batched_tokens = 17 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 17
│   --max-num-batched-tokens = 17
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
INFO 01-25 06:04:51 [datasets.py:612] Sampling input_len from [15, 15] and output_len from [1, 1]
INFO 01-25 06:04:51 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10', 'max_model_len': 17, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 17, 'max_num_seqs': 1, 'disable_log_stats': True, 'enable_lora': None, 'enable_chunked_prefill': False, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10'}
INFO 01-25 06:04:51 [model.py:514] Resolved architecture: LlamaForCausalLM
INFO 01-25 06:04:51 [model.py:1661] Using max model len 17
WARNING 01-25 06:04:51 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1084120) INFO 01-25 06:04:55 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10', speculative_config=None, tokenizer='/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=17, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10, enable_prefix_caching=True, enable_chunked_prefill=False, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [17], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 2, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=1084120) INFO 01-25 06:04:55 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:51099 backend=nccl
(EngineCore_DP0 pid=1084120) INFO 01-25 06:04:55 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=1084120) INFO 01-25 06:04:55 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10...
(EngineCore_DP0 pid=1084120) INFO 01-25 06:04:56 [compressed_tensors_w8a8_int8.py:62] Using CutlassScaledMMLinearKernel for CompressedTensorsW8A8Int8
(EngineCore_DP0 pid=1084120) INFO 01-25 06:04:56 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=1084120) INFO 01-25 06:04:56 [default_loader.py:308] Loading weights took 0.20 seconds
(EngineCore_DP0 pid=1084120) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1084120) INFO 01-25 06:04:56 [gpu_model_runner.py:3659] Model loading took 1.6137 GiB memory and 0.495908 seconds
(EngineCore_DP0 pid=1084120) WARNING 01-25 06:04:59 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=1084120) INFO 01-25 06:04:59 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/bf51733cb9/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=1084120) INFO 01-25 06:04:59 [backends.py:703] Dynamo bytecode transform time: 2.07 s
(EngineCore_DP0 pid=1084120) INFO 01-25 06:05:01 [backends.py:261] Cache the graph of compile range (1, 17) for later use
(EngineCore_DP0 pid=1084120) INFO 01-25 06:05:02 [backends.py:278] Compiling a graph for compile range (1, 17) takes 2.20 s
(EngineCore_DP0 pid=1084120) INFO 01-25 06:05:02 [monitor.py:34] torch.compile takes 4.27 s in total
(EngineCore_DP0 pid=1084120) INFO 01-25 06:05:03 [gpu_worker.py:375] Available KV cache memory: 10.18 GiB
(EngineCore_DP0 pid=1084120) INFO 01-25 06:05:03 [kv_cache_utils.py:1291] GPU KV cache size: 333,696 tokens
(EngineCore_DP0 pid=1084120) INFO 01-25 06:05:03 [kv_cache_utils.py:1296] Maximum concurrency for 17 tokens per request: 10428.00x
(EngineCore_DP0 pid=1084120) INFO 01-25 06:05:04 [gpu_model_runner.py:4587] Graph capturing finished in 1 secs, took 0.01 GiB
(EngineCore_DP0 pid=1084120) INFO 01-25 06:05:04 [core.py:259] init engine (profile, create kv cache, warmup model) took 7.53 seconds
INFO 01-25 06:05:04 [llm.py:360] Supported tasks: ['generate']
Throughput: 62.89 requests/s, 1069.14 total tokens/s, 62.89 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  128


─── STDERR ───
[2026-01-25 06:04:51] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:04:51] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-INT8'
[2026-01-25 06:04:51] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 06:04:51] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:04:51] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:04:51] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:04:51] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:04:51] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:04:51] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-INT8'
[2026-01-25 06:04:51] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 06:04:51] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:04:51] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:04:51] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:04:51] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 06:04:55] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:04:55] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-INT8'
[2026-01-25 06:04:55] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 06:04:55] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:04:55] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:04:55] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:04:55] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:04:55] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:04:55] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-INT8'
[2026-01-25 06:04:55] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 06:04:55] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:04:55] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:04:55] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:04:55] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1084120) [2026-01-25 06:04:56] INFO SlideSparseLinearMethod_INT8.py:801: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1084120) [2026-01-25 06:04:56] INFO gemm_wrapper.py:872: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1084120) [2026-01-25 06:04:56] INFO SlideSparseLinearMethod_INT8.py:448: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1084120) [2026-01-25 06:04:56] INFO SlideSparseLinearMethod_INT8.py:600: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=1084120) [2026-01-25 06:04:56] INFO kernels.py:132: Loaded tuned kernel for base model: Llama3.2-1B-INT8 (from Llama3.2-1B-INT8-SlideSparse-2_10)
(EngineCore_DP0 pid=1084120) [2026-01-25 06:04:56] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8-SlideSparse-2_10
(EngineCore_DP0 pid=1084120) [2026-01-25 06:04:56] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8-SlideSparse-2_10
(EngineCore_DP0 pid=1084120) [2026-01-25 06:04:56] INFO SlideSparseLinearMethod_INT8.py:617: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8-SlideSparse-2_10
(EngineCore_DP0 pid=1084120) [2026-01-25 06:04:56] INFO SlideSparseLinearMethod_INT8.py:621: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=1084120) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1084120) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.49it/s]
(EngineCore_DP0 pid=1084120) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.49it/s]
(EngineCore_DP0 pid=1084120) 
(EngineCore_DP0 pid=1084120) [2026-01-25 06:04:56] INFO SlideSparseLinearMethod_INT8.py:715: cuSPARSELt INT8 compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=1084120) [2026-01-25 06:04:56] INFO SlideSparseLinearMethod_INT8.py:725: cuSPARSELt INT8 compression done: 7618560 bytes
(EngineCore_DP0 pid=1084120) [2026-01-25 06:04:56] INFO SlideSparseLinearMethod_INT8.py:715: cuSPARSELt INT8 compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=1084120) [2026-01-25 06:04:56] INFO SlideSparseLinearMethod_INT8.py:725: cuSPARSELt INT8 compression done: 5079040 bytes
(EngineCore_DP0 pid=1084120) [2026-01-25 06:04:56] INFO SlideSparseLinearMethod_INT8.py:715: cuSPARSELt INT8 compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=1084120) [2026-01-25 06:04:56] INFO SlideSparseLinearMethod_INT8.py:725: cuSPARSELt INT8 compression done: 40632320 bytes
(EngineCore_DP0 pid=1084120) [2026-01-25 06:04:56] INFO SlideSparseLinearMethod_INT8.py:715: cuSPARSELt INT8 compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=1084120) [2026-01-25 06:04:56] INFO SlideSparseLinearMethod_INT8.py:725: cuSPARSELt INT8 compression done: 20185088 bytes
(EngineCore_DP0 pid=1084120) 2026-01-25 06:05:03,469 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1084120) 2026-01-25 06:05:03,483 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1084120) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  8.29it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  5.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  5.91it/s]
(EngineCore_DP0 pid=1084120) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.87it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.87it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:   5%|▌         | 7/128 [00:00<00:02, 46.90it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 788.07it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:13,  9.20it/s, est. speed input: 147.29 toks/s, output: 9.20 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:02, 46.14it/s, est. speed input: 651.21 toks/s, output: 40.70 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:01, 58.44it/s, est. speed input: 823.63 toks/s, output: 51.48 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:00<00:01, 64.18it/s, est. speed input: 909.49 toks/s, output: 56.84 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:00<00:01, 67.32it/s, est. speed input: 960.81 toks/s, output: 60.05 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:00<00:01, 68.29it/s, est. speed input: 988.47 toks/s, output: 61.78 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:00<00:01, 68.34it/s, est. speed input: 1002.70 toks/s, output: 62.67 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:00<00:01, 69.31it/s, est. speed input: 1020.39 toks/s, output: 63.77 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:00<00:00, 70.22it/s, est. speed input: 1035.48 toks/s, output: 64.72 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:01<00:00, 70.92it/s, est. speed input: 1047.98 toks/s, output: 65.50 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:01<00:00, 71.46it/s, est. speed input: 1058.40 toks/s, output: 66.15 toks/s]
Processed prompts:  69%|██████▉   | 88/128 [00:01<00:00, 71.72it/s, est. speed input: 1066.70 toks/s, output: 66.67 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:01<00:00, 71.16it/s, est. speed input: 1070.82 toks/s, output: 66.93 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:01<00:00, 71.69it/s, est. speed input: 1077.66 toks/s, output: 67.35 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:01<00:00, 72.05it/s, est. speed input: 1083.53 toks/s, output: 67.72 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:01<00:00, 72.60it/s, est. speed input: 1089.61 toks/s, output: 68.10 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 72.86it/s, est. speed input: 1094.60 toks/s, output: 68.41 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 72.86it/s, est. speed input: 1094.60 toks/s, output: 68.41 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 68.41it/s, est. speed input: 1094.60 toks/s, output: 68.41 toks/s]
[rank0]:[W125 06:05:07.858153714 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 20.2s

测试结果:
  Requests/s:   62.89
  Tokens/s:     1069.14
  Total Reqs:   128
  Elapsed:      2.04s

  [Prefill 分析]
  Total Prefill Tokens: 2048
  Prefill Tokens/s:     1006.25

============================================================
[2/3] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 128 (= 1 x 128)
│   M_decode      = 1
│   batched_tokens = 129 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 128
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 129
│   --max-num-batched-tokens = 129
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
INFO 01-25 06:05:11 [datasets.py:612] Sampling input_len from [127, 127] and output_len from [1, 1]
INFO 01-25 06:05:11 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10', 'max_model_len': 129, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 129, 'max_num_seqs': 1, 'disable_log_stats': True, 'enable_lora': None, 'enable_chunked_prefill': False, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10'}
INFO 01-25 06:05:11 [model.py:514] Resolved architecture: LlamaForCausalLM
INFO 01-25 06:05:11 [model.py:1661] Using max model len 129
WARNING 01-25 06:05:12 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1084522) INFO 01-25 06:05:16 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10', speculative_config=None, tokenizer='/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=129, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10, enable_prefix_caching=True, enable_chunked_prefill=False, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [129], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 2, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=1084522) INFO 01-25 06:05:16 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:54677 backend=nccl
(EngineCore_DP0 pid=1084522) INFO 01-25 06:05:16 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=1084522) INFO 01-25 06:05:16 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10...
(EngineCore_DP0 pid=1084522) INFO 01-25 06:05:16 [compressed_tensors_w8a8_int8.py:62] Using CutlassScaledMMLinearKernel for CompressedTensorsW8A8Int8
(EngineCore_DP0 pid=1084522) INFO 01-25 06:05:16 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=1084522) INFO 01-25 06:05:16 [default_loader.py:308] Loading weights took 0.20 seconds
(EngineCore_DP0 pid=1084522) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1084522) INFO 01-25 06:05:17 [gpu_model_runner.py:3659] Model loading took 1.6137 GiB memory and 0.495880 seconds
(EngineCore_DP0 pid=1084522) WARNING 01-25 06:05:19 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=1084522) INFO 01-25 06:05:19 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/92d4d71ab8/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=1084522) INFO 01-25 06:05:19 [backends.py:703] Dynamo bytecode transform time: 2.05 s
(EngineCore_DP0 pid=1084522) INFO 01-25 06:05:21 [backends.py:261] Cache the graph of compile range (1, 129) for later use
(EngineCore_DP0 pid=1084522) INFO 01-25 06:05:23 [backends.py:278] Compiling a graph for compile range (1, 129) takes 2.17 s
(EngineCore_DP0 pid=1084522) INFO 01-25 06:05:23 [monitor.py:34] torch.compile takes 4.22 s in total
(EngineCore_DP0 pid=1084522) INFO 01-25 06:05:23 [gpu_worker.py:375] Available KV cache memory: 10.18 GiB
(EngineCore_DP0 pid=1084522) INFO 01-25 06:05:23 [kv_cache_utils.py:1291] GPU KV cache size: 333,664 tokens
(EngineCore_DP0 pid=1084522) INFO 01-25 06:05:23 [kv_cache_utils.py:1296] Maximum concurrency for 129 tokens per request: 2317.11x
(EngineCore_DP0 pid=1084522) INFO 01-25 06:05:24 [gpu_model_runner.py:4587] Graph capturing finished in 1 secs, took 0.01 GiB
(EngineCore_DP0 pid=1084522) INFO 01-25 06:05:24 [core.py:259] init engine (profile, create kv cache, warmup model) took 7.08 seconds
INFO 01-25 06:05:25 [llm.py:360] Supported tasks: ['generate']
Throughput: 65.68 requests/s, 8472.73 total tokens/s, 65.68 output tokens/s
Total num prompt tokens:  16384
Total num output tokens:  128


─── STDERR ───
[2026-01-25 06:05:12] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:05:12] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-INT8'
[2026-01-25 06:05:12] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 06:05:12] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:05:12] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:05:12] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:05:12] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:05:12] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:05:12] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-INT8'
[2026-01-25 06:05:12] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 06:05:12] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:05:12] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:05:12] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:05:12] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 06:05:16] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:05:16] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-INT8'
[2026-01-25 06:05:16] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 06:05:16] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:05:16] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:05:16] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:05:16] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:05:16] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:05:16] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-INT8'
[2026-01-25 06:05:16] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 06:05:16] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:05:16] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:05:16] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:05:16] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1084522) [2026-01-25 06:05:16] INFO SlideSparseLinearMethod_INT8.py:801: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1084522) [2026-01-25 06:05:16] INFO gemm_wrapper.py:872: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1084522) [2026-01-25 06:05:16] INFO SlideSparseLinearMethod_INT8.py:448: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1084522) [2026-01-25 06:05:16] INFO SlideSparseLinearMethod_INT8.py:600: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=1084522) [2026-01-25 06:05:16] INFO kernels.py:132: Loaded tuned kernel for base model: Llama3.2-1B-INT8 (from Llama3.2-1B-INT8-SlideSparse-2_10)
(EngineCore_DP0 pid=1084522) [2026-01-25 06:05:16] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8-SlideSparse-2_10
(EngineCore_DP0 pid=1084522) [2026-01-25 06:05:16] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8-SlideSparse-2_10
(EngineCore_DP0 pid=1084522) [2026-01-25 06:05:16] INFO SlideSparseLinearMethod_INT8.py:617: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8-SlideSparse-2_10
(EngineCore_DP0 pid=1084522) [2026-01-25 06:05:16] INFO SlideSparseLinearMethod_INT8.py:621: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=1084522) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1084522) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.53it/s]
(EngineCore_DP0 pid=1084522) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.52it/s]
(EngineCore_DP0 pid=1084522) 
(EngineCore_DP0 pid=1084522) [2026-01-25 06:05:16] INFO SlideSparseLinearMethod_INT8.py:715: cuSPARSELt INT8 compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=1084522) [2026-01-25 06:05:17] INFO SlideSparseLinearMethod_INT8.py:725: cuSPARSELt INT8 compression done: 7618560 bytes
(EngineCore_DP0 pid=1084522) [2026-01-25 06:05:17] INFO SlideSparseLinearMethod_INT8.py:715: cuSPARSELt INT8 compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=1084522) [2026-01-25 06:05:17] INFO SlideSparseLinearMethod_INT8.py:725: cuSPARSELt INT8 compression done: 5079040 bytes
(EngineCore_DP0 pid=1084522) [2026-01-25 06:05:17] INFO SlideSparseLinearMethod_INT8.py:715: cuSPARSELt INT8 compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=1084522) [2026-01-25 06:05:17] INFO SlideSparseLinearMethod_INT8.py:725: cuSPARSELt INT8 compression done: 40632320 bytes
(EngineCore_DP0 pid=1084522) [2026-01-25 06:05:17] INFO SlideSparseLinearMethod_INT8.py:715: cuSPARSELt INT8 compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=1084522) [2026-01-25 06:05:17] INFO SlideSparseLinearMethod_INT8.py:725: cuSPARSELt INT8 compression done: 20185088 bytes
(EngineCore_DP0 pid=1084522) 2026-01-25 06:05:23,785 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1084522) 2026-01-25 06:05:23,799 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1084522) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 21.63it/s]
(EngineCore_DP0 pid=1084522) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  5.90it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  5.90it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 2537.34it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:15,  8.07it/s, est. speed input: 1033.54 toks/s, output: 8.07 toks/s]
Processed prompts:   6%|▋         | 8/128 [00:00<00:02, 41.19it/s, est. speed input: 4569.29 toks/s, output: 35.70 toks/s]
Processed prompts:  12%|█▎        | 16/128 [00:00<00:02, 54.84it/s, est. speed input: 6070.17 toks/s, output: 47.42 toks/s]
Processed prompts:  19%|█▉        | 24/128 [00:00<00:01, 61.71it/s, est. speed input: 6857.08 toks/s, output: 53.57 toks/s]
Processed prompts:  25%|██▌       | 32/128 [00:00<00:01, 65.45it/s, est. speed input: 7327.45 toks/s, output: 57.25 toks/s]
Processed prompts:  31%|███▏      | 40/128 [00:00<00:01, 67.74it/s, est. speed input: 7644.31 toks/s, output: 59.72 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:00<00:01, 69.67it/s, est. speed input: 7894.06 toks/s, output: 61.67 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:00<00:01, 70.42it/s, est. speed input: 8059.41 toks/s, output: 62.96 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:00<00:00, 71.12it/s, est. speed input: 8196.21 toks/s, output: 64.03 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:01<00:00, 71.33it/s, est. speed input: 8295.75 toks/s, output: 64.81 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:01<00:00, 71.03it/s, est. speed input: 8361.95 toks/s, output: 65.33 toks/s]
Processed prompts:  69%|██████▉   | 88/128 [00:01<00:00, 70.86it/s, est. speed input: 8417.67 toks/s, output: 65.76 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:01<00:00, 71.30it/s, est. speed input: 8481.87 toks/s, output: 66.26 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:01<00:00, 71.55it/s, est. speed input: 8535.33 toks/s, output: 66.68 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:01<00:00, 71.74it/s, est. speed input: 8582.01 toks/s, output: 67.05 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:01<00:00, 71.49it/s, est. speed input: 8613.21 toks/s, output: 67.29 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 71.22it/s, est. speed input: 8638.55 toks/s, output: 67.49 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 71.22it/s, est. speed input: 8638.55 toks/s, output: 67.49 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 67.48it/s, est. speed input: 8638.55 toks/s, output: 67.49 toks/s]
[rank0]:[W125 06:05:27.837357925 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 20.1s

测试结果:
  Requests/s:   65.68
  Tokens/s:     8472.73
  Total Reqs:   128
  Elapsed:      1.95s

  [Prefill 分析]
  Total Prefill Tokens: 16384
  Prefill Tokens/s:     8407.05

============================================================
[3/3] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 256 (= 1 x 256)
│   M_decode      = 1
│   batched_tokens = 257 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 256
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 257
│   --max-num-batched-tokens = 257
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
INFO 01-25 06:05:31 [datasets.py:612] Sampling input_len from [255, 255] and output_len from [1, 1]
INFO 01-25 06:05:32 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10', 'max_model_len': 257, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 257, 'max_num_seqs': 1, 'disable_log_stats': True, 'enable_lora': None, 'enable_chunked_prefill': False, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10'}
INFO 01-25 06:05:32 [model.py:514] Resolved architecture: LlamaForCausalLM
INFO 01-25 06:05:32 [model.py:1661] Using max model len 257
WARNING 01-25 06:05:32 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1084893) INFO 01-25 06:05:36 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10', speculative_config=None, tokenizer='/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=257, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10, enable_prefix_caching=True, enable_chunked_prefill=False, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [257], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 2, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=1084893) INFO 01-25 06:05:36 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:38239 backend=nccl
(EngineCore_DP0 pid=1084893) INFO 01-25 06:05:36 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=1084893) INFO 01-25 06:05:36 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10...
(EngineCore_DP0 pid=1084893) INFO 01-25 06:05:37 [compressed_tensors_w8a8_int8.py:62] Using CutlassScaledMMLinearKernel for CompressedTensorsW8A8Int8
(EngineCore_DP0 pid=1084893) INFO 01-25 06:05:37 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=1084893) INFO 01-25 06:05:37 [default_loader.py:308] Loading weights took 0.21 seconds
(EngineCore_DP0 pid=1084893) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1084893) INFO 01-25 06:05:37 [gpu_model_runner.py:3659] Model loading took 1.6129 GiB memory and 0.512021 seconds
(EngineCore_DP0 pid=1084893) WARNING 01-25 06:05:39 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=1084893) INFO 01-25 06:05:39 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/e854dc4982/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=1084893) INFO 01-25 06:05:39 [backends.py:703] Dynamo bytecode transform time: 2.05 s
(EngineCore_DP0 pid=1084893) INFO 01-25 06:05:42 [backends.py:261] Cache the graph of compile range (1, 257) for later use
(EngineCore_DP0 pid=1084893) INFO 01-25 06:05:43 [backends.py:278] Compiling a graph for compile range (1, 257) takes 2.05 s
(EngineCore_DP0 pid=1084893) INFO 01-25 06:05:43 [monitor.py:34] torch.compile takes 4.11 s in total
(EngineCore_DP0 pid=1084893) INFO 01-25 06:05:43 [gpu_worker.py:375] Available KV cache memory: 10.18 GiB
(EngineCore_DP0 pid=1084893) INFO 01-25 06:05:44 [kv_cache_utils.py:1291] GPU KV cache size: 333,648 tokens
(EngineCore_DP0 pid=1084893) INFO 01-25 06:05:44 [kv_cache_utils.py:1296] Maximum concurrency for 257 tokens per request: 1226.65x
(EngineCore_DP0 pid=1084893) INFO 01-25 06:05:44 [gpu_model_runner.py:4587] Graph capturing finished in 1 secs, took 0.01 GiB
(EngineCore_DP0 pid=1084893) INFO 01-25 06:05:44 [core.py:259] init engine (profile, create kv cache, warmup model) took 6.93 seconds
INFO 01-25 06:05:45 [llm.py:360] Supported tasks: ['generate']
Throughput: 57.24 requests/s, 14711.35 total tokens/s, 57.24 output tokens/s
Total num prompt tokens:  32768
Total num output tokens:  128


─── STDERR ───
[2026-01-25 06:05:32] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:05:32] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-INT8'
[2026-01-25 06:05:32] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 06:05:32] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:05:32] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:05:32] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:05:32] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:05:32] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:05:32] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-INT8'
[2026-01-25 06:05:32] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 06:05:32] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:05:32] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:05:32] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:05:32] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 06:05:36] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:05:36] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-INT8'
[2026-01-25 06:05:36] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 06:05:36] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:05:36] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:05:36] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:05:36] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:05:36] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:05:36] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-INT8'
[2026-01-25 06:05:36] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 06:05:36] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:05:36] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:05:36] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:05:36] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1084893) [2026-01-25 06:05:37] INFO SlideSparseLinearMethod_INT8.py:801: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1084893) [2026-01-25 06:05:37] INFO gemm_wrapper.py:872: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1084893) [2026-01-25 06:05:37] INFO SlideSparseLinearMethod_INT8.py:448: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1084893) [2026-01-25 06:05:37] INFO SlideSparseLinearMethod_INT8.py:600: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=1084893) [2026-01-25 06:05:37] INFO kernels.py:132: Loaded tuned kernel for base model: Llama3.2-1B-INT8 (from Llama3.2-1B-INT8-SlideSparse-2_10)
(EngineCore_DP0 pid=1084893) [2026-01-25 06:05:37] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8-SlideSparse-2_10
(EngineCore_DP0 pid=1084893) [2026-01-25 06:05:37] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8-SlideSparse-2_10
(EngineCore_DP0 pid=1084893) [2026-01-25 06:05:37] INFO SlideSparseLinearMethod_INT8.py:617: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8-SlideSparse-2_10
(EngineCore_DP0 pid=1084893) [2026-01-25 06:05:37] INFO SlideSparseLinearMethod_INT8.py:621: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=1084893) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1084893) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.38it/s]
(EngineCore_DP0 pid=1084893) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.37it/s]
(EngineCore_DP0 pid=1084893) 
(EngineCore_DP0 pid=1084893) [2026-01-25 06:05:37] INFO SlideSparseLinearMethod_INT8.py:715: cuSPARSELt INT8 compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=1084893) [2026-01-25 06:05:37] INFO SlideSparseLinearMethod_INT8.py:725: cuSPARSELt INT8 compression done: 7618560 bytes
(EngineCore_DP0 pid=1084893) [2026-01-25 06:05:37] INFO SlideSparseLinearMethod_INT8.py:715: cuSPARSELt INT8 compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=1084893) [2026-01-25 06:05:37] INFO SlideSparseLinearMethod_INT8.py:725: cuSPARSELt INT8 compression done: 5079040 bytes
(EngineCore_DP0 pid=1084893) [2026-01-25 06:05:37] INFO SlideSparseLinearMethod_INT8.py:715: cuSPARSELt INT8 compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=1084893) [2026-01-25 06:05:37] INFO SlideSparseLinearMethod_INT8.py:725: cuSPARSELt INT8 compression done: 40632320 bytes
(EngineCore_DP0 pid=1084893) [2026-01-25 06:05:37] INFO SlideSparseLinearMethod_INT8.py:715: cuSPARSELt INT8 compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=1084893) [2026-01-25 06:05:37] INFO SlideSparseLinearMethod_INT8.py:725: cuSPARSELt INT8 compression done: 20185088 bytes
(EngineCore_DP0 pid=1084893) 2026-01-25 06:05:44,079 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1084893) 2026-01-25 06:05:44,092 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1084893) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 31.57it/s]
(EngineCore_DP0 pid=1084893) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.88it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.88it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 1943.23it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:29,  4.31it/s, est. speed input: 1103.53 toks/s, output: 4.31 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:03, 31.47it/s, est. speed input: 6658.70 toks/s, output: 26.01 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:02, 46.45it/s, est. speed input: 9547.41 toks/s, output: 37.29 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:00<00:01, 55.27it/s, est. speed input: 11293.73 toks/s, output: 44.12 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:00<00:01, 60.91it/s, est. speed input: 12477.11 toks/s, output: 48.74 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:00<00:01, 64.71it/s, est. speed input: 13338.59 toks/s, output: 52.10 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:00<00:01, 67.42it/s, est. speed input: 14002.58 toks/s, output: 54.70 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:01<00:01, 68.75it/s, est. speed input: 14483.84 toks/s, output: 56.58 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:01<00:00, 68.69it/s, est. speed input: 14802.33 toks/s, output: 57.82 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:01<00:00, 69.71it/s, est. speed input: 15130.55 toks/s, output: 59.10 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:01<00:00, 70.48it/s, est. speed input: 15407.51 toks/s, output: 60.19 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:01<00:00, 67.40it/s, est. speed input: 15433.12 toks/s, output: 60.29 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:01<00:00, 64.04it/s, est. speed input: 15361.38 toks/s, output: 60.01 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:01<00:00, 61.85it/s, est. speed input: 15304.84 toks/s, output: 59.78 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:01<00:00, 60.05it/s, est. speed input: 15239.56 toks/s, output: 59.53 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:01<00:00, 58.74it/s, est. speed input: 15178.72 toks/s, output: 59.29 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:02<00:00, 57.93it/s, est. speed input: 15132.30 toks/s, output: 59.11 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 57.93it/s, est. speed input: 15104.43 toks/s, output: 59.00 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 59.00it/s, est. speed input: 15104.43 toks/s, output: 59.00 toks/s]
[rank0]:[W125 06:05:47.360580453 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 20.5s

测试结果:
  Requests/s:   57.24
  Tokens/s:     14711.35
  Total Reqs:   128
  Elapsed:      2.24s

  [Prefill 分析]
  Total Prefill Tokens: 32768
  Prefill Tokens/s:     14654.11


------------------------------------------------------------
  生成 CSV: Llama3.2-1B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_10/Llama3.2-1B-INT8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
16,16,1,128,128,62.8908,1069.1431,2.0353
128,128,1,128,128,65.6801,8472.7332,1.9488
256,256,1,128,128,57.2426,14711.3518,2.2361

------------------------------------------------------------

[INFO] 完成: 3 成功, 0 失败

============================================================
  Llama3.2-1B-INT8 | cuSPARSELt (2_10) | decode
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_10

============================================================
[1/3] 测试 M=16
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16
│   M_prefill     = 256 (= 16 x 16)
│   M_decode      = 16
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 16
│   --max-num-seqs           = 16
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
INFO 01-25 06:05:52 [datasets.py:612] Sampling input_len from [15, 15] and output_len from [256, 256]
INFO 01-25 06:05:52 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10', 'max_model_len': 272, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 272, 'max_num_seqs': 16, 'disable_log_stats': True, 'enable_lora': None, 'enable_chunked_prefill': False, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10'}
INFO 01-25 06:05:52 [model.py:514] Resolved architecture: LlamaForCausalLM
INFO 01-25 06:05:52 [model.py:1661] Using max model len 272
WARNING 01-25 06:05:52 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1085277) INFO 01-25 06:05:56 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10', speculative_config=None, tokenizer='/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=272, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10, enable_prefix_caching=True, enable_chunked_prefill=False, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [272], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 32, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=1085277) INFO 01-25 06:05:56 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:39715 backend=nccl
(EngineCore_DP0 pid=1085277) INFO 01-25 06:05:56 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=1085277) INFO 01-25 06:05:56 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10...
(EngineCore_DP0 pid=1085277) INFO 01-25 06:05:56 [compressed_tensors_w8a8_int8.py:62] Using CutlassScaledMMLinearKernel for CompressedTensorsW8A8Int8
(EngineCore_DP0 pid=1085277) INFO 01-25 06:05:56 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=1085277) INFO 01-25 06:05:57 [default_loader.py:308] Loading weights took 0.21 seconds
(EngineCore_DP0 pid=1085277) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1085277) INFO 01-25 06:05:57 [gpu_model_runner.py:3659] Model loading took 1.6129 GiB memory and 0.739602 seconds
(EngineCore_DP0 pid=1085277) WARNING 01-25 06:05:59 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=1085277) INFO 01-25 06:06:00 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/ca7c7ba882/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=1085277) INFO 01-25 06:06:00 [backends.py:703] Dynamo bytecode transform time: 2.06 s
(EngineCore_DP0 pid=1085277) INFO 01-25 06:06:02 [backends.py:261] Cache the graph of compile range (1, 272) for later use
(EngineCore_DP0 pid=1085277) INFO 01-25 06:06:03 [backends.py:278] Compiling a graph for compile range (1, 272) takes 1.99 s
(EngineCore_DP0 pid=1085277) INFO 01-25 06:06:03 [monitor.py:34] torch.compile takes 4.05 s in total
(EngineCore_DP0 pid=1085277) INFO 01-25 06:06:03 [gpu_worker.py:375] Available KV cache memory: 10.18 GiB
(EngineCore_DP0 pid=1085277) INFO 01-25 06:06:04 [kv_cache_utils.py:1291] GPU KV cache size: 333,648 tokens
(EngineCore_DP0 pid=1085277) INFO 01-25 06:06:04 [kv_cache_utils.py:1296] Maximum concurrency for 272 tokens per request: 1226.65x
(EngineCore_DP0 pid=1085277) INFO 01-25 06:06:04 [gpu_model_runner.py:4587] Graph capturing finished in 1 secs, took 0.03 GiB
(EngineCore_DP0 pid=1085277) INFO 01-25 06:06:04 [core.py:259] init engine (profile, create kv cache, warmup model) took 7.14 seconds
INFO 01-25 06:06:05 [llm.py:360] Supported tasks: ['generate']
Throughput: 17.64 requests/s, 4798.05 total tokens/s, 4515.81 output tokens/s
Total num prompt tokens:  256
Total num output tokens:  4096


─── STDERR ───
[2026-01-25 06:05:52] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:05:52] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-INT8'
[2026-01-25 06:05:52] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 06:05:52] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:05:52] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:05:52] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:05:52] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:05:52] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:05:52] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-INT8'
[2026-01-25 06:05:52] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 06:05:52] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:05:52] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:05:52] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:05:52] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 06:05:56] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:05:56] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-INT8'
[2026-01-25 06:05:56] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 06:05:56] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:05:56] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:05:56] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:05:56] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:05:56] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:05:56] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-INT8'
[2026-01-25 06:05:56] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 06:05:56] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:05:56] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:05:56] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:05:56] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1085277) [2026-01-25 06:05:56] INFO SlideSparseLinearMethod_INT8.py:801: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1085277) [2026-01-25 06:05:56] INFO gemm_wrapper.py:872: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1085277) [2026-01-25 06:05:56] INFO SlideSparseLinearMethod_INT8.py:448: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1085277) [2026-01-25 06:05:56] INFO SlideSparseLinearMethod_INT8.py:600: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=1085277) [2026-01-25 06:05:56] INFO kernels.py:132: Loaded tuned kernel for base model: Llama3.2-1B-INT8 (from Llama3.2-1B-INT8-SlideSparse-2_10)
(EngineCore_DP0 pid=1085277) [2026-01-25 06:05:56] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8-SlideSparse-2_10
(EngineCore_DP0 pid=1085277) [2026-01-25 06:05:56] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8-SlideSparse-2_10
(EngineCore_DP0 pid=1085277) [2026-01-25 06:05:56] INFO SlideSparseLinearMethod_INT8.py:617: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8-SlideSparse-2_10
(EngineCore_DP0 pid=1085277) [2026-01-25 06:05:56] INFO SlideSparseLinearMethod_INT8.py:621: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=1085277) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1085277) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.28it/s]
(EngineCore_DP0 pid=1085277) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.28it/s]
(EngineCore_DP0 pid=1085277) 
(EngineCore_DP0 pid=1085277) [2026-01-25 06:05:57] INFO SlideSparseLinearMethod_INT8.py:715: cuSPARSELt INT8 compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=1085277) [2026-01-25 06:05:57] INFO SlideSparseLinearMethod_INT8.py:725: cuSPARSELt INT8 compression done: 7618560 bytes
(EngineCore_DP0 pid=1085277) [2026-01-25 06:05:57] INFO SlideSparseLinearMethod_INT8.py:715: cuSPARSELt INT8 compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=1085277) [2026-01-25 06:05:57] INFO SlideSparseLinearMethod_INT8.py:725: cuSPARSELt INT8 compression done: 5079040 bytes
(EngineCore_DP0 pid=1085277) [2026-01-25 06:05:57] INFO SlideSparseLinearMethod_INT8.py:715: cuSPARSELt INT8 compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=1085277) [2026-01-25 06:05:57] INFO SlideSparseLinearMethod_INT8.py:725: cuSPARSELt INT8 compression done: 40632320 bytes
(EngineCore_DP0 pid=1085277) [2026-01-25 06:05:57] INFO SlideSparseLinearMethod_INT8.py:715: cuSPARSELt INT8 compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=1085277) [2026-01-25 06:05:57] INFO SlideSparseLinearMethod_INT8.py:725: cuSPARSELt INT8 compression done: 20185088 bytes
(EngineCore_DP0 pid=1085277) 2026-01-25 06:06:04,078 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1085277) 2026-01-25 06:06:04,092 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1085277) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00, 33.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 33.88it/s]
(EngineCore_DP0 pid=1085277) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 1/5 [00:00<00:00,  6.93it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 20.76it/s]

Adding requests:   0%|          | 0/16 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 16/16 [00:00<00:00, 1271.07it/s]

Processed prompts:   0%|          | 0/16 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   6%|▋         | 1/16 [00:00<00:13,  1.13it/s, est. speed input: 18.01 toks/s, output: 288.09 toks/s]
Processed prompts: 100%|██████████| 16/16 [00:00<00:00,  1.13it/s, est. speed input: 286.90 toks/s, output: 4590.38 toks/s]
Processed prompts: 100%|██████████| 16/16 [00:00<00:00, 17.93it/s, est. speed input: 286.90 toks/s, output: 4590.38 toks/s]
[rank0]:[W125 06:06:06.286777785 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 19.1s

测试结果:
  Requests/s:   17.64
  Tokens/s:     4798.05
  Total Reqs:   16
  Elapsed:      0.91s

  [Decode 分析]
  Total Decode Tokens:  4096
  Decode Tokens/s:      4515.81

============================================================
[2/3] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 2048 (= 128 x 16)
│   M_decode      = 128
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 128
│   --max-num-seqs           = 128
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
INFO 01-25 06:06:11 [datasets.py:612] Sampling input_len from [15, 15] and output_len from [256, 256]
INFO 01-25 06:06:11 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10', 'max_model_len': 272, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 272, 'max_num_seqs': 128, 'disable_log_stats': True, 'enable_lora': None, 'enable_chunked_prefill': False, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10'}
INFO 01-25 06:06:11 [model.py:514] Resolved architecture: LlamaForCausalLM
INFO 01-25 06:06:11 [model.py:1661] Using max model len 272
WARNING 01-25 06:06:11 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1085652) INFO 01-25 06:06:15 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10', speculative_config=None, tokenizer='/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=272, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10, enable_prefix_caching=True, enable_chunked_prefill=False, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [272], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 256, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=1085652) INFO 01-25 06:06:15 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:42903 backend=nccl
(EngineCore_DP0 pid=1085652) INFO 01-25 06:06:15 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=1085652) INFO 01-25 06:06:16 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10...
(EngineCore_DP0 pid=1085652) INFO 01-25 06:06:16 [compressed_tensors_w8a8_int8.py:62] Using CutlassScaledMMLinearKernel for CompressedTensorsW8A8Int8
(EngineCore_DP0 pid=1085652) INFO 01-25 06:06:16 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=1085652) INFO 01-25 06:06:16 [default_loader.py:308] Loading weights took 0.20 seconds
(EngineCore_DP0 pid=1085652) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1085652) INFO 01-25 06:06:16 [gpu_model_runner.py:3659] Model loading took 1.6129 GiB memory and 0.501510 seconds
(EngineCore_DP0 pid=1085652) WARNING 01-25 06:06:19 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=1085652) INFO 01-25 06:06:19 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/3be4b353c4/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=1085652) INFO 01-25 06:06:19 [backends.py:703] Dynamo bytecode transform time: 2.06 s
(EngineCore_DP0 pid=1085652) INFO 01-25 06:06:20 [backends.py:261] Cache the graph of compile range (1, 272) for later use
(EngineCore_DP0 pid=1085652) INFO 01-25 06:06:21 [backends.py:278] Compiling a graph for compile range (1, 272) takes 0.84 s
(EngineCore_DP0 pid=1085652) INFO 01-25 06:06:21 [monitor.py:34] torch.compile takes 2.90 s in total
(EngineCore_DP0 pid=1085652) INFO 01-25 06:06:21 [gpu_worker.py:375] Available KV cache memory: 10.02 GiB
(EngineCore_DP0 pid=1085652) INFO 01-25 06:06:21 [kv_cache_utils.py:1291] GPU KV cache size: 328,336 tokens
(EngineCore_DP0 pid=1085652) INFO 01-25 06:06:21 [kv_cache_utils.py:1296] Maximum concurrency for 272 tokens per request: 1207.12x
(EngineCore_DP0 pid=1085652) INFO 01-25 06:06:23 [gpu_model_runner.py:4587] Graph capturing finished in 2 secs, took 0.16 GiB
(EngineCore_DP0 pid=1085652) INFO 01-25 06:06:23 [core.py:259] init engine (profile, create kv cache, warmup model) took 7.02 seconds
INFO 01-25 06:06:24 [llm.py:360] Supported tasks: ['generate']
Throughput: 74.06 requests/s, 20145.67 total tokens/s, 18960.63 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768


─── STDERR ───
[2026-01-25 06:06:11] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:06:11] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-INT8'
[2026-01-25 06:06:11] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 06:06:11] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:06:11] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:06:11] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:06:11] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:06:11] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:06:11] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-INT8'
[2026-01-25 06:06:11] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 06:06:11] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:06:11] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:06:11] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:06:11] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 06:06:15] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:06:15] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-INT8'
[2026-01-25 06:06:15] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 06:06:15] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:06:15] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:06:15] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:06:15] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:06:15] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:06:15] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-INT8'
[2026-01-25 06:06:15] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 06:06:15] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:06:15] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:06:15] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:06:15] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1085652) [2026-01-25 06:06:16] INFO SlideSparseLinearMethod_INT8.py:801: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1085652) [2026-01-25 06:06:16] INFO gemm_wrapper.py:872: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1085652) [2026-01-25 06:06:16] INFO SlideSparseLinearMethod_INT8.py:448: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1085652) [2026-01-25 06:06:16] INFO SlideSparseLinearMethod_INT8.py:600: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=1085652) [2026-01-25 06:06:16] INFO kernels.py:132: Loaded tuned kernel for base model: Llama3.2-1B-INT8 (from Llama3.2-1B-INT8-SlideSparse-2_10)
(EngineCore_DP0 pid=1085652) [2026-01-25 06:06:16] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8-SlideSparse-2_10
(EngineCore_DP0 pid=1085652) [2026-01-25 06:06:16] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8-SlideSparse-2_10
(EngineCore_DP0 pid=1085652) [2026-01-25 06:06:16] INFO SlideSparseLinearMethod_INT8.py:617: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8-SlideSparse-2_10
(EngineCore_DP0 pid=1085652) [2026-01-25 06:06:16] INFO SlideSparseLinearMethod_INT8.py:621: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=1085652) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1085652) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.54it/s]
(EngineCore_DP0 pid=1085652) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.54it/s]
(EngineCore_DP0 pid=1085652) 
(EngineCore_DP0 pid=1085652) [2026-01-25 06:06:16] INFO SlideSparseLinearMethod_INT8.py:715: cuSPARSELt INT8 compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=1085652) [2026-01-25 06:06:16] INFO SlideSparseLinearMethod_INT8.py:725: cuSPARSELt INT8 compression done: 7618560 bytes
(EngineCore_DP0 pid=1085652) [2026-01-25 06:06:16] INFO SlideSparseLinearMethod_INT8.py:715: cuSPARSELt INT8 compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=1085652) [2026-01-25 06:06:16] INFO SlideSparseLinearMethod_INT8.py:725: cuSPARSELt INT8 compression done: 5079040 bytes
(EngineCore_DP0 pid=1085652) [2026-01-25 06:06:16] INFO SlideSparseLinearMethod_INT8.py:715: cuSPARSELt INT8 compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=1085652) [2026-01-25 06:06:16] INFO SlideSparseLinearMethod_INT8.py:725: cuSPARSELt INT8 compression done: 40632320 bytes
(EngineCore_DP0 pid=1085652) [2026-01-25 06:06:16] INFO SlideSparseLinearMethod_INT8.py:715: cuSPARSELt INT8 compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=1085652) [2026-01-25 06:06:16] INFO SlideSparseLinearMethod_INT8.py:725: cuSPARSELt INT8 compression done: 20185088 bytes
(EngineCore_DP0 pid=1085652) 2026-01-25 06:06:21,983 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1085652) 2026-01-25 06:06:21,998 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1085652) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█▏        | 4/35 [00:00<00:00, 34.22it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  23%|██▎       | 8/35 [00:00<00:00, 35.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  34%|███▍      | 12/35 [00:00<00:00, 35.86it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  46%|████▌     | 16/35 [00:00<00:00, 36.27it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 20/35 [00:00<00:00, 35.37it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 24/35 [00:00<00:00, 35.82it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 28/35 [00:00<00:00, 36.09it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████▏| 32/35 [00:00<00:00, 36.26it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:00<00:00, 35.72it/s]
(EngineCore_DP0 pid=1085652) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   5%|▌         | 1/19 [00:00<00:02,  6.86it/s]
Capturing CUDA graphs (decode, FULL):  26%|██▋       | 5/19 [00:00<00:00, 22.48it/s]
Capturing CUDA graphs (decode, FULL):  47%|████▋     | 9/19 [00:00<00:00, 29.34it/s]
Capturing CUDA graphs (decode, FULL):  74%|███████▎  | 14/19 [00:00<00:00, 34.05it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:00<00:00, 36.76it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:00<00:00, 31.74it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:   5%|▌         | 7/128 [00:00<00:02, 49.11it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 819.46it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:01<02:53,  1.36s/it, est. speed input: 11.74 toks/s, output: 187.91 toks/s]
Processed prompts:   5%|▌         | 7/128 [00:01<00:20,  6.00it/s, est. speed input: 73.43 toks/s, output: 1174.86 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00,  6.00it/s, est. speed input: 1304.26 toks/s, output: 20868.17 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 81.51it/s, est. speed input: 1304.26 toks/s, output: 20868.17 toks/s]
[rank0]:[W125 06:06:26.075688245 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 19.6s

测试结果:
  Requests/s:   74.06
  Tokens/s:     20145.67
  Total Reqs:   128
  Elapsed:      1.73s

  [Decode 分析]
  Total Decode Tokens:  32768
  Decode Tokens/s:      18960.63

============================================================
[3/3] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 4096 (= 256 x 16)
│   M_decode      = 256
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 256
│   --max-num-seqs           = 256
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
INFO 01-25 06:06:31 [datasets.py:612] Sampling input_len from [15, 15] and output_len from [256, 256]
INFO 01-25 06:06:31 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10', 'max_model_len': 272, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 272, 'max_num_seqs': 256, 'disable_log_stats': True, 'enable_lora': None, 'enable_chunked_prefill': False, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10'}
INFO 01-25 06:06:31 [model.py:514] Resolved architecture: LlamaForCausalLM
INFO 01-25 06:06:31 [model.py:1661] Using max model len 272
WARNING 01-25 06:06:31 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1086000) INFO 01-25 06:06:35 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10', speculative_config=None, tokenizer='/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=272, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10, enable_prefix_caching=True, enable_chunked_prefill=False, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [272], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 272, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=1086000) INFO 01-25 06:06:35 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:43545 backend=nccl
(EngineCore_DP0 pid=1086000) INFO 01-25 06:06:35 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=1086000) INFO 01-25 06:06:35 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10...
(EngineCore_DP0 pid=1086000) INFO 01-25 06:06:35 [compressed_tensors_w8a8_int8.py:62] Using CutlassScaledMMLinearKernel for CompressedTensorsW8A8Int8
(EngineCore_DP0 pid=1086000) INFO 01-25 06:06:35 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=1086000) INFO 01-25 06:06:35 [default_loader.py:308] Loading weights took 0.20 seconds
(EngineCore_DP0 pid=1086000) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1086000) INFO 01-25 06:06:36 [gpu_model_runner.py:3659] Model loading took 1.6129 GiB memory and 0.492781 seconds
(EngineCore_DP0 pid=1086000) WARNING 01-25 06:06:38 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=1086000) INFO 01-25 06:06:38 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/97ae5ffc52/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=1086000) INFO 01-25 06:06:38 [backends.py:703] Dynamo bytecode transform time: 2.06 s
(EngineCore_DP0 pid=1086000) INFO 01-25 06:06:39 [backends.py:261] Cache the graph of compile range (1, 272) for later use
(EngineCore_DP0 pid=1086000) INFO 01-25 06:06:40 [backends.py:278] Compiling a graph for compile range (1, 272) takes 0.83 s
(EngineCore_DP0 pid=1086000) INFO 01-25 06:06:40 [monitor.py:34] torch.compile takes 2.89 s in total
(EngineCore_DP0 pid=1086000) INFO 01-25 06:06:41 [gpu_worker.py:375] Available KV cache memory: 9.44 GiB
(EngineCore_DP0 pid=1086000) INFO 01-25 06:06:41 [kv_cache_utils.py:1291] GPU KV cache size: 309,296 tokens
(EngineCore_DP0 pid=1086000) INFO 01-25 06:06:41 [kv_cache_utils.py:1296] Maximum concurrency for 272 tokens per request: 1137.12x
(EngineCore_DP0 pid=1086000) INFO 01-25 06:06:43 [gpu_model_runner.py:4587] Graph capturing finished in 3 secs, took 0.21 GiB
(EngineCore_DP0 pid=1086000) INFO 01-25 06:06:43 [core.py:259] init engine (profile, create kv cache, warmup model) took 7.58 seconds
INFO 01-25 06:06:44 [llm.py:360] Supported tasks: ['generate']
Throughput: 84.40 requests/s, 22957.01 total tokens/s, 21606.60 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536


─── STDERR ───
[2026-01-25 06:06:31] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:06:31] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-INT8'
[2026-01-25 06:06:31] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 06:06:31] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:06:31] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:06:31] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:06:31] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:06:31] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:06:31] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-INT8'
[2026-01-25 06:06:31] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 06:06:31] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:06:31] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:06:31] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:06:31] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 06:06:34] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:06:35] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-INT8'
[2026-01-25 06:06:35] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 06:06:35] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:06:35] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:06:35] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:06:35] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:06:35] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:06:35] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-INT8'
[2026-01-25 06:06:35] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 06:06:35] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:06:35] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:06:35] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:06:35] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1086000) [2026-01-25 06:06:35] INFO SlideSparseLinearMethod_INT8.py:801: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1086000) [2026-01-25 06:06:35] INFO gemm_wrapper.py:872: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1086000) [2026-01-25 06:06:35] INFO SlideSparseLinearMethod_INT8.py:448: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1086000) [2026-01-25 06:06:35] INFO SlideSparseLinearMethod_INT8.py:600: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=1086000) [2026-01-25 06:06:35] INFO kernels.py:132: Loaded tuned kernel for base model: Llama3.2-1B-INT8 (from Llama3.2-1B-INT8-SlideSparse-2_10)
(EngineCore_DP0 pid=1086000) [2026-01-25 06:06:35] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8-SlideSparse-2_10
(EngineCore_DP0 pid=1086000) [2026-01-25 06:06:35] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8-SlideSparse-2_10
(EngineCore_DP0 pid=1086000) [2026-01-25 06:06:35] INFO SlideSparseLinearMethod_INT8.py:617: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8-SlideSparse-2_10
(EngineCore_DP0 pid=1086000) [2026-01-25 06:06:35] INFO SlideSparseLinearMethod_INT8.py:621: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=1086000) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1086000) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.54it/s]
(EngineCore_DP0 pid=1086000) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.54it/s]
(EngineCore_DP0 pid=1086000) 
(EngineCore_DP0 pid=1086000) [2026-01-25 06:06:35] INFO SlideSparseLinearMethod_INT8.py:715: cuSPARSELt INT8 compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=1086000) [2026-01-25 06:06:35] INFO SlideSparseLinearMethod_INT8.py:725: cuSPARSELt INT8 compression done: 7618560 bytes
(EngineCore_DP0 pid=1086000) [2026-01-25 06:06:35] INFO SlideSparseLinearMethod_INT8.py:715: cuSPARSELt INT8 compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=1086000) [2026-01-25 06:06:35] INFO SlideSparseLinearMethod_INT8.py:725: cuSPARSELt INT8 compression done: 5079040 bytes
(EngineCore_DP0 pid=1086000) [2026-01-25 06:06:35] INFO SlideSparseLinearMethod_INT8.py:715: cuSPARSELt INT8 compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=1086000) [2026-01-25 06:06:35] INFO SlideSparseLinearMethod_INT8.py:725: cuSPARSELt INT8 compression done: 40632320 bytes
(EngineCore_DP0 pid=1086000) [2026-01-25 06:06:35] INFO SlideSparseLinearMethod_INT8.py:715: cuSPARSELt INT8 compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=1086000) [2026-01-25 06:06:35] INFO SlideSparseLinearMethod_INT8.py:725: cuSPARSELt INT8 compression done: 20185088 bytes
(EngineCore_DP0 pid=1086000) 2026-01-25 06:06:41,236 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1086000) 2026-01-25 06:06:41,250 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1086000) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/36 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 4/36 [00:00<00:00, 34.33it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 8/36 [00:00<00:00, 35.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 12/36 [00:00<00:00, 36.18it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  44%|████▍     | 16/36 [00:00<00:00, 36.59it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  56%|█████▌    | 20/36 [00:00<00:00, 35.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 24/36 [00:00<00:00, 31.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 28/36 [00:00<00:00, 26.11it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 31/36 [00:01<00:00, 23.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  97%|█████████▋| 35/36 [00:01<00:00, 26.94it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:01<00:00, 29.20it/s]
(EngineCore_DP0 pid=1086000) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   3%|▎         | 1/35 [00:00<00:04,  6.88it/s]
Capturing CUDA graphs (decode, FULL):  17%|█▋        | 6/35 [00:00<00:01, 25.30it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 10/35 [00:00<00:00, 30.94it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 15/35 [00:00<00:00, 34.66it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 20/35 [00:00<00:00, 36.77it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 25/35 [00:00<00:00, 38.10it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 30/35 [00:00<00:00, 39.22it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:00<00:00, 40.16it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:00<00:00, 35.72it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 9343.71it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:02<11:38,  2.74s/it, est. speed input: 5.84 toks/s, output: 93.39 toks/s]
Processed prompts:  50%|████▉     | 127/256 [00:02<00:02, 62.72it/s, est. speed input: 714.88 toks/s, output: 11438.12 toks/s]
Processed prompts:  91%|█████████ | 232/256 [00:02<00:00, 127.89it/s, est. speed input: 1261.38 toks/s, output: 20181.98 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 127.89it/s, est. speed input: 1363.08 toks/s, output: 21809.31 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 85.19it/s, est. speed input: 1363.08 toks/s, output: 21809.31 toks/s] 
[rank0]:[W125 06:06:47.413710098 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 21.3s

测试结果:
  Requests/s:   84.40
  Tokens/s:     22957.01
  Total Reqs:   256
  Elapsed:      3.03s

  [Decode 分析]
  Total Decode Tokens:  65536
  Decode Tokens/s:      21606.60


------------------------------------------------------------
  生成 CSV: Llama3.2-1B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_10/Llama3.2-1B-INT8_decode.csv

预览:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
16,16,16,16,256,256,17.6399,4798.0479,0.9070
128,16,128,128,256,256,74.0650,20145.6725,1.7282
256,16,256,256,256,256,84.4008,22957.0123,3.0331

------------------------------------------------------------

[INFO] 完成: 3 成功, 0 失败

======================================================================
[SlideSparse] 未找到 Llama3.2-1B-FP8-SlideSparse-2_4 checkpoint
[SlideSparse] 发现基础模型: Llama3.2-1B-FP8
[SlideSparse] 自动转换中...
[SlideSparse] 开始转换: Llama3.2-1B-FP8 -> SlideSparse-2_4
======================================================================

[SlideSparse] 转换成功: Llama3.2-1B-FP8-SlideSparse-2_4
======================================================================


============================================================
  Llama3.2-1B-FP8 | cuSPARSELt (2_4) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_4
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_4

============================================================
[1/3] 测试 M=16
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-FP8                                 │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16
│   M_prefill     = 16 (= 1 x 16)
│   M_decode      = 1
│   batched_tokens = 17 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 17
│   --max-num-batched-tokens = 17
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
INFO 01-25 06:07:20 [datasets.py:612] Sampling input_len from [15, 15] and output_len from [1, 1]
INFO 01-25 06:07:21 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_4', 'max_model_len': 17, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 17, 'max_num_seqs': 1, 'disable_log_stats': True, 'enable_lora': None, 'enable_chunked_prefill': False, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_4'}
INFO 01-25 06:07:21 [model.py:514] Resolved architecture: LlamaForCausalLM
INFO 01-25 06:07:21 [model.py:1661] Using max model len 17
WARNING 01-25 06:07:21 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1086607) INFO 01-25 06:07:25 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_4', speculative_config=None, tokenizer='/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_4', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=17, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_4, enable_prefix_caching=True, enable_chunked_prefill=False, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [17], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 2, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=1086607) INFO 01-25 06:07:25 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:36281 backend=nccl
(EngineCore_DP0 pid=1086607) INFO 01-25 06:07:25 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=1086607) INFO 01-25 06:07:25 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_4...
(EngineCore_DP0 pid=1086607) INFO 01-25 06:07:25 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=1086607) INFO 01-25 06:07:26 [default_loader.py:308] Loading weights took 0.15 seconds
(EngineCore_DP0 pid=1086607) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1086607) INFO 01-25 06:07:26 [gpu_model_runner.py:3659] Model loading took 1.0738 GiB memory and 0.377893 seconds
(EngineCore_DP0 pid=1086607) WARNING 01-25 06:07:28 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=1086607) INFO 01-25 06:07:28 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/71a25ad151/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=1086607) INFO 01-25 06:07:28 [backends.py:703] Dynamo bytecode transform time: 2.05 s
(EngineCore_DP0 pid=1086607) INFO 01-25 06:07:30 [backends.py:261] Cache the graph of compile range (1, 17) for later use
(EngineCore_DP0 pid=1086607) INFO 01-25 06:07:32 [backends.py:278] Compiling a graph for compile range (1, 17) takes 2.58 s
(EngineCore_DP0 pid=1086607) INFO 01-25 06:07:32 [monitor.py:34] torch.compile takes 4.64 s in total
(EngineCore_DP0 pid=1086607) INFO 01-25 06:07:33 [gpu_worker.py:375] Available KV cache memory: 10.72 GiB
(EngineCore_DP0 pid=1086607) INFO 01-25 06:07:33 [kv_cache_utils.py:1291] GPU KV cache size: 351,184 tokens
(EngineCore_DP0 pid=1086607) INFO 01-25 06:07:33 [kv_cache_utils.py:1296] Maximum concurrency for 17 tokens per request: 10974.50x
(EngineCore_DP0 pid=1086607) INFO 01-25 06:07:34 [gpu_model_runner.py:4587] Graph capturing finished in 1 secs, took 0.01 GiB
(EngineCore_DP0 pid=1086607) INFO 01-25 06:07:34 [core.py:259] init engine (profile, create kv cache, warmup model) took 7.72 seconds
INFO 01-25 06:07:34 [llm.py:360] Supported tasks: ['generate']
Throughput: 65.01 requests/s, 1105.22 total tokens/s, 65.01 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  128


─── STDERR ───
[2026-01-25 06:07:21] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:07:21] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-FP8'
[2026-01-25 06:07:21] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 06:07:21] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:07:21] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:07:21] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:07:21] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:07:21] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:07:21] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-FP8'
[2026-01-25 06:07:21] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 06:07:21] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:07:21] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:07:21] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:07:21] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 06:07:25] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:07:25] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-FP8'
[2026-01-25 06:07:25] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 06:07:25] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:07:25] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:07:25] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:07:25] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:07:25] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:07:25] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-FP8'
[2026-01-25 06:07:25] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 06:07:25] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:07:25] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:07:25] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:07:25] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1086607) [2026-01-25 06:07:25] INFO SlideSparseLinearMethod_FP8.py:730: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1086607) [2026-01-25 06:07:25] INFO gemm_wrapper.py:872: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1086607) [2026-01-25 06:07:25] INFO SlideSparseLinearMethod_FP8.py:388: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1086607) [2026-01-25 06:07:25] INFO SlideSparseLinearMethod_FP8.py:535: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=1086607) [2026-01-25 06:07:25] INFO kernels.py:132: Loaded tuned kernel for base model: Llama3.2-1B-FP8 (from Llama3.2-1B-FP8-SlideSparse-2_4)
(EngineCore_DP0 pid=1086607) [2026-01-25 06:07:25] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8-SlideSparse-2_4
(EngineCore_DP0 pid=1086607) [2026-01-25 06:07:25] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8-SlideSparse-2_4
(EngineCore_DP0 pid=1086607) [2026-01-25 06:07:25] INFO SlideSparseLinearMethod_FP8.py:552: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8-SlideSparse-2_4
(EngineCore_DP0 pid=1086607) [2026-01-25 06:07:25] INFO SlideSparseLinearMethod_FP8.py:556: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=1086607) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1086607) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  7.46it/s]
(EngineCore_DP0 pid=1086607) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  7.45it/s]
(EngineCore_DP0 pid=1086607) 
(EngineCore_DP0 pid=1086607) [2026-01-25 06:07:26] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [3072, 2048] -> 1D uint8
(EngineCore_DP0 pid=1086607) [2026-01-25 06:07:26] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 3932160 bytes
(EngineCore_DP0 pid=1086607) [2026-01-25 06:07:26] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [2048, 2048] -> 1D uint8
(EngineCore_DP0 pid=1086607) [2026-01-25 06:07:26] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 2621440 bytes
(EngineCore_DP0 pid=1086607) [2026-01-25 06:07:26] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [16384, 2048] -> 1D uint8
(EngineCore_DP0 pid=1086607) [2026-01-25 06:07:26] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 20971520 bytes
(EngineCore_DP0 pid=1086607) [2026-01-25 06:07:26] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [2048, 8192] -> 1D uint8
(EngineCore_DP0 pid=1086607) [2026-01-25 06:07:26] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 10485760 bytes
(EngineCore_DP0 pid=1086607) 2026-01-25 06:07:33,257 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1086607) 2026-01-25 06:07:33,270 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1086607) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  7.80it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  5.66it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  5.91it/s]
(EngineCore_DP0 pid=1086607) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.01it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.01it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 5472.02it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:23,  5.38it/s, est. speed input: 86.07 toks/s, output: 5.38 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:03, 36.35it/s, est. speed input: 487.98 toks/s, output: 30.50 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:02, 51.05it/s, est. speed input: 674.01 toks/s, output: 42.12 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:00<00:01, 59.19it/s, est. speed input: 781.02 toks/s, output: 48.81 toks/s]
Processed prompts:  25%|██▌       | 32/128 [00:00<00:01, 60.99it/s, est. speed input: 824.91 toks/s, output: 51.56 toks/s]
Processed prompts:  31%|███▏      | 40/128 [00:00<00:01, 64.99it/s, est. speed input: 876.70 toks/s, output: 54.79 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:00<00:01, 67.54it/s, est. speed input: 914.64 toks/s, output: 57.16 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:00<00:01, 69.00it/s, est. speed input: 942.66 toks/s, output: 58.92 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:01<00:00, 70.24it/s, est. speed input: 965.95 toks/s, output: 60.37 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:01<00:00, 68.85it/s, est. speed input: 975.21 toks/s, output: 60.95 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:01<00:00, 70.39it/s, est. speed input: 992.79 toks/s, output: 62.05 toks/s]
Processed prompts:  69%|██████▉   | 88/128 [00:01<00:00, 71.29it/s, est. speed input: 1006.96 toks/s, output: 62.93 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:01<00:00, 71.95it/s, est. speed input: 1019.17 toks/s, output: 63.70 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:01<00:00, 72.32it/s, est. speed input: 1029.44 toks/s, output: 64.34 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:01<00:00, 72.71it/s, est. speed input: 1038.81 toks/s, output: 64.93 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:01<00:00, 72.60it/s, est. speed input: 1045.96 toks/s, output: 65.37 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 73.02it/s, est. speed input: 1053.63 toks/s, output: 65.85 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 73.02it/s, est. speed input: 1053.63 toks/s, output: 65.85 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 65.85it/s, est. speed input: 1053.63 toks/s, output: 65.85 toks/s]
[rank0]:[W125 06:07:36.645524916 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 20.8s

测试结果:
  Requests/s:   65.01
  Tokens/s:     1105.22
  Total Reqs:   128
  Elapsed:      1.97s

  [Prefill 分析]
  Total Prefill Tokens: 2048
  Prefill Tokens/s:     1040.21

============================================================
[2/3] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-FP8                                 │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 128 (= 1 x 128)
│   M_decode      = 1
│   batched_tokens = 129 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 128
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 129
│   --max-num-batched-tokens = 129
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
INFO 01-25 06:07:41 [datasets.py:612] Sampling input_len from [127, 127] and output_len from [1, 1]
INFO 01-25 06:07:41 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_4', 'max_model_len': 129, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 129, 'max_num_seqs': 1, 'disable_log_stats': True, 'enable_lora': None, 'enable_chunked_prefill': False, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_4'}
INFO 01-25 06:07:41 [model.py:514] Resolved architecture: LlamaForCausalLM
INFO 01-25 06:07:41 [model.py:1661] Using max model len 129
WARNING 01-25 06:07:41 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1087041) INFO 01-25 06:07:45 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_4', speculative_config=None, tokenizer='/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_4', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=129, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_4, enable_prefix_caching=True, enable_chunked_prefill=False, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [129], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 2, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=1087041) INFO 01-25 06:07:46 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:57401 backend=nccl
(EngineCore_DP0 pid=1087041) INFO 01-25 06:07:46 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=1087041) INFO 01-25 06:07:46 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_4...
(EngineCore_DP0 pid=1087041) INFO 01-25 06:07:46 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=1087041) INFO 01-25 06:07:46 [default_loader.py:308] Loading weights took 0.15 seconds
(EngineCore_DP0 pid=1087041) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1087041) INFO 01-25 06:07:47 [gpu_model_runner.py:3659] Model loading took 1.0738 GiB memory and 0.386113 seconds
(EngineCore_DP0 pid=1087041) WARNING 01-25 06:07:49 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=1087041) INFO 01-25 06:07:49 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/d06634a76e/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=1087041) INFO 01-25 06:07:49 [backends.py:703] Dynamo bytecode transform time: 2.08 s
(EngineCore_DP0 pid=1087041) INFO 01-25 06:07:51 [backends.py:261] Cache the graph of compile range (1, 129) for later use
(EngineCore_DP0 pid=1087041) INFO 01-25 06:07:52 [backends.py:278] Compiling a graph for compile range (1, 129) takes 2.30 s
(EngineCore_DP0 pid=1087041) INFO 01-25 06:07:52 [monitor.py:34] torch.compile takes 4.38 s in total
(EngineCore_DP0 pid=1087041) INFO 01-25 06:07:53 [gpu_worker.py:375] Available KV cache memory: 10.72 GiB
(EngineCore_DP0 pid=1087041) INFO 01-25 06:07:53 [kv_cache_utils.py:1291] GPU KV cache size: 351,152 tokens
(EngineCore_DP0 pid=1087041) INFO 01-25 06:07:53 [kv_cache_utils.py:1296] Maximum concurrency for 129 tokens per request: 2438.56x
(EngineCore_DP0 pid=1087041) INFO 01-25 06:07:54 [gpu_model_runner.py:4587] Graph capturing finished in 1 secs, took 0.01 GiB
(EngineCore_DP0 pid=1087041) INFO 01-25 06:07:54 [core.py:259] init engine (profile, create kv cache, warmup model) took 7.19 seconds
INFO 01-25 06:07:54 [llm.py:360] Supported tasks: ['generate']
Throughput: 68.24 requests/s, 8802.42 total tokens/s, 68.24 output tokens/s
Total num prompt tokens:  16384
Total num output tokens:  128


─── STDERR ───
[2026-01-25 06:07:41] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:07:41] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-FP8'
[2026-01-25 06:07:41] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 06:07:41] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:07:41] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:07:41] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:07:41] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:07:41] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:07:41] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-FP8'
[2026-01-25 06:07:41] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 06:07:41] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:07:41] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:07:41] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:07:41] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 06:07:45] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:07:45] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-FP8'
[2026-01-25 06:07:45] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 06:07:45] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:07:45] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:07:45] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:07:45] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:07:45] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:07:45] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-FP8'
[2026-01-25 06:07:45] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 06:07:45] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:07:45] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:07:45] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:07:45] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1087041) [2026-01-25 06:07:46] INFO SlideSparseLinearMethod_FP8.py:730: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1087041) [2026-01-25 06:07:46] INFO gemm_wrapper.py:872: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1087041) [2026-01-25 06:07:46] INFO SlideSparseLinearMethod_FP8.py:388: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1087041) [2026-01-25 06:07:46] INFO SlideSparseLinearMethod_FP8.py:535: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=1087041) [2026-01-25 06:07:46] INFO kernels.py:132: Loaded tuned kernel for base model: Llama3.2-1B-FP8 (from Llama3.2-1B-FP8-SlideSparse-2_4)
(EngineCore_DP0 pid=1087041) [2026-01-25 06:07:46] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8-SlideSparse-2_4
(EngineCore_DP0 pid=1087041) [2026-01-25 06:07:46] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8-SlideSparse-2_4
(EngineCore_DP0 pid=1087041) [2026-01-25 06:07:46] INFO SlideSparseLinearMethod_FP8.py:552: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8-SlideSparse-2_4
(EngineCore_DP0 pid=1087041) [2026-01-25 06:07:46] INFO SlideSparseLinearMethod_FP8.py:556: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=1087041) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1087041) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  7.24it/s]
(EngineCore_DP0 pid=1087041) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  7.23it/s]
(EngineCore_DP0 pid=1087041) 
(EngineCore_DP0 pid=1087041) [2026-01-25 06:07:46] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [3072, 2048] -> 1D uint8
(EngineCore_DP0 pid=1087041) [2026-01-25 06:07:46] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 3932160 bytes
(EngineCore_DP0 pid=1087041) [2026-01-25 06:07:46] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [2048, 2048] -> 1D uint8
(EngineCore_DP0 pid=1087041) [2026-01-25 06:07:46] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 2621440 bytes
(EngineCore_DP0 pid=1087041) [2026-01-25 06:07:46] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [16384, 2048] -> 1D uint8
(EngineCore_DP0 pid=1087041) [2026-01-25 06:07:46] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 20971520 bytes
(EngineCore_DP0 pid=1087041) [2026-01-25 06:07:46] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [2048, 8192] -> 1D uint8
(EngineCore_DP0 pid=1087041) [2026-01-25 06:07:46] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 10485760 bytes
(EngineCore_DP0 pid=1087041) 2026-01-25 06:07:53,664 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1087041) 2026-01-25 06:07:53,677 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1087041) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 31.39it/s]
(EngineCore_DP0 pid=1087041) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.35it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.34it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 3236.83it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:15,  8.11it/s, est. speed input: 1038.78 toks/s, output: 8.11 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:02, 44.76it/s, est. speed input: 4980.26 toks/s, output: 38.91 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:01, 57.46it/s, est. speed input: 6390.98 toks/s, output: 49.93 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:00<00:01, 62.83it/s, est. speed input: 7062.21 toks/s, output: 55.17 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:00<00:01, 66.67it/s, est. speed input: 7520.72 toks/s, output: 58.75 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:00<00:01, 68.53it/s, est. speed input: 7803.15 toks/s, output: 60.96 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:00<00:01, 70.21it/s, est. speed input: 8030.40 toks/s, output: 62.74 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:00<00:00, 71.21it/s, est. speed input: 8196.86 toks/s, output: 64.04 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:00<00:00, 71.86it/s, est. speed input: 8326.30 toks/s, output: 65.05 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:01<00:00, 72.52it/s, est. speed input: 8438.35 toks/s, output: 65.92 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:01<00:00, 73.29it/s, est. speed input: 8541.33 toks/s, output: 66.73 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:01<00:00, 73.69it/s, est. speed input: 8623.34 toks/s, output: 67.37 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:01<00:00, 74.05it/s, est. speed input: 8695.12 toks/s, output: 67.93 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:01<00:00, 74.56it/s, est. speed input: 8764.15 toks/s, output: 68.47 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:01<00:00, 74.97it/s, est. speed input: 8825.61 toks/s, output: 68.95 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:01<00:00, 75.24it/s, est. speed input: 8879.12 toks/s, output: 69.37 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 75.24it/s, est. speed input: 8925.86 toks/s, output: 69.73 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 69.73it/s, est. speed input: 8925.86 toks/s, output: 69.73 toks/s]
[rank0]:[W125 06:07:56.679348776 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 20.0s

测试结果:
  Requests/s:   68.24
  Tokens/s:     8802.42
  Total Reqs:   128
  Elapsed:      1.88s

  [Prefill 分析]
  Total Prefill Tokens: 16384
  Prefill Tokens/s:     8734.18

============================================================
[3/3] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-FP8                                 │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 256 (= 1 x 256)
│   M_decode      = 1
│   batched_tokens = 257 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 256
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 257
│   --max-num-batched-tokens = 257
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
INFO 01-25 06:08:01 [datasets.py:612] Sampling input_len from [255, 255] and output_len from [1, 1]
INFO 01-25 06:08:01 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_4', 'max_model_len': 257, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 257, 'max_num_seqs': 1, 'disable_log_stats': True, 'enable_lora': None, 'enable_chunked_prefill': False, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_4'}
INFO 01-25 06:08:01 [model.py:514] Resolved architecture: LlamaForCausalLM
INFO 01-25 06:08:01 [model.py:1661] Using max model len 257
WARNING 01-25 06:08:02 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1087412) INFO 01-25 06:08:06 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_4', speculative_config=None, tokenizer='/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_4', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=257, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_4, enable_prefix_caching=True, enable_chunked_prefill=False, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [257], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 2, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=1087412) INFO 01-25 06:08:06 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:43445 backend=nccl
(EngineCore_DP0 pid=1087412) INFO 01-25 06:08:06 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=1087412) INFO 01-25 06:08:06 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_4...
(EngineCore_DP0 pid=1087412) INFO 01-25 06:08:07 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=1087412) INFO 01-25 06:08:07 [default_loader.py:308] Loading weights took 0.15 seconds
(EngineCore_DP0 pid=1087412) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1087412) INFO 01-25 06:08:07 [gpu_model_runner.py:3659] Model loading took 1.0756 GiB memory and 0.394037 seconds
(EngineCore_DP0 pid=1087412) WARNING 01-25 06:08:09 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=1087412) INFO 01-25 06:08:09 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/549e2f2713/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=1087412) INFO 01-25 06:08:09 [backends.py:703] Dynamo bytecode transform time: 2.07 s
(EngineCore_DP0 pid=1087412) INFO 01-25 06:08:11 [backends.py:261] Cache the graph of compile range (1, 257) for later use
(EngineCore_DP0 pid=1087412) INFO 01-25 06:08:13 [backends.py:278] Compiling a graph for compile range (1, 257) takes 2.16 s
(EngineCore_DP0 pid=1087412) INFO 01-25 06:08:13 [monitor.py:34] torch.compile takes 4.23 s in total
(EngineCore_DP0 pid=1087412) INFO 01-25 06:08:13 [gpu_worker.py:375] Available KV cache memory: 10.71 GiB
(EngineCore_DP0 pid=1087412) INFO 01-25 06:08:14 [kv_cache_utils.py:1291] GPU KV cache size: 351,056 tokens
(EngineCore_DP0 pid=1087412) INFO 01-25 06:08:14 [kv_cache_utils.py:1296] Maximum concurrency for 257 tokens per request: 1290.65x
(EngineCore_DP0 pid=1087412) INFO 01-25 06:08:14 [gpu_model_runner.py:4587] Graph capturing finished in 1 secs, took 0.01 GiB
(EngineCore_DP0 pid=1087412) INFO 01-25 06:08:14 [core.py:259] init engine (profile, create kv cache, warmup model) took 7.10 seconds
INFO 01-25 06:08:15 [llm.py:360] Supported tasks: ['generate']
Throughput: 64.67 requests/s, 16619.33 total tokens/s, 64.67 output tokens/s
Total num prompt tokens:  32768
Total num output tokens:  128


─── STDERR ───
[2026-01-25 06:08:01] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:08:02] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-FP8'
[2026-01-25 06:08:02] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 06:08:02] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:08:02] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:08:02] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:08:02] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:08:02] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:08:02] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-FP8'
[2026-01-25 06:08:02] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 06:08:02] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:08:02] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:08:02] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:08:02] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 06:08:06] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:08:06] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-FP8'
[2026-01-25 06:08:06] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 06:08:06] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:08:06] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:08:06] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:08:06] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:08:06] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:08:06] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-FP8'
[2026-01-25 06:08:06] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 06:08:06] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:08:06] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:08:06] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:08:06] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1087412) [2026-01-25 06:08:06] INFO SlideSparseLinearMethod_FP8.py:730: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1087412) [2026-01-25 06:08:06] INFO gemm_wrapper.py:872: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1087412) [2026-01-25 06:08:06] INFO SlideSparseLinearMethod_FP8.py:388: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1087412) [2026-01-25 06:08:06] INFO SlideSparseLinearMethod_FP8.py:535: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=1087412) [2026-01-25 06:08:06] INFO kernels.py:132: Loaded tuned kernel for base model: Llama3.2-1B-FP8 (from Llama3.2-1B-FP8-SlideSparse-2_4)
(EngineCore_DP0 pid=1087412) [2026-01-25 06:08:06] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8-SlideSparse-2_4
(EngineCore_DP0 pid=1087412) [2026-01-25 06:08:06] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8-SlideSparse-2_4
(EngineCore_DP0 pid=1087412) [2026-01-25 06:08:06] INFO SlideSparseLinearMethod_FP8.py:552: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8-SlideSparse-2_4
(EngineCore_DP0 pid=1087412) [2026-01-25 06:08:06] INFO SlideSparseLinearMethod_FP8.py:556: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=1087412) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1087412) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  7.24it/s]
(EngineCore_DP0 pid=1087412) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  7.24it/s]
(EngineCore_DP0 pid=1087412) 
(EngineCore_DP0 pid=1087412) [2026-01-25 06:08:07] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [3072, 2048] -> 1D uint8
(EngineCore_DP0 pid=1087412) [2026-01-25 06:08:07] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 3932160 bytes
(EngineCore_DP0 pid=1087412) [2026-01-25 06:08:07] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [2048, 2048] -> 1D uint8
(EngineCore_DP0 pid=1087412) [2026-01-25 06:08:07] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 2621440 bytes
(EngineCore_DP0 pid=1087412) [2026-01-25 06:08:07] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [16384, 2048] -> 1D uint8
(EngineCore_DP0 pid=1087412) [2026-01-25 06:08:07] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 20971520 bytes
(EngineCore_DP0 pid=1087412) [2026-01-25 06:08:07] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [2048, 8192] -> 1D uint8
(EngineCore_DP0 pid=1087412) [2026-01-25 06:08:07] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 10485760 bytes
(EngineCore_DP0 pid=1087412) 2026-01-25 06:08:14,021 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1087412) 2026-01-25 06:08:14,034 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1087412) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 28.86it/s]
(EngineCore_DP0 pid=1087412) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.87it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.86it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 1914.10it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:22,  5.77it/s, est. speed input: 1476.96 toks/s, output: 5.77 toks/s]
Processed prompts:   6%|▋         | 8/128 [00:00<00:03, 34.54it/s, est. speed input: 7448.98 toks/s, output: 29.10 toks/s]
Processed prompts:  12%|█▎        | 16/128 [00:00<00:02, 50.46it/s, est. speed input: 10660.17 toks/s, output: 41.64 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:00<00:01, 56.70it/s, est. speed input: 12084.63 toks/s, output: 47.20 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:00<00:01, 62.52it/s, est. speed input: 13306.93 toks/s, output: 51.98 toks/s]
Processed prompts:  30%|███       | 39/128 [00:00<00:01, 65.87it/s, est. speed input: 14126.90 toks/s, output: 55.18 toks/s]
Processed prompts:  36%|███▌      | 46/128 [00:00<00:01, 66.99it/s, est. speed input: 14585.39 toks/s, output: 56.97 toks/s]
Processed prompts:  42%|████▏     | 54/128 [00:00<00:01, 69.38it/s, est. speed input: 15112.34 toks/s, output: 59.03 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:01<00:00, 71.26it/s, est. speed input: 15549.25 toks/s, output: 60.74 toks/s]
Processed prompts:  55%|█████▍    | 70/128 [00:01<00:00, 72.18it/s, est. speed input: 15879.18 toks/s, output: 62.03 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:01<00:00, 72.93it/s, est. speed input: 16158.98 toks/s, output: 63.12 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:01<00:00, 73.48it/s, est. speed input: 16396.04 toks/s, output: 64.05 toks/s]
Processed prompts:  73%|███████▎  | 94/128 [00:01<00:00, 73.84it/s, est. speed input: 16597.02 toks/s, output: 64.83 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:01<00:00, 74.03it/s, est. speed input: 16767.39 toks/s, output: 65.50 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:01<00:00, 74.21it/s, est. speed input: 16917.59 toks/s, output: 66.08 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:01<00:00, 73.49it/s, est. speed input: 17010.82 toks/s, output: 66.45 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:01<00:00, 73.61it/s, est. speed input: 17120.20 toks/s, output: 66.88 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 73.61it/s, est. speed input: 17141.91 toks/s, output: 66.96 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 66.96it/s, est. speed input: 17141.91 toks/s, output: 66.96 toks/s]
[rank0]:[W125 06:08:17.134877497 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 20.5s

测试结果:
  Requests/s:   64.67
  Tokens/s:     16619.33
  Total Reqs:   128
  Elapsed:      1.98s

  [Prefill 分析]
  Total Prefill Tokens: 32768
  Prefill Tokens/s:     16554.67


------------------------------------------------------------
  生成 CSV: Llama3.2-1B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_4/Llama3.2-1B-FP8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
16,16,1,128,128,65.0132,1105.2250,1.9688
128,128,1,128,128,68.2358,8802.4179,1.8758
256,256,1,128,128,64.6667,16619.3327,1.9794

------------------------------------------------------------

[INFO] 完成: 3 成功, 0 失败

============================================================
  Llama3.2-1B-FP8 | cuSPARSELt (2_4) | decode
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_4
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_4

============================================================
[1/3] 测试 M=16
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-FP8                                 │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16
│   M_prefill     = 256 (= 16 x 16)
│   M_decode      = 16
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 16
│   --max-num-seqs           = 16
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
INFO 01-25 06:08:22 [datasets.py:612] Sampling input_len from [15, 15] and output_len from [256, 256]
INFO 01-25 06:08:22 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_4', 'max_model_len': 272, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 272, 'max_num_seqs': 16, 'disable_log_stats': True, 'enable_lora': None, 'enable_chunked_prefill': False, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_4'}
INFO 01-25 06:08:22 [model.py:514] Resolved architecture: LlamaForCausalLM
INFO 01-25 06:08:22 [model.py:1661] Using max model len 272
WARNING 01-25 06:08:22 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1087783) INFO 01-25 06:08:26 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_4', speculative_config=None, tokenizer='/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_4', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=272, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_4, enable_prefix_caching=True, enable_chunked_prefill=False, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [272], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 32, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=1087783) INFO 01-25 06:08:26 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:53475 backend=nccl
(EngineCore_DP0 pid=1087783) INFO 01-25 06:08:26 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=1087783) INFO 01-25 06:08:26 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_4...
(EngineCore_DP0 pid=1087783) INFO 01-25 06:08:26 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=1087783) INFO 01-25 06:08:27 [default_loader.py:308] Loading weights took 0.16 seconds
(EngineCore_DP0 pid=1087783) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1087783) INFO 01-25 06:08:27 [gpu_model_runner.py:3659] Model loading took 1.0755 GiB memory and 0.672798 seconds
(EngineCore_DP0 pid=1087783) WARNING 01-25 06:08:29 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=1087783) INFO 01-25 06:08:29 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/3513dce69c/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=1087783) INFO 01-25 06:08:29 [backends.py:703] Dynamo bytecode transform time: 2.06 s
(EngineCore_DP0 pid=1087783) INFO 01-25 06:08:31 [backends.py:261] Cache the graph of compile range (1, 272) for later use
(EngineCore_DP0 pid=1087783) INFO 01-25 06:08:33 [backends.py:278] Compiling a graph for compile range (1, 272) takes 1.94 s
(EngineCore_DP0 pid=1087783) INFO 01-25 06:08:33 [monitor.py:34] torch.compile takes 4.00 s in total
(EngineCore_DP0 pid=1087783) INFO 01-25 06:08:33 [gpu_worker.py:375] Available KV cache memory: 10.71 GiB
(EngineCore_DP0 pid=1087783) INFO 01-25 06:08:33 [kv_cache_utils.py:1291] GPU KV cache size: 351,056 tokens
(EngineCore_DP0 pid=1087783) INFO 01-25 06:08:33 [kv_cache_utils.py:1296] Maximum concurrency for 272 tokens per request: 1290.65x
(EngineCore_DP0 pid=1087783) INFO 01-25 06:08:34 [gpu_model_runner.py:4587] Graph capturing finished in 1 secs, took 0.04 GiB
(EngineCore_DP0 pid=1087783) INFO 01-25 06:08:34 [core.py:259] init engine (profile, create kv cache, warmup model) took 7.06 seconds
INFO 01-25 06:08:35 [llm.py:360] Supported tasks: ['generate']
Throughput: 19.97 requests/s, 5432.95 total tokens/s, 5113.36 output tokens/s
Total num prompt tokens:  256
Total num output tokens:  4096


─── STDERR ───
[2026-01-25 06:08:22] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:08:22] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-FP8'
[2026-01-25 06:08:22] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 06:08:22] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:08:22] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:08:22] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:08:22] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:08:22] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:08:22] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-FP8'
[2026-01-25 06:08:22] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 06:08:22] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:08:22] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:08:22] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:08:22] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 06:08:26] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:08:26] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-FP8'
[2026-01-25 06:08:26] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 06:08:26] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:08:26] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:08:26] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:08:26] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:08:26] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:08:26] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-FP8'
[2026-01-25 06:08:26] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 06:08:26] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:08:26] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:08:26] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:08:26] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1087783) [2026-01-25 06:08:26] INFO SlideSparseLinearMethod_FP8.py:730: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1087783) [2026-01-25 06:08:26] INFO gemm_wrapper.py:872: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1087783) [2026-01-25 06:08:26] INFO SlideSparseLinearMethod_FP8.py:388: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1087783) [2026-01-25 06:08:26] INFO SlideSparseLinearMethod_FP8.py:535: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=1087783) [2026-01-25 06:08:26] INFO kernels.py:132: Loaded tuned kernel for base model: Llama3.2-1B-FP8 (from Llama3.2-1B-FP8-SlideSparse-2_4)
(EngineCore_DP0 pid=1087783) [2026-01-25 06:08:26] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8-SlideSparse-2_4
(EngineCore_DP0 pid=1087783) [2026-01-25 06:08:26] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8-SlideSparse-2_4
(EngineCore_DP0 pid=1087783) [2026-01-25 06:08:26] INFO SlideSparseLinearMethod_FP8.py:552: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8-SlideSparse-2_4
(EngineCore_DP0 pid=1087783) [2026-01-25 06:08:26] INFO SlideSparseLinearMethod_FP8.py:556: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=1087783) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1087783) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  7.17it/s]
(EngineCore_DP0 pid=1087783) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  7.16it/s]
(EngineCore_DP0 pid=1087783) 
(EngineCore_DP0 pid=1087783) [2026-01-25 06:08:27] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [3072, 2048] -> 1D uint8
(EngineCore_DP0 pid=1087783) [2026-01-25 06:08:27] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 3932160 bytes
(EngineCore_DP0 pid=1087783) [2026-01-25 06:08:27] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [2048, 2048] -> 1D uint8
(EngineCore_DP0 pid=1087783) [2026-01-25 06:08:27] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 2621440 bytes
(EngineCore_DP0 pid=1087783) [2026-01-25 06:08:27] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [16384, 2048] -> 1D uint8
(EngineCore_DP0 pid=1087783) [2026-01-25 06:08:27] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 20971520 bytes
(EngineCore_DP0 pid=1087783) [2026-01-25 06:08:27] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [2048, 8192] -> 1D uint8
(EngineCore_DP0 pid=1087783) [2026-01-25 06:08:27] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 10485760 bytes
(EngineCore_DP0 pid=1087783) 2026-01-25 06:08:33,956 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1087783) 2026-01-25 06:08:33,968 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1087783) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00, 34.22it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 34.55it/s]
(EngineCore_DP0 pid=1087783) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 1/5 [00:00<00:00,  6.99it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 21.50it/s]

Adding requests:   0%|          | 0/16 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 16/16 [00:00<00:00, 1354.50it/s]

Processed prompts:   0%|          | 0/16 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   6%|▋         | 1/16 [00:00<00:11,  1.28it/s, est. speed input: 20.41 toks/s, output: 326.49 toks/s]
Processed prompts: 100%|██████████| 16/16 [00:00<00:00,  1.28it/s, est. speed input: 325.11 toks/s, output: 5201.72 toks/s]
Processed prompts: 100%|██████████| 16/16 [00:00<00:00, 20.32it/s, est. speed input: 325.11 toks/s, output: 5201.72 toks/s]
[rank0]:[W125 06:08:36.134390298 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 19.1s

测试结果:
  Requests/s:   19.97
  Tokens/s:     5432.95
  Total Reqs:   16
  Elapsed:      0.80s

  [Decode 分析]
  Total Decode Tokens:  4096
  Decode Tokens/s:      5113.36

============================================================
[2/3] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-FP8                                 │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 2048 (= 128 x 16)
│   M_decode      = 128
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 128
│   --max-num-seqs           = 128
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
INFO 01-25 06:08:41 [datasets.py:612] Sampling input_len from [15, 15] and output_len from [256, 256]
INFO 01-25 06:08:41 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_4', 'max_model_len': 272, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 272, 'max_num_seqs': 128, 'disable_log_stats': True, 'enable_lora': None, 'enable_chunked_prefill': False, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_4'}
INFO 01-25 06:08:41 [model.py:514] Resolved architecture: LlamaForCausalLM
INFO 01-25 06:08:41 [model.py:1661] Using max model len 272
WARNING 01-25 06:08:41 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1088131) INFO 01-25 06:08:45 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_4', speculative_config=None, tokenizer='/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_4', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=272, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_4, enable_prefix_caching=True, enable_chunked_prefill=False, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [272], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 256, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=1088131) INFO 01-25 06:08:45 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:50223 backend=nccl
(EngineCore_DP0 pid=1088131) INFO 01-25 06:08:45 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=1088131) INFO 01-25 06:08:45 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_4...
(EngineCore_DP0 pid=1088131) INFO 01-25 06:08:46 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=1088131) INFO 01-25 06:08:46 [default_loader.py:308] Loading weights took 0.16 seconds
(EngineCore_DP0 pid=1088131) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1088131) INFO 01-25 06:08:46 [gpu_model_runner.py:3659] Model loading took 1.0755 GiB memory and 0.390010 seconds
(EngineCore_DP0 pid=1088131) WARNING 01-25 06:08:48 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=1088131) INFO 01-25 06:08:48 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/f706f86ea4/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=1088131) INFO 01-25 06:08:48 [backends.py:703] Dynamo bytecode transform time: 2.08 s
(EngineCore_DP0 pid=1088131) INFO 01-25 06:08:50 [backends.py:261] Cache the graph of compile range (1, 272) for later use
(EngineCore_DP0 pid=1088131) INFO 01-25 06:08:51 [backends.py:278] Compiling a graph for compile range (1, 272) takes 0.86 s
(EngineCore_DP0 pid=1088131) INFO 01-25 06:08:51 [monitor.py:34] torch.compile takes 2.94 s in total
(EngineCore_DP0 pid=1088131) INFO 01-25 06:08:51 [gpu_worker.py:375] Available KV cache memory: 10.55 GiB
(EngineCore_DP0 pid=1088131) INFO 01-25 06:08:51 [kv_cache_utils.py:1291] GPU KV cache size: 345,760 tokens
(EngineCore_DP0 pid=1088131) INFO 01-25 06:08:51 [kv_cache_utils.py:1296] Maximum concurrency for 272 tokens per request: 1271.18x
(EngineCore_DP0 pid=1088131) INFO 01-25 06:08:53 [gpu_model_runner.py:4587] Graph capturing finished in 2 secs, took 0.18 GiB
(EngineCore_DP0 pid=1088131) INFO 01-25 06:08:53 [core.py:259] init engine (profile, create kv cache, warmup model) took 7.08 seconds
INFO 01-25 06:08:54 [llm.py:360] Supported tasks: ['generate']
Throughput: 81.77 requests/s, 22241.19 total tokens/s, 20932.88 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768


─── STDERR ───
[2026-01-25 06:08:41] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:08:41] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-FP8'
[2026-01-25 06:08:41] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 06:08:41] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:08:41] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:08:41] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:08:41] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:08:41] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:08:41] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-FP8'
[2026-01-25 06:08:41] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 06:08:41] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:08:41] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:08:41] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:08:41] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 06:08:45] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:08:45] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-FP8'
[2026-01-25 06:08:45] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 06:08:45] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:08:45] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:08:45] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:08:45] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:08:45] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:08:45] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-FP8'
[2026-01-25 06:08:45] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 06:08:45] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:08:45] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:08:45] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:08:45] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1088131) [2026-01-25 06:08:46] INFO SlideSparseLinearMethod_FP8.py:730: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1088131) [2026-01-25 06:08:46] INFO gemm_wrapper.py:872: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1088131) [2026-01-25 06:08:46] INFO SlideSparseLinearMethod_FP8.py:388: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1088131) [2026-01-25 06:08:46] INFO SlideSparseLinearMethod_FP8.py:535: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=1088131) [2026-01-25 06:08:46] INFO kernels.py:132: Loaded tuned kernel for base model: Llama3.2-1B-FP8 (from Llama3.2-1B-FP8-SlideSparse-2_4)
(EngineCore_DP0 pid=1088131) [2026-01-25 06:08:46] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8-SlideSparse-2_4
(EngineCore_DP0 pid=1088131) [2026-01-25 06:08:46] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8-SlideSparse-2_4
(EngineCore_DP0 pid=1088131) [2026-01-25 06:08:46] INFO SlideSparseLinearMethod_FP8.py:552: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8-SlideSparse-2_4
(EngineCore_DP0 pid=1088131) [2026-01-25 06:08:46] INFO SlideSparseLinearMethod_FP8.py:556: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=1088131) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1088131) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  7.14it/s]
(EngineCore_DP0 pid=1088131) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  7.13it/s]
(EngineCore_DP0 pid=1088131) 
(EngineCore_DP0 pid=1088131) [2026-01-25 06:08:46] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [3072, 2048] -> 1D uint8
(EngineCore_DP0 pid=1088131) [2026-01-25 06:08:46] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 3932160 bytes
(EngineCore_DP0 pid=1088131) [2026-01-25 06:08:46] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [2048, 2048] -> 1D uint8
(EngineCore_DP0 pid=1088131) [2026-01-25 06:08:46] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 2621440 bytes
(EngineCore_DP0 pid=1088131) [2026-01-25 06:08:46] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [16384, 2048] -> 1D uint8
(EngineCore_DP0 pid=1088131) [2026-01-25 06:08:46] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 20971520 bytes
(EngineCore_DP0 pid=1088131) [2026-01-25 06:08:46] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [2048, 8192] -> 1D uint8
(EngineCore_DP0 pid=1088131) [2026-01-25 06:08:46] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 10485760 bytes
(EngineCore_DP0 pid=1088131) 2026-01-25 06:08:51,884 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1088131) 2026-01-25 06:08:51,897 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1088131) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▊         | 3/35 [00:00<00:01, 29.99it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 7/35 [00:00<00:00, 32.93it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 11/35 [00:00<00:00, 34.05it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 15/35 [00:00<00:00, 34.71it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  54%|█████▍    | 19/35 [00:00<00:00, 34.13it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  66%|██████▌   | 23/35 [00:00<00:00, 34.74it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  77%|███████▋  | 27/35 [00:00<00:00, 35.05it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▊ | 31/35 [00:00<00:00, 35.13it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:01<00:00, 35.07it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:01<00:00, 34.58it/s]
(EngineCore_DP0 pid=1088131) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   5%|▌         | 1/19 [00:00<00:02,  6.84it/s]
Capturing CUDA graphs (decode, FULL):  32%|███▏      | 6/19 [00:00<00:00, 25.22it/s]
Capturing CUDA graphs (decode, FULL):  58%|█████▊    | 11/19 [00:00<00:00, 32.15it/s]
Capturing CUDA graphs (decode, FULL):  84%|████████▍ | 16/19 [00:00<00:00, 35.64it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:00<00:00, 32.47it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 8040.72it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:01<03:10,  1.50s/it, est. speed input: 10.66 toks/s, output: 170.55 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00,  1.50s/it, est. speed input: 1322.39 toks/s, output: 21158.26 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 82.65it/s, est. speed input: 1322.39 toks/s, output: 21158.26 toks/s]
[rank0]:[W125 06:08:56.914763583 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 19.7s

测试结果:
  Requests/s:   81.77
  Tokens/s:     22241.19
  Total Reqs:   128
  Elapsed:      1.57s

  [Decode 分析]
  Total Decode Tokens:  32768
  Decode Tokens/s:      20932.88

============================================================
[3/3] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-FP8                                 │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 4096 (= 256 x 16)
│   M_decode      = 256
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 256
│   --max-num-seqs           = 256
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
INFO 01-25 06:09:01 [datasets.py:612] Sampling input_len from [15, 15] and output_len from [256, 256]
INFO 01-25 06:09:01 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_4', 'max_model_len': 272, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 272, 'max_num_seqs': 256, 'disable_log_stats': True, 'enable_lora': None, 'enable_chunked_prefill': False, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_4'}
INFO 01-25 06:09:01 [model.py:514] Resolved architecture: LlamaForCausalLM
INFO 01-25 06:09:01 [model.py:1661] Using max model len 272
WARNING 01-25 06:09:01 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1088483) INFO 01-25 06:09:05 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_4', speculative_config=None, tokenizer='/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_4', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=272, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_4, enable_prefix_caching=True, enable_chunked_prefill=False, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [272], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 272, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=1088483) INFO 01-25 06:09:05 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:38771 backend=nccl
(EngineCore_DP0 pid=1088483) INFO 01-25 06:09:05 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=1088483) INFO 01-25 06:09:05 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_4...
(EngineCore_DP0 pid=1088483) INFO 01-25 06:09:05 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=1088483) INFO 01-25 06:09:05 [default_loader.py:308] Loading weights took 0.15 seconds
(EngineCore_DP0 pid=1088483) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1088483) INFO 01-25 06:09:06 [gpu_model_runner.py:3659] Model loading took 1.0755 GiB memory and 0.396872 seconds
(EngineCore_DP0 pid=1088483) WARNING 01-25 06:09:08 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=1088483) INFO 01-25 06:09:08 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/bac84360ac/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=1088483) INFO 01-25 06:09:08 [backends.py:703] Dynamo bytecode transform time: 2.06 s
(EngineCore_DP0 pid=1088483) INFO 01-25 06:09:09 [backends.py:261] Cache the graph of compile range (1, 272) for later use
(EngineCore_DP0 pid=1088483) INFO 01-25 06:09:10 [backends.py:278] Compiling a graph for compile range (1, 272) takes 0.84 s
(EngineCore_DP0 pid=1088483) INFO 01-25 06:09:10 [monitor.py:34] torch.compile takes 2.91 s in total
(EngineCore_DP0 pid=1088483) INFO 01-25 06:09:11 [gpu_worker.py:375] Available KV cache memory: 9.97 GiB
(EngineCore_DP0 pid=1088483) INFO 01-25 06:09:11 [kv_cache_utils.py:1291] GPU KV cache size: 326,688 tokens
(EngineCore_DP0 pid=1088483) INFO 01-25 06:09:11 [kv_cache_utils.py:1296] Maximum concurrency for 272 tokens per request: 1201.06x
(EngineCore_DP0 pid=1088483) INFO 01-25 06:09:13 [gpu_model_runner.py:4587] Graph capturing finished in 3 secs, took 0.23 GiB
(EngineCore_DP0 pid=1088483) INFO 01-25 06:09:13 [core.py:259] init engine (profile, create kv cache, warmup model) took 7.60 seconds
INFO 01-25 06:09:14 [llm.py:360] Supported tasks: ['generate']
Throughput: 88.71 requests/s, 24129.34 total tokens/s, 22709.96 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536


─── STDERR ───
[2026-01-25 06:09:01] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:09:01] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-FP8'
[2026-01-25 06:09:01] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 06:09:01] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:09:01] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:09:01] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:09:01] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:09:01] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:09:01] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-FP8'
[2026-01-25 06:09:01] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 06:09:01] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:09:01] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:09:01] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:09:01] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 06:09:05] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:09:05] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-FP8'
[2026-01-25 06:09:05] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 06:09:05] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:09:05] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:09:05] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:09:05] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:09:05] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:09:05] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-FP8'
[2026-01-25 06:09:05] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 06:09:05] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:09:05] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:09:05] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:09:05] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1088483) [2026-01-25 06:09:05] INFO SlideSparseLinearMethod_FP8.py:730: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1088483) [2026-01-25 06:09:05] INFO gemm_wrapper.py:872: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1088483) [2026-01-25 06:09:05] INFO SlideSparseLinearMethod_FP8.py:388: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1088483) [2026-01-25 06:09:05] INFO SlideSparseLinearMethod_FP8.py:535: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=1088483) [2026-01-25 06:09:05] INFO kernels.py:132: Loaded tuned kernel for base model: Llama3.2-1B-FP8 (from Llama3.2-1B-FP8-SlideSparse-2_4)
(EngineCore_DP0 pid=1088483) [2026-01-25 06:09:05] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8-SlideSparse-2_4
(EngineCore_DP0 pid=1088483) [2026-01-25 06:09:05] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8-SlideSparse-2_4
(EngineCore_DP0 pid=1088483) [2026-01-25 06:09:05] INFO SlideSparseLinearMethod_FP8.py:552: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8-SlideSparse-2_4
(EngineCore_DP0 pid=1088483) [2026-01-25 06:09:05] INFO SlideSparseLinearMethod_FP8.py:556: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=1088483) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1088483) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  7.23it/s]
(EngineCore_DP0 pid=1088483) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  7.23it/s]
(EngineCore_DP0 pid=1088483) 
(EngineCore_DP0 pid=1088483) [2026-01-25 06:09:05] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [3072, 2048] -> 1D uint8
(EngineCore_DP0 pid=1088483) [2026-01-25 06:09:05] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 3932160 bytes
(EngineCore_DP0 pid=1088483) [2026-01-25 06:09:05] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [2048, 2048] -> 1D uint8
(EngineCore_DP0 pid=1088483) [2026-01-25 06:09:05] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 2621440 bytes
(EngineCore_DP0 pid=1088483) [2026-01-25 06:09:05] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [16384, 2048] -> 1D uint8
(EngineCore_DP0 pid=1088483) [2026-01-25 06:09:05] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 20971520 bytes
(EngineCore_DP0 pid=1088483) [2026-01-25 06:09:05] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [2048, 8192] -> 1D uint8
(EngineCore_DP0 pid=1088483) [2026-01-25 06:09:05] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 10485760 bytes
(EngineCore_DP0 pid=1088483) 2026-01-25 06:09:11,390 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1088483) 2026-01-25 06:09:11,404 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1088483) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/36 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 4/36 [00:00<00:00, 33.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 8/36 [00:00<00:00, 34.58it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 12/36 [00:00<00:00, 35.06it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  44%|████▍     | 16/36 [00:00<00:00, 32.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  56%|█████▌    | 20/36 [00:00<00:00, 26.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▍   | 23/36 [00:00<00:00, 22.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 27/36 [00:00<00:00, 25.99it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 31/36 [00:01<00:00, 28.66it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  97%|█████████▋| 35/36 [00:01<00:00, 31.13it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:01<00:00, 29.21it/s]
(EngineCore_DP0 pid=1088483) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   3%|▎         | 1/35 [00:00<00:04,  6.92it/s]
Capturing CUDA graphs (decode, FULL):  17%|█▋        | 6/35 [00:00<00:01, 25.66it/s]
Capturing CUDA graphs (decode, FULL):  31%|███▏      | 11/35 [00:00<00:00, 32.23it/s]
Capturing CUDA graphs (decode, FULL):  46%|████▌     | 16/35 [00:00<00:00, 35.66it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 21/35 [00:00<00:00, 37.87it/s]
Capturing CUDA graphs (decode, FULL):  74%|███████▍  | 26/35 [00:00<00:00, 39.34it/s]
Capturing CUDA graphs (decode, FULL):  89%|████████▊ | 31/35 [00:00<00:00, 40.44it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:00<00:00, 36.50it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 9208.37it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:02<11:06,  2.61s/it, est. speed input: 6.12 toks/s, output: 97.93 toks/s]
Processed prompts:  57%|█████▋    | 145/256 [00:02<00:01, 74.77it/s, est. speed input: 853.13 toks/s, output: 13650.03 toks/s]
Processed prompts:  95%|█████████▌| 244/256 [00:02<00:00, 137.54it/s, est. speed input: 1383.91 toks/s, output: 22142.54 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 137.54it/s, est. speed input: 1433.66 toks/s, output: 22938.56 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 89.60it/s, est. speed input: 1433.66 toks/s, output: 22938.56 toks/s] 
[rank0]:[W125 06:09:17.507652730 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 21.6s

测试结果:
  Requests/s:   88.71
  Tokens/s:     24129.34
  Total Reqs:   256
  Elapsed:      2.89s

  [Decode 分析]
  Total Decode Tokens:  65536
  Decode Tokens/s:      22709.96


------------------------------------------------------------
  生成 CSV: Llama3.2-1B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_4/Llama3.2-1B-FP8_decode.csv

预览:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
16,16,16,16,256,256,19.9741,5432.9459,0.8010
128,16,128,128,256,256,81.7691,22241.1868,1.5654
256,16,256,256,256,256,88.7108,24129.3367,2.8858

------------------------------------------------------------

[INFO] 完成: 3 成功, 0 失败

============================================================
  Llama3.2-1B-FP8 | cuSPARSELt (2_6) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6

============================================================
[1/3] 测试 M=16
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-FP8                                 │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16
│   M_prefill     = 16 (= 1 x 16)
│   M_decode      = 1
│   batched_tokens = 17 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 17
│   --max-num-batched-tokens = 17
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
INFO 01-25 06:09:22 [datasets.py:612] Sampling input_len from [15, 15] and output_len from [1, 1]
INFO 01-25 06:09:22 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6', 'max_model_len': 17, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 17, 'max_num_seqs': 1, 'disable_log_stats': True, 'enable_lora': None, 'enable_chunked_prefill': False, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6'}
INFO 01-25 06:09:22 [model.py:514] Resolved architecture: LlamaForCausalLM
INFO 01-25 06:09:22 [model.py:1661] Using max model len 17
WARNING 01-25 06:09:22 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1088891) INFO 01-25 06:09:26 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6', speculative_config=None, tokenizer='/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=17, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6, enable_prefix_caching=True, enable_chunked_prefill=False, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [17], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 2, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=1088891) INFO 01-25 06:09:26 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:53235 backend=nccl
(EngineCore_DP0 pid=1088891) INFO 01-25 06:09:26 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=1088891) INFO 01-25 06:09:27 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6...
(EngineCore_DP0 pid=1088891) INFO 01-25 06:09:27 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=1088891) INFO 01-25 06:09:28 [default_loader.py:308] Loading weights took 0.42 seconds
(EngineCore_DP0 pid=1088891) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1088891) INFO 01-25 06:09:28 [gpu_model_runner.py:3659] Model loading took 1.2745 GiB memory and 0.700317 seconds
(EngineCore_DP0 pid=1088891) WARNING 01-25 06:09:30 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=1088891) INFO 01-25 06:09:30 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/146363c2e4/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=1088891) INFO 01-25 06:09:30 [backends.py:703] Dynamo bytecode transform time: 2.07 s
(EngineCore_DP0 pid=1088891) INFO 01-25 06:09:32 [backends.py:261] Cache the graph of compile range (1, 17) for later use
(EngineCore_DP0 pid=1088891) INFO 01-25 06:09:34 [backends.py:278] Compiling a graph for compile range (1, 17) takes 2.13 s
(EngineCore_DP0 pid=1088891) INFO 01-25 06:09:34 [monitor.py:34] torch.compile takes 4.20 s in total
(EngineCore_DP0 pid=1088891) INFO 01-25 06:09:34 [gpu_worker.py:375] Available KV cache memory: 10.52 GiB
(EngineCore_DP0 pid=1088891) INFO 01-25 06:09:34 [kv_cache_utils.py:1291] GPU KV cache size: 344,544 tokens
(EngineCore_DP0 pid=1088891) INFO 01-25 06:09:34 [kv_cache_utils.py:1296] Maximum concurrency for 17 tokens per request: 10767.00x
(EngineCore_DP0 pid=1088891) INFO 01-25 06:09:35 [gpu_model_runner.py:4587] Graph capturing finished in 1 secs, took 0.01 GiB
(EngineCore_DP0 pid=1088891) INFO 01-25 06:09:35 [core.py:259] init engine (profile, create kv cache, warmup model) took 7.33 seconds
INFO 01-25 06:09:36 [llm.py:360] Supported tasks: ['generate']
Throughput: 65.67 requests/s, 1116.43 total tokens/s, 65.67 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  128


─── STDERR ───
[2026-01-25 06:09:22] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:09:22] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-FP8'
[2026-01-25 06:09:22] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 06:09:22] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:09:22] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:09:22] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:09:22] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:09:22] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:09:22] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-FP8'
[2026-01-25 06:09:22] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 06:09:22] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:09:22] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:09:22] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:09:22] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 06:09:26] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:09:26] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-FP8'
[2026-01-25 06:09:26] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 06:09:26] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:09:26] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:09:26] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:09:26] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:09:26] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:09:26] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-FP8'
[2026-01-25 06:09:26] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 06:09:26] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:09:26] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:09:26] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:09:26] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1088891) [2026-01-25 06:09:27] INFO SlideSparseLinearMethod_FP8.py:730: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1088891) [2026-01-25 06:09:27] INFO gemm_wrapper.py:872: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1088891) [2026-01-25 06:09:27] INFO SlideSparseLinearMethod_FP8.py:388: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1088891) [2026-01-25 06:09:27] INFO SlideSparseLinearMethod_FP8.py:535: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=1088891) [2026-01-25 06:09:27] INFO kernels.py:132: Loaded tuned kernel for base model: Llama3.2-1B-FP8 (from Llama3.2-1B-FP8-SlideSparse-2_6)
(EngineCore_DP0 pid=1088891) [2026-01-25 06:09:27] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8-SlideSparse-2_6
(EngineCore_DP0 pid=1088891) [2026-01-25 06:09:27] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8-SlideSparse-2_6
(EngineCore_DP0 pid=1088891) [2026-01-25 06:09:27] INFO SlideSparseLinearMethod_FP8.py:552: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8-SlideSparse-2_6
(EngineCore_DP0 pid=1088891) [2026-01-25 06:09:27] INFO SlideSparseLinearMethod_FP8.py:556: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=1088891) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1088891) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.60it/s]
(EngineCore_DP0 pid=1088891) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.60it/s]
(EngineCore_DP0 pid=1088891) 
(EngineCore_DP0 pid=1088891) [2026-01-25 06:09:28] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=1088891) [2026-01-25 06:09:28] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 5308416 bytes
(EngineCore_DP0 pid=1088891) [2026-01-25 06:09:28] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=1088891) [2026-01-25 06:09:28] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 3538944 bytes
(EngineCore_DP0 pid=1088891) [2026-01-25 06:09:28] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=1088891) [2026-01-25 06:09:28] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 28311552 bytes
(EngineCore_DP0 pid=1088891) [2026-01-25 06:09:28] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=1088891) [2026-01-25 06:09:28] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 14024704 bytes
(EngineCore_DP0 pid=1088891) 2026-01-25 06:09:34,813 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1088891) 2026-01-25 06:09:34,826 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1088891) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  8.87it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  6.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  6.60it/s]
(EngineCore_DP0 pid=1088891) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.06it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.06it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 5192.78it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:25,  5.01it/s, est. speed input: 80.11 toks/s, output: 5.01 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:03, 34.66it/s, est. speed input: 463.19 toks/s, output: 28.95 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:02, 49.58it/s, est. speed input: 648.59 toks/s, output: 40.54 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:00<00:01, 58.41it/s, est. speed input: 760.04 toks/s, output: 47.50 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:00<00:01, 63.76it/s, est. speed input: 833.11 toks/s, output: 52.07 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:00<00:01, 67.25it/s, est. speed input: 885.23 toks/s, output: 55.33 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:00<00:01, 69.18it/s, est. speed input: 922.11 toks/s, output: 57.63 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:00<00:01, 70.39it/s, est. speed input: 950.26 toks/s, output: 59.39 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:01<00:00, 71.48it/s, est. speed input: 973.83 toks/s, output: 60.86 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:01<00:00, 72.49it/s, est. speed input: 994.14 toks/s, output: 62.13 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:01<00:00, 72.90it/s, est. speed input: 1009.96 toks/s, output: 63.12 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:01<00:00, 72.09it/s, est. speed input: 1019.33 toks/s, output: 63.71 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:01<00:00, 72.82it/s, est. speed input: 1031.70 toks/s, output: 64.48 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:01<00:00, 73.65it/s, est. speed input: 1043.42 toks/s, output: 65.21 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:01<00:00, 73.86it/s, est. speed input: 1052.59 toks/s, output: 65.79 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:01<00:00, 73.51it/s, est. speed input: 1059.24 toks/s, output: 66.20 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 73.51it/s, est. speed input: 1064.73 toks/s, output: 66.55 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 66.54it/s, est. speed input: 1064.73 toks/s, output: 66.55 toks/s]
[rank0]:[W125 06:09:38.158096762 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 20.7s

测试结果:
  Requests/s:   65.67
  Tokens/s:     1116.43
  Total Reqs:   128
  Elapsed:      1.95s

  [Prefill 分析]
  Total Prefill Tokens: 2048
  Prefill Tokens/s:     1050.76

============================================================
[2/3] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-FP8                                 │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 128 (= 1 x 128)
│   M_decode      = 1
│   batched_tokens = 129 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 128
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 129
│   --max-num-batched-tokens = 129
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
INFO 01-25 06:09:43 [datasets.py:612] Sampling input_len from [127, 127] and output_len from [1, 1]
INFO 01-25 06:09:43 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6', 'max_model_len': 129, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 129, 'max_num_seqs': 1, 'disable_log_stats': True, 'enable_lora': None, 'enable_chunked_prefill': False, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6'}
INFO 01-25 06:09:43 [model.py:514] Resolved architecture: LlamaForCausalLM
INFO 01-25 06:09:43 [model.py:1661] Using max model len 129
WARNING 01-25 06:09:43 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1089303) INFO 01-25 06:09:47 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6', speculative_config=None, tokenizer='/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=129, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6, enable_prefix_caching=True, enable_chunked_prefill=False, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [129], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 2, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=1089303) INFO 01-25 06:09:47 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:47305 backend=nccl
(EngineCore_DP0 pid=1089303) INFO 01-25 06:09:47 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=1089303) INFO 01-25 06:09:47 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6...
(EngineCore_DP0 pid=1089303) INFO 01-25 06:09:48 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=1089303) INFO 01-25 06:09:48 [default_loader.py:308] Loading weights took 0.22 seconds
(EngineCore_DP0 pid=1089303) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1089303) INFO 01-25 06:09:48 [gpu_model_runner.py:3659] Model loading took 1.2745 GiB memory and 0.512398 seconds
(EngineCore_DP0 pid=1089303) WARNING 01-25 06:09:50 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=1089303) INFO 01-25 06:09:50 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/25cffd08cd/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=1089303) INFO 01-25 06:09:50 [backends.py:703] Dynamo bytecode transform time: 2.06 s
(EngineCore_DP0 pid=1089303) INFO 01-25 06:09:53 [backends.py:261] Cache the graph of compile range (1, 129) for later use
(EngineCore_DP0 pid=1089303) INFO 01-25 06:09:54 [backends.py:278] Compiling a graph for compile range (1, 129) takes 2.16 s
(EngineCore_DP0 pid=1089303) INFO 01-25 06:09:54 [monitor.py:34] torch.compile takes 4.21 s in total
(EngineCore_DP0 pid=1089303) INFO 01-25 06:09:55 [gpu_worker.py:375] Available KV cache memory: 10.51 GiB
(EngineCore_DP0 pid=1089303) INFO 01-25 06:09:55 [kv_cache_utils.py:1291] GPU KV cache size: 344,528 tokens
(EngineCore_DP0 pid=1089303) INFO 01-25 06:09:55 [kv_cache_utils.py:1296] Maximum concurrency for 129 tokens per request: 2392.56x
(EngineCore_DP0 pid=1089303) INFO 01-25 06:09:55 [gpu_model_runner.py:4587] Graph capturing finished in 1 secs, took 0.01 GiB
(EngineCore_DP0 pid=1089303) INFO 01-25 06:09:55 [core.py:259] init engine (profile, create kv cache, warmup model) took 7.05 seconds
INFO 01-25 06:09:56 [llm.py:360] Supported tasks: ['generate']
Throughput: 67.15 requests/s, 8662.20 total tokens/s, 67.15 output tokens/s
Total num prompt tokens:  16384
Total num output tokens:  128


─── STDERR ───
[2026-01-25 06:09:43] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:09:43] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-FP8'
[2026-01-25 06:09:43] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 06:09:43] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:09:43] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:09:43] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:09:43] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:09:43] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:09:43] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-FP8'
[2026-01-25 06:09:43] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 06:09:43] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:09:43] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:09:43] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:09:43] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 06:09:47] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:09:47] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-FP8'
[2026-01-25 06:09:47] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 06:09:47] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:09:47] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:09:47] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:09:47] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:09:47] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:09:47] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-FP8'
[2026-01-25 06:09:47] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 06:09:47] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:09:47] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:09:47] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:09:47] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1089303) [2026-01-25 06:09:48] INFO SlideSparseLinearMethod_FP8.py:730: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1089303) [2026-01-25 06:09:48] INFO gemm_wrapper.py:872: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1089303) [2026-01-25 06:09:48] INFO SlideSparseLinearMethod_FP8.py:388: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1089303) [2026-01-25 06:09:48] INFO SlideSparseLinearMethod_FP8.py:535: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=1089303) [2026-01-25 06:09:48] INFO kernels.py:132: Loaded tuned kernel for base model: Llama3.2-1B-FP8 (from Llama3.2-1B-FP8-SlideSparse-2_6)
(EngineCore_DP0 pid=1089303) [2026-01-25 06:09:48] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8-SlideSparse-2_6
(EngineCore_DP0 pid=1089303) [2026-01-25 06:09:48] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8-SlideSparse-2_6
(EngineCore_DP0 pid=1089303) [2026-01-25 06:09:48] INFO SlideSparseLinearMethod_FP8.py:552: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8-SlideSparse-2_6
(EngineCore_DP0 pid=1089303) [2026-01-25 06:09:48] INFO SlideSparseLinearMethod_FP8.py:556: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=1089303) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1089303) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.59it/s]
(EngineCore_DP0 pid=1089303) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.59it/s]
(EngineCore_DP0 pid=1089303) 
(EngineCore_DP0 pid=1089303) [2026-01-25 06:09:48] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=1089303) [2026-01-25 06:09:48] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 5308416 bytes
(EngineCore_DP0 pid=1089303) [2026-01-25 06:09:48] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=1089303) [2026-01-25 06:09:48] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 3538944 bytes
(EngineCore_DP0 pid=1089303) [2026-01-25 06:09:48] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=1089303) [2026-01-25 06:09:48] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 28311552 bytes
(EngineCore_DP0 pid=1089303) [2026-01-25 06:09:48] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=1089303) [2026-01-25 06:09:48] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 14024704 bytes
(EngineCore_DP0 pid=1089303) 2026-01-25 06:09:55,229 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1089303) 2026-01-25 06:09:55,242 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1089303) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 30.02it/s]
(EngineCore_DP0 pid=1089303) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.07it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.06it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 2401.67it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:13,  9.30it/s, est. speed input: 1190.51 toks/s, output: 9.30 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:02, 47.02it/s, est. speed input: 5302.14 toks/s, output: 41.42 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:01, 58.98it/s, est. speed input: 6661.80 toks/s, output: 52.04 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:00<00:01, 64.29it/s, est. speed input: 7318.19 toks/s, output: 57.17 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:00<00:01, 67.51it/s, est. speed input: 7729.31 toks/s, output: 60.38 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:00<00:01, 69.64it/s, est. speed input: 8014.17 toks/s, output: 62.61 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:00<00:01, 71.04it/s, est. speed input: 8219.36 toks/s, output: 64.21 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:00<00:00, 71.96it/s, est. speed input: 8374.08 toks/s, output: 65.42 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:00<00:00, 71.98it/s, est. speed input: 8469.67 toks/s, output: 66.17 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:01<00:00, 72.32it/s, est. speed input: 8558.46 toks/s, output: 66.86 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:01<00:00, 71.87it/s, est. speed input: 8606.69 toks/s, output: 67.24 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:01<00:00, 72.03it/s, est. speed input: 8661.98 toks/s, output: 67.67 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:01<00:00, 72.33it/s, est. speed input: 8714.75 toks/s, output: 68.08 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:01<00:00, 72.80it/s, est. speed input: 8767.30 toks/s, output: 68.49 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:01<00:00, 72.56it/s, est. speed input: 8797.74 toks/s, output: 68.73 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:01<00:00, 72.46it/s, est. speed input: 8825.99 toks/s, output: 68.95 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 72.46it/s, est. speed input: 8850.89 toks/s, output: 69.15 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 69.14it/s, est. speed input: 8850.89 toks/s, output: 69.15 toks/s]
[rank0]:[W125 06:09:58.274910382 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 20.1s

测试结果:
  Requests/s:   67.15
  Tokens/s:     8662.20
  Total Reqs:   128
  Elapsed:      1.91s

  [Prefill 分析]
  Total Prefill Tokens: 16384
  Prefill Tokens/s:     8595.05

============================================================
[3/3] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-FP8                                 │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 256 (= 1 x 256)
│   M_decode      = 1
│   batched_tokens = 257 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 256
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 257
│   --max-num-batched-tokens = 257
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
INFO 01-25 06:10:03 [datasets.py:612] Sampling input_len from [255, 255] and output_len from [1, 1]
INFO 01-25 06:10:03 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6', 'max_model_len': 257, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 257, 'max_num_seqs': 1, 'disable_log_stats': True, 'enable_lora': None, 'enable_chunked_prefill': False, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6'}
INFO 01-25 06:10:03 [model.py:514] Resolved architecture: LlamaForCausalLM
INFO 01-25 06:10:03 [model.py:1661] Using max model len 257
WARNING 01-25 06:10:03 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1089676) INFO 01-25 06:10:07 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6', speculative_config=None, tokenizer='/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=257, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6, enable_prefix_caching=True, enable_chunked_prefill=False, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [257], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 2, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=1089676) INFO 01-25 06:10:07 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:60997 backend=nccl
(EngineCore_DP0 pid=1089676) INFO 01-25 06:10:07 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=1089676) INFO 01-25 06:10:07 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6...
(EngineCore_DP0 pid=1089676) INFO 01-25 06:10:08 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=1089676) INFO 01-25 06:10:08 [default_loader.py:308] Loading weights took 0.22 seconds
(EngineCore_DP0 pid=1089676) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1089676) INFO 01-25 06:10:08 [gpu_model_runner.py:3659] Model loading took 1.2750 GiB memory and 0.452021 seconds
(EngineCore_DP0 pid=1089676) WARNING 01-25 06:10:11 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=1089676) INFO 01-25 06:10:11 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/7ae45fe82a/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=1089676) INFO 01-25 06:10:11 [backends.py:703] Dynamo bytecode transform time: 2.07 s
(EngineCore_DP0 pid=1089676) INFO 01-25 06:10:13 [backends.py:261] Cache the graph of compile range (1, 257) for later use
(EngineCore_DP0 pid=1089676) INFO 01-25 06:10:14 [backends.py:278] Compiling a graph for compile range (1, 257) takes 1.99 s
(EngineCore_DP0 pid=1089676) INFO 01-25 06:10:14 [monitor.py:34] torch.compile takes 4.06 s in total
(EngineCore_DP0 pid=1089676) INFO 01-25 06:10:14 [gpu_worker.py:375] Available KV cache memory: 10.51 GiB
(EngineCore_DP0 pid=1089676) INFO 01-25 06:10:15 [kv_cache_utils.py:1291] GPU KV cache size: 344,464 tokens
(EngineCore_DP0 pid=1089676) INFO 01-25 06:10:15 [kv_cache_utils.py:1296] Maximum concurrency for 257 tokens per request: 1266.41x
(EngineCore_DP0 pid=1089676) INFO 01-25 06:10:15 [gpu_model_runner.py:4587] Graph capturing finished in 1 secs, took 0.01 GiB
(EngineCore_DP0 pid=1089676) INFO 01-25 06:10:15 [core.py:259] init engine (profile, create kv cache, warmup model) took 6.99 seconds
INFO 01-25 06:10:16 [llm.py:360] Supported tasks: ['generate']
Throughput: 64.60 requests/s, 16602.15 total tokens/s, 64.60 output tokens/s
Total num prompt tokens:  32768
Total num output tokens:  128


─── STDERR ───
[2026-01-25 06:10:03] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:10:03] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-FP8'
[2026-01-25 06:10:03] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 06:10:03] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:10:03] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:10:03] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:10:03] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:10:03] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:10:03] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-FP8'
[2026-01-25 06:10:03] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 06:10:03] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:10:03] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:10:03] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:10:03] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 06:10:07] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:10:07] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-FP8'
[2026-01-25 06:10:07] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 06:10:07] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:10:07] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:10:07] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:10:07] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:10:07] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:10:07] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-FP8'
[2026-01-25 06:10:07] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 06:10:07] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:10:07] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:10:07] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:10:07] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1089676) [2026-01-25 06:10:08] INFO SlideSparseLinearMethod_FP8.py:730: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1089676) [2026-01-25 06:10:08] INFO gemm_wrapper.py:872: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1089676) [2026-01-25 06:10:08] INFO SlideSparseLinearMethod_FP8.py:388: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1089676) [2026-01-25 06:10:08] INFO SlideSparseLinearMethod_FP8.py:535: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=1089676) [2026-01-25 06:10:08] INFO kernels.py:132: Loaded tuned kernel for base model: Llama3.2-1B-FP8 (from Llama3.2-1B-FP8-SlideSparse-2_6)
(EngineCore_DP0 pid=1089676) [2026-01-25 06:10:08] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8-SlideSparse-2_6
(EngineCore_DP0 pid=1089676) [2026-01-25 06:10:08] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8-SlideSparse-2_6
(EngineCore_DP0 pid=1089676) [2026-01-25 06:10:08] INFO SlideSparseLinearMethod_FP8.py:552: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8-SlideSparse-2_6
(EngineCore_DP0 pid=1089676) [2026-01-25 06:10:08] INFO SlideSparseLinearMethod_FP8.py:556: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=1089676) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1089676) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.57it/s]
(EngineCore_DP0 pid=1089676) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.57it/s]
(EngineCore_DP0 pid=1089676) 
(EngineCore_DP0 pid=1089676) [2026-01-25 06:10:08] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=1089676) [2026-01-25 06:10:08] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 5308416 bytes
(EngineCore_DP0 pid=1089676) [2026-01-25 06:10:08] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=1089676) [2026-01-25 06:10:08] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 3538944 bytes
(EngineCore_DP0 pid=1089676) [2026-01-25 06:10:08] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=1089676) [2026-01-25 06:10:08] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 28311552 bytes
(EngineCore_DP0 pid=1089676) [2026-01-25 06:10:08] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=1089676) [2026-01-25 06:10:08] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 14024704 bytes
(EngineCore_DP0 pid=1089676) 2026-01-25 06:10:15,103 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1089676) 2026-01-25 06:10:15,117 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1089676) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 14.35it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 14.34it/s]
(EngineCore_DP0 pid=1089676) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.89it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.88it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 1718.75it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:20,  6.32it/s, est. speed input: 1617.91 toks/s, output: 6.32 toks/s]
Processed prompts:   6%|▋         | 8/128 [00:00<00:03, 34.70it/s, est. speed input: 7602.65 toks/s, output: 29.70 toks/s]
Processed prompts:  12%|█▎        | 16/128 [00:00<00:02, 50.57it/s, est. speed input: 10813.18 toks/s, output: 42.24 toks/s]
Processed prompts:  19%|█▉        | 24/128 [00:00<00:01, 59.03it/s, est. speed input: 12596.58 toks/s, output: 49.20 toks/s]
Processed prompts:  25%|██▌       | 32/128 [00:00<00:01, 64.26it/s, est. speed input: 13756.57 toks/s, output: 53.74 toks/s]
Processed prompts:  31%|███▏      | 40/128 [00:00<00:01, 67.05it/s, est. speed input: 14510.73 toks/s, output: 56.68 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:00<00:01, 69.01it/s, est. speed input: 15076.39 toks/s, output: 58.89 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:00<00:01, 70.66it/s, est. speed input: 15536.73 toks/s, output: 60.69 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:01<00:00, 71.52it/s, est. speed input: 15881.43 toks/s, output: 62.04 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:01<00:00, 72.52it/s, est. speed input: 16188.90 toks/s, output: 63.24 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:01<00:00, 72.94it/s, est. speed input: 16425.90 toks/s, output: 64.16 toks/s]
Processed prompts:  69%|██████▉   | 88/128 [00:01<00:00, 72.95it/s, est. speed input: 16608.17 toks/s, output: 64.88 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:01<00:00, 72.84it/s, est. speed input: 16756.43 toks/s, output: 65.45 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:01<00:00, 73.12it/s, est. speed input: 16903.22 toks/s, output: 66.03 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:01<00:00, 73.32it/s, est. speed input: 17031.17 toks/s, output: 66.53 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:01<00:00, 72.97it/s, est. speed input: 17120.13 toks/s, output: 66.88 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 72.77it/s, est. speed input: 17200.95 toks/s, output: 67.19 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 72.77it/s, est. speed input: 17200.95 toks/s, output: 67.19 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 67.18it/s, est. speed input: 17200.95 toks/s, output: 67.19 toks/s]
[rank0]:[W125 06:10:18.331015212 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 20.1s

测试结果:
  Requests/s:   64.60
  Tokens/s:     16602.15
  Total Reqs:   128
  Elapsed:      1.98s

  [Prefill 分析]
  Total Prefill Tokens: 32768
  Prefill Tokens/s:     16537.55


------------------------------------------------------------
  生成 CSV: Llama3.2-1B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/Llama3.2-1B-FP8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
16,16,1,128,128,65.6726,1116.4342,1.9491
128,128,1,128,128,67.1488,8662.2008,1.9062
256,256,1,128,128,64.5998,16602.1451,1.9814

------------------------------------------------------------

[INFO] 完成: 3 成功, 0 失败

============================================================
  Llama3.2-1B-FP8 | cuSPARSELt (2_6) | decode
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6

============================================================
[1/3] 测试 M=16
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-FP8                                 │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16
│   M_prefill     = 256 (= 16 x 16)
│   M_decode      = 16
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 16
│   --max-num-seqs           = 16
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
INFO 01-25 06:10:23 [datasets.py:612] Sampling input_len from [15, 15] and output_len from [256, 256]
INFO 01-25 06:10:23 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6', 'max_model_len': 272, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 272, 'max_num_seqs': 16, 'disable_log_stats': True, 'enable_lora': None, 'enable_chunked_prefill': False, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6'}
INFO 01-25 06:10:23 [model.py:514] Resolved architecture: LlamaForCausalLM
INFO 01-25 06:10:23 [model.py:1661] Using max model len 272
WARNING 01-25 06:10:23 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1090045) INFO 01-25 06:10:27 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6', speculative_config=None, tokenizer='/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=272, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6, enable_prefix_caching=True, enable_chunked_prefill=False, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [272], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 32, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=1090045) INFO 01-25 06:10:27 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:51941 backend=nccl
(EngineCore_DP0 pid=1090045) INFO 01-25 06:10:27 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=1090045) INFO 01-25 06:10:27 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6...
(EngineCore_DP0 pid=1090045) INFO 01-25 06:10:28 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=1090045) INFO 01-25 06:10:28 [default_loader.py:308] Loading weights took 0.22 seconds
(EngineCore_DP0 pid=1090045) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1090045) INFO 01-25 06:10:28 [gpu_model_runner.py:3659] Model loading took 1.2749 GiB memory and 0.468183 seconds
(EngineCore_DP0 pid=1090045) WARNING 01-25 06:10:30 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=1090045) INFO 01-25 06:10:30 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/44a3b0472a/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=1090045) INFO 01-25 06:10:30 [backends.py:703] Dynamo bytecode transform time: 2.06 s
(EngineCore_DP0 pid=1090045) INFO 01-25 06:10:33 [backends.py:261] Cache the graph of compile range (1, 272) for later use
(EngineCore_DP0 pid=1090045) INFO 01-25 06:10:34 [backends.py:278] Compiling a graph for compile range (1, 272) takes 2.02 s
(EngineCore_DP0 pid=1090045) INFO 01-25 06:10:34 [monitor.py:34] torch.compile takes 4.08 s in total
(EngineCore_DP0 pid=1090045) INFO 01-25 06:10:34 [gpu_worker.py:375] Available KV cache memory: 10.51 GiB
(EngineCore_DP0 pid=1090045) INFO 01-25 06:10:35 [kv_cache_utils.py:1291] GPU KV cache size: 344,464 tokens
(EngineCore_DP0 pid=1090045) INFO 01-25 06:10:35 [kv_cache_utils.py:1296] Maximum concurrency for 272 tokens per request: 1266.41x
(EngineCore_DP0 pid=1090045) INFO 01-25 06:10:35 [gpu_model_runner.py:4587] Graph capturing finished in 1 secs, took 0.04 GiB
(EngineCore_DP0 pid=1090045) INFO 01-25 06:10:35 [core.py:259] init engine (profile, create kv cache, warmup model) took 7.17 seconds
INFO 01-25 06:10:36 [llm.py:360] Supported tasks: ['generate']
Throughput: 17.78 requests/s, 4836.72 total tokens/s, 4552.21 output tokens/s
Total num prompt tokens:  256
Total num output tokens:  4096


─── STDERR ───
[2026-01-25 06:10:23] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:10:23] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-FP8'
[2026-01-25 06:10:23] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 06:10:23] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:10:23] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:10:23] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:10:23] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:10:23] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:10:23] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-FP8'
[2026-01-25 06:10:23] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 06:10:23] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:10:23] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:10:23] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:10:23] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 06:10:27] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:10:27] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-FP8'
[2026-01-25 06:10:27] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 06:10:27] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:10:27] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:10:27] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:10:27] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:10:27] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:10:27] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-FP8'
[2026-01-25 06:10:27] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 06:10:27] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:10:27] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:10:27] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:10:27] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1090045) [2026-01-25 06:10:28] INFO SlideSparseLinearMethod_FP8.py:730: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1090045) [2026-01-25 06:10:28] INFO gemm_wrapper.py:872: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1090045) [2026-01-25 06:10:28] INFO SlideSparseLinearMethod_FP8.py:388: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1090045) [2026-01-25 06:10:28] INFO SlideSparseLinearMethod_FP8.py:535: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=1090045) [2026-01-25 06:10:28] INFO kernels.py:132: Loaded tuned kernel for base model: Llama3.2-1B-FP8 (from Llama3.2-1B-FP8-SlideSparse-2_6)
(EngineCore_DP0 pid=1090045) [2026-01-25 06:10:28] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8-SlideSparse-2_6
(EngineCore_DP0 pid=1090045) [2026-01-25 06:10:28] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8-SlideSparse-2_6
(EngineCore_DP0 pid=1090045) [2026-01-25 06:10:28] INFO SlideSparseLinearMethod_FP8.py:552: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8-SlideSparse-2_6
(EngineCore_DP0 pid=1090045) [2026-01-25 06:10:28] INFO SlideSparseLinearMethod_FP8.py:556: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=1090045) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1090045) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.35it/s]
(EngineCore_DP0 pid=1090045) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.34it/s]
(EngineCore_DP0 pid=1090045) 
(EngineCore_DP0 pid=1090045) [2026-01-25 06:10:28] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=1090045) [2026-01-25 06:10:28] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 5308416 bytes
(EngineCore_DP0 pid=1090045) [2026-01-25 06:10:28] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=1090045) [2026-01-25 06:10:28] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 3538944 bytes
(EngineCore_DP0 pid=1090045) [2026-01-25 06:10:28] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=1090045) [2026-01-25 06:10:28] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 28311552 bytes
(EngineCore_DP0 pid=1090045) [2026-01-25 06:10:28] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=1090045) [2026-01-25 06:10:28] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 14024704 bytes
(EngineCore_DP0 pid=1090045) 2026-01-25 06:10:35,114 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1090045) 2026-01-25 06:10:35,127 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1090045) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00, 33.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 34.06it/s]
(EngineCore_DP0 pid=1090045) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 1/5 [00:00<00:00,  7.08it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 21.28it/s]

Adding requests:   0%|          | 0/16 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 16/16 [00:00<00:00, 1276.03it/s]

Processed prompts:   0%|          | 0/16 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   6%|▋         | 1/16 [00:00<00:13,  1.13it/s, est. speed input: 18.15 toks/s, output: 290.43 toks/s]
Processed prompts: 100%|██████████| 16/16 [00:00<00:00,  1.13it/s, est. speed input: 289.27 toks/s, output: 4628.25 toks/s]
Processed prompts: 100%|██████████| 16/16 [00:00<00:00, 18.08it/s, est. speed input: 289.27 toks/s, output: 4628.25 toks/s]
[rank0]:[W125 06:10:37.388748702 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 19.2s

测试结果:
  Requests/s:   17.78
  Tokens/s:     4836.72
  Total Reqs:   16
  Elapsed:      0.90s

  [Decode 分析]
  Total Decode Tokens:  4096
  Decode Tokens/s:      4552.21

============================================================
[2/3] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-FP8                                 │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 2048 (= 128 x 16)
│   M_decode      = 128
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 128
│   --max-num-seqs           = 128
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
INFO 01-25 06:10:42 [datasets.py:612] Sampling input_len from [15, 15] and output_len from [256, 256]
INFO 01-25 06:10:42 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6', 'max_model_len': 272, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 272, 'max_num_seqs': 128, 'disable_log_stats': True, 'enable_lora': None, 'enable_chunked_prefill': False, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6'}
INFO 01-25 06:10:42 [model.py:514] Resolved architecture: LlamaForCausalLM
INFO 01-25 06:10:42 [model.py:1661] Using max model len 272
WARNING 01-25 06:10:42 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1090393) INFO 01-25 06:10:46 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6', speculative_config=None, tokenizer='/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=272, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6, enable_prefix_caching=True, enable_chunked_prefill=False, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [272], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 256, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=1090393) INFO 01-25 06:10:46 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:54773 backend=nccl
(EngineCore_DP0 pid=1090393) INFO 01-25 06:10:46 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=1090393) INFO 01-25 06:10:47 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6...
(EngineCore_DP0 pid=1090393) INFO 01-25 06:10:47 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=1090393) INFO 01-25 06:10:47 [default_loader.py:308] Loading weights took 0.21 seconds
(EngineCore_DP0 pid=1090393) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1090393) INFO 01-25 06:10:48 [gpu_model_runner.py:3659] Model loading took 1.2749 GiB memory and 0.444296 seconds
(EngineCore_DP0 pid=1090393) WARNING 01-25 06:10:50 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=1090393) INFO 01-25 06:10:50 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/ce138505f3/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=1090393) INFO 01-25 06:10:50 [backends.py:703] Dynamo bytecode transform time: 2.07 s
(EngineCore_DP0 pid=1090393) INFO 01-25 06:10:51 [backends.py:261] Cache the graph of compile range (1, 272) for later use
(EngineCore_DP0 pid=1090393) INFO 01-25 06:10:52 [backends.py:278] Compiling a graph for compile range (1, 272) takes 0.83 s
(EngineCore_DP0 pid=1090393) INFO 01-25 06:10:52 [monitor.py:34] torch.compile takes 2.90 s in total
(EngineCore_DP0 pid=1090393) INFO 01-25 06:10:52 [gpu_worker.py:375] Available KV cache memory: 10.35 GiB
(EngineCore_DP0 pid=1090393) INFO 01-25 06:10:53 [kv_cache_utils.py:1291] GPU KV cache size: 339,152 tokens
(EngineCore_DP0 pid=1090393) INFO 01-25 06:10:53 [kv_cache_utils.py:1296] Maximum concurrency for 272 tokens per request: 1246.88x
(EngineCore_DP0 pid=1090393) INFO 01-25 06:10:55 [gpu_model_runner.py:4587] Graph capturing finished in 2 secs, took 0.18 GiB
(EngineCore_DP0 pid=1090393) INFO 01-25 06:10:55 [core.py:259] init engine (profile, create kv cache, warmup model) took 7.11 seconds
INFO 01-25 06:10:55 [llm.py:360] Supported tasks: ['generate']
Throughput: 76.60 requests/s, 20836.26 total tokens/s, 19610.59 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768


─── STDERR ───
[2026-01-25 06:10:42] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:10:42] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-FP8'
[2026-01-25 06:10:42] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 06:10:42] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:10:42] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:10:42] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:10:42] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:10:42] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:10:42] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-FP8'
[2026-01-25 06:10:42] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 06:10:42] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:10:42] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:10:42] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:10:42] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 06:10:46] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:10:46] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-FP8'
[2026-01-25 06:10:46] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 06:10:46] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:10:46] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:10:46] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:10:46] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:10:46] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:10:46] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-FP8'
[2026-01-25 06:10:46] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 06:10:46] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:10:46] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:10:46] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:10:46] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1090393) [2026-01-25 06:10:47] INFO SlideSparseLinearMethod_FP8.py:730: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1090393) [2026-01-25 06:10:47] INFO gemm_wrapper.py:872: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1090393) [2026-01-25 06:10:47] INFO SlideSparseLinearMethod_FP8.py:388: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1090393) [2026-01-25 06:10:47] INFO SlideSparseLinearMethod_FP8.py:535: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=1090393) [2026-01-25 06:10:47] INFO kernels.py:132: Loaded tuned kernel for base model: Llama3.2-1B-FP8 (from Llama3.2-1B-FP8-SlideSparse-2_6)
(EngineCore_DP0 pid=1090393) [2026-01-25 06:10:47] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8-SlideSparse-2_6
(EngineCore_DP0 pid=1090393) [2026-01-25 06:10:47] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8-SlideSparse-2_6
(EngineCore_DP0 pid=1090393) [2026-01-25 06:10:47] INFO SlideSparseLinearMethod_FP8.py:552: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8-SlideSparse-2_6
(EngineCore_DP0 pid=1090393) [2026-01-25 06:10:47] INFO SlideSparseLinearMethod_FP8.py:556: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=1090393) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1090393) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.67it/s]
(EngineCore_DP0 pid=1090393) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.66it/s]
(EngineCore_DP0 pid=1090393) 
(EngineCore_DP0 pid=1090393) [2026-01-25 06:10:47] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=1090393) [2026-01-25 06:10:47] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 5308416 bytes
(EngineCore_DP0 pid=1090393) [2026-01-25 06:10:47] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=1090393) [2026-01-25 06:10:47] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 3538944 bytes
(EngineCore_DP0 pid=1090393) [2026-01-25 06:10:47] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=1090393) [2026-01-25 06:10:47] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 28311552 bytes
(EngineCore_DP0 pid=1090393) [2026-01-25 06:10:47] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=1090393) [2026-01-25 06:10:47] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 14024704 bytes
(EngineCore_DP0 pid=1090393) 2026-01-25 06:10:53,003 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1090393) 2026-01-25 06:10:53,017 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1090393) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█▏        | 4/35 [00:00<00:00, 33.73it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  23%|██▎       | 8/35 [00:00<00:00, 35.14it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  34%|███▍      | 12/35 [00:00<00:00, 34.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  46%|████▌     | 16/35 [00:00<00:00, 34.65it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 20/35 [00:00<00:00, 33.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 24/35 [00:00<00:00, 34.41it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 28/35 [00:00<00:00, 35.01it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████▏| 32/35 [00:00<00:00, 31.67it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:01<00:00, 32.11it/s]
(EngineCore_DP0 pid=1090393) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   5%|▌         | 1/19 [00:00<00:03,  5.04it/s]
Capturing CUDA graphs (decode, FULL):  32%|███▏      | 6/19 [00:00<00:00, 21.83it/s]
Capturing CUDA graphs (decode, FULL):  58%|█████▊    | 11/19 [00:00<00:00, 29.50it/s]
Capturing CUDA graphs (decode, FULL):  79%|███████▉  | 15/19 [00:00<00:00, 32.58it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:00<00:00, 29.53it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 4416.00it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:01<03:21,  1.59s/it, est. speed input: 10.07 toks/s, output: 161.13 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00,  1.59s/it, est. speed input: 1248.67 toks/s, output: 19978.65 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 78.04it/s, est. speed input: 1248.67 toks/s, output: 19978.65 toks/s]
[rank0]:[W125 06:10:57.272364663 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 19.7s

测试结果:
  Requests/s:   76.60
  Tokens/s:     20836.26
  Total Reqs:   128
  Elapsed:      1.67s

  [Decode 分析]
  Total Decode Tokens:  32768
  Decode Tokens/s:      19610.59

============================================================
[3/3] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-FP8                                 │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 4096 (= 256 x 16)
│   M_decode      = 256
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 256
│   --max-num-seqs           = 256
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
INFO 01-25 06:11:02 [datasets.py:612] Sampling input_len from [15, 15] and output_len from [256, 256]
INFO 01-25 06:11:02 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6', 'max_model_len': 272, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 272, 'max_num_seqs': 256, 'disable_log_stats': True, 'enable_lora': None, 'enable_chunked_prefill': False, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6'}
INFO 01-25 06:11:02 [model.py:514] Resolved architecture: LlamaForCausalLM
INFO 01-25 06:11:02 [model.py:1661] Using max model len 272
WARNING 01-25 06:11:02 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1090742) INFO 01-25 06:11:06 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6', speculative_config=None, tokenizer='/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=272, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6, enable_prefix_caching=True, enable_chunked_prefill=False, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [272], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 272, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=1090742) INFO 01-25 06:11:06 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:59031 backend=nccl
(EngineCore_DP0 pid=1090742) INFO 01-25 06:11:06 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=1090742) INFO 01-25 06:11:06 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_6...
(EngineCore_DP0 pid=1090742) INFO 01-25 06:11:07 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=1090742) INFO 01-25 06:11:07 [default_loader.py:308] Loading weights took 0.22 seconds
(EngineCore_DP0 pid=1090742) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1090742) INFO 01-25 06:11:07 [gpu_model_runner.py:3659] Model loading took 1.2749 GiB memory and 0.457541 seconds
(EngineCore_DP0 pid=1090742) WARNING 01-25 06:11:09 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=1090742) INFO 01-25 06:11:09 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/bf7d7c4598/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=1090742) INFO 01-25 06:11:09 [backends.py:703] Dynamo bytecode transform time: 2.05 s
(EngineCore_DP0 pid=1090742) INFO 01-25 06:11:11 [backends.py:261] Cache the graph of compile range (1, 272) for later use
(EngineCore_DP0 pid=1090742) INFO 01-25 06:11:12 [backends.py:278] Compiling a graph for compile range (1, 272) takes 0.85 s
(EngineCore_DP0 pid=1090742) INFO 01-25 06:11:12 [monitor.py:34] torch.compile takes 2.90 s in total
(EngineCore_DP0 pid=1090742) INFO 01-25 06:11:12 [gpu_worker.py:375] Available KV cache memory: 9.77 GiB
(EngineCore_DP0 pid=1090742) INFO 01-25 06:11:12 [kv_cache_utils.py:1291] GPU KV cache size: 320,096 tokens
(EngineCore_DP0 pid=1090742) INFO 01-25 06:11:12 [kv_cache_utils.py:1296] Maximum concurrency for 272 tokens per request: 1176.82x
(EngineCore_DP0 pid=1090742) INFO 01-25 06:11:15 [gpu_model_runner.py:4587] Graph capturing finished in 2 secs, took 0.23 GiB
(EngineCore_DP0 pid=1090742) INFO 01-25 06:11:15 [core.py:259] init engine (profile, create kv cache, warmup model) took 7.37 seconds
INFO 01-25 06:11:15 [llm.py:360] Supported tasks: ['generate']
Throughput: 80.89 requests/s, 22003.21 total tokens/s, 20708.90 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536


─── STDERR ───
[2026-01-25 06:11:02] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:11:02] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-FP8'
[2026-01-25 06:11:02] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 06:11:02] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:11:02] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:11:02] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:11:02] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:11:02] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:11:02] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-FP8'
[2026-01-25 06:11:02] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 06:11:02] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:11:02] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:11:02] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:11:02] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 06:11:06] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:11:06] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-FP8'
[2026-01-25 06:11:06] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 06:11:06] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:11:06] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:11:06] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:11:06] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:11:06] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:11:06] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-FP8'
[2026-01-25 06:11:06] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 06:11:06] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:11:06] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:11:06] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:11:06] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1090742) [2026-01-25 06:11:07] INFO SlideSparseLinearMethod_FP8.py:730: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1090742) [2026-01-25 06:11:07] INFO gemm_wrapper.py:872: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1090742) [2026-01-25 06:11:07] INFO SlideSparseLinearMethod_FP8.py:388: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1090742) [2026-01-25 06:11:07] INFO SlideSparseLinearMethod_FP8.py:535: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=1090742) [2026-01-25 06:11:07] INFO kernels.py:132: Loaded tuned kernel for base model: Llama3.2-1B-FP8 (from Llama3.2-1B-FP8-SlideSparse-2_6)
(EngineCore_DP0 pid=1090742) [2026-01-25 06:11:07] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8-SlideSparse-2_6
(EngineCore_DP0 pid=1090742) [2026-01-25 06:11:07] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8-SlideSparse-2_6
(EngineCore_DP0 pid=1090742) [2026-01-25 06:11:07] INFO SlideSparseLinearMethod_FP8.py:552: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8-SlideSparse-2_6
(EngineCore_DP0 pid=1090742) [2026-01-25 06:11:07] INFO SlideSparseLinearMethod_FP8.py:556: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=1090742) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1090742) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.49it/s]
(EngineCore_DP0 pid=1090742) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.49it/s]
(EngineCore_DP0 pid=1090742) 
(EngineCore_DP0 pid=1090742) [2026-01-25 06:11:07] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=1090742) [2026-01-25 06:11:07] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 5308416 bytes
(EngineCore_DP0 pid=1090742) [2026-01-25 06:11:07] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=1090742) [2026-01-25 06:11:07] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 3538944 bytes
(EngineCore_DP0 pid=1090742) [2026-01-25 06:11:07] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=1090742) [2026-01-25 06:11:07] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 28311552 bytes
(EngineCore_DP0 pid=1090742) [2026-01-25 06:11:07] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=1090742) [2026-01-25 06:11:07] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 14024704 bytes
(EngineCore_DP0 pid=1090742) 2026-01-25 06:11:12,826 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1090742) 2026-01-25 06:11:12,839 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1090742) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/36 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 4/36 [00:00<00:00, 34.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 8/36 [00:00<00:00, 35.12it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 12/36 [00:00<00:00, 35.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  44%|████▍     | 16/36 [00:00<00:00, 35.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  56%|█████▌    | 20/36 [00:00<00:00, 34.68it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 24/36 [00:00<00:00, 35.25it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 28/36 [00:00<00:00, 35.65it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 32/36 [00:00<00:00, 35.88it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:01<00:00, 35.93it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:01<00:00, 35.54it/s]
(EngineCore_DP0 pid=1090742) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   3%|▎         | 1/35 [00:00<00:04,  7.01it/s]
Capturing CUDA graphs (decode, FULL):  17%|█▋        | 6/35 [00:00<00:01, 26.02it/s]
Capturing CUDA graphs (decode, FULL):  31%|███▏      | 11/35 [00:00<00:00, 32.72it/s]
Capturing CUDA graphs (decode, FULL):  46%|████▌     | 16/35 [00:00<00:00, 36.24it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 21/35 [00:00<00:00, 38.35it/s]
Capturing CUDA graphs (decode, FULL):  74%|███████▍  | 26/35 [00:00<00:00, 39.66it/s]
Capturing CUDA graphs (decode, FULL):  89%|████████▊ | 31/35 [00:00<00:00, 40.59it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:00<00:00, 36.84it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 9300.90it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:02<12:10,  2.86s/it, est. speed input: 5.59 toks/s, output: 89.42 toks/s]
Processed prompts:  50%|████▉     | 127/256 [00:02<00:02, 60.02it/s, est. speed input: 684.25 toks/s, output: 10947.95 toks/s]
Processed prompts:  91%|█████████ | 232/256 [00:03<00:00, 122.63it/s, est. speed input: 1208.51 toks/s, output: 19336.14 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 122.63it/s, est. speed input: 1306.02 toks/s, output: 20896.29 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 81.62it/s, est. speed input: 1306.02 toks/s, output: 20896.29 toks/s] 
[rank0]:[W125 06:11:19.960983600 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 21.7s

测试结果:
  Requests/s:   80.89
  Tokens/s:     22003.21
  Total Reqs:   256
  Elapsed:      3.16s

  [Decode 分析]
  Total Decode Tokens:  65536
  Decode Tokens/s:      20708.90


------------------------------------------------------------
  生成 CSV: Llama3.2-1B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/Llama3.2-1B-FP8_decode.csv

预览:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
16,16,16,16,256,256,17.7821,4836.7239,0.8998
128,16,128,128,256,256,76.6039,20836.2568,1.6709
256,16,256,256,256,256,80.8941,22003.2075,3.1646

------------------------------------------------------------

[INFO] 完成: 3 成功, 0 失败

============================================================
  Llama3.2-1B-FP8 | cuSPARSELt (2_10) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10

============================================================
[1/3] 测试 M=16
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-FP8                                 │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16
│   M_prefill     = 16 (= 1 x 16)
│   M_decode      = 1
│   batched_tokens = 17 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 17
│   --max-num-batched-tokens = 17
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
INFO 01-25 06:11:24 [datasets.py:612] Sampling input_len from [15, 15] and output_len from [1, 1]
INFO 01-25 06:11:24 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10', 'max_model_len': 17, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 17, 'max_num_seqs': 1, 'disable_log_stats': True, 'enable_lora': None, 'enable_chunked_prefill': False, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10'}
INFO 01-25 06:11:24 [model.py:514] Resolved architecture: LlamaForCausalLM
INFO 01-25 06:11:24 [model.py:1661] Using max model len 17
WARNING 01-25 06:11:24 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1091097) INFO 01-25 06:11:28 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10', speculative_config=None, tokenizer='/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=17, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10, enable_prefix_caching=True, enable_chunked_prefill=False, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [17], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 2, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=1091097) INFO 01-25 06:11:28 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:49991 backend=nccl
(EngineCore_DP0 pid=1091097) INFO 01-25 06:11:28 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=1091097) INFO 01-25 06:11:28 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10...
(EngineCore_DP0 pid=1091097) INFO 01-25 06:11:28 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=1091097) INFO 01-25 06:11:29 [default_loader.py:308] Loading weights took 0.23 seconds
(EngineCore_DP0 pid=1091097) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1091097) INFO 01-25 06:11:29 [gpu_model_runner.py:3659] Model loading took 1.4246 GiB memory and 0.469897 seconds
(EngineCore_DP0 pid=1091097) WARNING 01-25 06:11:31 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=1091097) INFO 01-25 06:11:31 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/206eff8cc3/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=1091097) INFO 01-25 06:11:31 [backends.py:703] Dynamo bytecode transform time: 2.08 s
(EngineCore_DP0 pid=1091097) INFO 01-25 06:11:33 [backends.py:261] Cache the graph of compile range (1, 17) for later use
(EngineCore_DP0 pid=1091097) INFO 01-25 06:11:35 [backends.py:278] Compiling a graph for compile range (1, 17) takes 2.12 s
(EngineCore_DP0 pid=1091097) INFO 01-25 06:11:35 [monitor.py:34] torch.compile takes 4.20 s in total
(EngineCore_DP0 pid=1091097) INFO 01-25 06:11:35 [gpu_worker.py:375] Available KV cache memory: 10.36 GiB
(EngineCore_DP0 pid=1091097) INFO 01-25 06:11:35 [kv_cache_utils.py:1291] GPU KV cache size: 339,632 tokens
(EngineCore_DP0 pid=1091097) INFO 01-25 06:11:35 [kv_cache_utils.py:1296] Maximum concurrency for 17 tokens per request: 10613.50x
(EngineCore_DP0 pid=1091097) INFO 01-25 06:11:36 [gpu_model_runner.py:4587] Graph capturing finished in 1 secs, took 0.01 GiB
(EngineCore_DP0 pid=1091097) INFO 01-25 06:11:36 [core.py:259] init engine (profile, create kv cache, warmup model) took 7.33 seconds
INFO 01-25 06:11:37 [llm.py:360] Supported tasks: ['generate']
Throughput: 65.93 requests/s, 1120.89 total tokens/s, 65.93 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  128


─── STDERR ───
[2026-01-25 06:11:24] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:11:24] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-FP8'
[2026-01-25 06:11:24] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 06:11:24] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:11:24] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:11:24] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:11:24] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:11:24] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:11:24] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-FP8'
[2026-01-25 06:11:24] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 06:11:24] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:11:24] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:11:24] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:11:24] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 06:11:28] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:11:28] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-FP8'
[2026-01-25 06:11:28] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 06:11:28] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:11:28] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:11:28] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:11:28] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:11:28] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:11:28] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-FP8'
[2026-01-25 06:11:28] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 06:11:28] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:11:28] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:11:28] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:11:28] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1091097) [2026-01-25 06:11:28] INFO SlideSparseLinearMethod_FP8.py:730: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1091097) [2026-01-25 06:11:28] INFO gemm_wrapper.py:872: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1091097) [2026-01-25 06:11:28] INFO SlideSparseLinearMethod_FP8.py:388: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1091097) [2026-01-25 06:11:28] INFO SlideSparseLinearMethod_FP8.py:535: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=1091097) [2026-01-25 06:11:28] INFO kernels.py:132: Loaded tuned kernel for base model: Llama3.2-1B-FP8 (from Llama3.2-1B-FP8-SlideSparse-2_10)
(EngineCore_DP0 pid=1091097) [2026-01-25 06:11:28] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8-SlideSparse-2_10
(EngineCore_DP0 pid=1091097) [2026-01-25 06:11:28] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8-SlideSparse-2_10
(EngineCore_DP0 pid=1091097) [2026-01-25 06:11:28] INFO SlideSparseLinearMethod_FP8.py:552: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8-SlideSparse-2_10
(EngineCore_DP0 pid=1091097) [2026-01-25 06:11:28] INFO SlideSparseLinearMethod_FP8.py:556: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=1091097) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1091097) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.09it/s]
(EngineCore_DP0 pid=1091097) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.09it/s]
(EngineCore_DP0 pid=1091097) 
(EngineCore_DP0 pid=1091097) [2026-01-25 06:11:29] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=1091097) [2026-01-25 06:11:29] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 6340608 bytes
(EngineCore_DP0 pid=1091097) [2026-01-25 06:11:29] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=1091097) [2026-01-25 06:11:29] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 4227072 bytes
(EngineCore_DP0 pid=1091097) [2026-01-25 06:11:29] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=1091097) [2026-01-25 06:11:29] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 33816576 bytes
(EngineCore_DP0 pid=1091097) [2026-01-25 06:11:29] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=1091097) [2026-01-25 06:11:29] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 16809984 bytes
(EngineCore_DP0 pid=1091097) 2026-01-25 06:11:35,876 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1091097) 2026-01-25 06:11:35,889 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1091097) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  6.66it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  5.06it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  5.25it/s]
(EngineCore_DP0 pid=1091097) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.86it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.85it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  83%|████████▎ | 106/128 [00:00<00:00, 650.66it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 771.51it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:02, 44.87it/s, est. speed input: 717.89 toks/s, output: 44.87 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:01, 61.41it/s, est. speed input: 942.50 toks/s, output: 58.90 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:01, 67.47it/s, est. speed input: 1026.04 toks/s, output: 64.13 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:00<00:01, 70.22it/s, est. speed input: 1066.92 toks/s, output: 66.68 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:00<00:01, 71.65it/s, est. speed input: 1090.81 toks/s, output: 68.18 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:00<00:01, 72.09it/s, est. speed input: 1103.56 toks/s, output: 68.97 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:00<00:01, 72.56it/s, est. speed input: 1113.96 toks/s, output: 69.62 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:00<00:00, 73.07it/s, est. speed input: 1123.03 toks/s, output: 70.19 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:00<00:00, 73.91it/s, est. speed input: 1132.74 toks/s, output: 70.80 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:01<00:00, 74.27it/s, est. speed input: 1139.49 toks/s, output: 71.22 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:01<00:00, 74.35it/s, est. speed input: 1144.27 toks/s, output: 71.52 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:01<00:00, 74.60it/s, est. speed input: 1149.07 toks/s, output: 71.82 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:01<00:00, 73.63it/s, est. speed input: 1148.69 toks/s, output: 71.79 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:01<00:00, 73.66it/s, est. speed input: 1150.92 toks/s, output: 71.93 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:01<00:00, 73.66it/s, est. speed input: 1152.77 toks/s, output: 72.05 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:01<00:00, 73.82it/s, est. speed input: 1154.89 toks/s, output: 72.18 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 73.82it/s, est. speed input: 1154.79 toks/s, output: 72.17 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 72.17it/s, est. speed input: 1154.79 toks/s, output: 72.17 toks/s]
[rank0]:[W125 06:11:39.293997200 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 20.3s

测试结果:
  Requests/s:   65.93
  Tokens/s:     1120.89
  Total Reqs:   128
  Elapsed:      1.94s

  [Prefill 分析]
  Total Prefill Tokens: 2048
  Prefill Tokens/s:     1054.96

============================================================
[2/3] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-FP8                                 │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 128 (= 1 x 128)
│   M_decode      = 1
│   batched_tokens = 129 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 128
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 129
│   --max-num-batched-tokens = 129
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
INFO 01-25 06:11:44 [datasets.py:612] Sampling input_len from [127, 127] and output_len from [1, 1]
INFO 01-25 06:11:44 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10', 'max_model_len': 129, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 129, 'max_num_seqs': 1, 'disable_log_stats': True, 'enable_lora': None, 'enable_chunked_prefill': False, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10'}
INFO 01-25 06:11:44 [model.py:514] Resolved architecture: LlamaForCausalLM
INFO 01-25 06:11:44 [model.py:1661] Using max model len 129
WARNING 01-25 06:11:44 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1091535) INFO 01-25 06:11:48 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10', speculative_config=None, tokenizer='/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=129, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10, enable_prefix_caching=True, enable_chunked_prefill=False, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [129], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 2, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=1091535) INFO 01-25 06:11:48 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:52913 backend=nccl
(EngineCore_DP0 pid=1091535) INFO 01-25 06:11:48 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=1091535) INFO 01-25 06:11:49 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10...
(EngineCore_DP0 pid=1091535) INFO 01-25 06:11:49 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=1091535) INFO 01-25 06:11:49 [default_loader.py:308] Loading weights took 0.23 seconds
(EngineCore_DP0 pid=1091535) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1091535) INFO 01-25 06:11:49 [gpu_model_runner.py:3659] Model loading took 1.4246 GiB memory and 0.471268 seconds
(EngineCore_DP0 pid=1091535) WARNING 01-25 06:11:52 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=1091535) INFO 01-25 06:11:52 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/ecb5244eb1/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=1091535) INFO 01-25 06:11:52 [backends.py:703] Dynamo bytecode transform time: 2.04 s
(EngineCore_DP0 pid=1091535) INFO 01-25 06:11:54 [backends.py:261] Cache the graph of compile range (1, 129) for later use
(EngineCore_DP0 pid=1091535) INFO 01-25 06:11:55 [backends.py:278] Compiling a graph for compile range (1, 129) takes 2.17 s
(EngineCore_DP0 pid=1091535) INFO 01-25 06:11:55 [monitor.py:34] torch.compile takes 4.22 s in total
(EngineCore_DP0 pid=1091535) INFO 01-25 06:11:56 [gpu_worker.py:375] Available KV cache memory: 10.36 GiB
(EngineCore_DP0 pid=1091535) INFO 01-25 06:11:56 [kv_cache_utils.py:1291] GPU KV cache size: 339,600 tokens
(EngineCore_DP0 pid=1091535) INFO 01-25 06:11:56 [kv_cache_utils.py:1296] Maximum concurrency for 129 tokens per request: 2358.33x
(EngineCore_DP0 pid=1091535) INFO 01-25 06:11:56 [gpu_model_runner.py:4587] Graph capturing finished in 1 secs, took 0.01 GiB
(EngineCore_DP0 pid=1091535) INFO 01-25 06:11:56 [core.py:259] init engine (profile, create kv cache, warmup model) took 7.04 seconds
INFO 01-25 06:11:57 [llm.py:360] Supported tasks: ['generate']
Throughput: 68.54 requests/s, 8841.48 total tokens/s, 68.54 output tokens/s
Total num prompt tokens:  16384
Total num output tokens:  128


─── STDERR ───
[2026-01-25 06:11:44] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:11:44] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-FP8'
[2026-01-25 06:11:44] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 06:11:44] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:11:44] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:11:44] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:11:44] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:11:44] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:11:44] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-FP8'
[2026-01-25 06:11:44] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 06:11:44] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:11:44] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:11:44] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:11:44] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 06:11:48] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:11:48] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-FP8'
[2026-01-25 06:11:48] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 06:11:48] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:11:48] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:11:48] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:11:48] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:11:48] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:11:48] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-FP8'
[2026-01-25 06:11:48] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 06:11:48] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:11:48] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:11:48] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:11:48] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1091535) [2026-01-25 06:11:49] INFO SlideSparseLinearMethod_FP8.py:730: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1091535) [2026-01-25 06:11:49] INFO gemm_wrapper.py:872: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1091535) [2026-01-25 06:11:49] INFO SlideSparseLinearMethod_FP8.py:388: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1091535) [2026-01-25 06:11:49] INFO SlideSparseLinearMethod_FP8.py:535: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=1091535) [2026-01-25 06:11:49] INFO kernels.py:132: Loaded tuned kernel for base model: Llama3.2-1B-FP8 (from Llama3.2-1B-FP8-SlideSparse-2_10)
(EngineCore_DP0 pid=1091535) [2026-01-25 06:11:49] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8-SlideSparse-2_10
(EngineCore_DP0 pid=1091535) [2026-01-25 06:11:49] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8-SlideSparse-2_10
(EngineCore_DP0 pid=1091535) [2026-01-25 06:11:49] INFO SlideSparseLinearMethod_FP8.py:552: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8-SlideSparse-2_10
(EngineCore_DP0 pid=1091535) [2026-01-25 06:11:49] INFO SlideSparseLinearMethod_FP8.py:556: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=1091535) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1091535) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.99it/s]
(EngineCore_DP0 pid=1091535) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.99it/s]
(EngineCore_DP0 pid=1091535) 
(EngineCore_DP0 pid=1091535) [2026-01-25 06:11:49] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=1091535) [2026-01-25 06:11:49] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 6340608 bytes
(EngineCore_DP0 pid=1091535) [2026-01-25 06:11:49] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=1091535) [2026-01-25 06:11:49] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 4227072 bytes
(EngineCore_DP0 pid=1091535) [2026-01-25 06:11:49] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=1091535) [2026-01-25 06:11:49] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 33816576 bytes
(EngineCore_DP0 pid=1091535) [2026-01-25 06:11:49] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=1091535) [2026-01-25 06:11:49] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 16809984 bytes
(EngineCore_DP0 pid=1091535) 2026-01-25 06:11:56,303 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1091535) 2026-01-25 06:11:56,316 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1091535) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 30.20it/s]
(EngineCore_DP0 pid=1091535) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.94it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.93it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 2468.05it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:14,  8.57it/s, est. speed input: 1096.79 toks/s, output: 8.57 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:02, 45.68it/s, est. speed input: 5109.61 toks/s, output: 39.92 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:01, 58.58it/s, est. speed input: 6541.20 toks/s, output: 51.10 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:00<00:01, 65.12it/s, est. speed input: 7295.58 toks/s, output: 57.00 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:00<00:01, 68.50it/s, est. speed input: 7737.42 toks/s, output: 60.45 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:00<00:01, 70.44it/s, est. speed input: 8028.02 toks/s, output: 62.72 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:00<00:01, 72.15it/s, est. speed input: 8260.10 toks/s, output: 64.53 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:00<00:00, 73.06it/s, est. speed input: 8425.06 toks/s, output: 65.82 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:00<00:00, 73.32it/s, est. speed input: 8540.04 toks/s, output: 66.72 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:01<00:00, 73.89it/s, est. speed input: 8646.58 toks/s, output: 67.55 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:01<00:00, 74.18it/s, est. speed input: 8730.62 toks/s, output: 68.21 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:01<00:00, 74.45it/s, est. speed input: 8802.99 toks/s, output: 68.77 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:01<00:00, 74.49it/s, est. speed input: 8859.74 toks/s, output: 69.22 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:01<00:00, 74.77it/s, est. speed input: 8915.76 toks/s, output: 69.65 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:01<00:00, 74.77it/s, est. speed input: 8959.00 toks/s, output: 69.99 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:01<00:00, 74.62it/s, est. speed input: 8993.42 toks/s, output: 70.26 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 74.62it/s, est. speed input: 9032.23 toks/s, output: 70.56 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 70.56it/s, est. speed input: 9032.23 toks/s, output: 70.56 toks/s]
[rank0]:[W125 06:11:59.303435282 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 20.0s

测试结果:
  Requests/s:   68.54
  Tokens/s:     8841.48
  Total Reqs:   128
  Elapsed:      1.87s

  [Prefill 分析]
  Total Prefill Tokens: 16384
  Prefill Tokens/s:     8772.94

============================================================
[3/3] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-FP8                                 │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 256 (= 1 x 256)
│   M_decode      = 1
│   batched_tokens = 257 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 256
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 257
│   --max-num-batched-tokens = 257
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
INFO 01-25 06:12:04 [datasets.py:612] Sampling input_len from [255, 255] and output_len from [1, 1]
INFO 01-25 06:12:04 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10', 'max_model_len': 257, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 257, 'max_num_seqs': 1, 'disable_log_stats': True, 'enable_lora': None, 'enable_chunked_prefill': False, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10'}
INFO 01-25 06:12:04 [model.py:514] Resolved architecture: LlamaForCausalLM
INFO 01-25 06:12:04 [model.py:1661] Using max model len 257
WARNING 01-25 06:12:04 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1091934) INFO 01-25 06:12:08 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10', speculative_config=None, tokenizer='/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=257, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10, enable_prefix_caching=True, enable_chunked_prefill=False, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [257], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 2, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=1091934) INFO 01-25 06:12:08 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:35417 backend=nccl
(EngineCore_DP0 pid=1091934) INFO 01-25 06:12:08 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=1091934) INFO 01-25 06:12:09 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10...
(EngineCore_DP0 pid=1091934) INFO 01-25 06:12:09 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=1091934) INFO 01-25 06:12:09 [default_loader.py:308] Loading weights took 0.23 seconds
(EngineCore_DP0 pid=1091934) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1091934) INFO 01-25 06:12:10 [gpu_model_runner.py:3659] Model loading took 1.4246 GiB memory and 0.522027 seconds
(EngineCore_DP0 pid=1091934) WARNING 01-25 06:12:12 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=1091934) INFO 01-25 06:12:12 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/ce7651de34/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=1091934) INFO 01-25 06:12:12 [backends.py:703] Dynamo bytecode transform time: 2.05 s
(EngineCore_DP0 pid=1091934) INFO 01-25 06:12:14 [backends.py:261] Cache the graph of compile range (1, 257) for later use
(EngineCore_DP0 pid=1091934) INFO 01-25 06:12:15 [backends.py:278] Compiling a graph for compile range (1, 257) takes 1.98 s
(EngineCore_DP0 pid=1091934) INFO 01-25 06:12:15 [monitor.py:34] torch.compile takes 4.04 s in total
(EngineCore_DP0 pid=1091934) INFO 01-25 06:12:16 [gpu_worker.py:375] Available KV cache memory: 10.36 GiB
(EngineCore_DP0 pid=1091934) INFO 01-25 06:12:16 [kv_cache_utils.py:1291] GPU KV cache size: 339,536 tokens
(EngineCore_DP0 pid=1091934) INFO 01-25 06:12:16 [kv_cache_utils.py:1296] Maximum concurrency for 257 tokens per request: 1248.29x
(EngineCore_DP0 pid=1091934) INFO 01-25 06:12:16 [gpu_model_runner.py:4587] Graph capturing finished in 1 secs, took 0.01 GiB
(EngineCore_DP0 pid=1091934) INFO 01-25 06:12:16 [core.py:259] init engine (profile, create kv cache, warmup model) took 6.85 seconds
INFO 01-25 06:12:17 [llm.py:360] Supported tasks: ['generate']
Throughput: 52.33 requests/s, 13448.34 total tokens/s, 52.33 output tokens/s
Total num prompt tokens:  32768
Total num output tokens:  128


─── STDERR ───
[2026-01-25 06:12:04] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:12:04] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-FP8'
[2026-01-25 06:12:04] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 06:12:04] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:12:04] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:12:04] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:12:04] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:12:04] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:12:04] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-FP8'
[2026-01-25 06:12:04] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 06:12:04] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:12:04] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:12:04] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:12:04] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 06:12:08] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:12:08] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-FP8'
[2026-01-25 06:12:08] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 06:12:08] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:12:08] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:12:08] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:12:08] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:12:08] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:12:08] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-FP8'
[2026-01-25 06:12:08] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 06:12:08] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:12:08] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:12:08] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:12:08] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1091934) [2026-01-25 06:12:09] INFO SlideSparseLinearMethod_FP8.py:730: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1091934) [2026-01-25 06:12:09] INFO gemm_wrapper.py:872: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1091934) [2026-01-25 06:12:09] INFO SlideSparseLinearMethod_FP8.py:388: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1091934) [2026-01-25 06:12:09] INFO SlideSparseLinearMethod_FP8.py:535: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=1091934) [2026-01-25 06:12:09] INFO kernels.py:132: Loaded tuned kernel for base model: Llama3.2-1B-FP8 (from Llama3.2-1B-FP8-SlideSparse-2_10)
(EngineCore_DP0 pid=1091934) [2026-01-25 06:12:09] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8-SlideSparse-2_10
(EngineCore_DP0 pid=1091934) [2026-01-25 06:12:09] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8-SlideSparse-2_10
(EngineCore_DP0 pid=1091934) [2026-01-25 06:12:09] INFO SlideSparseLinearMethod_FP8.py:552: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8-SlideSparse-2_10
(EngineCore_DP0 pid=1091934) [2026-01-25 06:12:09] INFO SlideSparseLinearMethod_FP8.py:556: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=1091934) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1091934) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.09it/s]
(EngineCore_DP0 pid=1091934) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.09it/s]
(EngineCore_DP0 pid=1091934) 
(EngineCore_DP0 pid=1091934) [2026-01-25 06:12:09] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=1091934) [2026-01-25 06:12:09] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 6340608 bytes
(EngineCore_DP0 pid=1091934) [2026-01-25 06:12:09] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=1091934) [2026-01-25 06:12:09] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 4227072 bytes
(EngineCore_DP0 pid=1091934) [2026-01-25 06:12:09] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=1091934) [2026-01-25 06:12:09] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 33816576 bytes
(EngineCore_DP0 pid=1091934) [2026-01-25 06:12:09] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=1091934) [2026-01-25 06:12:09] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 16809984 bytes
(EngineCore_DP0 pid=1091934) 2026-01-25 06:12:16,274 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1091934) 2026-01-25 06:12:16,287 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1091934) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 30.49it/s]
(EngineCore_DP0 pid=1091934) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.94it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.94it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 1920.41it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:21,  6.03it/s, est. speed input: 1543.38 toks/s, output: 6.03 toks/s]
Processed prompts:   5%|▌         | 7/128 [00:00<00:03, 30.52it/s, est. speed input: 6654.76 toks/s, output: 25.99 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:02, 41.19it/s, est. speed input: 8871.06 toks/s, output: 34.65 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:00<00:02, 46.82it/s, est. speed input: 10103.23 toks/s, output: 39.47 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:00<00:02, 50.22it/s, est. speed input: 10897.42 toks/s, output: 42.57 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:00<00:01, 52.39it/s, est. speed input: 11453.38 toks/s, output: 44.74 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:00<00:01, 54.03it/s, est. speed input: 11882.96 toks/s, output: 46.42 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:00<00:01, 53.19it/s, est. speed input: 12049.40 toks/s, output: 47.07 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:01<00:01, 54.72it/s, est. speed input: 12341.61 toks/s, output: 48.21 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:01<00:01, 55.88it/s, est. speed input: 12585.01 toks/s, output: 49.16 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:01<00:01, 56.51it/s, est. speed input: 12776.36 toks/s, output: 49.91 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:01<00:01, 56.99it/s, est. speed input: 12939.84 toks/s, output: 50.55 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:01<00:00, 57.13it/s, est. speed input: 13069.07 toks/s, output: 51.05 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:01<00:00, 57.46it/s, est. speed input: 13192.48 toks/s, output: 51.53 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:01<00:00, 57.42it/s, est. speed input: 13287.49 toks/s, output: 51.90 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:01<00:00, 57.57it/s, est. speed input: 13378.89 toks/s, output: 52.26 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:01<00:00, 57.97it/s, est. speed input: 13473.07 toks/s, output: 52.63 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:01<00:00, 58.45it/s, est. speed input: 13565.48 toks/s, output: 52.99 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:02<00:00, 58.50it/s, est. speed input: 13637.62 toks/s, output: 53.27 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:02<00:00, 58.07it/s, est. speed input: 13685.53 toks/s, output: 53.46 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:02<00:00, 57.79it/s, est. speed input: 13729.48 toks/s, output: 53.63 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:02<00:00, 57.81it/s, est. speed input: 13776.83 toks/s, output: 53.82 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 57.81it/s, est. speed input: 13782.04 toks/s, output: 53.84 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 53.83it/s, est. speed input: 13782.04 toks/s, output: 53.84 toks/s]
[rank0]:[W125 06:12:20.850772561 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 20.5s

测试结果:
  Requests/s:   52.33
  Tokens/s:     13448.34
  Total Reqs:   128
  Elapsed:      2.45s

  [Prefill 分析]
  Total Prefill Tokens: 32768
  Prefill Tokens/s:     13396.01


------------------------------------------------------------
  生成 CSV: Llama3.2-1B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/Llama3.2-1B-FP8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
16,16,1,128,128,65.9349,1120.8937,1.9413
128,128,1,128,128,68.5386,8841.4825,1.8676
256,256,1,128,128,52.3282,13448.3394,2.4461

------------------------------------------------------------

[INFO] 完成: 3 成功, 0 失败

============================================================
  Llama3.2-1B-FP8 | cuSPARSELt (2_10) | decode
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10

============================================================
[1/3] 测试 M=16
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-FP8                                 │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16
│   M_prefill     = 256 (= 16 x 16)
│   M_decode      = 16
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 16
│   --max-num-seqs           = 16
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
INFO 01-25 06:12:24 [datasets.py:612] Sampling input_len from [15, 15] and output_len from [256, 256]
INFO 01-25 06:12:25 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10', 'max_model_len': 272, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 272, 'max_num_seqs': 16, 'disable_log_stats': True, 'enable_lora': None, 'enable_chunked_prefill': False, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10'}
INFO 01-25 06:12:25 [model.py:514] Resolved architecture: LlamaForCausalLM
INFO 01-25 06:12:25 [model.py:1661] Using max model len 272
WARNING 01-25 06:12:25 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1092353) INFO 01-25 06:12:29 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10', speculative_config=None, tokenizer='/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=272, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10, enable_prefix_caching=True, enable_chunked_prefill=False, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [272], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 32, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=1092353) INFO 01-25 06:12:29 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:53975 backend=nccl
(EngineCore_DP0 pid=1092353) INFO 01-25 06:12:29 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=1092353) INFO 01-25 06:12:29 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10...
(EngineCore_DP0 pid=1092353) INFO 01-25 06:12:29 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=1092353) INFO 01-25 06:12:29 [default_loader.py:308] Loading weights took 0.24 seconds
(EngineCore_DP0 pid=1092353) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1092353) INFO 01-25 06:12:30 [gpu_model_runner.py:3659] Model loading took 1.4246 GiB memory and 0.476472 seconds
(EngineCore_DP0 pid=1092353) WARNING 01-25 06:12:32 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=1092353) INFO 01-25 06:12:32 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/68b1454616/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=1092353) INFO 01-25 06:12:32 [backends.py:703] Dynamo bytecode transform time: 2.08 s
(EngineCore_DP0 pid=1092353) INFO 01-25 06:12:34 [backends.py:261] Cache the graph of compile range (1, 272) for later use
(EngineCore_DP0 pid=1092353) INFO 01-25 06:12:35 [backends.py:278] Compiling a graph for compile range (1, 272) takes 2.01 s
(EngineCore_DP0 pid=1092353) INFO 01-25 06:12:35 [monitor.py:34] torch.compile takes 4.08 s in total
(EngineCore_DP0 pid=1092353) INFO 01-25 06:12:36 [gpu_worker.py:375] Available KV cache memory: 10.36 GiB
(EngineCore_DP0 pid=1092353) INFO 01-25 06:12:36 [kv_cache_utils.py:1291] GPU KV cache size: 339,536 tokens
(EngineCore_DP0 pid=1092353) INFO 01-25 06:12:36 [kv_cache_utils.py:1296] Maximum concurrency for 272 tokens per request: 1248.29x
(EngineCore_DP0 pid=1092353) INFO 01-25 06:12:37 [gpu_model_runner.py:4587] Graph capturing finished in 1 secs, took 0.04 GiB
(EngineCore_DP0 pid=1092353) INFO 01-25 06:12:37 [core.py:259] init engine (profile, create kv cache, warmup model) took 7.23 seconds
INFO 01-25 06:12:38 [llm.py:360] Supported tasks: ['generate']
Throughput: 16.09 requests/s, 4375.82 total tokens/s, 4118.42 output tokens/s
Total num prompt tokens:  256
Total num output tokens:  4096


─── STDERR ───
[2026-01-25 06:12:25] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:12:25] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-FP8'
[2026-01-25 06:12:25] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 06:12:25] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:12:25] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:12:25] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:12:25] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:12:25] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:12:25] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-FP8'
[2026-01-25 06:12:25] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 06:12:25] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:12:25] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:12:25] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:12:25] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 06:12:28] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:12:28] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-FP8'
[2026-01-25 06:12:28] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 06:12:28] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:12:28] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:12:28] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:12:28] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:12:28] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:12:28] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-FP8'
[2026-01-25 06:12:28] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 06:12:28] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:12:28] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:12:28] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:12:28] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1092353) [2026-01-25 06:12:29] INFO SlideSparseLinearMethod_FP8.py:730: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1092353) [2026-01-25 06:12:29] INFO gemm_wrapper.py:872: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1092353) [2026-01-25 06:12:29] INFO SlideSparseLinearMethod_FP8.py:388: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1092353) [2026-01-25 06:12:29] INFO SlideSparseLinearMethod_FP8.py:535: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=1092353) [2026-01-25 06:12:29] INFO kernels.py:132: Loaded tuned kernel for base model: Llama3.2-1B-FP8 (from Llama3.2-1B-FP8-SlideSparse-2_10)
(EngineCore_DP0 pid=1092353) [2026-01-25 06:12:29] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8-SlideSparse-2_10
(EngineCore_DP0 pid=1092353) [2026-01-25 06:12:29] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8-SlideSparse-2_10
(EngineCore_DP0 pid=1092353) [2026-01-25 06:12:29] INFO SlideSparseLinearMethod_FP8.py:552: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8-SlideSparse-2_10
(EngineCore_DP0 pid=1092353) [2026-01-25 06:12:29] INFO SlideSparseLinearMethod_FP8.py:556: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=1092353) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1092353) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.95it/s]
(EngineCore_DP0 pid=1092353) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.95it/s]
(EngineCore_DP0 pid=1092353) 
(EngineCore_DP0 pid=1092353) [2026-01-25 06:12:29] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=1092353) [2026-01-25 06:12:29] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 6340608 bytes
(EngineCore_DP0 pid=1092353) [2026-01-25 06:12:29] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=1092353) [2026-01-25 06:12:29] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 4227072 bytes
(EngineCore_DP0 pid=1092353) [2026-01-25 06:12:29] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=1092353) [2026-01-25 06:12:29] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 33816576 bytes
(EngineCore_DP0 pid=1092353) [2026-01-25 06:12:29] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=1092353) [2026-01-25 06:12:29] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 16809984 bytes
(EngineCore_DP0 pid=1092353) 2026-01-25 06:12:36,602 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1092353) 2026-01-25 06:12:36,615 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1092353) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00, 32.33it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 32.88it/s]
(EngineCore_DP0 pid=1092353) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 1/5 [00:00<00:00,  6.98it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 20.80it/s]

Adding requests:   0%|          | 0/16 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 16/16 [00:00<00:00, 1294.44it/s]

Processed prompts:   0%|          | 0/16 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   6%|▋         | 1/16 [00:00<00:14,  1.02it/s, est. speed input: 16.39 toks/s, output: 262.24 toks/s]
Processed prompts: 100%|██████████| 16/16 [00:00<00:00,  1.02it/s, est. speed input: 261.20 toks/s, output: 4179.21 toks/s]
Processed prompts: 100%|██████████| 16/16 [00:00<00:00, 16.32it/s, est. speed input: 261.20 toks/s, output: 4179.21 toks/s]
[rank0]:[W125 06:12:39.981954070 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 19.3s

测试结果:
  Requests/s:   16.09
  Tokens/s:     4375.82
  Total Reqs:   16
  Elapsed:      0.99s

  [Decode 分析]
  Total Decode Tokens:  4096
  Decode Tokens/s:      4118.42

============================================================
[2/3] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-FP8                                 │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 2048 (= 128 x 16)
│   M_decode      = 128
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 128
│   --max-num-seqs           = 128
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
INFO 01-25 06:12:44 [datasets.py:612] Sampling input_len from [15, 15] and output_len from [256, 256]
INFO 01-25 06:12:44 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10', 'max_model_len': 272, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 272, 'max_num_seqs': 128, 'disable_log_stats': True, 'enable_lora': None, 'enable_chunked_prefill': False, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10'}
INFO 01-25 06:12:44 [model.py:514] Resolved architecture: LlamaForCausalLM
INFO 01-25 06:12:44 [model.py:1661] Using max model len 272
WARNING 01-25 06:12:44 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1092747) INFO 01-25 06:12:48 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10', speculative_config=None, tokenizer='/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=272, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10, enable_prefix_caching=True, enable_chunked_prefill=False, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [272], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 256, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=1092747) INFO 01-25 06:12:48 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:45283 backend=nccl
(EngineCore_DP0 pid=1092747) INFO 01-25 06:12:48 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=1092747) INFO 01-25 06:12:49 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10...
(EngineCore_DP0 pid=1092747) INFO 01-25 06:12:49 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=1092747) INFO 01-25 06:12:49 [default_loader.py:308] Loading weights took 0.23 seconds
(EngineCore_DP0 pid=1092747) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1092747) INFO 01-25 06:12:49 [gpu_model_runner.py:3659] Model loading took 1.4246 GiB memory and 0.471149 seconds
(EngineCore_DP0 pid=1092747) WARNING 01-25 06:12:52 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=1092747) INFO 01-25 06:12:52 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/4e7b781e1f/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=1092747) INFO 01-25 06:12:52 [backends.py:703] Dynamo bytecode transform time: 2.06 s
(EngineCore_DP0 pid=1092747) INFO 01-25 06:12:53 [backends.py:261] Cache the graph of compile range (1, 272) for later use
(EngineCore_DP0 pid=1092747) INFO 01-25 06:12:54 [backends.py:278] Compiling a graph for compile range (1, 272) takes 0.84 s
(EngineCore_DP0 pid=1092747) INFO 01-25 06:12:54 [monitor.py:34] torch.compile takes 2.90 s in total
(EngineCore_DP0 pid=1092747) INFO 01-25 06:12:54 [gpu_worker.py:375] Available KV cache memory: 10.20 GiB
(EngineCore_DP0 pid=1092747) INFO 01-25 06:12:54 [kv_cache_utils.py:1291] GPU KV cache size: 334,256 tokens
(EngineCore_DP0 pid=1092747) INFO 01-25 06:12:54 [kv_cache_utils.py:1296] Maximum concurrency for 272 tokens per request: 1228.88x
(EngineCore_DP0 pid=1092747) INFO 01-25 06:12:56 [gpu_model_runner.py:4587] Graph capturing finished in 2 secs, took 0.18 GiB
(EngineCore_DP0 pid=1092747) INFO 01-25 06:12:56 [core.py:259] init engine (profile, create kv cache, warmup model) took 6.96 seconds
INFO 01-25 06:12:57 [llm.py:360] Supported tasks: ['generate']
Throughput: 74.23 requests/s, 20191.34 total tokens/s, 19003.61 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768


─── STDERR ───
[2026-01-25 06:12:44] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:12:44] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-FP8'
[2026-01-25 06:12:44] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 06:12:44] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:12:44] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:12:44] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:12:44] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:12:44] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:12:44] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-FP8'
[2026-01-25 06:12:44] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 06:12:44] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:12:44] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:12:44] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:12:44] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 06:12:48] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:12:48] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-FP8'
[2026-01-25 06:12:48] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 06:12:48] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:12:48] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:12:48] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:12:48] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:12:48] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:12:48] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-FP8'
[2026-01-25 06:12:48] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 06:12:48] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:12:48] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:12:48] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:12:48] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1092747) [2026-01-25 06:12:49] INFO SlideSparseLinearMethod_FP8.py:730: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1092747) [2026-01-25 06:12:49] INFO gemm_wrapper.py:872: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1092747) [2026-01-25 06:12:49] INFO SlideSparseLinearMethod_FP8.py:388: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1092747) [2026-01-25 06:12:49] INFO SlideSparseLinearMethod_FP8.py:535: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=1092747) [2026-01-25 06:12:49] INFO kernels.py:132: Loaded tuned kernel for base model: Llama3.2-1B-FP8 (from Llama3.2-1B-FP8-SlideSparse-2_10)
(EngineCore_DP0 pid=1092747) [2026-01-25 06:12:49] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8-SlideSparse-2_10
(EngineCore_DP0 pid=1092747) [2026-01-25 06:12:49] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8-SlideSparse-2_10
(EngineCore_DP0 pid=1092747) [2026-01-25 06:12:49] INFO SlideSparseLinearMethod_FP8.py:552: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8-SlideSparse-2_10
(EngineCore_DP0 pid=1092747) [2026-01-25 06:12:49] INFO SlideSparseLinearMethod_FP8.py:556: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=1092747) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1092747) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.09it/s]
(EngineCore_DP0 pid=1092747) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.09it/s]
(EngineCore_DP0 pid=1092747) 
(EngineCore_DP0 pid=1092747) [2026-01-25 06:12:49] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=1092747) [2026-01-25 06:12:49] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 6340608 bytes
(EngineCore_DP0 pid=1092747) [2026-01-25 06:12:49] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=1092747) [2026-01-25 06:12:49] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 4227072 bytes
(EngineCore_DP0 pid=1092747) [2026-01-25 06:12:49] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=1092747) [2026-01-25 06:12:49] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 33816576 bytes
(EngineCore_DP0 pid=1092747) [2026-01-25 06:12:49] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=1092747) [2026-01-25 06:12:49] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 16809984 bytes
(EngineCore_DP0 pid=1092747) 2026-01-25 06:12:54,879 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1092747) 2026-01-25 06:12:54,893 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1092747) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█▏        | 4/35 [00:00<00:00, 33.48it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  23%|██▎       | 8/35 [00:00<00:00, 34.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  34%|███▍      | 12/35 [00:00<00:00, 35.14it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  46%|████▌     | 16/35 [00:00<00:00, 35.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 20/35 [00:00<00:00, 34.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 24/35 [00:00<00:00, 35.01it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 28/35 [00:00<00:00, 35.22it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████▏| 32/35 [00:00<00:00, 35.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:01<00:00, 34.87it/s]
(EngineCore_DP0 pid=1092747) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   5%|▌         | 1/19 [00:00<00:02,  6.97it/s]
Capturing CUDA graphs (decode, FULL):  32%|███▏      | 6/19 [00:00<00:00, 25.66it/s]
Capturing CUDA graphs (decode, FULL):  58%|█████▊    | 11/19 [00:00<00:00, 32.20it/s]
Capturing CUDA graphs (decode, FULL):  84%|████████▍ | 16/19 [00:00<00:00, 35.78it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:00<00:00, 32.77it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 6645.18it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:01<03:29,  1.65s/it, est. speed input: 9.70 toks/s, output: 155.21 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00,  1.65s/it, est. speed input: 1201.79 toks/s, output: 19228.60 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 75.11it/s, est. speed input: 1201.79 toks/s, output: 19228.60 toks/s]
[rank0]:[W125 06:12:59.034322563 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 19.9s

测试结果:
  Requests/s:   74.23
  Tokens/s:     20191.34
  Total Reqs:   128
  Elapsed:      1.72s

  [Decode 分析]
  Total Decode Tokens:  32768
  Decode Tokens/s:      19003.61

============================================================
[3/3] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-FP8                                 │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 4096 (= 256 x 16)
│   M_decode      = 256
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 256
│   --max-num-seqs           = 256
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
INFO 01-25 06:13:04 [datasets.py:612] Sampling input_len from [15, 15] and output_len from [256, 256]
INFO 01-25 06:13:04 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10', 'max_model_len': 272, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 272, 'max_num_seqs': 256, 'disable_log_stats': True, 'enable_lora': None, 'enable_chunked_prefill': False, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10'}
INFO 01-25 06:13:04 [model.py:514] Resolved architecture: LlamaForCausalLM
INFO 01-25 06:13:04 [model.py:1661] Using max model len 272
WARNING 01-25 06:13:04 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1093107) INFO 01-25 06:13:08 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10', speculative_config=None, tokenizer='/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=272, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10, enable_prefix_caching=True, enable_chunked_prefill=False, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [272], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 272, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=1093107) INFO 01-25 06:13:08 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:36805 backend=nccl
(EngineCore_DP0 pid=1093107) INFO 01-25 06:13:08 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=1093107) INFO 01-25 06:13:08 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-FP8-SlideSparse-2_10...
(EngineCore_DP0 pid=1093107) INFO 01-25 06:13:08 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=1093107) INFO 01-25 06:13:09 [default_loader.py:308] Loading weights took 0.23 seconds
(EngineCore_DP0 pid=1093107) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1093107) INFO 01-25 06:13:09 [gpu_model_runner.py:3659] Model loading took 1.4246 GiB memory and 0.820641 seconds
(EngineCore_DP0 pid=1093107) WARNING 01-25 06:13:11 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=1093107) INFO 01-25 06:13:11 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/e642692a29/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=1093107) INFO 01-25 06:13:11 [backends.py:703] Dynamo bytecode transform time: 2.05 s
(EngineCore_DP0 pid=1093107) INFO 01-25 06:13:13 [backends.py:261] Cache the graph of compile range (1, 272) for later use
(EngineCore_DP0 pid=1093107) INFO 01-25 06:13:14 [backends.py:278] Compiling a graph for compile range (1, 272) takes 0.84 s
(EngineCore_DP0 pid=1093107) INFO 01-25 06:13:14 [monitor.py:34] torch.compile takes 2.89 s in total
(EngineCore_DP0 pid=1093107) INFO 01-25 06:13:14 [gpu_worker.py:375] Available KV cache memory: 9.62 GiB
(EngineCore_DP0 pid=1093107) INFO 01-25 06:13:14 [kv_cache_utils.py:1291] GPU KV cache size: 315,216 tokens
(EngineCore_DP0 pid=1093107) INFO 01-25 06:13:14 [kv_cache_utils.py:1296] Maximum concurrency for 272 tokens per request: 1158.88x
(EngineCore_DP0 pid=1093107) INFO 01-25 06:13:17 [gpu_model_runner.py:4587] Graph capturing finished in 3 secs, took 0.23 GiB
(EngineCore_DP0 pid=1093107) INFO 01-25 06:13:17 [core.py:259] init engine (profile, create kv cache, warmup model) took 7.62 seconds
INFO 01-25 06:13:18 [llm.py:360] Supported tasks: ['generate']
Throughput: 77.70 requests/s, 21134.12 total tokens/s, 19890.94 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536


─── STDERR ───
[2026-01-25 06:13:04] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:13:04] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-FP8'
[2026-01-25 06:13:04] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 06:13:04] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:13:04] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:13:04] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:13:04] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:13:04] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:13:04] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-FP8'
[2026-01-25 06:13:04] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 06:13:04] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:13:04] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:13:04] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:13:04] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 06:13:08] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:13:08] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-FP8'
[2026-01-25 06:13:08] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 06:13:08] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:13:08] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:13:08] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:13:08] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:13:08] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:13:08] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-FP8'
[2026-01-25 06:13:08] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 06:13:08] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:13:08] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:13:08] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:13:08] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1093107) [2026-01-25 06:13:08] INFO SlideSparseLinearMethod_FP8.py:730: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1093107) [2026-01-25 06:13:08] INFO gemm_wrapper.py:872: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1093107) [2026-01-25 06:13:08] INFO SlideSparseLinearMethod_FP8.py:388: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1093107) [2026-01-25 06:13:08] INFO SlideSparseLinearMethod_FP8.py:535: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=1093107) [2026-01-25 06:13:08] INFO kernels.py:132: Loaded tuned kernel for base model: Llama3.2-1B-FP8 (from Llama3.2-1B-FP8-SlideSparse-2_10)
(EngineCore_DP0 pid=1093107) [2026-01-25 06:13:08] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8-SlideSparse-2_10
(EngineCore_DP0 pid=1093107) [2026-01-25 06:13:08] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8-SlideSparse-2_10
(EngineCore_DP0 pid=1093107) [2026-01-25 06:13:08] INFO SlideSparseLinearMethod_FP8.py:552: Preloaded FP8 Triton kernels for model: Llama3.2-1B-FP8-SlideSparse-2_10
(EngineCore_DP0 pid=1093107) [2026-01-25 06:13:08] INFO SlideSparseLinearMethod_FP8.py:556: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=1093107) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1093107) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.06it/s]
(EngineCore_DP0 pid=1093107) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.05it/s]
(EngineCore_DP0 pid=1093107) 
(EngineCore_DP0 pid=1093107) [2026-01-25 06:13:09] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=1093107) [2026-01-25 06:13:09] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 6340608 bytes
(EngineCore_DP0 pid=1093107) [2026-01-25 06:13:09] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=1093107) [2026-01-25 06:13:09] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 4227072 bytes
(EngineCore_DP0 pid=1093107) [2026-01-25 06:13:09] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=1093107) [2026-01-25 06:13:09] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 33816576 bytes
(EngineCore_DP0 pid=1093107) [2026-01-25 06:13:09] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=1093107) [2026-01-25 06:13:09] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 16809984 bytes
(EngineCore_DP0 pid=1093107) 2026-01-25 06:13:14,792 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1093107) 2026-01-25 06:13:14,805 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1093107) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/36 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 2/36 [00:00<00:01, 19.02it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 4/36 [00:00<00:02, 14.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  19%|█▉        | 7/36 [00:00<00:01, 16.09it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  28%|██▊       | 10/36 [00:00<00:01, 19.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|███▉      | 14/36 [00:00<00:00, 24.44it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 18/36 [00:00<00:00, 27.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|██████    | 22/36 [00:00<00:00, 29.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  72%|███████▏  | 26/36 [00:01<00:00, 31.73it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  83%|████████▎ | 30/36 [00:01<00:00, 33.14it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 34/36 [00:01<00:00, 34.45it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:01<00:00, 27.96it/s]
(EngineCore_DP0 pid=1093107) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   3%|▎         | 1/35 [00:00<00:04,  7.02it/s]
Capturing CUDA graphs (decode, FULL):  17%|█▋        | 6/35 [00:00<00:01, 25.84it/s]
Capturing CUDA graphs (decode, FULL):  31%|███▏      | 11/35 [00:00<00:00, 33.06it/s]
Capturing CUDA graphs (decode, FULL):  46%|████▌     | 16/35 [00:00<00:00, 36.30it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 21/35 [00:00<00:00, 38.23it/s]
Capturing CUDA graphs (decode, FULL):  74%|███████▍  | 26/35 [00:00<00:00, 39.14it/s]
Capturing CUDA graphs (decode, FULL):  89%|████████▊ | 31/35 [00:00<00:00, 39.98it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:00<00:00, 36.48it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 9177.59it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:02<12:39,  2.98s/it, est. speed input: 5.37 toks/s, output: 85.94 toks/s]
Processed prompts:  46%|████▌     | 118/256 [00:03<00:02, 53.80it/s, est. speed input: 612.69 toks/s, output: 9803.06 toks/s]
Processed prompts:  88%|████████▊ | 225/256 [00:03<00:00, 115.83it/s, est. speed input: 1130.28 toks/s, output: 18084.51 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 115.83it/s, est. speed input: 1254.14 toks/s, output: 20066.17 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 78.38it/s, est. speed input: 1254.14 toks/s, output: 20066.17 toks/s] 
[rank0]:[W125 06:13:21.347123748 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 22.3s

测试结果:
  Requests/s:   77.70
  Tokens/s:     21134.12
  Total Reqs:   256
  Elapsed:      3.29s

  [Decode 分析]
  Total Decode Tokens:  65536
  Decode Tokens/s:      19890.94


------------------------------------------------------------
  生成 CSV: Llama3.2-1B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/Llama3.2-1B-FP8_decode.csv

预览:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
16,16,16,16,256,256,16.0876,4375.8171,0.9946
128,16,128,128,256,256,74.2329,20191.3393,1.7243
256,16,256,256,256,256,77.6990,21134.1221,3.2948

------------------------------------------------------------

[INFO] 完成: 3 成功, 0 失败


============================================================
  Benchmark 完成!
============================================================


总计: 36 成功, 0 失败
============================================================
