======================================================================
SlideSparse vLLM Throughput Benchmark Log
Created: 2026-01-25 00:17:02
======================================================================

原始命令:
  slidesparse/tools/throughput_benchmark.py --backend all --sparsity 2_4,2_6,2_10 --M quick

命令行参数:
  --model: None
  --backend: all
  --sparsity: 2_4,2_6,2_10
  --stage: None
  --M: quick
  --N: None
  --inner-32: False
  --eager: False
  --gpu-id: 0
  --gpu-mem: 0.8
  --dry-run: False
  --list-models: False

硬件信息:
  GPU: RTX5080
  Compute Capability: cc120
  VRAM: 15.5 GB
  CUDA: 12.9
  Python: py312

Backend 环境变量 (初始状态):
  DISABLE_SLIDESPARSE: 未设置
  USE_CUBLASLT: 未设置
  USE_CUSPARSELT: 未设置
  SPARSITY: 未设置
  INNER_DTYPE_32: 未设置

======================================================================

[WARNING] Backend 不支持，跳过: Llama3.2-1B-INT8 + cutlass
[WARNING]   原因: vLLM CUTLASS INT8 不支持: sm_120 >= sm_100

============================================================
  Llama3.2-1B-INT8 | cuBLASLt | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-INT8
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cublaslt

============================================================
[1/3] 测试 M=16
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度:
│   M_prefill = 16 (= 1 x 16)
│   M_decode  = 1
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len       = 16
│   --output-len      = 1
│   --num-prompts     = 128
│   --max-num-seqs    = 1
│   --max-model-len   = 145
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
INFO 01-25 00:17:06 [datasets.py:612] Sampling input_len from [15, 15] and output_len from [1, 1]
INFO 01-25 00:17:06 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints/Llama3.2-1B-INT8', 'max_model_len': 145, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 145, 'max_num_seqs': 1, 'disable_log_stats': True, 'enable_lora': None, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints/Llama3.2-1B-INT8'}
INFO 01-25 00:17:06 [model.py:514] Resolved architecture: LlamaForCausalLM
INFO 01-25 00:17:06 [model.py:1661] Using max model len 145
INFO 01-25 00:17:07 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=145.
(EngineCore_DP0 pid=858810) INFO 01-25 00:17:11 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints/Llama3.2-1B-INT8', speculative_config=None, tokenizer='/root/vllmbench/checkpoints/Llama3.2-1B-INT8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=145, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints/Llama3.2-1B-INT8, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [145], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 2, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=858810) INFO 01-25 00:17:11 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:42581 backend=nccl
(EngineCore_DP0 pid=858810) INFO 01-25 00:17:11 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=858810) INFO 01-25 00:17:11 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints/Llama3.2-1B-INT8...
(EngineCore_DP0 pid=858810) INFO 01-25 00:17:11 [compressed_tensors_w8a8_int8.py:62] Using CutlassScaledMMLinearKernel for CompressedTensorsW8A8Int8
(EngineCore_DP0 pid=858810) INFO 01-25 00:17:11 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=858810) INFO 01-25 00:17:12 [default_loader.py:308] Loading weights took 0.16 seconds
(EngineCore_DP0 pid=858810) INFO 01-25 00:17:12 [gpu_model_runner.py:3659] Model loading took 1.4137 GiB memory and 0.342843 seconds
(EngineCore_DP0 pid=858810) WARNING 01-25 00:17:14 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=858810) INFO 01-25 00:17:14 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/73a128f3ed/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=858810) INFO 01-25 00:17:14 [backends.py:703] Dynamo bytecode transform time: 2.01 s
(EngineCore_DP0 pid=858810) INFO 01-25 00:17:16 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 145) from the cache, took 0.418 s
(EngineCore_DP0 pid=858810) INFO 01-25 00:17:16 [monitor.py:34] torch.compile takes 2.43 s in total
(EngineCore_DP0 pid=858810) INFO 01-25 00:17:16 [gpu_worker.py:375] Available KV cache memory: 10.83 GiB
(EngineCore_DP0 pid=858810) INFO 01-25 00:17:16 [kv_cache_utils.py:1291] GPU KV cache size: 354,768 tokens
(EngineCore_DP0 pid=858810) INFO 01-25 00:17:16 [kv_cache_utils.py:1296] Maximum concurrency for 145 tokens per request: 2217.30x
(EngineCore_DP0 pid=858810) INFO 01-25 00:17:17 [gpu_model_runner.py:4587] Graph capturing finished in 1 secs, took -0.01 GiB
(EngineCore_DP0 pid=858810) INFO 01-25 00:17:17 [core.py:259] init engine (profile, create kv cache, warmup model) took 5.10 seconds
INFO 01-25 00:17:18 [llm.py:360] Supported tasks: ['generate']
Throughput: 77.41 requests/s, 1316.05 total tokens/s, 77.41 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  128


─── STDERR ───
[2026-01-25 00:17:06] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 00:17:06] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-INT8'
[2026-01-25 00:17:06] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 00:17:06] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:17:06] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:17:06] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:17:06] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:17:06] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:17:06] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-INT8'
[2026-01-25 00:17:06] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 00:17:06] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 00:17:06] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 00:17:06] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 00:17:06] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 00:17:11] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 00:17:11] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-INT8'
[2026-01-25 00:17:11] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 00:17:11] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:17:11] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:17:11] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:17:11] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:17:11] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:17:11] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-INT8'
[2026-01-25 00:17:11] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 00:17:11] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 00:17:11] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 00:17:11] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 00:17:11] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=858810) [2026-01-25 00:17:11] INFO SlideSparseLinearMethod_INT8.py:801: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=858810) [2026-01-25 00:17:11] INFO gemm_wrapper.py:872: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=858810) [2026-01-25 00:17:11] INFO SlideSparseLinearMethod_INT8.py:448: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=858810) [2026-01-25 00:17:11] INFO SlideSparseLinearMethod_INT8.py:617: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=858810) [2026-01-25 00:17:11] INFO SlideSparseLinearMethod_INT8.py:621: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=858810) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=858810) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.88it/s]
(EngineCore_DP0 pid=858810) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.87it/s]
(EngineCore_DP0 pid=858810) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=858810) 2026-01-25 00:17:16,942 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=858810) 2026-01-25 00:17:16,958 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=858810) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 26.57it/s]
(EngineCore_DP0 pid=858810) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.92it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.92it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 5344.02it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   5%|▍         | 6/128 [00:00<00:02, 53.88it/s, est. speed input: 862.28 toks/s, output: 53.89 toks/s]
Processed prompts:  11%|█         | 14/128 [00:00<00:01, 68.01it/s, est. speed input: 1052.77 toks/s, output: 65.79 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:00<00:01, 73.58it/s, est. speed input: 1130.31 toks/s, output: 70.64 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:00<00:01, 75.42it/s, est. speed input: 1159.93 toks/s, output: 72.50 toks/s]
Processed prompts:  30%|███       | 39/128 [00:00<00:01, 76.94it/s, est. speed input: 1182.14 toks/s, output: 73.88 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:00<00:01, 77.89it/s, est. speed input: 1197.25 toks/s, output: 74.83 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:00<00:00, 78.49it/s, est. speed input: 1208.13 toks/s, output: 75.51 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:00<00:00, 79.62it/s, est. speed input: 1221.48 toks/s, output: 76.34 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:00<00:00, 80.14it/s, est. speed input: 1230.57 toks/s, output: 76.91 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:01<00:00, 80.66it/s, est. speed input: 1238.67 toks/s, output: 77.42 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:01<00:00, 80.94it/s, est. speed input: 1244.95 toks/s, output: 77.81 toks/s]
Processed prompts:  78%|███████▊  | 100/128 [00:01<00:00, 80.82it/s, est. speed input: 1248.78 toks/s, output: 78.05 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:01<00:00, 80.74it/s, est. speed input: 1252.01 toks/s, output: 78.25 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:01<00:00, 80.81it/s, est. speed input: 1255.20 toks/s, output: 78.45 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:01<00:00, 80.84it/s, est. speed input: 1257.93 toks/s, output: 78.62 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 80.84it/s, est. speed input: 1258.26 toks/s, output: 78.64 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 78.63it/s, est. speed input: 1258.26 toks/s, output: 78.64 toks/s]
[rank0]:[W125 00:17:19.574004570 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 18.6s

测试结果:
  Requests/s:   77.41
  Tokens/s:     1316.05
  Total Reqs:   128
  Elapsed:      1.65s

  [Prefill 分析]
  Total Prefill Tokens: 2048
  Prefill Tokens/s:     1238.63

============================================================
[2/3] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度:
│   M_prefill = 128 (= 1 x 128)
│   M_decode  = 1
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len       = 128
│   --output-len      = 1
│   --num-prompts     = 128
│   --max-num-seqs    = 1
│   --max-model-len   = 257
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
INFO 01-25 00:17:24 [datasets.py:612] Sampling input_len from [127, 127] and output_len from [1, 1]
INFO 01-25 00:17:24 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints/Llama3.2-1B-INT8', 'max_model_len': 257, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 257, 'max_num_seqs': 1, 'disable_log_stats': True, 'enable_lora': None, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints/Llama3.2-1B-INT8'}
INFO 01-25 00:17:24 [model.py:514] Resolved architecture: LlamaForCausalLM
INFO 01-25 00:17:24 [model.py:1661] Using max model len 257
INFO 01-25 00:17:24 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=257.
(EngineCore_DP0 pid=859167) INFO 01-25 00:17:28 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints/Llama3.2-1B-INT8', speculative_config=None, tokenizer='/root/vllmbench/checkpoints/Llama3.2-1B-INT8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=257, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints/Llama3.2-1B-INT8, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [257], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 2, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=859167) INFO 01-25 00:17:28 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:59067 backend=nccl
(EngineCore_DP0 pid=859167) INFO 01-25 00:17:28 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=859167) INFO 01-25 00:17:28 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints/Llama3.2-1B-INT8...
(EngineCore_DP0 pid=859167) INFO 01-25 00:17:29 [compressed_tensors_w8a8_int8.py:62] Using CutlassScaledMMLinearKernel for CompressedTensorsW8A8Int8
(EngineCore_DP0 pid=859167) INFO 01-25 00:17:29 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=859167) INFO 01-25 00:17:29 [default_loader.py:308] Loading weights took 0.16 seconds
(EngineCore_DP0 pid=859167) INFO 01-25 00:17:29 [gpu_model_runner.py:3659] Model loading took 1.4137 GiB memory and 0.356792 seconds
(EngineCore_DP0 pid=859167) WARNING 01-25 00:17:32 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=859167) INFO 01-25 00:17:32 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/425b3d75ef/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=859167) INFO 01-25 00:17:32 [backends.py:703] Dynamo bytecode transform time: 2.10 s
(EngineCore_DP0 pid=859167) INFO 01-25 00:17:33 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 257) from the cache, took 0.469 s
(EngineCore_DP0 pid=859167) INFO 01-25 00:17:33 [monitor.py:34] torch.compile takes 2.57 s in total
(EngineCore_DP0 pid=859167) INFO 01-25 00:17:34 [gpu_worker.py:375] Available KV cache memory: 10.81 GiB
(EngineCore_DP0 pid=859167) INFO 01-25 00:17:34 [kv_cache_utils.py:1291] GPU KV cache size: 354,288 tokens
(EngineCore_DP0 pid=859167) INFO 01-25 00:17:34 [kv_cache_utils.py:1296] Maximum concurrency for 257 tokens per request: 1302.53x
(EngineCore_DP0 pid=859167) INFO 01-25 00:17:35 [gpu_model_runner.py:4587] Graph capturing finished in 1 secs, took -0.03 GiB
(EngineCore_DP0 pid=859167) INFO 01-25 00:17:35 [core.py:259] init engine (profile, create kv cache, warmup model) took 5.27 seconds
INFO 01-25 00:17:35 [llm.py:360] Supported tasks: ['generate']
Throughput: 74.34 requests/s, 9589.98 total tokens/s, 74.34 output tokens/s
Total num prompt tokens:  16384
Total num output tokens:  128


─── STDERR ───
[2026-01-25 00:17:24] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 00:17:24] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-INT8'
[2026-01-25 00:17:24] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 00:17:24] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:17:24] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:17:24] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:17:24] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:17:24] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:17:24] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-INT8'
[2026-01-25 00:17:24] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 00:17:24] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 00:17:24] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 00:17:24] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 00:17:24] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 00:17:28] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 00:17:28] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-INT8'
[2026-01-25 00:17:28] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 00:17:28] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:17:28] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:17:28] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:17:28] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:17:28] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:17:28] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-INT8'
[2026-01-25 00:17:28] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 00:17:28] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 00:17:28] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 00:17:28] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 00:17:28] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=859167) [2026-01-25 00:17:29] INFO SlideSparseLinearMethod_INT8.py:801: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=859167) [2026-01-25 00:17:29] INFO gemm_wrapper.py:872: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=859167) [2026-01-25 00:17:29] INFO SlideSparseLinearMethod_INT8.py:448: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=859167) [2026-01-25 00:17:29] INFO SlideSparseLinearMethod_INT8.py:617: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=859167) [2026-01-25 00:17:29] INFO SlideSparseLinearMethod_INT8.py:621: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=859167) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=859167) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.53it/s]
(EngineCore_DP0 pid=859167) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  6.53it/s]
(EngineCore_DP0 pid=859167) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=859167) 2026-01-25 00:17:34,455 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=859167) 2026-01-25 00:17:34,469 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=859167) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 26.78it/s]
(EngineCore_DP0 pid=859167) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.02it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.01it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 3451.35it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   5%|▌         | 7/128 [00:00<00:01, 67.20it/s, est. speed input: 8602.36 toks/s, output: 67.20 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:00<00:01, 73.26it/s, est. speed input: 9261.33 toks/s, output: 72.35 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:00<00:01, 74.92it/s, est. speed input: 9454.82 toks/s, output: 73.86 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:00<00:01, 76.53it/s, est. speed input: 9618.15 toks/s, output: 75.14 toks/s]
Processed prompts:  30%|███       | 39/128 [00:00<00:01, 76.99it/s, est. speed input: 9686.27 toks/s, output: 75.67 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:00<00:01, 77.16it/s, est. speed input: 9725.06 toks/s, output: 75.97 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:00<00:00, 76.77it/s, est. speed input: 9725.11 toks/s, output: 75.98 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:00<00:00, 77.25it/s, est. speed input: 9761.38 toks/s, output: 76.26 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:00<00:00, 77.48it/s, est. speed input: 9786.06 toks/s, output: 76.45 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:01<00:00, 77.87it/s, est. speed input: 9815.04 toks/s, output: 76.68 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:01<00:00, 77.66it/s, est. speed input: 9820.90 toks/s, output: 76.73 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:01<00:00, 78.02it/s, est. speed input: 9843.49 toks/s, output: 76.90 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:01<00:00, 78.36it/s, est. speed input: 9865.05 toks/s, output: 77.07 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:01<00:00, 78.59it/s, est. speed input: 9883.69 toks/s, output: 77.22 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:01<00:00, 76.54it/s, est. speed input: 9837.34 toks/s, output: 76.85 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:01<00:00, 73.20it/s, est. speed input: 9741.45 toks/s, output: 76.10 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 73.20it/s, est. speed input: 9730.04 toks/s, output: 76.02 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 76.01it/s, est. speed input: 9730.04 toks/s, output: 76.02 toks/s]
[rank0]:[W125 00:17:37.312702864 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 17.7s

测试结果:
  Requests/s:   74.34
  Tokens/s:     9589.98
  Total Reqs:   128
  Elapsed:      1.72s

  [Prefill 分析]
  Total Prefill Tokens: 16384
  Prefill Tokens/s:     9515.64

============================================================
[3/3] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度:
│   M_prefill = 256 (= 1 x 256)
│   M_decode  = 1
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len       = 256
│   --output-len      = 1
│   --num-prompts     = 128
│   --max-num-seqs    = 1
│   --max-model-len   = 385
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
INFO 01-25 00:17:42 [datasets.py:612] Sampling input_len from [255, 255] and output_len from [1, 1]
INFO 01-25 00:17:42 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints/Llama3.2-1B-INT8', 'max_model_len': 385, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 385, 'max_num_seqs': 1, 'disable_log_stats': True, 'enable_lora': None, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints/Llama3.2-1B-INT8'}
INFO 01-25 00:17:42 [model.py:514] Resolved architecture: LlamaForCausalLM
INFO 01-25 00:17:42 [model.py:1661] Using max model len 385
INFO 01-25 00:17:43 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=385.
(EngineCore_DP0 pid=859542) INFO 01-25 00:17:46 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints/Llama3.2-1B-INT8', speculative_config=None, tokenizer='/root/vllmbench/checkpoints/Llama3.2-1B-INT8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=385, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints/Llama3.2-1B-INT8, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [385], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 2, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=859542) INFO 01-25 00:17:46 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:48131 backend=nccl
(EngineCore_DP0 pid=859542) INFO 01-25 00:17:46 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=859542) INFO 01-25 00:17:47 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints/Llama3.2-1B-INT8...
(EngineCore_DP0 pid=859542) INFO 01-25 00:17:47 [compressed_tensors_w8a8_int8.py:62] Using CutlassScaledMMLinearKernel for CompressedTensorsW8A8Int8
(EngineCore_DP0 pid=859542) INFO 01-25 00:17:47 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=859542) INFO 01-25 00:17:47 [default_loader.py:308] Loading weights took 0.15 seconds
(EngineCore_DP0 pid=859542) INFO 01-25 00:17:47 [gpu_model_runner.py:3659] Model loading took 1.4137 GiB memory and 0.328760 seconds
(EngineCore_DP0 pid=859542) WARNING 01-25 00:17:50 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=859542) INFO 01-25 00:17:50 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/1ab92f29b4/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=859542) INFO 01-25 00:17:50 [backends.py:703] Dynamo bytecode transform time: 2.05 s
(EngineCore_DP0 pid=859542) INFO 01-25 00:17:51 [backends.py:261] Cache the graph of compile range (1, 385) for later use
(EngineCore_DP0 pid=859542) INFO 01-25 00:17:53 [backends.py:278] Compiling a graph for compile range (1, 385) takes 1.83 s
(EngineCore_DP0 pid=859542) INFO 01-25 00:17:53 [monitor.py:34] torch.compile takes 3.88 s in total
(EngineCore_DP0 pid=859542) INFO 01-25 00:17:53 [gpu_worker.py:375] Available KV cache memory: 10.76 GiB
(EngineCore_DP0 pid=859542) INFO 01-25 00:17:53 [kv_cache_utils.py:1291] GPU KV cache size: 352,480 tokens
(EngineCore_DP0 pid=859542) INFO 01-25 00:17:53 [kv_cache_utils.py:1296] Maximum concurrency for 385 tokens per request: 881.20x
(EngineCore_DP0 pid=859542) INFO 01-25 00:17:54 [gpu_model_runner.py:4587] Graph capturing finished in 1 secs, took -0.03 GiB
(EngineCore_DP0 pid=859542) INFO 01-25 00:17:54 [core.py:259] init engine (profile, create kv cache, warmup model) took 6.65 seconds
INFO 01-25 00:17:55 [llm.py:360] Supported tasks: ['generate']
Throughput: 63.37 requests/s, 16285.23 total tokens/s, 63.37 output tokens/s
Total num prompt tokens:  32768
Total num output tokens:  128


─── STDERR ───
[2026-01-25 00:17:42] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 00:17:42] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-INT8'
[2026-01-25 00:17:42] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 00:17:42] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:17:42] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:17:42] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:17:42] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:17:42] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:17:42] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-INT8'
[2026-01-25 00:17:42] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 00:17:42] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 00:17:42] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 00:17:42] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 00:17:42] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 00:17:46] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 00:17:46] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-INT8'
[2026-01-25 00:17:46] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 00:17:46] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:17:46] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:17:46] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:17:46] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:17:46] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:17:46] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-INT8'
[2026-01-25 00:17:46] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 00:17:46] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 00:17:46] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 00:17:46] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 00:17:46] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=859542) [2026-01-25 00:17:47] INFO SlideSparseLinearMethod_INT8.py:801: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=859542) [2026-01-25 00:17:47] INFO gemm_wrapper.py:872: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=859542) [2026-01-25 00:17:47] INFO SlideSparseLinearMethod_INT8.py:448: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=859542) [2026-01-25 00:17:47] INFO SlideSparseLinearMethod_INT8.py:617: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=859542) [2026-01-25 00:17:47] INFO SlideSparseLinearMethod_INT8.py:621: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=859542) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=859542) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  7.48it/s]
(EngineCore_DP0 pid=859542) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  7.48it/s]
(EngineCore_DP0 pid=859542) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=859542) 2026-01-25 00:17:53,906 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=859542) 2026-01-25 00:17:53,919 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=859542) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 19.74it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 19.72it/s]
(EngineCore_DP0 pid=859542) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.46it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.45it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 1830.06it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:41,  3.07it/s, est. speed input: 786.99 toks/s, output: 3.07 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:04, 26.27it/s, est. speed input: 5373.59 toks/s, output: 20.99 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:02, 42.15it/s, est. speed input: 8192.58 toks/s, output: 32.00 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:00<00:01, 53.04it/s, est. speed input: 10094.49 toks/s, output: 39.43 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:00<00:01, 60.75it/s, est. speed input: 11482.04 toks/s, output: 44.85 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:00<00:01, 66.24it/s, est. speed input: 12540.94 toks/s, output: 48.99 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:00<00:01, 70.19it/s, est. speed input: 13378.40 toks/s, output: 52.26 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:01<00:00, 73.10it/s, est. speed input: 14062.01 toks/s, output: 54.93 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:01<00:00, 74.42it/s, est. speed input: 14583.80 toks/s, output: 56.97 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:01<00:00, 75.15it/s, est. speed input: 15008.70 toks/s, output: 58.63 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:01<00:00, 75.65it/s, est. speed input: 15367.90 toks/s, output: 60.03 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:01<00:00, 76.38it/s, est. speed input: 15694.03 toks/s, output: 61.30 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:01<00:00, 77.14it/s, est. speed input: 15988.47 toks/s, output: 62.45 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:01<00:00, 77.96it/s, est. speed input: 16259.01 toks/s, output: 63.51 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:01<00:00, 78.34it/s, est. speed input: 16490.83 toks/s, output: 64.42 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:01<00:00, 77.55it/s, est. speed input: 16655.89 toks/s, output: 65.06 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 77.55it/s, est. speed input: 16819.40 toks/s, output: 65.70 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 65.70it/s, est. speed input: 16819.40 toks/s, output: 65.70 toks/s]
[rank0]:[W125 00:17:57.014699622 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 19.8s

测试结果:
  Requests/s:   63.37
  Tokens/s:     16285.23
  Total Reqs:   128
  Elapsed:      2.02s

  [Prefill 分析]
  Total Prefill Tokens: 32768
  Prefill Tokens/s:     16221.87


------------------------------------------------------------
  生成 CSV: Llama3.2-1B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cublaslt/Llama3.2-1B-INT8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
16,16,1,128,128,77.4145,1316.0466,1.6534
128,128,1,128,128,74.3409,9589.9797,1.7218
256,256,1,128,128,63.3667,16285.2347,2.0200

------------------------------------------------------------

[INFO] 完成: 3 成功, 0 失败

============================================================
  Llama3.2-1B-INT8 | cuBLASLt | decode
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-INT8
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_INT8_py312_cu129_x86_64/cublaslt

============================================================
[1/3] 测试 M=16
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度:
│   M_prefill = 256 (= 16 x 16)
│   M_decode  = 16
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len       = 16
│   --output-len      = 256
│   --num-prompts     = 16
│   --max-num-seqs    = 16
│   --max-model-len   = 400
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
INFO 01-25 00:18:02 [datasets.py:612] Sampling input_len from [15, 15] and output_len from [256, 256]
INFO 01-25 00:18:02 [utils.py:253] non-default args: {'tokenizer': '/root/vllmbench/checkpoints/Llama3.2-1B-INT8', 'max_model_len': 400, 'gpu_memory_utilization': 0.8, 'max_num_batched_tokens': 6400, 'max_num_seqs': 16, 'disable_log_stats': True, 'enable_lora': None, 'reasoning_parser_plugin': '', 'model': '/root/vllmbench/checkpoints/Llama3.2-1B-INT8'}
INFO 01-25 00:18:02 [model.py:514] Resolved architecture: LlamaForCausalLM
INFO 01-25 00:18:02 [model.py:1661] Using max model len 400
INFO 01-25 00:18:02 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=6400.
(EngineCore_DP0 pid=859946) INFO 01-25 00:18:06 [core.py:93] Initializing a V1 LLM engine (v0.13.1.dev7+gd5e6597bf) with config: model='/root/vllmbench/checkpoints/Llama3.2-1B-INT8', speculative_config=None, tokenizer='/root/vllmbench/checkpoints/Llama3.2-1B-INT8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=400, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/root/vllmbench/checkpoints/Llama3.2-1B-INT8, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [6400], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 32, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=859946) INFO 01-25 00:18:06 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.172.141.44:46277 backend=nccl
(EngineCore_DP0 pid=859946) INFO 01-25 00:18:06 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=859946) INFO 01-25 00:18:07 [gpu_model_runner.py:3562] Starting to load model /root/vllmbench/checkpoints/Llama3.2-1B-INT8...
(EngineCore_DP0 pid=859946) INFO 01-25 00:18:07 [compressed_tensors_w8a8_int8.py:62] Using CutlassScaledMMLinearKernel for CompressedTensorsW8A8Int8
(EngineCore_DP0 pid=859946) INFO 01-25 00:18:07 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')
(EngineCore_DP0 pid=859946) INFO 01-25 00:18:07 [default_loader.py:308] Loading weights took 0.15 seconds
(EngineCore_DP0 pid=859946) INFO 01-25 00:18:07 [gpu_model_runner.py:3659] Model loading took 1.4137 GiB memory and 0.337900 seconds
(EngineCore_DP0 pid=859946) WARNING 01-25 00:18:09 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=859946) INFO 01-25 00:18:09 [backends.py:643] Using cache directory: /root/.cache/vllm/torch_compile_cache/24aa14d19a/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=859946) INFO 01-25 00:18:09 [backends.py:703] Dynamo bytecode transform time: 2.03 s
(EngineCore_DP0 pid=859946) INFO 01-25 00:18:12 [backends.py:261] Cache the graph of compile range (1, 6400) for later use
(EngineCore_DP0 pid=859946) INFO 01-25 00:18:14 [backends.py:278] Compiling a graph for compile range (1, 6400) takes 2.95 s
(EngineCore_DP0 pid=859946) INFO 01-25 00:18:14 [monitor.py:34] torch.compile takes 4.98 s in total
(EngineCore_DP0 pid=859946) INFO 01-25 00:18:14 [gpu_worker.py:375] Available KV cache memory: 10.18 GiB
(EngineCore_DP0 pid=859946) INFO 01-25 00:18:14 [kv_cache_utils.py:1291] GPU KV cache size: 333,648 tokens
(EngineCore_DP0 pid=859946) INFO 01-25 00:18:14 [kv_cache_utils.py:1296] Maximum concurrency for 400 tokens per request: 834.12x
(EngineCore_DP0 pid=859946) INFO 01-25 00:18:16 [gpu_model_runner.py:4587] Graph capturing finished in 1 secs, took -0.68 GiB
(EngineCore_DP0 pid=859946) INFO 01-25 00:18:16 [core.py:259] init engine (profile, create kv cache, warmup model) took 8.37 seconds
INFO 01-25 00:18:16 [llm.py:360] Supported tasks: ['generate']
Throughput: 16.63 requests/s, 4524.54 total tokens/s, 4258.39 output tokens/s
Total num prompt tokens:  256
Total num output tokens:  4096


─── STDERR ───
[2026-01-25 00:18:02] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 00:18:02] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-INT8'
[2026-01-25 00:18:02] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 00:18:02] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:18:02] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:18:02] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:18:02] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:18:02] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:18:02] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-INT8'
[2026-01-25 00:18:02] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 00:18:02] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 00:18:02] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 00:18:02] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 00:18:02] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 00:18:06] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 00:18:06] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Llama3.2-1B-INT8'
[2026-01-25 00:18:06] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 00:18:06] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:18:06] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:18:06] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:18:06] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:18:06] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 00:18:06] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Llama3.2-1B-INT8'
[2026-01-25 00:18:06] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 00:18:06] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 00:18:06] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 00:18:06] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 00:18:06] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=859946) [2026-01-25 00:18:07] INFO SlideSparseLinearMethod_INT8.py:801: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=859946) [2026-01-25 00:18:07] INFO gemm_wrapper.py:872: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=859946) [2026-01-25 00:18:07] INFO SlideSparseLinearMethod_INT8.py:448: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=859946) [2026-01-25 00:18:07] INFO SlideSparseLinearMethod_INT8.py:617: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=859946) [2026-01-25 00:18:07] INFO SlideSparseLinearMethod_INT8.py:621: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=859946) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=859946) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  7.48it/s]
(EngineCore_DP0 pid=859946) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  7.48it/s]
(EngineCore_DP0 pid=859946) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=859946) 2026-01-25 00:18:14,963 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=859946) 2026-01-25 00:18:14,977 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=859946) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 1/7 [00:00<00:00,  8.92it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00,  6.14it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:00<00:00, 16.68it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 14.83it/s]
(EngineCore_DP0 pid=859946) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 1/5 [00:00<00:00,  6.98it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 22.28it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 19.68it/s]

Adding requests:   0%|          | 0/16 [00:00<?, ?it/s]
Adding requests:  88%|████████▊ | 14/16 [00:00<00:00, 93.50it/s]
Adding requests: 100%|██████████| 16/16 [00:00<00:00, 106.38it/s]

Processed prompts:   0%|          | 0/16 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   6%|▋         | 1/16 [00:00<00:11,  1.36it/s, est. speed input: 21.76 toks/s, output: 348.22 toks/s]
Processed prompts: 100%|██████████| 16/16 [00:00<00:00,  1.36it/s, est. speed input: 316.47 toks/s, output: 5063.57 toks/s]
Processed prompts: 100%|██████████| 16/16 [00:00<00:00, 19.78it/s, est. speed input: 316.47 toks/s, output: 5063.57 toks/s]
[rank0]:[W125 00:18:17.491233165 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 20.4s

测试结果:
  Requests/s:   16.63
  Tokens/s:     4524.54
  Total Reqs:   16
  Elapsed:      0.96s

  [Decode 分析]
  Total Decode Tokens:  4096
  Decode Tokens/s:      4258.39

============================================================
[2/3] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度:
│   M_prefill = 2048 (= 128 x 16)
│   M_decode  = 128
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len       = 16
│   --output-len      = 256
│   --num-prompts     = 128
│   --max-num-seqs    = 128
│   --max-model-len   = 400
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

============================================================
测试被中断!
当前结果目录: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_INT8_py312_cu129_x86_64/cublaslt
============================================================
