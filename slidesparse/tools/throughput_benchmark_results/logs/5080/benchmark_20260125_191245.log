======================================================================
SlideSparse vLLM Throughput Benchmark Log
Created: 2026-01-25 19:12:45
======================================================================

原始命令:
  /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model llama3.2-3b-int8 --backend cublaslt,cusparselt --stage prefill --sparsity 2_4,2_6,2_8,2_10 --M 512,1024,2048,4096,8192,16384,32768,65536

命令行参数:
  --model: llama3.2-3b-int8
  --backend: cublaslt,cusparselt
  --sparsity: 2_4,2_6,2_8,2_10
  --stage: prefill
  --M: 512,1024,2048,4096,8192,16384,32768,65536
  --N: None
  --inner-32: False
  --eager: False
  --gpu-id: 0
  --gpu-mem: 0.8
  --dry-run: False
  --list-models: False

硬件信息:
  GPU: RTX5080
  Compute Capability: cc120
  VRAM: 15.5 GB
  CUDA: 12.9
  Python: py312

Backend 环境变量 (初始状态):
  DISABLE_SLIDESPARSE: 未设置
  USE_CUBLASLT: 未设置
  USE_CUSPARSELT: 未设置
  SPARSITY: 未设置
  INNER_DTYPE_32: 未设置

======================================================================


============================================================
  Llama3.2-3B-INT8 | cuBLASLt | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints/Llama3.2-3B-INT8
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cublaslt

============================================================
[1/8] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-3B-INT8                                │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 19:12:49 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=356974) WARNING 01-25 19:12:58 [backends.py:609] Failed to read file <frozen os>
Throughput: 4.22 requests/s, 2164.54 total tokens/s, 4.22 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-25 19:12:49] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:12:49] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:12:49] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:12:49] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:12:49] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:12:49] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:12:49] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:12:49] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:12:49] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:12:49] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:12:49] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:12:49] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:12:49] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:12:49] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 19:12:53] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:12:53] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:12:53] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:12:53] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:12:53] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:12:53] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:12:53] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:12:53] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:12:53] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:12:53] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:12:53] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:12:53] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:12:53] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:12:53] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=356974) [2026-01-25 19:12:54] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=356974) [2026-01-25 19:12:54] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=356974) [2026-01-25 19:12:54] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=356974) [2026-01-25 19:12:54] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=356974) [2026-01-25 19:12:54] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=356974) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=356974) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.48it/s]
(EngineCore_DP0 pid=356974) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.48it/s]
(EngineCore_DP0 pid=356974) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=356974) 2026-01-25 19:13:06,586 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=356974) 2026-01-25 19:13:06,602 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=356974) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  4.42it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  2.99it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.14it/s]
(EngineCore_DP0 pid=356974) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [01:03<00:00, 63.89s/it]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [01:03<00:00, 63.89s/it]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 1273.19it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 1271.96it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:28<59:18, 28.02s/it, est. speed input: 18.27 toks/s, output: 0.04 toks/s]
Processed prompts:   5%|▌         | 7/128 [00:28<05:56,  2.94s/it, est. speed input: 127.41 toks/s, output: 0.25 toks/s]
Processed prompts:  10%|█         | 13/128 [00:28<02:29,  1.30s/it, est. speed input: 235.71 toks/s, output: 0.46 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:28<01:19,  1.37it/s, est. speed input: 343.20 toks/s, output: 0.67 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:28<00:46,  2.21it/s, est. speed input: 449.92 toks/s, output: 0.88 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:28<00:28,  3.36it/s, est. speed input: 555.86 toks/s, output: 1.09 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:28<00:18,  4.93it/s, est. speed input: 661.06 toks/s, output: 1.29 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:28<00:12,  7.01it/s, est. speed input: 765.42 toks/s, output: 1.49 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:28<00:08,  9.72it/s, est. speed input: 869.08 toks/s, output: 1.70 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:28<00:05, 13.11it/s, est. speed input: 971.93 toks/s, output: 1.90 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:29<00:03, 17.21it/s, est. speed input: 1074.11 toks/s, output: 2.10 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:29<00:02, 21.94it/s, est. speed input: 1175.61 toks/s, output: 2.30 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:29<00:02, 26.96it/s, est. speed input: 1276.24 toks/s, output: 2.49 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:29<00:01, 32.13it/s, est. speed input: 1376.25 toks/s, output: 2.69 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:29<00:01, 37.14it/s, est. speed input: 1475.58 toks/s, output: 2.88 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:29<00:00, 41.64it/s, est. speed input: 1574.21 toks/s, output: 3.07 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:29<00:00, 45.19it/s, est. speed input: 1671.99 toks/s, output: 3.27 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:29<00:00, 48.09it/s, est. speed input: 1769.09 toks/s, output: 3.46 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:29<00:00, 50.98it/s, est. speed input: 1865.80 toks/s, output: 3.64 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:30<00:00, 53.25it/s, est. speed input: 1961.88 toks/s, output: 3.83 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:30<00:00, 55.02it/s, est. speed input: 2057.35 toks/s, output: 4.02 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:30<00:00, 55.69it/s, est. speed input: 2151.89 toks/s, output: 4.20 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:30<00:00, 55.69it/s, est. speed input: 2167.57 toks/s, output: 4.23 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:30<00:00,  4.23it/s, est. speed input: 2167.57 toks/s, output: 4.23 toks/s]
[rank0]:[W125 19:14:42.382539224 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 118.1s

测试结果:
  Requests/s:   4.22
  Tokens/s:     2164.54
  Total Reqs:   128
  Elapsed:      30.34s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     2160.32

============================================================
[2/8] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-3B-INT8                                │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 19:14:48 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=359196) WARNING 01-25 19:14:56 [backends.py:609] Failed to read file <frozen os>
Throughput: 31.77 requests/s, 32564.04 total tokens/s, 31.77 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-25 19:14:47] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:14:48] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:14:48] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:14:48] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:14:48] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:14:48] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:14:48] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:14:48] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:14:48] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:14:48] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:14:48] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:14:48] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:14:48] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:14:48] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 19:14:52] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:14:52] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:14:52] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:14:52] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:14:52] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:14:52] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:14:52] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:14:52] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:14:52] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:14:52] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:14:52] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:14:52] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:14:52] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:14:52] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=359196) [2026-01-25 19:14:52] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=359196) [2026-01-25 19:14:52] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=359196) [2026-01-25 19:14:52] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=359196) [2026-01-25 19:14:52] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=359196) [2026-01-25 19:14:52] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=359196) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=359196) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.91it/s]
(EngineCore_DP0 pid=359196) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.91it/s]
(EngineCore_DP0 pid=359196) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=359196) 2026-01-25 19:15:03,381 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=359196) 2026-01-25 19:15:03,398 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=359196) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 22.57it/s]
(EngineCore_DP0 pid=359196) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.41it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.41it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  48%|████▊     | 62/128 [00:00<00:00, 618.07it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 677.85it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:14,  8.72it/s, est. speed input: 8932.64 toks/s, output: 8.72 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:05, 23.77it/s, est. speed input: 22058.66 toks/s, output: 21.54 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:04, 28.49it/s, est. speed input: 26362.65 toks/s, output: 25.74 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:03, 30.77it/s, est. speed input: 28550.67 toks/s, output: 27.88 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:03, 31.97it/s, est. speed input: 29825.20 toks/s, output: 29.13 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:03, 32.69it/s, est. speed input: 30675.26 toks/s, output: 29.96 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:00<00:03, 33.16it/s, est. speed input: 31286.68 toks/s, output: 30.55 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:00<00:02, 33.48it/s, est. speed input: 31751.93 toks/s, output: 31.01 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:01<00:02, 33.70it/s, est. speed input: 32111.23 toks/s, output: 31.36 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:01<00:02, 33.77it/s, est. speed input: 32375.82 toks/s, output: 31.62 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:01<00:02, 33.79it/s, est. speed input: 32584.50 toks/s, output: 31.82 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:01<00:02, 33.93it/s, est. speed input: 32790.45 toks/s, output: 32.02 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:01<00:02, 34.01it/s, est. speed input: 32962.72 toks/s, output: 32.19 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:01<00:02, 34.05it/s, est. speed input: 33104.92 toks/s, output: 32.33 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:01<00:02, 33.99it/s, est. speed input: 33210.33 toks/s, output: 32.43 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:01<00:01, 34.04it/s, est. speed input: 33319.71 toks/s, output: 32.54 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:01<00:01, 34.04it/s, est. speed input: 33410.31 toks/s, output: 32.63 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:02<00:01, 34.08it/s, est. speed input: 33497.86 toks/s, output: 32.71 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:02<00:01, 34.09it/s, est. speed input: 33574.24 toks/s, output: 32.79 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:02<00:01, 34.12it/s, est. speed input: 33645.42 toks/s, output: 32.86 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:02<00:01, 34.13it/s, est. speed input: 33709.38 toks/s, output: 32.92 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:02<00:01, 34.12it/s, est. speed input: 33764.20 toks/s, output: 32.97 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:02<00:01, 34.13it/s, est. speed input: 33816.92 toks/s, output: 33.02 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:02<00:01, 34.13it/s, est. speed input: 33863.74 toks/s, output: 33.07 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:02<00:00, 34.14it/s, est. speed input: 33909.17 toks/s, output: 33.11 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:03<00:00, 34.13it/s, est. speed input: 33947.64 toks/s, output: 33.15 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:03<00:00, 34.14it/s, est. speed input: 33985.88 toks/s, output: 33.19 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:03<00:00, 34.10it/s, est. speed input: 34016.08 toks/s, output: 33.22 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:03<00:00, 34.07it/s, est. speed input: 34043.49 toks/s, output: 33.25 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:03<00:00, 34.10it/s, est. speed input: 34075.04 toks/s, output: 33.28 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:03<00:00, 34.12it/s, est. speed input: 34104.48 toks/s, output: 33.31 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:03<00:00, 34.08it/s, est. speed input: 34126.02 toks/s, output: 33.33 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 34.08it/s, est. speed input: 34146.28 toks/s, output: 33.35 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 33.35it/s, est. speed input: 34146.28 toks/s, output: 33.35 toks/s]
[rank0]:[W125 19:15:08.540790293 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 26.0s

测试结果:
  Requests/s:   31.77
  Tokens/s:     32564.04
  Total Reqs:   128
  Elapsed:      4.03s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     32532.27

============================================================
[3/8] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-3B-INT8                                │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 19:15:14 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=359874) WARNING 01-25 19:15:23 [backends.py:609] Failed to read file <frozen os>
Throughput: 35.78 requests/s, 36676.16 total tokens/s, 35.78 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-25 19:15:14] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:15:14] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:15:14] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:15:14] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:15:14] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:15:14] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:15:14] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:15:14] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:15:14] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:15:14] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:15:14] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:15:14] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:15:14] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:15:14] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 19:15:18] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:15:18] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:15:18] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:15:18] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:15:18] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:15:18] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:15:18] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:15:18] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:15:18] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:15:18] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:15:18] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:15:18] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:15:18] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:15:18] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=359874) [2026-01-25 19:15:19] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=359874) [2026-01-25 19:15:19] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=359874) [2026-01-25 19:15:19] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=359874) [2026-01-25 19:15:19] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=359874) [2026-01-25 19:15:19] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=359874) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=359874) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.92it/s]
(EngineCore_DP0 pid=359874) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.91it/s]
(EngineCore_DP0 pid=359874) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=359874) 2026-01-25 19:15:29,833 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=359874) 2026-01-25 19:15:29,849 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=359874) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 18.14it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 18.13it/s]
(EngineCore_DP0 pid=359874) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 1/2 [00:00<00:00,  6.44it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00,  8.87it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   0%|          | 1/256 [00:00<00:34,  7.35it/s]
Adding requests:  29%|██▉       | 75/256 [00:00<00:00, 381.21it/s]
Adding requests:  58%|█████▊    | 148/256 [00:00<00:00, 528.17it/s]
Adding requests:  86%|████████▌ | 220/256 [00:00<00:00, 599.79it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 528.54it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   5%|▌         | 14/256 [00:00<00:01, 129.92it/s, est. speed input: 133052.63 toks/s, output: 129.93 toks/s]
Processed prompts:  11%|█         | 27/256 [00:00<00:04, 56.78it/s, est. speed input: 63727.54 toks/s, output: 62.23 toks/s]   
Processed prompts:  14%|█▎        | 35/256 [00:00<00:04, 48.11it/s, est. speed input: 55074.19 toks/s, output: 53.78 toks/s]
Processed prompts:  16%|█▌        | 41/256 [00:00<00:04, 44.54it/s, est. speed input: 51615.23 toks/s, output: 50.41 toks/s]
Processed prompts:  18%|█▊        | 47/256 [00:00<00:04, 42.14it/s, est. speed input: 49298.72 toks/s, output: 48.14 toks/s]
Processed prompts:  20%|██        | 52/256 [00:01<00:05, 38.59it/s, est. speed input: 46736.17 toks/s, output: 45.64 toks/s]
Processed prompts:  22%|██▏       | 57/256 [00:01<00:04, 40.24it/s, est. speed input: 46751.83 toks/s, output: 45.66 toks/s]
Processed prompts:  24%|██▍       | 62/256 [00:01<00:05, 37.09it/s, est. speed input: 44972.38 toks/s, output: 43.92 toks/s]
Processed prompts:  26%|██▌       | 66/256 [00:01<00:05, 37.01it/s, est. speed input: 44448.72 toks/s, output: 43.41 toks/s]
Processed prompts:  27%|██▋       | 70/256 [00:01<00:05, 36.96it/s, est. speed input: 43998.05 toks/s, output: 42.97 toks/s]
Processed prompts:  29%|██▉       | 74/256 [00:01<00:04, 36.91it/s, est. speed input: 43600.67 toks/s, output: 42.58 toks/s]
Processed prompts:  30%|███       | 78/256 [00:01<00:04, 36.83it/s, est. speed input: 43239.63 toks/s, output: 42.23 toks/s]
Processed prompts:  32%|███▏      | 82/256 [00:01<00:04, 36.77it/s, est. speed input: 42919.13 toks/s, output: 41.91 toks/s]
Processed prompts:  34%|███▎      | 86/256 [00:02<00:04, 36.70it/s, est. speed input: 42627.48 toks/s, output: 41.63 toks/s]
Processed prompts:  35%|███▌      | 90/256 [00:02<00:04, 36.69it/s, est. speed input: 42371.96 toks/s, output: 41.38 toks/s]
Processed prompts:  37%|███▋      | 94/256 [00:02<00:04, 36.69it/s, est. speed input: 42142.54 toks/s, output: 41.15 toks/s]
Processed prompts:  38%|███▊      | 98/256 [00:02<00:04, 36.71it/s, est. speed input: 41938.38 toks/s, output: 40.96 toks/s]
Processed prompts:  40%|███▉      | 102/256 [00:02<00:04, 36.74it/s, est. speed input: 41754.53 toks/s, output: 40.78 toks/s]
Processed prompts:  41%|████▏     | 106/256 [00:02<00:04, 36.78it/s, est. speed input: 41587.82 toks/s, output: 40.61 toks/s]
Processed prompts:  43%|████▎     | 110/256 [00:02<00:03, 36.79it/s, est. speed input: 41432.07 toks/s, output: 40.46 toks/s]
Processed prompts:  45%|████▍     | 114/256 [00:02<00:03, 36.78it/s, est. speed input: 41286.81 toks/s, output: 40.32 toks/s]
Processed prompts:  46%|████▌     | 118/256 [00:02<00:03, 36.77it/s, est. speed input: 41150.79 toks/s, output: 40.19 toks/s]
Processed prompts:  48%|████▊     | 122/256 [00:03<00:03, 36.78it/s, est. speed input: 41026.61 toks/s, output: 40.06 toks/s]
Processed prompts:  49%|████▉     | 126/256 [00:03<00:03, 36.74it/s, est. speed input: 40906.42 toks/s, output: 39.95 toks/s]
Processed prompts:  51%|█████     | 130/256 [00:03<00:03, 36.75it/s, est. speed input: 40797.09 toks/s, output: 39.84 toks/s]
Processed prompts:  52%|█████▏    | 134/256 [00:03<00:03, 36.74it/s, est. speed input: 40693.87 toks/s, output: 39.74 toks/s]
Processed prompts:  54%|█████▍    | 138/256 [00:03<00:03, 36.75it/s, est. speed input: 40598.53 toks/s, output: 39.65 toks/s]
Processed prompts:  55%|█████▌    | 142/256 [00:03<00:03, 36.76it/s, est. speed input: 40510.55 toks/s, output: 39.56 toks/s]
Processed prompts:  57%|█████▋    | 146/256 [00:03<00:02, 36.79it/s, est. speed input: 40428.80 toks/s, output: 39.48 toks/s]
Processed prompts:  59%|█████▊    | 150/256 [00:03<00:02, 36.80it/s, est. speed input: 40350.82 toks/s, output: 39.41 toks/s]
Processed prompts:  60%|██████    | 154/256 [00:03<00:02, 36.79it/s, est. speed input: 40276.18 toks/s, output: 39.33 toks/s]
Processed prompts:  62%|██████▏   | 158/256 [00:04<00:02, 36.76it/s, est. speed input: 40203.20 toks/s, output: 39.26 toks/s]
Processed prompts:  63%|██████▎   | 162/256 [00:04<00:02, 36.68it/s, est. speed input: 40127.68 toks/s, output: 39.19 toks/s]
Processed prompts:  65%|██████▍   | 166/256 [00:04<00:02, 36.68it/s, est. speed input: 40062.07 toks/s, output: 39.12 toks/s]
Processed prompts:  66%|██████▋   | 170/256 [00:04<00:02, 36.64it/s, est. speed input: 39995.15 toks/s, output: 39.06 toks/s]
Processed prompts:  68%|██████▊   | 174/256 [00:04<00:02, 36.69it/s, est. speed input: 39939.02 toks/s, output: 39.00 toks/s]
Processed prompts:  70%|██████▉   | 178/256 [00:04<00:02, 36.71it/s, est. speed input: 39884.42 toks/s, output: 38.95 toks/s]
Processed prompts:  71%|███████   | 182/256 [00:04<00:02, 36.75it/s, est. speed input: 39834.31 toks/s, output: 38.90 toks/s]
Processed prompts:  73%|███████▎  | 186/256 [00:04<00:01, 36.74it/s, est. speed input: 39783.48 toks/s, output: 38.85 toks/s]
Processed prompts:  74%|███████▍  | 190/256 [00:04<00:01, 36.72it/s, est. speed input: 39733.79 toks/s, output: 38.80 toks/s]
Processed prompts:  76%|███████▌  | 194/256 [00:05<00:01, 36.72it/s, est. speed input: 39687.53 toks/s, output: 38.76 toks/s]
Processed prompts:  77%|███████▋  | 198/256 [00:05<00:01, 36.70it/s, est. speed input: 39641.74 toks/s, output: 38.71 toks/s]
Processed prompts:  79%|███████▉  | 202/256 [00:05<00:01, 36.70it/s, est. speed input: 39598.91 toks/s, output: 38.67 toks/s]
Processed prompts:  80%|████████  | 206/256 [00:05<00:01, 36.73it/s, est. speed input: 39559.60 toks/s, output: 38.63 toks/s]
Processed prompts:  82%|████████▏ | 210/256 [00:05<00:01, 36.75it/s, est. speed input: 39521.78 toks/s, output: 38.60 toks/s]
Processed prompts:  84%|████████▎ | 214/256 [00:05<00:01, 36.74it/s, est. speed input: 39484.06 toks/s, output: 38.56 toks/s]
Processed prompts:  85%|████████▌ | 218/256 [00:05<00:01, 36.75it/s, est. speed input: 39449.10 toks/s, output: 38.52 toks/s]
Processed prompts:  87%|████████▋ | 222/256 [00:05<00:00, 36.78it/s, est. speed input: 39416.88 toks/s, output: 38.49 toks/s]
Processed prompts:  88%|████████▊ | 226/256 [00:05<00:00, 36.79it/s, est. speed input: 39385.23 toks/s, output: 38.46 toks/s]
Processed prompts:  90%|████████▉ | 230/256 [00:05<00:00, 36.78it/s, est. speed input: 39353.21 toks/s, output: 38.43 toks/s]
Processed prompts:  91%|█████████▏| 234/256 [00:06<00:00, 36.70it/s, est. speed input: 39318.14 toks/s, output: 38.40 toks/s]
Processed prompts:  93%|█████████▎| 238/256 [00:06<00:00, 36.68it/s, est. speed input: 39286.10 toks/s, output: 38.37 toks/s]
Processed prompts:  95%|█████████▍| 242/256 [00:06<00:00, 36.68it/s, est. speed input: 39256.43 toks/s, output: 38.34 toks/s]
Processed prompts:  96%|█████████▌| 246/256 [00:06<00:00, 36.67it/s, est. speed input: 39227.16 toks/s, output: 38.31 toks/s]
Processed prompts:  98%|█████████▊| 250/256 [00:06<00:00, 36.66it/s, est. speed input: 39198.69 toks/s, output: 38.28 toks/s]
Processed prompts:  99%|█████████▉| 254/256 [00:06<00:00, 36.69it/s, est. speed input: 39173.11 toks/s, output: 38.25 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:06<00:00, 36.69it/s, est. speed input: 39310.05 toks/s, output: 38.39 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:06<00:00, 38.39it/s, est. speed input: 39310.05 toks/s, output: 38.39 toks/s]
[rank0]:[W125 19:15:38.284631713 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 29.7s

测试结果:
  Requests/s:   35.78
  Tokens/s:     36676.16
  Total Reqs:   256
  Elapsed:      7.15s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     36640.38

============================================================
[4/8] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-3B-INT8                                │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 19:15:44 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=360575) WARNING 01-25 19:15:53 [backends.py:609] Failed to read file <frozen os>
Throughput: 35.14 requests/s, 36022.67 total tokens/s, 35.14 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-25 19:15:44] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:15:44] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:15:44] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:15:44] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:15:44] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:15:44] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:15:44] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:15:44] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:15:44] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:15:44] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:15:44] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:15:44] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:15:44] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:15:44] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 19:15:48] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:15:48] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:15:48] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:15:48] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:15:48] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:15:48] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:15:48] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:15:48] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:15:48] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:15:48] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:15:48] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:15:48] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:15:48] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:15:48] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=360575) [2026-01-25 19:15:49] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=360575) [2026-01-25 19:15:49] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=360575) [2026-01-25 19:15:49] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=360575) [2026-01-25 19:15:49] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=360575) [2026-01-25 19:15:49] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=360575) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=360575) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.91it/s]
(EngineCore_DP0 pid=360575) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.91it/s]
(EngineCore_DP0 pid=360575) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=360575) 2026-01-25 19:15:59,440 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=360575) 2026-01-25 19:15:59,456 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=360575) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 3/4 [00:00<00:00, 25.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 23.70it/s]
(EngineCore_DP0 pid=360575) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  33%|███▎      | 1/3 [00:00<00:00,  7.42it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 13.31it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:  14%|█▍        | 73/512 [00:00<00:00, 723.74it/s]
Adding requests:  29%|██▉       | 148/512 [00:00<00:00, 736.07it/s]
Adding requests:  44%|████▎     | 223/512 [00:00<00:00, 741.47it/s]
Adding requests:  58%|█████▊    | 299/512 [00:00<00:00, 748.07it/s]
Adding requests:  73%|███████▎  | 375/512 [00:00<00:00, 749.82it/s]
Adding requests:  88%|████████▊ | 450/512 [00:00<00:00, 748.70it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 744.32it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   5%|▌         | 26/512 [00:00<00:03, 141.66it/s, est. speed input: 145071.97 toks/s, output: 141.66 toks/s]
Processed prompts:   8%|▊         | 41/512 [00:00<00:06, 70.95it/s, est. speed input: 80272.99 toks/s, output: 78.39 toks/s]   
Processed prompts:  10%|▉         | 50/512 [00:00<00:09, 48.35it/s, est. speed input: 59377.46 toks/s, output: 57.99 toks/s]
Processed prompts:  11%|█         | 56/512 [00:00<00:09, 49.27it/s, est. speed input: 58791.61 toks/s, output: 57.41 toks/s]
Processed prompts:  12%|█▏        | 62/512 [00:01<00:10, 41.10it/s, est. speed input: 52816.71 toks/s, output: 51.58 toks/s]
Processed prompts:  13%|█▎        | 67/512 [00:01<00:10, 41.70it/s, est. speed input: 52149.25 toks/s, output: 50.93 toks/s]
Processed prompts:  14%|█▍        | 72/512 [00:01<00:10, 42.22it/s, est. speed input: 51586.31 toks/s, output: 50.38 toks/s]
Processed prompts:  15%|█▌        | 77/512 [00:01<00:10, 42.66it/s, est. speed input: 51106.61 toks/s, output: 49.91 toks/s]
Processed prompts:  16%|█▌        | 82/512 [00:01<00:12, 34.18it/s, est. speed input: 47442.92 toks/s, output: 46.33 toks/s]
Processed prompts:  17%|█▋        | 86/512 [00:01<00:12, 34.43it/s, est. speed input: 46759.34 toks/s, output: 45.66 toks/s]
Processed prompts:  18%|█▊        | 90/512 [00:01<00:12, 34.62it/s, est. speed input: 46149.55 toks/s, output: 45.07 toks/s]
Processed prompts:  18%|█▊        | 94/512 [00:02<00:12, 34.78it/s, est. speed input: 45609.04 toks/s, output: 44.54 toks/s]
Processed prompts:  19%|█▉        | 98/512 [00:02<00:11, 34.91it/s, est. speed input: 45124.32 toks/s, output: 44.07 toks/s]
Processed prompts:  20%|█▉        | 102/512 [00:02<00:11, 35.02it/s, est. speed input: 44689.78 toks/s, output: 43.64 toks/s]
Processed prompts:  21%|██        | 106/512 [00:02<00:11, 35.12it/s, est. speed input: 44298.02 toks/s, output: 43.26 toks/s]
Processed prompts:  21%|██▏       | 110/512 [00:02<00:11, 35.19it/s, est. speed input: 43941.95 toks/s, output: 42.91 toks/s]
Processed prompts:  22%|██▏       | 114/512 [00:02<00:11, 35.23it/s, est. speed input: 43613.79 toks/s, output: 42.59 toks/s]
Processed prompts:  23%|██▎       | 118/512 [00:02<00:11, 35.28it/s, est. speed input: 43314.85 toks/s, output: 42.30 toks/s]
Processed prompts:  24%|██▍       | 122/512 [00:02<00:11, 35.29it/s, est. speed input: 43036.44 toks/s, output: 42.03 toks/s]
Processed prompts:  25%|██▍       | 126/512 [00:03<00:10, 35.30it/s, est. speed input: 42778.46 toks/s, output: 41.78 toks/s]
Processed prompts:  25%|██▌       | 130/512 [00:03<00:10, 35.30it/s, est. speed input: 42538.25 toks/s, output: 41.54 toks/s]
Processed prompts:  26%|██▌       | 134/512 [00:03<00:10, 35.31it/s, est. speed input: 42316.45 toks/s, output: 41.32 toks/s]
Processed prompts:  27%|██▋       | 138/512 [00:03<00:10, 35.27it/s, est. speed input: 42103.33 toks/s, output: 41.12 toks/s]
Processed prompts:  28%|██▊       | 142/512 [00:03<00:10, 35.26it/s, est. speed input: 41906.01 toks/s, output: 40.92 toks/s]
Processed prompts:  29%|██▊       | 146/512 [00:03<00:10, 35.27it/s, est. speed input: 41724.29 toks/s, output: 40.75 toks/s]
Processed prompts:  29%|██▉       | 150/512 [00:03<00:10, 35.29it/s, est. speed input: 41554.27 toks/s, output: 40.58 toks/s]
Processed prompts:  30%|███       | 154/512 [00:03<00:10, 35.27it/s, est. speed input: 41391.23 toks/s, output: 40.42 toks/s]
Processed prompts:  31%|███       | 158/512 [00:03<00:10, 35.29it/s, est. speed input: 41240.82 toks/s, output: 40.27 toks/s]
Processed prompts:  32%|███▏      | 162/512 [00:04<00:09, 35.29it/s, est. speed input: 41097.42 toks/s, output: 40.13 toks/s]
Processed prompts:  32%|███▏      | 166/512 [00:04<00:09, 35.25it/s, est. speed input: 40957.42 toks/s, output: 40.00 toks/s]
Processed prompts:  33%|███▎      | 170/512 [00:04<00:09, 35.22it/s, est. speed input: 40825.58 toks/s, output: 39.87 toks/s]
Processed prompts:  34%|███▍      | 174/512 [00:04<00:09, 35.22it/s, est. speed input: 40701.87 toks/s, output: 39.75 toks/s]
Processed prompts:  35%|███▍      | 178/512 [00:04<00:09, 35.24it/s, est. speed input: 40586.47 toks/s, output: 39.64 toks/s]
Processed prompts:  36%|███▌      | 182/512 [00:04<00:09, 35.25it/s, est. speed input: 40476.90 toks/s, output: 39.53 toks/s]
Processed prompts:  36%|███▋      | 186/512 [00:04<00:09, 35.27it/s, est. speed input: 40372.88 toks/s, output: 39.43 toks/s]
Processed prompts:  37%|███▋      | 190/512 [00:04<00:09, 35.29it/s, est. speed input: 40274.80 toks/s, output: 39.33 toks/s]
Processed prompts:  38%|███▊      | 194/512 [00:04<00:09, 35.26it/s, est. speed input: 40177.55 toks/s, output: 39.24 toks/s]
Processed prompts:  39%|███▊      | 198/512 [00:05<00:08, 35.25it/s, est. speed input: 40084.82 toks/s, output: 39.15 toks/s]
Processed prompts:  39%|███▉      | 202/512 [00:05<00:08, 35.26it/s, est. speed input: 39998.42 toks/s, output: 39.06 toks/s]
Processed prompts:  40%|████      | 206/512 [00:05<00:08, 35.28it/s, est. speed input: 39916.51 toks/s, output: 38.98 toks/s]
Processed prompts:  41%|████      | 210/512 [00:05<00:08, 35.24it/s, est. speed input: 39833.68 toks/s, output: 38.90 toks/s]
Processed prompts:  42%|████▏     | 214/512 [00:05<00:08, 35.23it/s, est. speed input: 39755.59 toks/s, output: 38.82 toks/s]
Processed prompts:  43%|████▎     | 218/512 [00:05<00:08, 35.24it/s, est. speed input: 39682.54 toks/s, output: 38.75 toks/s]
Processed prompts:  43%|████▎     | 222/512 [00:05<00:08, 35.24it/s, est. speed input: 39611.24 toks/s, output: 38.68 toks/s]
Processed prompts:  44%|████▍     | 226/512 [00:05<00:08, 35.25it/s, est. speed input: 39543.22 toks/s, output: 38.62 toks/s]
Processed prompts:  45%|████▍     | 230/512 [00:05<00:07, 35.27it/s, est. speed input: 39479.05 toks/s, output: 38.55 toks/s]
Processed prompts:  46%|████▌     | 234/512 [00:06<00:07, 35.28it/s, est. speed input: 39417.16 toks/s, output: 38.49 toks/s]
Processed prompts:  46%|████▋     | 238/512 [00:06<00:07, 35.26it/s, est. speed input: 39355.89 toks/s, output: 38.43 toks/s]
Processed prompts:  47%|████▋     | 242/512 [00:06<00:07, 35.23it/s, est. speed input: 39294.94 toks/s, output: 38.37 toks/s]
Processed prompts:  48%|████▊     | 246/512 [00:06<00:07, 35.26it/s, est. speed input: 39240.34 toks/s, output: 38.32 toks/s]
Processed prompts:  49%|████▉     | 250/512 [00:06<00:07, 35.29it/s, est. speed input: 39187.44 toks/s, output: 38.27 toks/s]
Processed prompts:  50%|████▉     | 254/512 [00:06<00:07, 35.25it/s, est. speed input: 39133.08 toks/s, output: 38.22 toks/s]
Processed prompts:  50%|█████     | 258/512 [00:06<00:07, 35.26it/s, est. speed input: 39082.87 toks/s, output: 38.17 toks/s]
Processed prompts:  51%|█████     | 262/512 [00:06<00:07, 35.27it/s, est. speed input: 39034.48 toks/s, output: 38.12 toks/s]
Processed prompts:  52%|█████▏    | 266/512 [00:06<00:06, 35.27it/s, est. speed input: 38987.07 toks/s, output: 38.07 toks/s]
Processed prompts:  53%|█████▎    | 270/512 [00:07<00:06, 35.30it/s, est. speed input: 38942.97 toks/s, output: 38.03 toks/s]
Processed prompts:  54%|█████▎    | 274/512 [00:07<00:06, 35.30it/s, est. speed input: 38899.19 toks/s, output: 37.99 toks/s]
Processed prompts:  54%|█████▍    | 278/512 [00:07<00:06, 35.28it/s, est. speed input: 38855.56 toks/s, output: 37.94 toks/s]
Processed prompts:  55%|█████▌    | 282/512 [00:07<00:06, 35.27it/s, est. speed input: 38813.24 toks/s, output: 37.90 toks/s]
Processed prompts:  56%|█████▌    | 286/512 [00:07<00:06, 35.25it/s, est. speed input: 38771.84 toks/s, output: 37.86 toks/s]
Processed prompts:  57%|█████▋    | 290/512 [00:07<00:06, 35.25it/s, est. speed input: 38732.24 toks/s, output: 37.82 toks/s]
Processed prompts:  57%|█████▋    | 294/512 [00:07<00:06, 35.23it/s, est. speed input: 38692.89 toks/s, output: 37.79 toks/s]
Processed prompts:  58%|█████▊    | 298/512 [00:07<00:06, 35.21it/s, est. speed input: 38653.97 toks/s, output: 37.75 toks/s]
Processed prompts:  59%|█████▉    | 302/512 [00:08<00:05, 35.20it/s, est. speed input: 38616.80 toks/s, output: 37.71 toks/s]
Processed prompts:  60%|█████▉    | 306/512 [00:08<00:05, 35.22it/s, est. speed input: 38581.81 toks/s, output: 37.68 toks/s]
Processed prompts:  61%|██████    | 310/512 [00:08<00:05, 35.23it/s, est. speed input: 38547.74 toks/s, output: 37.64 toks/s]
Processed prompts:  61%|██████▏   | 314/512 [00:08<00:05, 35.20it/s, est. speed input: 38512.69 toks/s, output: 37.61 toks/s]
Processed prompts:  62%|██████▏   | 318/512 [00:08<00:05, 35.24it/s, est. speed input: 38481.27 toks/s, output: 37.58 toks/s]
Processed prompts:  63%|██████▎   | 322/512 [00:08<00:05, 35.27it/s, est. speed input: 38450.97 toks/s, output: 37.55 toks/s]
Processed prompts:  64%|██████▎   | 326/512 [00:08<00:05, 35.27it/s, est. speed input: 38420.53 toks/s, output: 37.52 toks/s]
Processed prompts:  64%|██████▍   | 330/512 [00:08<00:05, 35.26it/s, est. speed input: 38390.62 toks/s, output: 37.49 toks/s]
Processed prompts:  65%|██████▌   | 334/512 [00:08<00:05, 35.26it/s, est. speed input: 38361.42 toks/s, output: 37.46 toks/s]
Processed prompts:  66%|██████▌   | 338/512 [00:09<00:04, 35.26it/s, est. speed input: 38333.24 toks/s, output: 37.43 toks/s]
Processed prompts:  67%|██████▋   | 342/512 [00:09<00:04, 35.27it/s, est. speed input: 38305.91 toks/s, output: 37.41 toks/s]
Processed prompts:  68%|██████▊   | 346/512 [00:09<00:04, 35.30it/s, est. speed input: 38280.48 toks/s, output: 37.38 toks/s]
Processed prompts:  68%|██████▊   | 350/512 [00:09<00:04, 35.31it/s, est. speed input: 38254.90 toks/s, output: 37.36 toks/s]
Processed prompts:  69%|██████▉   | 354/512 [00:09<00:04, 35.30it/s, est. speed input: 38229.49 toks/s, output: 37.33 toks/s]
Processed prompts:  70%|██████▉   | 358/512 [00:09<00:04, 35.27it/s, est. speed input: 38203.76 toks/s, output: 37.31 toks/s]
Processed prompts:  71%|███████   | 362/512 [00:09<00:04, 35.24it/s, est. speed input: 38178.05 toks/s, output: 37.28 toks/s]
Processed prompts:  71%|███████▏  | 366/512 [00:09<00:04, 35.28it/s, est. speed input: 38155.52 toks/s, output: 37.26 toks/s]
Processed prompts:  72%|███████▏  | 370/512 [00:09<00:04, 35.30it/s, est. speed input: 38133.29 toks/s, output: 37.24 toks/s]
Processed prompts:  73%|███████▎  | 374/512 [00:10<00:03, 35.27it/s, est. speed input: 38109.50 toks/s, output: 37.22 toks/s]
Processed prompts:  74%|███████▍  | 378/512 [00:10<00:03, 35.22it/s, est. speed input: 38085.37 toks/s, output: 37.19 toks/s]
Processed prompts:  75%|███████▍  | 382/512 [00:10<00:03, 35.22it/s, est. speed input: 38063.15 toks/s, output: 37.17 toks/s]
Processed prompts:  75%|███████▌  | 386/512 [00:10<00:03, 35.20it/s, est. speed input: 38040.57 toks/s, output: 37.15 toks/s]
Processed prompts:  76%|███████▌  | 390/512 [00:10<00:03, 35.21it/s, est. speed input: 38019.45 toks/s, output: 37.13 toks/s]
Processed prompts:  77%|███████▋  | 394/512 [00:10<00:03, 35.23it/s, est. speed input: 37999.02 toks/s, output: 37.11 toks/s]
Processed prompts:  78%|███████▊  | 398/512 [00:10<00:03, 35.23it/s, est. speed input: 37978.58 toks/s, output: 37.09 toks/s]
Processed prompts:  79%|███████▊  | 402/512 [00:10<00:03, 35.24it/s, est. speed input: 37959.18 toks/s, output: 37.07 toks/s]
Processed prompts:  79%|███████▉  | 406/512 [00:10<00:03, 35.20it/s, est. speed input: 37938.40 toks/s, output: 37.05 toks/s]
Processed prompts:  80%|████████  | 410/512 [00:11<00:02, 35.23it/s, est. speed input: 37919.80 toks/s, output: 37.03 toks/s]
Processed prompts:  81%|████████  | 414/512 [00:11<00:02, 35.22it/s, est. speed input: 37900.87 toks/s, output: 37.01 toks/s]
Processed prompts:  82%|████████▏ | 418/512 [00:11<00:02, 35.25it/s, est. speed input: 37883.45 toks/s, output: 37.00 toks/s]
Processed prompts:  82%|████████▏ | 422/512 [00:11<00:02, 35.27it/s, est. speed input: 37866.32 toks/s, output: 36.98 toks/s]
Processed prompts:  83%|████████▎ | 426/512 [00:11<00:02, 35.27it/s, est. speed input: 37849.11 toks/s, output: 36.96 toks/s]
Processed prompts:  84%|████████▍ | 430/512 [00:11<00:02, 35.25it/s, est. speed input: 37831.66 toks/s, output: 36.94 toks/s]
Processed prompts:  85%|████████▍ | 434/512 [00:11<00:02, 35.23it/s, est. speed input: 37814.30 toks/s, output: 36.93 toks/s]
Processed prompts:  86%|████████▌ | 438/512 [00:11<00:02, 35.24it/s, est. speed input: 37798.05 toks/s, output: 36.91 toks/s]
Processed prompts:  86%|████████▋ | 442/512 [00:11<00:01, 35.27it/s, est. speed input: 37782.59 toks/s, output: 36.90 toks/s]
Processed prompts:  87%|████████▋ | 446/512 [00:12<00:01, 35.22it/s, est. speed input: 37765.43 toks/s, output: 36.88 toks/s]
Processed prompts:  88%|████████▊ | 450/512 [00:12<00:01, 35.19it/s, est. speed input: 37748.47 toks/s, output: 36.86 toks/s]
Processed prompts:  89%|████████▊ | 454/512 [00:12<00:01, 35.23it/s, est. speed input: 37734.08 toks/s, output: 36.85 toks/s]
Processed prompts:  89%|████████▉ | 458/512 [00:12<00:01, 35.25it/s, est. speed input: 37719.64 toks/s, output: 36.84 toks/s]
Processed prompts:  90%|█████████ | 462/512 [00:12<00:01, 35.22it/s, est. speed input: 37703.84 toks/s, output: 36.82 toks/s]
Processed prompts:  91%|█████████ | 466/512 [00:12<00:01, 35.22it/s, est. speed input: 37689.26 toks/s, output: 36.81 toks/s]
Processed prompts:  92%|█████████▏| 470/512 [00:12<00:01, 35.23it/s, est. speed input: 37675.09 toks/s, output: 36.79 toks/s]
Processed prompts:  93%|█████████▎| 474/512 [00:12<00:01, 35.26it/s, est. speed input: 37661.86 toks/s, output: 36.78 toks/s]
Processed prompts:  93%|█████████▎| 478/512 [00:13<00:00, 35.27it/s, est. speed input: 37648.61 toks/s, output: 36.77 toks/s]
Processed prompts:  94%|█████████▍| 482/512 [00:13<00:00, 35.22it/s, est. speed input: 37633.94 toks/s, output: 36.75 toks/s]
Processed prompts:  95%|█████████▍| 486/512 [00:13<00:00, 35.22it/s, est. speed input: 37620.39 toks/s, output: 36.74 toks/s]
Processed prompts:  96%|█████████▌| 490/512 [00:13<00:00, 35.23it/s, est. speed input: 37607.49 toks/s, output: 36.73 toks/s]
Processed prompts:  96%|█████████▋| 494/512 [00:13<00:00, 35.23it/s, est. speed input: 37594.64 toks/s, output: 36.71 toks/s]
Processed prompts:  97%|█████████▋| 498/512 [00:13<00:00, 35.24it/s, est. speed input: 37582.11 toks/s, output: 36.70 toks/s]
Processed prompts:  98%|█████████▊| 502/512 [00:13<00:00, 35.22it/s, est. speed input: 37569.22 toks/s, output: 36.69 toks/s]
Processed prompts:  99%|█████████▉| 506/512 [00:13<00:00, 35.21it/s, est. speed input: 37556.58 toks/s, output: 36.68 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:13<00:00, 35.21it/s, est. speed input: 37773.16 toks/s, output: 36.89 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:13<00:00, 36.89it/s, est. speed input: 37773.16 toks/s, output: 36.89 toks/s]
[rank0]:[W125 19:16:15.458473246 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 37.2s

测试结果:
  Requests/s:   35.14
  Tokens/s:     36022.67
  Total Reqs:   512
  Elapsed:      14.57s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     35987.52

============================================================
[5/8] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-3B-INT8                                │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 19:16:23 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=361377) WARNING 01-25 19:16:32 [backends.py:609] Failed to read file <frozen os>
Throughput: 34.49 requests/s, 35352.16 total tokens/s, 34.49 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-25 19:16:23] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:16:23] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:16:23] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:16:23] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:16:23] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:16:23] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:16:23] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:16:23] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:16:23] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:16:23] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:16:23] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:16:23] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:16:23] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:16:23] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 19:16:27] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:16:27] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:16:27] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:16:27] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:16:27] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:16:27] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:16:27] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:16:27] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:16:27] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:16:27] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:16:27] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:16:27] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:16:27] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:16:27] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=361377) [2026-01-25 19:16:28] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=361377) [2026-01-25 19:16:28] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=361377) [2026-01-25 19:16:28] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=361377) [2026-01-25 19:16:28] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=361377) [2026-01-25 19:16:28] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=361377) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=361377) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.90it/s]
(EngineCore_DP0 pid=361377) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.89it/s]
(EngineCore_DP0 pid=361377) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=361377) 2026-01-25 19:16:38,649 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=361377) 2026-01-25 19:16:38,673 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=361377) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 1/5 [00:00<00:00,  4.09it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 3/5 [00:00<00:00,  9.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 12.19it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 10.46it/s]
(EngineCore_DP0 pid=361377) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 1/4 [00:00<00:00,  7.39it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 18.63it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 16.72it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   7%|▋         | 72/1024 [00:00<00:01, 714.29it/s]
Adding requests:  14%|█▍        | 148/1024 [00:00<00:01, 739.42it/s]
Adding requests:  22%|██▏       | 223/1024 [00:00<00:01, 742.80it/s]
Adding requests:  29%|██▉       | 298/1024 [00:00<00:00, 743.41it/s]
Adding requests:  36%|███▋      | 373/1024 [00:00<00:00, 744.60it/s]
Adding requests:  44%|████▍     | 448/1024 [00:00<00:00, 744.40it/s]
Adding requests:  51%|█████     | 523/1024 [00:00<00:00, 739.52it/s]
Adding requests:  58%|█████▊    | 597/1024 [00:00<00:00, 737.40it/s]
Adding requests:  66%|██████▌   | 673/1024 [00:00<00:00, 741.75it/s]
Adding requests:  73%|███████▎  | 750/1024 [00:01<00:00, 748.65it/s]
Adding requests:  81%|████████  | 825/1024 [00:01<00:00, 738.88it/s]
Adding requests:  88%|████████▊ | 904/1024 [00:01<00:00, 753.58it/s]
Adding requests:  96%|█████████▌| 980/1024 [00:01<00:00, 746.13it/s]
Adding requests: 100%|██████████| 1024/1024 [00:01<00:00, 744.20it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   5%|▍         | 50/1024 [00:00<00:06, 156.17it/s, est. speed input: 159920.80 toks/s, output: 156.17 toks/s]
Processed prompts:   6%|▋         | 66/1024 [00:00<00:12, 74.33it/s, est. speed input: 86404.76 toks/s, output: 84.38 toks/s]   
Processed prompts:   7%|▋         | 75/1024 [00:01<00:15, 62.82it/s, est. speed input: 75776.99 toks/s, output: 74.00 toks/s]
Processed prompts:   8%|▊         | 82/1024 [00:01<00:17, 52.54it/s, est. speed input: 67492.39 toks/s, output: 65.91 toks/s]
Processed prompts:   9%|▉         | 90/1024 [00:01<00:19, 46.96it/s, est. speed input: 62477.09 toks/s, output: 61.01 toks/s]
Processed prompts:  10%|▉         | 98/1024 [00:01<00:21, 43.16it/s, est. speed input: 58817.36 toks/s, output: 57.44 toks/s]
Processed prompts:  10%|█         | 106/1024 [00:01<00:22, 40.56it/s, est. speed input: 56038.58 toks/s, output: 54.72 toks/s]
Processed prompts:  11%|█         | 114/1024 [00:02<00:23, 38.76it/s, est. speed input: 53844.04 toks/s, output: 52.58 toks/s]
Processed prompts:  12%|█▏        | 122/1024 [00:02<00:24, 37.50it/s, est. speed input: 52070.22 toks/s, output: 50.85 toks/s]
Processed prompts:  13%|█▎        | 130/1024 [00:02<00:24, 36.65it/s, est. speed input: 50616.61 toks/s, output: 49.43 toks/s]
Processed prompts:  13%|█▎        | 138/1024 [00:02<00:24, 36.04it/s, est. speed input: 49393.75 toks/s, output: 48.24 toks/s]
Processed prompts:  14%|█▍        | 146/1024 [00:03<00:24, 35.62it/s, est. speed input: 48353.61 toks/s, output: 47.22 toks/s]
Processed prompts:  15%|█▌        | 154/1024 [00:03<00:24, 35.32it/s, est. speed input: 47457.89 toks/s, output: 46.35 toks/s]
Processed prompts:  16%|█▌        | 162/1024 [00:03<00:24, 35.09it/s, est. speed input: 46671.63 toks/s, output: 45.58 toks/s]
Processed prompts:  17%|█▋        | 170/1024 [00:03<00:24, 34.95it/s, est. speed input: 45985.49 toks/s, output: 44.91 toks/s]
Processed prompts:  17%|█▋        | 178/1024 [00:04<00:24, 34.84it/s, est. speed input: 45376.98 toks/s, output: 44.31 toks/s]
Processed prompts:  18%|█▊        | 186/1024 [00:04<00:24, 34.78it/s, est. speed input: 44838.44 toks/s, output: 43.79 toks/s]
Processed prompts:  19%|█▉        | 194/1024 [00:04<00:23, 34.73it/s, est. speed input: 44353.40 toks/s, output: 43.31 toks/s]
Processed prompts:  20%|█▉        | 202/1024 [00:04<00:23, 34.70it/s, est. speed input: 43918.20 toks/s, output: 42.89 toks/s]
Processed prompts:  21%|██        | 210/1024 [00:04<00:23, 34.70it/s, est. speed input: 43525.75 toks/s, output: 42.51 toks/s]
Processed prompts:  21%|██▏       | 218/1024 [00:05<00:23, 34.66it/s, est. speed input: 43162.60 toks/s, output: 42.15 toks/s]
Processed prompts:  22%|██▏       | 226/1024 [00:05<00:23, 34.66it/s, est. speed input: 42835.26 toks/s, output: 41.83 toks/s]
Processed prompts:  23%|██▎       | 234/1024 [00:05<00:22, 34.63it/s, est. speed input: 42529.66 toks/s, output: 41.53 toks/s]
Processed prompts:  24%|██▎       | 242/1024 [00:05<00:22, 34.63it/s, est. speed input: 42250.71 toks/s, output: 41.26 toks/s]
Processed prompts:  24%|██▍       | 250/1024 [00:06<00:22, 34.61it/s, est. speed input: 41990.01 toks/s, output: 41.01 toks/s]
Processed prompts:  25%|██▌       | 258/1024 [00:06<00:22, 34.62it/s, est. speed input: 41752.71 toks/s, output: 40.77 toks/s]
Processed prompts:  26%|██▌       | 266/1024 [00:06<00:21, 34.61it/s, est. speed input: 41529.17 toks/s, output: 40.56 toks/s]
Processed prompts:  27%|██▋       | 274/1024 [00:06<00:21, 34.60it/s, est. speed input: 41320.90 toks/s, output: 40.35 toks/s]
Processed prompts:  28%|██▊       | 282/1024 [00:07<00:21, 34.59it/s, est. speed input: 41125.33 toks/s, output: 40.16 toks/s]
Processed prompts:  28%|██▊       | 290/1024 [00:07<00:21, 34.60it/s, est. speed input: 40945.26 toks/s, output: 39.99 toks/s]
Processed prompts:  29%|██▉       | 298/1024 [00:07<00:20, 34.61it/s, est. speed input: 40776.21 toks/s, output: 39.82 toks/s]
Processed prompts:  30%|██▉       | 306/1024 [00:07<00:20, 34.60it/s, est. speed input: 40614.34 toks/s, output: 39.66 toks/s]
Processed prompts:  31%|███       | 314/1024 [00:07<00:20, 34.58it/s, est. speed input: 40461.81 toks/s, output: 39.51 toks/s]
Processed prompts:  31%|███▏      | 322/1024 [00:08<00:20, 34.57it/s, est. speed input: 40317.63 toks/s, output: 39.37 toks/s]
Processed prompts:  32%|███▏      | 330/1024 [00:08<00:20, 34.56it/s, est. speed input: 40181.82 toks/s, output: 39.24 toks/s]
Processed prompts:  33%|███▎      | 338/1024 [00:08<00:19, 34.57it/s, est. speed input: 40054.38 toks/s, output: 39.12 toks/s]
Processed prompts:  34%|███▍      | 346/1024 [00:08<00:19, 34.60it/s, est. speed input: 39935.44 toks/s, output: 39.00 toks/s]
Processed prompts:  35%|███▍      | 354/1024 [00:09<00:19, 34.59it/s, est. speed input: 39820.55 toks/s, output: 38.89 toks/s]
Processed prompts:  35%|███▌      | 362/1024 [00:09<00:19, 34.60it/s, est. speed input: 39711.90 toks/s, output: 38.78 toks/s]
Processed prompts:  36%|███▌      | 370/1024 [00:09<00:18, 34.60it/s, est. speed input: 39608.50 toks/s, output: 38.68 toks/s]
Processed prompts:  37%|███▋      | 378/1024 [00:09<00:18, 34.58it/s, est. speed input: 39508.27 toks/s, output: 38.58 toks/s]
Processed prompts:  38%|███▊      | 386/1024 [00:10<00:18, 34.56it/s, est. speed input: 39411.84 toks/s, output: 38.49 toks/s]
Processed prompts:  38%|███▊      | 394/1024 [00:10<00:18, 34.57it/s, est. speed input: 39322.33 toks/s, output: 38.40 toks/s]
Processed prompts:  39%|███▉      | 402/1024 [00:10<00:17, 34.58it/s, est. speed input: 39236.25 toks/s, output: 38.32 toks/s]
Processed prompts:  40%|████      | 410/1024 [00:10<00:17, 34.57it/s, est. speed input: 39153.18 toks/s, output: 38.24 toks/s]
Processed prompts:  41%|████      | 418/1024 [00:10<00:17, 34.59it/s, est. speed input: 39075.02 toks/s, output: 38.16 toks/s]
Processed prompts:  42%|████▏     | 426/1024 [00:11<00:17, 34.57it/s, est. speed input: 38998.30 toks/s, output: 38.08 toks/s]
Processed prompts:  42%|████▏     | 434/1024 [00:11<00:17, 34.57it/s, est. speed input: 38925.11 toks/s, output: 38.01 toks/s]
Processed prompts:  43%|████▎     | 442/1024 [00:11<00:16, 34.56it/s, est. speed input: 38854.68 toks/s, output: 37.94 toks/s]
Processed prompts:  44%|████▍     | 450/1024 [00:11<00:16, 34.56it/s, est. speed input: 38787.16 toks/s, output: 37.88 toks/s]
Processed prompts:  45%|████▍     | 458/1024 [00:12<00:16, 34.55it/s, est. speed input: 38721.11 toks/s, output: 37.81 toks/s]
Processed prompts:  46%|████▌     | 466/1024 [00:12<00:16, 34.56it/s, est. speed input: 38659.06 toks/s, output: 37.75 toks/s]
Processed prompts:  46%|████▋     | 474/1024 [00:12<00:15, 34.55it/s, est. speed input: 38598.52 toks/s, output: 37.69 toks/s]
Processed prompts:  47%|████▋     | 482/1024 [00:12<00:15, 34.55it/s, est. speed input: 38539.90 toks/s, output: 37.64 toks/s]
Processed prompts:  48%|████▊     | 490/1024 [00:13<00:15, 34.53it/s, est. speed input: 38482.98 toks/s, output: 37.58 toks/s]
Processed prompts:  49%|████▊     | 498/1024 [00:13<00:15, 34.55it/s, est. speed input: 38429.26 toks/s, output: 37.53 toks/s]
Processed prompts:  49%|████▉     | 506/1024 [00:13<00:14, 34.56it/s, est. speed input: 38378.01 toks/s, output: 37.48 toks/s]
Processed prompts:  50%|█████     | 514/1024 [00:13<00:14, 34.57it/s, est. speed input: 38327.99 toks/s, output: 37.43 toks/s]
Processed prompts:  51%|█████     | 522/1024 [00:13<00:14, 34.55it/s, est. speed input: 38278.52 toks/s, output: 37.38 toks/s]
Processed prompts:  52%|█████▏    | 530/1024 [00:14<00:14, 34.52it/s, est. speed input: 38229.43 toks/s, output: 37.33 toks/s]
Processed prompts:  53%|█████▎    | 538/1024 [00:14<00:14, 34.52it/s, est. speed input: 38183.14 toks/s, output: 37.29 toks/s]
Processed prompts:  53%|█████▎    | 546/1024 [00:14<00:13, 34.53it/s, est. speed input: 38138.85 toks/s, output: 37.24 toks/s]
Processed prompts:  54%|█████▍    | 554/1024 [00:14<00:13, 34.53it/s, est. speed input: 38095.71 toks/s, output: 37.20 toks/s]
Processed prompts:  55%|█████▍    | 562/1024 [00:15<00:13, 34.53it/s, est. speed input: 38053.58 toks/s, output: 37.16 toks/s]
Processed prompts:  56%|█████▌    | 570/1024 [00:15<00:13, 34.54it/s, est. speed input: 38013.32 toks/s, output: 37.12 toks/s]
Processed prompts:  56%|█████▋    | 578/1024 [00:15<00:12, 34.53it/s, est. speed input: 37973.91 toks/s, output: 37.08 toks/s]
Processed prompts:  57%|█████▋    | 586/1024 [00:15<00:12, 34.52it/s, est. speed input: 37935.00 toks/s, output: 37.05 toks/s]
Processed prompts:  58%|█████▊    | 594/1024 [00:16<00:12, 34.52it/s, est. speed input: 37897.83 toks/s, output: 37.01 toks/s]
Processed prompts:  59%|█████▉    | 602/1024 [00:16<00:12, 34.52it/s, est. speed input: 37861.27 toks/s, output: 36.97 toks/s]
Processed prompts:  60%|█████▉    | 610/1024 [00:16<00:11, 34.52it/s, est. speed input: 37826.25 toks/s, output: 36.94 toks/s]
Processed prompts:  60%|██████    | 618/1024 [00:16<00:11, 34.52it/s, est. speed input: 37791.73 toks/s, output: 36.91 toks/s]
Processed prompts:  61%|██████    | 626/1024 [00:16<00:11, 34.52it/s, est. speed input: 37758.62 toks/s, output: 36.87 toks/s]
Processed prompts:  62%|██████▏   | 634/1024 [00:17<00:11, 34.52it/s, est. speed input: 37725.90 toks/s, output: 36.84 toks/s]
Processed prompts:  63%|██████▎   | 642/1024 [00:17<00:11, 34.53it/s, est. speed input: 37694.77 toks/s, output: 36.81 toks/s]
Processed prompts:  63%|██████▎   | 650/1024 [00:17<00:10, 34.52it/s, est. speed input: 37663.97 toks/s, output: 36.78 toks/s]
Processed prompts:  64%|██████▍   | 658/1024 [00:17<00:10, 34.53it/s, est. speed input: 37634.11 toks/s, output: 36.75 toks/s]
Processed prompts:  65%|██████▌   | 666/1024 [00:18<00:10, 34.51it/s, est. speed input: 37604.25 toks/s, output: 36.72 toks/s]
Processed prompts:  66%|██████▌   | 674/1024 [00:18<00:10, 34.51it/s, est. speed input: 37575.60 toks/s, output: 36.69 toks/s]
Processed prompts:  67%|██████▋   | 682/1024 [00:18<00:09, 34.51it/s, est. speed input: 37547.54 toks/s, output: 36.67 toks/s]
Processed prompts:  67%|██████▋   | 690/1024 [00:18<00:09, 34.49it/s, est. speed input: 37519.76 toks/s, output: 36.64 toks/s]
Processed prompts:  68%|██████▊   | 698/1024 [00:19<00:09, 34.49it/s, est. speed input: 37492.64 toks/s, output: 36.61 toks/s]
Processed prompts:  69%|██████▉   | 706/1024 [00:19<00:09, 34.51it/s, est. speed input: 37467.37 toks/s, output: 36.59 toks/s]
Processed prompts:  70%|██████▉   | 714/1024 [00:19<00:08, 34.52it/s, est. speed input: 37442.52 toks/s, output: 36.56 toks/s]
Processed prompts:  71%|███████   | 722/1024 [00:19<00:08, 34.52it/s, est. speed input: 37417.88 toks/s, output: 36.54 toks/s]
Processed prompts:  71%|███████▏  | 730/1024 [00:19<00:08, 34.50it/s, est. speed input: 37393.06 toks/s, output: 36.52 toks/s]
Processed prompts:  72%|███████▏  | 738/1024 [00:20<00:08, 34.49it/s, est. speed input: 37369.22 toks/s, output: 36.49 toks/s]
Processed prompts:  73%|███████▎  | 746/1024 [00:20<00:08, 34.49it/s, est. speed input: 37345.71 toks/s, output: 36.47 toks/s]
Processed prompts:  74%|███████▎  | 754/1024 [00:20<00:07, 34.51it/s, est. speed input: 37323.91 toks/s, output: 36.45 toks/s]
Processed prompts:  74%|███████▍  | 762/1024 [00:20<00:07, 34.49it/s, est. speed input: 37301.15 toks/s, output: 36.43 toks/s]
Processed prompts:  75%|███████▌  | 770/1024 [00:21<00:07, 34.47it/s, est. speed input: 37278.60 toks/s, output: 36.40 toks/s]
Processed prompts:  76%|███████▌  | 778/1024 [00:21<00:07, 34.49it/s, est. speed input: 37257.72 toks/s, output: 36.38 toks/s]
Processed prompts:  77%|███████▋  | 786/1024 [00:21<00:06, 34.51it/s, est. speed input: 37237.65 toks/s, output: 36.36 toks/s]
Processed prompts:  78%|███████▊  | 794/1024 [00:21<00:06, 34.49it/s, est. speed input: 37216.62 toks/s, output: 36.34 toks/s]
Processed prompts:  78%|███████▊  | 802/1024 [00:22<00:06, 34.49it/s, est. speed input: 37196.93 toks/s, output: 36.33 toks/s]
Processed prompts:  79%|███████▉  | 810/1024 [00:22<00:06, 34.47it/s, est. speed input: 37176.67 toks/s, output: 36.31 toks/s]
Processed prompts:  80%|███████▉  | 818/1024 [00:22<00:05, 34.47it/s, est. speed input: 37157.25 toks/s, output: 36.29 toks/s]
Processed prompts:  81%|████████  | 826/1024 [00:22<00:05, 34.48it/s, est. speed input: 37138.76 toks/s, output: 36.27 toks/s]
Processed prompts:  81%|████████▏ | 834/1024 [00:23<00:05, 34.47it/s, est. speed input: 37119.78 toks/s, output: 36.25 toks/s]
Processed prompts:  82%|████████▏ | 842/1024 [00:23<00:05, 34.48it/s, est. speed input: 37102.10 toks/s, output: 36.23 toks/s]
Processed prompts:  83%|████████▎ | 850/1024 [00:23<00:05, 34.51it/s, est. speed input: 37085.22 toks/s, output: 36.22 toks/s]
Processed prompts:  84%|████████▍ | 858/1024 [00:23<00:04, 34.51it/s, est. speed input: 37068.16 toks/s, output: 36.20 toks/s]
Processed prompts:  85%|████████▍ | 866/1024 [00:23<00:04, 34.51it/s, est. speed input: 37051.38 toks/s, output: 36.18 toks/s]
Processed prompts:  85%|████████▌ | 874/1024 [00:24<00:04, 34.51it/s, est. speed input: 37034.90 toks/s, output: 36.17 toks/s]
Processed prompts:  86%|████████▌ | 882/1024 [00:24<00:04, 34.49it/s, est. speed input: 37018.29 toks/s, output: 36.15 toks/s]
Processed prompts:  87%|████████▋ | 890/1024 [00:24<00:03, 34.50it/s, est. speed input: 37002.40 toks/s, output: 36.14 toks/s]
Processed prompts:  88%|████████▊ | 898/1024 [00:24<00:03, 34.49it/s, est. speed input: 36986.43 toks/s, output: 36.12 toks/s]
Processed prompts:  88%|████████▊ | 906/1024 [00:25<00:03, 34.46it/s, est. speed input: 36970.15 toks/s, output: 36.10 toks/s]
Processed prompts:  89%|████████▉ | 914/1024 [00:25<00:03, 34.46it/s, est. speed input: 36954.81 toks/s, output: 36.09 toks/s]
Processed prompts:  90%|█████████ | 922/1024 [00:25<00:02, 34.49it/s, est. speed input: 36940.43 toks/s, output: 36.07 toks/s]
Processed prompts:  91%|█████████ | 930/1024 [00:25<00:02, 34.50it/s, est. speed input: 36926.11 toks/s, output: 36.06 toks/s]
Processed prompts:  92%|█████████▏| 938/1024 [00:26<00:02, 34.47it/s, est. speed input: 36911.05 toks/s, output: 36.05 toks/s]
Processed prompts:  92%|█████████▏| 946/1024 [00:26<00:02, 34.46it/s, est. speed input: 36896.39 toks/s, output: 36.03 toks/s]
Processed prompts:  93%|█████████▎| 954/1024 [00:26<00:02, 34.47it/s, est. speed input: 36882.59 toks/s, output: 36.02 toks/s]
Processed prompts:  94%|█████████▍| 962/1024 [00:26<00:01, 34.47it/s, est. speed input: 36868.88 toks/s, output: 36.00 toks/s]
Processed prompts:  95%|█████████▍| 970/1024 [00:26<00:01, 34.48it/s, est. speed input: 36855.50 toks/s, output: 35.99 toks/s]
Processed prompts:  96%|█████████▌| 978/1024 [00:27<00:01, 34.45it/s, est. speed input: 36841.47 toks/s, output: 35.98 toks/s]
Processed prompts:  96%|█████████▋| 986/1024 [00:27<00:01, 34.46it/s, est. speed input: 36828.58 toks/s, output: 35.97 toks/s]
Processed prompts:  97%|█████████▋| 994/1024 [00:27<00:00, 34.47it/s, est. speed input: 36815.95 toks/s, output: 35.95 toks/s]
Processed prompts:  98%|█████████▊| 1002/1024 [00:27<00:00, 34.46it/s, est. speed input: 36802.95 toks/s, output: 35.94 toks/s]
Processed prompts:  99%|█████████▊| 1010/1024 [00:28<00:00, 34.46it/s, est. speed input: 36790.33 toks/s, output: 35.93 toks/s]
Processed prompts:  99%|█████████▉| 1018/1024 [00:28<00:00, 35.92it/s, est. speed input: 36818.90 toks/s, output: 35.96 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:28<00:00, 35.92it/s, est. speed input: 37035.75 toks/s, output: 36.17 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:28<00:00, 36.17it/s, est. speed input: 37035.75 toks/s, output: 36.17 toks/s]
[rank0]:[W125 19:17:10.028733326 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 54.6s

测试结果:
  Requests/s:   34.49
  Tokens/s:     35352.16
  Total Reqs:   1024
  Elapsed:      29.69s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     35317.67

============================================================
[6/8] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-3B-INT8                                │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 19:17:20 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=362460) WARNING 01-25 19:17:29 [backends.py:609] Failed to read file <frozen os>
Throughput: 34.25 requests/s, 35110.68 total tokens/s, 34.25 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-25 19:17:20] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:17:20] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:17:20] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:17:20] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:17:20] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:17:20] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:17:20] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:17:20] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:17:20] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:17:20] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:17:20] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:17:20] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:17:20] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:17:20] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 19:17:24] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:17:24] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:17:24] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:17:24] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:17:24] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:17:24] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:17:24] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:17:24] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:17:24] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:17:24] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:17:24] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:17:24] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:17:24] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:17:24] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=362460) [2026-01-25 19:17:25] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=362460) [2026-01-25 19:17:25] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=362460) [2026-01-25 19:17:25] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=362460) [2026-01-25 19:17:25] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=362460) [2026-01-25 19:17:25] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=362460) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=362460) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.89it/s]
(EngineCore_DP0 pid=362460) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.89it/s]
(EngineCore_DP0 pid=362460) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=362460) 2026-01-25 19:17:35,862 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=362460) 2026-01-25 19:17:35,878 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=362460) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 1/7 [00:00<00:01,  3.06it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 3/7 [00:00<00:00,  6.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:00<00:00, 12.86it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 10.84it/s]
(EngineCore_DP0 pid=362460) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 1/5 [00:00<00:00,  7.33it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00, 18.86it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 18.46it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   4%|▎         | 73/2048 [00:00<00:02, 729.88it/s]
Adding requests:   7%|▋         | 146/2048 [00:00<00:02, 729.62it/s]
Adding requests:  11%|█         | 219/2048 [00:00<00:02, 726.07it/s]
Adding requests:  14%|█▍        | 296/2048 [00:00<00:02, 739.41it/s]
Adding requests:  18%|█▊        | 371/2048 [00:00<00:02, 742.95it/s]
Adding requests:  22%|██▏       | 448/2048 [00:00<00:02, 749.44it/s]
Adding requests:  26%|██▌       | 523/2048 [00:00<00:02, 735.80it/s]
Adding requests:  29%|██▉       | 601/2048 [00:00<00:01, 747.31it/s]
Adding requests:  33%|███▎      | 678/2048 [00:00<00:01, 752.23it/s]
Adding requests:  37%|███▋      | 755/2048 [00:01<00:01, 757.35it/s]
Adding requests:  41%|████      | 831/2048 [00:01<00:01, 739.46it/s]
Adding requests:  44%|████▍     | 908/2048 [00:01<00:01, 747.85it/s]
Adding requests:  48%|████▊     | 985/2048 [00:01<00:01, 754.11it/s]
Adding requests:  52%|█████▏    | 1063/2048 [00:01<00:01, 760.88it/s]
Adding requests:  56%|█████▌    | 1140/2048 [00:01<00:01, 748.34it/s]
Adding requests:  60%|█████▉    | 1221/2048 [00:01<00:01, 763.63it/s]
Adding requests:  63%|██████▎   | 1298/2048 [00:01<00:00, 751.38it/s]
Adding requests:  67%|██████▋   | 1376/2048 [00:01<00:00, 758.99it/s]
Adding requests:  71%|███████   | 1452/2048 [00:01<00:00, 758.34it/s]
Adding requests:  75%|███████▍  | 1530/2048 [00:02<00:00, 764.56it/s]
Adding requests:  79%|███████▊  | 1610/2048 [00:02<00:00, 772.87it/s]
Adding requests:  82%|████████▏ | 1688/2048 [00:02<00:00, 768.85it/s]
Adding requests:  86%|████████▌ | 1765/2048 [00:02<00:00, 760.61it/s]
Adding requests:  90%|████████▉ | 1842/2048 [00:02<00:00, 749.39it/s]
Adding requests:  94%|█████████▎| 1919/2048 [00:02<00:00, 755.08it/s]
Adding requests:  98%|█████████▊| 1998/2048 [00:02<00:00, 763.89it/s]
Adding requests: 100%|██████████| 2048/2048 [00:02<00:00, 754.17it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   4%|▍         | 82/2048 [00:00<00:03, 545.51it/s, est. speed input: 558658.60 toks/s, output: 545.53 toks/s]
Processed prompts:   7%|▋         | 137/2048 [00:01<00:25, 74.81it/s, est. speed input: 90650.09 toks/s, output: 88.53 toks/s]  
Processed prompts:   8%|▊         | 162/2048 [00:02<00:36, 52.30it/s, est. speed input: 66942.53 toks/s, output: 65.37 toks/s]
Processed prompts:   9%|▊         | 178/2048 [00:02<00:39, 47.82it/s, est. speed input: 61938.38 toks/s, output: 60.49 toks/s]
Processed prompts:   9%|▉         | 194/2048 [00:03<00:41, 44.28it/s, est. speed input: 58293.71 toks/s, output: 56.93 toks/s]
Processed prompts:  10%|█         | 210/2048 [00:03<00:44, 41.57it/s, est. speed input: 55519.53 toks/s, output: 54.22 toks/s]
Processed prompts:  11%|█         | 226/2048 [00:04<00:46, 39.54it/s, est. speed input: 53340.20 toks/s, output: 52.09 toks/s]
Processed prompts:  12%|█▏        | 242/2048 [00:04<00:47, 38.06it/s, est. speed input: 51582.87 toks/s, output: 50.37 toks/s]
Processed prompts:  13%|█▎        | 258/2048 [00:05<00:48, 36.98it/s, est. speed input: 50132.90 toks/s, output: 48.96 toks/s]
Processed prompts:  13%|█▎        | 274/2048 [00:05<00:48, 36.22it/s, est. speed input: 48922.07 toks/s, output: 47.78 toks/s]
Processed prompts:  14%|█▍        | 290/2048 [00:06<00:49, 35.68it/s, est. speed input: 47892.76 toks/s, output: 46.77 toks/s]
Processed prompts:  15%|█▍        | 306/2048 [00:06<00:49, 35.29it/s, est. speed input: 47005.33 toks/s, output: 45.90 toks/s]
Processed prompts:  16%|█▌        | 322/2048 [00:07<00:49, 35.02it/s, est. speed input: 46236.52 toks/s, output: 45.15 toks/s]
Processed prompts:  17%|█▋        | 338/2048 [00:07<00:49, 34.83it/s, est. speed input: 45559.89 toks/s, output: 44.49 toks/s]
Processed prompts:  17%|█▋        | 354/2048 [00:08<00:48, 34.68it/s, est. speed input: 44958.34 toks/s, output: 43.90 toks/s]
Processed prompts:  18%|█▊        | 370/2048 [00:08<00:48, 34.57it/s, est. speed input: 44422.46 toks/s, output: 43.38 toks/s]
Processed prompts:  19%|█▉        | 386/2048 [00:08<00:48, 34.50it/s, est. speed input: 43943.05 toks/s, output: 42.91 toks/s]
Processed prompts:  20%|█▉        | 402/2048 [00:09<00:47, 34.45it/s, est. speed input: 43509.19 toks/s, output: 42.49 toks/s]
Processed prompts:  20%|██        | 418/2048 [00:09<00:47, 34.41it/s, est. speed input: 43116.81 toks/s, output: 42.11 toks/s]
Processed prompts:  21%|██        | 434/2048 [00:10<00:46, 34.39it/s, est. speed input: 42759.92 toks/s, output: 41.76 toks/s]
Processed prompts:  22%|██▏       | 450/2048 [00:10<00:46, 34.37it/s, est. speed input: 42433.36 toks/s, output: 41.44 toks/s]
Processed prompts:  23%|██▎       | 466/2048 [00:11<00:46, 34.35it/s, est. speed input: 42132.30 toks/s, output: 41.14 toks/s]
Processed prompts:  24%|██▎       | 482/2048 [00:11<00:45, 34.34it/s, est. speed input: 41856.18 toks/s, output: 40.88 toks/s]
Processed prompts:  24%|██▍       | 498/2048 [00:12<00:45, 34.33it/s, est. speed input: 41599.76 toks/s, output: 40.62 toks/s]
Processed prompts:  25%|██▌       | 514/2048 [00:12<00:44, 34.32it/s, est. speed input: 41362.88 toks/s, output: 40.39 toks/s]
Processed prompts:  26%|██▌       | 530/2048 [00:13<00:44, 34.32it/s, est. speed input: 41142.90 toks/s, output: 40.18 toks/s]
Processed prompts:  27%|██▋       | 546/2048 [00:13<00:43, 34.32it/s, est. speed input: 40937.37 toks/s, output: 39.98 toks/s]
Processed prompts:  27%|██▋       | 562/2048 [00:14<00:43, 34.31it/s, est. speed input: 40745.61 toks/s, output: 39.79 toks/s]
Processed prompts:  28%|██▊       | 578/2048 [00:14<00:42, 34.31it/s, est. speed input: 40565.60 toks/s, output: 39.61 toks/s]
Processed prompts:  29%|██▉       | 594/2048 [00:15<00:42, 34.30it/s, est. speed input: 40396.61 toks/s, output: 39.45 toks/s]
Processed prompts:  30%|██▉       | 610/2048 [00:15<00:41, 34.30it/s, est. speed input: 40238.15 toks/s, output: 39.30 toks/s]
Processed prompts:  31%|███       | 626/2048 [00:15<00:41, 34.30it/s, est. speed input: 40088.73 toks/s, output: 39.15 toks/s]
Processed prompts:  31%|███▏      | 642/2048 [00:16<00:40, 34.30it/s, est. speed input: 39948.07 toks/s, output: 39.01 toks/s]
Processed prompts:  32%|███▏      | 658/2048 [00:16<00:40, 34.29it/s, est. speed input: 39814.50 toks/s, output: 38.88 toks/s]
Processed prompts:  33%|███▎      | 674/2048 [00:17<00:40, 34.29it/s, est. speed input: 39687.83 toks/s, output: 38.76 toks/s]
Processed prompts:  34%|███▎      | 690/2048 [00:17<00:39, 34.29it/s, est. speed input: 39568.67 toks/s, output: 38.64 toks/s]
Processed prompts:  34%|███▍      | 706/2048 [00:18<00:39, 34.29it/s, est. speed input: 39454.99 toks/s, output: 38.53 toks/s]
Processed prompts:  35%|███▌      | 722/2048 [00:18<00:38, 34.29it/s, est. speed input: 39347.50 toks/s, output: 38.43 toks/s]
Processed prompts:  36%|███▌      | 738/2048 [00:19<00:38, 34.28it/s, est. speed input: 39244.11 toks/s, output: 38.32 toks/s]
Processed prompts:  37%|███▋      | 754/2048 [00:19<00:37, 34.29it/s, est. speed input: 39146.62 toks/s, output: 38.23 toks/s]
Processed prompts:  38%|███▊      | 770/2048 [00:20<00:37, 34.28it/s, est. speed input: 39052.96 toks/s, output: 38.14 toks/s]
Processed prompts:  38%|███▊      | 786/2048 [00:20<00:36, 34.28it/s, est. speed input: 38963.22 toks/s, output: 38.05 toks/s]
Processed prompts:  39%|███▉      | 802/2048 [00:21<00:36, 34.28it/s, est. speed input: 38877.76 toks/s, output: 37.97 toks/s]
Processed prompts:  40%|███▉      | 818/2048 [00:21<00:35, 34.27it/s, est. speed input: 38795.57 toks/s, output: 37.89 toks/s]
Processed prompts:  41%|████      | 834/2048 [00:22<00:35, 34.27it/s, est. speed input: 38717.45 toks/s, output: 37.81 toks/s]
Processed prompts:  42%|████▏     | 850/2048 [00:22<00:34, 34.27it/s, est. speed input: 38642.38 toks/s, output: 37.74 toks/s]
Processed prompts:  42%|████▏     | 866/2048 [00:22<00:34, 34.27it/s, est. speed input: 38570.33 toks/s, output: 37.67 toks/s]
Processed prompts:  43%|████▎     | 882/2048 [00:23<00:34, 34.27it/s, est. speed input: 38500.90 toks/s, output: 37.60 toks/s]
Processed prompts:  44%|████▍     | 898/2048 [00:23<00:33, 34.27it/s, est. speed input: 38434.40 toks/s, output: 37.53 toks/s]
Processed prompts:  45%|████▍     | 914/2048 [00:24<00:33, 34.26it/s, est. speed input: 38370.01 toks/s, output: 37.47 toks/s]
Processed prompts:  45%|████▌     | 930/2048 [00:24<00:32, 34.27it/s, est. speed input: 38308.45 toks/s, output: 37.41 toks/s]
Processed prompts:  46%|████▌     | 946/2048 [00:25<00:32, 34.26it/s, est. speed input: 38248.52 toks/s, output: 37.35 toks/s]
Processed prompts:  47%|████▋     | 962/2048 [00:25<00:31, 34.26it/s, est. speed input: 38191.46 toks/s, output: 37.30 toks/s]
Processed prompts:  48%|████▊     | 978/2048 [00:26<00:31, 34.25it/s, est. speed input: 38135.55 toks/s, output: 37.24 toks/s]
Processed prompts:  49%|████▊     | 994/2048 [00:26<00:30, 34.25it/s, est. speed input: 38081.88 toks/s, output: 37.19 toks/s]
Processed prompts:  49%|████▉     | 1010/2048 [00:27<00:30, 34.25it/s, est. speed input: 38030.36 toks/s, output: 37.14 toks/s]
Processed prompts:  50%|█████     | 1026/2048 [00:27<00:29, 34.25it/s, est. speed input: 37979.99 toks/s, output: 37.09 toks/s]
Processed prompts:  51%|█████     | 1042/2048 [00:28<00:29, 34.26it/s, est. speed input: 37932.36 toks/s, output: 37.04 toks/s]
Processed prompts:  52%|█████▏    | 1058/2048 [00:28<00:28, 34.25it/s, est. speed input: 37885.54 toks/s, output: 37.00 toks/s]
Processed prompts:  52%|█████▏    | 1074/2048 [00:29<00:28, 34.25it/s, est. speed input: 37840.21 toks/s, output: 36.95 toks/s]
Processed prompts:  53%|█████▎    | 1090/2048 [00:29<00:27, 34.24it/s, est. speed input: 37796.06 toks/s, output: 36.91 toks/s]
Processed prompts:  54%|█████▍    | 1106/2048 [00:29<00:27, 34.25it/s, est. speed input: 37753.83 toks/s, output: 36.87 toks/s]
Processed prompts:  55%|█████▍    | 1122/2048 [00:30<00:27, 34.24it/s, est. speed input: 37712.38 toks/s, output: 36.83 toks/s]
Processed prompts:  56%|█████▌    | 1138/2048 [00:30<00:26, 34.24it/s, est. speed input: 37672.23 toks/s, output: 36.79 toks/s]
Processed prompts:  56%|█████▋    | 1154/2048 [00:31<00:26, 34.24it/s, est. speed input: 37633.15 toks/s, output: 36.75 toks/s]
Processed prompts:  57%|█████▋    | 1170/2048 [00:31<00:25, 34.24it/s, est. speed input: 37595.37 toks/s, output: 36.71 toks/s]
Processed prompts:  58%|█████▊    | 1186/2048 [00:32<00:25, 34.23it/s, est. speed input: 37558.45 toks/s, output: 36.68 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [00:32<00:24, 34.24it/s, est. speed input: 37523.11 toks/s, output: 36.64 toks/s]
Processed prompts:  59%|█████▉    | 1218/2048 [00:33<00:24, 34.23it/s, est. speed input: 37488.26 toks/s, output: 36.61 toks/s]
Processed prompts:  60%|██████    | 1234/2048 [00:33<00:23, 34.23it/s, est. speed input: 37454.44 toks/s, output: 36.58 toks/s]
Processed prompts:  61%|██████    | 1250/2048 [00:34<00:23, 34.24it/s, est. speed input: 37421.82 toks/s, output: 36.54 toks/s]
Processed prompts:  62%|██████▏   | 1266/2048 [00:34<00:22, 34.23it/s, est. speed input: 37389.62 toks/s, output: 36.51 toks/s]
Processed prompts:  63%|██████▎   | 1282/2048 [00:35<00:22, 34.22it/s, est. speed input: 37358.31 toks/s, output: 36.48 toks/s]
Processed prompts:  63%|██████▎   | 1298/2048 [00:35<00:21, 34.21it/s, est. speed input: 37327.46 toks/s, output: 36.45 toks/s]
Processed prompts:  64%|██████▍   | 1314/2048 [00:36<00:21, 34.22it/s, est. speed input: 37298.11 toks/s, output: 36.42 toks/s]
Processed prompts:  65%|██████▍   | 1330/2048 [00:36<00:20, 34.22it/s, est. speed input: 37269.10 toks/s, output: 36.40 toks/s]
Processed prompts:  66%|██████▌   | 1346/2048 [00:37<00:20, 34.22it/s, est. speed input: 37241.10 toks/s, output: 36.37 toks/s]
Processed prompts:  67%|██████▋   | 1362/2048 [00:37<00:20, 34.22it/s, est. speed input: 37213.50 toks/s, output: 36.34 toks/s]
Processed prompts:  67%|██████▋   | 1378/2048 [00:37<00:19, 34.22it/s, est. speed input: 37186.71 toks/s, output: 36.32 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [00:38<00:19, 34.22it/s, est. speed input: 37160.50 toks/s, output: 36.29 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [00:38<00:18, 34.21it/s, est. speed input: 37134.83 toks/s, output: 36.26 toks/s]
Processed prompts:  70%|██████▉   | 1426/2048 [00:39<00:18, 34.21it/s, est. speed input: 37109.87 toks/s, output: 36.24 toks/s]
Processed prompts:  70%|███████   | 1442/2048 [00:39<00:17, 34.22it/s, est. speed input: 37085.63 toks/s, output: 36.22 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [00:40<00:17, 34.22it/s, est. speed input: 37061.99 toks/s, output: 36.19 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [00:40<00:16, 34.21it/s, est. speed input: 37038.52 toks/s, output: 36.17 toks/s]
Processed prompts:  73%|███████▎  | 1490/2048 [00:41<00:16, 34.21it/s, est. speed input: 37015.74 toks/s, output: 36.15 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [00:41<00:15, 34.21it/s, est. speed input: 36993.39 toks/s, output: 36.13 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [00:42<00:15, 34.22it/s, est. speed input: 36971.83 toks/s, output: 36.11 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [00:42<00:14, 34.21it/s, est. speed input: 36950.36 toks/s, output: 36.08 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [00:43<00:14, 34.21it/s, est. speed input: 36929.67 toks/s, output: 36.06 toks/s]
Processed prompts:  77%|███████▋  | 1570/2048 [00:43<00:13, 34.21it/s, est. speed input: 36909.27 toks/s, output: 36.04 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [00:44<00:13, 34.21it/s, est. speed input: 36889.30 toks/s, output: 36.02 toks/s]
Processed prompts:  78%|███████▊  | 1602/2048 [00:44<00:13, 34.20it/s, est. speed input: 36869.54 toks/s, output: 36.01 toks/s]
Processed prompts:  79%|███████▉  | 1618/2048 [00:44<00:12, 34.21it/s, est. speed input: 36850.58 toks/s, output: 35.99 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [00:45<00:12, 34.20it/s, est. speed input: 36831.32 toks/s, output: 35.97 toks/s]
Processed prompts:  81%|████████  | 1650/2048 [00:45<00:11, 34.20it/s, est. speed input: 36812.99 toks/s, output: 35.95 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [00:46<00:11, 34.20it/s, est. speed input: 36794.78 toks/s, output: 35.93 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [00:46<00:10, 34.20it/s, est. speed input: 36777.14 toks/s, output: 35.92 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [00:47<00:10, 34.19it/s, est. speed input: 36759.49 toks/s, output: 35.90 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [00:47<00:09, 34.20it/s, est. speed input: 36742.49 toks/s, output: 35.88 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [00:48<00:09, 34.19it/s, est. speed input: 36725.48 toks/s, output: 35.86 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [00:48<00:08, 34.19it/s, est. speed input: 36709.22 toks/s, output: 35.85 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [00:49<00:08, 34.20it/s, est. speed input: 36693.28 toks/s, output: 35.83 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [00:49<00:07, 34.19it/s, est. speed input: 36677.22 toks/s, output: 35.82 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [00:50<00:07, 34.19it/s, est. speed input: 36661.52 toks/s, output: 35.80 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [00:50<00:06, 34.19it/s, est. speed input: 36646.17 toks/s, output: 35.79 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [00:51<00:06, 34.18it/s, est. speed input: 36631.08 toks/s, output: 35.77 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [00:51<00:06, 34.19it/s, est. speed input: 36616.41 toks/s, output: 35.76 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [00:51<00:05, 34.18it/s, est. speed input: 36601.79 toks/s, output: 35.74 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [00:52<00:04, 34.80it/s, est. speed input: 36606.87 toks/s, output: 35.75 toks/s]
Processed prompts:  92%|█████████▏| 1890/2048 [00:52<00:04, 34.62it/s, est. speed input: 36592.85 toks/s, output: 35.74 toks/s]
Processed prompts:  93%|█████████▎| 1906/2048 [00:53<00:04, 34.48it/s, est. speed input: 36578.67 toks/s, output: 35.72 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [00:53<00:03, 34.39it/s, est. speed input: 36564.98 toks/s, output: 35.71 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [00:54<00:03, 34.32it/s, est. speed input: 36551.39 toks/s, output: 35.69 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [00:54<00:02, 34.29it/s, est. speed input: 36538.36 toks/s, output: 35.68 toks/s]
Processed prompts:  96%|█████████▌| 1970/2048 [00:55<00:02, 34.26it/s, est. speed input: 36525.43 toks/s, output: 35.67 toks/s]
Processed prompts:  97%|█████████▋| 1986/2048 [00:55<00:01, 34.24it/s, est. speed input: 36512.69 toks/s, output: 35.66 toks/s]
Processed prompts:  98%|█████████▊| 2002/2048 [00:56<00:01, 34.22it/s, est. speed input: 36500.03 toks/s, output: 35.64 toks/s]
Processed prompts:  99%|█████████▊| 2018/2048 [00:56<00:00, 34.20it/s, est. speed input: 36487.53 toks/s, output: 35.63 toks/s]
Processed prompts:  99%|█████████▉| 2034/2048 [00:57<00:00, 34.88it/s, est. speed input: 36494.86 toks/s, output: 35.64 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:57<00:00, 34.88it/s, est. speed input: 36745.95 toks/s, output: 35.88 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:57<00:00, 35.88it/s, est. speed input: 36745.95 toks/s, output: 35.88 toks/s]
[rank0]:[W125 19:18:37.554754817 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 87.5s

测试结果:
  Requests/s:   34.25
  Tokens/s:     35110.68
  Total Reqs:   2048
  Elapsed:      59.79s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     35076.43

============================================================
[7/8] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-3B-INT8                                │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 19:18:53 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=364007) WARNING 01-25 19:19:02 [backends.py:609] Failed to read file <frozen os>
Throughput: 34.21 requests/s, 35067.14 total tokens/s, 34.21 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-25 19:18:53] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:18:53] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:18:53] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:18:53] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:18:53] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:18:53] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:18:53] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:18:53] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:18:53] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:18:53] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:18:53] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:18:53] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:18:53] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:18:53] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 19:18:57] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:18:57] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:18:57] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:18:57] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:18:57] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:18:57] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:18:57] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:18:57] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:18:57] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:18:57] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:18:57] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:18:57] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:18:57] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:18:57] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=364007) [2026-01-25 19:18:58] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=364007) [2026-01-25 19:18:58] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=364007) [2026-01-25 19:18:58] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=364007) [2026-01-25 19:18:58] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=364007) [2026-01-25 19:18:58] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=364007) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=364007) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.90it/s]
(EngineCore_DP0 pid=364007) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.90it/s]
(EngineCore_DP0 pid=364007) 
(EngineCore_DP0 pid=364007) [rank0]:W0125 19:19:05.623000 364007 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=364007) [rank0]:W0125 19:19:06.309000 364007 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=364007) [rank0]:W0125 19:19:07.527000 364007 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=364007) [rank0]:W0125 19:19:07.601000 364007 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=364007) 2026-01-25 19:19:10,697 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=364007) 2026-01-25 19:19:10,713 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=364007) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▉         | 1/11 [00:00<00:06,  1.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:00<00:01,  6.44it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▎   | 7/11 [00:00<00:00, 11.04it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 10/11 [00:01<00:00, 14.14it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00, 10.34it/s]
(EngineCore_DP0 pid=364007) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 1/7 [00:00<00:00,  7.24it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 3/7 [00:00<00:00, 12.35it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00, 16.61it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 15.05it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 72/4096 [00:00<00:05, 718.63it/s]
Adding requests:   4%|▎         | 148/4096 [00:00<00:05, 742.09it/s]
Adding requests:   5%|▌         | 225/4096 [00:00<00:05, 752.03it/s]
Adding requests:   7%|▋         | 302/4096 [00:00<00:05, 756.73it/s]
Adding requests:   9%|▉         | 379/4096 [00:00<00:04, 758.98it/s]
Adding requests:  11%|█         | 455/4096 [00:00<00:04, 750.36it/s]
Adding requests:  13%|█▎        | 531/4096 [00:00<00:04, 738.56it/s]
Adding requests:  15%|█▍        | 610/4096 [00:00<00:04, 752.07it/s]
Adding requests:  17%|█▋        | 691/4096 [00:00<00:04, 769.14it/s]
Adding requests:  19%|█▉        | 768/4096 [00:01<00:04, 762.12it/s]
Adding requests:  21%|██        | 845/4096 [00:01<00:04, 749.31it/s]
Adding requests:  23%|██▎       | 925/4096 [00:01<00:04, 763.92it/s]
Adding requests:  24%|██▍       | 1002/4096 [00:01<00:04, 750.17it/s]
Adding requests:  26%|██▋       | 1078/4096 [00:01<00:04, 750.38it/s]
Adding requests:  28%|██▊       | 1154/4096 [00:01<00:03, 751.81it/s]
Adding requests:  30%|███       | 1233/4096 [00:01<00:03, 763.02it/s]
Adding requests:  32%|███▏      | 1310/4096 [00:01<00:03, 745.63it/s]
Adding requests:  34%|███▍      | 1389/4096 [00:01<00:03, 756.93it/s]
Adding requests:  36%|███▌      | 1467/4096 [00:01<00:03, 763.24it/s]
Adding requests:  38%|███▊      | 1545/4096 [00:02<00:03, 766.64it/s]
Adding requests:  40%|███▉      | 1624/4096 [00:02<00:03, 771.67it/s]
Adding requests:  42%|████▏     | 1702/4096 [00:02<00:03, 766.42it/s]
Adding requests:  43%|████▎     | 1779/4096 [00:02<00:03, 744.08it/s]
Adding requests:  45%|████▌     | 1858/4096 [00:02<00:02, 755.08it/s]
Adding requests:  47%|████▋     | 1936/4096 [00:02<00:02, 760.74it/s]
Adding requests:  49%|████▉     | 2013/4096 [00:02<00:02, 760.00it/s]
Adding requests:  51%|█████     | 2091/4096 [00:02<00:02, 763.63it/s]
Adding requests:  53%|█████▎    | 2168/4096 [00:02<00:02, 759.70it/s]
Adding requests:  55%|█████▍    | 2245/4096 [00:02<00:02, 746.46it/s]
Adding requests:  57%|█████▋    | 2321/4096 [00:03<00:02, 748.92it/s]
Adding requests:  59%|█████▊    | 2399/4096 [00:03<00:02, 756.11it/s]
Adding requests:  60%|██████    | 2477/4096 [00:03<00:02, 760.64it/s]
Adding requests:  62%|██████▏   | 2554/4096 [00:03<00:02, 759.86it/s]
Adding requests:  64%|██████▍   | 2631/4096 [00:03<00:01, 760.89it/s]
Adding requests:  66%|██████▌   | 2708/4096 [00:03<00:01, 761.23it/s]
Adding requests:  68%|██████▊   | 2787/4096 [00:03<00:01, 766.43it/s]
Adding requests:  70%|██████▉   | 2866/4096 [00:03<00:01, 771.14it/s]
Adding requests:  72%|███████▏  | 2944/4096 [00:03<00:01, 765.33it/s]
Adding requests:  74%|███████▍  | 3022/4096 [00:03<00:01, 769.05it/s]
Adding requests:  76%|███████▌  | 3100/4096 [00:04<00:01, 771.06it/s]
Adding requests:  78%|███████▊  | 3180/4096 [00:04<00:01, 777.25it/s]
Adding requests:  80%|███████▉  | 3259/4096 [00:04<00:01, 778.92it/s]
Adding requests:  82%|████████▏ | 3339/4096 [00:04<00:00, 783.34it/s]
Adding requests:  83%|████████▎ | 3418/4096 [00:04<00:00, 782.30it/s]
Adding requests:  85%|████████▌ | 3497/4096 [00:04<00:00, 774.00it/s]
Adding requests:  87%|████████▋ | 3575/4096 [00:04<00:00, 759.07it/s]
Adding requests:  89%|████████▉ | 3651/4096 [00:04<00:00, 755.79it/s]
Adding requests:  91%|█████████ | 3729/4096 [00:04<00:00, 759.92it/s]
Adding requests:  93%|█████████▎| 3808/4096 [00:05<00:00, 765.96it/s]
Adding requests:  95%|█████████▍| 3887/4096 [00:05<00:00, 771.00it/s]
Adding requests:  97%|█████████▋| 3965/4096 [00:05<00:00, 773.00it/s]
Adding requests:  99%|█████████▊| 4043/4096 [00:05<00:00, 756.88it/s]
Adding requests: 100%|██████████| 4096/4096 [00:05<00:00, 760.91it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   4%|▍         | 162/4096 [00:00<00:06, 585.95it/s, est. speed input: 600046.14 toks/s, output: 585.96 toks/s]
Processed prompts:   5%|▌         | 221/4096 [00:01<00:25, 153.29it/s, est. speed input: 187397.05 toks/s, output: 183.00 toks/s]
Processed prompts:   6%|▌         | 249/4096 [00:02<00:44, 86.54it/s, est. speed input: 119233.46 toks/s, output: 116.44 toks/s] 
Processed prompts:   6%|▋         | 266/4096 [00:03<01:07, 56.75it/s, est. speed input: 88743.07 toks/s, output: 86.66 toks/s]  
Processed prompts:   7%|▋         | 290/4096 [00:04<01:24, 44.85it/s, est. speed input: 74231.10 toks/s, output: 72.49 toks/s]
Processed prompts:   8%|▊         | 322/4096 [00:04<01:31, 41.13it/s, est. speed input: 66855.62 toks/s, output: 65.29 toks/s]
Processed prompts:   9%|▊         | 354/4096 [00:05<01:36, 38.86it/s, est. speed input: 61824.42 toks/s, output: 60.38 toks/s]
Processed prompts:   9%|▉         | 386/4096 [00:06<01:39, 37.39it/s, est. speed input: 58165.49 toks/s, output: 56.80 toks/s]
Processed prompts:  10%|█         | 418/4096 [00:07<01:40, 36.42it/s, est. speed input: 55390.22 toks/s, output: 54.09 toks/s]
Processed prompts:  11%|█         | 450/4096 [00:08<01:41, 35.77it/s, est. speed input: 53211.72 toks/s, output: 51.96 toks/s]
Processed prompts:  12%|█▏        | 482/4096 [00:09<01:42, 35.32it/s, est. speed input: 51454.34 toks/s, output: 50.25 toks/s]
Processed prompts:  13%|█▎        | 514/4096 [00:10<01:42, 35.01it/s, est. speed input: 50009.22 toks/s, output: 48.84 toks/s]
Processed prompts:  13%|█▎        | 546/4096 [00:11<01:42, 34.80it/s, est. speed input: 48798.90 toks/s, output: 47.66 toks/s]
Processed prompts:  14%|█▍        | 578/4096 [00:12<01:41, 34.65it/s, est. speed input: 47769.93 toks/s, output: 46.65 toks/s]
Processed prompts:  15%|█▍        | 610/4096 [00:13<01:40, 34.54it/s, est. speed input: 46884.10 toks/s, output: 45.79 toks/s]
Processed prompts:  16%|█▌        | 642/4096 [00:14<01:40, 34.48it/s, est. speed input: 46115.90 toks/s, output: 45.04 toks/s]
Processed prompts:  16%|█▋        | 674/4096 [00:15<01:39, 34.42it/s, est. speed input: 45439.69 toks/s, output: 44.37 toks/s]
Processed prompts:  17%|█▋        | 706/4096 [00:16<01:38, 34.39it/s, est. speed input: 44843.77 toks/s, output: 43.79 toks/s]
Processed prompts:  18%|█▊        | 738/4096 [00:17<01:37, 34.36it/s, est. speed input: 44311.10 toks/s, output: 43.27 toks/s]
Processed prompts:  19%|█▉        | 770/4096 [00:17<01:36, 34.33it/s, est. speed input: 43833.12 toks/s, output: 42.81 toks/s]
Processed prompts:  20%|█▉        | 802/4096 [00:18<01:35, 34.32it/s, est. speed input: 43402.52 toks/s, output: 42.39 toks/s]
Processed prompts:  20%|██        | 834/4096 [00:19<01:35, 34.30it/s, est. speed input: 43011.56 toks/s, output: 42.00 toks/s]
Processed prompts:  21%|██        | 866/4096 [00:20<01:34, 34.29it/s, est. speed input: 42656.09 toks/s, output: 41.66 toks/s]
Processed prompts:  22%|██▏       | 898/4096 [00:21<01:33, 34.29it/s, est. speed input: 42330.63 toks/s, output: 41.34 toks/s]
Processed prompts:  23%|██▎       | 930/4096 [00:22<01:32, 34.28it/s, est. speed input: 42032.71 toks/s, output: 41.05 toks/s]
Processed prompts:  23%|██▎       | 962/4096 [00:23<01:31, 34.28it/s, est. speed input: 41757.45 toks/s, output: 40.78 toks/s]
Processed prompts:  24%|██▍       | 994/4096 [00:24<01:30, 34.27it/s, est. speed input: 41502.88 toks/s, output: 40.53 toks/s]
Processed prompts:  25%|██▌       | 1026/4096 [00:25<01:29, 34.27it/s, est. speed input: 41267.44 toks/s, output: 40.30 toks/s]
Processed prompts:  26%|██▌       | 1058/4096 [00:26<01:28, 34.26it/s, est. speed input: 41048.38 toks/s, output: 40.09 toks/s]
Processed prompts:  27%|██▋       | 1090/4096 [00:27<01:27, 34.26it/s, est. speed input: 40843.99 toks/s, output: 39.89 toks/s]
Processed prompts:  27%|██▋       | 1122/4096 [00:28<01:26, 34.26it/s, est. speed input: 40653.41 toks/s, output: 39.70 toks/s]
Processed prompts:  28%|██▊       | 1154/4096 [00:29<01:25, 34.26it/s, est. speed input: 40475.00 toks/s, output: 39.53 toks/s]
Processed prompts:  29%|██▉       | 1186/4096 [00:30<01:24, 34.25it/s, est. speed input: 40307.36 toks/s, output: 39.36 toks/s]
Processed prompts:  30%|██▉       | 1218/4096 [00:31<01:24, 34.26it/s, est. speed input: 40150.34 toks/s, output: 39.21 toks/s]
Processed prompts:  31%|███       | 1250/4096 [00:31<01:23, 34.25it/s, est. speed input: 40001.67 toks/s, output: 39.06 toks/s]
Processed prompts:  31%|███▏      | 1282/4096 [00:32<01:22, 34.25it/s, est. speed input: 39861.26 toks/s, output: 38.93 toks/s]
Processed prompts:  32%|███▏      | 1314/4096 [00:33<01:21, 34.24it/s, est. speed input: 39728.55 toks/s, output: 38.80 toks/s]
Processed prompts:  33%|███▎      | 1346/4096 [00:34<01:20, 34.24it/s, est. speed input: 39603.39 toks/s, output: 38.68 toks/s]
Processed prompts:  34%|███▎      | 1378/4096 [00:35<01:19, 34.24it/s, est. speed input: 39483.96 toks/s, output: 38.56 toks/s]
Processed prompts:  34%|███▍      | 1410/4096 [00:36<01:18, 34.23it/s, est. speed input: 39370.96 toks/s, output: 38.45 toks/s]
Processed prompts:  35%|███▌      | 1442/4096 [00:37<01:17, 34.23it/s, est. speed input: 39263.51 toks/s, output: 38.34 toks/s]
Processed prompts:  36%|███▌      | 1474/4096 [00:38<01:16, 34.23it/s, est. speed input: 39161.08 toks/s, output: 38.24 toks/s]
Processed prompts:  37%|███▋      | 1506/4096 [00:39<01:15, 34.23it/s, est. speed input: 39063.85 toks/s, output: 38.15 toks/s]
Processed prompts:  38%|███▊      | 1538/4096 [00:40<01:14, 34.23it/s, est. speed input: 38970.65 toks/s, output: 38.06 toks/s]
Processed prompts:  38%|███▊      | 1570/4096 [00:41<01:13, 34.22it/s, est. speed input: 38881.41 toks/s, output: 37.97 toks/s]
Processed prompts:  39%|███▉      | 1602/4096 [00:42<01:12, 34.22it/s, est. speed input: 38796.29 toks/s, output: 37.89 toks/s]
Processed prompts:  40%|███▉      | 1634/4096 [00:43<01:11, 34.21it/s, est. speed input: 38714.69 toks/s, output: 37.81 toks/s]
Processed prompts:  41%|████      | 1666/4096 [00:44<01:11, 34.21it/s, est. speed input: 38636.61 toks/s, output: 37.73 toks/s]
Processed prompts:  41%|████▏     | 1698/4096 [00:45<01:10, 34.21it/s, est. speed input: 38561.95 toks/s, output: 37.66 toks/s]
Processed prompts:  42%|████▏     | 1730/4096 [00:46<01:09, 34.21it/s, est. speed input: 38490.06 toks/s, output: 37.59 toks/s]
Processed prompts:  43%|████▎     | 1762/4096 [00:46<01:08, 34.21it/s, est. speed input: 38420.94 toks/s, output: 37.52 toks/s]
Processed prompts:  44%|████▍     | 1794/4096 [00:47<01:07, 34.20it/s, est. speed input: 38354.30 toks/s, output: 37.46 toks/s]
Processed prompts:  45%|████▍     | 1826/4096 [00:48<01:06, 34.20it/s, est. speed input: 38290.26 toks/s, output: 37.39 toks/s]
Processed prompts:  45%|████▌     | 1858/4096 [00:49<01:04, 34.50it/s, est. speed input: 38249.58 toks/s, output: 37.35 toks/s]
Processed prompts:  46%|████▌     | 1890/4096 [00:50<01:04, 34.40it/s, est. speed input: 38189.70 toks/s, output: 37.29 toks/s]
Processed prompts:  47%|████▋     | 1922/4096 [00:51<01:03, 34.34it/s, est. speed input: 38131.98 toks/s, output: 37.24 toks/s]
Processed prompts:  48%|████▊     | 1954/4096 [00:52<01:02, 34.30it/s, est. speed input: 38076.64 toks/s, output: 37.18 toks/s]
Processed prompts:  48%|████▊     | 1986/4096 [00:53<01:01, 34.27it/s, est. speed input: 38023.03 toks/s, output: 37.13 toks/s]
Processed prompts:  49%|████▉     | 2018/4096 [00:54<01:00, 34.24it/s, est. speed input: 37970.96 toks/s, output: 37.08 toks/s]
Processed prompts:  50%|█████     | 2050/4096 [00:55<00:59, 34.23it/s, est. speed input: 37920.97 toks/s, output: 37.03 toks/s]
Processed prompts:  51%|█████     | 2082/4096 [00:56<00:58, 34.21it/s, est. speed input: 37872.55 toks/s, output: 36.98 toks/s]
Processed prompts:  52%|█████▏    | 2114/4096 [00:57<00:57, 34.21it/s, est. speed input: 37825.85 toks/s, output: 36.94 toks/s]
Processed prompts:  52%|█████▏    | 2146/4096 [00:58<00:57, 34.20it/s, est. speed input: 37780.19 toks/s, output: 36.89 toks/s]
Processed prompts:  53%|█████▎    | 2178/4096 [00:59<00:56, 34.19it/s, est. speed input: 37736.22 toks/s, output: 36.85 toks/s]
Processed prompts:  54%|█████▍    | 2210/4096 [01:00<00:55, 34.18it/s, est. speed input: 37693.12 toks/s, output: 36.81 toks/s]
Processed prompts:  55%|█████▍    | 2242/4096 [01:00<00:54, 34.18it/s, est. speed input: 37651.54 toks/s, output: 36.77 toks/s]
Processed prompts:  56%|█████▌    | 2274/4096 [01:01<00:53, 34.17it/s, est. speed input: 37611.27 toks/s, output: 36.73 toks/s]
Processed prompts:  56%|█████▋    | 2306/4096 [01:02<00:52, 34.17it/s, est. speed input: 37572.15 toks/s, output: 36.69 toks/s]
Processed prompts:  57%|█████▋    | 2338/4096 [01:03<00:51, 34.17it/s, est. speed input: 37534.20 toks/s, output: 36.65 toks/s]
Processed prompts:  58%|█████▊    | 2370/4096 [01:04<00:50, 34.17it/s, est. speed input: 37497.21 toks/s, output: 36.62 toks/s]
Processed prompts:  59%|█████▊    | 2402/4096 [01:05<00:49, 34.16it/s, est. speed input: 37461.29 toks/s, output: 36.58 toks/s]
Processed prompts:  59%|█████▉    | 2434/4096 [01:06<00:48, 34.16it/s, est. speed input: 37426.42 toks/s, output: 36.55 toks/s]
Processed prompts:  60%|██████    | 2466/4096 [01:07<00:47, 34.16it/s, est. speed input: 37392.50 toks/s, output: 36.52 toks/s]
Processed prompts:  61%|██████    | 2498/4096 [01:08<00:46, 34.17it/s, est. speed input: 37359.67 toks/s, output: 36.48 toks/s]
Processed prompts:  62%|██████▏   | 2530/4096 [01:09<00:45, 34.16it/s, est. speed input: 37327.37 toks/s, output: 36.45 toks/s]
Processed prompts:  63%|██████▎   | 2562/4096 [01:10<00:44, 34.16it/s, est. speed input: 37296.16 toks/s, output: 36.42 toks/s]
Processed prompts:  63%|██████▎   | 2594/4096 [01:11<00:43, 34.16it/s, est. speed input: 37265.72 toks/s, output: 36.39 toks/s]
Processed prompts:  64%|██████▍   | 2626/4096 [01:12<00:43, 34.16it/s, est. speed input: 37236.00 toks/s, output: 36.36 toks/s]
Processed prompts:  65%|██████▍   | 2658/4096 [01:13<00:42, 34.16it/s, est. speed input: 37207.18 toks/s, output: 36.34 toks/s]
Processed prompts:  66%|██████▌   | 2690/4096 [01:14<00:41, 34.16it/s, est. speed input: 37179.17 toks/s, output: 36.31 toks/s]
Processed prompts:  66%|██████▋   | 2722/4096 [01:15<00:40, 34.16it/s, est. speed input: 37151.52 toks/s, output: 36.28 toks/s]
Processed prompts:  67%|██████▋   | 2754/4096 [01:15<00:39, 34.16it/s, est. speed input: 37124.83 toks/s, output: 36.25 toks/s]
Processed prompts:  68%|██████▊   | 2786/4096 [01:16<00:38, 34.15it/s, est. speed input: 37098.42 toks/s, output: 36.23 toks/s]
Processed prompts:  69%|██████▉   | 2818/4096 [01:17<00:37, 34.16it/s, est. speed input: 37073.01 toks/s, output: 36.20 toks/s]
Processed prompts:  70%|██████▉   | 2850/4096 [01:18<00:36, 34.15it/s, est. speed input: 37047.92 toks/s, output: 36.18 toks/s]
Processed prompts:  70%|███████   | 2882/4096 [01:19<00:35, 34.15it/s, est. speed input: 37023.56 toks/s, output: 36.16 toks/s]
Processed prompts:  71%|███████   | 2914/4096 [01:20<00:34, 34.15it/s, est. speed input: 36999.71 toks/s, output: 36.13 toks/s]
Processed prompts:  72%|███████▏  | 2946/4096 [01:21<00:33, 34.15it/s, est. speed input: 36976.30 toks/s, output: 36.11 toks/s]
Processed prompts:  73%|███████▎  | 2978/4096 [01:22<00:32, 34.15it/s, est. speed input: 36953.32 toks/s, output: 36.09 toks/s]
Processed prompts:  73%|███████▎  | 3010/4096 [01:23<00:31, 34.14it/s, est. speed input: 36930.78 toks/s, output: 36.07 toks/s]
Processed prompts:  74%|███████▍  | 3042/4096 [01:24<00:30, 34.14it/s, est. speed input: 36909.08 toks/s, output: 36.04 toks/s]
Processed prompts:  75%|███████▌  | 3074/4096 [01:25<00:29, 34.15it/s, est. speed input: 36887.71 toks/s, output: 36.02 toks/s]
Processed prompts:  76%|███████▌  | 3106/4096 [01:26<00:28, 34.15it/s, est. speed input: 36866.87 toks/s, output: 36.00 toks/s]
Processed prompts:  77%|███████▋  | 3138/4096 [01:27<00:28, 34.14it/s, est. speed input: 36846.26 toks/s, output: 35.98 toks/s]
Processed prompts:  77%|███████▋  | 3170/4096 [01:28<00:27, 34.14it/s, est. speed input: 36826.22 toks/s, output: 35.96 toks/s]
Processed prompts:  78%|███████▊  | 3202/4096 [01:29<00:26, 34.15it/s, est. speed input: 36806.76 toks/s, output: 35.94 toks/s]
Processed prompts:  79%|███████▉  | 3234/4096 [01:30<00:25, 34.14it/s, est. speed input: 36787.43 toks/s, output: 35.93 toks/s]
Processed prompts:  80%|███████▉  | 3266/4096 [01:30<00:24, 34.14it/s, est. speed input: 36768.72 toks/s, output: 35.91 toks/s]
Processed prompts:  81%|████████  | 3298/4096 [01:31<00:23, 34.14it/s, est. speed input: 36750.19 toks/s, output: 35.89 toks/s]
Processed prompts:  81%|████████▏ | 3330/4096 [01:32<00:22, 34.14it/s, est. speed input: 36731.95 toks/s, output: 35.87 toks/s]
Processed prompts:  82%|████████▏ | 3362/4096 [01:33<00:21, 34.14it/s, est. speed input: 36714.24 toks/s, output: 35.85 toks/s]
Processed prompts:  83%|████████▎ | 3394/4096 [01:34<00:20, 34.14it/s, est. speed input: 36696.82 toks/s, output: 35.84 toks/s]
Processed prompts:  84%|████████▎ | 3426/4096 [01:35<00:19, 34.14it/s, est. speed input: 36679.70 toks/s, output: 35.82 toks/s]
Processed prompts:  84%|████████▍ | 3458/4096 [01:36<00:18, 34.13it/s, est. speed input: 36662.93 toks/s, output: 35.80 toks/s]
Processed prompts:  85%|████████▌ | 3490/4096 [01:37<00:17, 34.13it/s, est. speed input: 36646.34 toks/s, output: 35.79 toks/s]
Processed prompts:  86%|████████▌ | 3522/4096 [01:38<00:16, 34.13it/s, est. speed input: 36630.34 toks/s, output: 35.77 toks/s]
Processed prompts:  87%|████████▋ | 3554/4096 [01:39<00:15, 34.13it/s, est. speed input: 36614.50 toks/s, output: 35.76 toks/s]
Processed prompts:  88%|████████▊ | 3586/4096 [01:40<00:14, 34.13it/s, est. speed input: 36598.84 toks/s, output: 35.74 toks/s]
Processed prompts:  88%|████████▊ | 3618/4096 [01:41<00:14, 34.13it/s, est. speed input: 36583.64 toks/s, output: 35.73 toks/s]
Processed prompts:  89%|████████▉ | 3650/4096 [01:42<00:13, 34.12it/s, est. speed input: 36568.35 toks/s, output: 35.71 toks/s]
Processed prompts:  90%|████████▉ | 3682/4096 [01:43<00:12, 34.12it/s, est. speed input: 36553.55 toks/s, output: 35.70 toks/s]
Processed prompts:  91%|█████████ | 3714/4096 [01:44<00:11, 34.13it/s, est. speed input: 36539.22 toks/s, output: 35.68 toks/s]
Processed prompts:  91%|█████████▏| 3746/4096 [01:45<00:10, 34.12it/s, est. speed input: 36524.89 toks/s, output: 35.67 toks/s]
Processed prompts:  92%|█████████▏| 3778/4096 [01:45<00:09, 34.12it/s, est. speed input: 36510.83 toks/s, output: 35.66 toks/s]
Processed prompts:  93%|█████████▎| 3810/4096 [01:46<00:08, 34.13it/s, est. speed input: 36497.24 toks/s, output: 35.64 toks/s]
Processed prompts:  94%|█████████▍| 3842/4096 [01:47<00:07, 34.12it/s, est. speed input: 36483.61 toks/s, output: 35.63 toks/s]
Processed prompts:  95%|█████████▍| 3874/4096 [01:48<00:06, 34.12it/s, est. speed input: 36470.25 toks/s, output: 35.62 toks/s]
Processed prompts:  95%|█████████▌| 3906/4096 [01:49<00:05, 34.13it/s, est. speed input: 36457.29 toks/s, output: 35.60 toks/s]
Processed prompts:  96%|█████████▌| 3938/4096 [01:50<00:04, 34.12it/s, est. speed input: 36444.41 toks/s, output: 35.59 toks/s]
Processed prompts:  97%|█████████▋| 3970/4096 [01:51<00:03, 34.12it/s, est. speed input: 36431.68 toks/s, output: 35.58 toks/s]
Processed prompts:  98%|█████████▊| 4002/4096 [01:52<00:02, 34.12it/s, est. speed input: 36419.36 toks/s, output: 35.57 toks/s]
Processed prompts:  98%|█████████▊| 4034/4096 [01:53<00:01, 34.43it/s, est. speed input: 36415.98 toks/s, output: 35.56 toks/s]
Processed prompts:  99%|█████████▉| 4066/4096 [01:54<00:00, 34.69it/s, est. speed input: 36414.01 toks/s, output: 35.56 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [01:54<00:00, 34.69it/s, est. speed input: 36682.60 toks/s, output: 35.82 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [01:54<00:00, 35.82it/s, est. speed input: 36682.60 toks/s, output: 35.82 toks/s]
[rank0]:[W125 19:21:13.964894571 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 155.4s

测试结果:
  Requests/s:   34.21
  Tokens/s:     35067.14
  Total Reqs:   4096
  Elapsed:      119.72s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     35032.93

============================================================
[8/8] 测试 M=65536
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-3B-INT8                                │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 65536
│   M_prefill     = 65536 (= 64 x 1024)
│   M_decode      = 64
│   batched_tokens = 65536 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 8192
│   --max-num-seqs           = 64
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 65536
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 19:21:40 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=366737) WARNING 01-25 19:21:49 [backends.py:609] Failed to read file <frozen os>
Throughput: 34.16 requests/s, 35015.53 total tokens/s, 34.16 output tokens/s
Total num prompt tokens:  8388608
Total num output tokens:  8192


─── STDERR ───
[2026-01-25 19:21:40] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:21:40] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:21:40] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:21:40] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:21:40] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:21:40] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:21:40] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:21:40] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:21:40] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:21:40] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:21:40] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:21:40] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:21:40] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:21:40] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 19:21:44] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:21:44] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:21:44] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:21:44] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:21:44] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:21:44] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:21:44] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:21:44] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:21:44] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:21:44] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:21:44] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:21:44] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:21:44] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:21:44] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:44] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:44] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:44] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:44] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:44] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=366737) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=366737) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.88it/s]
(EngineCore_DP0 pid=366737) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.87it/s]
(EngineCore_DP0 pid=366737) 
(EngineCore_DP0 pid=366737) [rank0]:W0125 19:21:52.199000 366737 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:53] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=5120, K=3072), falling back to default heuristic
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=366737) [rank0]:W0125 19:21:53.778000 366737 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:54] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:54] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:54] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:54] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=5120, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:54] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:54] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:54] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:54] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=5120, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:54] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:54] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:54] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:54] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=5120, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:54] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:54] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:54] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:54] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=5120, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:54] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:54] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:54] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:54] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=5120, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:54] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:54] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:54] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:54] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=5120, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:55] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:55] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:55] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:55] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=5120, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:55] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:55] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:55] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:55] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=5120, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:55] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:55] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:55] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:55] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=5120, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:55] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:55] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:55] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:55] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=5120, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:55] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:55] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:55] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:55] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=5120, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:55] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:55] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:55] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:55] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=5120, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:55] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:55] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:55] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:55] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=5120, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:55] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:55] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:55] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:55] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=5120, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:55] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:55] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:55] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:55] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=5120, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:55] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:55] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:55] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:55] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=5120, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:55] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:55] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:55] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:55] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=5120, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:55] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:55] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:55] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:55] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=5120, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:55] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:55] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:55] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:55] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=5120, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:55] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:55] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:55] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:55] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=5120, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=5120, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=5120, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=5120, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=5120, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=5120, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=5120, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=5120, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=366737) 2026-01-25 19:21:57,762 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=5120, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=5120, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=5120, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=5120, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=5120, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=5120, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=5120, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=5120, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=5120, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=5120, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=5120, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=5120, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=5120, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=5120, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=5120, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=5120, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=5120, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=5120, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=5120, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=5120, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=5120, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=5120, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=5120, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=5120, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=5120, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=5120, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=5120, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=5120, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=3072), falling back to default heuristic
(EngineCore_DP0 pid=366737) [2026-01-25 19:21:57] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=366737) 2026-01-25 19:21:57,783 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=366737) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   5%|▌         | 1/19 [00:01<00:29,  1.63s/it]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 3/19 [00:01<00:07,  2.17it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  32%|███▏      | 6/19 [00:01<00:02,  5.01it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 9/19 [00:01<00:01,  8.11it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 12/19 [00:02<00:00, 11.03it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  79%|███████▉  | 15/19 [00:02<00:00, 12.94it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  95%|█████████▍| 18/19 [00:02<00:00, 13.90it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:02<00:00,  7.59it/s]
(EngineCore_DP0 pid=366737) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   9%|▉         | 1/11 [00:00<00:01,  7.04it/s]
Capturing CUDA graphs (decode, FULL):  36%|███▋      | 4/11 [00:00<00:00, 16.71it/s]
Capturing CUDA graphs (decode, FULL):  64%|██████▎   | 7/11 [00:00<00:00, 21.60it/s]
Capturing CUDA graphs (decode, FULL):  91%|█████████ | 10/11 [00:00<00:00, 24.40it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:00<00:00, 21.84it/s]

Adding requests:   0%|          | 0/8192 [00:00<?, ?it/s]
Adding requests:   1%|          | 73/8192 [00:00<00:11, 724.98it/s]
Adding requests:   2%|▏         | 148/8192 [00:00<00:10, 738.44it/s]
Adding requests:   3%|▎         | 225/8192 [00:00<00:10, 749.01it/s]
Adding requests:   4%|▎         | 301/8192 [00:00<00:10, 751.58it/s]
Adding requests:   5%|▍         | 379/8192 [00:00<00:10, 758.22it/s]
Adding requests:   6%|▌         | 455/8192 [00:00<00:10, 758.47it/s]
Adding requests:   6%|▋         | 531/8192 [00:00<00:10, 746.30it/s]
Adding requests:   7%|▋         | 608/8192 [00:00<00:10, 750.43it/s]
Adding requests:   8%|▊         | 685/8192 [00:00<00:09, 755.01it/s]
Adding requests:   9%|▉         | 761/8192 [00:01<00:09, 753.57it/s]
Adding requests:  10%|█         | 837/8192 [00:01<00:09, 737.31it/s]
Adding requests:  11%|█         | 914/8192 [00:01<00:09, 744.89it/s]
Adding requests:  12%|█▏        | 991/8192 [00:01<00:09, 751.16it/s]
Adding requests:  13%|█▎        | 1067/8192 [00:01<00:09, 741.16it/s]
Adding requests:  14%|█▍        | 1142/8192 [00:01<00:09, 737.11it/s]
Adding requests:  15%|█▍        | 1221/8192 [00:01<00:09, 752.68it/s]
Adding requests:  16%|█▌        | 1297/8192 [00:01<00:09, 720.17it/s]
Adding requests:  17%|█▋        | 1374/8192 [00:01<00:09, 732.30it/s]
Adding requests:  18%|█▊        | 1450/8192 [00:01<00:09, 740.29it/s]
Adding requests:  19%|█▊        | 1527/8192 [00:02<00:08, 747.55it/s]
Adding requests:  20%|█▉        | 1606/8192 [00:02<00:08, 758.99it/s]
Adding requests:  21%|██        | 1683/8192 [00:02<00:08, 755.84it/s]
Adding requests:  21%|██▏       | 1760/8192 [00:02<00:08, 759.66it/s]
Adding requests:  22%|██▏       | 1837/8192 [00:02<00:08, 746.96it/s]
Adding requests:  23%|██▎       | 1914/8192 [00:02<00:08, 753.17it/s]
Adding requests:  24%|██▍       | 1993/8192 [00:02<00:08, 761.00it/s]
Adding requests:  25%|██▌       | 2072/8192 [00:02<00:07, 767.82it/s]
Adding requests:  26%|██▌       | 2149/8192 [00:02<00:07, 761.54it/s]
Adding requests:  27%|██▋       | 2226/8192 [00:02<00:07, 752.55it/s]
Adding requests:  28%|██▊       | 2304/8192 [00:03<00:07, 759.05it/s]
Adding requests:  29%|██▉       | 2380/8192 [00:03<00:07, 756.03it/s]
Adding requests:  30%|██▉       | 2457/8192 [00:03<00:07, 759.22it/s]
Adding requests:  31%|███       | 2533/8192 [00:03<00:07, 759.39it/s]
Adding requests:  32%|███▏      | 2609/8192 [00:03<00:07, 751.28it/s]
Adding requests:  33%|███▎      | 2686/8192 [00:03<00:07, 755.06it/s]
Adding requests:  34%|███▎      | 2763/8192 [00:03<00:07, 757.26it/s]
Adding requests:  35%|███▍      | 2839/8192 [00:03<00:07, 757.80it/s]
Adding requests:  36%|███▌      | 2918/8192 [00:03<00:06, 765.60it/s]
Adding requests:  37%|███▋      | 2995/8192 [00:03<00:06, 760.56it/s]
Adding requests:  38%|███▊      | 3072/8192 [00:04<00:06, 753.28it/s]
Adding requests:  38%|███▊      | 3148/8192 [00:04<00:06, 732.21it/s]
Adding requests:  39%|███▉      | 3225/8192 [00:04<00:06, 742.74it/s]
Adding requests:  40%|████      | 3303/8192 [00:04<00:06, 752.61it/s]
Adding requests:  41%|████▏     | 3380/8192 [00:04<00:06, 755.27it/s]
Adding requests:  42%|████▏     | 3456/8192 [00:04<00:06, 755.47it/s]
Adding requests:  43%|████▎     | 3532/8192 [00:04<00:06, 747.68it/s]
Adding requests:  44%|████▍     | 3607/8192 [00:04<00:06, 732.47it/s]
Adding requests:  45%|████▍     | 3683/8192 [00:04<00:06, 740.36it/s]
Adding requests:  46%|████▌     | 3759/8192 [00:05<00:05, 744.30it/s]
Adding requests:  47%|████▋     | 3836/8192 [00:05<00:05, 751.29it/s]
Adding requests:  48%|████▊     | 3916/8192 [00:05<00:05, 762.98it/s]
Adding requests:  49%|████▊     | 3993/8192 [00:05<00:05, 763.56it/s]
Adding requests:  50%|████▉     | 4070/8192 [00:05<00:05, 764.70it/s]
Adding requests:  51%|█████     | 4147/8192 [00:05<00:05, 764.27it/s]
Adding requests:  52%|█████▏    | 4224/8192 [00:05<00:05, 764.40it/s]
Adding requests:  53%|█████▎    | 4301/8192 [00:05<00:05, 764.97it/s]
Adding requests:  53%|█████▎    | 4382/8192 [00:05<00:04, 777.57it/s]
Adding requests:  54%|█████▍    | 4462/8192 [00:05<00:04, 782.72it/s]
Adding requests:  55%|█████▌    | 4541/8192 [00:06<00:04, 757.37it/s]
Adding requests:  56%|█████▋    | 4620/8192 [00:06<00:04, 763.86it/s]
Adding requests:  57%|█████▋    | 4697/8192 [00:06<00:04, 764.79it/s]
Adding requests:  58%|█████▊    | 4776/8192 [00:06<00:04, 771.43it/s]
Adding requests:  59%|█████▉    | 4854/8192 [00:06<00:04, 769.56it/s]
Adding requests:  60%|██████    | 4932/8192 [00:06<00:04, 772.11it/s]
Adding requests:  61%|██████    | 5010/8192 [00:06<00:04, 773.96it/s]
Adding requests:  62%|██████▏   | 5091/8192 [00:06<00:03, 782.92it/s]
Adding requests:  63%|██████▎   | 5170/8192 [00:06<00:03, 782.01it/s]
Adding requests:  64%|██████▍   | 5249/8192 [00:06<00:03, 774.02it/s]
Adding requests:  65%|██████▌   | 5329/8192 [00:07<00:03, 779.44it/s]
Adding requests:  66%|██████▌   | 5407/8192 [00:07<00:03, 771.39it/s]
Adding requests:  67%|██████▋   | 5485/8192 [00:07<00:03, 768.48it/s]
Adding requests:  68%|██████▊   | 5562/8192 [00:07<00:03, 764.63it/s]
Adding requests:  69%|██████▉   | 5639/8192 [00:07<00:03, 755.57it/s]
Adding requests:  70%|██████▉   | 5716/8192 [00:07<00:03, 757.85it/s]
Adding requests:  71%|███████   | 5792/8192 [00:07<00:03, 756.48it/s]
Adding requests:  72%|███████▏  | 5868/8192 [00:07<00:03, 740.67it/s]
Adding requests:  73%|███████▎  | 5947/8192 [00:07<00:02, 753.46it/s]
Adding requests:  74%|███████▎  | 6025/8192 [00:07<00:02, 761.13it/s]
Adding requests:  74%|███████▍  | 6102/8192 [00:08<00:02, 762.40it/s]
Adding requests:  75%|███████▌  | 6181/8192 [00:08<00:02, 770.21it/s]
Adding requests:  76%|███████▋  | 6265/8192 [00:08<00:02, 790.72it/s]
Adding requests:  77%|███████▋  | 6346/8192 [00:08<00:02, 796.14it/s]
Adding requests:  78%|███████▊  | 6427/8192 [00:08<00:02, 799.77it/s]
Adding requests:  79%|███████▉  | 6510/8192 [00:08<00:02, 808.01it/s]
Adding requests:  80%|████████  | 6591/8192 [00:08<00:01, 801.67it/s]
Adding requests:  81%|████████▏ | 6672/8192 [00:08<00:01, 797.71it/s]
Adding requests:  82%|████████▏ | 6752/8192 [00:08<00:01, 797.59it/s]
Adding requests:  83%|████████▎ | 6834/8192 [00:08<00:01, 802.06it/s]
Adding requests:  84%|████████▍ | 6916/8192 [00:09<00:01, 805.02it/s]
Adding requests:  85%|████████▌ | 6997/8192 [00:09<00:01, 801.15it/s]
Adding requests:  86%|████████▋ | 7078/8192 [00:09<00:01, 794.50it/s]
Adding requests:  87%|████████▋ | 7159/8192 [00:09<00:01, 796.36it/s]
Adding requests:  88%|████████▊ | 7239/8192 [00:09<00:01, 780.38it/s]
Adding requests:  89%|████████▉ | 7320/8192 [00:09<00:01, 789.04it/s]
Adding requests:  90%|█████████ | 7399/8192 [00:09<00:01, 787.77it/s]
Adding requests:  91%|█████████▏| 7479/8192 [00:09<00:00, 789.88it/s]
Adding requests:  92%|█████████▏| 7559/8192 [00:09<00:00, 792.47it/s]
Adding requests:  93%|█████████▎| 7639/8192 [00:09<00:00, 793.88it/s]
Adding requests:  94%|█████████▍| 7719/8192 [00:10<00:00, 793.09it/s]
Adding requests:  95%|█████████▌| 7799/8192 [00:10<00:00, 778.74it/s]
Adding requests:  96%|█████████▌| 7879/8192 [00:10<00:00, 783.46it/s]
Adding requests:  97%|█████████▋| 7958/8192 [00:10<00:00, 780.66it/s]
Adding requests:  98%|█████████▊| 8037/8192 [00:10<00:00, 777.97it/s]
Adding requests:  99%|█████████▉| 8118/8192 [00:10<00:00, 784.95it/s]
Adding requests: 100%|██████████| 8192/8192 [00:10<00:00, 765.61it/s]

Processed prompts:   0%|          | 0/8192 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   4%|▍         | 362/8192 [00:00<00:06, 1223.00it/s, est. speed input: 1252424.18 toks/s, output: 1223.03 toks/s]
Processed prompts:   6%|▌         | 485/8192 [00:03<01:15, 101.65it/s, est. speed input: 130985.55 toks/s, output: 127.92 toks/s]   
Processed prompts:   7%|▋         | 538/8192 [00:05<01:34, 81.09it/s, est. speed input: 107968.87 toks/s, output: 105.44 toks/s] 
Processed prompts:   7%|▋         | 569/8192 [00:05<01:48, 70.03it/s, est. speed input: 97488.63 toks/s, output: 95.20 toks/s]  
Processed prompts:   7%|▋         | 589/8192 [00:06<02:11, 57.90it/s, est. speed input: 88034.13 toks/s, output: 85.97 toks/s]
Processed prompts:   7%|▋         | 603/8192 [00:07<02:20, 53.88it/s, est. speed input: 84718.57 toks/s, output: 82.73 toks/s]
Processed prompts:   8%|▊         | 617/8192 [00:07<02:31, 49.91it/s, est. speed input: 81780.72 toks/s, output: 79.86 toks/s]
Processed prompts:   8%|▊         | 632/8192 [00:08<02:41, 46.70it/s, est. speed input: 79283.34 toks/s, output: 77.42 toks/s]
Processed prompts:   8%|▊         | 647/8192 [00:08<02:51, 43.88it/s, est. speed input: 77038.85 toks/s, output: 75.23 toks/s]
Processed prompts:   8%|▊         | 662/8192 [00:09<03:01, 41.54it/s, est. speed input: 75013.33 toks/s, output: 73.26 toks/s]
Processed prompts:   8%|▊         | 677/8192 [00:09<03:09, 39.66it/s, est. speed input: 73169.83 toks/s, output: 71.45 toks/s]
Processed prompts:   8%|▊         | 692/8192 [00:09<03:16, 38.21it/s, est. speed input: 71491.80 toks/s, output: 69.82 toks/s]
Processed prompts:   9%|▊         | 707/8192 [00:10<03:21, 37.12it/s, est. speed input: 69955.22 toks/s, output: 68.32 toks/s]
Processed prompts:   9%|▉         | 722/8192 [00:10<03:25, 36.33it/s, est. speed input: 68544.91 toks/s, output: 66.94 toks/s]
Processed prompts:   9%|▉         | 737/8192 [00:11<03:28, 35.74it/s, est. speed input: 67243.29 toks/s, output: 65.67 toks/s]
Processed prompts:   9%|▉         | 752/8192 [00:11<03:30, 35.31it/s, est. speed input: 66036.72 toks/s, output: 64.49 toks/s]
Processed prompts:   9%|▉         | 767/8192 [00:12<03:32, 35.01it/s, est. speed input: 64917.95 toks/s, output: 63.40 toks/s]
Processed prompts:  10%|▉         | 782/8192 [00:12<03:32, 34.79it/s, est. speed input: 63876.74 toks/s, output: 62.38 toks/s]
Processed prompts:  10%|▉         | 797/8192 [00:12<03:33, 34.65it/s, est. speed input: 62909.24 toks/s, output: 61.43 toks/s]
Processed prompts:  10%|▉         | 812/8192 [00:13<03:33, 34.54it/s, est. speed input: 62001.76 toks/s, output: 60.55 toks/s]
Processed prompts:  10%|█         | 827/8192 [00:13<03:33, 34.47it/s, est. speed input: 61153.61 toks/s, output: 59.72 toks/s]
Processed prompts:  10%|█         | 842/8192 [00:14<03:33, 34.41it/s, est. speed input: 60354.89 toks/s, output: 58.94 toks/s]
Processed prompts:  10%|█         | 857/8192 [00:14<03:33, 34.37it/s, est. speed input: 59604.99 toks/s, output: 58.21 toks/s]
Processed prompts:  11%|█         | 872/8192 [00:15<03:33, 34.35it/s, est. speed input: 58898.54 toks/s, output: 57.52 toks/s]
Processed prompts:  11%|█         | 887/8192 [00:15<03:32, 34.33it/s, est. speed input: 58230.77 toks/s, output: 56.87 toks/s]
Processed prompts:  11%|█         | 902/8192 [00:16<03:32, 34.32it/s, est. speed input: 57600.17 toks/s, output: 56.25 toks/s]
Processed prompts:  11%|█         | 917/8192 [00:16<03:32, 34.31it/s, est. speed input: 57002.61 toks/s, output: 55.67 toks/s]
Processed prompts:  11%|█▏        | 932/8192 [00:16<03:31, 34.30it/s, est. speed input: 56436.55 toks/s, output: 55.11 toks/s]
Processed prompts:  12%|█▏        | 947/8192 [00:17<03:31, 34.29it/s, est. speed input: 55897.75 toks/s, output: 54.59 toks/s]
Processed prompts:  12%|█▏        | 962/8192 [00:17<03:30, 34.29it/s, est. speed input: 55386.28 toks/s, output: 54.09 toks/s]
Processed prompts:  12%|█▏        | 977/8192 [00:18<03:30, 34.28it/s, est. speed input: 54898.37 toks/s, output: 53.61 toks/s]
Processed prompts:  12%|█▏        | 992/8192 [00:18<03:30, 34.27it/s, est. speed input: 54433.45 toks/s, output: 53.16 toks/s]
Processed prompts:  12%|█▏        | 1007/8192 [00:19<03:29, 34.28it/s, est. speed input: 53990.77 toks/s, output: 52.73 toks/s]
Processed prompts:  12%|█▏        | 1022/8192 [00:19<03:29, 34.27it/s, est. speed input: 53566.82 toks/s, output: 52.31 toks/s]
Processed prompts:  13%|█▎        | 1037/8192 [00:19<03:28, 34.27it/s, est. speed input: 53161.84 toks/s, output: 51.92 toks/s]
Processed prompts:  13%|█▎        | 1052/8192 [00:20<03:28, 34.26it/s, est. speed input: 52773.69 toks/s, output: 51.54 toks/s]
Processed prompts:  13%|█▎        | 1067/8192 [00:20<03:27, 34.27it/s, est. speed input: 52402.77 toks/s, output: 51.17 toks/s]
Processed prompts:  13%|█▎        | 1082/8192 [00:21<03:27, 34.26it/s, est. speed input: 52046.00 toks/s, output: 50.83 toks/s]
Processed prompts:  13%|█▎        | 1097/8192 [00:21<03:27, 34.26it/s, est. speed input: 51704.51 toks/s, output: 50.49 toks/s]
Processed prompts:  14%|█▎        | 1112/8192 [00:22<03:26, 34.26it/s, est. speed input: 51376.21 toks/s, output: 50.17 toks/s]
Processed prompts:  14%|█▍        | 1127/8192 [00:22<03:26, 34.26it/s, est. speed input: 51060.02 toks/s, output: 49.86 toks/s]
Processed prompts:  14%|█▍        | 1142/8192 [00:23<03:25, 34.26it/s, est. speed input: 50756.47 toks/s, output: 49.57 toks/s]
Processed prompts:  14%|█▍        | 1157/8192 [00:23<03:25, 34.26it/s, est. speed input: 50464.08 toks/s, output: 49.28 toks/s]
Processed prompts:  14%|█▍        | 1172/8192 [00:23<03:24, 34.26it/s, est. speed input: 50182.93 toks/s, output: 49.01 toks/s]
Processed prompts:  14%|█▍        | 1187/8192 [00:24<03:24, 34.25it/s, est. speed input: 49909.98 toks/s, output: 48.74 toks/s]
Processed prompts:  15%|█▍        | 1202/8192 [00:24<03:24, 34.26it/s, est. speed input: 49648.63 toks/s, output: 48.48 toks/s]
Processed prompts:  15%|█▍        | 1217/8192 [00:25<03:23, 34.26it/s, est. speed input: 49395.73 toks/s, output: 48.24 toks/s]
Processed prompts:  15%|█▌        | 1232/8192 [00:25<03:23, 34.25it/s, est. speed input: 49151.06 toks/s, output: 48.00 toks/s]
Processed prompts:  15%|█▌        | 1247/8192 [00:26<03:22, 34.25it/s, est. speed input: 48914.80 toks/s, output: 47.77 toks/s]
Processed prompts:  15%|█▌        | 1262/8192 [00:26<03:22, 34.25it/s, est. speed input: 48686.45 toks/s, output: 47.55 toks/s]
Processed prompts:  16%|█▌        | 1277/8192 [00:26<03:21, 34.25it/s, est. speed input: 48465.65 toks/s, output: 47.33 toks/s]
Processed prompts:  16%|█▌        | 1292/8192 [00:27<03:21, 34.26it/s, est. speed input: 48251.94 toks/s, output: 47.12 toks/s]
Processed prompts:  16%|█▌        | 1307/8192 [00:27<03:21, 34.25it/s, est. speed input: 48044.42 toks/s, output: 46.92 toks/s]
Processed prompts:  16%|█▌        | 1322/8192 [00:28<03:20, 34.24it/s, est. speed input: 47843.15 toks/s, output: 46.72 toks/s]
Processed prompts:  16%|█▋        | 1337/8192 [00:28<03:20, 34.25it/s, est. speed input: 47648.86 toks/s, output: 46.53 toks/s]
Processed prompts:  17%|█▋        | 1352/8192 [00:29<03:19, 34.26it/s, est. speed input: 47460.42 toks/s, output: 46.35 toks/s]
Processed prompts:  17%|█▋        | 1367/8192 [00:29<03:19, 34.26it/s, est. speed input: 47277.25 toks/s, output: 46.17 toks/s]
Processed prompts:  17%|█▋        | 1382/8192 [00:30<03:18, 34.25it/s, est. speed input: 47099.07 toks/s, output: 46.00 toks/s]
Processed prompts:  17%|█▋        | 1397/8192 [00:30<03:18, 34.24it/s, est. speed input: 46925.82 toks/s, output: 45.83 toks/s]
Processed prompts:  17%|█▋        | 1412/8192 [00:30<03:18, 34.24it/s, est. speed input: 46757.72 toks/s, output: 45.66 toks/s]
Processed prompts:  17%|█▋        | 1427/8192 [00:31<03:17, 34.24it/s, est. speed input: 46594.35 toks/s, output: 45.50 toks/s]
Processed prompts:  18%|█▊        | 1442/8192 [00:31<03:17, 34.24it/s, est. speed input: 46435.56 toks/s, output: 45.35 toks/s]
Processed prompts:  18%|█▊        | 1457/8192 [00:32<03:16, 34.24it/s, est. speed input: 46280.85 toks/s, output: 45.20 toks/s]
Processed prompts:  18%|█▊        | 1472/8192 [00:32<03:16, 34.23it/s, est. speed input: 46129.96 toks/s, output: 45.05 toks/s]
Processed prompts:  18%|█▊        | 1487/8192 [00:33<03:15, 34.23it/s, est. speed input: 45983.32 toks/s, output: 44.91 toks/s]
Processed prompts:  18%|█▊        | 1502/8192 [00:33<03:15, 34.23it/s, est. speed input: 45840.71 toks/s, output: 44.77 toks/s]
Processed prompts:  19%|█▊        | 1517/8192 [00:33<03:14, 34.24it/s, est. speed input: 45701.94 toks/s, output: 44.63 toks/s]
Processed prompts:  19%|█▊        | 1532/8192 [00:34<03:14, 34.23it/s, est. speed input: 45565.94 toks/s, output: 44.50 toks/s]
Processed prompts:  19%|█▉        | 1547/8192 [00:34<03:14, 34.23it/s, est. speed input: 45433.99 toks/s, output: 44.37 toks/s]
Processed prompts:  19%|█▉        | 1562/8192 [00:35<03:13, 34.23it/s, est. speed input: 45304.91 toks/s, output: 44.24 toks/s]
Processed prompts:  19%|█▉        | 1577/8192 [00:35<03:13, 34.23it/s, est. speed input: 45179.27 toks/s, output: 44.12 toks/s]
Processed prompts:  19%|█▉        | 1592/8192 [00:36<03:12, 34.23it/s, est. speed input: 45056.44 toks/s, output: 44.00 toks/s]
Processed prompts:  20%|█▉        | 1607/8192 [00:36<03:12, 34.22it/s, est. speed input: 44936.44 toks/s, output: 43.88 toks/s]
Processed prompts:  20%|█▉        | 1622/8192 [00:37<03:11, 34.23it/s, est. speed input: 44819.76 toks/s, output: 43.77 toks/s]
Processed prompts:  20%|█▉        | 1637/8192 [00:37<03:11, 34.23it/s, est. speed input: 44705.43 toks/s, output: 43.66 toks/s]
Processed prompts:  20%|██        | 1652/8192 [00:37<03:11, 34.23it/s, est. speed input: 44594.04 toks/s, output: 43.55 toks/s]
Processed prompts:  20%|██        | 1667/8192 [00:38<03:10, 34.21it/s, est. speed input: 44484.15 toks/s, output: 43.44 toks/s]
Processed prompts:  21%|██        | 1682/8192 [00:38<03:10, 34.21it/s, est. speed input: 44377.53 toks/s, output: 43.34 toks/s]
Processed prompts:  21%|██        | 1697/8192 [00:39<03:09, 34.22it/s, est. speed input: 44273.41 toks/s, output: 43.24 toks/s]
Processed prompts:  21%|██        | 1712/8192 [00:39<03:09, 34.21it/s, est. speed input: 44171.20 toks/s, output: 43.14 toks/s]
Processed prompts:  21%|██        | 1727/8192 [00:40<03:08, 34.22it/s, est. speed input: 44071.59 toks/s, output: 43.04 toks/s]
Processed prompts:  21%|██▏       | 1742/8192 [00:40<03:08, 34.22it/s, est. speed input: 43973.94 toks/s, output: 42.94 toks/s]
Processed prompts:  21%|██▏       | 1757/8192 [00:41<03:08, 34.22it/s, est. speed input: 43878.61 toks/s, output: 42.85 toks/s]
Processed prompts:  22%|██▏       | 1772/8192 [00:41<03:07, 34.21it/s, est. speed input: 43784.69 toks/s, output: 42.76 toks/s]
Processed prompts:  22%|██▏       | 1787/8192 [00:41<03:07, 34.21it/s, est. speed input: 43693.10 toks/s, output: 42.67 toks/s]
Processed prompts:  22%|██▏       | 1802/8192 [00:42<03:06, 34.21it/s, est. speed input: 43603.12 toks/s, output: 42.58 toks/s]
Processed prompts:  22%|██▏       | 1817/8192 [00:42<03:06, 34.21it/s, est. speed input: 43515.31 toks/s, output: 42.50 toks/s]
Processed prompts:  22%|██▏       | 1832/8192 [00:43<03:05, 34.22it/s, est. speed input: 43429.51 toks/s, output: 42.41 toks/s]
Processed prompts:  23%|██▎       | 1847/8192 [00:43<03:05, 34.22it/s, est. speed input: 43345.18 toks/s, output: 42.33 toks/s]
Processed prompts:  23%|██▎       | 1862/8192 [00:44<03:04, 34.22it/s, est. speed input: 43262.61 toks/s, output: 42.25 toks/s]
Processed prompts:  23%|██▎       | 1877/8192 [00:44<03:04, 34.21it/s, est. speed input: 43181.33 toks/s, output: 42.17 toks/s]
Processed prompts:  23%|██▎       | 1892/8192 [00:44<03:04, 34.22it/s, est. speed input: 43102.01 toks/s, output: 42.09 toks/s]
Processed prompts:  23%|██▎       | 1907/8192 [00:45<03:03, 34.20it/s, est. speed input: 43023.58 toks/s, output: 42.02 toks/s]
Processed prompts:  23%|██▎       | 1922/8192 [00:45<03:03, 34.20it/s, est. speed input: 42946.84 toks/s, output: 41.94 toks/s]
Processed prompts:  24%|██▎       | 1937/8192 [00:46<03:02, 34.19it/s, est. speed input: 42871.34 toks/s, output: 41.87 toks/s]
Processed prompts:  24%|██▍       | 1952/8192 [00:46<03:02, 34.19it/s, est. speed input: 42797.62 toks/s, output: 41.79 toks/s]
Processed prompts:  24%|██▍       | 1967/8192 [00:47<03:01, 34.21it/s, est. speed input: 42725.69 toks/s, output: 41.72 toks/s]
Processed prompts:  24%|██▍       | 1982/8192 [00:47<03:01, 34.19it/s, est. speed input: 42654.29 toks/s, output: 41.65 toks/s]
Processed prompts:  24%|██▍       | 1997/8192 [00:48<03:01, 34.20it/s, est. speed input: 42584.74 toks/s, output: 41.59 toks/s]
Processed prompts:  25%|██▍       | 2012/8192 [00:48<03:00, 34.20it/s, est. speed input: 42516.29 toks/s, output: 41.52 toks/s]
Processed prompts:  25%|██▍       | 2027/8192 [00:48<03:00, 34.20it/s, est. speed input: 42449.08 toks/s, output: 41.45 toks/s]
Processed prompts:  25%|██▍       | 2042/8192 [00:49<02:59, 34.19it/s, est. speed input: 42382.56 toks/s, output: 41.39 toks/s]
Processed prompts:  25%|██▌       | 2057/8192 [00:49<02:59, 34.19it/s, est. speed input: 42317.63 toks/s, output: 41.33 toks/s]
Processed prompts:  25%|██▌       | 2072/8192 [00:50<02:58, 34.20it/s, est. speed input: 42254.17 toks/s, output: 41.26 toks/s]
Processed prompts:  25%|██▌       | 2087/8192 [00:50<02:58, 34.20it/s, est. speed input: 42191.37 toks/s, output: 41.20 toks/s]
Processed prompts:  26%|██▌       | 2102/8192 [00:51<02:58, 34.19it/s, est. speed input: 42129.67 toks/s, output: 41.14 toks/s]
Processed prompts:  26%|██▌       | 2117/8192 [00:51<02:57, 34.19it/s, est. speed input: 42068.84 toks/s, output: 41.08 toks/s]
Processed prompts:  26%|██▌       | 2132/8192 [00:51<02:57, 34.19it/s, est. speed input: 42009.41 toks/s, output: 41.02 toks/s]
Processed prompts:  26%|██▌       | 2147/8192 [00:52<02:56, 34.20it/s, est. speed input: 41951.03 toks/s, output: 40.97 toks/s]
Processed prompts:  26%|██▋       | 2162/8192 [00:52<02:56, 34.19it/s, est. speed input: 41893.31 toks/s, output: 40.91 toks/s]
Processed prompts:  27%|██▋       | 2177/8192 [00:53<02:55, 34.18it/s, est. speed input: 41836.26 toks/s, output: 40.86 toks/s]
Processed prompts:  27%|██▋       | 2192/8192 [00:53<02:55, 34.19it/s, est. speed input: 41780.75 toks/s, output: 40.80 toks/s]
Processed prompts:  27%|██▋       | 2207/8192 [00:54<02:55, 34.19it/s, est. speed input: 41725.79 toks/s, output: 40.75 toks/s]
Processed prompts:  27%|██▋       | 2222/8192 [00:54<02:54, 34.18it/s, est. speed input: 41671.47 toks/s, output: 40.69 toks/s]
Processed prompts:  27%|██▋       | 2237/8192 [00:55<02:54, 34.19it/s, est. speed input: 41618.65 toks/s, output: 40.64 toks/s]
Processed prompts:  27%|██▋       | 2252/8192 [00:55<02:53, 34.19it/s, est. speed input: 41566.43 toks/s, output: 40.59 toks/s]
Processed prompts:  28%|██▊       | 2267/8192 [00:55<02:53, 34.20it/s, est. speed input: 41515.15 toks/s, output: 40.54 toks/s]
Processed prompts:  28%|██▊       | 2282/8192 [00:56<02:52, 34.19it/s, est. speed input: 41464.30 toks/s, output: 40.49 toks/s]
Processed prompts:  28%|██▊       | 2297/8192 [00:56<02:52, 34.19it/s, est. speed input: 41414.54 toks/s, output: 40.44 toks/s]
Processed prompts:  28%|██▊       | 2312/8192 [00:57<02:51, 34.19it/s, est. speed input: 41365.39 toks/s, output: 40.40 toks/s]
Processed prompts:  28%|██▊       | 2327/8192 [00:57<02:51, 34.19it/s, est. speed input: 41317.00 toks/s, output: 40.35 toks/s]
Processed prompts:  29%|██▊       | 2342/8192 [00:58<02:51, 34.19it/s, est. speed input: 41269.40 toks/s, output: 40.30 toks/s]
Processed prompts:  29%|██▉       | 2357/8192 [00:58<02:50, 34.19it/s, est. speed input: 41222.43 toks/s, output: 40.26 toks/s]
Processed prompts:  29%|██▉       | 2372/8192 [00:58<02:50, 34.19it/s, est. speed input: 41176.25 toks/s, output: 40.21 toks/s]
Processed prompts:  29%|██▉       | 2387/8192 [00:59<02:49, 34.18it/s, est. speed input: 41130.44 toks/s, output: 40.17 toks/s]
Processed prompts:  29%|██▉       | 2402/8192 [00:59<02:49, 34.18it/s, est. speed input: 41085.56 toks/s, output: 40.12 toks/s]
Processed prompts:  30%|██▉       | 2417/8192 [01:00<02:48, 34.18it/s, est. speed input: 41041.12 toks/s, output: 40.08 toks/s]
Processed prompts:  30%|██▉       | 2432/8192 [01:00<02:48, 34.19it/s, est. speed input: 40997.77 toks/s, output: 40.04 toks/s]
Processed prompts:  30%|██▉       | 2447/8192 [01:01<02:48, 34.19it/s, est. speed input: 40954.77 toks/s, output: 39.99 toks/s]
Processed prompts:  30%|███       | 2462/8192 [01:01<02:47, 34.18it/s, est. speed input: 40912.24 toks/s, output: 39.95 toks/s]
Processed prompts:  30%|███       | 2477/8192 [01:02<02:47, 34.18it/s, est. speed input: 40870.47 toks/s, output: 39.91 toks/s]
Processed prompts:  30%|███       | 2492/8192 [01:02<02:46, 34.18it/s, est. speed input: 40829.11 toks/s, output: 39.87 toks/s]
Processed prompts:  31%|███       | 2507/8192 [01:02<02:46, 34.18it/s, est. speed input: 40788.63 toks/s, output: 39.83 toks/s]
Processed prompts:  31%|███       | 2522/8192 [01:03<02:45, 34.17it/s, est. speed input: 40748.33 toks/s, output: 39.79 toks/s]
Processed prompts:  31%|███       | 2537/8192 [01:03<02:45, 34.17it/s, est. speed input: 40708.72 toks/s, output: 39.75 toks/s]
Processed prompts:  31%|███       | 2552/8192 [01:04<02:45, 34.18it/s, est. speed input: 40669.84 toks/s, output: 39.72 toks/s]
Processed prompts:  31%|███▏      | 2567/8192 [01:04<02:44, 34.17it/s, est. speed input: 40631.22 toks/s, output: 39.68 toks/s]
Processed prompts:  32%|███▏      | 2582/8192 [01:05<02:44, 34.17it/s, est. speed input: 40593.11 toks/s, output: 39.64 toks/s]
Processed prompts:  32%|███▏      | 2597/8192 [01:05<02:43, 34.17it/s, est. speed input: 40555.72 toks/s, output: 39.61 toks/s]
Processed prompts:  32%|███▏      | 2612/8192 [01:06<02:43, 34.17it/s, est. speed input: 40518.72 toks/s, output: 39.57 toks/s]
Processed prompts:  32%|███▏      | 2627/8192 [01:06<02:42, 34.17it/s, est. speed input: 40482.26 toks/s, output: 39.53 toks/s]
Processed prompts:  32%|███▏      | 2642/8192 [01:06<02:42, 34.17it/s, est. speed input: 40446.16 toks/s, output: 39.50 toks/s]
Processed prompts:  32%|███▏      | 2657/8192 [01:07<02:41, 34.17it/s, est. speed input: 40410.55 toks/s, output: 39.46 toks/s]
Processed prompts:  33%|███▎      | 2672/8192 [01:07<02:41, 34.17it/s, est. speed input: 40375.43 toks/s, output: 39.43 toks/s]
Processed prompts:  33%|███▎      | 2687/8192 [01:08<02:41, 34.16it/s, est. speed input: 40340.59 toks/s, output: 39.40 toks/s]
Processed prompts:  33%|███▎      | 2702/8192 [01:08<02:40, 34.16it/s, est. speed input: 40306.36 toks/s, output: 39.36 toks/s]
Processed prompts:  33%|███▎      | 2717/8192 [01:09<02:40, 34.17it/s, est. speed input: 40272.60 toks/s, output: 39.33 toks/s]
Processed prompts:  33%|███▎      | 2732/8192 [01:09<02:39, 34.16it/s, est. speed input: 40238.99 toks/s, output: 39.30 toks/s]
Processed prompts:  34%|███▎      | 2747/8192 [01:09<02:39, 34.16it/s, est. speed input: 40206.07 toks/s, output: 39.26 toks/s]
Processed prompts:  34%|███▎      | 2762/8192 [01:10<02:38, 34.16it/s, est. speed input: 40173.42 toks/s, output: 39.23 toks/s]
Processed prompts:  34%|███▍      | 2777/8192 [01:10<02:38, 34.16it/s, est. speed input: 40141.13 toks/s, output: 39.20 toks/s]
Processed prompts:  34%|███▍      | 2792/8192 [01:11<02:38, 34.15it/s, est. speed input: 40109.27 toks/s, output: 39.17 toks/s]
Processed prompts:  34%|███▍      | 2807/8192 [01:11<02:37, 34.16it/s, est. speed input: 40077.87 toks/s, output: 39.14 toks/s]
Processed prompts:  34%|███▍      | 2822/8192 [01:12<02:37, 34.15it/s, est. speed input: 40046.78 toks/s, output: 39.11 toks/s]
Processed prompts:  35%|███▍      | 2837/8192 [01:12<02:36, 34.16it/s, est. speed input: 40016.11 toks/s, output: 39.08 toks/s]
Processed prompts:  35%|███▍      | 2852/8192 [01:13<02:36, 34.17it/s, est. speed input: 39986.05 toks/s, output: 39.05 toks/s]
Processed prompts:  35%|███▍      | 2867/8192 [01:13<02:35, 34.16it/s, est. speed input: 39955.92 toks/s, output: 39.02 toks/s]
Processed prompts:  35%|███▌      | 2882/8192 [01:13<02:35, 34.16it/s, est. speed input: 39926.41 toks/s, output: 38.99 toks/s]
Processed prompts:  35%|███▌      | 2897/8192 [01:14<02:35, 34.15it/s, est. speed input: 39896.99 toks/s, output: 38.96 toks/s]
Processed prompts:  36%|███▌      | 2912/8192 [01:14<02:34, 34.15it/s, est. speed input: 39868.09 toks/s, output: 38.93 toks/s]
Processed prompts:  36%|███▌      | 2927/8192 [01:15<02:34, 34.15it/s, est. speed input: 39839.40 toks/s, output: 38.91 toks/s]
Processed prompts:  36%|███▌      | 2942/8192 [01:15<02:33, 34.14it/s, est. speed input: 39810.90 toks/s, output: 38.88 toks/s]
Processed prompts:  36%|███▌      | 2957/8192 [01:16<02:33, 34.14it/s, est. speed input: 39783.04 toks/s, output: 38.85 toks/s]
Processed prompts:  36%|███▋      | 2972/8192 [01:16<02:32, 34.14it/s, est. speed input: 39755.29 toks/s, output: 38.82 toks/s]
Processed prompts:  36%|███▋      | 2987/8192 [01:16<02:32, 34.15it/s, est. speed input: 39728.05 toks/s, output: 38.80 toks/s]
Processed prompts:  37%|███▋      | 3002/8192 [01:17<02:31, 34.15it/s, est. speed input: 39701.10 toks/s, output: 38.77 toks/s]
Processed prompts:  37%|███▋      | 3017/8192 [01:17<02:31, 34.15it/s, est. speed input: 39674.39 toks/s, output: 38.74 toks/s]
Processed prompts:  37%|███▋      | 3032/8192 [01:18<02:31, 34.14it/s, est. speed input: 39647.82 toks/s, output: 38.72 toks/s]
Processed prompts:  37%|███▋      | 3047/8192 [01:18<02:30, 34.15it/s, est. speed input: 39621.80 toks/s, output: 38.69 toks/s]
Processed prompts:  37%|███▋      | 3062/8192 [01:19<02:30, 34.15it/s, est. speed input: 39596.03 toks/s, output: 38.67 toks/s]
Processed prompts:  38%|███▊      | 3077/8192 [01:19<02:29, 34.14it/s, est. speed input: 39570.38 toks/s, output: 38.64 toks/s]
Processed prompts:  38%|███▊      | 3092/8192 [01:20<02:29, 34.15it/s, est. speed input: 39545.33 toks/s, output: 38.62 toks/s]
Processed prompts:  38%|███▊      | 3107/8192 [01:20<02:28, 34.14it/s, est. speed input: 39520.17 toks/s, output: 38.59 toks/s]
Processed prompts:  38%|███▊      | 3122/8192 [01:20<02:28, 34.14it/s, est. speed input: 39495.39 toks/s, output: 38.57 toks/s]
Processed prompts:  38%|███▊      | 3137/8192 [01:21<02:28, 34.14it/s, est. speed input: 39470.80 toks/s, output: 38.55 toks/s]
Processed prompts:  38%|███▊      | 3152/8192 [01:21<02:27, 34.14it/s, est. speed input: 39446.67 toks/s, output: 38.52 toks/s]
Processed prompts:  39%|███▊      | 3167/8192 [01:22<02:27, 34.13it/s, est. speed input: 39422.51 toks/s, output: 38.50 toks/s]
Processed prompts:  39%|███▉      | 3182/8192 [01:22<02:26, 34.14it/s, est. speed input: 39398.84 toks/s, output: 38.48 toks/s]
Processed prompts:  39%|███▉      | 3197/8192 [01:23<02:26, 34.15it/s, est. speed input: 39375.58 toks/s, output: 38.45 toks/s]
Processed prompts:  39%|███▉      | 3212/8192 [01:23<02:25, 34.14it/s, est. speed input: 39352.35 toks/s, output: 38.43 toks/s]
Processed prompts:  39%|███▉      | 3227/8192 [01:24<02:25, 34.15it/s, est. speed input: 39329.45 toks/s, output: 38.41 toks/s]
Processed prompts:  40%|███▉      | 3242/8192 [01:24<02:25, 34.13it/s, est. speed input: 39306.48 toks/s, output: 38.39 toks/s]
Processed prompts:  40%|███▉      | 3257/8192 [01:24<02:24, 34.14it/s, est. speed input: 39284.12 toks/s, output: 38.36 toks/s]
Processed prompts:  40%|███▉      | 3272/8192 [01:25<02:24, 34.14it/s, est. speed input: 39261.82 toks/s, output: 38.34 toks/s]
Processed prompts:  40%|████      | 3287/8192 [01:25<02:23, 34.14it/s, est. speed input: 39239.78 toks/s, output: 38.32 toks/s]
Processed prompts:  40%|████      | 3302/8192 [01:26<02:23, 34.14it/s, est. speed input: 39218.02 toks/s, output: 38.30 toks/s]
Processed prompts:  40%|████      | 3317/8192 [01:26<02:22, 34.14it/s, est. speed input: 39196.45 toks/s, output: 38.28 toks/s]
Processed prompts:  41%|████      | 3332/8192 [01:27<02:22, 34.14it/s, est. speed input: 39175.03 toks/s, output: 38.26 toks/s]
Processed prompts:  41%|████      | 3347/8192 [01:27<02:21, 34.13it/s, est. speed input: 39153.69 toks/s, output: 38.24 toks/s]
Processed prompts:  41%|████      | 3362/8192 [01:27<02:21, 34.14it/s, est. speed input: 39132.85 toks/s, output: 38.22 toks/s]
Processed prompts:  41%|████      | 3377/8192 [01:28<02:21, 34.13it/s, est. speed input: 39112.02 toks/s, output: 38.20 toks/s]
Processed prompts:  41%|████▏     | 3392/8192 [01:28<02:20, 34.13it/s, est. speed input: 39091.45 toks/s, output: 38.18 toks/s]
Processed prompts:  42%|████▏     | 3407/8192 [01:29<02:20, 34.14it/s, est. speed input: 39071.14 toks/s, output: 38.16 toks/s]
Processed prompts:  42%|████▏     | 3422/8192 [01:29<02:19, 34.14it/s, est. speed input: 39051.10 toks/s, output: 38.14 toks/s]
Processed prompts:  42%|████▏     | 3437/8192 [01:30<02:19, 34.15it/s, est. speed input: 39031.22 toks/s, output: 38.12 toks/s]
Processed prompts:  42%|████▏     | 3452/8192 [01:30<02:18, 34.15it/s, est. speed input: 39011.56 toks/s, output: 38.10 toks/s]
Processed prompts:  42%|████▏     | 3467/8192 [01:31<02:18, 34.15it/s, est. speed input: 38992.11 toks/s, output: 38.08 toks/s]
Processed prompts:  43%|████▎     | 3482/8192 [01:31<02:17, 34.15it/s, est. speed input: 38972.73 toks/s, output: 38.06 toks/s]
Processed prompts:  43%|████▎     | 3497/8192 [01:31<02:17, 34.15it/s, est. speed input: 38953.62 toks/s, output: 38.04 toks/s]
Processed prompts:  43%|████▎     | 3512/8192 [01:32<02:17, 34.14it/s, est. speed input: 38934.59 toks/s, output: 38.02 toks/s]
Processed prompts:  43%|████▎     | 3527/8192 [01:32<02:16, 34.14it/s, est. speed input: 38915.66 toks/s, output: 38.00 toks/s]
Processed prompts:  43%|████▎     | 3542/8192 [01:33<02:16, 34.14it/s, est. speed input: 38897.06 toks/s, output: 37.99 toks/s]
Processed prompts:  43%|████▎     | 3557/8192 [01:33<02:15, 34.14it/s, est. speed input: 38878.68 toks/s, output: 37.97 toks/s]
Processed prompts:  44%|████▎     | 3572/8192 [01:34<02:15, 34.14it/s, est. speed input: 38860.33 toks/s, output: 37.95 toks/s]
Processed prompts:  44%|████▍     | 3587/8192 [01:34<02:14, 34.14it/s, est. speed input: 38842.12 toks/s, output: 37.93 toks/s]
Processed prompts:  44%|████▍     | 3602/8192 [01:35<02:14, 34.14it/s, est. speed input: 38824.27 toks/s, output: 37.91 toks/s]
Processed prompts:  44%|████▍     | 3617/8192 [01:35<02:14, 34.13it/s, est. speed input: 38806.33 toks/s, output: 37.90 toks/s]
Processed prompts:  44%|████▍     | 3632/8192 [01:35<02:13, 34.13it/s, est. speed input: 38788.68 toks/s, output: 37.88 toks/s]
Processed prompts:  45%|████▍     | 3647/8192 [01:36<02:13, 34.13it/s, est. speed input: 38771.03 toks/s, output: 37.86 toks/s]
Processed prompts:  45%|████▍     | 3662/8192 [01:36<02:12, 34.13it/s, est. speed input: 38753.66 toks/s, output: 37.85 toks/s]
Processed prompts:  45%|████▍     | 3677/8192 [01:37<02:12, 34.13it/s, est. speed input: 38736.54 toks/s, output: 37.83 toks/s]
Processed prompts:  45%|████▌     | 3692/8192 [01:37<02:11, 34.13it/s, est. speed input: 38719.52 toks/s, output: 37.81 toks/s]
Processed prompts:  45%|████▌     | 3707/8192 [01:38<02:11, 34.14it/s, est. speed input: 38702.74 toks/s, output: 37.80 toks/s]
Processed prompts:  45%|████▌     | 3722/8192 [01:38<02:10, 34.13it/s, est. speed input: 38685.96 toks/s, output: 37.78 toks/s]
Processed prompts:  46%|████▌     | 3737/8192 [01:38<02:10, 34.14it/s, est. speed input: 38669.43 toks/s, output: 37.76 toks/s]
Processed prompts:  46%|████▌     | 3752/8192 [01:39<02:10, 34.13it/s, est. speed input: 38652.94 toks/s, output: 37.75 toks/s]
Processed prompts:  46%|████▌     | 3767/8192 [01:39<02:09, 34.14it/s, est. speed input: 38636.72 toks/s, output: 37.73 toks/s]
Processed prompts:  46%|████▌     | 3782/8192 [01:40<02:09, 34.14it/s, est. speed input: 38620.57 toks/s, output: 37.72 toks/s]
Processed prompts:  46%|████▋     | 3797/8192 [01:40<02:08, 34.14it/s, est. speed input: 38604.69 toks/s, output: 37.70 toks/s]
Processed prompts:  47%|████▋     | 3812/8192 [01:41<02:08, 34.14it/s, est. speed input: 38588.79 toks/s, output: 37.68 toks/s]
Processed prompts:  47%|████▋     | 3827/8192 [01:41<02:07, 34.14it/s, est. speed input: 38573.07 toks/s, output: 37.67 toks/s]
Processed prompts:  47%|████▋     | 3842/8192 [01:42<02:07, 34.14it/s, est. speed input: 38557.52 toks/s, output: 37.65 toks/s]
Processed prompts:  47%|████▋     | 3857/8192 [01:42<02:07, 34.13it/s, est. speed input: 38541.95 toks/s, output: 37.64 toks/s]
Processed prompts:  47%|████▋     | 3872/8192 [01:42<02:06, 34.13it/s, est. speed input: 38526.65 toks/s, output: 37.62 toks/s]
Processed prompts:  47%|████▋     | 3887/8192 [01:43<02:06, 34.13it/s, est. speed input: 38511.32 toks/s, output: 37.61 toks/s]
Processed prompts:  48%|████▊     | 3902/8192 [01:43<02:05, 34.14it/s, est. speed input: 38496.45 toks/s, output: 37.59 toks/s]
Processed prompts:  48%|████▊     | 3917/8192 [01:44<02:05, 34.14it/s, est. speed input: 38481.55 toks/s, output: 37.58 toks/s]
Processed prompts:  48%|████▊     | 3932/8192 [01:44<02:04, 34.14it/s, est. speed input: 38466.73 toks/s, output: 37.57 toks/s]
Processed prompts:  48%|████▊     | 3947/8192 [01:45<02:04, 34.13it/s, est. speed input: 38451.99 toks/s, output: 37.55 toks/s]
Processed prompts:  48%|████▊     | 3962/8192 [01:45<02:03, 34.14it/s, est. speed input: 38437.52 toks/s, output: 37.54 toks/s]
Processed prompts:  49%|████▊     | 3977/8192 [01:45<02:03, 34.13it/s, est. speed input: 38423.01 toks/s, output: 37.52 toks/s]
Processed prompts:  49%|████▊     | 3992/8192 [01:46<02:03, 34.13it/s, est. speed input: 38408.62 toks/s, output: 37.51 toks/s]
Processed prompts:  49%|████▉     | 4007/8192 [01:46<02:02, 34.14it/s, est. speed input: 38394.47 toks/s, output: 37.49 toks/s]
Processed prompts:  49%|████▉     | 4022/8192 [01:47<02:02, 34.13it/s, est. speed input: 38380.33 toks/s, output: 37.48 toks/s]
Processed prompts:  49%|████▉     | 4037/8192 [01:47<02:01, 34.13it/s, est. speed input: 38366.35 toks/s, output: 37.47 toks/s]
Processed prompts:  49%|████▉     | 4052/8192 [01:48<02:01, 34.13it/s, est. speed input: 38352.41 toks/s, output: 37.45 toks/s]
Processed prompts:  50%|████▉     | 4067/8192 [01:48<02:00, 34.12it/s, est. speed input: 38338.57 toks/s, output: 37.44 toks/s]
Processed prompts:  50%|████▉     | 4082/8192 [01:49<02:00, 34.13it/s, est. speed input: 38324.94 toks/s, output: 37.43 toks/s]
Processed prompts:  50%|█████     | 4097/8192 [01:49<02:00, 34.12it/s, est. speed input: 38311.34 toks/s, output: 37.41 toks/s]
Processed prompts:  50%|█████     | 4112/8192 [01:49<01:59, 34.13it/s, est. speed input: 38297.98 toks/s, output: 37.40 toks/s]
Processed prompts:  50%|█████     | 4127/8192 [01:50<01:59, 34.12it/s, est. speed input: 38284.54 toks/s, output: 37.39 toks/s]
Processed prompts:  51%|█████     | 4142/8192 [01:50<01:58, 34.12it/s, est. speed input: 38271.29 toks/s, output: 37.37 toks/s]
Processed prompts:  51%|█████     | 4157/8192 [01:51<01:58, 34.12it/s, est. speed input: 38258.14 toks/s, output: 37.36 toks/s]
Processed prompts:  51%|█████     | 4172/8192 [01:51<01:57, 34.13it/s, est. speed input: 38245.21 toks/s, output: 37.35 toks/s]
Processed prompts:  51%|█████     | 4187/8192 [01:52<01:57, 34.14it/s, est. speed input: 38232.36 toks/s, output: 37.34 toks/s]
Processed prompts:  51%|█████▏    | 4202/8192 [01:52<01:56, 34.13it/s, est. speed input: 38219.47 toks/s, output: 37.32 toks/s]
Processed prompts:  51%|█████▏    | 4217/8192 [01:53<01:56, 34.13it/s, est. speed input: 38206.83 toks/s, output: 37.31 toks/s]
Processed prompts:  52%|█████▏    | 4232/8192 [01:53<01:56, 34.13it/s, est. speed input: 38194.18 toks/s, output: 37.30 toks/s]
Processed prompts:  52%|█████▏    | 4247/8192 [01:53<01:55, 34.14it/s, est. speed input: 38181.75 toks/s, output: 37.29 toks/s]
Processed prompts:  52%|█████▏    | 4262/8192 [01:54<01:55, 34.13it/s, est. speed input: 38169.19 toks/s, output: 37.27 toks/s]
Processed prompts:  52%|█████▏    | 4277/8192 [01:54<01:54, 34.14it/s, est. speed input: 38156.98 toks/s, output: 37.26 toks/s]
Processed prompts:  52%|█████▏    | 4292/8192 [01:55<01:54, 34.13it/s, est. speed input: 38144.76 toks/s, output: 37.25 toks/s]
Processed prompts:  53%|█████▎    | 4307/8192 [01:55<01:53, 34.13it/s, est. speed input: 38132.51 toks/s, output: 37.24 toks/s]
Processed prompts:  53%|█████▎    | 4322/8192 [01:56<01:53, 34.13it/s, est. speed input: 38120.48 toks/s, output: 37.23 toks/s]
Processed prompts:  53%|█████▎    | 4337/8192 [01:56<01:52, 34.13it/s, est. speed input: 38108.49 toks/s, output: 37.22 toks/s]
Processed prompts:  53%|█████▎    | 4352/8192 [01:56<01:52, 34.13it/s, est. speed input: 38096.61 toks/s, output: 37.20 toks/s]
Processed prompts:  53%|█████▎    | 4367/8192 [01:57<01:52, 34.12it/s, est. speed input: 38084.76 toks/s, output: 37.19 toks/s]
Processed prompts:  53%|█████▎    | 4382/8192 [01:57<01:51, 34.12it/s, est. speed input: 38073.06 toks/s, output: 37.18 toks/s]
Processed prompts:  54%|█████▎    | 4397/8192 [01:58<01:51, 34.12it/s, est. speed input: 38061.40 toks/s, output: 37.17 toks/s]
Processed prompts:  54%|█████▍    | 4412/8192 [01:58<01:50, 34.13it/s, est. speed input: 38049.93 toks/s, output: 37.16 toks/s]
Processed prompts:  54%|█████▍    | 4427/8192 [01:59<01:50, 34.14it/s, est. speed input: 38038.60 toks/s, output: 37.15 toks/s]
Processed prompts:  54%|█████▍    | 4442/8192 [01:59<01:49, 34.13it/s, est. speed input: 38027.15 toks/s, output: 37.14 toks/s]
Processed prompts:  54%|█████▍    | 4457/8192 [02:00<01:49, 34.12it/s, est. speed input: 38015.83 toks/s, output: 37.12 toks/s]
Processed prompts:  55%|█████▍    | 4472/8192 [02:00<01:49, 34.12it/s, est. speed input: 38004.58 toks/s, output: 37.11 toks/s]
Processed prompts:  55%|█████▍    | 4487/8192 [02:00<01:48, 34.13it/s, est. speed input: 37993.49 toks/s, output: 37.10 toks/s]
Processed prompts:  55%|█████▍    | 4502/8192 [02:01<01:48, 34.12it/s, est. speed input: 37982.39 toks/s, output: 37.09 toks/s]
Processed prompts:  55%|█████▌    | 4517/8192 [02:01<01:47, 34.13it/s, est. speed input: 37971.51 toks/s, output: 37.08 toks/s]
Processed prompts:  55%|█████▌    | 4532/8192 [02:02<01:47, 34.13it/s, est. speed input: 37960.67 toks/s, output: 37.07 toks/s]
Processed prompts:  56%|█████▌    | 4547/8192 [02:02<01:46, 34.12it/s, est. speed input: 37949.81 toks/s, output: 37.06 toks/s]
Processed prompts:  56%|█████▌    | 4562/8192 [02:03<01:46, 34.12it/s, est. speed input: 37939.04 toks/s, output: 37.05 toks/s]
Processed prompts:  56%|█████▌    | 4577/8192 [02:03<01:45, 34.12it/s, est. speed input: 37928.35 toks/s, output: 37.04 toks/s]
Processed prompts:  56%|█████▌    | 4592/8192 [02:04<01:45, 34.13it/s, est. speed input: 37917.88 toks/s, output: 37.03 toks/s]
Processed prompts:  56%|█████▌    | 4607/8192 [02:04<01:45, 34.12it/s, est. speed input: 37907.28 toks/s, output: 37.02 toks/s]
Processed prompts:  56%|█████▋    | 4622/8192 [02:04<01:44, 34.12it/s, est. speed input: 37896.84 toks/s, output: 37.01 toks/s]
Processed prompts:  57%|█████▋    | 4637/8192 [02:05<01:44, 34.12it/s, est. speed input: 37886.46 toks/s, output: 37.00 toks/s]
Processed prompts:  57%|█████▋    | 4652/8192 [02:05<01:43, 34.12it/s, est. speed input: 37876.13 toks/s, output: 36.99 toks/s]
Processed prompts:  57%|█████▋    | 4667/8192 [02:06<01:43, 34.13it/s, est. speed input: 37865.99 toks/s, output: 36.98 toks/s]
Processed prompts:  57%|█████▋    | 4682/8192 [02:06<01:42, 34.12it/s, est. speed input: 37855.75 toks/s, output: 36.97 toks/s]
Processed prompts:  57%|█████▋    | 4697/8192 [02:07<01:42, 34.12it/s, est. speed input: 37845.69 toks/s, output: 36.96 toks/s]
Processed prompts:  58%|█████▊    | 4712/8192 [02:07<01:41, 34.12it/s, est. speed input: 37835.64 toks/s, output: 36.95 toks/s]
Processed prompts:  58%|█████▊    | 4727/8192 [02:07<01:41, 34.12it/s, est. speed input: 37825.70 toks/s, output: 36.94 toks/s]
Processed prompts:  58%|█████▊    | 4742/8192 [02:08<01:41, 34.12it/s, est. speed input: 37815.83 toks/s, output: 36.93 toks/s]
Processed prompts:  58%|█████▊    | 4757/8192 [02:08<01:40, 34.12it/s, est. speed input: 37806.04 toks/s, output: 36.92 toks/s]
Processed prompts:  58%|█████▊    | 4772/8192 [02:09<01:40, 34.12it/s, est. speed input: 37796.27 toks/s, output: 36.91 toks/s]
Processed prompts:  58%|█████▊    | 4787/8192 [02:09<01:39, 34.12it/s, est. speed input: 37786.63 toks/s, output: 36.90 toks/s]
Processed prompts:  59%|█████▊    | 4802/8192 [02:10<01:39, 34.13it/s, est. speed input: 37777.10 toks/s, output: 36.89 toks/s]
Processed prompts:  59%|█████▉    | 4817/8192 [02:10<01:38, 34.13it/s, est. speed input: 37767.54 toks/s, output: 36.88 toks/s]
Processed prompts:  59%|█████▉    | 4832/8192 [02:11<01:38, 34.13it/s, est. speed input: 37758.16 toks/s, output: 36.87 toks/s]
Processed prompts:  59%|█████▉    | 4847/8192 [02:11<01:38, 34.13it/s, est. speed input: 37748.73 toks/s, output: 36.86 toks/s]
Processed prompts:  59%|█████▉    | 4862/8192 [02:11<01:37, 34.13it/s, est. speed input: 37739.42 toks/s, output: 36.85 toks/s]
Processed prompts:  60%|█████▉    | 4877/8192 [02:12<01:37, 34.12it/s, est. speed input: 37730.07 toks/s, output: 36.85 toks/s]
Processed prompts:  60%|█████▉    | 4892/8192 [02:12<01:36, 34.13it/s, est. speed input: 37720.87 toks/s, output: 36.84 toks/s]
Processed prompts:  60%|█████▉    | 4907/8192 [02:13<01:36, 34.13it/s, est. speed input: 37711.72 toks/s, output: 36.83 toks/s]
Processed prompts:  60%|██████    | 4922/8192 [02:13<01:35, 34.12it/s, est. speed input: 37702.52 toks/s, output: 36.82 toks/s]
Processed prompts:  60%|██████    | 4937/8192 [02:14<01:35, 34.13it/s, est. speed input: 37693.57 toks/s, output: 36.81 toks/s]
Processed prompts:  60%|██████    | 4952/8192 [02:14<01:34, 34.12it/s, est. speed input: 37684.48 toks/s, output: 36.80 toks/s]
Processed prompts:  61%|██████    | 4967/8192 [02:14<01:34, 34.13it/s, est. speed input: 37675.64 toks/s, output: 36.79 toks/s]
Processed prompts:  61%|██████    | 4982/8192 [02:15<01:34, 34.12it/s, est. speed input: 37666.70 toks/s, output: 36.78 toks/s]
Processed prompts:  61%|██████    | 4997/8192 [02:15<01:33, 34.12it/s, est. speed input: 37657.90 toks/s, output: 36.78 toks/s]
Processed prompts:  61%|██████    | 5012/8192 [02:16<01:33, 34.11it/s, est. speed input: 37649.05 toks/s, output: 36.77 toks/s]
Processed prompts:  61%|██████▏   | 5027/8192 [02:16<01:32, 34.11it/s, est. speed input: 37640.29 toks/s, output: 36.76 toks/s]
Processed prompts:  62%|██████▏   | 5042/8192 [02:17<01:32, 34.13it/s, est. speed input: 37631.76 toks/s, output: 36.75 toks/s]
Processed prompts:  62%|██████▏   | 5057/8192 [02:17<01:31, 34.12it/s, est. speed input: 37623.09 toks/s, output: 36.74 toks/s]
Processed prompts:  62%|██████▏   | 5072/8192 [02:18<01:31, 34.12it/s, est. speed input: 37614.60 toks/s, output: 36.73 toks/s]
Processed prompts:  62%|██████▏   | 5087/8192 [02:18<01:31, 34.12it/s, est. speed input: 37606.07 toks/s, output: 36.72 toks/s]
Processed prompts:  62%|██████▏   | 5102/8192 [02:18<01:30, 34.11it/s, est. speed input: 37597.57 toks/s, output: 36.72 toks/s]
Processed prompts:  62%|██████▏   | 5117/8192 [02:19<01:30, 34.11it/s, est. speed input: 37589.14 toks/s, output: 36.71 toks/s]
Processed prompts:  63%|██████▎   | 5132/8192 [02:19<01:29, 34.12it/s, est. speed input: 37580.83 toks/s, output: 36.70 toks/s]
Processed prompts:  63%|██████▎   | 5147/8192 [02:20<01:29, 34.12it/s, est. speed input: 37572.52 toks/s, output: 36.69 toks/s]
Processed prompts:  63%|██████▎   | 5162/8192 [02:20<01:28, 34.12it/s, est. speed input: 37564.32 toks/s, output: 36.68 toks/s]
Processed prompts:  63%|██████▎   | 5177/8192 [02:21<01:28, 34.12it/s, est. speed input: 37556.15 toks/s, output: 36.68 toks/s]
Processed prompts:  63%|██████▎   | 5192/8192 [02:21<01:27, 34.12it/s, est. speed input: 37548.03 toks/s, output: 36.67 toks/s]
Processed prompts:  64%|██████▎   | 5207/8192 [02:22<01:27, 34.13it/s, est. speed input: 37540.01 toks/s, output: 36.66 toks/s]
Processed prompts:  64%|██████▎   | 5222/8192 [02:22<01:27, 34.12it/s, est. speed input: 37531.89 toks/s, output: 36.65 toks/s]
Processed prompts:  64%|██████▍   | 5237/8192 [02:22<01:26, 34.12it/s, est. speed input: 37524.01 toks/s, output: 36.64 toks/s]
Processed prompts:  64%|██████▍   | 5252/8192 [02:23<01:26, 34.13it/s, est. speed input: 37516.12 toks/s, output: 36.64 toks/s]
Processed prompts:  64%|██████▍   | 5267/8192 [02:23<01:25, 34.12it/s, est. speed input: 37508.23 toks/s, output: 36.63 toks/s]
Processed prompts:  64%|██████▍   | 5282/8192 [02:24<01:25, 34.13it/s, est. speed input: 37500.43 toks/s, output: 36.62 toks/s]
Processed prompts:  65%|██████▍   | 5297/8192 [02:24<01:24, 34.12it/s, est. speed input: 37492.61 toks/s, output: 36.61 toks/s]
Processed prompts:  65%|██████▍   | 5312/8192 [02:25<01:24, 34.12it/s, est. speed input: 37484.86 toks/s, output: 36.61 toks/s]
Processed prompts:  65%|██████▌   | 5327/8192 [02:25<01:23, 34.11it/s, est. speed input: 37477.12 toks/s, output: 36.60 toks/s]
Processed prompts:  65%|██████▌   | 5342/8192 [02:25<01:23, 34.11it/s, est. speed input: 37469.41 toks/s, output: 36.59 toks/s]
Processed prompts:  65%|██████▌   | 5357/8192 [02:26<01:23, 34.11it/s, est. speed input: 37461.79 toks/s, output: 36.58 toks/s]
Processed prompts:  66%|██████▌   | 5372/8192 [02:26<01:22, 34.12it/s, est. speed input: 37454.28 toks/s, output: 36.58 toks/s]
Processed prompts:  66%|██████▌   | 5387/8192 [02:27<01:22, 34.11it/s, est. speed input: 37446.67 toks/s, output: 36.57 toks/s]
Processed prompts:  66%|██████▌   | 5402/8192 [02:27<01:21, 34.12it/s, est. speed input: 37439.31 toks/s, output: 36.56 toks/s]
Processed prompts:  66%|██████▌   | 5417/8192 [02:28<01:21, 34.11it/s, est. speed input: 37431.79 toks/s, output: 36.55 toks/s]
Processed prompts:  66%|██████▋   | 5432/8192 [02:28<01:20, 34.11it/s, est. speed input: 37424.43 toks/s, output: 36.55 toks/s]
Processed prompts:  66%|██████▋   | 5447/8192 [02:29<01:20, 34.11it/s, est. speed input: 37417.06 toks/s, output: 36.54 toks/s]
Processed prompts:  67%|██████▋   | 5462/8192 [02:29<01:20, 34.12it/s, est. speed input: 37409.83 toks/s, output: 36.53 toks/s]
Processed prompts:  67%|██████▋   | 5477/8192 [02:29<01:19, 34.12it/s, est. speed input: 37402.60 toks/s, output: 36.53 toks/s]
Processed prompts:  67%|██████▋   | 5492/8192 [02:30<01:19, 34.11it/s, est. speed input: 37395.32 toks/s, output: 36.52 toks/s]
Processed prompts:  67%|██████▋   | 5507/8192 [02:30<01:18, 34.12it/s, est. speed input: 37388.18 toks/s, output: 36.51 toks/s]
Processed prompts:  67%|██████▋   | 5522/8192 [02:31<01:18, 34.12it/s, est. speed input: 37381.04 toks/s, output: 36.50 toks/s]
Processed prompts:  68%|██████▊   | 5537/8192 [02:31<01:17, 34.12it/s, est. speed input: 37373.96 toks/s, output: 36.50 toks/s]
Processed prompts:  68%|██████▊   | 5552/8192 [02:32<01:17, 34.12it/s, est. speed input: 37366.97 toks/s, output: 36.49 toks/s]
Processed prompts:  68%|██████▊   | 5567/8192 [02:32<01:16, 34.12it/s, est. speed input: 37359.94 toks/s, output: 36.48 toks/s]
Processed prompts:  68%|██████▊   | 5582/8192 [02:33<01:16, 34.11it/s, est. speed input: 37352.93 toks/s, output: 36.48 toks/s]
Processed prompts:  68%|██████▊   | 5597/8192 [02:33<01:16, 34.11it/s, est. speed input: 37345.96 toks/s, output: 36.47 toks/s]
Processed prompts:  69%|██████▊   | 5612/8192 [02:33<01:15, 34.11it/s, est. speed input: 37339.10 toks/s, output: 36.46 toks/s]
Processed prompts:  69%|██████▊   | 5627/8192 [02:34<01:15, 34.11it/s, est. speed input: 37332.17 toks/s, output: 36.46 toks/s]
Processed prompts:  69%|██████▉   | 5642/8192 [02:34<01:14, 34.11it/s, est. speed input: 37325.41 toks/s, output: 36.45 toks/s]
Processed prompts:  69%|██████▉   | 5657/8192 [02:35<01:14, 34.12it/s, est. speed input: 37318.67 toks/s, output: 36.44 toks/s]
Processed prompts:  69%|██████▉   | 5672/8192 [02:35<01:13, 34.12it/s, est. speed input: 37311.91 toks/s, output: 36.44 toks/s]
Processed prompts:  69%|██████▉   | 5687/8192 [02:36<01:13, 34.12it/s, est. speed input: 37305.22 toks/s, output: 36.43 toks/s]
Processed prompts:  70%|██████▉   | 5702/8192 [02:36<01:12, 34.12it/s, est. speed input: 37298.56 toks/s, output: 36.42 toks/s]
Processed prompts:  70%|██████▉   | 5717/8192 [02:36<01:12, 34.12it/s, est. speed input: 37291.96 toks/s, output: 36.42 toks/s]
Processed prompts:  70%|██████▉   | 5732/8192 [02:37<01:12, 34.11it/s, est. speed input: 37285.30 toks/s, output: 36.41 toks/s]
Processed prompts:  70%|███████   | 5747/8192 [02:37<01:11, 34.12it/s, est. speed input: 37278.82 toks/s, output: 36.41 toks/s]
Processed prompts:  70%|███████   | 5762/8192 [02:38<01:11, 34.12it/s, est. speed input: 37272.33 toks/s, output: 36.40 toks/s]
Processed prompts:  71%|███████   | 5777/8192 [02:38<01:10, 34.12it/s, est. speed input: 37265.85 toks/s, output: 36.39 toks/s]
Processed prompts:  71%|███████   | 5792/8192 [02:39<01:10, 34.12it/s, est. speed input: 37259.42 toks/s, output: 36.39 toks/s]
Processed prompts:  71%|███████   | 5807/8192 [02:39<01:09, 34.11it/s, est. speed input: 37252.99 toks/s, output: 36.38 toks/s]
Processed prompts:  71%|███████   | 5822/8192 [02:40<01:09, 34.12it/s, est. speed input: 37246.71 toks/s, output: 36.37 toks/s]
Processed prompts:  71%|███████▏  | 5837/8192 [02:40<01:09, 34.11it/s, est. speed input: 37240.29 toks/s, output: 36.37 toks/s]
Processed prompts:  71%|███████▏  | 5852/8192 [02:40<01:08, 34.11it/s, est. speed input: 37234.00 toks/s, output: 36.36 toks/s]
Processed prompts:  72%|███████▏  | 5867/8192 [02:41<01:08, 34.11it/s, est. speed input: 37227.70 toks/s, output: 36.36 toks/s]
Processed prompts:  72%|███████▏  | 5882/8192 [02:41<01:07, 34.11it/s, est. speed input: 37221.42 toks/s, output: 36.35 toks/s]
Processed prompts:  72%|███████▏  | 5897/8192 [02:42<01:07, 34.11it/s, est. speed input: 37215.24 toks/s, output: 36.34 toks/s]
Processed prompts:  72%|███████▏  | 5912/8192 [02:42<01:06, 34.11it/s, est. speed input: 37209.09 toks/s, output: 36.34 toks/s]
Processed prompts:  72%|███████▏  | 5927/8192 [02:43<01:06, 34.12it/s, est. speed input: 37202.98 toks/s, output: 36.33 toks/s]
Processed prompts:  73%|███████▎  | 5942/8192 [02:43<01:05, 34.11it/s, est. speed input: 37196.84 toks/s, output: 36.33 toks/s]
Processed prompts:  73%|███████▎  | 5957/8192 [02:44<01:05, 34.12it/s, est. speed input: 37190.81 toks/s, output: 36.32 toks/s]
Processed prompts:  73%|███████▎  | 5972/8192 [02:44<01:05, 34.12it/s, est. speed input: 37184.78 toks/s, output: 36.31 toks/s]
Processed prompts:  73%|███████▎  | 5987/8192 [02:44<01:04, 34.11it/s, est. speed input: 37178.76 toks/s, output: 36.31 toks/s]
Processed prompts:  73%|███████▎  | 6002/8192 [02:45<01:04, 34.11it/s, est. speed input: 37172.74 toks/s, output: 36.30 toks/s]
Processed prompts:  73%|███████▎  | 6017/8192 [02:45<01:03, 34.11it/s, est. speed input: 37166.78 toks/s, output: 36.30 toks/s]
Processed prompts:  74%|███████▎  | 6032/8192 [02:46<01:03, 34.11it/s, est. speed input: 37160.82 toks/s, output: 36.29 toks/s]
Processed prompts:  74%|███████▍  | 6047/8192 [02:46<01:02, 34.11it/s, est. speed input: 37154.99 toks/s, output: 36.28 toks/s]
Processed prompts:  74%|███████▍  | 6062/8192 [02:47<01:02, 34.12it/s, est. speed input: 37149.21 toks/s, output: 36.28 toks/s]
Processed prompts:  74%|███████▍  | 6077/8192 [02:47<01:01, 34.11it/s, est. speed input: 37143.35 toks/s, output: 36.27 toks/s]
Processed prompts:  74%|███████▍  | 6092/8192 [02:47<01:01, 34.11it/s, est. speed input: 37137.52 toks/s, output: 36.27 toks/s]
Processed prompts:  75%|███████▍  | 6107/8192 [02:48<01:01, 34.11it/s, est. speed input: 37131.74 toks/s, output: 36.26 toks/s]
Processed prompts:  75%|███████▍  | 6122/8192 [02:48<01:00, 34.11it/s, est. speed input: 37126.01 toks/s, output: 36.26 toks/s]
Processed prompts:  75%|███████▍  | 6137/8192 [02:49<01:00, 34.11it/s, est. speed input: 37120.32 toks/s, output: 36.25 toks/s]
Processed prompts:  75%|███████▌  | 6152/8192 [02:49<00:59, 34.11it/s, est. speed input: 37114.66 toks/s, output: 36.24 toks/s]
Processed prompts:  75%|███████▌  | 6167/8192 [02:50<00:59, 34.11it/s, est. speed input: 37109.02 toks/s, output: 36.24 toks/s]
Processed prompts:  75%|███████▌  | 6182/8192 [02:50<00:58, 34.12it/s, est. speed input: 37103.44 toks/s, output: 36.23 toks/s]
Processed prompts:  76%|███████▌  | 6197/8192 [02:51<00:58, 34.11it/s, est. speed input: 37097.83 toks/s, output: 36.23 toks/s]
Processed prompts:  76%|███████▌  | 6212/8192 [02:51<00:58, 34.12it/s, est. speed input: 37092.32 toks/s, output: 36.22 toks/s]
Processed prompts:  76%|███████▌  | 6227/8192 [02:51<00:57, 34.12it/s, est. speed input: 37086.86 toks/s, output: 36.22 toks/s]
Processed prompts:  76%|███████▌  | 6242/8192 [02:52<00:57, 34.10it/s, est. speed input: 37081.21 toks/s, output: 36.21 toks/s]
Processed prompts:  76%|███████▋  | 6257/8192 [02:52<00:56, 34.12it/s, est. speed input: 37075.85 toks/s, output: 36.21 toks/s]
Processed prompts:  77%|███████▋  | 6272/8192 [02:53<00:56, 34.12it/s, est. speed input: 37070.43 toks/s, output: 36.20 toks/s]
Processed prompts:  77%|███████▋  | 6287/8192 [02:53<00:55, 34.11it/s, est. speed input: 37064.90 toks/s, output: 36.20 toks/s]
Processed prompts:  77%|███████▋  | 6302/8192 [02:54<00:55, 34.11it/s, est. speed input: 37059.54 toks/s, output: 36.19 toks/s]
Processed prompts:  77%|███████▋  | 6317/8192 [02:54<00:54, 34.10it/s, est. speed input: 37054.10 toks/s, output: 36.19 toks/s]
Processed prompts:  77%|███████▋  | 6332/8192 [02:55<00:54, 34.11it/s, est. speed input: 37048.85 toks/s, output: 36.18 toks/s]
Processed prompts:  77%|███████▋  | 6347/8192 [02:55<00:54, 34.10it/s, est. speed input: 37043.45 toks/s, output: 36.18 toks/s]
Processed prompts:  78%|███████▊  | 6362/8192 [02:55<00:53, 34.11it/s, est. speed input: 37038.17 toks/s, output: 36.17 toks/s]
Processed prompts:  78%|███████▊  | 6377/8192 [02:56<00:53, 34.10it/s, est. speed input: 37032.87 toks/s, output: 36.16 toks/s]
Processed prompts:  78%|███████▊  | 6392/8192 [02:56<00:52, 34.11it/s, est. speed input: 37027.68 toks/s, output: 36.16 toks/s]
Processed prompts:  78%|███████▊  | 6407/8192 [02:57<00:52, 34.10it/s, est. speed input: 37022.42 toks/s, output: 36.15 toks/s]
Processed prompts:  78%|███████▊  | 6422/8192 [02:57<00:51, 34.11it/s, est. speed input: 37017.26 toks/s, output: 36.15 toks/s]
Processed prompts:  79%|███████▊  | 6437/8192 [02:58<00:51, 34.11it/s, est. speed input: 37012.11 toks/s, output: 36.14 toks/s]
Processed prompts:  79%|███████▉  | 6452/8192 [02:58<00:51, 34.11it/s, est. speed input: 37006.93 toks/s, output: 36.14 toks/s]
Processed prompts:  79%|███████▉  | 6467/8192 [02:58<00:50, 34.11it/s, est. speed input: 37001.88 toks/s, output: 36.13 toks/s]
Processed prompts:  79%|███████▉  | 6482/8192 [02:59<00:50, 34.11it/s, est. speed input: 36996.77 toks/s, output: 36.13 toks/s]
Processed prompts:  79%|███████▉  | 6497/8192 [02:59<00:49, 34.12it/s, est. speed input: 36991.80 toks/s, output: 36.12 toks/s]
Processed prompts:  79%|███████▉  | 6512/8192 [03:00<00:49, 34.11it/s, est. speed input: 36986.72 toks/s, output: 36.12 toks/s]
Processed prompts:  80%|███████▉  | 6527/8192 [03:00<00:48, 34.12it/s, est. speed input: 36981.76 toks/s, output: 36.12 toks/s]
Processed prompts:  80%|███████▉  | 6542/8192 [03:01<00:48, 34.12it/s, est. speed input: 36976.85 toks/s, output: 36.11 toks/s]
Processed prompts:  80%|████████  | 6557/8192 [03:01<00:47, 34.12it/s, est. speed input: 36971.88 toks/s, output: 36.11 toks/s]
Processed prompts:  80%|████████  | 6572/8192 [03:02<00:47, 34.11it/s, est. speed input: 36966.93 toks/s, output: 36.10 toks/s]
Processed prompts:  80%|████████  | 6587/8192 [03:02<00:46, 34.76it/s, est. speed input: 36967.52 toks/s, output: 36.10 toks/s]
Processed prompts:  81%|████████  | 6602/8192 [03:02<00:45, 34.57it/s, est. speed input: 36962.67 toks/s, output: 36.10 toks/s]
Processed prompts:  81%|████████  | 6617/8192 [03:03<00:45, 34.42it/s, est. speed input: 36957.73 toks/s, output: 36.09 toks/s]
Processed prompts:  81%|████████  | 6632/8192 [03:03<00:45, 34.33it/s, est. speed input: 36952.94 toks/s, output: 36.09 toks/s]
Processed prompts:  81%|████████  | 6647/8192 [03:04<00:45, 34.26it/s, est. speed input: 36948.04 toks/s, output: 36.08 toks/s]
Processed prompts:  81%|████████▏ | 6662/8192 [03:04<00:44, 34.21it/s, est. speed input: 36943.22 toks/s, output: 36.08 toks/s]
Processed prompts:  82%|████████▏ | 6677/8192 [03:05<00:44, 34.18it/s, est. speed input: 36938.40 toks/s, output: 36.07 toks/s]
Processed prompts:  82%|████████▏ | 6692/8192 [03:05<00:43, 34.16it/s, est. speed input: 36933.69 toks/s, output: 36.07 toks/s]
Processed prompts:  82%|████████▏ | 6707/8192 [03:05<00:43, 34.15it/s, est. speed input: 36928.97 toks/s, output: 36.06 toks/s]
Processed prompts:  82%|████████▏ | 6722/8192 [03:06<00:43, 34.13it/s, est. speed input: 36924.19 toks/s, output: 36.06 toks/s]
Processed prompts:  82%|████████▏ | 6737/8192 [03:06<00:42, 34.13it/s, est. speed input: 36919.58 toks/s, output: 36.05 toks/s]
Processed prompts:  82%|████████▏ | 6752/8192 [03:07<00:42, 34.12it/s, est. speed input: 36914.87 toks/s, output: 36.05 toks/s]
Processed prompts:  83%|████████▎ | 6767/8192 [03:07<00:41, 34.12it/s, est. speed input: 36910.27 toks/s, output: 36.05 toks/s]
Processed prompts:  83%|████████▎ | 6782/8192 [03:08<00:41, 34.12it/s, est. speed input: 36905.62 toks/s, output: 36.04 toks/s]
Processed prompts:  83%|████████▎ | 6797/8192 [03:08<00:40, 34.12it/s, est. speed input: 36901.03 toks/s, output: 36.04 toks/s]
Processed prompts:  83%|████████▎ | 6812/8192 [03:09<00:40, 34.11it/s, est. speed input: 36896.43 toks/s, output: 36.03 toks/s]
Processed prompts:  83%|████████▎ | 6827/8192 [03:09<00:40, 34.11it/s, est. speed input: 36891.82 toks/s, output: 36.03 toks/s]
Processed prompts:  84%|████████▎ | 6842/8192 [03:09<00:39, 34.11it/s, est. speed input: 36887.30 toks/s, output: 36.02 toks/s]
Processed prompts:  84%|████████▎ | 6857/8192 [03:10<00:39, 34.11it/s, est. speed input: 36882.74 toks/s, output: 36.02 toks/s]
Processed prompts:  84%|████████▍ | 6872/8192 [03:10<00:38, 34.10it/s, est. speed input: 36878.20 toks/s, output: 36.01 toks/s]
Processed prompts:  84%|████████▍ | 6887/8192 [03:11<00:38, 34.11it/s, est. speed input: 36873.78 toks/s, output: 36.01 toks/s]
Processed prompts:  84%|████████▍ | 6902/8192 [03:11<00:37, 34.10it/s, est. speed input: 36869.23 toks/s, output: 36.01 toks/s]
Processed prompts:  84%|████████▍ | 6917/8192 [03:12<00:37, 34.11it/s, est. speed input: 36864.85 toks/s, output: 36.00 toks/s]
Processed prompts:  85%|████████▍ | 6932/8192 [03:12<00:36, 34.11it/s, est. speed input: 36860.38 toks/s, output: 36.00 toks/s]
Processed prompts:  85%|████████▍ | 6947/8192 [03:13<00:36, 34.11it/s, est. speed input: 36856.02 toks/s, output: 35.99 toks/s]
Processed prompts:  85%|████████▍ | 6962/8192 [03:13<00:36, 34.11it/s, est. speed input: 36851.65 toks/s, output: 35.99 toks/s]
Processed prompts:  85%|████████▌ | 6977/8192 [03:13<00:35, 34.12it/s, est. speed input: 36847.37 toks/s, output: 35.98 toks/s]
Processed prompts:  85%|████████▌ | 6992/8192 [03:14<00:35, 34.11it/s, est. speed input: 36842.97 toks/s, output: 35.98 toks/s]
Processed prompts:  86%|████████▌ | 7007/8192 [03:14<00:34, 34.11it/s, est. speed input: 36838.64 toks/s, output: 35.98 toks/s]
Processed prompts:  86%|████████▌ | 7022/8192 [03:15<00:34, 34.11it/s, est. speed input: 36834.32 toks/s, output: 35.97 toks/s]
Processed prompts:  86%|████████▌ | 7037/8192 [03:15<00:33, 34.11it/s, est. speed input: 36830.03 toks/s, output: 35.97 toks/s]
Processed prompts:  86%|████████▌ | 7052/8192 [03:16<00:33, 34.11it/s, est. speed input: 36825.79 toks/s, output: 35.96 toks/s]
Processed prompts:  86%|████████▋ | 7067/8192 [03:16<00:32, 34.11it/s, est. speed input: 36821.55 toks/s, output: 35.96 toks/s]
Processed prompts:  86%|████████▋ | 7082/8192 [03:16<00:32, 34.11it/s, est. speed input: 36817.34 toks/s, output: 35.95 toks/s]
Processed prompts:  87%|████████▋ | 7097/8192 [03:17<00:32, 34.11it/s, est. speed input: 36813.12 toks/s, output: 35.95 toks/s]
Processed prompts:  87%|████████▋ | 7112/8192 [03:17<00:31, 34.12it/s, est. speed input: 36808.97 toks/s, output: 35.95 toks/s]
Processed prompts:  87%|████████▋ | 7127/8192 [03:18<00:31, 34.11it/s, est. speed input: 36804.74 toks/s, output: 35.94 toks/s]
Processed prompts:  87%|████████▋ | 7142/8192 [03:18<00:30, 34.11it/s, est. speed input: 36800.61 toks/s, output: 35.94 toks/s]
Processed prompts:  87%|████████▋ | 7157/8192 [03:19<00:30, 34.11it/s, est. speed input: 36796.48 toks/s, output: 35.93 toks/s]
Processed prompts:  88%|████████▊ | 7172/8192 [03:19<00:29, 34.11it/s, est. speed input: 36792.36 toks/s, output: 35.93 toks/s]
Processed prompts:  88%|████████▊ | 7187/8192 [03:20<00:29, 34.12it/s, est. speed input: 36788.33 toks/s, output: 35.93 toks/s]
Processed prompts:  88%|████████▊ | 7202/8192 [03:20<00:29, 34.12it/s, est. speed input: 36784.25 toks/s, output: 35.92 toks/s]
Processed prompts:  88%|████████▊ | 7217/8192 [03:20<00:28, 34.11it/s, est. speed input: 36780.19 toks/s, output: 35.92 toks/s]
Processed prompts:  88%|████████▊ | 7232/8192 [03:21<00:28, 34.11it/s, est. speed input: 36776.12 toks/s, output: 35.91 toks/s]
Processed prompts:  88%|████████▊ | 7247/8192 [03:21<00:27, 34.12it/s, est. speed input: 36772.19 toks/s, output: 35.91 toks/s]
Processed prompts:  89%|████████▊ | 7262/8192 [03:22<00:27, 34.13it/s, est. speed input: 36768.25 toks/s, output: 35.91 toks/s]
Processed prompts:  89%|████████▉ | 7277/8192 [03:22<00:26, 34.12it/s, est. speed input: 36764.22 toks/s, output: 35.90 toks/s]
Processed prompts:  89%|████████▉ | 7292/8192 [03:23<00:26, 34.11it/s, est. speed input: 36760.24 toks/s, output: 35.90 toks/s]
Processed prompts:  89%|████████▉ | 7307/8192 [03:23<00:25, 34.11it/s, est. speed input: 36756.27 toks/s, output: 35.89 toks/s]
Processed prompts:  89%|████████▉ | 7322/8192 [03:24<00:25, 34.11it/s, est. speed input: 36752.33 toks/s, output: 35.89 toks/s]
Processed prompts:  90%|████████▉ | 7337/8192 [03:24<00:25, 34.10it/s, est. speed input: 36748.34 toks/s, output: 35.89 toks/s]
Processed prompts:  90%|████████▉ | 7352/8192 [03:24<00:24, 34.11it/s, est. speed input: 36744.45 toks/s, output: 35.88 toks/s]
Processed prompts:  90%|████████▉ | 7367/8192 [03:25<00:24, 34.10it/s, est. speed input: 36740.53 toks/s, output: 35.88 toks/s]
Processed prompts:  90%|█████████ | 7382/8192 [03:25<00:23, 34.11it/s, est. speed input: 36736.69 toks/s, output: 35.88 toks/s]
Processed prompts:  90%|█████████ | 7397/8192 [03:26<00:23, 34.12it/s, est. speed input: 36732.88 toks/s, output: 35.87 toks/s]
Processed prompts:  90%|█████████ | 7412/8192 [03:26<00:22, 34.12it/s, est. speed input: 36729.08 toks/s, output: 35.87 toks/s]
Processed prompts:  91%|█████████ | 7427/8192 [03:27<00:22, 34.11it/s, est. speed input: 36725.22 toks/s, output: 35.86 toks/s]
Processed prompts:  91%|█████████ | 7442/8192 [03:27<00:21, 34.11it/s, est. speed input: 36721.41 toks/s, output: 35.86 toks/s]
Processed prompts:  91%|█████████ | 7457/8192 [03:27<00:21, 34.12it/s, est. speed input: 36717.69 toks/s, output: 35.86 toks/s]
Processed prompts:  91%|█████████ | 7472/8192 [03:28<00:21, 34.12it/s, est. speed input: 36713.92 toks/s, output: 35.85 toks/s]
Processed prompts:  91%|█████████▏| 7487/8192 [03:28<00:20, 34.12it/s, est. speed input: 36710.20 toks/s, output: 35.85 toks/s]
Processed prompts:  92%|█████████▏| 7502/8192 [03:29<00:20, 34.11it/s, est. speed input: 36706.41 toks/s, output: 35.85 toks/s]
Processed prompts:  92%|█████████▏| 7517/8192 [03:29<00:19, 34.12it/s, est. speed input: 36702.75 toks/s, output: 35.84 toks/s]
Processed prompts:  92%|█████████▏| 7532/8192 [03:30<00:19, 34.12it/s, est. speed input: 36699.02 toks/s, output: 35.84 toks/s]
Processed prompts:  92%|█████████▏| 7547/8192 [03:30<00:18, 34.11it/s, est. speed input: 36695.32 toks/s, output: 35.84 toks/s]
Processed prompts:  92%|█████████▏| 7562/8192 [03:31<00:18, 34.11it/s, est. speed input: 36691.65 toks/s, output: 35.83 toks/s]
Processed prompts:  92%|█████████▏| 7577/8192 [03:31<00:18, 34.11it/s, est. speed input: 36687.93 toks/s, output: 35.83 toks/s]
Processed prompts:  93%|█████████▎| 7592/8192 [03:31<00:17, 34.12it/s, est. speed input: 36684.37 toks/s, output: 35.82 toks/s]
Processed prompts:  93%|█████████▎| 7607/8192 [03:32<00:17, 34.12it/s, est. speed input: 36680.74 toks/s, output: 35.82 toks/s]
Processed prompts:  93%|█████████▎| 7622/8192 [03:32<00:16, 34.12it/s, est. speed input: 36677.13 toks/s, output: 35.82 toks/s]
Processed prompts:  93%|█████████▎| 7637/8192 [03:33<00:16, 34.11it/s, est. speed input: 36673.50 toks/s, output: 35.81 toks/s]
Processed prompts:  93%|█████████▎| 7652/8192 [03:33<00:15, 34.12it/s, est. speed input: 36669.95 toks/s, output: 35.81 toks/s]
Processed prompts:  94%|█████████▎| 7667/8192 [03:34<00:15, 34.11it/s, est. speed input: 36666.37 toks/s, output: 35.81 toks/s]
Processed prompts:  94%|█████████▍| 7682/8192 [03:34<00:14, 34.12it/s, est. speed input: 36662.83 toks/s, output: 35.80 toks/s]
Processed prompts:  94%|█████████▍| 7697/8192 [03:34<00:14, 34.11it/s, est. speed input: 36659.22 toks/s, output: 35.80 toks/s]
Processed prompts:  94%|█████████▍| 7712/8192 [03:35<00:14, 34.10it/s, est. speed input: 36655.67 toks/s, output: 35.80 toks/s]
Processed prompts:  94%|█████████▍| 7727/8192 [03:35<00:13, 34.11it/s, est. speed input: 36652.14 toks/s, output: 35.79 toks/s]
Processed prompts:  95%|█████████▍| 7742/8192 [03:36<00:13, 34.10it/s, est. speed input: 36648.61 toks/s, output: 35.79 toks/s]
Processed prompts:  95%|█████████▍| 7757/8192 [03:36<00:12, 34.11it/s, est. speed input: 36645.15 toks/s, output: 35.79 toks/s]
Processed prompts:  95%|█████████▍| 7772/8192 [03:37<00:12, 34.12it/s, est. speed input: 36641.73 toks/s, output: 35.78 toks/s]
Processed prompts:  95%|█████████▌| 7787/8192 [03:37<00:11, 34.10it/s, est. speed input: 36638.19 toks/s, output: 35.78 toks/s]
Processed prompts:  95%|█████████▌| 7802/8192 [03:38<00:11, 34.11it/s, est. speed input: 36634.79 toks/s, output: 35.78 toks/s]
Processed prompts:  95%|█████████▌| 7817/8192 [03:38<00:10, 34.12it/s, est. speed input: 36631.38 toks/s, output: 35.77 toks/s]
Processed prompts:  96%|█████████▌| 7832/8192 [03:38<00:10, 34.12it/s, est. speed input: 36628.03 toks/s, output: 35.77 toks/s]
Processed prompts:  96%|█████████▌| 7847/8192 [03:39<00:10, 34.11it/s, est. speed input: 36624.56 toks/s, output: 35.77 toks/s]
Processed prompts:  96%|█████████▌| 7862/8192 [03:39<00:09, 34.12it/s, est. speed input: 36621.24 toks/s, output: 35.76 toks/s]
Processed prompts:  96%|█████████▌| 7877/8192 [03:40<00:09, 34.12it/s, est. speed input: 36617.85 toks/s, output: 35.76 toks/s]
Processed prompts:  96%|█████████▋| 7892/8192 [03:40<00:08, 34.12it/s, est. speed input: 36614.50 toks/s, output: 35.76 toks/s]
Processed prompts:  97%|█████████▋| 7907/8192 [03:41<00:08, 34.12it/s, est. speed input: 36611.17 toks/s, output: 35.75 toks/s]
Processed prompts:  97%|█████████▋| 7922/8192 [03:41<00:07, 34.12it/s, est. speed input: 36607.84 toks/s, output: 35.75 toks/s]
Processed prompts:  97%|█████████▋| 7937/8192 [03:42<00:07, 34.11it/s, est. speed input: 36604.51 toks/s, output: 35.75 toks/s]
Processed prompts:  97%|█████████▋| 7952/8192 [03:42<00:07, 34.11it/s, est. speed input: 36601.16 toks/s, output: 35.74 toks/s]
Processed prompts:  97%|█████████▋| 7967/8192 [03:42<00:06, 34.11it/s, est. speed input: 36597.90 toks/s, output: 35.74 toks/s]
Processed prompts:  97%|█████████▋| 7982/8192 [03:43<00:06, 34.12it/s, est. speed input: 36594.63 toks/s, output: 35.74 toks/s]
Processed prompts:  98%|█████████▊| 7997/8192 [03:43<00:05, 34.11it/s, est. speed input: 36591.37 toks/s, output: 35.73 toks/s]
Processed prompts:  98%|█████████▊| 8012/8192 [03:44<00:05, 34.12it/s, est. speed input: 36588.16 toks/s, output: 35.73 toks/s]
Processed prompts:  98%|█████████▊| 8027/8192 [03:44<00:04, 34.12it/s, est. speed input: 36584.89 toks/s, output: 35.73 toks/s]
Processed prompts:  98%|█████████▊| 8042/8192 [03:45<00:04, 34.11it/s, est. speed input: 36581.62 toks/s, output: 35.72 toks/s]
Processed prompts:  98%|█████████▊| 8057/8192 [03:45<00:03, 34.11it/s, est. speed input: 36578.37 toks/s, output: 35.72 toks/s]
Processed prompts:  99%|█████████▊| 8072/8192 [03:45<00:03, 34.12it/s, est. speed input: 36575.23 toks/s, output: 35.72 toks/s]
Processed prompts:  99%|█████████▊| 8087/8192 [03:46<00:03, 34.11it/s, est. speed input: 36572.03 toks/s, output: 35.71 toks/s]
Processed prompts:  99%|█████████▉| 8102/8192 [03:46<00:02, 34.12it/s, est. speed input: 36568.90 toks/s, output: 35.71 toks/s]
Processed prompts:  99%|█████████▉| 8117/8192 [03:47<00:02, 34.10it/s, est. speed input: 36565.61 toks/s, output: 35.71 toks/s]
Processed prompts:  99%|█████████▉| 8132/8192 [03:47<00:01, 34.11it/s, est. speed input: 36562.52 toks/s, output: 35.71 toks/s]
Processed prompts:  99%|█████████▉| 8147/8192 [03:48<00:01, 34.10it/s, est. speed input: 36559.31 toks/s, output: 35.70 toks/s]
Processed prompts: 100%|█████████▉| 8162/8192 [03:48<00:00, 34.11it/s, est. speed input: 36556.18 toks/s, output: 35.70 toks/s]
Processed prompts: 100%|█████████▉| 8177/8192 [03:49<00:00, 34.13it/s, est. speed input: 36553.19 toks/s, output: 35.70 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [03:49<00:00, 34.13it/s, est. speed input: 36615.43 toks/s, output: 35.76 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [03:49<00:00, 35.76it/s, est. speed input: 36615.43 toks/s, output: 35.76 toks/s]
[rank0]:[W125 19:26:01.655969708 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 288.8s

测试结果:
  Requests/s:   34.16
  Tokens/s:     35015.53
  Total Reqs:   8192
  Elapsed:      239.80s

  [Prefill 分析]
  Total Prefill Tokens: 8388608
  Prefill Tokens/s:     34981.36


------------------------------------------------------------
  生成 CSV: Llama3.2-3B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cublaslt/Llama3.2-3B-INT8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,4.2194,2164.5419,30.3362
1024,1024,1,128,128,31.7698,32564.0440,4.0290
2048,1024,2,256,128,35.7816,36676.1583,7.1545
4096,1024,4,512,128,35.1441,36022.6670,14.5686
8192,1024,8,1024,128,34.4899,35352.1634,29.6898
16384,1024,16,2048,128,34.2543,35110.6813,59.7881
32768,1024,32,4096,128,34.2118,35067.1412,119.7246
65536,1024,64,8192,128,34.1615,35015.5261,239.8022

------------------------------------------------------------

[INFO] 完成: 8 成功, 0 失败

============================================================
  Llama3.2-3B-INT8 | cuSPARSELt (2_4) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_4
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_4

============================================================
[1/8] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-3B-INT8                                │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 19:26:07 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=370809) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=370809) WARNING 01-25 19:26:16 [backends.py:609] Failed to read file <frozen os>
Throughput: 49.55 requests/s, 25418.46 total tokens/s, 49.55 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-25 19:26:07] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:26:07] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:26:07] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:26:07] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:26:07] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:26:07] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:26:07] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:26:07] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:26:07] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:26:07] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:26:07] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:26:07] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:26:07] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:26:07] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 19:26:11] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:26:11] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:26:11] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:26:11] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:26:11] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:26:11] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:26:11] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:26:11] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:26:11] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:26:11] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:26:11] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:26:11] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:26:11] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:26:11] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=370809) [2026-01-25 19:26:11] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=370809) [2026-01-25 19:26:11] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=370809) [2026-01-25 19:26:11] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=370809) [2026-01-25 19:26:11] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=370809) [2026-01-25 19:26:11] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=370809) [2026-01-25 19:26:11] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=370809) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=370809) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.16it/s]
(EngineCore_DP0 pid=370809) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.16it/s]
(EngineCore_DP0 pid=370809) 
(EngineCore_DP0 pid=370809) [2026-01-25 19:26:12] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 3072] -> 1D uint8
(EngineCore_DP0 pid=370809) [2026-01-25 19:26:12] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11796480 bytes
(EngineCore_DP0 pid=370809) [2026-01-25 19:26:12] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=370809) [2026-01-25 19:26:12] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7077888 bytes
(EngineCore_DP0 pid=370809) [2026-01-25 19:26:12] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=370809) [2026-01-25 19:26:12] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 37748736 bytes
(EngineCore_DP0 pid=370809) [2026-01-25 19:26:12] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 8192] -> 1D uint8
(EngineCore_DP0 pid=370809) [2026-01-25 19:26:12] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 18874368 bytes
(EngineCore_DP0 pid=370809) 2026-01-25 19:26:23,029 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=370809) 2026-01-25 19:26:23,042 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=370809) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  2.70it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  2.33it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  2.38it/s]
(EngineCore_DP0 pid=370809) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.31it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.31it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  96%|█████████▌| 123/128 [00:00<00:00, 1224.58it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 1228.59it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<01:00,  2.10it/s, est. speed input: 1077.63 toks/s, output: 2.10 toks/s]
Processed prompts:   6%|▋         | 8/128 [00:00<00:06, 17.17it/s, est. speed input: 6931.31 toks/s, output: 13.54 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:00<00:03, 29.02it/s, est. speed input: 10908.13 toks/s, output: 21.30 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:00<00:02, 38.21it/s, est. speed input: 13815.10 toks/s, output: 26.98 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:00<00:02, 45.27it/s, est. speed input: 16048.77 toks/s, output: 31.35 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:01<00:01, 50.45it/s, est. speed input: 17801.28 toks/s, output: 34.77 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:01<00:01, 54.30it/s, est. speed input: 19226.44 toks/s, output: 37.55 toks/s]
Processed prompts:  39%|███▉      | 50/128 [00:01<00:01, 57.19it/s, est. speed input: 20414.70 toks/s, output: 39.87 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:01<00:01, 59.28it/s, est. speed input: 21414.61 toks/s, output: 41.83 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:01<00:01, 60.57it/s, est. speed input: 22248.48 toks/s, output: 43.45 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:01<00:00, 61.72it/s, est. speed input: 22986.96 toks/s, output: 44.90 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:01<00:00, 62.19it/s, est. speed input: 23602.64 toks/s, output: 46.10 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:01<00:00, 62.73it/s, est. speed input: 24159.22 toks/s, output: 47.19 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:01<00:00, 62.67it/s, est. speed input: 24618.50 toks/s, output: 48.08 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:02<00:00, 62.96it/s, est. speed input: 25051.77 toks/s, output: 48.93 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:02<00:00, 63.15it/s, est. speed input: 25438.87 toks/s, output: 49.69 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:02<00:00, 63.53it/s, est. speed input: 25804.91 toks/s, output: 50.40 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:02<00:00, 63.37it/s, est. speed input: 26109.51 toks/s, output: 51.00 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:02<00:00, 63.54it/s, est. speed input: 26404.17 toks/s, output: 51.57 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 63.54it/s, est. speed input: 26442.70 toks/s, output: 51.65 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 51.64it/s, est. speed input: 26442.70 toks/s, output: 51.65 toks/s]
[rank0]:[W125 19:26:27.505829667 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 25.7s

测试结果:
  Requests/s:   49.55
  Tokens/s:     25418.46
  Total Reqs:   128
  Elapsed:      2.58s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     25368.92

============================================================
[2/8] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-3B-INT8                                │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 19:26:33 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=371594) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=371594) WARNING 01-25 19:26:41 [backends.py:609] Failed to read file <frozen os>
Throughput: 40.97 requests/s, 41999.37 total tokens/s, 40.97 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-25 19:26:32] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:26:33] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:26:33] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:26:33] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:26:33] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:26:33] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:26:33] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:26:33] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:26:33] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:26:33] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:26:33] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:26:33] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:26:33] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:26:33] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 19:26:36] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:26:36] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:26:36] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:26:36] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:26:36] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:26:36] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:26:36] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:26:36] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:26:36] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:26:36] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:26:36] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:26:36] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:26:36] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:26:36] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=371594) [2026-01-25 19:26:37] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=371594) [2026-01-25 19:26:37] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=371594) [2026-01-25 19:26:37] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=371594) [2026-01-25 19:26:37] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=371594) [2026-01-25 19:26:37] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=371594) [2026-01-25 19:26:37] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=371594) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=371594) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.00it/s]
(EngineCore_DP0 pid=371594) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.00it/s]
(EngineCore_DP0 pid=371594) 
(EngineCore_DP0 pid=371594) [2026-01-25 19:26:38] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 3072] -> 1D uint8
(EngineCore_DP0 pid=371594) [2026-01-25 19:26:38] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11796480 bytes
(EngineCore_DP0 pid=371594) [2026-01-25 19:26:38] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=371594) [2026-01-25 19:26:38] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7077888 bytes
(EngineCore_DP0 pid=371594) [2026-01-25 19:26:38] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=371594) [2026-01-25 19:26:38] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 37748736 bytes
(EngineCore_DP0 pid=371594) [2026-01-25 19:26:38] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 8192] -> 1D uint8
(EngineCore_DP0 pid=371594) [2026-01-25 19:26:38] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 18874368 bytes
(EngineCore_DP0 pid=371594) 2026-01-25 19:26:47,928 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=371594) 2026-01-25 19:26:47,942 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=371594) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 14.26it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 14.25it/s]
(EngineCore_DP0 pid=371594) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  5.93it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  5.93it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  50%|█████     | 64/128 [00:00<00:00, 638.75it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 680.03it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:37,  3.39it/s, est. speed input: 3473.29 toks/s, output: 3.39 toks/s]
Processed prompts:   5%|▍         | 6/128 [00:00<00:06, 18.35it/s, est. speed input: 15395.44 toks/s, output: 15.03 toks/s]
Processed prompts:   9%|▊         | 11/128 [00:00<00:04, 27.97it/s, est. speed input: 22393.68 toks/s, output: 21.87 toks/s]
Processed prompts:  12%|█▎        | 16/128 [00:00<00:03, 34.37it/s, est. speed input: 27010.68 toks/s, output: 26.38 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:02, 38.67it/s, est. speed input: 30273.90 toks/s, output: 29.56 toks/s]
Processed prompts:  20%|██        | 26/128 [00:00<00:02, 41.60it/s, est. speed input: 32705.61 toks/s, output: 31.94 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:00<00:02, 43.61it/s, est. speed input: 34586.48 toks/s, output: 33.78 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:01<00:02, 45.01it/s, est. speed input: 36087.63 toks/s, output: 35.24 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:01<00:01, 46.00it/s, est. speed input: 37318.20 toks/s, output: 36.44 toks/s]
Processed prompts:  36%|███▌      | 46/128 [00:01<00:01, 46.68it/s, est. speed input: 38338.40 toks/s, output: 37.44 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:01<00:01, 47.09it/s, est. speed input: 39185.80 toks/s, output: 38.27 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:01<00:01, 47.38it/s, est. speed input: 39911.05 toks/s, output: 38.98 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:01<00:01, 47.60it/s, est. speed input: 40542.14 toks/s, output: 39.59 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:01<00:01, 47.69it/s, est. speed input: 41082.52 toks/s, output: 40.12 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:01<00:01, 47.82it/s, est. speed input: 41569.36 toks/s, output: 40.59 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:01<00:01, 47.92it/s, est. speed input: 42003.71 toks/s, output: 41.02 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:01<00:00, 47.99it/s, est. speed input: 42390.94 toks/s, output: 41.40 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:02<00:00, 47.99it/s, est. speed input: 42731.58 toks/s, output: 41.73 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:02<00:00, 48.06it/s, est. speed input: 43050.62 toks/s, output: 42.04 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:02<00:00, 48.07it/s, est. speed input: 43334.10 toks/s, output: 42.32 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:02<00:00, 48.05it/s, est. speed input: 43590.20 toks/s, output: 42.57 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:02<00:00, 48.03it/s, est. speed input: 43823.26 toks/s, output: 42.80 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:02<00:00, 48.05it/s, est. speed input: 44041.84 toks/s, output: 43.01 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:02<00:00, 48.09it/s, est. speed input: 44246.11 toks/s, output: 43.21 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:02<00:00, 48.12it/s, est. speed input: 44436.01 toks/s, output: 43.39 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:02<00:00, 48.14it/s, est. speed input: 44611.93 toks/s, output: 43.57 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 48.14it/s, est. speed input: 44676.89 toks/s, output: 43.63 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 43.63it/s, est. speed input: 44676.89 toks/s, output: 43.63 toks/s]
[rank0]:[W125 19:26:52.320335507 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 24.8s

测试结果:
  Requests/s:   40.97
  Tokens/s:     41999.37
  Total Reqs:   128
  Elapsed:      3.12s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     41958.40

============================================================
[3/8] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-3B-INT8                                │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 19:26:58 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=372272) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=372272) WARNING 01-25 19:27:07 [backends.py:609] Failed to read file <frozen os>
Throughput: 49.29 requests/s, 50522.66 total tokens/s, 49.29 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-25 19:26:58] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:26:58] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:26:58] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:26:58] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:26:58] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:26:58] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:26:58] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:26:58] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:26:58] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:26:58] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:26:58] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:26:58] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:26:58] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:26:58] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 19:27:01] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:27:01] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:27:01] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:27:01] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:27:01] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:27:01] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:27:01] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:27:01] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:27:01] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:27:01] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:27:01] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:27:01] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:27:01] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:27:01] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=372272) [2026-01-25 19:27:02] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=372272) [2026-01-25 19:27:02] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=372272) [2026-01-25 19:27:02] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=372272) [2026-01-25 19:27:02] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=372272) [2026-01-25 19:27:02] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=372272) [2026-01-25 19:27:02] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=372272) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=372272) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.07it/s]
(EngineCore_DP0 pid=372272) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.07it/s]
(EngineCore_DP0 pid=372272) 
(EngineCore_DP0 pid=372272) [2026-01-25 19:27:03] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 3072] -> 1D uint8
(EngineCore_DP0 pid=372272) [2026-01-25 19:27:03] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11796480 bytes
(EngineCore_DP0 pid=372272) [2026-01-25 19:27:03] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=372272) [2026-01-25 19:27:03] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7077888 bytes
(EngineCore_DP0 pid=372272) [2026-01-25 19:27:03] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=372272) [2026-01-25 19:27:03] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 37748736 bytes
(EngineCore_DP0 pid=372272) [2026-01-25 19:27:03] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 8192] -> 1D uint8
(EngineCore_DP0 pid=372272) [2026-01-25 19:27:03] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 18874368 bytes
(EngineCore_DP0 pid=372272) 2026-01-25 19:27:13,284 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=372272) 2026-01-25 19:27:13,298 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=372272) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 24.72it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 24.70it/s]
(EngineCore_DP0 pid=372272) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 1/2 [00:00<00:00,  7.52it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 12.25it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   0%|          | 1/256 [00:00<00:36,  7.00it/s]
Adding requests:  29%|██▉       | 75/256 [00:00<00:00, 371.46it/s]
Adding requests:  59%|█████▊    | 150/256 [00:00<00:00, 528.58it/s]
Adding requests:  87%|████████▋ | 222/256 [00:00<00:00, 598.58it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 523.36it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   8%|▊         | 20/256 [00:00<00:01, 170.34it/s, est. speed input: 174447.50 toks/s, output: 170.35 toks/s]
Processed prompts:  15%|█▍        | 38/256 [00:00<00:02, 73.90it/s, est. speed input: 83108.90 toks/s, output: 81.16 toks/s]   
Processed prompts:  19%|█▉        | 49/256 [00:00<00:03, 67.04it/s, est. speed input: 75644.78 toks/s, output: 73.87 toks/s]
Processed prompts:  23%|██▎       | 58/256 [00:00<00:03, 59.57it/s, est. speed input: 69207.36 toks/s, output: 67.58 toks/s]
Processed prompts:  25%|██▌       | 65/256 [00:00<00:03, 59.59it/s, est. speed input: 68234.55 toks/s, output: 66.64 toks/s]
Processed prompts:  28%|██▊       | 72/256 [00:01<00:03, 55.05it/s, est. speed input: 65165.07 toks/s, output: 63.64 toks/s]
Processed prompts:  30%|███       | 78/256 [00:01<00:03, 54.12it/s, est. speed input: 63982.97 toks/s, output: 62.48 toks/s]
Processed prompts:  33%|███▎      | 84/256 [00:01<00:03, 53.30it/s, est. speed input: 62970.73 toks/s, output: 61.49 toks/s]
Processed prompts:  35%|███▌      | 90/256 [00:01<00:03, 52.74it/s, est. speed input: 62142.69 toks/s, output: 60.69 toks/s]
Processed prompts:  38%|███▊      | 96/256 [00:01<00:03, 52.37it/s, est. speed input: 61450.22 toks/s, output: 60.01 toks/s]
Processed prompts:  40%|███▉      | 102/256 [00:01<00:02, 52.04it/s, est. speed input: 60836.22 toks/s, output: 59.41 toks/s]
Processed prompts:  42%|████▏     | 108/256 [00:01<00:02, 51.71it/s, est. speed input: 60276.98 toks/s, output: 58.86 toks/s]
Processed prompts:  45%|████▍     | 114/256 [00:01<00:02, 51.58it/s, est. speed input: 59811.22 toks/s, output: 58.41 toks/s]
Processed prompts:  47%|████▋     | 120/256 [00:02<00:02, 51.53it/s, est. speed input: 59406.19 toks/s, output: 58.01 toks/s]
Processed prompts:  49%|████▉     | 126/256 [00:02<00:02, 51.51it/s, est. speed input: 59047.64 toks/s, output: 57.66 toks/s]
Processed prompts:  52%|█████▏    | 132/256 [00:02<00:02, 51.49it/s, est. speed input: 58724.66 toks/s, output: 57.35 toks/s]
Processed prompts:  54%|█████▍    | 138/256 [00:02<00:02, 51.45it/s, est. speed input: 58428.14 toks/s, output: 57.06 toks/s]
Processed prompts:  56%|█████▋    | 144/256 [00:02<00:02, 51.39it/s, est. speed input: 58153.55 toks/s, output: 56.79 toks/s]
Processed prompts:  59%|█████▊    | 150/256 [00:02<00:02, 51.37it/s, est. speed input: 57907.11 toks/s, output: 56.55 toks/s]
Processed prompts:  61%|██████    | 156/256 [00:02<00:01, 51.34it/s, est. speed input: 57678.29 toks/s, output: 56.33 toks/s]
Processed prompts:  63%|██████▎   | 162/256 [00:02<00:01, 51.31it/s, est. speed input: 57467.14 toks/s, output: 56.12 toks/s]
Processed prompts:  66%|██████▌   | 168/256 [00:03<00:01, 51.23it/s, est. speed input: 57264.34 toks/s, output: 55.92 toks/s]
Processed prompts:  68%|██████▊   | 174/256 [00:03<00:01, 51.21it/s, est. speed input: 57081.19 toks/s, output: 55.74 toks/s]
Processed prompts:  70%|███████   | 180/256 [00:03<00:01, 51.24it/s, est. speed input: 56917.81 toks/s, output: 55.58 toks/s]
Processed prompts:  73%|███████▎  | 186/256 [00:03<00:01, 51.22it/s, est. speed input: 56759.73 toks/s, output: 55.43 toks/s]
Processed prompts:  75%|███████▌  | 192/256 [00:03<00:01, 51.08it/s, est. speed input: 56596.19 toks/s, output: 55.27 toks/s]
Processed prompts:  77%|███████▋  | 198/256 [00:03<00:01, 51.14it/s, est. speed input: 56463.64 toks/s, output: 55.14 toks/s]
Processed prompts:  80%|███████▉  | 204/256 [00:03<00:01, 51.23it/s, est. speed input: 56343.88 toks/s, output: 55.02 toks/s]
Processed prompts:  82%|████████▏ | 210/256 [00:03<00:00, 51.14it/s, est. speed input: 56215.36 toks/s, output: 54.90 toks/s]
Processed prompts:  84%|████████▍ | 216/256 [00:03<00:00, 51.13it/s, est. speed input: 56099.57 toks/s, output: 54.78 toks/s]
Processed prompts:  87%|████████▋ | 222/256 [00:04<00:00, 51.17it/s, est. speed input: 55995.90 toks/s, output: 54.68 toks/s]
Processed prompts:  89%|████████▉ | 228/256 [00:04<00:00, 51.23it/s, est. speed input: 55900.66 toks/s, output: 54.59 toks/s]
Processed prompts:  91%|█████████▏| 234/256 [00:04<00:00, 51.29it/s, est. speed input: 55812.23 toks/s, output: 54.50 toks/s]
Processed prompts:  94%|█████████▍| 240/256 [00:04<00:00, 51.32it/s, est. speed input: 55727.74 toks/s, output: 54.42 toks/s]
Processed prompts:  96%|█████████▌| 246/256 [00:04<00:00, 51.29it/s, est. speed input: 55643.65 toks/s, output: 54.34 toks/s]
Processed prompts:  98%|█████████▊| 252/256 [00:04<00:00, 51.28it/s, est. speed input: 55564.20 toks/s, output: 54.26 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:04<00:00, 51.28it/s, est. speed input: 55730.39 toks/s, output: 54.42 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:04<00:00, 54.42it/s, est. speed input: 55730.39 toks/s, output: 54.42 toks/s]
[rank0]:[W125 19:27:19.676167522 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 27.4s

测试结果:
  Requests/s:   49.29
  Tokens/s:     50522.66
  Total Reqs:   256
  Elapsed:      5.19s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     50473.37

============================================================
[4/8] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-3B-INT8                                │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 19:27:26 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=372948) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=372948) WARNING 01-25 19:27:35 [backends.py:609] Failed to read file <frozen os>
Throughput: 48.16 requests/s, 49360.53 total tokens/s, 48.16 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-25 19:27:26] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:27:26] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:27:26] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:27:26] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:27:26] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:27:26] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:27:26] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:27:26] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:27:26] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:27:26] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:27:26] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:27:26] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:27:26] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:27:26] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 19:27:29] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:27:30] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:27:30] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:27:30] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:27:30] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:27:30] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:27:30] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:27:30] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:27:30] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:27:30] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:27:30] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:27:30] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:27:30] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:27:30] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=372948) [2026-01-25 19:27:30] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=372948) [2026-01-25 19:27:30] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=372948) [2026-01-25 19:27:30] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=372948) [2026-01-25 19:27:30] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=372948) [2026-01-25 19:27:30] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=372948) [2026-01-25 19:27:30] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=372948) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=372948) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.82it/s]
(EngineCore_DP0 pid=372948) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.82it/s]
(EngineCore_DP0 pid=372948) 
(EngineCore_DP0 pid=372948) [2026-01-25 19:27:31] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 3072] -> 1D uint8
(EngineCore_DP0 pid=372948) [2026-01-25 19:27:31] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11796480 bytes
(EngineCore_DP0 pid=372948) [2026-01-25 19:27:31] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=372948) [2026-01-25 19:27:31] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7077888 bytes
(EngineCore_DP0 pid=372948) [2026-01-25 19:27:31] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=372948) [2026-01-25 19:27:31] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 37748736 bytes
(EngineCore_DP0 pid=372948) [2026-01-25 19:27:31] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 8192] -> 1D uint8
(EngineCore_DP0 pid=372948) [2026-01-25 19:27:31] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 18874368 bytes
(EngineCore_DP0 pid=372948) 2026-01-25 19:27:40,926 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=372948) 2026-01-25 19:27:40,941 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=372948) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 3/4 [00:00<00:00, 27.81it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 25.80it/s]
(EngineCore_DP0 pid=372948) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  33%|███▎      | 1/3 [00:00<00:00,  7.53it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 15.56it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:  14%|█▍        | 71/512 [00:00<00:00, 709.97it/s]
Adding requests:  28%|██▊       | 142/512 [00:00<00:00, 649.22it/s]
Adding requests:  42%|████▏     | 213/512 [00:00<00:00, 674.34it/s]
Adding requests:  57%|█████▋    | 290/512 [00:00<00:00, 707.80it/s]
Adding requests:  71%|███████▏  | 366/512 [00:00<00:00, 724.44it/s]
Adding requests:  86%|████████▌ | 441/512 [00:00<00:00, 731.18it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 717.01it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   7%|▋         | 34/512 [00:00<00:01, 303.53it/s, est. speed input: 310849.90 toks/s, output: 303.54 toks/s]
Processed prompts:  13%|█▎        | 65/512 [00:00<00:05, 83.67it/s, est. speed input: 96668.45 toks/s, output: 94.40 toks/s]   
Processed prompts:  16%|█▌        | 81/512 [00:01<00:06, 68.98it/s, est. speed input: 81470.99 toks/s, output: 79.56 toks/s]
Processed prompts:  18%|█▊        | 92/512 [00:01<00:06, 61.42it/s, est. speed input: 74469.61 toks/s, output: 72.72 toks/s]
Processed prompts:  20%|█▉        | 101/512 [00:01<00:06, 59.79it/s, est. speed input: 72282.22 toks/s, output: 70.59 toks/s]
Processed prompts:  21%|██▏       | 109/512 [00:01<00:07, 57.00it/s, est. speed input: 69933.92 toks/s, output: 68.29 toks/s]
Processed prompts:  23%|██▎       | 116/512 [00:01<00:07, 53.23it/s, est. speed input: 67458.58 toks/s, output: 65.88 toks/s]
Processed prompts:  24%|██▍       | 122/512 [00:01<00:08, 48.65it/s, est. speed input: 64862.79 toks/s, output: 63.34 toks/s]
Processed prompts:  25%|██▌       | 130/512 [00:02<00:07, 48.59it/s, est. speed input: 63657.84 toks/s, output: 62.17 toks/s]
Processed prompts:  27%|██▋       | 138/512 [00:02<00:07, 48.58it/s, est. speed input: 62639.12 toks/s, output: 61.17 toks/s]
Processed prompts:  29%|██▊       | 146/512 [00:02<00:07, 48.58it/s, est. speed input: 61760.55 toks/s, output: 60.31 toks/s]
Processed prompts:  30%|███       | 154/512 [00:02<00:07, 48.51it/s, est. speed input: 60976.18 toks/s, output: 59.55 toks/s]
Processed prompts:  32%|███▏      | 162/512 [00:02<00:07, 48.49it/s, est. speed input: 60294.52 toks/s, output: 58.88 toks/s]
Processed prompts:  33%|███▎      | 170/512 [00:02<00:07, 48.48it/s, est. speed input: 59690.29 toks/s, output: 58.29 toks/s]
Processed prompts:  35%|███▍      | 178/512 [00:03<00:06, 48.44it/s, est. speed input: 59143.95 toks/s, output: 57.76 toks/s]
Processed prompts:  36%|███▋      | 186/512 [00:03<00:06, 48.50it/s, est. speed input: 58670.56 toks/s, output: 57.30 toks/s]
Processed prompts:  38%|███▊      | 194/512 [00:03<00:06, 48.49it/s, est. speed input: 58233.52 toks/s, output: 56.87 toks/s]
Processed prompts:  39%|███▉      | 202/512 [00:03<00:06, 48.45it/s, est. speed input: 57830.67 toks/s, output: 56.48 toks/s]
Processed prompts:  41%|████      | 210/512 [00:03<00:06, 48.50it/s, est. speed input: 57476.03 toks/s, output: 56.13 toks/s]
Processed prompts:  43%|████▎     | 218/512 [00:03<00:06, 48.52it/s, est. speed input: 57149.49 toks/s, output: 55.81 toks/s]
Processed prompts:  44%|████▍     | 226/512 [00:04<00:05, 48.51it/s, est. speed input: 56845.24 toks/s, output: 55.51 toks/s]
Processed prompts:  46%|████▌     | 234/512 [00:04<00:05, 48.51it/s, est. speed input: 56566.97 toks/s, output: 55.24 toks/s]
Processed prompts:  47%|████▋     | 242/512 [00:04<00:05, 48.52it/s, est. speed input: 56310.29 toks/s, output: 54.99 toks/s]
Processed prompts:  49%|████▉     | 250/512 [00:04<00:05, 48.53it/s, est. speed input: 56071.37 toks/s, output: 54.76 toks/s]
Processed prompts:  50%|█████     | 258/512 [00:04<00:05, 48.48it/s, est. speed input: 55843.52 toks/s, output: 54.53 toks/s]
Processed prompts:  52%|█████▏    | 266/512 [00:04<00:05, 48.44it/s, est. speed input: 55629.20 toks/s, output: 54.33 toks/s]
Processed prompts:  54%|█████▎    | 274/512 [00:05<00:04, 48.40it/s, est. speed input: 55427.41 toks/s, output: 54.13 toks/s]
Processed prompts:  55%|█████▌    | 282/512 [00:05<00:04, 48.44it/s, est. speed input: 55246.62 toks/s, output: 53.95 toks/s]
Processed prompts:  57%|█████▋    | 290/512 [00:05<00:04, 48.43it/s, est. speed input: 55072.30 toks/s, output: 53.78 toks/s]
Processed prompts:  58%|█████▊    | 298/512 [00:05<00:04, 48.38it/s, est. speed input: 54904.52 toks/s, output: 53.62 toks/s]
Processed prompts:  60%|█████▉    | 306/512 [00:05<00:04, 48.40it/s, est. speed input: 54751.12 toks/s, output: 53.47 toks/s]
Processed prompts:  61%|██████▏   | 314/512 [00:05<00:04, 48.41it/s, est. speed input: 54606.59 toks/s, output: 53.33 toks/s]
Processed prompts:  63%|██████▎   | 322/512 [00:06<00:03, 48.45it/s, est. speed input: 54473.45 toks/s, output: 53.20 toks/s]
Processed prompts:  64%|██████▍   | 330/512 [00:06<00:03, 48.47it/s, est. speed input: 54345.83 toks/s, output: 53.07 toks/s]
Processed prompts:  66%|██████▌   | 338/512 [00:06<00:03, 48.50it/s, est. speed input: 54227.56 toks/s, output: 52.96 toks/s]
Processed prompts:  68%|██████▊   | 346/512 [00:06<00:03, 48.50it/s, est. speed input: 54112.04 toks/s, output: 52.84 toks/s]
Processed prompts:  69%|██████▉   | 354/512 [00:06<00:03, 48.49it/s, est. speed input: 54002.42 toks/s, output: 52.74 toks/s]
Processed prompts:  71%|███████   | 362/512 [00:06<00:03, 48.48it/s, est. speed input: 53897.20 toks/s, output: 52.63 toks/s]
Processed prompts:  72%|███████▏  | 370/512 [00:07<00:02, 48.47it/s, est. speed input: 53796.70 toks/s, output: 52.54 toks/s]
Processed prompts:  74%|███████▍  | 378/512 [00:07<00:02, 48.45it/s, est. speed input: 53699.53 toks/s, output: 52.44 toks/s]
Processed prompts:  75%|███████▌  | 386/512 [00:07<00:02, 48.44it/s, est. speed input: 53607.19 toks/s, output: 52.35 toks/s]
Processed prompts:  77%|███████▋  | 394/512 [00:07<00:02, 48.46it/s, est. speed input: 53520.87 toks/s, output: 52.27 toks/s]
Processed prompts:  79%|███████▊  | 402/512 [00:07<00:02, 48.40it/s, est. speed input: 53432.95 toks/s, output: 52.18 toks/s]
Processed prompts:  80%|████████  | 410/512 [00:07<00:02, 48.43it/s, est. speed input: 53353.52 toks/s, output: 52.10 toks/s]
Processed prompts:  82%|████████▏ | 418/512 [00:08<00:01, 48.41it/s, est. speed input: 53275.05 toks/s, output: 52.03 toks/s]
Processed prompts:  83%|████████▎ | 426/512 [00:08<00:01, 48.44it/s, est. speed input: 53202.21 toks/s, output: 51.96 toks/s]
Processed prompts:  85%|████████▍ | 434/512 [00:08<00:01, 48.45it/s, est. speed input: 53131.93 toks/s, output: 51.89 toks/s]
Processed prompts:  86%|████████▋ | 442/512 [00:08<00:01, 48.42it/s, est. speed input: 53061.69 toks/s, output: 51.82 toks/s]
Processed prompts:  88%|████████▊ | 450/512 [00:08<00:01, 48.44it/s, est. speed input: 52996.73 toks/s, output: 51.75 toks/s]
Processed prompts:  89%|████████▉ | 458/512 [00:08<00:01, 48.44it/s, est. speed input: 52933.73 toks/s, output: 51.69 toks/s]
Processed prompts:  91%|█████████ | 466/512 [00:09<00:00, 48.47it/s, est. speed input: 52874.94 toks/s, output: 51.64 toks/s]
Processed prompts:  93%|█████████▎| 474/512 [00:09<00:00, 48.45it/s, est. speed input: 52814.90 toks/s, output: 51.58 toks/s]
Processed prompts:  94%|█████████▍| 482/512 [00:09<00:00, 48.47it/s, est. speed input: 52759.85 toks/s, output: 51.52 toks/s]
Processed prompts:  96%|█████████▌| 490/512 [00:09<00:00, 48.47it/s, est. speed input: 52705.49 toks/s, output: 51.47 toks/s]
Processed prompts:  97%|█████████▋| 498/512 [00:09<00:00, 48.40it/s, est. speed input: 52649.11 toks/s, output: 51.42 toks/s]
Processed prompts:  99%|█████████▉| 506/512 [00:09<00:00, 48.40it/s, est. speed input: 52597.43 toks/s, output: 51.36 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:09<00:00, 48.40it/s, est. speed input: 52867.85 toks/s, output: 51.63 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:09<00:00, 51.63it/s, est. speed input: 52867.85 toks/s, output: 51.63 toks/s]
[rank0]:[W125 19:27:53.945594269 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 33.3s

测试结果:
  Requests/s:   48.16
  Tokens/s:     49360.53
  Total Reqs:   512
  Elapsed:      10.63s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     49312.37

============================================================
[5/8] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-3B-INT8                                │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 19:28:00 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=373695) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=373695) WARNING 01-25 19:28:09 [backends.py:609] Failed to read file <frozen os>
Throughput: 46.40 requests/s, 47556.97 total tokens/s, 46.40 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-25 19:28:00] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:28:00] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:28:00] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:28:00] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:28:00] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:28:00] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:28:00] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:28:00] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:28:00] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:28:00] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:28:00] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:28:00] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:28:00] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:28:00] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 19:28:04] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:28:04] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:28:04] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:28:04] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:28:04] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:28:04] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:28:04] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:28:04] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:28:04] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:28:04] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:28:04] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:28:04] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:28:04] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:28:04] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=373695) [2026-01-25 19:28:05] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=373695) [2026-01-25 19:28:05] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=373695) [2026-01-25 19:28:05] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=373695) [2026-01-25 19:28:05] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=373695) [2026-01-25 19:28:05] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=373695) [2026-01-25 19:28:05] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=373695) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=373695) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.84it/s]
(EngineCore_DP0 pid=373695) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.84it/s]
(EngineCore_DP0 pid=373695) 
(EngineCore_DP0 pid=373695) [2026-01-25 19:28:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 3072] -> 1D uint8
(EngineCore_DP0 pid=373695) [2026-01-25 19:28:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11796480 bytes
(EngineCore_DP0 pid=373695) [2026-01-25 19:28:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=373695) [2026-01-25 19:28:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7077888 bytes
(EngineCore_DP0 pid=373695) [2026-01-25 19:28:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=373695) [2026-01-25 19:28:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 37748736 bytes
(EngineCore_DP0 pid=373695) [2026-01-25 19:28:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 8192] -> 1D uint8
(EngineCore_DP0 pid=373695) [2026-01-25 19:28:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 18874368 bytes
(EngineCore_DP0 pid=373695) 2026-01-25 19:28:15,554 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=373695) 2026-01-25 19:28:15,569 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=373695) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 1/5 [00:00<00:00,  4.08it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 14.44it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 12.53it/s]
(EngineCore_DP0 pid=373695) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 1/4 [00:00<00:00,  7.39it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 17.59it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   7%|▋         | 72/1024 [00:00<00:01, 717.19it/s]
Adding requests:  14%|█▍        | 147/1024 [00:00<00:01, 733.09it/s]
Adding requests:  22%|██▏       | 222/1024 [00:00<00:01, 737.22it/s]
Adding requests:  29%|██▉       | 299/1024 [00:00<00:00, 746.07it/s]
Adding requests:  37%|███▋      | 374/1024 [00:00<00:00, 746.17it/s]
Adding requests:  44%|████▍     | 449/1024 [00:00<00:00, 731.79it/s]
Adding requests:  51%|█████     | 523/1024 [00:00<00:00, 719.94it/s]
Adding requests:  58%|█████▊    | 596/1024 [00:00<00:00, 717.43it/s]
Adding requests:  66%|██████▌   | 671/1024 [00:00<00:00, 726.54it/s]
Adding requests:  73%|███████▎  | 744/1024 [00:01<00:00, 724.72it/s]
Adding requests:  80%|███████▉  | 817/1024 [00:01<00:00, 696.02it/s]
Adding requests:  87%|████████▋ | 891/1024 [00:01<00:00, 707.09it/s]
Adding requests:  94%|█████████▍| 966/1024 [00:01<00:00, 719.06it/s]
Adding requests: 100%|██████████| 1024/1024 [00:01<00:00, 724.45it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   6%|▋         | 66/1024 [00:00<00:03, 318.70it/s, est. speed input: 326362.29 toks/s, output: 318.70 toks/s]
Processed prompts:  10%|▉         | 98/1024 [00:00<00:09, 94.18it/s, est. speed input: 112447.49 toks/s, output: 109.81 toks/s] 
Processed prompts:  11%|█         | 114/1024 [00:01<00:11, 76.54it/s, est. speed input: 94489.96 toks/s, output: 92.27 toks/s] 
Processed prompts:  12%|█▏        | 125/1024 [00:01<00:12, 73.92it/s, est. speed input: 90963.59 toks/s, output: 88.83 toks/s]
Processed prompts:  13%|█▎        | 135/1024 [00:01<00:12, 70.33it/s, est. speed input: 87572.43 toks/s, output: 85.52 toks/s]
Processed prompts:  14%|█▍        | 143/1024 [00:01<00:13, 64.48it/s, est. speed input: 83676.66 toks/s, output: 81.72 toks/s]
Processed prompts:  15%|█▍        | 150/1024 [00:01<00:15, 58.25it/s, est. speed input: 79916.74 toks/s, output: 78.04 toks/s]
Processed prompts:  15%|█▌        | 156/1024 [00:02<00:16, 51.91it/s, est. speed input: 76298.07 toks/s, output: 74.51 toks/s]
Processed prompts:  16%|█▌        | 162/1024 [00:02<00:18, 47.17it/s, est. speed input: 73231.23 toks/s, output: 71.51 toks/s]
Processed prompts:  17%|█▋        | 170/1024 [00:02<00:18, 47.01it/s, est. speed input: 71433.95 toks/s, output: 69.76 toks/s]
Processed prompts:  17%|█▋        | 178/1024 [00:02<00:18, 46.92it/s, est. speed input: 69884.11 toks/s, output: 68.25 toks/s]
Processed prompts:  18%|█▊        | 186/1024 [00:02<00:17, 46.85it/s, est. speed input: 68521.47 toks/s, output: 66.92 toks/s]
Processed prompts:  19%|█▉        | 194/1024 [00:02<00:17, 46.76it/s, est. speed input: 67307.78 toks/s, output: 65.73 toks/s]
Processed prompts:  20%|█▉        | 202/1024 [00:03<00:17, 46.72it/s, est. speed input: 66233.21 toks/s, output: 64.68 toks/s]
Processed prompts:  21%|██        | 210/1024 [00:03<00:17, 46.70it/s, est. speed input: 65272.22 toks/s, output: 63.74 toks/s]
Processed prompts:  21%|██▏       | 218/1024 [00:03<00:17, 46.65it/s, est. speed input: 64398.94 toks/s, output: 62.89 toks/s]
Processed prompts:  22%|██▏       | 226/1024 [00:03<00:17, 46.66it/s, est. speed input: 63615.91 toks/s, output: 62.12 toks/s]
Processed prompts:  23%|██▎       | 234/1024 [00:03<00:16, 46.65it/s, est. speed input: 62900.94 toks/s, output: 61.43 toks/s]
Processed prompts:  24%|██▎       | 242/1024 [00:03<00:16, 46.62it/s, est. speed input: 62243.87 toks/s, output: 60.78 toks/s]
Processed prompts:  24%|██▍       | 250/1024 [00:04<00:16, 46.59it/s, est. speed input: 61638.93 toks/s, output: 60.19 toks/s]
Processed prompts:  25%|██▌       | 258/1024 [00:04<00:16, 46.62it/s, est. speed input: 61091.50 toks/s, output: 59.66 toks/s]
Processed prompts:  26%|██▌       | 266/1024 [00:04<00:16, 46.65it/s, est. speed input: 60585.77 toks/s, output: 59.17 toks/s]
Processed prompts:  27%|██▋       | 274/1024 [00:04<00:16, 46.65it/s, est. speed input: 60116.11 toks/s, output: 58.71 toks/s]
Processed prompts:  28%|██▊       | 282/1024 [00:04<00:15, 46.68it/s, est. speed input: 59682.06 toks/s, output: 58.28 toks/s]
Processed prompts:  28%|██▊       | 290/1024 [00:05<00:15, 46.66it/s, est. speed input: 59273.29 toks/s, output: 57.88 toks/s]
Processed prompts:  29%|██▉       | 298/1024 [00:05<00:15, 46.63it/s, est. speed input: 58889.33 toks/s, output: 57.51 toks/s]
Processed prompts:  30%|██▉       | 306/1024 [00:05<00:15, 46.62it/s, est. speed input: 58530.39 toks/s, output: 57.16 toks/s]
Processed prompts:  31%|███       | 314/1024 [00:05<00:15, 46.62it/s, est. speed input: 58195.53 toks/s, output: 56.83 toks/s]
Processed prompts:  31%|███▏      | 322/1024 [00:05<00:15, 46.61it/s, est. speed input: 57879.50 toks/s, output: 56.52 toks/s]
Processed prompts:  32%|███▏      | 330/1024 [00:05<00:14, 46.63it/s, est. speed input: 57584.26 toks/s, output: 56.23 toks/s]
Processed prompts:  33%|███▎      | 338/1024 [00:06<00:14, 46.56it/s, est. speed input: 57297.19 toks/s, output: 55.95 toks/s]
Processed prompts:  34%|███▍      | 346/1024 [00:06<00:14, 46.59it/s, est. speed input: 57034.32 toks/s, output: 55.70 toks/s]
Processed prompts:  35%|███▍      | 354/1024 [00:06<00:14, 46.60it/s, est. speed input: 56785.04 toks/s, output: 55.45 toks/s]
Processed prompts:  35%|███▌      | 362/1024 [00:06<00:14, 46.63it/s, est. speed input: 56550.09 toks/s, output: 55.22 toks/s]
Processed prompts:  36%|███▌      | 370/1024 [00:06<00:14, 46.66it/s, est. speed input: 56328.46 toks/s, output: 55.01 toks/s]
Processed prompts:  37%|███▋      | 378/1024 [00:06<00:13, 46.65it/s, est. speed input: 56114.91 toks/s, output: 54.80 toks/s]
Processed prompts:  38%|███▊      | 386/1024 [00:07<00:13, 46.63it/s, est. speed input: 55910.53 toks/s, output: 54.60 toks/s]
Processed prompts:  38%|███▊      | 394/1024 [00:07<00:13, 46.61it/s, est. speed input: 55715.89 toks/s, output: 54.41 toks/s]
Processed prompts:  39%|███▉      | 402/1024 [00:07<00:13, 46.63it/s, est. speed input: 55532.56 toks/s, output: 54.23 toks/s]
Processed prompts:  40%|████      | 410/1024 [00:07<00:13, 46.60it/s, est. speed input: 55353.69 toks/s, output: 54.06 toks/s]
Processed prompts:  41%|████      | 418/1024 [00:07<00:12, 46.62it/s, est. speed input: 55186.29 toks/s, output: 53.89 toks/s]
Processed prompts:  42%|████▏     | 426/1024 [00:07<00:12, 46.62it/s, est. speed input: 55025.37 toks/s, output: 53.74 toks/s]
Processed prompts:  42%|████▏     | 434/1024 [00:08<00:12, 46.56it/s, est. speed input: 54866.07 toks/s, output: 53.58 toks/s]
Processed prompts:  43%|████▎     | 442/1024 [00:08<00:12, 46.55it/s, est. speed input: 54716.17 toks/s, output: 53.43 toks/s]
Processed prompts:  44%|████▍     | 450/1024 [00:08<00:12, 46.57it/s, est. speed input: 54573.81 toks/s, output: 53.29 toks/s]
Processed prompts:  45%|████▍     | 458/1024 [00:08<00:12, 46.56it/s, est. speed input: 54436.03 toks/s, output: 53.16 toks/s]
Processed prompts:  46%|████▌     | 466/1024 [00:08<00:11, 46.56it/s, est. speed input: 54303.73 toks/s, output: 53.03 toks/s]
Processed prompts:  46%|████▋     | 474/1024 [00:08<00:11, 46.55it/s, est. speed input: 54176.03 toks/s, output: 52.91 toks/s]
Processed prompts:  47%|████▋     | 482/1024 [00:09<00:11, 46.53it/s, est. speed input: 54052.31 toks/s, output: 52.79 toks/s]
Processed prompts:  48%|████▊     | 490/1024 [00:09<00:11, 46.55it/s, est. speed input: 53935.28 toks/s, output: 52.67 toks/s]
Processed prompts:  49%|████▊     | 498/1024 [00:09<00:11, 46.55it/s, est. speed input: 53821.54 toks/s, output: 52.56 toks/s]
Processed prompts:  49%|████▉     | 506/1024 [00:09<00:11, 46.54it/s, est. speed input: 53710.97 toks/s, output: 52.45 toks/s]
Processed prompts:  50%|█████     | 514/1024 [00:09<00:10, 46.55it/s, est. speed input: 53606.16 toks/s, output: 52.35 toks/s]
Processed prompts:  51%|█████     | 522/1024 [00:09<00:10, 46.58it/s, est. speed input: 53505.52 toks/s, output: 52.25 toks/s]
Processed prompts:  52%|█████▏    | 530/1024 [00:10<00:10, 46.53it/s, est. speed input: 53404.42 toks/s, output: 52.15 toks/s]
Processed prompts:  53%|█████▎    | 538/1024 [00:10<00:10, 46.54it/s, est. speed input: 53309.24 toks/s, output: 52.06 toks/s]
Processed prompts:  53%|█████▎    | 546/1024 [00:10<00:10, 46.58it/s, est. speed input: 53218.96 toks/s, output: 51.97 toks/s]
Processed prompts:  54%|█████▍    | 554/1024 [00:10<00:10, 46.60it/s, est. speed input: 53131.42 toks/s, output: 51.89 toks/s]
Processed prompts:  55%|█████▍    | 562/1024 [00:10<00:09, 46.60it/s, est. speed input: 53045.72 toks/s, output: 51.80 toks/s]
Processed prompts:  56%|█████▌    | 570/1024 [00:11<00:09, 46.60it/s, est. speed input: 52962.95 toks/s, output: 51.72 toks/s]
Processed prompts:  56%|█████▋    | 578/1024 [00:11<00:09, 46.56it/s, est. speed input: 52880.23 toks/s, output: 51.64 toks/s]
Processed prompts:  57%|█████▋    | 586/1024 [00:11<00:09, 46.56it/s, est. speed input: 52801.41 toks/s, output: 51.56 toks/s]
Processed prompts:  58%|█████▊    | 594/1024 [00:11<00:09, 46.54it/s, est. speed input: 52724.28 toks/s, output: 51.49 toks/s]
Processed prompts:  59%|█████▉    | 602/1024 [00:11<00:09, 46.51it/s, est. speed input: 52648.11 toks/s, output: 51.41 toks/s]
Processed prompts:  60%|█████▉    | 610/1024 [00:11<00:08, 46.51it/s, est. speed input: 52575.51 toks/s, output: 51.34 toks/s]
Processed prompts:  60%|██████    | 618/1024 [00:12<00:08, 46.51it/s, est. speed input: 52504.89 toks/s, output: 51.27 toks/s]
Processed prompts:  61%|██████    | 626/1024 [00:12<00:08, 46.50it/s, est. speed input: 52435.84 toks/s, output: 51.21 toks/s]
Processed prompts:  62%|██████▏   | 634/1024 [00:12<00:08, 46.53it/s, est. speed input: 52370.53 toks/s, output: 51.14 toks/s]
Processed prompts:  63%|██████▎   | 642/1024 [00:12<00:08, 46.55it/s, est. speed input: 52306.89 toks/s, output: 51.08 toks/s]
Processed prompts:  63%|██████▎   | 650/1024 [00:12<00:08, 46.57it/s, est. speed input: 52245.09 toks/s, output: 51.02 toks/s]
Processed prompts:  64%|██████▍   | 658/1024 [00:12<00:07, 46.58it/s, est. speed input: 52184.92 toks/s, output: 50.96 toks/s]
Processed prompts:  65%|██████▌   | 666/1024 [00:13<00:07, 46.55it/s, est. speed input: 52124.82 toks/s, output: 50.90 toks/s]
Processed prompts:  66%|██████▌   | 674/1024 [00:13<00:07, 46.59it/s, est. speed input: 52069.04 toks/s, output: 50.85 toks/s]
Processed prompts:  67%|██████▋   | 682/1024 [00:13<00:07, 46.62it/s, est. speed input: 52014.49 toks/s, output: 50.80 toks/s]
Processed prompts:  67%|██████▋   | 690/1024 [00:13<00:07, 46.60it/s, est. speed input: 51959.52 toks/s, output: 50.74 toks/s]
Processed prompts:  68%|██████▊   | 698/1024 [00:13<00:07, 46.55it/s, est. speed input: 51904.54 toks/s, output: 50.69 toks/s]
Processed prompts:  69%|██████▉   | 706/1024 [00:13<00:06, 46.55it/s, est. speed input: 51852.27 toks/s, output: 50.64 toks/s]
Processed prompts:  70%|██████▉   | 714/1024 [00:14<00:06, 46.53it/s, est. speed input: 51800.59 toks/s, output: 50.59 toks/s]
Processed prompts:  71%|███████   | 722/1024 [00:14<00:06, 46.54it/s, est. speed input: 51751.12 toks/s, output: 50.54 toks/s]
Processed prompts:  71%|███████▏  | 730/1024 [00:14<00:06, 46.56it/s, est. speed input: 51703.13 toks/s, output: 50.49 toks/s]
Processed prompts:  72%|███████▏  | 738/1024 [00:14<00:06, 46.55it/s, est. speed input: 51655.61 toks/s, output: 50.44 toks/s]
Processed prompts:  73%|███████▎  | 746/1024 [00:14<00:05, 46.54it/s, est. speed input: 51608.61 toks/s, output: 50.40 toks/s]
Processed prompts:  74%|███████▎  | 754/1024 [00:14<00:05, 46.55it/s, est. speed input: 51563.94 toks/s, output: 50.36 toks/s]
Processed prompts:  74%|███████▍  | 762/1024 [00:15<00:05, 46.54it/s, est. speed input: 51518.98 toks/s, output: 50.31 toks/s]
Processed prompts:  75%|███████▌  | 770/1024 [00:15<00:05, 46.54it/s, est. speed input: 51475.78 toks/s, output: 50.27 toks/s]
Processed prompts:  76%|███████▌  | 778/1024 [00:15<00:05, 46.58it/s, est. speed input: 51435.07 toks/s, output: 50.23 toks/s]
Processed prompts:  77%|███████▋  | 786/1024 [00:15<00:05, 46.57it/s, est. speed input: 51393.80 toks/s, output: 50.19 toks/s]
Processed prompts:  78%|███████▊  | 794/1024 [00:15<00:04, 46.57it/s, est. speed input: 51353.33 toks/s, output: 50.15 toks/s]
Processed prompts:  78%|███████▊  | 802/1024 [00:16<00:04, 46.54it/s, est. speed input: 51312.99 toks/s, output: 50.11 toks/s]
Processed prompts:  79%|███████▉  | 810/1024 [00:16<00:04, 46.52it/s, est. speed input: 51273.32 toks/s, output: 50.07 toks/s]
Processed prompts:  80%|███████▉  | 818/1024 [00:16<00:04, 46.51it/s, est. speed input: 51234.57 toks/s, output: 50.03 toks/s]
Processed prompts:  81%|████████  | 826/1024 [00:16<00:04, 46.54it/s, est. speed input: 51198.17 toks/s, output: 50.00 toks/s]
Processed prompts:  81%|████████▏ | 834/1024 [00:16<00:04, 46.56it/s, est. speed input: 51162.31 toks/s, output: 49.96 toks/s]
Processed prompts:  82%|████████▏ | 842/1024 [00:16<00:03, 46.57it/s, est. speed input: 51127.28 toks/s, output: 49.93 toks/s]
Processed prompts:  83%|████████▎ | 850/1024 [00:17<00:03, 46.55it/s, est. speed input: 51092.01 toks/s, output: 49.89 toks/s]
Processed prompts:  84%|████████▍ | 858/1024 [00:17<00:03, 46.53it/s, est. speed input: 51057.08 toks/s, output: 49.86 toks/s]
Processed prompts:  85%|████████▍ | 866/1024 [00:17<00:03, 46.54it/s, est. speed input: 51023.48 toks/s, output: 49.83 toks/s]
Processed prompts:  85%|████████▌ | 874/1024 [00:17<00:03, 46.53it/s, est. speed input: 50990.15 toks/s, output: 49.80 toks/s]
Processed prompts:  86%|████████▌ | 882/1024 [00:17<00:03, 46.52it/s, est. speed input: 50957.40 toks/s, output: 49.76 toks/s]
Processed prompts:  87%|████████▋ | 890/1024 [00:17<00:02, 46.51it/s, est. speed input: 50925.19 toks/s, output: 49.73 toks/s]
Processed prompts:  88%|████████▊ | 898/1024 [00:18<00:02, 46.47it/s, est. speed input: 50892.23 toks/s, output: 49.70 toks/s]
Processed prompts:  88%|████████▊ | 906/1024 [00:18<00:02, 46.47it/s, est. speed input: 50860.96 toks/s, output: 49.67 toks/s]
Processed prompts:  89%|████████▉ | 914/1024 [00:18<00:02, 46.48it/s, est. speed input: 50830.73 toks/s, output: 49.64 toks/s]
Processed prompts:  90%|█████████ | 922/1024 [00:18<00:02, 46.51it/s, est. speed input: 50801.77 toks/s, output: 49.61 toks/s]
Processed prompts:  91%|█████████ | 930/1024 [00:18<00:02, 46.52it/s, est. speed input: 50773.13 toks/s, output: 49.58 toks/s]
Processed prompts:  92%|█████████▏| 938/1024 [00:18<00:01, 46.51it/s, est. speed input: 50744.29 toks/s, output: 49.55 toks/s]
Processed prompts:  92%|█████████▏| 946/1024 [00:19<00:01, 46.50it/s, est. speed input: 50715.70 toks/s, output: 49.53 toks/s]
Processed prompts:  93%|█████████▎| 954/1024 [00:19<00:01, 46.51it/s, est. speed input: 50688.50 toks/s, output: 49.50 toks/s]
Processed prompts:  94%|█████████▍| 962/1024 [00:19<00:01, 46.53it/s, est. speed input: 50662.14 toks/s, output: 49.47 toks/s]
Processed prompts:  95%|█████████▍| 970/1024 [00:19<00:01, 46.53it/s, est. speed input: 50635.51 toks/s, output: 49.45 toks/s]
Processed prompts:  96%|█████████▌| 978/1024 [00:19<00:00, 46.53it/s, est. speed input: 50609.63 toks/s, output: 49.42 toks/s]
Processed prompts:  96%|█████████▋| 986/1024 [00:19<00:00, 46.50it/s, est. speed input: 50583.06 toks/s, output: 49.40 toks/s]
Processed prompts:  97%|█████████▋| 994/1024 [00:20<00:00, 46.44it/s, est. speed input: 50555.93 toks/s, output: 49.37 toks/s]
Processed prompts:  98%|█████████▊| 1002/1024 [00:20<00:00, 46.45it/s, est. speed input: 50530.70 toks/s, output: 49.35 toks/s]
Processed prompts:  99%|█████████▊| 1010/1024 [00:20<00:00, 46.46it/s, est. speed input: 50506.02 toks/s, output: 49.32 toks/s]
Processed prompts:  99%|█████████▉| 1018/1024 [00:20<00:00, 45.97it/s, est. speed input: 50466.60 toks/s, output: 49.28 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:20<00:00, 45.97it/s, est. speed input: 50763.77 toks/s, output: 49.57 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:20<00:00, 49.57it/s, est. speed input: 50763.77 toks/s, output: 49.57 toks/s]
[rank0]:[W125 19:28:39.315257843 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 46.3s

测试结果:
  Requests/s:   46.40
  Tokens/s:     47556.97
  Total Reqs:   1024
  Elapsed:      22.07s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     47510.57

============================================================
[6/8] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-3B-INT8                                │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 19:28:50 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=374672) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=374672) WARNING 01-25 19:28:59 [backends.py:609] Failed to read file <frozen os>
Throughput: 45.83 requests/s, 46975.61 total tokens/s, 45.83 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-25 19:28:49] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:28:49] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:28:49] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:28:49] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:28:49] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:28:49] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:28:49] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:28:49] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:28:49] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:28:49] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:28:49] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:28:49] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:28:49] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:28:49] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 19:28:53] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:28:53] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:28:53] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:28:53] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:28:53] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:28:53] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:28:53] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:28:53] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:28:53] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:28:53] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:28:53] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:28:53] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:28:53] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:28:53] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=374672) [2026-01-25 19:28:54] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=374672) [2026-01-25 19:28:54] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=374672) [2026-01-25 19:28:54] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=374672) [2026-01-25 19:28:54] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=374672) [2026-01-25 19:28:54] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=374672) [2026-01-25 19:28:54] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=374672) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=374672) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.86it/s]
(EngineCore_DP0 pid=374672) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.86it/s]
(EngineCore_DP0 pid=374672) 
(EngineCore_DP0 pid=374672) [2026-01-25 19:28:55] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 3072] -> 1D uint8
(EngineCore_DP0 pid=374672) [2026-01-25 19:28:55] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11796480 bytes
(EngineCore_DP0 pid=374672) [2026-01-25 19:28:55] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=374672) [2026-01-25 19:28:55] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7077888 bytes
(EngineCore_DP0 pid=374672) [2026-01-25 19:28:55] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=374672) [2026-01-25 19:28:55] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 37748736 bytes
(EngineCore_DP0 pid=374672) [2026-01-25 19:28:55] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 8192] -> 1D uint8
(EngineCore_DP0 pid=374672) [2026-01-25 19:28:55] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 18874368 bytes
(EngineCore_DP0 pid=374672) 2026-01-25 19:29:04,983 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=374672) 2026-01-25 19:29:04,998 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=374672) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 1/7 [00:00<00:00,  8.45it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00, 20.04it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 23.59it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 21.34it/s]
(EngineCore_DP0 pid=374672) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 1/5 [00:00<00:00,  7.55it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 22.22it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 19.89it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   4%|▎         | 73/2048 [00:00<00:02, 726.64it/s]
Adding requests:   7%|▋         | 146/2048 [00:00<00:02, 727.85it/s]
Adding requests:  11%|█         | 221/2048 [00:00<00:02, 736.21it/s]
Adding requests:  15%|█▍        | 299/2048 [00:00<00:02, 748.76it/s]
Adding requests:  18%|█▊        | 374/2048 [00:00<00:02, 746.66it/s]
Adding requests:  22%|██▏       | 450/2048 [00:00<00:02, 750.26it/s]
Adding requests:  26%|██▌       | 526/2048 [00:00<00:02, 741.97it/s]
Adding requests:  29%|██▉       | 602/2048 [00:00<00:01, 747.38it/s]
Adding requests:  33%|███▎      | 682/2048 [00:00<00:01, 762.80it/s]
Adding requests:  37%|███▋      | 759/2048 [00:01<00:01, 756.32it/s]
Adding requests:  41%|████      | 835/2048 [00:01<00:01, 741.36it/s]
Adding requests:  45%|████▍     | 913/2048 [00:01<00:01, 752.23it/s]
Adding requests:  48%|████▊     | 991/2048 [00:01<00:01, 759.51it/s]
Adding requests:  52%|█████▏    | 1070/2048 [00:01<00:01, 765.08it/s]
Adding requests:  56%|█████▌    | 1147/2048 [00:01<00:01, 764.74it/s]
Adding requests:  60%|█████▉    | 1227/2048 [00:01<00:01, 773.90it/s]
Adding requests:  64%|██████▎   | 1305/2048 [00:01<00:00, 763.90it/s]
Adding requests:  67%|██████▋   | 1382/2048 [00:01<00:00, 762.56it/s]
Adding requests:  71%|███████▏  | 1461/2048 [00:01<00:00, 768.49it/s]
Adding requests:  75%|███████▌  | 1540/2048 [00:02<00:00, 774.30it/s]
Adding requests:  79%|███████▉  | 1621/2048 [00:02<00:00, 783.22it/s]
Adding requests:  83%|████████▎ | 1700/2048 [00:02<00:00, 772.56it/s]
Adding requests:  87%|████████▋ | 1778/2048 [00:02<00:00, 764.14it/s]
Adding requests:  91%|█████████ | 1855/2048 [00:02<00:00, 757.63it/s]
Adding requests:  94%|█████████▍| 1931/2048 [00:02<00:00, 743.63it/s]
Adding requests:  98%|█████████▊| 2008/2048 [00:02<00:00, 750.06it/s]
Adding requests: 100%|██████████| 2048/2048 [00:02<00:00, 756.80it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   6%|▌         | 114/2048 [00:00<00:02, 732.72it/s, est. speed input: 750380.49 toks/s, output: 732.75 toks/s]
Processed prompts:   9%|▉         | 188/2048 [00:01<00:18, 102.46it/s, est. speed input: 124377.97 toks/s, output: 121.46 toks/s]
Processed prompts:  11%|█         | 222/2048 [00:02<00:22, 81.11it/s, est. speed input: 101327.20 toks/s, output: 98.95 toks/s]  
Processed prompts:  12%|█▏        | 243/2048 [00:02<00:28, 62.62it/s, est. speed input: 84641.61 toks/s, output: 82.66 toks/s] 
Processed prompts:  13%|█▎        | 258/2048 [00:03<00:30, 58.61it/s, est. speed input: 80358.49 toks/s, output: 78.47 toks/s]
Processed prompts:  13%|█▎        | 274/2048 [00:03<00:31, 55.73it/s, est. speed input: 77167.49 toks/s, output: 75.36 toks/s]
Processed prompts:  14%|█▍        | 290/2048 [00:03<00:32, 53.35it/s, est. speed input: 74542.27 toks/s, output: 72.79 toks/s]
Processed prompts:  15%|█▍        | 306/2048 [00:04<00:33, 51.43it/s, est. speed input: 72332.39 toks/s, output: 70.64 toks/s]
Processed prompts:  16%|█▌        | 322/2048 [00:04<00:34, 49.94it/s, est. speed input: 70449.07 toks/s, output: 68.80 toks/s]
Processed prompts:  17%|█▋        | 338/2048 [00:05<00:35, 48.84it/s, est. speed input: 68833.28 toks/s, output: 67.22 toks/s]
Processed prompts:  17%|█▋        | 354/2048 [00:05<00:35, 47.92it/s, est. speed input: 67393.06 toks/s, output: 65.81 toks/s]
Processed prompts:  18%|█▊        | 370/2048 [00:05<00:35, 47.34it/s, est. speed input: 66153.27 toks/s, output: 64.60 toks/s]
Processed prompts:  19%|█▉        | 386/2048 [00:06<00:35, 46.94it/s, est. speed input: 65058.34 toks/s, output: 63.53 toks/s]
Processed prompts:  20%|█▉        | 402/2048 [00:06<00:35, 46.64it/s, est. speed input: 64081.25 toks/s, output: 62.58 toks/s]
Processed prompts:  20%|██        | 418/2048 [00:06<00:35, 46.43it/s, est. speed input: 63204.23 toks/s, output: 61.72 toks/s]
Processed prompts:  21%|██        | 434/2048 [00:07<00:34, 46.31it/s, est. speed input: 62418.57 toks/s, output: 60.96 toks/s]
Processed prompts:  22%|██▏       | 450/2048 [00:07<00:34, 46.20it/s, est. speed input: 61701.56 toks/s, output: 60.26 toks/s]
Processed prompts:  23%|██▎       | 466/2048 [00:07<00:34, 46.12it/s, est. speed input: 61047.62 toks/s, output: 59.62 toks/s]
Processed prompts:  24%|██▎       | 482/2048 [00:08<00:33, 46.07it/s, est. speed input: 60450.61 toks/s, output: 59.03 toks/s]
Processed prompts:  24%|██▍       | 498/2048 [00:08<00:33, 46.01it/s, est. speed input: 59899.60 toks/s, output: 58.50 toks/s]
Processed prompts:  25%|██▌       | 514/2048 [00:08<00:33, 46.00it/s, est. speed input: 59395.80 toks/s, output: 58.00 toks/s]
Processed prompts:  26%|██▌       | 530/2048 [00:09<00:33, 45.98it/s, est. speed input: 58928.16 toks/s, output: 57.55 toks/s]
Processed prompts:  27%|██▋       | 546/2048 [00:09<00:32, 45.96it/s, est. speed input: 58493.28 toks/s, output: 57.12 toks/s]
Processed prompts:  27%|██▋       | 562/2048 [00:09<00:32, 45.96it/s, est. speed input: 58091.83 toks/s, output: 56.73 toks/s]
Processed prompts:  28%|██▊       | 578/2048 [00:10<00:31, 45.95it/s, est. speed input: 57716.13 toks/s, output: 56.36 toks/s]
Processed prompts:  29%|██▉       | 594/2048 [00:10<00:31, 45.94it/s, est. speed input: 57364.09 toks/s, output: 56.02 toks/s]
Processed prompts:  30%|██▉       | 610/2048 [00:10<00:31, 45.93it/s, est. speed input: 57034.67 toks/s, output: 55.70 toks/s]
Processed prompts:  31%|███       | 626/2048 [00:11<00:30, 45.94it/s, est. speed input: 56727.73 toks/s, output: 55.40 toks/s]
Processed prompts:  31%|███▏      | 642/2048 [00:11<00:30, 45.93it/s, est. speed input: 56436.60 toks/s, output: 55.11 toks/s]
Processed prompts:  32%|███▏      | 658/2048 [00:11<00:30, 45.92it/s, est. speed input: 56162.90 toks/s, output: 54.85 toks/s]
Processed prompts:  33%|███▎      | 674/2048 [00:12<00:29, 45.92it/s, est. speed input: 55904.16 toks/s, output: 54.59 toks/s]
Processed prompts:  34%|███▎      | 690/2048 [00:12<00:29, 45.91it/s, est. speed input: 55659.61 toks/s, output: 54.36 toks/s]
Processed prompts:  34%|███▍      | 706/2048 [00:13<00:29, 45.93it/s, est. speed input: 55430.32 toks/s, output: 54.13 toks/s]
Processed prompts:  35%|███▌      | 722/2048 [00:13<00:28, 45.93it/s, est. speed input: 55212.40 toks/s, output: 53.92 toks/s]
Processed prompts:  36%|███▌      | 738/2048 [00:13<00:28, 45.92it/s, est. speed input: 55003.69 toks/s, output: 53.71 toks/s]
Processed prompts:  37%|███▋      | 754/2048 [00:14<00:28, 45.92it/s, est. speed input: 54805.89 toks/s, output: 53.52 toks/s]
Processed prompts:  38%|███▊      | 770/2048 [00:14<00:27, 45.90it/s, est. speed input: 54616.37 toks/s, output: 53.34 toks/s]
Processed prompts:  38%|███▊      | 786/2048 [00:14<00:27, 45.89it/s, est. speed input: 54435.72 toks/s, output: 53.16 toks/s]
Processed prompts:  39%|███▉      | 802/2048 [00:15<00:27, 45.90it/s, est. speed input: 54265.31 toks/s, output: 52.99 toks/s]
Processed prompts:  40%|███▉      | 818/2048 [00:15<00:26, 45.89it/s, est. speed input: 54100.98 toks/s, output: 52.83 toks/s]
Processed prompts:  41%|████      | 834/2048 [00:15<00:26, 45.86it/s, est. speed input: 53941.97 toks/s, output: 52.68 toks/s]
Processed prompts:  42%|████▏     | 850/2048 [00:16<00:26, 45.88it/s, est. speed input: 53793.06 toks/s, output: 52.53 toks/s]
Processed prompts:  42%|████▏     | 866/2048 [00:16<00:25, 45.86it/s, est. speed input: 53647.51 toks/s, output: 52.39 toks/s]
Processed prompts:  43%|████▎     | 882/2048 [00:16<00:25, 45.88it/s, est. speed input: 53511.06 toks/s, output: 52.26 toks/s]
Processed prompts:  44%|████▍     | 898/2048 [00:17<00:25, 45.88it/s, est. speed input: 53378.83 toks/s, output: 52.13 toks/s]
Processed prompts:  45%|████▍     | 914/2048 [00:17<00:24, 45.87it/s, est. speed input: 53250.77 toks/s, output: 52.00 toks/s]
Processed prompts:  45%|████▌     | 930/2048 [00:17<00:24, 45.87it/s, est. speed input: 53128.66 toks/s, output: 51.88 toks/s]
Processed prompts:  46%|████▌     | 946/2048 [00:18<00:24, 45.87it/s, est. speed input: 53011.12 toks/s, output: 51.77 toks/s]
Processed prompts:  47%|████▋     | 962/2048 [00:18<00:23, 45.85it/s, est. speed input: 52896.32 toks/s, output: 51.66 toks/s]
Processed prompts:  48%|████▊     | 978/2048 [00:18<00:23, 45.87it/s, est. speed input: 52788.21 toks/s, output: 51.55 toks/s]
Processed prompts:  49%|████▊     | 994/2048 [00:19<00:22, 45.88it/s, est. speed input: 52683.95 toks/s, output: 51.45 toks/s]
Processed prompts:  49%|████▉     | 1010/2048 [00:19<00:22, 45.88it/s, est. speed input: 52582.71 toks/s, output: 51.35 toks/s]
Processed prompts:  50%|█████     | 1026/2048 [00:20<00:22, 45.86it/s, est. speed input: 52484.25 toks/s, output: 51.25 toks/s]
Processed prompts:  51%|█████     | 1042/2048 [00:20<00:21, 45.86it/s, est. speed input: 52389.33 toks/s, output: 51.16 toks/s]
Processed prompts:  52%|█████▏    | 1058/2048 [00:20<00:21, 45.84it/s, est. speed input: 52296.87 toks/s, output: 51.07 toks/s]
Processed prompts:  52%|█████▏    | 1074/2048 [00:21<00:21, 45.86it/s, est. speed input: 52209.06 toks/s, output: 50.99 toks/s]
Processed prompts:  53%|█████▎    | 1090/2048 [00:21<00:20, 45.85it/s, est. speed input: 52122.89 toks/s, output: 50.90 toks/s]
Processed prompts:  54%|█████▍    | 1106/2048 [00:21<00:20, 45.84it/s, est. speed input: 52039.53 toks/s, output: 50.82 toks/s]
Processed prompts:  55%|█████▍    | 1122/2048 [00:22<00:20, 45.86it/s, est. speed input: 51960.35 toks/s, output: 50.74 toks/s]
Processed prompts:  56%|█████▌    | 1138/2048 [00:22<00:19, 45.86it/s, est. speed input: 51882.43 toks/s, output: 50.67 toks/s]
Processed prompts:  56%|█████▋    | 1154/2048 [00:22<00:19, 45.84it/s, est. speed input: 51806.00 toks/s, output: 50.59 toks/s]
Processed prompts:  57%|█████▋    | 1170/2048 [00:23<00:19, 45.85it/s, est. speed input: 51733.25 toks/s, output: 50.52 toks/s]
Processed prompts:  58%|█████▊    | 1186/2048 [00:23<00:18, 45.85it/s, est. speed input: 51662.13 toks/s, output: 50.45 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [00:23<00:18, 45.86it/s, est. speed input: 51594.02 toks/s, output: 50.38 toks/s]
Processed prompts:  59%|█████▉    | 1218/2048 [00:24<00:18, 45.86it/s, est. speed input: 51526.93 toks/s, output: 50.32 toks/s]
Processed prompts:  60%|██████    | 1234/2048 [00:24<00:17, 45.85it/s, est. speed input: 51461.50 toks/s, output: 50.26 toks/s]
Processed prompts:  61%|██████    | 1250/2048 [00:24<00:17, 45.84it/s, est. speed input: 51398.08 toks/s, output: 50.19 toks/s]
Processed prompts:  62%|██████▏   | 1266/2048 [00:25<00:17, 45.85it/s, est. speed input: 51337.01 toks/s, output: 50.13 toks/s]
Processed prompts:  63%|██████▎   | 1282/2048 [00:25<00:16, 45.85it/s, est. speed input: 51277.17 toks/s, output: 50.08 toks/s]
Processed prompts:  63%|██████▎   | 1298/2048 [00:25<00:16, 45.86it/s, est. speed input: 51219.42 toks/s, output: 50.02 toks/s]
Processed prompts:  64%|██████▍   | 1314/2048 [00:26<00:16, 45.85it/s, est. speed input: 51162.20 toks/s, output: 49.96 toks/s]
Processed prompts:  65%|██████▍   | 1330/2048 [00:26<00:15, 45.84it/s, est. speed input: 51106.69 toks/s, output: 49.91 toks/s]
Processed prompts:  66%|██████▌   | 1346/2048 [00:26<00:15, 45.84it/s, est. speed input: 51052.69 toks/s, output: 49.86 toks/s]
Processed prompts:  67%|██████▋   | 1362/2048 [00:27<00:14, 45.84it/s, est. speed input: 51000.10 toks/s, output: 49.80 toks/s]
Processed prompts:  67%|██████▋   | 1378/2048 [00:27<00:14, 45.83it/s, est. speed input: 50948.52 toks/s, output: 49.75 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [00:28<00:14, 45.84it/s, est. speed input: 50898.84 toks/s, output: 49.71 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [00:28<00:13, 45.84it/s, est. speed input: 50850.14 toks/s, output: 49.66 toks/s]
Processed prompts:  70%|██████▉   | 1426/2048 [00:28<00:13, 45.83it/s, est. speed input: 50802.35 toks/s, output: 49.61 toks/s]
Processed prompts:  70%|███████   | 1442/2048 [00:29<00:13, 45.83it/s, est. speed input: 50755.76 toks/s, output: 49.57 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [00:29<00:12, 45.83it/s, est. speed input: 50710.54 toks/s, output: 49.52 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [00:29<00:12, 45.83it/s, est. speed input: 50666.26 toks/s, output: 49.48 toks/s]
Processed prompts:  73%|███████▎  | 1490/2048 [00:30<00:12, 45.82it/s, est. speed input: 50622.68 toks/s, output: 49.44 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [00:30<00:11, 45.82it/s, est. speed input: 50580.21 toks/s, output: 49.39 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [00:30<00:11, 45.81it/s, est. speed input: 50538.40 toks/s, output: 49.35 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [00:31<00:11, 45.83it/s, est. speed input: 50498.48 toks/s, output: 49.31 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [00:31<00:10, 45.83it/s, est. speed input: 50458.89 toks/s, output: 49.28 toks/s]
Processed prompts:  77%|███████▋  | 1570/2048 [00:31<00:10, 45.83it/s, est. speed input: 50420.44 toks/s, output: 49.24 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [00:32<00:10, 45.84it/s, est. speed input: 50383.11 toks/s, output: 49.20 toks/s]
Processed prompts:  78%|███████▊  | 1602/2048 [00:32<00:09, 45.83it/s, est. speed input: 50345.63 toks/s, output: 49.17 toks/s]
Processed prompts:  79%|███████▉  | 1618/2048 [00:32<00:09, 45.83it/s, est. speed input: 50309.33 toks/s, output: 49.13 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [00:33<00:09, 45.81it/s, est. speed input: 50273.26 toks/s, output: 49.09 toks/s]
Processed prompts:  81%|████████  | 1650/2048 [00:33<00:08, 45.80it/s, est. speed input: 50237.87 toks/s, output: 49.06 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [00:33<00:08, 45.80it/s, est. speed input: 50203.77 toks/s, output: 49.03 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [00:34<00:07, 45.82it/s, est. speed input: 50170.63 toks/s, output: 48.99 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [00:34<00:07, 45.80it/s, est. speed input: 50137.34 toks/s, output: 48.96 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [00:35<00:07, 45.80it/s, est. speed input: 50105.09 toks/s, output: 48.93 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [00:35<00:06, 45.81it/s, est. speed input: 50073.70 toks/s, output: 48.90 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [00:35<00:06, 45.80it/s, est. speed input: 50042.31 toks/s, output: 48.87 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [00:36<00:06, 45.82it/s, est. speed input: 50012.50 toks/s, output: 48.84 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [00:36<00:05, 45.82it/s, est. speed input: 49982.87 toks/s, output: 48.81 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [00:36<00:05, 45.80it/s, est. speed input: 49953.07 toks/s, output: 48.78 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [00:37<00:05, 45.80it/s, est. speed input: 49924.46 toks/s, output: 48.75 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [00:37<00:04, 45.81it/s, est. speed input: 49896.42 toks/s, output: 48.73 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [00:37<00:04, 45.81it/s, est. speed input: 49868.84 toks/s, output: 48.70 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [00:38<00:04, 45.82it/s, est. speed input: 49842.12 toks/s, output: 48.67 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [00:38<00:03, 46.54it/s, est. speed input: 49839.04 toks/s, output: 48.67 toks/s]
Processed prompts:  92%|█████████▏| 1890/2048 [00:38<00:03, 46.32it/s, est. speed input: 49812.85 toks/s, output: 48.65 toks/s]
Processed prompts:  93%|█████████▎| 1906/2048 [00:39<00:03, 46.15it/s, est. speed input: 49786.48 toks/s, output: 48.62 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [00:39<00:02, 46.03it/s, est. speed input: 49760.44 toks/s, output: 48.59 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [00:39<00:02, 45.98it/s, est. speed input: 49735.93 toks/s, output: 48.57 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [00:40<00:02, 45.93it/s, est. speed input: 49711.44 toks/s, output: 48.55 toks/s]
Processed prompts:  96%|█████████▌| 1970/2048 [00:40<00:01, 45.89it/s, est. speed input: 49687.28 toks/s, output: 48.52 toks/s]
Processed prompts:  97%|█████████▋| 1986/2048 [00:40<00:01, 45.86it/s, est. speed input: 49663.36 toks/s, output: 48.50 toks/s]
Processed prompts:  98%|█████████▊| 2002/2048 [00:41<00:01, 45.84it/s, est. speed input: 49640.01 toks/s, output: 48.48 toks/s]
Processed prompts:  99%|█████████▊| 2018/2048 [00:41<00:00, 45.82it/s, est. speed input: 49616.78 toks/s, output: 48.45 toks/s]
Processed prompts:  99%|█████████▉| 2034/2048 [00:41<00:00, 46.51it/s, est. speed input: 49614.73 toks/s, output: 48.45 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:41<00:00, 46.51it/s, est. speed input: 49956.02 toks/s, output: 48.79 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:41<00:00, 48.79it/s, est. speed input: 49956.02 toks/s, output: 48.79 toks/s]
[rank0]:[W125 19:29:51.196025909 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 71.9s

测试结果:
  Requests/s:   45.83
  Tokens/s:     46975.61
  Total Reqs:   2048
  Elapsed:      44.69s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     46929.78

============================================================
[7/8] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-3B-INT8                                │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 19:30:07 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=376023) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=376023) WARNING 01-25 19:30:16 [backends.py:609] Failed to read file <frozen os>
Throughput: 45.73 requests/s, 46874.86 total tokens/s, 45.73 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-25 19:30:07] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:30:07] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:30:07] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:30:07] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:30:07] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:30:07] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:30:07] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:30:07] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:30:07] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:30:07] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:30:07] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:30:07] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:30:07] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:30:07] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 19:30:11] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:30:11] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:30:11] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:30:11] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:30:11] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:30:11] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:30:11] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:30:11] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:30:11] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:30:11] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:30:11] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:30:11] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:30:11] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:30:11] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=376023) [2026-01-25 19:30:12] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=376023) [2026-01-25 19:30:12] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=376023) [2026-01-25 19:30:12] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=376023) [2026-01-25 19:30:12] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=376023) [2026-01-25 19:30:12] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=376023) [2026-01-25 19:30:12] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=376023) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=376023) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.82it/s]
(EngineCore_DP0 pid=376023) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.82it/s]
(EngineCore_DP0 pid=376023) 
(EngineCore_DP0 pid=376023) [2026-01-25 19:30:12] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 3072] -> 1D uint8
(EngineCore_DP0 pid=376023) [2026-01-25 19:30:12] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11796480 bytes
(EngineCore_DP0 pid=376023) [2026-01-25 19:30:12] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=376023) [2026-01-25 19:30:12] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7077888 bytes
(EngineCore_DP0 pid=376023) [2026-01-25 19:30:12] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=376023) [2026-01-25 19:30:12] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 37748736 bytes
(EngineCore_DP0 pid=376023) [2026-01-25 19:30:12] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 8192] -> 1D uint8
(EngineCore_DP0 pid=376023) [2026-01-25 19:30:12] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 18874368 bytes
(EngineCore_DP0 pid=376023) [rank0]:W0125 19:30:19.671000 376023 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=376023) [rank0]:W0125 19:30:19.722000 376023 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=376023) [rank0]:W0125 19:30:20.353000 376023 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=376023) [rank0]:W0125 19:30:20.426000 376023 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=376023) 2026-01-25 19:30:22,636 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=376023) 2026-01-25 19:30:22,652 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=376023) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▉         | 1/11 [00:00<00:04,  2.33it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 3/11 [00:00<00:01,  6.74it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 6/11 [00:00<00:00, 12.85it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 10/11 [00:00<00:00, 18.77it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 13.62it/s]
(EngineCore_DP0 pid=376023) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 1/7 [00:00<00:00,  7.47it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 5/7 [00:00<00:00, 21.34it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 21.79it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 71/4096 [00:00<00:05, 705.48it/s]
Adding requests:   4%|▎         | 146/4096 [00:00<00:05, 728.77it/s]
Adding requests:   5%|▌         | 220/4096 [00:00<00:05, 733.33it/s]
Adding requests:   7%|▋         | 298/4096 [00:00<00:05, 748.95it/s]
Adding requests:   9%|▉         | 374/4096 [00:00<00:04, 750.30it/s]
Adding requests:  11%|█         | 450/4096 [00:00<00:04, 752.39it/s]
Adding requests:  13%|█▎        | 526/4096 [00:00<00:04, 742.40it/s]
Adding requests:  15%|█▍        | 603/4096 [00:00<00:04, 748.90it/s]
Adding requests:  17%|█▋        | 683/4096 [00:00<00:04, 763.37it/s]
Adding requests:  19%|█▊        | 760/4096 [00:01<00:04, 763.85it/s]
Adding requests:  20%|██        | 837/4096 [00:01<00:04, 744.70it/s]
Adding requests:  22%|██▏       | 917/4096 [00:01<00:04, 760.34it/s]
Adding requests:  24%|██▍       | 995/4096 [00:01<00:04, 766.17it/s]
Adding requests:  26%|██▌       | 1072/4096 [00:01<00:04, 750.55it/s]
Adding requests:  28%|██▊       | 1148/4096 [00:01<00:03, 751.23it/s]
Adding requests:  30%|██▉       | 1225/4096 [00:01<00:03, 756.59it/s]
Adding requests:  32%|███▏      | 1301/4096 [00:01<00:03, 750.70it/s]
Adding requests:  34%|███▎      | 1379/4096 [00:01<00:03, 758.07it/s]
Adding requests:  36%|███▌      | 1458/4096 [00:01<00:03, 766.68it/s]
Adding requests:  38%|███▊      | 1538/4096 [00:02<00:03, 774.83it/s]
Adding requests:  40%|███▉      | 1618/4096 [00:02<00:03, 780.51it/s]
Adding requests:  41%|████▏     | 1697/4096 [00:02<00:03, 765.11it/s]
Adding requests:  43%|████▎     | 1774/4096 [00:02<00:03, 765.12it/s]
Adding requests:  45%|████▌     | 1851/4096 [00:02<00:02, 765.51it/s]
Adding requests:  47%|████▋     | 1928/4096 [00:02<00:02, 764.35it/s]
Adding requests:  49%|████▉     | 2006/4096 [00:02<00:02, 766.76it/s]
Adding requests:  51%|█████     | 2083/4096 [00:02<00:02, 765.37it/s]
Adding requests:  53%|█████▎    | 2160/4096 [00:02<00:02, 750.43it/s]
Adding requests:  55%|█████▍    | 2236/4096 [00:02<00:02, 738.45it/s]
Adding requests:  56%|█████▋    | 2314/4096 [00:03<00:02, 748.69it/s]
Adding requests:  58%|█████▊    | 2389/4096 [00:03<00:02, 747.26it/s]
Adding requests:  60%|██████    | 2466/4096 [00:03<00:02, 753.13it/s]
Adding requests:  62%|██████▏   | 2543/4096 [00:03<00:02, 756.35it/s]
Adding requests:  64%|██████▍   | 2622/4096 [00:03<00:01, 764.14it/s]
Adding requests:  66%|██████▌   | 2700/4096 [00:03<00:01, 767.97it/s]
Adding requests:  68%|██████▊   | 2777/4096 [00:03<00:01, 760.50it/s]
Adding requests:  70%|██████▉   | 2854/4096 [00:03<00:01, 757.80it/s]
Adding requests:  72%|███████▏  | 2931/4096 [00:03<00:01, 758.44it/s]
Adding requests:  73%|███████▎  | 3007/4096 [00:03<00:01, 757.84it/s]
Adding requests:  75%|███████▌  | 3084/4096 [00:04<00:01, 759.12it/s]
Adding requests:  77%|███████▋  | 3163/4096 [00:04<00:01, 766.24it/s]
Adding requests:  79%|███████▉  | 3241/4096 [00:04<00:01, 769.97it/s]
Adding requests:  81%|████████  | 3319/4096 [00:04<00:01, 772.84it/s]
Adding requests:  83%|████████▎ | 3397/4096 [00:04<00:00, 774.16it/s]
Adding requests:  85%|████████▍ | 3475/4096 [00:04<00:00, 765.95it/s]
Adding requests:  87%|████████▋ | 3555/4096 [00:04<00:00, 775.03it/s]
Adding requests:  89%|████████▊ | 3633/4096 [00:04<00:00, 754.00it/s]
Adding requests:  91%|█████████ | 3711/4096 [00:04<00:00, 760.55it/s]
Adding requests:  93%|█████████▎| 3789/4096 [00:04<00:00, 765.67it/s]
Adding requests:  94%|█████████▍| 3868/4096 [00:05<00:00, 772.66it/s]
Adding requests:  96%|█████████▋| 3946/4096 [00:05<00:00, 764.46it/s]
Adding requests:  98%|█████████▊| 4024/4096 [00:05<00:00, 765.69it/s]
Adding requests: 100%|██████████| 4096/4096 [00:05<00:00, 759.74it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   6%|▌         | 226/4096 [00:00<00:04, 834.05it/s, est. speed input: 854115.93 toks/s, output: 834.07 toks/s]
Processed prompts:   8%|▊         | 310/4096 [00:01<00:24, 152.85it/s, est. speed input: 190556.26 toks/s, output: 186.09 toks/s]
Processed prompts:   8%|▊         | 348/4096 [00:02<00:32, 114.92it/s, est. speed input: 150832.06 toks/s, output: 147.30 toks/s]
Processed prompts:   9%|▉         | 372/4096 [00:03<00:43, 86.25it/s, est. speed input: 124446.06 toks/s, output: 121.53 toks/s] 
Processed prompts:   9%|▉         | 388/4096 [00:03<00:57, 64.96it/s, est. speed input: 105737.00 toks/s, output: 103.26 toks/s]
Processed prompts:  10%|█         | 418/4096 [00:04<01:03, 57.83it/s, est. speed input: 96074.17 toks/s, output: 93.82 toks/s]  
Processed prompts:  11%|█         | 450/4096 [00:05<01:07, 54.02it/s, est. speed input: 89415.56 toks/s, output: 87.32 toks/s]
Processed prompts:  12%|█▏        | 482/4096 [00:05<01:10, 51.47it/s, est. speed input: 84351.71 toks/s, output: 82.37 toks/s]
Processed prompts:  13%|█▎        | 514/4096 [00:06<01:12, 49.74it/s, est. speed input: 80366.27 toks/s, output: 78.48 toks/s]
Processed prompts:  13%|█▎        | 546/4096 [00:07<01:13, 48.55it/s, est. speed input: 77148.14 toks/s, output: 75.34 toks/s]
Processed prompts:  14%|█▍        | 578/4096 [00:07<01:13, 47.73it/s, est. speed input: 74493.29 toks/s, output: 72.75 toks/s]
Processed prompts:  15%|█▍        | 610/4096 [00:08<01:13, 47.16it/s, est. speed input: 72268.39 toks/s, output: 70.57 toks/s]
Processed prompts:  16%|█▌        | 642/4096 [00:09<01:13, 46.77it/s, est. speed input: 70379.45 toks/s, output: 68.73 toks/s]
Processed prompts:  16%|█▋        | 674/4096 [00:10<01:13, 46.47it/s, est. speed input: 68743.68 toks/s, output: 67.13 toks/s]
Processed prompts:  17%|█▋        | 706/4096 [00:10<01:13, 46.28it/s, est. speed input: 67324.48 toks/s, output: 65.75 toks/s]
Processed prompts:  18%|█▊        | 738/4096 [00:11<01:12, 46.14it/s, est. speed input: 66077.87 toks/s, output: 64.53 toks/s]
Processed prompts:  19%|█▉        | 770/4096 [00:12<01:12, 46.04it/s, est. speed input: 64973.78 toks/s, output: 63.45 toks/s]
Processed prompts:  20%|█▉        | 802/4096 [00:12<01:11, 45.97it/s, est. speed input: 63989.84 toks/s, output: 62.49 toks/s]
Processed prompts:  20%|██        | 834/4096 [00:13<01:11, 45.92it/s, est. speed input: 63109.08 toks/s, output: 61.63 toks/s]
Processed prompts:  21%|██        | 866/4096 [00:14<01:10, 45.89it/s, est. speed input: 62314.55 toks/s, output: 60.85 toks/s]
Processed prompts:  22%|██▏       | 898/4096 [00:14<01:09, 45.85it/s, est. speed input: 61590.21 toks/s, output: 60.15 toks/s]
Processed prompts:  23%|██▎       | 930/4096 [00:15<01:09, 45.83it/s, est. speed input: 60932.21 toks/s, output: 59.50 toks/s]
Processed prompts:  23%|██▎       | 962/4096 [00:16<01:08, 45.83it/s, est. speed input: 60333.21 toks/s, output: 58.92 toks/s]
Processed prompts:  24%|██▍       | 994/4096 [00:17<01:07, 45.81it/s, est. speed input: 59780.88 toks/s, output: 58.38 toks/s]
Processed prompts:  25%|██▌       | 1026/4096 [00:17<01:07, 45.80it/s, est. speed input: 59271.41 toks/s, output: 57.88 toks/s]
Processed prompts:  26%|██▌       | 1058/4096 [00:18<01:06, 45.79it/s, est. speed input: 58800.80 toks/s, output: 57.42 toks/s]
Processed prompts:  27%|██▋       | 1090/4096 [00:19<01:05, 45.78it/s, est. speed input: 58364.28 toks/s, output: 57.00 toks/s]
Processed prompts:  27%|██▋       | 1122/4096 [00:19<01:04, 45.78it/s, est. speed input: 57959.58 toks/s, output: 56.60 toks/s]
Processed prompts:  28%|██▊       | 1154/4096 [00:20<01:04, 45.79it/s, est. speed input: 57583.29 toks/s, output: 56.23 toks/s]
Processed prompts:  29%|██▉       | 1186/4096 [00:21<01:03, 45.77it/s, est. speed input: 57228.62 toks/s, output: 55.89 toks/s]
Processed prompts:  30%|██▉       | 1218/4096 [00:21<01:02, 45.77it/s, est. speed input: 56897.91 toks/s, output: 55.56 toks/s]
Processed prompts:  31%|███       | 1250/4096 [00:22<01:02, 45.77it/s, est. speed input: 56588.27 toks/s, output: 55.26 toks/s]
Processed prompts:  31%|███▏      | 1282/4096 [00:23<01:01, 45.74it/s, est. speed input: 56292.68 toks/s, output: 54.97 toks/s]
Processed prompts:  32%|███▏      | 1314/4096 [00:24<01:00, 45.74it/s, est. speed input: 56017.97 toks/s, output: 54.71 toks/s]
Processed prompts:  33%|███▎      | 1346/4096 [00:24<01:00, 45.74it/s, est. speed input: 55757.92 toks/s, output: 54.45 toks/s]
Processed prompts:  34%|███▎      | 1378/4096 [00:25<00:59, 45.77it/s, est. speed input: 55515.31 toks/s, output: 54.21 toks/s]
Processed prompts:  34%|███▍      | 1410/4096 [00:26<00:58, 45.75it/s, est. speed input: 55281.97 toks/s, output: 53.99 toks/s]
Processed prompts:  35%|███▌      | 1442/4096 [00:26<00:58, 45.76it/s, est. speed input: 55062.84 toks/s, output: 53.77 toks/s]
Processed prompts:  36%|███▌      | 1474/4096 [00:27<00:57, 45.75it/s, est. speed input: 54853.79 toks/s, output: 53.57 toks/s]
Processed prompts:  37%|███▋      | 1506/4096 [00:28<00:56, 45.75it/s, est. speed input: 54655.30 toks/s, output: 53.37 toks/s]
Processed prompts:  38%|███▊      | 1538/4096 [00:28<00:55, 45.75it/s, est. speed input: 54466.70 toks/s, output: 53.19 toks/s]
Processed prompts:  38%|███▊      | 1570/4096 [00:29<00:55, 45.76it/s, est. speed input: 54286.97 toks/s, output: 53.01 toks/s]
Processed prompts:  39%|███▉      | 1602/4096 [00:30<00:54, 45.75it/s, est. speed input: 54115.30 toks/s, output: 52.85 toks/s]
Processed prompts:  40%|███▉      | 1634/4096 [00:31<00:53, 45.75it/s, est. speed input: 53951.04 toks/s, output: 52.69 toks/s]
Processed prompts:  41%|████      | 1666/4096 [00:31<00:53, 45.75it/s, est. speed input: 53794.86 toks/s, output: 52.53 toks/s]
Processed prompts:  41%|████▏     | 1698/4096 [00:32<00:52, 45.75it/s, est. speed input: 53644.31 toks/s, output: 52.39 toks/s]
Processed prompts:  42%|████▏     | 1730/4096 [00:33<00:51, 45.75it/s, est. speed input: 53500.56 toks/s, output: 52.25 toks/s]
Processed prompts:  43%|████▎     | 1762/4096 [00:33<00:51, 45.74it/s, est. speed input: 53362.12 toks/s, output: 52.11 toks/s]
Processed prompts:  44%|████▍     | 1794/4096 [00:34<00:50, 45.74it/s, est. speed input: 53230.36 toks/s, output: 51.98 toks/s]
Processed prompts:  45%|████▍     | 1826/4096 [00:35<00:49, 45.74it/s, est. speed input: 53102.93 toks/s, output: 51.86 toks/s]
Processed prompts:  45%|████▌     | 1858/4096 [00:35<00:48, 46.11it/s, est. speed input: 53008.47 toks/s, output: 51.77 toks/s]
Processed prompts:  46%|████▌     | 1890/4096 [00:36<00:47, 45.99it/s, est. speed input: 52890.03 toks/s, output: 51.65 toks/s]
Processed prompts:  47%|████▋     | 1922/4096 [00:37<00:47, 45.91it/s, est. speed input: 52776.10 toks/s, output: 51.54 toks/s]
Processed prompts:  48%|████▊     | 1954/4096 [00:37<00:46, 45.86it/s, est. speed input: 52666.75 toks/s, output: 51.43 toks/s]
Processed prompts:  48%|████▊     | 1986/4096 [00:38<00:46, 45.82it/s, est. speed input: 52561.02 toks/s, output: 51.33 toks/s]
Processed prompts:  49%|████▉     | 2018/4096 [00:39<00:45, 45.80it/s, est. speed input: 52459.45 toks/s, output: 51.23 toks/s]
Processed prompts:  50%|█████     | 2050/4096 [00:40<00:44, 45.77it/s, est. speed input: 52360.91 toks/s, output: 51.13 toks/s]
Processed prompts:  51%|█████     | 2082/4096 [00:40<00:44, 45.75it/s, est. speed input: 52265.65 toks/s, output: 51.04 toks/s]
Processed prompts:  52%|█████▏    | 2114/4096 [00:41<00:43, 45.75it/s, est. speed input: 52174.12 toks/s, output: 50.95 toks/s]
Processed prompts:  52%|█████▏    | 2146/4096 [00:42<00:42, 45.74it/s, est. speed input: 52084.90 toks/s, output: 50.86 toks/s]
Processed prompts:  53%|█████▎    | 2178/4096 [00:42<00:41, 45.72it/s, est. speed input: 51998.45 toks/s, output: 50.78 toks/s]
Processed prompts:  54%|█████▍    | 2210/4096 [00:43<00:41, 45.72it/s, est. speed input: 51915.08 toks/s, output: 50.70 toks/s]
Processed prompts:  55%|█████▍    | 2242/4096 [00:44<00:40, 45.72it/s, est. speed input: 51834.45 toks/s, output: 50.62 toks/s]
Processed prompts:  56%|█████▌    | 2274/4096 [00:44<00:39, 45.72it/s, est. speed input: 51756.37 toks/s, output: 50.54 toks/s]
Processed prompts:  56%|█████▋    | 2306/4096 [00:45<00:39, 45.71it/s, est. speed input: 51680.19 toks/s, output: 50.47 toks/s]
Processed prompts:  57%|█████▋    | 2338/4096 [00:46<00:38, 45.71it/s, est. speed input: 51606.80 toks/s, output: 50.40 toks/s]
Processed prompts:  58%|█████▊    | 2370/4096 [00:47<00:37, 45.71it/s, est. speed input: 51535.49 toks/s, output: 50.33 toks/s]
Processed prompts:  59%|█████▊    | 2402/4096 [00:47<00:37, 45.71it/s, est. speed input: 51466.21 toks/s, output: 50.26 toks/s]
Processed prompts:  59%|█████▉    | 2434/4096 [00:48<00:36, 45.71it/s, est. speed input: 51398.80 toks/s, output: 50.19 toks/s]
Processed prompts:  60%|██████    | 2466/4096 [00:49<00:35, 45.70it/s, est. speed input: 51333.02 toks/s, output: 50.13 toks/s]
Processed prompts:  61%|██████    | 2498/4096 [00:49<00:34, 45.70it/s, est. speed input: 51269.29 toks/s, output: 50.07 toks/s]
Processed prompts:  62%|██████▏   | 2530/4096 [00:50<00:34, 45.70it/s, est. speed input: 51207.44 toks/s, output: 50.01 toks/s]
Processed prompts:  63%|██████▎   | 2562/4096 [00:51<00:33, 45.70it/s, est. speed input: 51147.37 toks/s, output: 49.95 toks/s]
Processed prompts:  63%|██████▎   | 2594/4096 [00:51<00:32, 45.70it/s, est. speed input: 51088.75 toks/s, output: 49.89 toks/s]
Processed prompts:  64%|██████▍   | 2626/4096 [00:52<00:32, 45.70it/s, est. speed input: 51031.52 toks/s, output: 49.84 toks/s]
Processed prompts:  65%|██████▍   | 2658/4096 [00:53<00:31, 45.70it/s, est. speed input: 50976.16 toks/s, output: 49.78 toks/s]
Processed prompts:  66%|██████▌   | 2690/4096 [00:54<00:30, 45.71it/s, est. speed input: 50922.31 toks/s, output: 49.73 toks/s]
Processed prompts:  66%|██████▋   | 2722/4096 [00:54<00:30, 45.70it/s, est. speed input: 50869.44 toks/s, output: 49.68 toks/s]
Processed prompts:  67%|██████▋   | 2754/4096 [00:55<00:29, 45.71it/s, est. speed input: 50818.43 toks/s, output: 49.63 toks/s]
Processed prompts:  68%|██████▊   | 2786/4096 [00:56<00:28, 45.70it/s, est. speed input: 50768.20 toks/s, output: 49.58 toks/s]
Processed prompts:  69%|██████▉   | 2818/4096 [00:56<00:27, 45.70it/s, est. speed input: 50719.11 toks/s, output: 49.53 toks/s]
Processed prompts:  70%|██████▉   | 2850/4096 [00:57<00:27, 45.70it/s, est. speed input: 50671.58 toks/s, output: 49.48 toks/s]
Processed prompts:  70%|███████   | 2882/4096 [00:58<00:26, 45.70it/s, est. speed input: 50624.90 toks/s, output: 49.44 toks/s]
Processed prompts:  71%|███████   | 2914/4096 [00:58<00:25, 45.69it/s, est. speed input: 50579.14 toks/s, output: 49.39 toks/s]
Processed prompts:  72%|███████▏  | 2946/4096 [00:59<00:25, 45.69it/s, est. speed input: 50534.50 toks/s, output: 49.35 toks/s]
Processed prompts:  73%|███████▎  | 2978/4096 [01:00<00:24, 45.70it/s, est. speed input: 50491.33 toks/s, output: 49.31 toks/s]
Processed prompts:  73%|███████▎  | 3010/4096 [01:01<00:23, 45.69it/s, est. speed input: 50448.78 toks/s, output: 49.27 toks/s]
Processed prompts:  74%|███████▍  | 3042/4096 [01:01<00:23, 45.69it/s, est. speed input: 50407.41 toks/s, output: 49.23 toks/s]
Processed prompts:  75%|███████▌  | 3074/4096 [01:02<00:22, 45.69it/s, est. speed input: 50366.83 toks/s, output: 49.19 toks/s]
Processed prompts:  76%|███████▌  | 3106/4096 [01:03<00:21, 45.69it/s, est. speed input: 50327.11 toks/s, output: 49.15 toks/s]
Processed prompts:  77%|███████▋  | 3138/4096 [01:03<00:20, 45.68it/s, est. speed input: 50288.03 toks/s, output: 49.11 toks/s]
Processed prompts:  77%|███████▋  | 3170/4096 [01:04<00:20, 45.69it/s, est. speed input: 50250.39 toks/s, output: 49.07 toks/s]
Processed prompts:  78%|███████▊  | 3202/4096 [01:05<00:19, 45.69it/s, est. speed input: 50213.00 toks/s, output: 49.04 toks/s]
Processed prompts:  79%|███████▉  | 3234/4096 [01:05<00:18, 45.69it/s, est. speed input: 50176.68 toks/s, output: 49.00 toks/s]
Processed prompts:  80%|███████▉  | 3266/4096 [01:06<00:18, 45.68it/s, est. speed input: 50140.84 toks/s, output: 48.97 toks/s]
Processed prompts:  81%|████████  | 3298/4096 [01:07<00:17, 45.69it/s, est. speed input: 50106.09 toks/s, output: 48.93 toks/s]
Processed prompts:  81%|████████▏ | 3330/4096 [01:08<00:16, 45.69it/s, est. speed input: 50071.83 toks/s, output: 48.90 toks/s]
Processed prompts:  82%|████████▏ | 3362/4096 [01:08<00:16, 45.69it/s, est. speed input: 50038.36 toks/s, output: 48.87 toks/s]
Processed prompts:  83%|████████▎ | 3394/4096 [01:09<00:15, 45.68it/s, est. speed input: 50005.44 toks/s, output: 48.83 toks/s]
Processed prompts:  84%|████████▎ | 3426/4096 [01:10<00:14, 45.68it/s, est. speed input: 49973.06 toks/s, output: 48.80 toks/s]
Processed prompts:  84%|████████▍ | 3458/4096 [01:10<00:13, 45.67it/s, est. speed input: 49941.35 toks/s, output: 48.77 toks/s]
Processed prompts:  85%|████████▌ | 3490/4096 [01:11<00:13, 45.67it/s, est. speed input: 49910.23 toks/s, output: 48.74 toks/s]
Processed prompts:  86%|████████▌ | 3522/4096 [01:12<00:12, 45.67it/s, est. speed input: 49879.81 toks/s, output: 48.71 toks/s]
Processed prompts:  87%|████████▋ | 3554/4096 [01:13<00:11, 45.67it/s, est. speed input: 49850.00 toks/s, output: 48.68 toks/s]
Processed prompts:  88%|████████▊ | 3586/4096 [01:13<00:11, 45.67it/s, est. speed input: 49820.65 toks/s, output: 48.65 toks/s]
Processed prompts:  88%|████████▊ | 3618/4096 [01:14<00:10, 45.68it/s, est. speed input: 49792.11 toks/s, output: 48.63 toks/s]
Processed prompts:  89%|████████▉ | 3650/4096 [01:15<00:09, 45.68it/s, est. speed input: 49763.99 toks/s, output: 48.60 toks/s]
Processed prompts:  90%|████████▉ | 3682/4096 [01:15<00:09, 45.67it/s, est. speed input: 49736.21 toks/s, output: 48.57 toks/s]
Processed prompts:  91%|█████████ | 3714/4096 [01:16<00:08, 45.68it/s, est. speed input: 49709.28 toks/s, output: 48.54 toks/s]
Processed prompts:  91%|█████████▏| 3746/4096 [01:17<00:07, 45.68it/s, est. speed input: 49682.50 toks/s, output: 48.52 toks/s]
Processed prompts:  92%|█████████▏| 3778/4096 [01:17<00:06, 45.67it/s, est. speed input: 49656.21 toks/s, output: 48.49 toks/s]
Processed prompts:  93%|█████████▎| 3810/4096 [01:18<00:06, 45.66it/s, est. speed input: 49630.22 toks/s, output: 48.47 toks/s]
Processed prompts:  94%|█████████▍| 3842/4096 [01:19<00:05, 45.67it/s, est. speed input: 49605.19 toks/s, output: 48.44 toks/s]
Processed prompts:  95%|█████████▍| 3874/4096 [01:20<00:04, 45.67it/s, est. speed input: 49580.20 toks/s, output: 48.42 toks/s]
Processed prompts:  95%|█████████▌| 3906/4096 [01:20<00:04, 45.67it/s, est. speed input: 49555.77 toks/s, output: 48.39 toks/s]
Processed prompts:  96%|█████████▌| 3938/4096 [01:21<00:03, 45.68it/s, est. speed input: 49532.07 toks/s, output: 48.37 toks/s]
Processed prompts:  97%|█████████▋| 3970/4096 [01:22<00:02, 45.68it/s, est. speed input: 49508.58 toks/s, output: 48.35 toks/s]
Processed prompts:  98%|█████████▊| 4002/4096 [01:22<00:02, 45.67it/s, est. speed input: 49485.26 toks/s, output: 48.33 toks/s]
Processed prompts:  98%|█████████▊| 4034/4096 [01:23<00:01, 46.08it/s, est. speed input: 49474.63 toks/s, output: 48.32 toks/s]
Processed prompts:  99%|█████████▉| 4066/4096 [01:24<00:00, 46.37it/s, est. speed input: 49464.40 toks/s, output: 48.31 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [01:24<00:00, 46.37it/s, est. speed input: 49829.20 toks/s, output: 48.66 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [01:24<00:00, 48.66it/s, est. speed input: 49829.20 toks/s, output: 48.66 toks/s]
[rank0]:[W125 19:31:54.320511836 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 123.2s

测试结果:
  Requests/s:   45.73
  Tokens/s:     46874.86
  Total Reqs:   4096
  Elapsed:      89.57s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     46829.13

============================================================
[8/8] 测试 M=65536
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-3B-INT8                                │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 65536
│   M_prefill     = 65536 (= 64 x 1024)
│   M_decode      = 64
│   batched_tokens = 65536 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 8192
│   --max-num-seqs           = 64
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 65536
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 19:32:21 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=378157) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=378157) WARNING 01-25 19:32:30 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 256, in _initialize_kv_caches
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]     self.model_executor.initialize_from_config(kv_cache_configs)
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 116, in initialize_from_config
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]     self.collective_rpc("compile_or_warm_up_model")
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 455, in compile_or_warm_up_model
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]     kernel_warmup(self)
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]   File "/root/vllmbench/vllm/model_executor/warmup/kernel_warmup.py", line 41, in kernel_warmup
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]     flashinfer_autotune(worker.model_runner)
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]   File "/root/vllmbench/vllm/model_executor/warmup/kernel_warmup.py", line 94, in flashinfer_autotune
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]     runner._dummy_run(
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 623, in forward
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]     model_output = self.model(
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]                    ^^^^^^^^^^^
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 439, in __call__
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]     return TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 223, in __call__
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 412, in forward
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]     def forward(
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]     raise e
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]   File "<eval_with_key>.58", line 304, in forward
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]     submod_4 = self.submod_4(getitem_8, s72, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_, getitem_9, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_8 = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_ = getitem_9 = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = None
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]     return range_entry.runnable(*args)
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]     return self._compiled_fn(*args)
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]     return compiled_fn(full_args)
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]                             ^^^^^^^
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]     return self.current_callable(inputs)
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]     out = model(new_inputs)
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]   File "/tmp/torchinductor_root/g2/cg2wfmgsahzalcxqtalqmwiy5csdc7yif7xb7qocn2t577jm2cke.py", line 983, in call
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]     buf14 = torch.ops.slidesparse.dequant_bias.default(reinterpret_tensor(buf13, (s72, 16384), (16384, 1), 0), reinterpret_tensor(buf11, (s72, ), (1, ), 0), arg7_1, None, 'bfloat16', 'Llama3.2-3B-INT8')
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 483, in _dequant_bias_impl
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]     return fn(gemm_output, scale_a, scale_b, bias, out_dtype)
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]   File "/root/vllmbench/slidesparse/csrc/fused_dequant_bias_triton/build/RTX5080_cc120_py312_cu129_x86_64/dequant_bias_tuned_Llama3.2-3B.py", line 112, in dequant_bias_triton
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]     output = torch.empty((M, N), dtype=torch.bfloat16, device=gemm_output.device)
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=378157) ERROR 01-25 19:32:37 [core.py:866] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 15.46 GiB of which 1.91 GiB is free. Including non-PyTorch memory, this process has 13.33 GiB memory in use. Of the allocated memory 10.67 GiB is allocated by PyTorch, and 2.29 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


─── STDERR ───
[2026-01-25 19:32:21] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:32:21] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:32:21] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:32:21] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:32:21] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:32:21] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:32:21] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:32:21] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:32:21] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:32:21] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:32:21] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:32:21] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:32:21] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:32:21] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 19:32:25] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:32:25] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:32:25] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:32:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:32:25] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:32:25] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:32:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:32:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:32:25] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:32:25] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:32:25] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:32:25] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:32:25] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:32:25] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=378157) [2026-01-25 19:32:26] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=378157) [2026-01-25 19:32:26] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=378157) [2026-01-25 19:32:26] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=378157) [2026-01-25 19:32:26] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=378157) [2026-01-25 19:32:26] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=378157) [2026-01-25 19:32:26] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=378157) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=378157) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.76it/s]
(EngineCore_DP0 pid=378157) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.76it/s]
(EngineCore_DP0 pid=378157) 
(EngineCore_DP0 pid=378157) [2026-01-25 19:32:26] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 3072] -> 1D uint8
(EngineCore_DP0 pid=378157) [2026-01-25 19:32:26] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11796480 bytes
(EngineCore_DP0 pid=378157) [2026-01-25 19:32:26] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=378157) [2026-01-25 19:32:26] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7077888 bytes
(EngineCore_DP0 pid=378157) [2026-01-25 19:32:26] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=378157) [2026-01-25 19:32:26] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 37748736 bytes
(EngineCore_DP0 pid=378157) [2026-01-25 19:32:26] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 8192] -> 1D uint8
(EngineCore_DP0 pid=378157) [2026-01-25 19:32:26] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 18874368 bytes
(EngineCore_DP0 pid=378157) [rank0]:W0125 19:32:33.509000 378157 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=378157) [rank0]:W0125 19:32:34.108000 378157 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=378157) 2026-01-25 19:32:36,961 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=378157) 2026-01-25 19:32:37,037 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=378157) Process EngineCore_DP0:
(EngineCore_DP0 pid=378157) Traceback (most recent call last):
(EngineCore_DP0 pid=378157)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=378157)     self.run()
(EngineCore_DP0 pid=378157)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=378157)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=378157)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=378157)     raise e
(EngineCore_DP0 pid=378157)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=378157)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=378157)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=378157)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=378157)     super().__init__(
(EngineCore_DP0 pid=378157)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=378157)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=378157)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=378157)   File "/root/vllmbench/vllm/v1/engine/core.py", line 256, in _initialize_kv_caches
(EngineCore_DP0 pid=378157)     self.model_executor.initialize_from_config(kv_cache_configs)
(EngineCore_DP0 pid=378157)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 116, in initialize_from_config
(EngineCore_DP0 pid=378157)     self.collective_rpc("compile_or_warm_up_model")
(EngineCore_DP0 pid=378157)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=378157)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=378157)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=378157)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=378157)     return func(*args, **kwargs)
(EngineCore_DP0 pid=378157)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=378157)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 455, in compile_or_warm_up_model
(EngineCore_DP0 pid=378157)     kernel_warmup(self)
(EngineCore_DP0 pid=378157)   File "/root/vllmbench/vllm/model_executor/warmup/kernel_warmup.py", line 41, in kernel_warmup
(EngineCore_DP0 pid=378157)     flashinfer_autotune(worker.model_runner)
(EngineCore_DP0 pid=378157)   File "/root/vllmbench/vllm/model_executor/warmup/kernel_warmup.py", line 94, in flashinfer_autotune
(EngineCore_DP0 pid=378157)     runner._dummy_run(
(EngineCore_DP0 pid=378157)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=378157)     return func(*args, **kwargs)
(EngineCore_DP0 pid=378157)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=378157)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=378157)     outputs = self.model(
(EngineCore_DP0 pid=378157)               ^^^^^^^^^^^
(EngineCore_DP0 pid=378157)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=378157)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=378157)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=378157)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=378157)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=378157)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=378157)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=378157)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=378157)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=378157)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 623, in forward
(EngineCore_DP0 pid=378157)     model_output = self.model(
(EngineCore_DP0 pid=378157)                    ^^^^^^^^^^^
(EngineCore_DP0 pid=378157)   File "/root/vllmbench/vllm/compilation/decorators.py", line 439, in __call__
(EngineCore_DP0 pid=378157)     return TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=378157)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=378157)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 223, in __call__
(EngineCore_DP0 pid=378157)     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=378157)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=378157)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=378157)     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=378157)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=378157)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 412, in forward
(EngineCore_DP0 pid=378157)     def forward(
(EngineCore_DP0 pid=378157)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=378157)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=378157)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=378157)   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=378157)     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=378157)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=378157)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=378157)     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=378157)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=378157)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=378157)     raise e
(EngineCore_DP0 pid=378157)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=378157)     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=378157)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=378157)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=378157)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=378157)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=378157)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=378157)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=378157)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=378157)   File "<eval_with_key>.58", line 304, in forward
(EngineCore_DP0 pid=378157)     submod_4 = self.submod_4(getitem_8, s72, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_, getitem_9, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_8 = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_ = getitem_9 = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = None
(EngineCore_DP0 pid=378157)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=378157)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=378157)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=378157)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=378157)   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=378157)     return range_entry.runnable(*args)
(EngineCore_DP0 pid=378157)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=378157)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=378157)     return self._compiled_fn(*args)
(EngineCore_DP0 pid=378157)            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=378157)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=378157)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=378157)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=378157)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=378157)     return compiled_fn(full_args)
(EngineCore_DP0 pid=378157)            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=378157)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=378157)     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=378157)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=378157)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=378157)     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=378157)                             ^^^^^^^
(EngineCore_DP0 pid=378157)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=378157)     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=378157)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=378157)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=378157)     return self.current_callable(inputs)
(EngineCore_DP0 pid=378157)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=378157)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=378157)     out = model(new_inputs)
(EngineCore_DP0 pid=378157)           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=378157)   File "/tmp/torchinductor_root/g2/cg2wfmgsahzalcxqtalqmwiy5csdc7yif7xb7qocn2t577jm2cke.py", line 983, in call
(EngineCore_DP0 pid=378157)     buf14 = torch.ops.slidesparse.dequant_bias.default(reinterpret_tensor(buf13, (s72, 16384), (16384, 1), 0), reinterpret_tensor(buf11, (s72, ), (1, ), 0), arg7_1, None, 'bfloat16', 'Llama3.2-3B-INT8')
(EngineCore_DP0 pid=378157)             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=378157)   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
(EngineCore_DP0 pid=378157)     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=378157)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=378157)   File "/root/vllmbench/slidesparse/core/kernels.py", line 483, in _dequant_bias_impl
(EngineCore_DP0 pid=378157)     return fn(gemm_output, scale_a, scale_b, bias, out_dtype)
(EngineCore_DP0 pid=378157)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=378157)   File "/root/vllmbench/slidesparse/csrc/fused_dequant_bias_triton/build/RTX5080_cc120_py312_cu129_x86_64/dequant_bias_tuned_Llama3.2-3B.py", line 112, in dequant_bias_triton
(EngineCore_DP0 pid=378157)     output = torch.empty((M, N), dtype=torch.bfloat16, device=gemm_output.device)
(EngineCore_DP0 pid=378157)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=378157) torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 15.46 GiB of which 1.91 GiB is free. Including non-PyTorch memory, this process has 13.33 GiB memory in use. Of the allocated memory 10.67 GiB is allocated by PyTorch, and 2.29 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W125 19:32:37.322903644 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[ERROR] 测试失败: M=65536 (exit code: 1)


------------------------------------------------------------
  生成 CSV: Llama3.2-3B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_4/Llama3.2-3B-INT8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,49.5487,25418.4642,2.5833
1024,1024,1,128,128,40.9750,41999.3732,3.1239
2048,1024,2,256,128,49.2904,50522.6647,5.1937
4096,1024,4,512,128,48.1566,49360.5261,10.6320
8192,1024,8,1024,128,46.3970,47556.9675,22.0704
16384,1024,16,2048,128,45.8299,46975.6079,44.6870
32768,1024,32,4096,128,45.7316,46874.8568,89.5661
65536,1024,64,8192,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[INFO] 完成: 7 成功, 1 失败

============================================================
  Llama3.2-3B-INT8 | cuSPARSELt (2_6) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_6
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_6

============================================================
[1/8] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-3B-INT8                                │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 19:32:43 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=378702) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=378702) WARNING 01-25 19:32:52 [backends.py:609] Failed to read file <frozen os>
Throughput: 56.40 requests/s, 28935.64 total tokens/s, 56.40 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-25 19:32:43] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:32:43] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:32:43] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:32:43] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:32:43] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:32:43] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:32:43] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:32:43] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:32:43] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:32:43] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:32:43] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:32:43] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:32:43] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:32:43] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 19:32:46] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:32:46] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:32:46] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:32:46] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:32:46] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:32:46] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:32:46] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:32:46] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:32:46] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:32:46] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:32:46] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:32:46] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:32:46] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:32:46] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=378702) [2026-01-25 19:32:47] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=378702) [2026-01-25 19:32:47] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=378702) [2026-01-25 19:32:47] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=378702) [2026-01-25 19:32:47] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=378702) [2026-01-25 19:32:47] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=378702) [2026-01-25 19:32:47] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=378702) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=378702) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.15s/it]
(EngineCore_DP0 pid=378702) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.15s/it]
(EngineCore_DP0 pid=378702) 
(EngineCore_DP0 pid=378702) [2026-01-25 19:32:48] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 4096] -> 1D uint8
(EngineCore_DP0 pid=378702) [2026-01-25 19:32:48] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15728640 bytes
(EngineCore_DP0 pid=378702) [2026-01-25 19:32:48] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 4096] -> 1D uint8
(EngineCore_DP0 pid=378702) [2026-01-25 19:32:48] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9437184 bytes
(EngineCore_DP0 pid=378702) [2026-01-25 19:32:48] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 4096] -> 1D uint8
(EngineCore_DP0 pid=378702) [2026-01-25 19:32:48] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 50331648 bytes
(EngineCore_DP0 pid=378702) [2026-01-25 19:32:48] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 10944] -> 1D uint8
(EngineCore_DP0 pid=378702) [2026-01-25 19:32:48] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 25264128 bytes
(EngineCore_DP0 pid=378702) 2026-01-25 19:32:58,752 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=378702) 2026-01-25 19:32:58,766 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=378702) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  4.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  4.05it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  4.09it/s]
(EngineCore_DP0 pid=378702) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.24it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.24it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  95%|█████████▍| 121/128 [00:00<00:00, 1209.21it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 1215.88it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:21,  5.84it/s, est. speed input: 2989.47 toks/s, output: 5.84 toks/s]
Processed prompts:   6%|▋         | 8/128 [00:00<00:03, 33.11it/s, est. speed input: 14425.88 toks/s, output: 28.17 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:00<00:02, 45.38it/s, est. speed input: 19483.32 toks/s, output: 38.05 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:00<00:02, 51.80it/s, est. speed input: 22277.82 toks/s, output: 43.51 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:00<00:01, 55.62it/s, est. speed input: 24068.02 toks/s, output: 47.01 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:00<00:01, 58.39it/s, est. speed input: 25385.81 toks/s, output: 49.58 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:00<00:01, 59.88it/s, est. speed input: 26299.24 toks/s, output: 51.37 toks/s]
Processed prompts:  39%|███▉      | 50/128 [00:00<00:01, 61.17it/s, est. speed input: 27048.57 toks/s, output: 52.83 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:01<00:01, 62.12it/s, est. speed input: 27652.36 toks/s, output: 54.01 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:01<00:01, 62.79it/s, est. speed input: 28146.44 toks/s, output: 54.97 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:01<00:00, 63.28it/s, est. speed input: 28558.53 toks/s, output: 55.78 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:01<00:00, 63.55it/s, est. speed input: 28897.80 toks/s, output: 56.44 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:01<00:00, 63.72it/s, est. speed input: 29184.81 toks/s, output: 57.00 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:01<00:00, 63.80it/s, est. speed input: 29429.37 toks/s, output: 57.48 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:01<00:00, 63.85it/s, est. speed input: 29641.58 toks/s, output: 57.89 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:01<00:00, 63.99it/s, est. speed input: 29838.57 toks/s, output: 58.28 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:01<00:00, 64.01it/s, est. speed input: 30005.92 toks/s, output: 58.61 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:02<00:00, 64.05it/s, est. speed input: 30157.79 toks/s, output: 58.90 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:02<00:00, 63.84it/s, est. speed input: 30274.93 toks/s, output: 59.13 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 63.84it/s, est. speed input: 30293.42 toks/s, output: 59.17 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 59.16it/s, est. speed input: 30293.42 toks/s, output: 59.17 toks/s]
[rank0]:[W125 19:33:02.523130380 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 24.8s

测试结果:
  Requests/s:   56.40
  Tokens/s:     28935.64
  Total Reqs:   128
  Elapsed:      2.27s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     28879.24

============================================================
[2/8] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-3B-INT8                                │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 19:33:08 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=379384) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=379384) WARNING 01-25 19:33:17 [backends.py:609] Failed to read file <frozen os>
Throughput: 38.41 requests/s, 39374.71 total tokens/s, 38.41 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-25 19:33:08] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:33:08] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:33:08] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:33:08] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:33:08] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:33:08] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:33:08] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:33:08] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:33:08] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:33:08] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:33:08] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:33:08] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:33:08] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:33:08] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 19:33:11] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:33:11] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:33:11] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:33:11] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:33:11] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:33:11] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:33:11] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:33:11] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:33:11] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:33:11] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:33:11] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:33:11] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:33:11] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:33:11] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=379384) [2026-01-25 19:33:12] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=379384) [2026-01-25 19:33:12] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=379384) [2026-01-25 19:33:12] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=379384) [2026-01-25 19:33:12] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=379384) [2026-01-25 19:33:12] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=379384) [2026-01-25 19:33:12] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=379384) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=379384) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.32it/s]
(EngineCore_DP0 pid=379384) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.32it/s]
(EngineCore_DP0 pid=379384) 
(EngineCore_DP0 pid=379384) [2026-01-25 19:33:13] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 4096] -> 1D uint8
(EngineCore_DP0 pid=379384) [2026-01-25 19:33:13] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15728640 bytes
(EngineCore_DP0 pid=379384) [2026-01-25 19:33:13] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 4096] -> 1D uint8
(EngineCore_DP0 pid=379384) [2026-01-25 19:33:13] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9437184 bytes
(EngineCore_DP0 pid=379384) [2026-01-25 19:33:13] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 4096] -> 1D uint8
(EngineCore_DP0 pid=379384) [2026-01-25 19:33:13] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 50331648 bytes
(EngineCore_DP0 pid=379384) [2026-01-25 19:33:13] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 10944] -> 1D uint8
(EngineCore_DP0 pid=379384) [2026-01-25 19:33:13] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 25264128 bytes
(EngineCore_DP0 pid=379384) 2026-01-25 19:33:23,081 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=379384) 2026-01-25 19:33:23,095 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=379384) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 24.90it/s]
(EngineCore_DP0 pid=379384) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.58it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.57it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  52%|█████▏    | 67/128 [00:00<00:00, 666.43it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 666.13it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 2/128 [00:00<00:07, 16.71it/s, est. speed input: 17113.88 toks/s, output: 16.71 toks/s]
Processed prompts:   5%|▌         | 7/128 [00:00<00:03, 31.48it/s, est. speed input: 29967.41 toks/s, output: 29.26 toks/s]
Processed prompts:   9%|▉         | 12/128 [00:00<00:03, 36.01it/s, est. speed input: 34117.57 toks/s, output: 33.32 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:02, 38.29it/s, est. speed input: 36283.91 toks/s, output: 35.43 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:00<00:02, 39.53it/s, est. speed input: 37575.16 toks/s, output: 36.69 toks/s]
Processed prompts:  21%|██        | 27/128 [00:00<00:02, 40.33it/s, est. speed input: 38455.62 toks/s, output: 37.55 toks/s]
Processed prompts:  25%|██▌       | 32/128 [00:00<00:02, 40.80it/s, est. speed input: 39073.95 toks/s, output: 38.16 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:00<00:02, 41.12it/s, est. speed input: 39537.72 toks/s, output: 38.61 toks/s]
Processed prompts:  33%|███▎      | 42/128 [00:01<00:02, 41.35it/s, est. speed input: 39907.19 toks/s, output: 38.97 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:01<00:01, 41.52it/s, est. speed input: 40205.59 toks/s, output: 39.26 toks/s]
Processed prompts:  41%|████      | 52/128 [00:01<00:01, 41.61it/s, est. speed input: 40442.39 toks/s, output: 39.49 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:01<00:01, 41.65it/s, est. speed input: 40633.81 toks/s, output: 39.68 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:01<00:01, 41.73it/s, est. speed input: 40809.60 toks/s, output: 39.85 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:01<00:01, 41.74it/s, est. speed input: 40949.63 toks/s, output: 39.99 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:01<00:01, 41.76it/s, est. speed input: 41073.62 toks/s, output: 40.11 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:01<00:01, 41.79it/s, est. speed input: 41185.72 toks/s, output: 40.22 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:02<00:01, 41.82it/s, est. speed input: 41285.19 toks/s, output: 40.32 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:02<00:00, 41.80it/s, est. speed input: 41367.66 toks/s, output: 40.40 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:02<00:00, 41.84it/s, est. speed input: 41450.03 toks/s, output: 40.48 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:02<00:00, 41.73it/s, est. speed input: 41501.92 toks/s, output: 40.53 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:02<00:00, 41.49it/s, est. speed input: 41522.21 toks/s, output: 40.55 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:02<00:00, 41.60it/s, est. speed input: 41583.50 toks/s, output: 40.61 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:02<00:00, 41.68it/s, est. speed input: 41638.72 toks/s, output: 40.66 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:02<00:00, 41.73it/s, est. speed input: 41689.47 toks/s, output: 40.71 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:02<00:00, 41.66it/s, est. speed input: 41721.16 toks/s, output: 40.74 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:03<00:00, 41.68it/s, est. speed input: 41760.26 toks/s, output: 40.78 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 41.68it/s, est. speed input: 41768.27 toks/s, output: 40.79 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 40.79it/s, est. speed input: 41768.27 toks/s, output: 40.79 toks/s]
[rank0]:[W125 19:33:27.524715008 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 25.0s

测试结果:
  Requests/s:   38.41
  Tokens/s:     39374.71
  Total Reqs:   128
  Elapsed:      3.33s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     39336.30

============================================================
[3/8] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-3B-INT8                                │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 19:33:33 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=380033) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=380033) WARNING 01-25 19:33:42 [backends.py:609] Failed to read file <frozen os>
Throughput: 44.14 requests/s, 45244.05 total tokens/s, 44.14 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-25 19:33:33] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:33:33] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:33:33] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:33:33] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:33:33] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:33:33] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:33:33] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:33:33] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:33:33] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:33:33] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:33:33] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:33:33] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:33:33] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:33:33] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 19:33:37] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:33:37] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:33:37] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:33:37] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:33:37] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:33:37] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:33:37] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:33:37] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:33:37] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:33:37] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:33:37] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:33:37] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:33:37] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:33:37] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=380033) [2026-01-25 19:33:38] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=380033) [2026-01-25 19:33:38] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=380033) [2026-01-25 19:33:38] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=380033) [2026-01-25 19:33:38] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=380033) [2026-01-25 19:33:38] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=380033) [2026-01-25 19:33:38] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=380033) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=380033) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.28it/s]
(EngineCore_DP0 pid=380033) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.28it/s]
(EngineCore_DP0 pid=380033) 
(EngineCore_DP0 pid=380033) [2026-01-25 19:33:38] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 4096] -> 1D uint8
(EngineCore_DP0 pid=380033) [2026-01-25 19:33:38] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15728640 bytes
(EngineCore_DP0 pid=380033) [2026-01-25 19:33:38] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 4096] -> 1D uint8
(EngineCore_DP0 pid=380033) [2026-01-25 19:33:38] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9437184 bytes
(EngineCore_DP0 pid=380033) [2026-01-25 19:33:38] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 4096] -> 1D uint8
(EngineCore_DP0 pid=380033) [2026-01-25 19:33:38] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 50331648 bytes
(EngineCore_DP0 pid=380033) [2026-01-25 19:33:38] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 10944] -> 1D uint8
(EngineCore_DP0 pid=380033) [2026-01-25 19:33:38] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 25264128 bytes
(EngineCore_DP0 pid=380033) 2026-01-25 19:33:48,618 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=380033) 2026-01-25 19:33:48,633 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=380033) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 24.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 24.54it/s]
(EngineCore_DP0 pid=380033) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 1/2 [00:00<00:00,  7.61it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 12.36it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  25%|██▌       | 64/256 [00:00<00:00, 639.16it/s]
Adding requests:  54%|█████▍    | 139/256 [00:00<00:00, 703.38it/s]
Adding requests:  83%|████████▎ | 212/256 [00:00<00:00, 714.53it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 710.83it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   7%|▋         | 18/256 [00:00<00:01, 144.40it/s, est. speed input: 147870.32 toks/s, output: 144.40 toks/s]
Processed prompts:  13%|█▎        | 33/256 [00:00<00:03, 68.96it/s, est. speed input: 77220.64 toks/s, output: 75.41 toks/s]   
Processed prompts:  16%|█▋        | 42/256 [00:00<00:03, 56.16it/s, est. speed input: 64993.29 toks/s, output: 63.47 toks/s]
Processed prompts:  19%|█▉        | 49/256 [00:00<00:03, 55.02it/s, est. speed input: 63013.21 toks/s, output: 61.54 toks/s]
Processed prompts:  22%|██▏       | 56/256 [00:00<00:04, 49.54it/s, est. speed input: 58795.37 toks/s, output: 57.42 toks/s]
Processed prompts:  24%|██▍       | 62/256 [00:01<00:04, 48.17it/s, est. speed input: 57182.60 toks/s, output: 55.84 toks/s]
Processed prompts:  27%|██▋       | 68/256 [00:01<00:03, 47.15it/s, est. speed input: 55929.49 toks/s, output: 54.62 toks/s]
Processed prompts:  29%|██▉       | 74/256 [00:01<00:03, 46.36it/s, est. speed input: 54901.61 toks/s, output: 53.61 toks/s]
Processed prompts:  31%|███▏      | 80/256 [00:01<00:03, 45.84it/s, est. speed input: 54078.35 toks/s, output: 52.81 toks/s]
Processed prompts:  34%|███▎      | 86/256 [00:01<00:03, 45.42it/s, est. speed input: 53371.69 toks/s, output: 52.12 toks/s]
Processed prompts:  36%|███▌      | 92/256 [00:01<00:03, 45.22it/s, est. speed input: 52805.05 toks/s, output: 51.57 toks/s]
Processed prompts:  38%|███▊      | 98/256 [00:01<00:03, 45.10it/s, est. speed input: 52320.84 toks/s, output: 51.09 toks/s]
Processed prompts:  41%|████      | 104/256 [00:02<00:03, 44.92it/s, est. speed input: 51877.68 toks/s, output: 50.66 toks/s]
Processed prompts:  43%|████▎     | 110/256 [00:02<00:03, 44.75it/s, est. speed input: 51479.23 toks/s, output: 50.27 toks/s]
Processed prompts:  45%|████▌     | 116/256 [00:02<00:03, 44.77it/s, est. speed input: 51155.32 toks/s, output: 49.96 toks/s]
Processed prompts:  48%|████▊     | 122/256 [00:02<00:02, 44.75it/s, est. speed input: 50861.03 toks/s, output: 49.67 toks/s]
Processed prompts:  50%|█████     | 128/256 [00:02<00:02, 44.71it/s, est. speed input: 50592.61 toks/s, output: 49.41 toks/s]
Processed prompts:  52%|█████▏    | 134/256 [00:02<00:02, 44.69it/s, est. speed input: 50352.32 toks/s, output: 49.17 toks/s]
Processed prompts:  55%|█████▍    | 140/256 [00:02<00:02, 44.66it/s, est. speed input: 50132.59 toks/s, output: 48.96 toks/s]
Processed prompts:  57%|█████▋    | 146/256 [00:02<00:02, 44.68it/s, est. speed input: 49938.22 toks/s, output: 48.77 toks/s]
Processed prompts:  59%|█████▉    | 152/256 [00:03<00:02, 44.68it/s, est. speed input: 49758.13 toks/s, output: 48.59 toks/s]
Processed prompts:  62%|██████▏   | 158/256 [00:03<00:02, 44.71it/s, est. speed input: 49598.43 toks/s, output: 48.44 toks/s]
Processed prompts:  64%|██████▍   | 164/256 [00:03<00:02, 44.71it/s, est. speed input: 49447.47 toks/s, output: 48.29 toks/s]
Processed prompts:  66%|██████▋   | 170/256 [00:03<00:01, 44.63it/s, est. speed input: 49297.00 toks/s, output: 48.14 toks/s]
Processed prompts:  69%|██████▉   | 176/256 [00:03<00:01, 44.65it/s, est. speed input: 49167.63 toks/s, output: 48.02 toks/s]
Processed prompts:  71%|███████   | 182/256 [00:03<00:01, 44.61it/s, est. speed input: 49041.32 toks/s, output: 47.89 toks/s]
Processed prompts:  73%|███████▎  | 188/256 [00:03<00:01, 44.66it/s, est. speed input: 48932.79 toks/s, output: 47.79 toks/s]
Processed prompts:  76%|███████▌  | 194/256 [00:04<00:01, 44.60it/s, est. speed input: 48820.09 toks/s, output: 47.68 toks/s]
Processed prompts:  78%|███████▊  | 200/256 [00:04<00:01, 44.57it/s, est. speed input: 48715.66 toks/s, output: 47.57 toks/s]
Processed prompts:  80%|████████  | 206/256 [00:04<00:01, 44.50it/s, est. speed input: 48611.94 toks/s, output: 47.47 toks/s]
Processed prompts:  83%|████████▎ | 212/256 [00:04<00:00, 44.53it/s, est. speed input: 48523.82 toks/s, output: 47.39 toks/s]
Processed prompts:  85%|████████▌ | 218/256 [00:04<00:00, 44.58it/s, est. speed input: 48443.18 toks/s, output: 47.31 toks/s]
Processed prompts:  88%|████████▊ | 224/256 [00:04<00:00, 44.61it/s, est. speed input: 48367.12 toks/s, output: 47.23 toks/s]
Processed prompts:  90%|████████▉ | 230/256 [00:04<00:00, 44.61it/s, est. speed input: 48293.07 toks/s, output: 47.16 toks/s]
Processed prompts:  92%|█████████▏| 236/256 [00:05<00:00, 44.64it/s, est. speed input: 48225.74 toks/s, output: 47.10 toks/s]
Processed prompts:  95%|█████████▍| 242/256 [00:05<00:00, 44.65it/s, est. speed input: 48160.84 toks/s, output: 47.03 toks/s]
Processed prompts:  97%|█████████▋| 248/256 [00:05<00:00, 44.66it/s, est. speed input: 48099.56 toks/s, output: 46.97 toks/s]
Processed prompts:  99%|█████████▉| 254/256 [00:05<00:00, 44.67it/s, est. speed input: 48042.24 toks/s, output: 46.92 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:05<00:00, 44.67it/s, est. speed input: 48209.38 toks/s, output: 47.08 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:05<00:00, 47.08it/s, est. speed input: 48209.38 toks/s, output: 47.08 toks/s]
[rank0]:[W125 19:33:55.596163472 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 27.9s

测试结果:
  Requests/s:   44.14
  Tokens/s:     45244.05
  Total Reqs:   256
  Elapsed:      5.80s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     45199.91

============================================================
[4/8] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-3B-INT8                                │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 19:34:02 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=380699) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=380699) WARNING 01-25 19:34:11 [backends.py:609] Failed to read file <frozen os>
Throughput: 42.10 requests/s, 43151.59 total tokens/s, 42.10 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-25 19:34:02] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:34:02] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:34:02] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:34:02] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:34:02] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:34:02] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:34:02] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:34:02] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:34:02] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:34:02] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:34:02] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:34:02] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:34:02] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:34:02] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 19:34:05] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:34:05] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:34:05] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:34:05] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:34:05] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:34:05] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:34:05] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:34:05] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:34:05] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:34:05] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:34:05] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:34:05] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:34:05] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:34:05] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=380699) [2026-01-25 19:34:06] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=380699) [2026-01-25 19:34:06] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=380699) [2026-01-25 19:34:06] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=380699) [2026-01-25 19:34:06] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=380699) [2026-01-25 19:34:06] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=380699) [2026-01-25 19:34:06] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=380699) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=380699) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.29it/s]
(EngineCore_DP0 pid=380699) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.29it/s]
(EngineCore_DP0 pid=380699) 
(EngineCore_DP0 pid=380699) [2026-01-25 19:34:07] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 4096] -> 1D uint8
(EngineCore_DP0 pid=380699) [2026-01-25 19:34:07] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15728640 bytes
(EngineCore_DP0 pid=380699) [2026-01-25 19:34:07] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 4096] -> 1D uint8
(EngineCore_DP0 pid=380699) [2026-01-25 19:34:07] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9437184 bytes
(EngineCore_DP0 pid=380699) [2026-01-25 19:34:07] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 4096] -> 1D uint8
(EngineCore_DP0 pid=380699) [2026-01-25 19:34:07] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 50331648 bytes
(EngineCore_DP0 pid=380699) [2026-01-25 19:34:07] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 10944] -> 1D uint8
(EngineCore_DP0 pid=380699) [2026-01-25 19:34:07] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 25264128 bytes
(EngineCore_DP0 pid=380699) 2026-01-25 19:34:16,935 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=380699) 2026-01-25 19:34:16,950 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=380699) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 3/4 [00:00<00:00, 28.34it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 26.07it/s]
(EngineCore_DP0 pid=380699) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  33%|███▎      | 1/3 [00:00<00:00,  7.54it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 15.74it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:  14%|█▍        | 73/512 [00:00<00:00, 723.00it/s]
Adding requests:  29%|██▉       | 149/512 [00:00<00:00, 742.80it/s]
Adding requests:  44%|████▍     | 225/512 [00:00<00:00, 748.90it/s]
Adding requests:  59%|█████▊    | 300/512 [00:00<00:00, 748.57it/s]
Adding requests:  74%|███████▎  | 377/512 [00:00<00:00, 754.70it/s]
Adding requests:  88%|████████▊ | 453/512 [00:00<00:00, 753.38it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 749.33it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   6%|▌         | 30/512 [00:00<00:02, 197.70it/s, est. speed input: 202468.32 toks/s, output: 197.70 toks/s]
Processed prompts:  10%|▉         | 50/512 [00:00<00:06, 70.93it/s, est. speed input: 82108.43 toks/s, output: 80.18 toks/s]   
Processed prompts:  12%|█▏        | 61/512 [00:00<00:06, 66.87it/s, est. speed input: 76869.23 toks/s, output: 75.07 toks/s]
Processed prompts:  14%|█▎        | 70/512 [00:01<00:08, 52.61it/s, est. speed input: 65380.57 toks/s, output: 63.85 toks/s]
Processed prompts:  15%|█▌        | 78/512 [00:01<00:08, 49.79it/s, est. speed input: 62167.79 toks/s, output: 60.71 toks/s]
Processed prompts:  17%|█▋        | 86/512 [00:01<00:08, 47.70it/s, est. speed input: 59777.49 toks/s, output: 58.38 toks/s]
Processed prompts:  18%|█▊        | 94/512 [00:01<00:09, 46.19it/s, est. speed input: 57931.17 toks/s, output: 56.57 toks/s]
Processed prompts:  20%|█▉        | 102/512 [00:01<00:09, 45.06it/s, est. speed input: 56442.49 toks/s, output: 55.12 toks/s]
Processed prompts:  21%|██▏       | 110/512 [00:02<00:09, 44.25it/s, est. speed input: 55225.82 toks/s, output: 53.93 toks/s]
Processed prompts:  23%|██▎       | 118/512 [00:02<00:09, 43.65it/s, est. speed input: 54208.80 toks/s, output: 52.94 toks/s]
Processed prompts:  25%|██▍       | 126/512 [00:02<00:08, 43.25it/s, est. speed input: 53355.15 toks/s, output: 52.10 toks/s]
Processed prompts:  26%|██▌       | 134/512 [00:02<00:08, 42.97it/s, est. speed input: 52629.18 toks/s, output: 51.40 toks/s]
Processed prompts:  28%|██▊       | 142/512 [00:02<00:08, 42.79it/s, est. speed input: 52003.74 toks/s, output: 50.78 toks/s]
Processed prompts:  29%|██▉       | 150/512 [00:02<00:08, 42.65it/s, est. speed input: 51455.95 toks/s, output: 50.25 toks/s]
Processed prompts:  31%|███       | 158/512 [00:03<00:08, 42.54it/s, est. speed input: 50968.32 toks/s, output: 49.77 toks/s]
Processed prompts:  32%|███▏      | 166/512 [00:03<00:08, 42.50it/s, est. speed input: 50545.49 toks/s, output: 49.36 toks/s]
Processed prompts:  34%|███▍      | 174/512 [00:03<00:07, 42.48it/s, est. speed input: 50168.15 toks/s, output: 48.99 toks/s]
Processed prompts:  36%|███▌      | 182/512 [00:03<00:07, 42.45it/s, est. speed input: 49826.06 toks/s, output: 48.66 toks/s]
Processed prompts:  37%|███▋      | 190/512 [00:03<00:07, 42.43it/s, est. speed input: 49517.95 toks/s, output: 48.36 toks/s]
Processed prompts:  39%|███▊      | 198/512 [00:04<00:07, 42.42it/s, est. speed input: 49238.44 toks/s, output: 48.08 toks/s]
Processed prompts:  40%|████      | 206/512 [00:04<00:07, 42.37it/s, est. speed input: 48975.43 toks/s, output: 47.83 toks/s]
Processed prompts:  42%|████▏     | 214/512 [00:04<00:07, 42.32it/s, est. speed input: 48733.64 toks/s, output: 47.59 toks/s]
Processed prompts:  43%|████▎     | 222/512 [00:04<00:06, 42.35it/s, est. speed input: 48519.26 toks/s, output: 47.38 toks/s]
Processed prompts:  45%|████▍     | 230/512 [00:04<00:06, 42.33it/s, est. speed input: 48317.24 toks/s, output: 47.18 toks/s]
Processed prompts:  46%|████▋     | 238/512 [00:05<00:06, 42.30it/s, est. speed input: 48126.98 toks/s, output: 47.00 toks/s]
Processed prompts:  48%|████▊     | 246/512 [00:05<00:06, 42.27it/s, est. speed input: 47950.32 toks/s, output: 46.83 toks/s]
Processed prompts:  50%|████▉     | 254/512 [00:05<00:06, 42.21it/s, est. speed input: 47780.68 toks/s, output: 46.66 toks/s]
Processed prompts:  51%|█████     | 262/512 [00:05<00:05, 42.24it/s, est. speed input: 47630.60 toks/s, output: 46.51 toks/s]
Processed prompts:  53%|█████▎    | 270/512 [00:05<00:05, 42.28it/s, est. speed input: 47493.10 toks/s, output: 46.38 toks/s]
Processed prompts:  54%|█████▍    | 278/512 [00:06<00:05, 42.30it/s, est. speed input: 47363.42 toks/s, output: 46.25 toks/s]
Processed prompts:  56%|█████▌    | 286/512 [00:06<00:05, 42.32it/s, est. speed input: 47242.55 toks/s, output: 46.14 toks/s]
Processed prompts:  57%|█████▋    | 294/512 [00:06<00:05, 42.30it/s, est. speed input: 47123.96 toks/s, output: 46.02 toks/s]
Processed prompts:  59%|█████▉    | 302/512 [00:06<00:04, 42.29it/s, est. speed input: 47013.44 toks/s, output: 45.91 toks/s]
Processed prompts:  61%|██████    | 310/512 [00:06<00:04, 42.29it/s, est. speed input: 46910.13 toks/s, output: 45.81 toks/s]
Processed prompts:  62%|██████▏   | 318/512 [00:06<00:04, 42.33it/s, est. speed input: 46816.10 toks/s, output: 45.72 toks/s]
Processed prompts:  64%|██████▎   | 326/512 [00:07<00:04, 42.29it/s, est. speed input: 46720.68 toks/s, output: 45.63 toks/s]
Processed prompts:  65%|██████▌   | 334/512 [00:07<00:04, 42.28it/s, est. speed input: 46631.68 toks/s, output: 45.54 toks/s]
Processed prompts:  67%|██████▋   | 342/512 [00:07<00:04, 42.29it/s, est. speed input: 46548.79 toks/s, output: 45.46 toks/s]
Processed prompts:  68%|██████▊   | 350/512 [00:07<00:03, 42.30it/s, est. speed input: 46469.90 toks/s, output: 45.38 toks/s]
Processed prompts:  70%|██████▉   | 358/512 [00:07<00:03, 42.28it/s, est. speed input: 46392.27 toks/s, output: 45.30 toks/s]
Processed prompts:  71%|███████▏  | 366/512 [00:08<00:03, 42.28it/s, est. speed input: 46319.70 toks/s, output: 45.23 toks/s]
Processed prompts:  73%|███████▎  | 374/512 [00:08<00:03, 42.28it/s, est. speed input: 46250.55 toks/s, output: 45.17 toks/s]
Processed prompts:  75%|███████▍  | 382/512 [00:08<00:03, 42.27it/s, est. speed input: 46183.88 toks/s, output: 45.10 toks/s]
Processed prompts:  76%|███████▌  | 390/512 [00:08<00:02, 42.26it/s, est. speed input: 46119.49 toks/s, output: 45.04 toks/s]
Processed prompts:  78%|███████▊  | 398/512 [00:08<00:02, 42.28it/s, est. speed input: 46060.50 toks/s, output: 44.98 toks/s]
Processed prompts:  79%|███████▉  | 406/512 [00:09<00:02, 42.31it/s, est. speed input: 46004.90 toks/s, output: 44.93 toks/s]
Processed prompts:  81%|████████  | 414/512 [00:09<00:02, 42.28it/s, est. speed input: 45947.54 toks/s, output: 44.87 toks/s]
Processed prompts:  82%|████████▏ | 422/512 [00:09<00:02, 42.28it/s, est. speed input: 45894.36 toks/s, output: 44.82 toks/s]
Processed prompts:  84%|████████▍ | 430/512 [00:09<00:01, 42.30it/s, est. speed input: 45844.81 toks/s, output: 44.77 toks/s]
Processed prompts:  86%|████████▌ | 438/512 [00:09<00:01, 42.31it/s, est. speed input: 45796.28 toks/s, output: 44.72 toks/s]
Processed prompts:  87%|████████▋ | 446/512 [00:09<00:01, 42.33it/s, est. speed input: 45751.07 toks/s, output: 44.68 toks/s]
Processed prompts:  89%|████████▊ | 454/512 [00:10<00:01, 42.35it/s, est. speed input: 45707.45 toks/s, output: 44.64 toks/s]
Processed prompts:  90%|█████████ | 462/512 [00:10<00:01, 42.33it/s, est. speed input: 45663.59 toks/s, output: 44.59 toks/s]
Processed prompts:  92%|█████████▏| 470/512 [00:10<00:00, 42.31it/s, est. speed input: 45620.61 toks/s, output: 44.55 toks/s]
Processed prompts:  93%|█████████▎| 478/512 [00:10<00:00, 42.25it/s, est. speed input: 45576.78 toks/s, output: 44.51 toks/s]
Processed prompts:  95%|█████████▍| 486/512 [00:10<00:00, 42.27it/s, est. speed input: 45537.76 toks/s, output: 44.47 toks/s]
Processed prompts:  96%|█████████▋| 494/512 [00:11<00:00, 42.29it/s, est. speed input: 45500.32 toks/s, output: 44.43 toks/s]
Processed prompts:  98%|█████████▊| 502/512 [00:11<00:00, 42.26it/s, est. speed input: 45462.23 toks/s, output: 44.40 toks/s]
Processed prompts: 100%|█████████▉| 510/512 [00:11<00:00, 43.57it/s, est. speed input: 45501.21 toks/s, output: 44.43 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:11<00:00, 43.57it/s, est. speed input: 45679.28 toks/s, output: 44.61 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:11<00:00, 44.61it/s, est. speed input: 45679.28 toks/s, output: 44.61 toks/s]
[rank0]:[W125 19:34:30.480596960 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 34.9s

测试结果:
  Requests/s:   42.10
  Tokens/s:     43151.59
  Total Reqs:   512
  Elapsed:      12.16s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     43109.49

============================================================
[5/8] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-3B-INT8                                │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 19:34:38 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=381469) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=381469) WARNING 01-25 19:34:47 [backends.py:609] Failed to read file <frozen os>
Throughput: 40.55 requests/s, 41566.37 total tokens/s, 40.55 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-25 19:34:38] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:34:38] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:34:38] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:34:38] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:34:38] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:34:38] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:34:38] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:34:38] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:34:38] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:34:38] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:34:38] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:34:38] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:34:38] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:34:38] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 19:34:42] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:34:42] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:34:42] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:34:42] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:34:42] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:34:42] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:34:42] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:34:42] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:34:42] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:34:42] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:34:42] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:34:42] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:34:42] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:34:42] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=381469) [2026-01-25 19:34:43] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=381469) [2026-01-25 19:34:43] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=381469) [2026-01-25 19:34:43] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=381469) [2026-01-25 19:34:43] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=381469) [2026-01-25 19:34:43] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=381469) [2026-01-25 19:34:43] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=381469) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=381469) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.24it/s]
(EngineCore_DP0 pid=381469) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.23it/s]
(EngineCore_DP0 pid=381469) 
(EngineCore_DP0 pid=381469) [2026-01-25 19:34:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 4096] -> 1D uint8
(EngineCore_DP0 pid=381469) [2026-01-25 19:34:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15728640 bytes
(EngineCore_DP0 pid=381469) [2026-01-25 19:34:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 4096] -> 1D uint8
(EngineCore_DP0 pid=381469) [2026-01-25 19:34:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9437184 bytes
(EngineCore_DP0 pid=381469) [2026-01-25 19:34:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 4096] -> 1D uint8
(EngineCore_DP0 pid=381469) [2026-01-25 19:34:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 50331648 bytes
(EngineCore_DP0 pid=381469) [2026-01-25 19:34:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 10944] -> 1D uint8
(EngineCore_DP0 pid=381469) [2026-01-25 19:34:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 25264128 bytes
(EngineCore_DP0 pid=381469) 2026-01-25 19:34:53,672 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=381469) 2026-01-25 19:34:53,686 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=381469) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 1/5 [00:00<00:00,  6.54it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00, 16.07it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 14.31it/s]
(EngineCore_DP0 pid=381469) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 1/4 [00:00<00:00,  5.98it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 15.13it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 13.57it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   7%|▋         | 72/1024 [00:00<00:01, 712.86it/s]
Adding requests:  14%|█▍        | 148/1024 [00:00<00:01, 740.36it/s]
Adding requests:  22%|██▏       | 224/1024 [00:00<00:01, 748.24it/s]
Adding requests:  29%|██▉       | 299/1024 [00:00<00:00, 746.42it/s]
Adding requests:  37%|███▋      | 374/1024 [00:00<00:00, 746.90it/s]
Adding requests:  44%|████▍     | 450/1024 [00:00<00:00, 749.39it/s]
Adding requests:  51%|█████▏    | 525/1024 [00:00<00:00, 738.76it/s]
Adding requests:  59%|█████▊    | 600/1024 [00:00<00:00, 737.43it/s]
Adding requests:  66%|██████▋   | 679/1024 [00:00<00:00, 751.15it/s]
Adding requests:  74%|███████▍  | 757/1024 [00:01<00:00, 758.30it/s]
Adding requests:  81%|████████▏ | 833/1024 [00:01<00:00, 747.20it/s]
Adding requests:  89%|████████▉ | 911/1024 [00:01<00:00, 756.93it/s]
Adding requests:  97%|█████████▋| 989/1024 [00:01<00:00, 760.82it/s]
Adding requests: 100%|██████████| 1024/1024 [00:01<00:00, 751.62it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   6%|▌         | 58/1024 [00:00<00:04, 202.36it/s, est. speed input: 207235.20 toks/s, output: 202.37 toks/s]
Processed prompts:   8%|▊         | 79/1024 [00:00<00:09, 103.76it/s, est. speed input: 119024.75 toks/s, output: 116.23 toks/s]
Processed prompts:   9%|▉         | 91/1024 [00:01<00:13, 68.14it/s, est. speed input: 86840.31 toks/s, output: 84.80 toks/s]   
Processed prompts:  10%|▉         | 99/1024 [00:01<00:15, 61.08it/s, est. speed input: 79855.63 toks/s, output: 77.98 toks/s]
Processed prompts:  10%|█         | 106/1024 [00:01<00:16, 54.20it/s, est. speed input: 74024.80 toks/s, output: 72.29 toks/s]
Processed prompts:  11%|█         | 114/1024 [00:01<00:18, 50.44it/s, est. speed input: 70190.95 toks/s, output: 68.55 toks/s]
Processed prompts:  12%|█▏        | 122/1024 [00:01<00:18, 47.67it/s, est. speed input: 67177.32 toks/s, output: 65.60 toks/s]
Processed prompts:  13%|█▎        | 130/1024 [00:02<00:19, 45.69it/s, est. speed input: 64752.08 toks/s, output: 63.23 toks/s]
Processed prompts:  13%|█▎        | 138/1024 [00:02<00:20, 44.22it/s, est. speed input: 62735.67 toks/s, output: 61.26 toks/s]
Processed prompts:  14%|█▍        | 146/1024 [00:02<00:20, 43.19it/s, est. speed input: 61045.10 toks/s, output: 59.61 toks/s]
Processed prompts:  15%|█▌        | 154/1024 [00:02<00:20, 42.48it/s, est. speed input: 59615.64 toks/s, output: 58.22 toks/s]
Processed prompts:  16%|█▌        | 162/1024 [00:02<00:20, 41.95it/s, est. speed input: 58375.20 toks/s, output: 57.01 toks/s]
Processed prompts:  17%|█▋        | 170/1024 [00:03<00:20, 41.58it/s, est. speed input: 57295.55 toks/s, output: 55.95 toks/s]
Processed prompts:  17%|█▋        | 178/1024 [00:03<00:20, 41.31it/s, est. speed input: 56344.10 toks/s, output: 55.02 toks/s]
Processed prompts:  18%|█▊        | 186/1024 [00:03<00:20, 41.12it/s, est. speed input: 55502.36 toks/s, output: 54.20 toks/s]
Processed prompts:  19%|█▉        | 194/1024 [00:03<00:20, 40.98it/s, est. speed input: 54749.29 toks/s, output: 53.47 toks/s]
Processed prompts:  20%|█▉        | 202/1024 [00:03<00:20, 40.88it/s, est. speed input: 54073.32 toks/s, output: 52.81 toks/s]
Processed prompts:  21%|██        | 210/1024 [00:04<00:19, 40.83it/s, est. speed input: 53469.27 toks/s, output: 52.22 toks/s]
Processed prompts:  21%|██▏       | 218/1024 [00:04<00:19, 40.79it/s, est. speed input: 52920.11 toks/s, output: 51.68 toks/s]
Processed prompts:  22%|██▏       | 226/1024 [00:04<00:19, 40.77it/s, est. speed input: 52420.92 toks/s, output: 51.19 toks/s]
Processed prompts:  23%|██▎       | 234/1024 [00:04<00:19, 40.75it/s, est. speed input: 51962.28 toks/s, output: 50.74 toks/s]
Processed prompts:  24%|██▎       | 242/1024 [00:04<00:19, 40.74it/s, est. speed input: 51542.87 toks/s, output: 50.33 toks/s]
Processed prompts:  24%|██▍       | 250/1024 [00:05<00:19, 40.72it/s, est. speed input: 51153.88 toks/s, output: 49.95 toks/s]
Processed prompts:  25%|██▌       | 258/1024 [00:05<00:18, 40.70it/s, est. speed input: 50793.19 toks/s, output: 49.60 toks/s]
Processed prompts:  26%|██▌       | 266/1024 [00:05<00:18, 40.69it/s, est. speed input: 50459.43 toks/s, output: 49.28 toks/s]
Processed prompts:  27%|██▋       | 274/1024 [00:05<00:18, 40.69it/s, est. speed input: 50150.74 toks/s, output: 48.98 toks/s]
Processed prompts:  28%|██▊       | 282/1024 [00:05<00:18, 40.67it/s, est. speed input: 49860.40 toks/s, output: 48.69 toks/s]
Processed prompts:  28%|██▊       | 290/1024 [00:05<00:18, 40.68it/s, est. speed input: 49592.17 toks/s, output: 48.43 toks/s]
Processed prompts:  29%|██▉       | 298/1024 [00:06<00:17, 40.69it/s, est. speed input: 49340.54 toks/s, output: 48.18 toks/s]
Processed prompts:  30%|██▉       | 306/1024 [00:06<00:17, 40.69it/s, est. speed input: 49103.71 toks/s, output: 47.95 toks/s]
Processed prompts:  31%|███       | 314/1024 [00:06<00:17, 40.67it/s, est. speed input: 48879.38 toks/s, output: 47.73 toks/s]
Processed prompts:  31%|███▏      | 322/1024 [00:06<00:17, 40.68it/s, est. speed input: 48670.42 toks/s, output: 47.53 toks/s]
Processed prompts:  32%|███▏      | 330/1024 [00:06<00:17, 40.69it/s, est. speed input: 48473.50 toks/s, output: 47.34 toks/s]
Processed prompts:  33%|███▎      | 338/1024 [00:07<00:16, 40.64it/s, est. speed input: 48281.62 toks/s, output: 47.15 toks/s]
Processed prompts:  34%|███▍      | 346/1024 [00:07<00:16, 40.66it/s, est. speed input: 48106.01 toks/s, output: 46.98 toks/s]
Processed prompts:  35%|███▍      | 354/1024 [00:07<00:16, 40.65it/s, est. speed input: 47936.86 toks/s, output: 46.81 toks/s]
Processed prompts:  35%|███▌      | 362/1024 [00:07<00:16, 40.67it/s, est. speed input: 47778.15 toks/s, output: 46.66 toks/s]
Processed prompts:  36%|███▌      | 370/1024 [00:07<00:16, 40.66it/s, est. speed input: 47625.58 toks/s, output: 46.51 toks/s]
Processed prompts:  37%|███▋      | 378/1024 [00:08<00:15, 40.65it/s, est. speed input: 47480.50 toks/s, output: 46.37 toks/s]
Processed prompts:  38%|███▊      | 386/1024 [00:08<00:15, 40.69it/s, est. speed input: 47346.32 toks/s, output: 46.24 toks/s]
Processed prompts:  38%|███▊      | 394/1024 [00:08<00:15, 40.69it/s, est. speed input: 47215.59 toks/s, output: 46.11 toks/s]
Processed prompts:  39%|███▉      | 402/1024 [00:08<00:15, 40.72it/s, est. speed input: 47092.97 toks/s, output: 45.99 toks/s]
Processed prompts:  40%|████      | 410/1024 [00:08<00:15, 40.70it/s, est. speed input: 46973.07 toks/s, output: 45.87 toks/s]
Processed prompts:  41%|████      | 418/1024 [00:09<00:14, 40.68it/s, est. speed input: 46857.02 toks/s, output: 45.76 toks/s]
Processed prompts:  42%|████▏     | 426/1024 [00:09<00:14, 40.68it/s, est. speed input: 46747.21 toks/s, output: 45.65 toks/s]
Processed prompts:  42%|████▏     | 434/1024 [00:09<00:14, 40.67it/s, est. speed input: 46641.87 toks/s, output: 45.55 toks/s]
Processed prompts:  43%|████▎     | 442/1024 [00:09<00:14, 40.66it/s, est. speed input: 46539.96 toks/s, output: 45.45 toks/s]
Processed prompts:  44%|████▍     | 450/1024 [00:09<00:14, 40.64it/s, est. speed input: 46440.81 toks/s, output: 45.35 toks/s]
Processed prompts:  45%|████▍     | 458/1024 [00:10<00:13, 40.63it/s, est. speed input: 46346.22 toks/s, output: 45.26 toks/s]
Processed prompts:  46%|████▌     | 466/1024 [00:10<00:13, 40.65it/s, est. speed input: 46257.72 toks/s, output: 45.17 toks/s]
Processed prompts:  46%|████▋     | 474/1024 [00:10<00:13, 40.68it/s, est. speed input: 46173.11 toks/s, output: 45.09 toks/s]
Processed prompts:  47%|████▋     | 482/1024 [00:10<00:13, 40.69it/s, est. speed input: 46090.39 toks/s, output: 45.01 toks/s]
Processed prompts:  48%|████▊     | 490/1024 [00:10<00:13, 40.70it/s, est. speed input: 46011.49 toks/s, output: 44.93 toks/s]
Processed prompts:  49%|████▊     | 498/1024 [00:11<00:12, 40.68it/s, est. speed input: 45933.12 toks/s, output: 44.86 toks/s]
Processed prompts:  49%|████▉     | 506/1024 [00:11<00:12, 40.67it/s, est. speed input: 45858.02 toks/s, output: 44.78 toks/s]
Processed prompts:  50%|█████     | 514/1024 [00:11<00:12, 40.67it/s, est. speed input: 45785.83 toks/s, output: 44.71 toks/s]
Processed prompts:  51%|█████     | 522/1024 [00:11<00:12, 40.66it/s, est. speed input: 45715.97 toks/s, output: 44.64 toks/s]
Processed prompts:  52%|█████▏    | 530/1024 [00:11<00:12, 40.65it/s, est. speed input: 45647.41 toks/s, output: 44.58 toks/s]
Processed prompts:  53%|█████▎    | 538/1024 [00:12<00:11, 40.64it/s, est. speed input: 45581.79 toks/s, output: 44.51 toks/s]
Processed prompts:  53%|█████▎    | 546/1024 [00:12<00:11, 40.64it/s, est. speed input: 45517.73 toks/s, output: 44.45 toks/s]
Processed prompts:  54%|█████▍    | 554/1024 [00:12<00:11, 40.63it/s, est. speed input: 45456.03 toks/s, output: 44.39 toks/s]
Processed prompts:  55%|█████▍    | 562/1024 [00:12<00:11, 40.65it/s, est. speed input: 45397.27 toks/s, output: 44.33 toks/s]
Processed prompts:  56%|█████▌    | 570/1024 [00:12<00:11, 40.64it/s, est. speed input: 45339.20 toks/s, output: 44.28 toks/s]
Processed prompts:  56%|█████▋    | 578/1024 [00:13<00:10, 40.63it/s, est. speed input: 45282.42 toks/s, output: 44.22 toks/s]
Processed prompts:  57%|█████▋    | 586/1024 [00:13<00:10, 40.61it/s, est. speed input: 45226.78 toks/s, output: 44.17 toks/s]
Processed prompts:  58%|█████▊    | 594/1024 [00:13<00:10, 40.64it/s, est. speed input: 45174.97 toks/s, output: 44.12 toks/s]
Processed prompts:  59%|█████▉    | 602/1024 [00:13<00:10, 40.64it/s, est. speed input: 45123.84 toks/s, output: 44.07 toks/s]
Processed prompts:  60%|█████▉    | 610/1024 [00:13<00:10, 40.63it/s, est. speed input: 45073.64 toks/s, output: 44.02 toks/s]
Processed prompts:  60%|██████    | 618/1024 [00:14<00:09, 40.63it/s, est. speed input: 45024.71 toks/s, output: 43.97 toks/s]
Processed prompts:  61%|██████    | 626/1024 [00:14<00:09, 40.60it/s, est. speed input: 44976.27 toks/s, output: 43.92 toks/s]
Processed prompts:  62%|██████▏   | 634/1024 [00:14<00:09, 40.63it/s, est. speed input: 44931.20 toks/s, output: 43.88 toks/s]
Processed prompts:  63%|██████▎   | 642/1024 [00:14<00:09, 40.64it/s, est. speed input: 44886.85 toks/s, output: 43.83 toks/s]
Processed prompts:  63%|██████▎   | 650/1024 [00:14<00:09, 40.66it/s, est. speed input: 44844.48 toks/s, output: 43.79 toks/s]
Processed prompts:  64%|██████▍   | 658/1024 [00:15<00:09, 40.65it/s, est. speed input: 44802.17 toks/s, output: 43.75 toks/s]
Processed prompts:  65%|██████▌   | 666/1024 [00:15<00:08, 40.62it/s, est. speed input: 44759.41 toks/s, output: 43.71 toks/s]
Processed prompts:  66%|██████▌   | 674/1024 [00:15<00:08, 40.60it/s, est. speed input: 44718.35 toks/s, output: 43.67 toks/s]
Processed prompts:  67%|██████▋   | 682/1024 [00:15<00:08, 40.62it/s, est. speed input: 44679.48 toks/s, output: 43.63 toks/s]
Processed prompts:  67%|██████▋   | 690/1024 [00:15<00:08, 40.62it/s, est. speed input: 44641.05 toks/s, output: 43.59 toks/s]
Processed prompts:  68%|██████▊   | 698/1024 [00:16<00:08, 40.60it/s, est. speed input: 44603.00 toks/s, output: 43.56 toks/s]
Processed prompts:  69%|██████▉   | 706/1024 [00:16<00:07, 40.59it/s, est. speed input: 44565.77 toks/s, output: 43.52 toks/s]
Processed prompts:  70%|██████▉   | 714/1024 [00:16<00:07, 40.60it/s, est. speed input: 44530.22 toks/s, output: 43.49 toks/s]
Processed prompts:  71%|███████   | 722/1024 [00:16<00:07, 40.61it/s, est. speed input: 44495.61 toks/s, output: 43.45 toks/s]
Processed prompts:  71%|███████▏  | 730/1024 [00:16<00:07, 40.63it/s, est. speed input: 44462.29 toks/s, output: 43.42 toks/s]
Processed prompts:  72%|███████▏  | 738/1024 [00:17<00:07, 40.64it/s, est. speed input: 44429.60 toks/s, output: 43.39 toks/s]
Processed prompts:  73%|███████▎  | 746/1024 [00:17<00:06, 40.62it/s, est. speed input: 44396.70 toks/s, output: 43.36 toks/s]
Processed prompts:  74%|███████▎  | 754/1024 [00:17<00:06, 40.62it/s, est. speed input: 44364.91 toks/s, output: 43.33 toks/s]
Processed prompts:  74%|███████▍  | 762/1024 [00:17<00:06, 40.62it/s, est. speed input: 44334.15 toks/s, output: 43.30 toks/s]
Processed prompts:  75%|███████▌  | 770/1024 [00:17<00:06, 40.61it/s, est. speed input: 44303.47 toks/s, output: 43.27 toks/s]
Processed prompts:  76%|███████▌  | 778/1024 [00:17<00:06, 40.60it/s, est. speed input: 44273.14 toks/s, output: 43.24 toks/s]
Processed prompts:  77%|███████▋  | 786/1024 [00:18<00:05, 40.57it/s, est. speed input: 44242.75 toks/s, output: 43.21 toks/s]
Processed prompts:  78%|███████▊  | 794/1024 [00:18<00:05, 40.57it/s, est. speed input: 44213.90 toks/s, output: 43.18 toks/s]
Processed prompts:  78%|███████▊  | 802/1024 [00:18<00:05, 40.59it/s, est. speed input: 44186.09 toks/s, output: 43.15 toks/s]
Processed prompts:  79%|███████▉  | 810/1024 [00:18<00:05, 40.58it/s, est. speed input: 44158.20 toks/s, output: 43.12 toks/s]
Processed prompts:  80%|███████▉  | 818/1024 [00:18<00:05, 40.57it/s, est. speed input: 44130.94 toks/s, output: 43.10 toks/s]
Processed prompts:  81%|████████  | 826/1024 [00:19<00:04, 40.55it/s, est. speed input: 44103.54 toks/s, output: 43.07 toks/s]
Processed prompts:  81%|████████▏ | 834/1024 [00:19<00:04, 40.57it/s, est. speed input: 44077.89 toks/s, output: 43.04 toks/s]
Processed prompts:  82%|████████▏ | 842/1024 [00:19<00:04, 40.57it/s, est. speed input: 44052.52 toks/s, output: 43.02 toks/s]
Processed prompts:  83%|████████▎ | 850/1024 [00:19<00:04, 40.58it/s, est. speed input: 44027.63 toks/s, output: 43.00 toks/s]
Processed prompts:  84%|████████▍ | 858/1024 [00:19<00:04, 40.57it/s, est. speed input: 44002.87 toks/s, output: 42.97 toks/s]
Processed prompts:  85%|████████▍ | 866/1024 [00:20<00:03, 40.56it/s, est. speed input: 43978.56 toks/s, output: 42.95 toks/s]
Processed prompts:  85%|████████▌ | 874/1024 [00:20<00:03, 40.56it/s, est. speed input: 43954.99 toks/s, output: 42.92 toks/s]
Processed prompts:  86%|████████▌ | 882/1024 [00:20<00:03, 40.59it/s, est. speed input: 43932.60 toks/s, output: 42.90 toks/s]
Processed prompts:  87%|████████▋ | 890/1024 [00:20<00:03, 40.60it/s, est. speed input: 43910.47 toks/s, output: 42.88 toks/s]
Processed prompts:  88%|████████▊ | 898/1024 [00:20<00:03, 40.60it/s, est. speed input: 43888.53 toks/s, output: 42.86 toks/s]
Processed prompts:  88%|████████▊ | 906/1024 [00:21<00:02, 40.58it/s, est. speed input: 43866.34 toks/s, output: 42.84 toks/s]
Processed prompts:  89%|████████▉ | 914/1024 [00:21<00:02, 40.56it/s, est. speed input: 43844.25 toks/s, output: 42.82 toks/s]
Processed prompts:  90%|█████████ | 922/1024 [00:21<00:02, 40.55it/s, est. speed input: 43822.93 toks/s, output: 42.80 toks/s]
Processed prompts:  91%|█████████ | 930/1024 [00:21<00:02, 40.57it/s, est. speed input: 43802.47 toks/s, output: 42.78 toks/s]
Processed prompts:  92%|█████████▏| 938/1024 [00:21<00:02, 40.55it/s, est. speed input: 43781.66 toks/s, output: 42.76 toks/s]
Processed prompts:  92%|█████████▏| 946/1024 [00:22<00:01, 40.53it/s, est. speed input: 43760.80 toks/s, output: 42.74 toks/s]
Processed prompts:  93%|█████████▎| 954/1024 [00:22<00:01, 40.52it/s, est. speed input: 43740.71 toks/s, output: 42.72 toks/s]
Processed prompts:  94%|█████████▍| 962/1024 [00:22<00:01, 40.55it/s, est. speed input: 43721.85 toks/s, output: 42.70 toks/s]
Processed prompts:  95%|█████████▍| 970/1024 [00:22<00:01, 40.58it/s, est. speed input: 43703.63 toks/s, output: 42.68 toks/s]
Processed prompts:  96%|█████████▌| 978/1024 [00:22<00:01, 40.59it/s, est. speed input: 43685.42 toks/s, output: 42.66 toks/s]
Processed prompts:  96%|█████████▋| 986/1024 [00:23<00:00, 40.56it/s, est. speed input: 43666.46 toks/s, output: 42.64 toks/s]
Processed prompts:  97%|█████████▋| 994/1024 [00:23<00:00, 40.56it/s, est. speed input: 43648.46 toks/s, output: 42.63 toks/s]
Processed prompts:  98%|█████████▊| 1002/1024 [00:23<00:00, 40.58it/s, est. speed input: 43631.19 toks/s, output: 42.61 toks/s]
Processed prompts:  99%|█████████▊| 1010/1024 [00:23<00:00, 40.58it/s, est. speed input: 43613.91 toks/s, output: 42.59 toks/s]
Processed prompts:  99%|█████████▉| 1018/1024 [00:23<00:00, 42.07it/s, est. speed input: 43639.51 toks/s, output: 42.62 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:23<00:00, 42.07it/s, est. speed input: 43896.49 toks/s, output: 42.87 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:23<00:00, 42.87it/s, est. speed input: 43896.49 toks/s, output: 42.87 toks/s]
[rank0]:[W125 19:35:20.498544376 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 50.0s

测试结果:
  Requests/s:   40.55
  Tokens/s:     41566.37
  Total Reqs:   1024
  Elapsed:      25.25s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     41525.82

============================================================
[6/8] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-3B-INT8                                │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 19:35:31 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=382490) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=382490) WARNING 01-25 19:35:40 [backends.py:609] Failed to read file <frozen os>
Throughput: 40.05 requests/s, 41049.98 total tokens/s, 40.05 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-25 19:35:31] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:35:31] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:35:31] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:35:31] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:35:31] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:35:31] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:35:31] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:35:31] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:35:31] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:35:31] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:35:31] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:35:31] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:35:31] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:35:31] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 19:35:35] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:35:35] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:35:35] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:35:35] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:35:35] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:35:35] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:35:35] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:35:35] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:35:35] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:35:35] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:35:35] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:35:35] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:35:35] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:35:35] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=382490) [2026-01-25 19:35:35] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=382490) [2026-01-25 19:35:35] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=382490) [2026-01-25 19:35:35] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=382490) [2026-01-25 19:35:35] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=382490) [2026-01-25 19:35:35] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=382490) [2026-01-25 19:35:35] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=382490) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=382490) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.24it/s]
(EngineCore_DP0 pid=382490) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.23it/s]
(EngineCore_DP0 pid=382490) 
(EngineCore_DP0 pid=382490) [2026-01-25 19:35:36] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 4096] -> 1D uint8
(EngineCore_DP0 pid=382490) [2026-01-25 19:35:36] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15728640 bytes
(EngineCore_DP0 pid=382490) [2026-01-25 19:35:36] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 4096] -> 1D uint8
(EngineCore_DP0 pid=382490) [2026-01-25 19:35:36] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9437184 bytes
(EngineCore_DP0 pid=382490) [2026-01-25 19:35:36] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 4096] -> 1D uint8
(EngineCore_DP0 pid=382490) [2026-01-25 19:35:36] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 50331648 bytes
(EngineCore_DP0 pid=382490) [2026-01-25 19:35:36] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 10944] -> 1D uint8
(EngineCore_DP0 pid=382490) [2026-01-25 19:35:36] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 25264128 bytes
(EngineCore_DP0 pid=382490) 2026-01-25 19:35:46,287 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=382490) 2026-01-25 19:35:46,303 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=382490) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 1/7 [00:00<00:01,  5.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00, 14.77it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 19.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 17.02it/s]
(EngineCore_DP0 pid=382490) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 1/5 [00:00<00:00,  7.56it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 21.90it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 19.66it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   3%|▎         | 71/2048 [00:00<00:02, 704.29it/s]
Adding requests:   7%|▋         | 145/2048 [00:00<00:02, 723.85it/s]
Adding requests:  11%|█         | 218/2048 [00:00<00:02, 725.69it/s]
Adding requests:  14%|█▍        | 293/2048 [00:00<00:02, 733.50it/s]
Adding requests:  18%|█▊        | 367/2048 [00:00<00:02, 731.49it/s]
Adding requests:  22%|██▏       | 441/2048 [00:00<00:02, 728.72it/s]
Adding requests:  25%|██▌       | 516/2048 [00:00<00:02, 735.13it/s]
Adding requests:  29%|██▉       | 593/2048 [00:00<00:01, 743.15it/s]
Adding requests:  33%|███▎      | 670/2048 [00:00<00:01, 749.63it/s]
Adding requests:  37%|███▋      | 748/2048 [00:01<00:01, 757.16it/s]
Adding requests:  40%|████      | 824/2048 [00:01<00:01, 732.86it/s]
Adding requests:  44%|████▍     | 902/2048 [00:01<00:01, 743.98it/s]
Adding requests:  48%|████▊     | 979/2048 [00:01<00:01, 751.51it/s]
Adding requests:  52%|█████▏    | 1057/2048 [00:01<00:01, 759.38it/s]
Adding requests:  55%|█████▌    | 1134/2048 [00:01<00:01, 758.95it/s]
Adding requests:  59%|█████▉    | 1215/2048 [00:01<00:01, 773.39it/s]
Adding requests:  63%|██████▎   | 1293/2048 [00:01<00:00, 761.09it/s]
Adding requests:  67%|██████▋   | 1372/2048 [00:01<00:00, 766.73it/s]
Adding requests:  71%|███████   | 1449/2048 [00:01<00:00, 765.95it/s]
Adding requests:  75%|███████▍  | 1527/2048 [00:02<00:00, 769.69it/s]
Adding requests:  78%|███████▊  | 1606/2048 [00:02<00:00, 773.98it/s]
Adding requests:  82%|████████▏ | 1684/2048 [00:02<00:00, 765.99it/s]
Adding requests:  86%|████████▌ | 1761/2048 [00:02<00:00, 765.40it/s]
Adding requests:  90%|████████▉ | 1839/2048 [00:02<00:00, 769.05it/s]
Adding requests:  94%|█████████▎| 1916/2048 [00:02<00:00, 758.49it/s]
Adding requests:  97%|█████████▋| 1992/2048 [00:02<00:00, 745.12it/s]
Adding requests: 100%|██████████| 2048/2048 [00:02<00:00, 752.75it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   5%|▍         | 98/2048 [00:00<00:03, 581.47it/s, est. speed input: 595466.65 toks/s, output: 581.48 toks/s]
Processed prompts:   8%|▊         | 157/2048 [00:01<00:19, 97.28it/s, est. speed input: 118019.77 toks/s, output: 115.25 toks/s]
Processed prompts:   9%|▉         | 184/2048 [00:02<00:27, 67.87it/s, est. speed input: 87311.81 toks/s, output: 85.27 toks/s]  
Processed prompts:  10%|▉         | 201/2048 [00:02<00:29, 61.59it/s, est. speed input: 80515.84 toks/s, output: 78.63 toks/s]
Processed prompts:  10%|█         | 213/2048 [00:02<00:34, 53.36it/s, est. speed input: 73842.59 toks/s, output: 72.11 toks/s]
Processed prompts:  11%|█         | 226/2048 [00:03<00:38, 47.71it/s, est. speed input: 69045.50 toks/s, output: 67.43 toks/s]
Processed prompts:  12%|█▏        | 242/2048 [00:03<00:39, 45.60it/s, est. speed input: 66081.45 toks/s, output: 64.53 toks/s]
Processed prompts:  13%|█▎        | 258/2048 [00:04<00:40, 44.06it/s, est. speed input: 63690.80 toks/s, output: 62.20 toks/s]
Processed prompts:  13%|█▎        | 274/2048 [00:04<00:41, 42.95it/s, est. speed input: 61723.67 toks/s, output: 60.28 toks/s]
Processed prompts:  14%|█▍        | 290/2048 [00:04<00:41, 42.15it/s, est. speed input: 60070.29 toks/s, output: 58.66 toks/s]
Processed prompts:  15%|█▍        | 306/2048 [00:05<00:41, 41.57it/s, est. speed input: 58659.82 toks/s, output: 57.28 toks/s]
Processed prompts:  16%|█▌        | 322/2048 [00:05<00:42, 41.07it/s, est. speed input: 57417.82 toks/s, output: 56.07 toks/s]
Processed prompts:  17%|█▋        | 338/2048 [00:06<00:41, 40.80it/s, est. speed input: 56359.68 toks/s, output: 55.04 toks/s]
Processed prompts:  17%|█▋        | 354/2048 [00:06<00:41, 40.61it/s, est. speed input: 55433.22 toks/s, output: 54.13 toks/s]
Processed prompts:  18%|█▊        | 370/2048 [00:06<00:41, 40.48it/s, est. speed input: 54612.27 toks/s, output: 53.33 toks/s]
Processed prompts:  19%|█▉        | 386/2048 [00:07<00:41, 40.38it/s, est. speed input: 53879.20 toks/s, output: 52.62 toks/s]
Processed prompts:  20%|█▉        | 402/2048 [00:07<00:40, 40.32it/s, est. speed input: 53223.14 toks/s, output: 51.98 toks/s]
Processed prompts:  20%|██        | 418/2048 [00:08<00:40, 40.28it/s, est. speed input: 52631.09 toks/s, output: 51.40 toks/s]
Processed prompts:  21%|██        | 434/2048 [00:08<00:40, 40.25it/s, est. speed input: 52094.50 toks/s, output: 50.87 toks/s]
Processed prompts:  22%|██▏       | 450/2048 [00:08<00:39, 40.22it/s, est. speed input: 51605.26 toks/s, output: 50.40 toks/s]
Processed prompts:  23%|██▎       | 466/2048 [00:09<00:39, 40.20it/s, est. speed input: 51157.00 toks/s, output: 49.96 toks/s]
Processed prompts:  24%|██▎       | 482/2048 [00:09<00:38, 40.19it/s, est. speed input: 50745.83 toks/s, output: 49.56 toks/s]
Processed prompts:  24%|██▍       | 498/2048 [00:10<00:38, 40.17it/s, est. speed input: 50365.53 toks/s, output: 49.19 toks/s]
Processed prompts:  25%|██▌       | 514/2048 [00:10<00:38, 40.16it/s, est. speed input: 50015.33 toks/s, output: 48.84 toks/s]
Processed prompts:  26%|██▌       | 530/2048 [00:10<00:37, 40.17it/s, est. speed input: 49691.97 toks/s, output: 48.53 toks/s]
Processed prompts:  27%|██▋       | 546/2048 [00:11<00:37, 40.15it/s, est. speed input: 49388.29 toks/s, output: 48.23 toks/s]
Processed prompts:  27%|██▋       | 562/2048 [00:11<00:37, 40.14it/s, est. speed input: 49105.91 toks/s, output: 47.95 toks/s]
Processed prompts:  28%|██▊       | 578/2048 [00:12<00:36, 40.14it/s, est. speed input: 48841.83 toks/s, output: 47.70 toks/s]
Processed prompts:  29%|██▉       | 594/2048 [00:12<00:36, 40.14it/s, est. speed input: 48596.10 toks/s, output: 47.46 toks/s]
Processed prompts:  30%|██▉       | 610/2048 [00:12<00:35, 40.14it/s, est. speed input: 48364.67 toks/s, output: 47.23 toks/s]
Processed prompts:  31%|███       | 626/2048 [00:13<00:35, 40.13it/s, est. speed input: 48146.01 toks/s, output: 47.02 toks/s]
Processed prompts:  31%|███▏      | 642/2048 [00:13<00:35, 40.13it/s, est. speed input: 47941.15 toks/s, output: 46.82 toks/s]
Processed prompts:  32%|███▏      | 658/2048 [00:14<00:34, 40.13it/s, est. speed input: 47747.77 toks/s, output: 46.63 toks/s]
Processed prompts:  33%|███▎      | 674/2048 [00:14<00:34, 40.11it/s, est. speed input: 47562.32 toks/s, output: 46.45 toks/s]
Processed prompts:  34%|███▎      | 690/2048 [00:14<00:33, 40.13it/s, est. speed input: 47391.11 toks/s, output: 46.28 toks/s]
Processed prompts:  34%|███▍      | 706/2048 [00:15<00:33, 40.12it/s, est. speed input: 47226.25 toks/s, output: 46.12 toks/s]
Processed prompts:  35%|███▌      | 722/2048 [00:15<00:33, 40.12it/s, est. speed input: 47070.32 toks/s, output: 45.97 toks/s]
Processed prompts:  36%|███▌      | 738/2048 [00:16<00:32, 40.11it/s, est. speed input: 46921.28 toks/s, output: 45.82 toks/s]
Processed prompts:  37%|███▋      | 754/2048 [00:16<00:32, 40.11it/s, est. speed input: 46779.33 toks/s, output: 45.68 toks/s]
Processed prompts:  38%|███▊      | 770/2048 [00:16<00:31, 40.11it/s, est. speed input: 46644.73 toks/s, output: 45.55 toks/s]
Processed prompts:  38%|███▊      | 786/2048 [00:17<00:31, 40.11it/s, est. speed input: 46516.20 toks/s, output: 45.43 toks/s]
Processed prompts:  39%|███▉      | 802/2048 [00:17<00:31, 40.11it/s, est. speed input: 46393.65 toks/s, output: 45.31 toks/s]
Processed prompts:  40%|███▉      | 818/2048 [00:18<00:30, 40.11it/s, est. speed input: 46276.11 toks/s, output: 45.19 toks/s]
Processed prompts:  41%|████      | 834/2048 [00:18<00:30, 40.10it/s, est. speed input: 46163.38 toks/s, output: 45.08 toks/s]
Processed prompts:  42%|████▏     | 850/2048 [00:18<00:29, 40.10it/s, est. speed input: 46055.61 toks/s, output: 44.98 toks/s]
Processed prompts:  42%|████▏     | 866/2048 [00:19<00:29, 40.09it/s, est. speed input: 45951.64 toks/s, output: 44.87 toks/s]
Processed prompts:  43%|████▎     | 882/2048 [00:19<00:29, 40.09it/s, est. speed input: 45852.53 toks/s, output: 44.78 toks/s]
Processed prompts:  44%|████▍     | 898/2048 [00:20<00:28, 40.10it/s, est. speed input: 45757.59 toks/s, output: 44.69 toks/s]
Processed prompts:  45%|████▍     | 914/2048 [00:20<00:28, 40.09it/s, est. speed input: 45665.36 toks/s, output: 44.60 toks/s]
Processed prompts:  45%|████▌     | 930/2048 [00:20<00:27, 40.10it/s, est. speed input: 45578.08 toks/s, output: 44.51 toks/s]
Processed prompts:  46%|████▌     | 946/2048 [00:21<00:27, 40.10it/s, est. speed input: 45493.21 toks/s, output: 44.43 toks/s]
Processed prompts:  47%|████▋     | 962/2048 [00:21<00:27, 40.10it/s, est. speed input: 45411.76 toks/s, output: 44.35 toks/s]
Processed prompts:  48%|████▊     | 978/2048 [00:22<00:26, 40.10it/s, est. speed input: 45333.19 toks/s, output: 44.27 toks/s]
Processed prompts:  49%|████▊     | 994/2048 [00:22<00:26, 40.09it/s, est. speed input: 45256.97 toks/s, output: 44.20 toks/s]
Processed prompts:  49%|████▉     | 1010/2048 [00:22<00:25, 40.09it/s, est. speed input: 45183.86 toks/s, output: 44.12 toks/s]
Processed prompts:  50%|█████     | 1026/2048 [00:23<00:25, 40.08it/s, est. speed input: 45112.01 toks/s, output: 44.05 toks/s]
Processed prompts:  51%|█████     | 1042/2048 [00:23<00:25, 40.08it/s, est. speed input: 45043.32 toks/s, output: 43.99 toks/s]
Processed prompts:  52%|█████▏    | 1058/2048 [00:24<00:24, 40.09it/s, est. speed input: 44977.54 toks/s, output: 43.92 toks/s]
Processed prompts:  52%|█████▏    | 1074/2048 [00:24<00:24, 40.08it/s, est. speed input: 44913.40 toks/s, output: 43.86 toks/s]
Processed prompts:  53%|█████▎    | 1090/2048 [00:24<00:23, 40.09it/s, est. speed input: 44851.46 toks/s, output: 43.80 toks/s]
Processed prompts:  54%|█████▍    | 1106/2048 [00:25<00:23, 40.07it/s, est. speed input: 44790.76 toks/s, output: 43.74 toks/s]
Processed prompts:  55%|█████▍    | 1122/2048 [00:25<00:23, 40.08it/s, est. speed input: 44732.67 toks/s, output: 43.68 toks/s]
Processed prompts:  56%|█████▌    | 1138/2048 [00:26<00:22, 40.09it/s, est. speed input: 44676.99 toks/s, output: 43.63 toks/s]
Processed prompts:  56%|█████▋    | 1154/2048 [00:26<00:22, 40.07it/s, est. speed input: 44621.14 toks/s, output: 43.58 toks/s]
Processed prompts:  57%|█████▋    | 1170/2048 [00:26<00:21, 40.08it/s, est. speed input: 44568.14 toks/s, output: 43.52 toks/s]
Processed prompts:  58%|█████▊    | 1186/2048 [00:27<00:21, 40.07it/s, est. speed input: 44516.31 toks/s, output: 43.47 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [00:27<00:21, 40.07it/s, est. speed input: 44466.05 toks/s, output: 43.42 toks/s]
Processed prompts:  59%|█████▉    | 1218/2048 [00:28<00:20, 40.07it/s, est. speed input: 44417.04 toks/s, output: 43.38 toks/s]
Processed prompts:  60%|██████    | 1234/2048 [00:28<00:20, 40.06it/s, est. speed input: 44369.08 toks/s, output: 43.33 toks/s]
Processed prompts:  61%|██████    | 1250/2048 [00:28<00:19, 40.08it/s, est. speed input: 44323.58 toks/s, output: 43.28 toks/s]
Processed prompts:  62%|██████▏   | 1266/2048 [00:29<00:19, 40.05it/s, est. speed input: 44277.60 toks/s, output: 43.24 toks/s]
Processed prompts:  63%|██████▎   | 1282/2048 [00:29<00:19, 40.06it/s, est. speed input: 44233.99 toks/s, output: 43.20 toks/s]
Processed prompts:  63%|██████▎   | 1298/2048 [00:30<00:18, 40.06it/s, est. speed input: 44191.59 toks/s, output: 43.16 toks/s]
Processed prompts:  64%|██████▍   | 1314/2048 [00:30<00:18, 40.06it/s, est. speed input: 44149.87 toks/s, output: 43.12 toks/s]
Processed prompts:  65%|██████▍   | 1330/2048 [00:30<00:17, 40.06it/s, est. speed input: 44109.37 toks/s, output: 43.08 toks/s]
Processed prompts:  66%|██████▌   | 1346/2048 [00:31<00:17, 40.04it/s, est. speed input: 44069.09 toks/s, output: 43.04 toks/s]
Processed prompts:  67%|██████▋   | 1362/2048 [00:31<00:17, 40.06it/s, est. speed input: 44031.13 toks/s, output: 43.00 toks/s]
Processed prompts:  67%|██████▋   | 1378/2048 [00:32<00:16, 40.06it/s, est. speed input: 43993.59 toks/s, output: 42.96 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [00:32<00:16, 40.05it/s, est. speed input: 43956.70 toks/s, output: 42.93 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [00:32<00:15, 40.05it/s, est. speed input: 43921.06 toks/s, output: 42.89 toks/s]
Processed prompts:  70%|██████▉   | 1426/2048 [00:33<00:15, 40.05it/s, est. speed input: 43885.83 toks/s, output: 42.86 toks/s]
Processed prompts:  70%|███████   | 1442/2048 [00:33<00:15, 40.05it/s, est. speed input: 43851.83 toks/s, output: 42.82 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [00:34<00:14, 40.05it/s, est. speed input: 43818.59 toks/s, output: 42.79 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [00:34<00:14, 40.04it/s, est. speed input: 43785.77 toks/s, output: 42.76 toks/s]
Processed prompts:  73%|███████▎  | 1490/2048 [00:34<00:13, 40.05it/s, est. speed input: 43754.23 toks/s, output: 42.73 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [00:35<00:13, 40.04it/s, est. speed input: 43722.56 toks/s, output: 42.70 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [00:35<00:13, 40.04it/s, est. speed input: 43692.17 toks/s, output: 42.67 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [00:36<00:12, 40.05it/s, est. speed input: 43662.64 toks/s, output: 42.64 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [00:36<00:12, 40.04it/s, est. speed input: 43633.45 toks/s, output: 42.61 toks/s]
Processed prompts:  77%|███████▋  | 1570/2048 [00:36<00:11, 40.05it/s, est. speed input: 43605.02 toks/s, output: 42.58 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [00:37<00:11, 40.04it/s, est. speed input: 43576.82 toks/s, output: 42.56 toks/s]
Processed prompts:  78%|███████▊  | 1602/2048 [00:37<00:11, 40.03it/s, est. speed input: 43549.10 toks/s, output: 42.53 toks/s]
Processed prompts:  79%|███████▉  | 1618/2048 [00:38<00:10, 40.03it/s, est. speed input: 43522.49 toks/s, output: 42.50 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [00:38<00:10, 40.03it/s, est. speed input: 43496.06 toks/s, output: 42.48 toks/s]
Processed prompts:  81%|████████  | 1650/2048 [00:38<00:09, 40.04it/s, est. speed input: 43470.70 toks/s, output: 42.45 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [00:39<00:09, 40.04it/s, est. speed input: 43445.41 toks/s, output: 42.43 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [00:39<00:09, 40.04it/s, est. speed input: 43420.84 toks/s, output: 42.40 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [00:40<00:08, 40.03it/s, est. speed input: 43396.50 toks/s, output: 42.38 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [00:40<00:08, 40.03it/s, est. speed input: 43372.81 toks/s, output: 42.36 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [00:40<00:07, 40.04it/s, est. speed input: 43349.83 toks/s, output: 42.33 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [00:41<00:07, 40.04it/s, est. speed input: 43327.02 toks/s, output: 42.31 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [00:41<00:07, 40.04it/s, est. speed input: 43304.58 toks/s, output: 42.29 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [00:42<00:06, 40.03it/s, est. speed input: 43282.60 toks/s, output: 42.27 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [00:42<00:06, 40.03it/s, est. speed input: 43260.90 toks/s, output: 42.25 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [00:42<00:05, 40.02it/s, est. speed input: 43239.52 toks/s, output: 42.23 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [00:43<00:05, 40.03it/s, est. speed input: 43218.78 toks/s, output: 42.21 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [00:43<00:05, 40.03it/s, est. speed input: 43198.37 toks/s, output: 42.19 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [00:44<00:04, 40.03it/s, est. speed input: 43178.34 toks/s, output: 42.17 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [00:44<00:04, 40.68it/s, est. speed input: 43179.53 toks/s, output: 42.17 toks/s]
Processed prompts:  92%|█████████▏| 1890/2048 [00:44<00:03, 40.49it/s, est. speed input: 43160.25 toks/s, output: 42.15 toks/s]
Processed prompts:  93%|█████████▎| 1906/2048 [00:45<00:03, 40.34it/s, est. speed input: 43140.81 toks/s, output: 42.13 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [00:45<00:03, 40.25it/s, est. speed input: 43121.92 toks/s, output: 42.11 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [00:46<00:02, 40.18it/s, est. speed input: 43103.30 toks/s, output: 42.09 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [00:46<00:02, 40.13it/s, est. speed input: 43085.03 toks/s, output: 42.08 toks/s]
Processed prompts:  96%|█████████▌| 1970/2048 [00:46<00:01, 40.11it/s, est. speed input: 43067.44 toks/s, output: 42.06 toks/s]
Processed prompts:  97%|█████████▋| 1986/2048 [00:47<00:01, 40.08it/s, est. speed input: 43049.59 toks/s, output: 42.04 toks/s]
Processed prompts:  98%|█████████▊| 2002/2048 [00:47<00:01, 40.06it/s, est. speed input: 43032.35 toks/s, output: 42.02 toks/s]
Processed prompts:  99%|█████████▊| 2018/2048 [00:48<00:00, 40.05it/s, est. speed input: 43015.24 toks/s, output: 42.01 toks/s]
Processed prompts:  99%|█████████▉| 2034/2048 [00:48<00:00, 40.76it/s, est. speed input: 43019.40 toks/s, output: 42.01 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:48<00:00, 40.76it/s, est. speed input: 43315.34 toks/s, output: 42.30 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:48<00:00, 42.30it/s, est. speed input: 43315.34 toks/s, output: 42.30 toks/s]
[rank0]:[W125 19:36:39.013425212 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 78.7s

测试结果:
  Requests/s:   40.05
  Tokens/s:     41049.98
  Total Reqs:   2048
  Elapsed:      51.14s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     41009.93

============================================================
[7/8] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-3B-INT8                                │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 19:36:55 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=383925) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=383925) WARNING 01-25 19:37:05 [backends.py:609] Failed to read file <frozen os>
Throughput: 40.12 requests/s, 41123.46 total tokens/s, 40.12 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-25 19:36:55] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:36:55] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:36:55] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:36:55] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:36:55] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:36:55] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:36:55] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:36:55] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:36:55] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:36:55] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:36:55] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:36:55] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:36:55] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:36:55] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 19:36:59] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:36:59] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:36:59] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:36:59] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:36:59] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:36:59] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:36:59] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:36:59] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:36:59] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:36:59] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:36:59] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:36:59] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:36:59] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:36:59] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=383925) [2026-01-25 19:37:00] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=383925) [2026-01-25 19:37:00] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=383925) [2026-01-25 19:37:00] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=383925) [2026-01-25 19:37:00] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=383925) [2026-01-25 19:37:00] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=383925) [2026-01-25 19:37:00] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=383925) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=383925) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.25it/s]
(EngineCore_DP0 pid=383925) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.25it/s]
(EngineCore_DP0 pid=383925) 
(EngineCore_DP0 pid=383925) [2026-01-25 19:37:01] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 4096] -> 1D uint8
(EngineCore_DP0 pid=383925) [2026-01-25 19:37:01] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15728640 bytes
(EngineCore_DP0 pid=383925) [2026-01-25 19:37:01] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 4096] -> 1D uint8
(EngineCore_DP0 pid=383925) [2026-01-25 19:37:01] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9437184 bytes
(EngineCore_DP0 pid=383925) [2026-01-25 19:37:01] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 4096] -> 1D uint8
(EngineCore_DP0 pid=383925) [2026-01-25 19:37:01] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 50331648 bytes
(EngineCore_DP0 pid=383925) [2026-01-25 19:37:01] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 10944] -> 1D uint8
(EngineCore_DP0 pid=383925) [2026-01-25 19:37:01] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 25264128 bytes
(EngineCore_DP0 pid=383925) [rank0]:W0125 19:37:07.784000 383925 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=383925) [rank0]:W0125 19:37:07.834000 383925 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=383925) [rank0]:W0125 19:37:08.466000 383925 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=383925) [rank0]:W0125 19:37:08.538000 383925 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=383925) 2026-01-25 19:37:10,776 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=383925) 2026-01-25 19:37:10,792 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=383925) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▉         | 1/11 [00:00<00:05,  1.92it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:00<00:00,  7.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▎   | 7/11 [00:00<00:00, 12.44it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 17.39it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 12.33it/s]
(EngineCore_DP0 pid=383925) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 1/7 [00:00<00:00,  7.45it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00, 16.39it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 20.09it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 18.12it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 72/4096 [00:00<00:05, 716.96it/s]
Adding requests:   4%|▎         | 149/4096 [00:00<00:05, 743.74it/s]
Adding requests:   5%|▌         | 225/4096 [00:00<00:05, 749.36it/s]
Adding requests:   7%|▋         | 301/4096 [00:00<00:05, 753.03it/s]
Adding requests:   9%|▉         | 379/4096 [00:00<00:04, 759.90it/s]
Adding requests:  11%|█         | 455/4096 [00:00<00:04, 756.41it/s]
Adding requests:  13%|█▎        | 531/4096 [00:00<00:04, 740.55it/s]
Adding requests:  15%|█▍        | 609/4096 [00:00<00:04, 751.42it/s]
Adding requests:  17%|█▋        | 689/4096 [00:00<00:04, 764.18it/s]
Adding requests:  19%|█▊        | 766/4096 [00:01<00:04, 765.13it/s]
Adding requests:  21%|██        | 843/4096 [00:01<00:04, 744.96it/s]
Adding requests:  22%|██▏       | 918/4096 [00:01<00:04, 742.31it/s]
Adding requests:  24%|██▍       | 994/4096 [00:01<00:04, 746.82it/s]
Adding requests:  26%|██▌       | 1072/4096 [00:01<00:04, 754.35it/s]
Adding requests:  28%|██▊       | 1148/4096 [00:01<00:03, 742.17it/s]
Adding requests:  30%|██▉       | 1227/4096 [00:01<00:03, 754.85it/s]
Adding requests:  32%|███▏      | 1303/4096 [00:01<00:03, 752.33it/s]
Adding requests:  34%|███▎      | 1382/4096 [00:01<00:03, 761.47it/s]
Adding requests:  36%|███▌      | 1459/4096 [00:01<00:03, 761.39it/s]
Adding requests:  38%|███▊      | 1538/4096 [00:02<00:03, 767.27it/s]
Adding requests:  40%|███▉      | 1618/4096 [00:02<00:03, 774.50it/s]
Adding requests:  41%|████▏     | 1696/4096 [00:02<00:03, 769.99it/s]
Adding requests:  43%|████▎     | 1774/4096 [00:02<00:03, 767.56it/s]
Adding requests:  45%|████▌     | 1851/4096 [00:02<00:02, 765.25it/s]
Adding requests:  47%|████▋     | 1928/4096 [00:02<00:02, 758.58it/s]
Adding requests:  49%|████▉     | 2007/4096 [00:02<00:02, 765.09it/s]
Adding requests:  51%|█████     | 2085/4096 [00:02<00:02, 767.51it/s]
Adding requests:  53%|█████▎    | 2162/4096 [00:02<00:02, 758.49it/s]
Adding requests:  55%|█████▍    | 2241/4096 [00:02<00:02, 766.03it/s]
Adding requests:  57%|█████▋    | 2318/4096 [00:03<00:02, 760.81it/s]
Adding requests:  58%|█████▊    | 2395/4096 [00:03<00:02, 754.52it/s]
Adding requests:  60%|██████    | 2471/4096 [00:03<00:02, 754.97it/s]
Adding requests:  62%|██████▏   | 2548/4096 [00:03<00:02, 758.17it/s]
Adding requests:  64%|██████▍   | 2627/4096 [00:03<00:01, 766.17it/s]
Adding requests:  66%|██████▌   | 2705/4096 [00:03<00:01, 769.34it/s]
Adding requests:  68%|██████▊   | 2783/4096 [00:03<00:01, 770.81it/s]
Adding requests:  70%|██████▉   | 2861/4096 [00:03<00:01, 771.51it/s]
Adding requests:  72%|███████▏  | 2939/4096 [00:03<00:01, 768.48it/s]
Adding requests:  74%|███████▎  | 3016/4096 [00:03<00:01, 768.14it/s]
Adding requests:  76%|███████▌  | 3094/4096 [00:04<00:01, 770.26it/s]
Adding requests:  77%|███████▋  | 3172/4096 [00:04<00:01, 771.84it/s]
Adding requests:  79%|███████▉  | 3252/4096 [00:04<00:01, 778.93it/s]
Adding requests:  81%|████████▏ | 3330/4096 [00:04<00:00, 778.38it/s]
Adding requests:  83%|████████▎ | 3409/4096 [00:04<00:00, 780.90it/s]
Adding requests:  85%|████████▌ | 3488/4096 [00:04<00:00, 771.11it/s]
Adding requests:  87%|████████▋ | 3567/4096 [00:04<00:00, 776.54it/s]
Adding requests:  89%|████████▉ | 3645/4096 [00:04<00:00, 769.17it/s]
Adding requests:  91%|█████████ | 3722/4096 [00:04<00:00, 751.41it/s]
Adding requests:  93%|█████████▎| 3801/4096 [00:04<00:00, 760.80it/s]
Adding requests:  95%|█████████▍| 3881/4096 [00:05<00:00, 767.01it/s]
Adding requests:  97%|█████████▋| 3959/4096 [00:05<00:00, 769.14it/s]
Adding requests:  99%|█████████▊| 4036/4096 [00:05<00:00, 760.17it/s]
Adding requests: 100%|██████████| 4096/4096 [00:05<00:00, 761.88it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   5%|▍         | 194/4096 [00:00<00:05, 740.49it/s, est. speed input: 758309.50 toks/s, output: 740.51 toks/s]
Processed prompts:   7%|▋         | 269/4096 [00:01<00:32, 118.91it/s, est. speed input: 148790.75 toks/s, output: 145.30 toks/s]
Processed prompts:   7%|▋         | 303/4096 [00:02<00:42, 89.21it/s, est. speed input: 117280.79 toks/s, output: 114.53 toks/s] 
Processed prompts:   8%|▊         | 324/4096 [00:03<00:56, 66.71it/s, est. speed input: 96424.26 toks/s, output: 94.16 toks/s]  
Processed prompts:   9%|▊         | 354/4096 [00:04<01:05, 56.91it/s, est. speed input: 85584.31 toks/s, output: 83.58 toks/s]
Processed prompts:   9%|▉         | 386/4096 [00:05<01:12, 51.48it/s, est. speed input: 78572.78 toks/s, output: 76.73 toks/s]
Processed prompts:  10%|█         | 418/4096 [00:05<01:16, 47.91it/s, est. speed input: 73471.09 toks/s, output: 71.75 toks/s]
Processed prompts:  11%|█         | 450/4096 [00:06<01:20, 45.51it/s, est. speed input: 69591.58 toks/s, output: 67.96 toks/s]
Processed prompts:  12%|█▏        | 482/4096 [00:07<01:22, 43.87it/s, est. speed input: 66544.07 toks/s, output: 64.98 toks/s]
Processed prompts:  13%|█▎        | 514/4096 [00:08<01:23, 42.77it/s, est. speed input: 64091.56 toks/s, output: 62.59 toks/s]
Processed prompts:  13%|█▎        | 546/4096 [00:09<01:24, 41.99it/s, est. speed input: 62068.54 toks/s, output: 60.61 toks/s]
Processed prompts:  14%|█▍        | 578/4096 [00:09<01:24, 41.45it/s, est. speed input: 60371.09 toks/s, output: 58.96 toks/s]
Processed prompts:  15%|█▍        | 610/4096 [00:10<01:24, 41.07it/s, est. speed input: 58930.13 toks/s, output: 57.55 toks/s]
Processed prompts:  16%|█▌        | 642/4096 [00:11<01:24, 40.82it/s, est. speed input: 57691.75 toks/s, output: 56.34 toks/s]
Processed prompts:  16%|█▋        | 674/4096 [00:12<01:24, 40.63it/s, est. speed input: 56612.36 toks/s, output: 55.29 toks/s]
Processed prompts:  17%|█▋        | 706/4096 [00:12<01:23, 40.50it/s, est. speed input: 55666.07 toks/s, output: 54.36 toks/s]
Processed prompts:  18%|█▊        | 738/4096 [00:13<01:23, 40.41it/s, est. speed input: 54828.51 toks/s, output: 53.54 toks/s]
Processed prompts:  19%|█▉        | 770/4096 [00:14<01:22, 40.34it/s, est. speed input: 54080.06 toks/s, output: 52.81 toks/s]
Processed prompts:  20%|█▉        | 802/4096 [00:15<01:21, 40.29it/s, est. speed input: 53410.11 toks/s, output: 52.16 toks/s]
Processed prompts:  20%|██        | 834/4096 [00:16<01:21, 40.26it/s, est. speed input: 52807.00 toks/s, output: 51.57 toks/s]
Processed prompts:  21%|██        | 866/4096 [00:16<01:20, 40.24it/s, est. speed input: 52259.47 toks/s, output: 51.03 toks/s]
Processed prompts:  22%|██▏       | 898/4096 [00:17<01:19, 40.22it/s, est. speed input: 51761.11 toks/s, output: 50.55 toks/s]
Processed prompts:  23%|██▎       | 930/4096 [00:18<01:18, 40.20it/s, est. speed input: 51305.05 toks/s, output: 50.10 toks/s]
Processed prompts:  23%|██▎       | 962/4096 [00:19<01:17, 40.20it/s, est. speed input: 50887.70 toks/s, output: 49.69 toks/s]
Processed prompts:  24%|██▍       | 994/4096 [00:20<01:17, 40.20it/s, est. speed input: 50503.13 toks/s, output: 49.32 toks/s]
Processed prompts:  25%|██▌       | 1026/4096 [00:20<01:16, 40.19it/s, est. speed input: 50146.69 toks/s, output: 48.97 toks/s]
Processed prompts:  26%|██▌       | 1058/4096 [00:21<01:15, 40.18it/s, est. speed input: 49815.87 toks/s, output: 48.65 toks/s]
Processed prompts:  27%|██▋       | 1090/4096 [00:22<01:14, 40.17it/s, est. speed input: 49508.85 toks/s, output: 48.35 toks/s]
Processed prompts:  27%|██▋       | 1122/4096 [00:23<01:14, 40.17it/s, est. speed input: 49223.18 toks/s, output: 48.07 toks/s]
Processed prompts:  28%|██▊       | 1154/4096 [00:24<01:13, 40.17it/s, est. speed input: 48955.25 toks/s, output: 47.81 toks/s]
Processed prompts:  29%|██▉       | 1186/4096 [00:24<01:12, 40.16it/s, est. speed input: 48704.38 toks/s, output: 47.56 toks/s]
Processed prompts:  30%|██▉       | 1218/4096 [00:25<01:11, 40.16it/s, est. speed input: 48469.28 toks/s, output: 47.33 toks/s]
Processed prompts:  31%|███       | 1250/4096 [00:26<01:10, 40.12it/s, est. speed input: 48244.39 toks/s, output: 47.11 toks/s]
Processed prompts:  31%|███▏      | 1282/4096 [00:27<01:10, 40.13it/s, est. speed input: 48036.79 toks/s, output: 46.91 toks/s]
Processed prompts:  32%|███▏      | 1314/4096 [00:28<01:09, 40.14it/s, est. speed input: 47840.41 toks/s, output: 46.72 toks/s]
Processed prompts:  33%|███▎      | 1346/4096 [00:28<01:08, 40.14it/s, est. speed input: 47654.70 toks/s, output: 46.54 toks/s]
Processed prompts:  34%|███▎      | 1378/4096 [00:29<01:07, 40.14it/s, est. speed input: 47479.09 toks/s, output: 46.37 toks/s]
Processed prompts:  34%|███▍      | 1410/4096 [00:30<01:06, 40.14it/s, est. speed input: 47312.73 toks/s, output: 46.20 toks/s]
Processed prompts:  35%|███▌      | 1442/4096 [00:31<01:06, 40.14it/s, est. speed input: 47154.54 toks/s, output: 46.05 toks/s]
Processed prompts:  36%|███▌      | 1474/4096 [00:32<01:05, 40.14it/s, est. speed input: 47004.37 toks/s, output: 45.90 toks/s]
Processed prompts:  37%|███▋      | 1506/4096 [00:32<01:04, 40.13it/s, est. speed input: 46860.84 toks/s, output: 45.76 toks/s]
Processed prompts:  38%|███▊      | 1538/4096 [00:33<01:03, 40.13it/s, est. speed input: 46724.30 toks/s, output: 45.63 toks/s]
Processed prompts:  38%|███▊      | 1570/4096 [00:34<01:02, 40.14it/s, est. speed input: 46594.65 toks/s, output: 45.50 toks/s]
Processed prompts:  39%|███▉      | 1602/4096 [00:35<01:02, 40.15it/s, est. speed input: 46471.46 toks/s, output: 45.38 toks/s]
Processed prompts:  40%|███▉      | 1634/4096 [00:36<01:01, 40.13it/s, est. speed input: 46351.40 toks/s, output: 45.27 toks/s]
Processed prompts:  41%|████      | 1666/4096 [00:36<01:00, 40.13it/s, est. speed input: 46237.96 toks/s, output: 45.15 toks/s]
Processed prompts:  41%|████▏     | 1698/4096 [00:37<00:59, 40.13it/s, est. speed input: 46129.03 toks/s, output: 45.05 toks/s]
Processed prompts:  42%|████▏     | 1730/4096 [00:38<00:58, 40.12it/s, est. speed input: 46024.33 toks/s, output: 44.95 toks/s]
Processed prompts:  43%|████▎     | 1762/4096 [00:39<00:58, 40.13it/s, est. speed input: 45924.21 toks/s, output: 44.85 toks/s]
Processed prompts:  44%|████▍     | 1794/4096 [00:40<00:57, 40.12it/s, est. speed input: 45827.60 toks/s, output: 44.75 toks/s]
Processed prompts:  45%|████▍     | 1826/4096 [00:40<00:56, 40.12it/s, est. speed input: 45734.91 toks/s, output: 44.66 toks/s]
Processed prompts:  45%|████▌     | 1858/4096 [00:41<00:55, 40.46it/s, est. speed input: 45670.10 toks/s, output: 44.60 toks/s]
Processed prompts:  46%|████▌     | 1890/4096 [00:42<00:54, 40.35it/s, est. speed input: 45583.53 toks/s, output: 44.52 toks/s]
Processed prompts:  47%|████▋     | 1922/4096 [00:43<00:53, 40.27it/s, est. speed input: 45500.18 toks/s, output: 44.43 toks/s]
Processed prompts:  48%|████▊     | 1954/4096 [00:44<00:53, 40.23it/s, est. speed input: 45420.19 toks/s, output: 44.36 toks/s]
Processed prompts:  48%|████▊     | 1986/4096 [00:44<00:52, 40.19it/s, est. speed input: 45342.65 toks/s, output: 44.28 toks/s]
Processed prompts:  49%|████▉     | 2018/4096 [00:45<00:51, 40.17it/s, est. speed input: 45268.16 toks/s, output: 44.21 toks/s]
Processed prompts:  50%|█████     | 2050/4096 [00:46<00:50, 40.15it/s, est. speed input: 45196.03 toks/s, output: 44.14 toks/s]
Processed prompts:  51%|█████     | 2082/4096 [00:47<00:50, 40.14it/s, est. speed input: 45126.49 toks/s, output: 44.07 toks/s]
Processed prompts:  52%|█████▏    | 2114/4096 [00:48<00:49, 40.13it/s, est. speed input: 45059.05 toks/s, output: 44.00 toks/s]
Processed prompts:  52%|█████▏    | 2146/4096 [00:48<00:48, 40.12it/s, est. speed input: 44993.51 toks/s, output: 43.94 toks/s]
Processed prompts:  53%|█████▎    | 2178/4096 [00:49<00:47, 40.11it/s, est. speed input: 44930.35 toks/s, output: 43.88 toks/s]
Processed prompts:  54%|█████▍    | 2210/4096 [00:50<00:47, 40.10it/s, est. speed input: 44868.79 toks/s, output: 43.82 toks/s]
Processed prompts:  55%|█████▍    | 2242/4096 [00:51<00:46, 40.11it/s, est. speed input: 44809.79 toks/s, output: 43.76 toks/s]
Processed prompts:  56%|█████▌    | 2274/4096 [00:52<00:45, 40.10it/s, est. speed input: 44751.99 toks/s, output: 43.70 toks/s]
Processed prompts:  56%|█████▋    | 2306/4096 [00:52<00:44, 40.10it/s, est. speed input: 44696.40 toks/s, output: 43.65 toks/s]
Processed prompts:  57%|█████▋    | 2338/4096 [00:53<00:43, 40.10it/s, est. speed input: 44642.11 toks/s, output: 43.60 toks/s]
Processed prompts:  58%|█████▊    | 2370/4096 [00:54<00:43, 40.10it/s, est. speed input: 44589.55 toks/s, output: 43.54 toks/s]
Processed prompts:  59%|█████▊    | 2402/4096 [00:55<00:42, 40.10it/s, est. speed input: 44538.63 toks/s, output: 43.49 toks/s]
Processed prompts:  59%|█████▉    | 2434/4096 [00:56<00:41, 40.10it/s, est. speed input: 44489.13 toks/s, output: 43.45 toks/s]
Processed prompts:  60%|██████    | 2466/4096 [00:56<00:40, 40.09it/s, est. speed input: 44440.77 toks/s, output: 43.40 toks/s]
Processed prompts:  61%|██████    | 2498/4096 [00:57<00:39, 40.09it/s, est. speed input: 44393.61 toks/s, output: 43.35 toks/s]
Processed prompts:  62%|██████▏   | 2530/4096 [00:58<00:39, 40.09it/s, est. speed input: 44348.11 toks/s, output: 43.31 toks/s]
Processed prompts:  63%|██████▎   | 2562/4096 [00:59<00:38, 40.10it/s, est. speed input: 44303.97 toks/s, output: 43.27 toks/s]
Processed prompts:  63%|██████▎   | 2594/4096 [01:00<00:37, 40.09it/s, est. speed input: 44260.59 toks/s, output: 43.22 toks/s]
Processed prompts:  64%|██████▍   | 2626/4096 [01:00<00:36, 40.09it/s, est. speed input: 44218.25 toks/s, output: 43.18 toks/s]
Processed prompts:  65%|██████▍   | 2658/4096 [01:01<00:35, 40.09it/s, est. speed input: 44177.11 toks/s, output: 43.14 toks/s]
Processed prompts:  66%|██████▌   | 2690/4096 [01:02<00:35, 40.08it/s, est. speed input: 44136.96 toks/s, output: 43.10 toks/s]
Processed prompts:  66%|██████▋   | 2722/4096 [01:03<00:34, 40.09it/s, est. speed input: 44098.06 toks/s, output: 43.06 toks/s]
Processed prompts:  67%|██████▋   | 2754/4096 [01:04<00:33, 40.08it/s, est. speed input: 44059.92 toks/s, output: 43.03 toks/s]
Processed prompts:  68%|██████▊   | 2786/4096 [01:04<00:32, 40.08it/s, est. speed input: 44022.62 toks/s, output: 42.99 toks/s]
Processed prompts:  69%|██████▉   | 2818/4096 [01:05<00:31, 40.08it/s, est. speed input: 43986.25 toks/s, output: 42.96 toks/s]
Processed prompts:  70%|██████▉   | 2850/4096 [01:06<00:31, 40.07it/s, est. speed input: 43950.63 toks/s, output: 42.92 toks/s]
Processed prompts:  70%|███████   | 2882/4096 [01:07<00:30, 40.08it/s, est. speed input: 43916.05 toks/s, output: 42.89 toks/s]
Processed prompts:  71%|███████   | 2914/4096 [01:07<00:29, 40.07it/s, est. speed input: 43882.13 toks/s, output: 42.85 toks/s]
Processed prompts:  72%|███████▏  | 2946/4096 [01:08<00:28, 40.08it/s, est. speed input: 43849.21 toks/s, output: 42.82 toks/s]
Processed prompts:  73%|███████▎  | 2978/4096 [01:09<00:27, 40.07it/s, est. speed input: 43816.65 toks/s, output: 42.79 toks/s]
Processed prompts:  73%|███████▎  | 3010/4096 [01:10<00:27, 40.07it/s, est. speed input: 43785.02 toks/s, output: 42.76 toks/s]
Processed prompts:  74%|███████▍  | 3042/4096 [01:11<00:26, 40.08it/s, est. speed input: 43754.42 toks/s, output: 42.73 toks/s]
Processed prompts:  75%|███████▌  | 3074/4096 [01:11<00:25, 40.07it/s, est. speed input: 43724.06 toks/s, output: 42.70 toks/s]
Processed prompts:  76%|███████▌  | 3106/4096 [01:12<00:24, 40.07it/s, est. speed input: 43694.35 toks/s, output: 42.67 toks/s]
Processed prompts:  77%|███████▋  | 3138/4096 [01:13<00:23, 40.06it/s, est. speed input: 43665.30 toks/s, output: 42.64 toks/s]
Processed prompts:  77%|███████▋  | 3170/4096 [01:14<00:23, 40.06it/s, est. speed input: 43636.94 toks/s, output: 42.61 toks/s]
Processed prompts:  78%|███████▊  | 3202/4096 [01:15<00:22, 40.06it/s, est. speed input: 43609.09 toks/s, output: 42.59 toks/s]
Processed prompts:  79%|███████▉  | 3234/4096 [01:15<00:21, 40.06it/s, est. speed input: 43581.83 toks/s, output: 42.56 toks/s]
Processed prompts:  80%|███████▉  | 3266/4096 [01:16<00:20, 40.06it/s, est. speed input: 43555.20 toks/s, output: 42.53 toks/s]
Processed prompts:  81%|████████  | 3298/4096 [01:17<00:19, 40.06it/s, est. speed input: 43529.09 toks/s, output: 42.51 toks/s]
Processed prompts:  81%|████████▏ | 3330/4096 [01:18<00:19, 40.06it/s, est. speed input: 43503.45 toks/s, output: 42.48 toks/s]
Processed prompts:  82%|████████▏ | 3362/4096 [01:19<00:18, 40.06it/s, est. speed input: 43478.46 toks/s, output: 42.46 toks/s]
Processed prompts:  83%|████████▎ | 3394/4096 [01:19<00:17, 40.06it/s, est. speed input: 43453.91 toks/s, output: 42.44 toks/s]
Processed prompts:  84%|████████▎ | 3426/4096 [01:20<00:16, 40.06it/s, est. speed input: 43429.77 toks/s, output: 42.41 toks/s]
Processed prompts:  84%|████████▍ | 3458/4096 [01:21<00:15, 40.05it/s, est. speed input: 43406.00 toks/s, output: 42.39 toks/s]
Processed prompts:  85%|████████▌ | 3490/4096 [01:22<00:15, 40.06it/s, est. speed input: 43382.90 toks/s, output: 42.37 toks/s]
Processed prompts:  86%|████████▌ | 3522/4096 [01:23<00:14, 40.06it/s, est. speed input: 43360.19 toks/s, output: 42.34 toks/s]
Processed prompts:  87%|████████▋ | 3554/4096 [01:23<00:13, 40.06it/s, est. speed input: 43337.90 toks/s, output: 42.32 toks/s]
Processed prompts:  88%|████████▊ | 3586/4096 [01:24<00:12, 40.05it/s, est. speed input: 43315.84 toks/s, output: 42.30 toks/s]
Processed prompts:  88%|████████▊ | 3618/4096 [01:25<00:11, 40.05it/s, est. speed input: 43294.31 toks/s, output: 42.28 toks/s]
Processed prompts:  89%|████████▉ | 3650/4096 [01:26<00:11, 40.04it/s, est. speed input: 43272.96 toks/s, output: 42.26 toks/s]
Processed prompts:  90%|████████▉ | 3682/4096 [01:27<00:10, 40.05it/s, est. speed input: 43252.24 toks/s, output: 42.24 toks/s]
Processed prompts:  91%|█████████ | 3714/4096 [01:27<00:09, 40.04it/s, est. speed input: 43231.73 toks/s, output: 42.22 toks/s]
Processed prompts:  91%|█████████▏| 3746/4096 [01:28<00:08, 40.04it/s, est. speed input: 43211.69 toks/s, output: 42.20 toks/s]
Processed prompts:  92%|█████████▏| 3778/4096 [01:29<00:07, 40.04it/s, est. speed input: 43191.91 toks/s, output: 42.18 toks/s]
Processed prompts:  93%|█████████▎| 3810/4096 [01:30<00:07, 40.03it/s, est. speed input: 43172.30 toks/s, output: 42.16 toks/s]
Processed prompts:  94%|█████████▍| 3842/4096 [01:31<00:06, 40.04it/s, est. speed input: 43153.57 toks/s, output: 42.14 toks/s]
Processed prompts:  95%|█████████▍| 3874/4096 [01:31<00:05, 40.04it/s, est. speed input: 43134.91 toks/s, output: 42.12 toks/s]
Processed prompts:  95%|█████████▌| 3906/4096 [01:32<00:04, 40.04it/s, est. speed input: 43116.49 toks/s, output: 42.11 toks/s]
Processed prompts:  96%|█████████▌| 3938/4096 [01:33<00:03, 40.04it/s, est. speed input: 43098.44 toks/s, output: 42.09 toks/s]
Processed prompts:  97%|█████████▋| 3970/4096 [01:34<00:03, 40.04it/s, est. speed input: 43080.76 toks/s, output: 42.07 toks/s]
Processed prompts:  98%|█████████▊| 4002/4096 [01:35<00:02, 40.04it/s, est. speed input: 43063.34 toks/s, output: 42.05 toks/s]
Processed prompts:  98%|█████████▊| 4034/4096 [01:35<00:01, 40.39it/s, est. speed input: 43056.47 toks/s, output: 42.05 toks/s]
Processed prompts:  99%|█████████▉| 4066/4096 [01:36<00:00, 40.66it/s, est. speed input: 43050.23 toks/s, output: 42.04 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [01:36<00:00, 40.66it/s, est. speed input: 43367.62 toks/s, output: 42.35 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [01:36<00:00, 42.35it/s, est. speed input: 43367.62 toks/s, output: 42.35 toks/s]
[rank0]:[W125 19:38:55.115518319 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 135.9s

测试结果:
  Requests/s:   40.12
  Tokens/s:     41123.46
  Total Reqs:   4096
  Elapsed:      102.09s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     41083.34

============================================================
[8/8] 测试 M=65536
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-3B-INT8                                │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 65536
│   M_prefill     = 65536 (= 64 x 1024)
│   M_decode      = 64
│   batched_tokens = 65536 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 8192
│   --max-num-seqs           = 64
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 65536
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 19:39:22 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=386226) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=386226) WARNING 01-25 19:39:32 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 256, in _initialize_kv_caches
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]     self.model_executor.initialize_from_config(kv_cache_configs)
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 116, in initialize_from_config
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]     self.collective_rpc("compile_or_warm_up_model")
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 455, in compile_or_warm_up_model
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]     kernel_warmup(self)
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]   File "/root/vllmbench/vllm/model_executor/warmup/kernel_warmup.py", line 41, in kernel_warmup
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]     flashinfer_autotune(worker.model_runner)
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]   File "/root/vllmbench/vllm/model_executor/warmup/kernel_warmup.py", line 94, in flashinfer_autotune
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]     runner._dummy_run(
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 623, in forward
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]     model_output = self.model(
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]                    ^^^^^^^^^^^
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 439, in __call__
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]     return TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 223, in __call__
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 412, in forward
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]     def forward(
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]     raise e
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]   File "<eval_with_key>.58", line 304, in forward
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]     submod_4 = self.submod_4(getitem_8, s72, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_, getitem_9, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_8 = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_ = getitem_9 = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = None
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]     return range_entry.runnable(*args)
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]     return self._compiled_fn(*args)
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]     return compiled_fn(full_args)
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]                             ^^^^^^^
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]     return self.current_callable(inputs)
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]     out = model(new_inputs)
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]   File "/tmp/torchinductor_root/rk/crkxlvzuy365gv62vpjxi3gkogstlgibexososupxgmbv5wc564q.py", line 983, in call
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]     buf14 = torch.ops.slidesparse.dequant_bias.default(reinterpret_tensor(buf13, (s72, 16384), (16384, 1), 0), reinterpret_tensor(buf11, (s72, ), (1, ), 0), arg7_1, None, 'bfloat16', 'Llama3.2-3B-INT8')
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 483, in _dequant_bias_impl
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]     return fn(gemm_output, scale_a, scale_b, bias, out_dtype)
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]   File "/root/vllmbench/slidesparse/csrc/fused_dequant_bias_triton/build/RTX5080_cc120_py312_cu129_x86_64/dequant_bias_tuned_Llama3.2-3B.py", line 112, in dequant_bias_triton
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]     output = torch.empty((M, N), dtype=torch.bfloat16, device=gemm_output.device)
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226) ERROR 01-25 19:39:38 [core.py:866] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 15.46 GiB of which 1.97 GiB is free. Including non-PyTorch memory, this process has 13.27 GiB memory in use. Of the allocated memory 10.68 GiB is allocated by PyTorch, and 2.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


─── STDERR ───
[2026-01-25 19:39:22] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:39:22] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:39:22] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:39:22] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:39:22] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:39:22] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:39:22] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:39:22] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:39:22] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:39:22] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:39:22] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:39:22] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:39:22] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:39:22] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 19:39:26] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:39:26] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:39:26] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:39:26] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:39:26] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:39:26] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:39:26] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:39:26] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:39:26] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:39:26] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:39:26] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:39:26] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:39:26] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:39:26] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=386226) [2026-01-25 19:39:27] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=386226) [2026-01-25 19:39:27] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=386226) [2026-01-25 19:39:27] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=386226) [2026-01-25 19:39:27] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=386226) [2026-01-25 19:39:27] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=386226) [2026-01-25 19:39:27] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=386226) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=386226) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.14it/s]
(EngineCore_DP0 pid=386226) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.14it/s]
(EngineCore_DP0 pid=386226) 
(EngineCore_DP0 pid=386226) [2026-01-25 19:39:28] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 4096] -> 1D uint8
(EngineCore_DP0 pid=386226) [2026-01-25 19:39:28] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15728640 bytes
(EngineCore_DP0 pid=386226) [2026-01-25 19:39:28] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 4096] -> 1D uint8
(EngineCore_DP0 pid=386226) [2026-01-25 19:39:28] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9437184 bytes
(EngineCore_DP0 pid=386226) [2026-01-25 19:39:28] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 4096] -> 1D uint8
(EngineCore_DP0 pid=386226) [2026-01-25 19:39:28] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 50331648 bytes
(EngineCore_DP0 pid=386226) [2026-01-25 19:39:28] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 10944] -> 1D uint8
(EngineCore_DP0 pid=386226) [2026-01-25 19:39:28] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 25264128 bytes
(EngineCore_DP0 pid=386226) [rank0]:W0125 19:39:34.802000 386226 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=386226) [rank0]:W0125 19:39:35.398000 386226 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=386226) 2026-01-25 19:39:38,340 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=386226) 2026-01-25 19:39:38,427 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=386226) Process EngineCore_DP0:
(EngineCore_DP0 pid=386226) Traceback (most recent call last):
(EngineCore_DP0 pid=386226)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=386226)     self.run()
(EngineCore_DP0 pid=386226)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=386226)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=386226)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=386226)     raise e
(EngineCore_DP0 pid=386226)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=386226)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=386226)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=386226)     super().__init__(
(EngineCore_DP0 pid=386226)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=386226)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=386226)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226)   File "/root/vllmbench/vllm/v1/engine/core.py", line 256, in _initialize_kv_caches
(EngineCore_DP0 pid=386226)     self.model_executor.initialize_from_config(kv_cache_configs)
(EngineCore_DP0 pid=386226)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 116, in initialize_from_config
(EngineCore_DP0 pid=386226)     self.collective_rpc("compile_or_warm_up_model")
(EngineCore_DP0 pid=386226)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=386226)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=386226)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=386226)     return func(*args, **kwargs)
(EngineCore_DP0 pid=386226)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 455, in compile_or_warm_up_model
(EngineCore_DP0 pid=386226)     kernel_warmup(self)
(EngineCore_DP0 pid=386226)   File "/root/vllmbench/vllm/model_executor/warmup/kernel_warmup.py", line 41, in kernel_warmup
(EngineCore_DP0 pid=386226)     flashinfer_autotune(worker.model_runner)
(EngineCore_DP0 pid=386226)   File "/root/vllmbench/vllm/model_executor/warmup/kernel_warmup.py", line 94, in flashinfer_autotune
(EngineCore_DP0 pid=386226)     runner._dummy_run(
(EngineCore_DP0 pid=386226)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=386226)     return func(*args, **kwargs)
(EngineCore_DP0 pid=386226)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=386226)     outputs = self.model(
(EngineCore_DP0 pid=386226)               ^^^^^^^^^^^
(EngineCore_DP0 pid=386226)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=386226)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=386226)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=386226)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=386226)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=386226)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=386226)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 623, in forward
(EngineCore_DP0 pid=386226)     model_output = self.model(
(EngineCore_DP0 pid=386226)                    ^^^^^^^^^^^
(EngineCore_DP0 pid=386226)   File "/root/vllmbench/vllm/compilation/decorators.py", line 439, in __call__
(EngineCore_DP0 pid=386226)     return TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=386226)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 223, in __call__
(EngineCore_DP0 pid=386226)     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=386226)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=386226)     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=386226)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 412, in forward
(EngineCore_DP0 pid=386226)     def forward(
(EngineCore_DP0 pid=386226)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=386226)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=386226)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226)   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=386226)     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=386226)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=386226)     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=386226)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=386226)     raise e
(EngineCore_DP0 pid=386226)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=386226)     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=386226)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=386226)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=386226)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=386226)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=386226)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226)   File "<eval_with_key>.58", line 304, in forward
(EngineCore_DP0 pid=386226)     submod_4 = self.submod_4(getitem_8, s72, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_, getitem_9, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_8 = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_ = getitem_9 = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = None
(EngineCore_DP0 pid=386226)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=386226)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=386226)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226)   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=386226)     return range_entry.runnable(*args)
(EngineCore_DP0 pid=386226)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=386226)     return self._compiled_fn(*args)
(EngineCore_DP0 pid=386226)            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=386226)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=386226)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=386226)     return compiled_fn(full_args)
(EngineCore_DP0 pid=386226)            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=386226)     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=386226)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=386226)     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=386226)                             ^^^^^^^
(EngineCore_DP0 pid=386226)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=386226)     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=386226)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=386226)     return self.current_callable(inputs)
(EngineCore_DP0 pid=386226)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=386226)     out = model(new_inputs)
(EngineCore_DP0 pid=386226)           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226)   File "/tmp/torchinductor_root/rk/crkxlvzuy365gv62vpjxi3gkogstlgibexososupxgmbv5wc564q.py", line 983, in call
(EngineCore_DP0 pid=386226)     buf14 = torch.ops.slidesparse.dequant_bias.default(reinterpret_tensor(buf13, (s72, 16384), (16384, 1), 0), reinterpret_tensor(buf11, (s72, ), (1, ), 0), arg7_1, None, 'bfloat16', 'Llama3.2-3B-INT8')
(EngineCore_DP0 pid=386226)             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226)   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
(EngineCore_DP0 pid=386226)     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=386226)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226)   File "/root/vllmbench/slidesparse/core/kernels.py", line 483, in _dequant_bias_impl
(EngineCore_DP0 pid=386226)     return fn(gemm_output, scale_a, scale_b, bias, out_dtype)
(EngineCore_DP0 pid=386226)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226)   File "/root/vllmbench/slidesparse/csrc/fused_dequant_bias_triton/build/RTX5080_cc120_py312_cu129_x86_64/dequant_bias_tuned_Llama3.2-3B.py", line 112, in dequant_bias_triton
(EngineCore_DP0 pid=386226)     output = torch.empty((M, N), dtype=torch.bfloat16, device=gemm_output.device)
(EngineCore_DP0 pid=386226)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=386226) torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 15.46 GiB of which 1.97 GiB is free. Including non-PyTorch memory, this process has 13.27 GiB memory in use. Of the allocated memory 10.68 GiB is allocated by PyTorch, and 2.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W125 19:39:38.720725389 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[ERROR] 测试失败: M=65536 (exit code: 1)


------------------------------------------------------------
  生成 CSV: Llama3.2-3B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_6/Llama3.2-3B-INT8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,56.4048,28935.6401,2.2693
1024,1024,1,128,128,38.4144,39374.7134,3.3321
2048,1024,2,256,128,44.1405,45244.0487,5.7997
4096,1024,4,512,128,42.0991,43151.5891,12.1618
8192,1024,8,1024,128,40.5526,41566.3727,25.2512
16384,1024,16,2048,128,40.0488,41049.9814,51.1377
32768,1024,32,4096,128,40.1205,41123.4631,102.0926
65536,1024,64,8192,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[INFO] 完成: 7 成功, 1 失败

============================================================
  Llama3.2-3B-INT8 | cuSPARSELt (2_8) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_8
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_8

============================================================
[1/8] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-3B-INT8                                │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 19:39:44 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=386791) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=386791) WARNING 01-25 19:39:54 [backends.py:609] Failed to read file <frozen os>
Throughput: 55.40 requests/s, 28418.71 total tokens/s, 55.40 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-25 19:39:44] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:39:44] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:39:44] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:39:44] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:39:44] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:39:44] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:39:44] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:39:44] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:39:44] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:39:44] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:39:44] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:39:44] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:39:44] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:39:44] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 19:39:48] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:39:48] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:39:48] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:39:48] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:39:48] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:39:48] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:39:48] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:39:48] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:39:48] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:39:48] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:39:48] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:39:48] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:39:48] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:39:48] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=386791) [2026-01-25 19:39:49] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=386791) [2026-01-25 19:39:49] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=386791) [2026-01-25 19:39:49] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=386791) [2026-01-25 19:39:49] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=386791) [2026-01-25 19:39:49] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=386791) [2026-01-25 19:39:49] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=386791) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=386791) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.25s/it]
(EngineCore_DP0 pid=386791) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.25s/it]
(EngineCore_DP0 pid=386791) 
(EngineCore_DP0 pid=386791) [2026-01-25 19:39:50] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 4608] -> 1D uint8
(EngineCore_DP0 pid=386791) [2026-01-25 19:39:50] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 17694720 bytes
(EngineCore_DP0 pid=386791) [2026-01-25 19:39:50] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 4608] -> 1D uint8
(EngineCore_DP0 pid=386791) [2026-01-25 19:39:50] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 10616832 bytes
(EngineCore_DP0 pid=386791) [2026-01-25 19:39:50] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 4608] -> 1D uint8
(EngineCore_DP0 pid=386791) [2026-01-25 19:39:50] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 56623104 bytes
(EngineCore_DP0 pid=386791) [2026-01-25 19:39:50] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 12288] -> 1D uint8
(EngineCore_DP0 pid=386791) [2026-01-25 19:39:50] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 28311552 bytes
(EngineCore_DP0 pid=386791) 2026-01-25 19:40:00,597 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=386791) 2026-01-25 19:40:00,611 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=386791) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  4.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  4.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  4.39it/s]
(EngineCore_DP0 pid=386791) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.43it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.43it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  98%|█████████▊| 125/128 [00:00<00:00, 1245.00it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 1248.34it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:22,  5.65it/s, est. speed input: 2894.51 toks/s, output: 5.65 toks/s]
Processed prompts:   6%|▋         | 8/128 [00:00<00:03, 32.34it/s, est. speed input: 14069.89 toks/s, output: 27.48 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:00<00:02, 44.48it/s, est. speed input: 19055.01 toks/s, output: 37.22 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:00<00:02, 50.86it/s, est. speed input: 21819.74 toks/s, output: 42.62 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:00<00:01, 55.02it/s, est. speed input: 23675.08 toks/s, output: 46.24 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:00<00:01, 57.39it/s, est. speed input: 24918.41 toks/s, output: 48.67 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:00<00:01, 59.20it/s, est. speed input: 25882.99 toks/s, output: 50.55 toks/s]
Processed prompts:  39%|███▉      | 50/128 [00:00<00:01, 60.50it/s, est. speed input: 26638.02 toks/s, output: 52.03 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:01<00:01, 61.49it/s, est. speed input: 27251.77 toks/s, output: 53.23 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:01<00:01, 62.11it/s, est. speed input: 27744.26 toks/s, output: 54.19 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:01<00:00, 61.20it/s, est. speed input: 27980.45 toks/s, output: 54.65 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:01<00:00, 61.88it/s, est. speed input: 28333.54 toks/s, output: 55.34 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:01<00:00, 62.06it/s, est. speed input: 28602.43 toks/s, output: 55.86 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:01<00:00, 62.19it/s, est. speed input: 28835.67 toks/s, output: 56.32 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:01<00:00, 62.27it/s, est. speed input: 29037.22 toks/s, output: 56.71 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:01<00:00, 62.47it/s, est. speed input: 29227.72 toks/s, output: 57.09 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:01<00:00, 62.67it/s, est. speed input: 29402.88 toks/s, output: 57.43 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:02<00:00, 62.73it/s, est. speed input: 29551.77 toks/s, output: 57.72 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:02<00:00, 62.81it/s, est. speed input: 29689.09 toks/s, output: 57.99 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 62.81it/s, est. speed input: 29704.98 toks/s, output: 58.02 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 58.02it/s, est. speed input: 29704.98 toks/s, output: 58.02 toks/s]
[rank0]:[W125 19:40:04.400410493 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 25.2s

测试结果:
  Requests/s:   55.40
  Tokens/s:     28418.71
  Total Reqs:   128
  Elapsed:      2.31s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     28363.31

============================================================
[2/8] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-3B-INT8                                │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 19:40:09 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=387467) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=387467) WARNING 01-25 19:40:19 [backends.py:609] Failed to read file <frozen os>
Throughput: 35.89 requests/s, 36789.16 total tokens/s, 35.89 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-25 19:40:09] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:40:09] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:40:09] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:40:09] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:40:09] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:40:09] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:40:09] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:40:09] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:40:09] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:40:09] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:40:09] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:40:09] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:40:09] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:40:09] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 19:40:13] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:40:13] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:40:13] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:40:13] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:40:13] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:40:13] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:40:13] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:40:13] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:40:13] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:40:13] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:40:13] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:40:13] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:40:13] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:40:13] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=387467) [2026-01-25 19:40:14] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=387467) [2026-01-25 19:40:14] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=387467) [2026-01-25 19:40:14] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=387467) [2026-01-25 19:40:14] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=387467) [2026-01-25 19:40:14] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=387467) [2026-01-25 19:40:14] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=387467) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=387467) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.15it/s]
(EngineCore_DP0 pid=387467) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.15it/s]
(EngineCore_DP0 pid=387467) 
(EngineCore_DP0 pid=387467) [2026-01-25 19:40:14] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 4608] -> 1D uint8
(EngineCore_DP0 pid=387467) [2026-01-25 19:40:14] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 17694720 bytes
(EngineCore_DP0 pid=387467) [2026-01-25 19:40:14] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 4608] -> 1D uint8
(EngineCore_DP0 pid=387467) [2026-01-25 19:40:14] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 10616832 bytes
(EngineCore_DP0 pid=387467) [2026-01-25 19:40:14] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 4608] -> 1D uint8
(EngineCore_DP0 pid=387467) [2026-01-25 19:40:14] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 56623104 bytes
(EngineCore_DP0 pid=387467) [2026-01-25 19:40:14] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 12288] -> 1D uint8
(EngineCore_DP0 pid=387467) [2026-01-25 19:40:14] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 28311552 bytes
(EngineCore_DP0 pid=387467) 2026-01-25 19:40:25,094 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=387467) 2026-01-25 19:40:25,109 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=387467) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 25.03it/s]
(EngineCore_DP0 pid=387467) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.31it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.31it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  52%|█████▏    | 66/128 [00:00<00:00, 651.04it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 686.70it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 2/128 [00:00<00:07, 16.57it/s, est. speed input: 16973.04 toks/s, output: 16.57 toks/s]
Processed prompts:   5%|▍         | 6/128 [00:00<00:04, 28.75it/s, est. speed input: 27426.30 toks/s, output: 26.78 toks/s]
Processed prompts:   8%|▊         | 10/128 [00:00<00:03, 33.11it/s, est. speed input: 31266.21 toks/s, output: 30.53 toks/s]
Processed prompts:  11%|█         | 14/128 [00:00<00:03, 35.29it/s, est. speed input: 33283.02 toks/s, output: 32.50 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:00<00:03, 36.45it/s, est. speed input: 34487.39 toks/s, output: 33.68 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:00<00:02, 37.22it/s, est. speed input: 35326.74 toks/s, output: 34.50 toks/s]
Processed prompts:  20%|██        | 26/128 [00:00<00:02, 37.62it/s, est. speed input: 35894.34 toks/s, output: 35.05 toks/s]
Processed prompts:  23%|██▎       | 30/128 [00:00<00:02, 37.91it/s, est. speed input: 36331.79 toks/s, output: 35.48 toks/s]
Processed prompts:  27%|██▋       | 34/128 [00:00<00:02, 38.17it/s, est. speed input: 36696.18 toks/s, output: 35.84 toks/s]
Processed prompts:  30%|██▉       | 38/128 [00:01<00:02, 38.24it/s, est. speed input: 36956.00 toks/s, output: 36.09 toks/s]
Processed prompts:  33%|███▎      | 42/128 [00:01<00:02, 38.42it/s, est. speed input: 37205.00 toks/s, output: 36.33 toks/s]
Processed prompts:  36%|███▌      | 46/128 [00:01<00:02, 38.53it/s, est. speed input: 37411.51 toks/s, output: 36.53 toks/s]
Processed prompts:  39%|███▉      | 50/128 [00:01<00:02, 38.59it/s, est. speed input: 37579.88 toks/s, output: 36.70 toks/s]
Processed prompts:  42%|████▏     | 54/128 [00:01<00:01, 38.65it/s, est. speed input: 37730.55 toks/s, output: 36.85 toks/s]
Processed prompts:  45%|████▌     | 58/128 [00:01<00:01, 38.66it/s, est. speed input: 37855.80 toks/s, output: 36.97 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:01<00:01, 38.63it/s, est. speed input: 37956.39 toks/s, output: 37.07 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:01<00:01, 38.67it/s, est. speed input: 38057.42 toks/s, output: 37.17 toks/s]
Processed prompts:  55%|█████▍    | 70/128 [00:01<00:01, 38.58it/s, est. speed input: 38126.66 toks/s, output: 37.23 toks/s]
Processed prompts:  58%|█████▊    | 74/128 [00:01<00:01, 38.64it/s, est. speed input: 38208.19 toks/s, output: 37.31 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:02<00:01, 38.70it/s, est. speed input: 38285.11 toks/s, output: 37.39 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:02<00:01, 38.73it/s, est. speed input: 38353.41 toks/s, output: 37.45 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:02<00:01, 38.68it/s, est. speed input: 38404.26 toks/s, output: 37.50 toks/s]
Processed prompts:  70%|███████   | 90/128 [00:02<00:00, 38.70it/s, est. speed input: 38459.35 toks/s, output: 37.56 toks/s]
Processed prompts:  73%|███████▎  | 94/128 [00:02<00:00, 38.69it/s, est. speed input: 38505.97 toks/s, output: 37.60 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:02<00:00, 38.70it/s, est. speed input: 38551.92 toks/s, output: 37.65 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:02<00:00, 38.73it/s, est. speed input: 38596.88 toks/s, output: 37.69 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:02<00:00, 38.73it/s, est. speed input: 38635.52 toks/s, output: 37.73 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:02<00:00, 38.69it/s, est. speed input: 38667.74 toks/s, output: 37.76 toks/s]
Processed prompts:  89%|████████▉ | 114/128 [00:03<00:00, 38.70it/s, est. speed input: 38701.26 toks/s, output: 37.79 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:03<00:00, 38.71it/s, est. speed input: 38732.58 toks/s, output: 37.82 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:03<00:00, 38.69it/s, est. speed input: 38759.75 toks/s, output: 37.85 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:03<00:00, 38.65it/s, est. speed input: 38781.87 toks/s, output: 37.87 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 38.65it/s, est. speed input: 38798.95 toks/s, output: 37.89 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 37.89it/s, est. speed input: 38798.95 toks/s, output: 37.89 toks/s]
[rank0]:[W125 19:40:29.788997955 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 25.5s

测试结果:
  Requests/s:   35.89
  Tokens/s:     36789.16
  Total Reqs:   128
  Elapsed:      3.57s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     36753.26

============================================================
[3/8] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-3B-INT8                                │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 19:40:35 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=388115) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=388115) WARNING 01-25 19:40:45 [backends.py:609] Failed to read file <frozen os>
Throughput: 40.72 requests/s, 41733.23 total tokens/s, 40.72 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-25 19:40:35] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:40:35] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:40:35] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:40:35] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:40:35] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:40:35] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:40:35] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:40:35] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:40:35] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:40:35] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:40:35] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:40:35] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:40:35] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:40:35] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 19:40:39] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:40:39] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:40:39] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:40:39] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:40:39] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:40:39] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:40:39] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:40:39] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:40:39] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:40:39] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:40:39] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:40:39] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:40:39] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:40:39] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=388115) [2026-01-25 19:40:40] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=388115) [2026-01-25 19:40:40] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=388115) [2026-01-25 19:40:40] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=388115) [2026-01-25 19:40:40] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=388115) [2026-01-25 19:40:40] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=388115) [2026-01-25 19:40:40] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=388115) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=388115) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.16it/s]
(EngineCore_DP0 pid=388115) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.16it/s]
(EngineCore_DP0 pid=388115) 
(EngineCore_DP0 pid=388115) [2026-01-25 19:40:40] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 4608] -> 1D uint8
(EngineCore_DP0 pid=388115) [2026-01-25 19:40:41] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 17694720 bytes
(EngineCore_DP0 pid=388115) [2026-01-25 19:40:41] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 4608] -> 1D uint8
(EngineCore_DP0 pid=388115) [2026-01-25 19:40:41] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 10616832 bytes
(EngineCore_DP0 pid=388115) [2026-01-25 19:40:41] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 4608] -> 1D uint8
(EngineCore_DP0 pid=388115) [2026-01-25 19:40:41] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 56623104 bytes
(EngineCore_DP0 pid=388115) [2026-01-25 19:40:41] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 12288] -> 1D uint8
(EngineCore_DP0 pid=388115) [2026-01-25 19:40:41] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 28311552 bytes
(EngineCore_DP0 pid=388115) 2026-01-25 19:40:50,799 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=388115) 2026-01-25 19:40:50,814 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=388115) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:00<00:00, 16.25it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 16.21it/s]
(EngineCore_DP0 pid=388115) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 1/2 [00:00<00:00,  6.26it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00,  9.34it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  24%|██▍       | 61/256 [00:00<00:00, 606.73it/s]
Adding requests:  53%|█████▎    | 136/256 [00:00<00:00, 689.83it/s]
Adding requests:  82%|████████▏ | 210/256 [00:00<00:00, 710.82it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 709.76it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   6%|▋         | 16/256 [00:00<00:01, 136.97it/s, est. speed input: 140271.60 toks/s, output: 136.98 toks/s]
Processed prompts:  12%|█▏        | 30/256 [00:00<00:03, 59.81it/s, est. speed input: 67314.73 toks/s, output: 65.74 toks/s]   
Processed prompts:  15%|█▌        | 39/256 [00:00<00:03, 54.40it/s, est. speed input: 61328.77 toks/s, output: 59.89 toks/s]
Processed prompts:  18%|█▊        | 46/256 [00:00<00:04, 47.79it/s, est. speed input: 55744.38 toks/s, output: 54.44 toks/s]
Processed prompts:  20%|██        | 52/256 [00:00<00:04, 45.95it/s, est. speed input: 53752.76 toks/s, output: 52.49 toks/s]
Processed prompts:  23%|██▎       | 58/256 [00:01<00:04, 44.63it/s, est. speed input: 52285.80 toks/s, output: 51.06 toks/s]
Processed prompts:  25%|██▌       | 64/256 [00:01<00:04, 43.60it/s, est. speed input: 51124.42 toks/s, output: 49.93 toks/s]
Processed prompts:  27%|██▋       | 70/256 [00:01<00:04, 42.86it/s, est. speed input: 50195.91 toks/s, output: 49.02 toks/s]
Processed prompts:  30%|██▉       | 76/256 [00:01<00:04, 42.35it/s, est. speed input: 49446.37 toks/s, output: 48.29 toks/s]
Processed prompts:  32%|███▏      | 82/256 [00:01<00:04, 42.01it/s, est. speed input: 48832.05 toks/s, output: 47.69 toks/s]
Processed prompts:  34%|███▍      | 88/256 [00:01<00:04, 41.74it/s, est. speed input: 48306.03 toks/s, output: 47.17 toks/s]
Processed prompts:  37%|███▋      | 94/256 [00:02<00:03, 41.53it/s, est. speed input: 47848.70 toks/s, output: 46.73 toks/s]
Processed prompts:  39%|███▉      | 100/256 [00:02<00:03, 41.42it/s, est. speed input: 47463.04 toks/s, output: 46.35 toks/s]
Processed prompts:  41%|████▏     | 106/256 [00:02<00:03, 41.30it/s, est. speed input: 47115.68 toks/s, output: 46.01 toks/s]
Processed prompts:  44%|████▍     | 112/256 [00:02<00:03, 41.25it/s, est. speed input: 46819.01 toks/s, output: 45.72 toks/s]
Processed prompts:  46%|████▌     | 118/256 [00:02<00:03, 41.22it/s, est. speed input: 46556.64 toks/s, output: 45.47 toks/s]
Processed prompts:  48%|████▊     | 124/256 [00:02<00:03, 41.21it/s, est. speed input: 46324.13 toks/s, output: 45.24 toks/s]
Processed prompts:  51%|█████     | 130/256 [00:02<00:03, 41.22it/s, est. speed input: 46117.49 toks/s, output: 45.04 toks/s]
Processed prompts:  53%|█████▎    | 136/256 [00:03<00:02, 41.23it/s, est. speed input: 45932.35 toks/s, output: 44.86 toks/s]
Processed prompts:  55%|█████▌    | 142/256 [00:03<00:02, 41.24it/s, est. speed input: 45763.92 toks/s, output: 44.69 toks/s]
Processed prompts:  58%|█████▊    | 148/256 [00:03<00:02, 41.20it/s, est. speed input: 45603.12 toks/s, output: 44.53 toks/s]
Processed prompts:  60%|██████    | 154/256 [00:03<00:02, 41.20it/s, est. speed input: 45458.58 toks/s, output: 44.39 toks/s]
Processed prompts:  62%|██████▎   | 160/256 [00:03<00:02, 41.21it/s, est. speed input: 45328.18 toks/s, output: 44.27 toks/s]
Processed prompts:  65%|██████▍   | 166/256 [00:03<00:02, 41.22it/s, est. speed input: 45208.42 toks/s, output: 44.15 toks/s]
Processed prompts:  67%|██████▋   | 172/256 [00:03<00:02, 41.21it/s, est. speed input: 45095.67 toks/s, output: 44.04 toks/s]
Processed prompts:  70%|██████▉   | 178/256 [00:04<00:01, 41.20it/s, est. speed input: 44990.52 toks/s, output: 43.94 toks/s]
Processed prompts:  72%|███████▏  | 184/256 [00:04<00:01, 41.14it/s, est. speed input: 44885.30 toks/s, output: 43.83 toks/s]
Processed prompts:  74%|███████▍  | 190/256 [00:04<00:01, 41.11it/s, est. speed input: 44789.49 toks/s, output: 43.74 toks/s]
Processed prompts:  77%|███████▋  | 196/256 [00:04<00:01, 41.10it/s, est. speed input: 44700.54 toks/s, output: 43.65 toks/s]
Processed prompts:  79%|███████▉  | 202/256 [00:04<00:01, 41.12it/s, est. speed input: 44620.04 toks/s, output: 43.57 toks/s]
Processed prompts:  81%|████████▏ | 208/256 [00:04<00:01, 41.14it/s, est. speed input: 44545.89 toks/s, output: 43.50 toks/s]
Processed prompts:  84%|████████▎ | 214/256 [00:04<00:01, 41.11it/s, est. speed input: 44471.01 toks/s, output: 43.43 toks/s]
Processed prompts:  86%|████████▌ | 220/256 [00:05<00:00, 41.14it/s, est. speed input: 44405.81 toks/s, output: 43.36 toks/s]
Processed prompts:  88%|████████▊ | 226/256 [00:05<00:00, 41.17it/s, est. speed input: 44345.27 toks/s, output: 43.31 toks/s]
Processed prompts:  91%|█████████ | 232/256 [00:05<00:00, 41.18it/s, est. speed input: 44286.52 toks/s, output: 43.25 toks/s]
Processed prompts:  93%|█████████▎| 238/256 [00:05<00:00, 41.16it/s, est. speed input: 44228.38 toks/s, output: 43.19 toks/s]
Processed prompts:  95%|█████████▌| 244/256 [00:05<00:00, 41.17it/s, est. speed input: 44175.80 toks/s, output: 43.14 toks/s]
Processed prompts:  98%|█████████▊| 250/256 [00:05<00:00, 41.12it/s, est. speed input: 44120.82 toks/s, output: 43.09 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:05<00:00, 43.18it/s, est. speed input: 44243.86 toks/s, output: 43.21 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:05<00:00, 43.18it/s, est. speed input: 44243.86 toks/s, output: 43.21 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:05<00:00, 43.21it/s, est. speed input: 44243.86 toks/s, output: 43.21 toks/s]
[rank0]:[W125 19:40:58.439116017 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 28.5s

测试结果:
  Requests/s:   40.72
  Tokens/s:     41733.23
  Total Reqs:   256
  Elapsed:      6.29s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     41692.52

============================================================
[4/8] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-3B-INT8                                │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 19:41:05 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=388792) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=388792) WARNING 01-25 19:41:14 [backends.py:609] Failed to read file <frozen os>
Throughput: 39.03 requests/s, 40004.34 total tokens/s, 39.03 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-25 19:41:04] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:41:05] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:41:05] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:41:05] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:41:05] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:41:05] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:41:05] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:41:05] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:41:05] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:41:05] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:41:05] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:41:05] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:41:05] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:41:05] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 19:41:08] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:41:08] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:41:08] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:41:08] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:41:08] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:41:08] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:41:08] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:41:08] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:41:08] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:41:08] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:41:08] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:41:08] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:41:08] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:41:08] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=388792) [2026-01-25 19:41:09] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=388792) [2026-01-25 19:41:09] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=388792) [2026-01-25 19:41:09] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=388792) [2026-01-25 19:41:09] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=388792) [2026-01-25 19:41:09] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=388792) [2026-01-25 19:41:09] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=388792) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=388792) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.09it/s]
(EngineCore_DP0 pid=388792) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.09it/s]
(EngineCore_DP0 pid=388792) 
(EngineCore_DP0 pid=388792) [2026-01-25 19:41:10] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 4608] -> 1D uint8
(EngineCore_DP0 pid=388792) [2026-01-25 19:41:10] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 17694720 bytes
(EngineCore_DP0 pid=388792) [2026-01-25 19:41:10] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 4608] -> 1D uint8
(EngineCore_DP0 pid=388792) [2026-01-25 19:41:10] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 10616832 bytes
(EngineCore_DP0 pid=388792) [2026-01-25 19:41:10] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 4608] -> 1D uint8
(EngineCore_DP0 pid=388792) [2026-01-25 19:41:10] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 56623104 bytes
(EngineCore_DP0 pid=388792) [2026-01-25 19:41:10] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 12288] -> 1D uint8
(EngineCore_DP0 pid=388792) [2026-01-25 19:41:10] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 28311552 bytes
(EngineCore_DP0 pid=388792) 2026-01-25 19:41:19,932 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=388792) 2026-01-25 19:41:19,946 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=388792) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 3/4 [00:00<00:00, 28.65it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 26.31it/s]
(EngineCore_DP0 pid=388792) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  33%|███▎      | 1/3 [00:00<00:00,  7.41it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 15.35it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:  14%|█▍        | 72/512 [00:00<00:00, 714.33it/s]
Adding requests:  29%|██▉       | 148/512 [00:00<00:00, 736.67it/s]
Adding requests:  44%|████▍     | 224/512 [00:00<00:00, 744.60it/s]
Adding requests:  58%|█████▊    | 299/512 [00:00<00:00, 745.68it/s]
Adding requests:  73%|███████▎  | 375/512 [00:00<00:00, 749.11it/s]
Adding requests:  88%|████████▊ | 450/512 [00:00<00:00, 747.09it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 742.85it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   5%|▌         | 26/512 [00:00<00:01, 252.03it/s, est. speed input: 258124.14 toks/s, output: 252.05 toks/s]
Processed prompts:  10%|█         | 52/512 [00:00<00:07, 64.65it/s, est. speed input: 74507.79 toks/s, output: 72.76 toks/s]   
Processed prompts:  13%|█▎        | 65/512 [00:01<00:07, 56.07it/s, est. speed input: 65275.26 toks/s, output: 63.75 toks/s]
Processed prompts:  14%|█▍        | 74/512 [00:01<00:09, 46.55it/s, est. speed input: 57179.67 toks/s, output: 55.84 toks/s]
Processed prompts:  16%|█▌        | 81/512 [00:01<00:08, 49.77it/s, est. speed input: 58118.35 toks/s, output: 56.76 toks/s]
Processed prompts:  17%|█▋        | 88/512 [00:01<00:09, 45.22it/s, est. speed input: 55241.69 toks/s, output: 53.95 toks/s]
Processed prompts:  18%|█▊        | 94/512 [00:01<00:10, 40.55it/s, est. speed input: 52451.41 toks/s, output: 51.22 toks/s]
Processed prompts:  19%|█▉        | 99/512 [00:01<00:09, 42.04it/s, est. speed input: 52335.55 toks/s, output: 51.11 toks/s]
Processed prompts:  20%|██        | 104/512 [00:02<00:09, 43.42it/s, est. speed input: 52226.04 toks/s, output: 51.00 toks/s]
Processed prompts:  21%|██▏       | 109/512 [00:02<00:09, 44.66it/s, est. speed input: 52128.93 toks/s, output: 50.91 toks/s]
Processed prompts:  22%|██▏       | 114/512 [00:02<00:10, 36.83it/s, est. speed input: 49774.68 toks/s, output: 48.61 toks/s]
Processed prompts:  23%|██▎       | 119/512 [00:02<00:09, 39.47it/s, est. speed input: 49789.90 toks/s, output: 48.62 toks/s]
Processed prompts:  24%|██▍       | 124/512 [00:02<00:09, 41.69it/s, est. speed input: 49798.21 toks/s, output: 48.63 toks/s]
Processed prompts:  25%|██▌       | 129/512 [00:02<00:08, 43.54it/s, est. speed input: 49813.31 toks/s, output: 48.65 toks/s]
Processed prompts:  26%|██▌       | 134/512 [00:02<00:10, 35.61it/s, est. speed input: 48058.25 toks/s, output: 46.93 toks/s]
Processed prompts:  27%|██▋       | 138/512 [00:02<00:10, 36.44it/s, est. speed input: 47786.04 toks/s, output: 46.67 toks/s]
Processed prompts:  28%|██▊       | 142/512 [00:03<00:09, 37.16it/s, est. speed input: 47537.22 toks/s, output: 46.42 toks/s]
Processed prompts:  29%|██▊       | 146/512 [00:03<00:09, 37.73it/s, est. speed input: 47303.31 toks/s, output: 46.19 toks/s]
Processed prompts:  29%|██▉       | 150/512 [00:03<00:09, 38.15it/s, est. speed input: 47082.73 toks/s, output: 45.98 toks/s]
Processed prompts:  30%|███       | 154/512 [00:03<00:09, 38.49it/s, est. speed input: 46878.37 toks/s, output: 45.78 toks/s]
Processed prompts:  31%|███       | 158/512 [00:03<00:09, 38.72it/s, est. speed input: 46682.88 toks/s, output: 45.59 toks/s]
Processed prompts:  32%|███▏      | 162/512 [00:03<00:09, 38.84it/s, est. speed input: 46493.77 toks/s, output: 45.40 toks/s]
Processed prompts:  32%|███▏      | 166/512 [00:03<00:08, 38.93it/s, est. speed input: 46315.42 toks/s, output: 45.23 toks/s]
Processed prompts:  33%|███▎      | 170/512 [00:03<00:08, 39.05it/s, est. speed input: 46152.95 toks/s, output: 45.07 toks/s]
Processed prompts:  34%|███▍      | 174/512 [00:03<00:08, 39.11it/s, est. speed input: 45995.74 toks/s, output: 44.92 toks/s]
Processed prompts:  35%|███▍      | 178/512 [00:03<00:08, 39.17it/s, est. speed input: 45849.30 toks/s, output: 44.77 toks/s]
Processed prompts:  36%|███▌      | 182/512 [00:04<00:08, 39.17it/s, est. speed input: 45705.08 toks/s, output: 44.63 toks/s]
Processed prompts:  36%|███▋      | 186/512 [00:04<00:08, 39.18it/s, est. speed input: 45569.63 toks/s, output: 44.50 toks/s]
Processed prompts:  37%|███▋      | 190/512 [00:04<00:08, 39.18it/s, est. speed input: 45439.76 toks/s, output: 44.37 toks/s]
Processed prompts:  38%|███▊      | 194/512 [00:04<00:08, 39.20it/s, est. speed input: 45317.32 toks/s, output: 44.25 toks/s]
Processed prompts:  39%|███▊      | 198/512 [00:04<00:08, 39.19it/s, est. speed input: 45199.00 toks/s, output: 44.14 toks/s]
Processed prompts:  39%|███▉      | 202/512 [00:04<00:07, 39.19it/s, est. speed input: 45085.88 toks/s, output: 44.03 toks/s]
Processed prompts:  40%|████      | 206/512 [00:04<00:07, 39.15it/s, est. speed input: 44975.96 toks/s, output: 43.92 toks/s]
Processed prompts:  41%|████      | 210/512 [00:04<00:07, 39.19it/s, est. speed input: 44873.60 toks/s, output: 43.82 toks/s]
Processed prompts:  42%|████▏     | 214/512 [00:04<00:07, 39.16it/s, est. speed input: 44772.61 toks/s, output: 43.72 toks/s]
Processed prompts:  43%|████▎     | 218/512 [00:04<00:07, 39.19it/s, est. speed input: 44679.59 toks/s, output: 43.63 toks/s]
Processed prompts:  43%|████▎     | 222/512 [00:05<00:07, 39.22it/s, est. speed input: 44590.95 toks/s, output: 43.55 toks/s]
Processed prompts:  44%|████▍     | 226/512 [00:05<00:07, 39.24it/s, est. speed input: 44505.17 toks/s, output: 43.46 toks/s]
Processed prompts:  45%|████▍     | 230/512 [00:05<00:07, 39.22it/s, est. speed input: 44421.06 toks/s, output: 43.38 toks/s]
Processed prompts:  46%|████▌     | 234/512 [00:05<00:07, 39.24it/s, est. speed input: 44341.69 toks/s, output: 43.30 toks/s]
Processed prompts:  46%|████▋     | 238/512 [00:05<00:06, 39.20it/s, est. speed input: 44262.06 toks/s, output: 43.22 toks/s]
Processed prompts:  47%|████▋     | 242/512 [00:05<00:06, 39.19it/s, est. speed input: 44186.36 toks/s, output: 43.15 toks/s]
Processed prompts:  48%|████▊     | 246/512 [00:05<00:06, 39.17it/s, est. speed input: 44112.43 toks/s, output: 43.08 toks/s]
Processed prompts:  49%|████▉     | 250/512 [00:05<00:06, 39.20it/s, est. speed input: 44044.36 toks/s, output: 43.01 toks/s]
Processed prompts:  50%|████▉     | 254/512 [00:05<00:06, 39.24it/s, est. speed input: 43979.73 toks/s, output: 42.95 toks/s]
Processed prompts:  50%|█████     | 258/512 [00:06<00:06, 39.24it/s, est. speed input: 43915.21 toks/s, output: 42.89 toks/s]
Processed prompts:  51%|█████     | 262/512 [00:06<00:06, 39.20it/s, est. speed input: 43850.77 toks/s, output: 42.82 toks/s]
Processed prompts:  52%|█████▏    | 266/512 [00:06<00:06, 39.19it/s, est. speed input: 43788.89 toks/s, output: 42.76 toks/s]
Processed prompts:  53%|█████▎    | 270/512 [00:06<00:06, 39.16it/s, est. speed input: 43728.00 toks/s, output: 42.70 toks/s]
Processed prompts:  54%|█████▎    | 274/512 [00:06<00:06, 39.18it/s, est. speed input: 43671.61 toks/s, output: 42.65 toks/s]
Processed prompts:  54%|█████▍    | 278/512 [00:06<00:05, 39.16it/s, est. speed input: 43614.78 toks/s, output: 42.59 toks/s]
Processed prompts:  55%|█████▌    | 282/512 [00:06<00:05, 39.08it/s, est. speed input: 43556.33 toks/s, output: 42.54 toks/s]
Processed prompts:  56%|█████▌    | 286/512 [00:06<00:05, 39.12it/s, est. speed input: 43504.77 toks/s, output: 42.49 toks/s]
Processed prompts:  57%|█████▋    | 290/512 [00:06<00:05, 39.17it/s, est. speed input: 43455.92 toks/s, output: 42.44 toks/s]
Processed prompts:  57%|█████▋    | 294/512 [00:06<00:05, 39.15it/s, est. speed input: 43405.64 toks/s, output: 42.39 toks/s]
Processed prompts:  58%|█████▊    | 298/512 [00:07<00:05, 39.18it/s, est. speed input: 43359.21 toks/s, output: 42.34 toks/s]
Processed prompts:  59%|█████▉    | 302/512 [00:07<00:05, 39.21it/s, est. speed input: 43314.40 toks/s, output: 42.30 toks/s]
Processed prompts:  60%|█████▉    | 306/512 [00:07<00:05, 39.19it/s, est. speed input: 43268.88 toks/s, output: 42.25 toks/s]
Processed prompts:  61%|██████    | 310/512 [00:07<00:05, 39.21it/s, est. speed input: 43226.06 toks/s, output: 42.21 toks/s]
Processed prompts:  61%|██████▏   | 314/512 [00:07<00:05, 39.23it/s, est. speed input: 43185.16 toks/s, output: 42.17 toks/s]
Processed prompts:  62%|██████▏   | 318/512 [00:07<00:04, 39.20it/s, est. speed input: 43142.86 toks/s, output: 42.13 toks/s]
Processed prompts:  63%|██████▎   | 322/512 [00:07<00:04, 39.17it/s, est. speed input: 43101.40 toks/s, output: 42.09 toks/s]
Processed prompts:  64%|██████▎   | 326/512 [00:07<00:04, 39.15it/s, est. speed input: 43060.88 toks/s, output: 42.05 toks/s]
Processed prompts:  64%|██████▍   | 330/512 [00:07<00:04, 39.14it/s, est. speed input: 43021.71 toks/s, output: 42.01 toks/s]
Processed prompts:  65%|██████▌   | 334/512 [00:07<00:04, 39.16it/s, est. speed input: 42984.85 toks/s, output: 41.98 toks/s]
Processed prompts:  66%|██████▌   | 338/512 [00:08<00:04, 39.12it/s, est. speed input: 42946.58 toks/s, output: 41.94 toks/s]
Processed prompts:  67%|██████▋   | 342/512 [00:08<00:04, 39.17it/s, est. speed input: 42912.44 toks/s, output: 41.91 toks/s]
Processed prompts:  68%|██████▊   | 346/512 [00:08<00:04, 39.17it/s, est. speed input: 42878.14 toks/s, output: 41.87 toks/s]
Processed prompts:  68%|██████▊   | 350/512 [00:08<00:04, 39.16it/s, est. speed input: 42843.56 toks/s, output: 41.84 toks/s]
Processed prompts:  69%|██████▉   | 354/512 [00:08<00:04, 39.15it/s, est. speed input: 42810.20 toks/s, output: 41.81 toks/s]
Processed prompts:  70%|██████▉   | 358/512 [00:08<00:03, 39.10it/s, est. speed input: 42775.68 toks/s, output: 41.77 toks/s]
Processed prompts:  71%|███████   | 362/512 [00:08<00:03, 39.09it/s, est. speed input: 42742.69 toks/s, output: 41.74 toks/s]
Processed prompts:  71%|███████▏  | 366/512 [00:08<00:03, 39.12it/s, est. speed input: 42712.39 toks/s, output: 41.71 toks/s]
Processed prompts:  72%|███████▏  | 370/512 [00:08<00:03, 39.17it/s, est. speed input: 42683.82 toks/s, output: 41.68 toks/s]
Processed prompts:  73%|███████▎  | 374/512 [00:08<00:03, 39.19it/s, est. speed input: 42655.31 toks/s, output: 41.66 toks/s]
Processed prompts:  74%|███████▍  | 378/512 [00:09<00:03, 39.20it/s, est. speed input: 42627.58 toks/s, output: 41.63 toks/s]
Processed prompts:  75%|███████▍  | 382/512 [00:09<00:03, 39.22it/s, est. speed input: 42600.80 toks/s, output: 41.60 toks/s]
Processed prompts:  75%|███████▌  | 386/512 [00:09<00:03, 39.22it/s, est. speed input: 42573.90 toks/s, output: 41.58 toks/s]
Processed prompts:  76%|███████▌  | 390/512 [00:09<00:03, 39.23it/s, est. speed input: 42548.13 toks/s, output: 41.55 toks/s]
Processed prompts:  77%|███████▋  | 394/512 [00:09<00:03, 39.23it/s, est. speed input: 42522.53 toks/s, output: 41.53 toks/s]
Processed prompts:  78%|███████▊  | 398/512 [00:09<00:02, 39.22it/s, est. speed input: 42497.24 toks/s, output: 41.50 toks/s]
Processed prompts:  79%|███████▊  | 402/512 [00:09<00:02, 39.21it/s, est. speed input: 42472.17 toks/s, output: 41.48 toks/s]
Processed prompts:  79%|███████▉  | 406/512 [00:09<00:02, 39.18it/s, est. speed input: 42446.95 toks/s, output: 41.45 toks/s]
Processed prompts:  80%|████████  | 410/512 [00:09<00:02, 39.17it/s, est. speed input: 42422.51 toks/s, output: 41.43 toks/s]
Processed prompts:  81%|████████  | 414/512 [00:09<00:02, 39.17it/s, est. speed input: 42399.02 toks/s, output: 41.41 toks/s]
Processed prompts:  82%|████████▏ | 418/512 [00:10<00:02, 39.18it/s, est. speed input: 42376.07 toks/s, output: 41.38 toks/s]
Processed prompts:  82%|████████▏ | 422/512 [00:10<00:02, 39.17it/s, est. speed input: 42353.33 toks/s, output: 41.36 toks/s]
Processed prompts:  83%|████████▎ | 426/512 [00:10<00:02, 39.18it/s, est. speed input: 42331.40 toks/s, output: 41.34 toks/s]
Processed prompts:  84%|████████▍ | 430/512 [00:10<00:02, 39.20it/s, est. speed input: 42310.28 toks/s, output: 41.32 toks/s]
Processed prompts:  85%|████████▍ | 434/512 [00:10<00:01, 39.14it/s, est. speed input: 42287.34 toks/s, output: 41.30 toks/s]
Processed prompts:  86%|████████▌ | 438/512 [00:10<00:01, 39.13it/s, est. speed input: 42265.79 toks/s, output: 41.28 toks/s]
Processed prompts:  86%|████████▋ | 442/512 [00:10<00:01, 39.09it/s, est. speed input: 42243.33 toks/s, output: 41.25 toks/s]
Processed prompts:  87%|████████▋ | 446/512 [00:10<00:01, 39.13it/s, est. speed input: 42223.74 toks/s, output: 41.23 toks/s]
Processed prompts:  88%|████████▊ | 450/512 [00:10<00:01, 39.14it/s, est. speed input: 42203.93 toks/s, output: 41.21 toks/s]
Processed prompts:  89%|████████▊ | 454/512 [00:11<00:01, 39.15it/s, est. speed input: 42184.68 toks/s, output: 41.20 toks/s]
Processed prompts:  89%|████████▉ | 458/512 [00:11<00:01, 39.18it/s, est. speed input: 42166.23 toks/s, output: 41.18 toks/s]
Processed prompts:  90%|█████████ | 462/512 [00:11<00:01, 39.17it/s, est. speed input: 42147.49 toks/s, output: 41.16 toks/s]
Processed prompts:  91%|█████████ | 466/512 [00:11<00:01, 39.18it/s, est. speed input: 42129.35 toks/s, output: 41.14 toks/s]
Processed prompts:  92%|█████████▏| 470/512 [00:11<00:01, 39.15it/s, est. speed input: 42110.31 toks/s, output: 41.12 toks/s]
Processed prompts:  93%|█████████▎| 474/512 [00:11<00:00, 39.13it/s, est. speed input: 42091.80 toks/s, output: 41.11 toks/s]
Processed prompts:  93%|█████████▎| 478/512 [00:11<00:00, 39.12it/s, est. speed input: 42073.78 toks/s, output: 41.09 toks/s]
Processed prompts:  94%|█████████▍| 482/512 [00:11<00:00, 39.14it/s, est. speed input: 42056.80 toks/s, output: 41.07 toks/s]
Processed prompts:  95%|█████████▍| 486/512 [00:11<00:00, 39.15it/s, est. speed input: 42039.94 toks/s, output: 41.05 toks/s]
Processed prompts:  96%|█████████▌| 490/512 [00:11<00:00, 39.13it/s, est. speed input: 42022.65 toks/s, output: 41.04 toks/s]
Processed prompts:  96%|█████████▋| 494/512 [00:12<00:00, 39.17it/s, est. speed input: 42007.24 toks/s, output: 41.02 toks/s]
Processed prompts:  97%|█████████▋| 498/512 [00:12<00:00, 39.17it/s, est. speed input: 41991.29 toks/s, output: 41.01 toks/s]
Processed prompts:  98%|█████████▊| 502/512 [00:12<00:00, 39.18it/s, est. speed input: 41975.85 toks/s, output: 40.99 toks/s]
Processed prompts:  99%|█████████▉| 506/512 [00:12<00:00, 39.18it/s, est. speed input: 41960.67 toks/s, output: 40.98 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:12<00:00, 39.18it/s, est. speed input: 42184.64 toks/s, output: 41.20 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:12<00:00, 41.20it/s, est. speed input: 42184.64 toks/s, output: 41.20 toks/s]
[rank0]:[W125 19:41:34.492007366 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 36.0s

测试结果:
  Requests/s:   39.03
  Tokens/s:     40004.34
  Total Reqs:   512
  Elapsed:      13.12s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     39965.31

============================================================
[5/8] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-3B-INT8                                │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 19:41:42 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=389594) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=389594) WARNING 01-25 19:41:51 [backends.py:609] Failed to read file <frozen os>
Throughput: 37.92 requests/s, 38871.93 total tokens/s, 37.92 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-25 19:41:42] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:41:42] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:41:42] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:41:42] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:41:42] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:41:42] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:41:42] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:41:42] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:41:42] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:41:42] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:41:42] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:41:42] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:41:42] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:41:42] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 19:41:46] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:41:46] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:41:46] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:41:46] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:41:46] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:41:46] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:41:46] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:41:46] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:41:46] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:41:46] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:41:46] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:41:46] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:41:46] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:41:46] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=389594) [2026-01-25 19:41:47] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=389594) [2026-01-25 19:41:47] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=389594) [2026-01-25 19:41:47] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=389594) [2026-01-25 19:41:47] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=389594) [2026-01-25 19:41:47] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=389594) [2026-01-25 19:41:47] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=389594) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=389594) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.10it/s]
(EngineCore_DP0 pid=389594) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.10it/s]
(EngineCore_DP0 pid=389594) 
(EngineCore_DP0 pid=389594) [2026-01-25 19:41:47] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 4608] -> 1D uint8
(EngineCore_DP0 pid=389594) [2026-01-25 19:41:47] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 17694720 bytes
(EngineCore_DP0 pid=389594) [2026-01-25 19:41:47] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 4608] -> 1D uint8
(EngineCore_DP0 pid=389594) [2026-01-25 19:41:47] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 10616832 bytes
(EngineCore_DP0 pid=389594) [2026-01-25 19:41:47] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 4608] -> 1D uint8
(EngineCore_DP0 pid=389594) [2026-01-25 19:41:47] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 56623104 bytes
(EngineCore_DP0 pid=389594) [2026-01-25 19:41:47] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 12288] -> 1D uint8
(EngineCore_DP0 pid=389594) [2026-01-25 19:41:47] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 28311552 bytes
(EngineCore_DP0 pid=389594) 2026-01-25 19:41:57,542 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=389594) 2026-01-25 19:41:57,557 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=389594) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 1/5 [00:00<00:00,  7.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 19.72it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 17.85it/s]
(EngineCore_DP0 pid=389594) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 1/4 [00:00<00:00,  7.39it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 17.69it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   7%|▋         | 73/1024 [00:00<00:01, 726.68it/s]
Adding requests:  14%|█▍        | 147/1024 [00:00<00:01, 731.58it/s]
Adding requests:  22%|██▏       | 222/1024 [00:00<00:01, 738.76it/s]
Adding requests:  29%|██▉       | 299/1024 [00:00<00:00, 746.45it/s]
Adding requests:  37%|███▋      | 375/1024 [00:00<00:00, 748.52it/s]
Adding requests:  44%|████▍     | 451/1024 [00:00<00:00, 751.07it/s]
Adding requests:  51%|█████▏    | 527/1024 [00:00<00:00, 736.10it/s]
Adding requests:  59%|█████▉    | 602/1024 [00:00<00:00, 737.99it/s]
Adding requests:  66%|██████▋   | 680/1024 [00:00<00:00, 750.35it/s]
Adding requests:  74%|███████▍  | 756/1024 [00:01<00:00, 752.74it/s]
Adding requests:  81%|████████▏ | 832/1024 [00:01<00:00, 739.94it/s]
Adding requests:  89%|████████▉ | 909/1024 [00:01<00:00, 746.57it/s]
Adding requests:  96%|█████████▌| 984/1024 [00:01<00:00, 746.74it/s]
Adding requests: 100%|██████████| 1024/1024 [00:01<00:00, 745.43it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   5%|▍         | 50/1024 [00:00<00:03, 277.91it/s, est. speed input: 284606.82 toks/s, output: 277.92 toks/s]
Processed prompts:   8%|▊         | 78/1024 [00:00<00:11, 83.34it/s, est. speed input: 98617.14 toks/s, output: 96.31 toks/s]   
Processed prompts:   9%|▉         | 93/1024 [00:01<00:14, 62.24it/s, est. speed input: 77405.06 toks/s, output: 75.59 toks/s]
Processed prompts:  10%|█         | 103/1024 [00:01<00:15, 58.71it/s, est. speed input: 73216.96 toks/s, output: 71.50 toks/s]
Processed prompts:  11%|█         | 111/1024 [00:01<00:17, 53.46it/s, est. speed input: 68873.99 toks/s, output: 67.26 toks/s]
Processed prompts:  12%|█▏        | 118/1024 [00:01<00:18, 48.09it/s, est. speed input: 64959.05 toks/s, output: 63.44 toks/s]
Processed prompts:  12%|█▏        | 124/1024 [00:02<00:21, 42.67it/s, est. speed input: 61323.00 toks/s, output: 59.89 toks/s]
Processed prompts:  13%|█▎        | 130/1024 [00:02<00:23, 38.68it/s, est. speed input: 58372.32 toks/s, output: 57.00 toks/s]
Processed prompts:  13%|█▎        | 138/1024 [00:02<00:23, 38.50it/s, est. speed input: 56733.18 toks/s, output: 55.40 toks/s]
Processed prompts:  14%|█▍        | 146/1024 [00:02<00:22, 38.38it/s, est. speed input: 55355.99 toks/s, output: 54.06 toks/s]
Processed prompts:  15%|█▌        | 154/1024 [00:02<00:22, 38.30it/s, est. speed input: 54175.71 toks/s, output: 52.91 toks/s]
Processed prompts:  16%|█▌        | 162/1024 [00:03<00:22, 38.23it/s, est. speed input: 53152.90 toks/s, output: 51.91 toks/s]
Processed prompts:  17%|█▋        | 170/1024 [00:03<00:22, 38.19it/s, est. speed input: 52260.70 toks/s, output: 51.04 toks/s]
Processed prompts:  17%|█▋        | 178/1024 [00:03<00:22, 38.13it/s, est. speed input: 51467.85 toks/s, output: 50.26 toks/s]
Processed prompts:  18%|█▊        | 186/1024 [00:03<00:21, 38.13it/s, est. speed input: 50773.32 toks/s, output: 49.58 toks/s]
Processed prompts:  19%|█▉        | 194/1024 [00:03<00:21, 38.13it/s, est. speed input: 50151.42 toks/s, output: 48.98 toks/s]
Processed prompts:  20%|█▉        | 202/1024 [00:04<00:21, 38.11it/s, est. speed input: 49587.88 toks/s, output: 48.43 toks/s]
Processed prompts:  21%|██        | 210/1024 [00:04<00:21, 38.08it/s, est. speed input: 49076.52 toks/s, output: 47.93 toks/s]
Processed prompts:  21%|██▏       | 218/1024 [00:04<00:21, 38.07it/s, est. speed input: 48613.71 toks/s, output: 47.47 toks/s]
Processed prompts:  22%|██▏       | 226/1024 [00:04<00:20, 38.08it/s, est. speed input: 48194.23 toks/s, output: 47.06 toks/s]
Processed prompts:  23%|██▎       | 234/1024 [00:05<00:20, 38.08it/s, est. speed input: 47807.77 toks/s, output: 46.69 toks/s]
Processed prompts:  24%|██▎       | 242/1024 [00:05<00:20, 38.08it/s, est. speed input: 47452.66 toks/s, output: 46.34 toks/s]
Processed prompts:  24%|██▍       | 250/1024 [00:05<00:20, 38.05it/s, est. speed input: 47120.58 toks/s, output: 46.02 toks/s]
Processed prompts:  25%|██▌       | 258/1024 [00:05<00:20, 38.06it/s, est. speed input: 46818.46 toks/s, output: 45.72 toks/s]
Processed prompts:  26%|██▌       | 266/1024 [00:05<00:19, 38.07it/s, est. speed input: 46537.89 toks/s, output: 45.45 toks/s]
Processed prompts:  27%|██▋       | 274/1024 [00:06<00:19, 38.06it/s, est. speed input: 46274.31 toks/s, output: 45.19 toks/s]
Processed prompts:  28%|██▊       | 282/1024 [00:06<00:19, 38.03it/s, est. speed input: 46026.74 toks/s, output: 44.95 toks/s]
Processed prompts:  28%|██▊       | 290/1024 [00:06<00:19, 38.02it/s, est. speed input: 45794.60 toks/s, output: 44.72 toks/s]
Processed prompts:  29%|██▉       | 298/1024 [00:06<00:19, 38.04it/s, est. speed input: 45581.69 toks/s, output: 44.51 toks/s]
Processed prompts:  30%|██▉       | 306/1024 [00:06<00:18, 38.06it/s, est. speed input: 45382.70 toks/s, output: 44.32 toks/s]
Processed prompts:  31%|███       | 314/1024 [00:07<00:18, 38.06it/s, est. speed input: 45193.01 toks/s, output: 44.13 toks/s]
Processed prompts:  31%|███▏      | 322/1024 [00:07<00:18, 38.06it/s, est. speed input: 45014.43 toks/s, output: 43.96 toks/s]
Processed prompts:  32%|███▏      | 330/1024 [00:07<00:18, 38.04it/s, est. speed input: 44843.52 toks/s, output: 43.79 toks/s]
Processed prompts:  33%|███▎      | 338/1024 [00:07<00:18, 38.05it/s, est. speed input: 44684.37 toks/s, output: 43.64 toks/s]
Processed prompts:  34%|███▍      | 346/1024 [00:07<00:17, 38.06it/s, est. speed input: 44534.45 toks/s, output: 43.49 toks/s]
Processed prompts:  35%|███▍      | 354/1024 [00:08<00:17, 38.06it/s, est. speed input: 44391.78 toks/s, output: 43.35 toks/s]
Processed prompts:  35%|███▌      | 362/1024 [00:08<00:17, 38.03it/s, est. speed input: 44252.63 toks/s, output: 43.22 toks/s]
Processed prompts:  36%|███▌      | 370/1024 [00:08<00:17, 38.04it/s, est. speed input: 44123.46 toks/s, output: 43.09 toks/s]
Processed prompts:  37%|███▋      | 378/1024 [00:08<00:16, 38.03it/s, est. speed input: 43999.32 toks/s, output: 42.97 toks/s]
Processed prompts:  38%|███▊      | 386/1024 [00:09<00:16, 38.05it/s, est. speed input: 43882.81 toks/s, output: 42.85 toks/s]
Processed prompts:  38%|███▊      | 394/1024 [00:09<00:16, 38.03it/s, est. speed input: 43769.04 toks/s, output: 42.74 toks/s]
Processed prompts:  39%|███▉      | 402/1024 [00:09<00:16, 38.02it/s, est. speed input: 43660.53 toks/s, output: 42.64 toks/s]
Processed prompts:  40%|████      | 410/1024 [00:09<00:16, 38.02it/s, est. speed input: 43557.45 toks/s, output: 42.54 toks/s]
Processed prompts:  41%|████      | 418/1024 [00:09<00:15, 38.02it/s, est. speed input: 43458.51 toks/s, output: 42.44 toks/s]
Processed prompts:  42%|████▏     | 426/1024 [00:10<00:15, 38.04it/s, est. speed input: 43365.39 toks/s, output: 42.35 toks/s]
Processed prompts:  42%|████▏     | 434/1024 [00:10<00:15, 38.05it/s, est. speed input: 43275.58 toks/s, output: 42.26 toks/s]
Processed prompts:  43%|████▎     | 442/1024 [00:10<00:15, 38.01it/s, est. speed input: 43186.21 toks/s, output: 42.17 toks/s]
Processed prompts:  44%|████▍     | 450/1024 [00:10<00:15, 38.03it/s, est. speed input: 43103.54 toks/s, output: 42.09 toks/s]
Processed prompts:  45%|████▍     | 458/1024 [00:10<00:14, 38.03it/s, est. speed input: 43023.23 toks/s, output: 42.01 toks/s]
Processed prompts:  46%|████▌     | 466/1024 [00:11<00:14, 38.03it/s, est. speed input: 42946.09 toks/s, output: 41.94 toks/s]
Processed prompts:  46%|████▋     | 474/1024 [00:11<00:14, 38.02it/s, est. speed input: 42871.05 toks/s, output: 41.87 toks/s]
Processed prompts:  47%|████▋     | 482/1024 [00:11<00:14, 38.01it/s, est. speed input: 42798.26 toks/s, output: 41.80 toks/s]
Processed prompts:  48%|████▊     | 490/1024 [00:11<00:14, 38.00it/s, est. speed input: 42728.57 toks/s, output: 41.73 toks/s]
Processed prompts:  49%|████▊     | 498/1024 [00:11<00:13, 38.01it/s, est. speed input: 42661.90 toks/s, output: 41.66 toks/s]
Processed prompts:  49%|████▉     | 506/1024 [00:12<00:13, 38.02it/s, est. speed input: 42597.68 toks/s, output: 41.60 toks/s]
Processed prompts:  50%|█████     | 514/1024 [00:12<00:13, 38.00it/s, est. speed input: 42533.95 toks/s, output: 41.54 toks/s]
Processed prompts:  51%|█████     | 522/1024 [00:12<00:13, 37.99it/s, est. speed input: 42472.74 toks/s, output: 41.48 toks/s]
Processed prompts:  52%|█████▏    | 530/1024 [00:12<00:12, 38.01it/s, est. speed input: 42415.10 toks/s, output: 41.42 toks/s]
Processed prompts:  53%|█████▎    | 538/1024 [00:13<00:12, 38.00it/s, est. speed input: 42358.17 toks/s, output: 41.37 toks/s]
Processed prompts:  53%|█████▎    | 546/1024 [00:13<00:12, 38.00it/s, est. speed input: 42302.95 toks/s, output: 41.31 toks/s]
Processed prompts:  54%|█████▍    | 554/1024 [00:13<00:12, 37.98it/s, est. speed input: 42249.05 toks/s, output: 41.26 toks/s]
Processed prompts:  55%|█████▍    | 562/1024 [00:13<00:12, 37.98it/s, est. speed input: 42197.07 toks/s, output: 41.21 toks/s]
Processed prompts:  56%|█████▌    | 570/1024 [00:13<00:11, 37.99it/s, est. speed input: 42147.46 toks/s, output: 41.16 toks/s]
Processed prompts:  56%|█████▋    | 578/1024 [00:14<00:11, 37.99it/s, est. speed input: 42098.73 toks/s, output: 41.11 toks/s]
Processed prompts:  57%|█████▋    | 586/1024 [00:14<00:11, 37.99it/s, est. speed input: 42051.65 toks/s, output: 41.07 toks/s]
Processed prompts:  58%|█████▊    | 594/1024 [00:14<00:11, 37.98it/s, est. speed input: 42005.00 toks/s, output: 41.02 toks/s]
Processed prompts:  59%|█████▉    | 602/1024 [00:14<00:11, 37.98it/s, est. speed input: 41960.44 toks/s, output: 40.98 toks/s]
Processed prompts:  60%|█████▉    | 610/1024 [00:14<00:10, 37.97it/s, est. speed input: 41916.58 toks/s, output: 40.93 toks/s]
Processed prompts:  60%|██████    | 618/1024 [00:15<00:10, 37.97it/s, est. speed input: 41874.35 toks/s, output: 40.89 toks/s]
Processed prompts:  61%|██████    | 626/1024 [00:15<00:10, 37.97it/s, est. speed input: 41833.23 toks/s, output: 40.85 toks/s]
Processed prompts:  62%|██████▏   | 634/1024 [00:15<00:10, 37.96it/s, est. speed input: 41792.43 toks/s, output: 40.81 toks/s]
Processed prompts:  63%|██████▎   | 642/1024 [00:15<00:10, 37.97it/s, est. speed input: 41753.75 toks/s, output: 40.78 toks/s]
Processed prompts:  63%|██████▎   | 650/1024 [00:15<00:09, 37.98it/s, est. speed input: 41716.47 toks/s, output: 40.74 toks/s]
Processed prompts:  64%|██████▍   | 658/1024 [00:16<00:09, 38.00it/s, est. speed input: 41680.45 toks/s, output: 40.70 toks/s]
Processed prompts:  65%|██████▌   | 666/1024 [00:16<00:09, 37.99it/s, est. speed input: 41644.45 toks/s, output: 40.67 toks/s]
Processed prompts:  66%|██████▌   | 674/1024 [00:16<00:09, 37.97it/s, est. speed input: 41608.91 toks/s, output: 40.63 toks/s]
Processed prompts:  67%|██████▋   | 682/1024 [00:16<00:09, 37.97it/s, est. speed input: 41574.45 toks/s, output: 40.60 toks/s]
Processed prompts:  67%|██████▋   | 690/1024 [00:17<00:08, 37.98it/s, est. speed input: 41541.68 toks/s, output: 40.57 toks/s]
Processed prompts:  68%|██████▊   | 698/1024 [00:17<00:08, 37.98it/s, est. speed input: 41509.12 toks/s, output: 40.54 toks/s]
Processed prompts:  69%|██████▉   | 706/1024 [00:17<00:08, 37.97it/s, est. speed input: 41476.98 toks/s, output: 40.50 toks/s]
Processed prompts:  70%|██████▉   | 714/1024 [00:17<00:08, 37.98it/s, est. speed input: 41446.50 toks/s, output: 40.48 toks/s]
Processed prompts:  71%|███████   | 722/1024 [00:17<00:07, 37.97it/s, est. speed input: 41415.91 toks/s, output: 40.45 toks/s]
Processed prompts:  71%|███████▏  | 730/1024 [00:18<00:07, 37.97it/s, est. speed input: 41386.14 toks/s, output: 40.42 toks/s]
Processed prompts:  72%|███████▏  | 738/1024 [00:18<00:07, 37.96it/s, est. speed input: 41356.87 toks/s, output: 40.39 toks/s]
Processed prompts:  73%|███████▎  | 746/1024 [00:18<00:07, 37.94it/s, est. speed input: 41327.83 toks/s, output: 40.36 toks/s]
Processed prompts:  74%|███████▎  | 754/1024 [00:18<00:07, 37.94it/s, est. speed input: 41299.67 toks/s, output: 40.33 toks/s]
Processed prompts:  74%|███████▍  | 762/1024 [00:18<00:06, 37.94it/s, est. speed input: 41272.59 toks/s, output: 40.31 toks/s]
Processed prompts:  75%|███████▌  | 770/1024 [00:19<00:06, 37.96it/s, est. speed input: 41246.48 toks/s, output: 40.28 toks/s]
Processed prompts:  76%|███████▌  | 778/1024 [00:19<00:06, 37.95it/s, est. speed input: 41220.33 toks/s, output: 40.25 toks/s]
Processed prompts:  77%|███████▋  | 786/1024 [00:19<00:06, 37.95it/s, est. speed input: 41194.76 toks/s, output: 40.23 toks/s]
Processed prompts:  78%|███████▊  | 794/1024 [00:19<00:06, 37.97it/s, est. speed input: 41170.68 toks/s, output: 40.21 toks/s]
Processed prompts:  78%|███████▊  | 802/1024 [00:19<00:05, 37.97it/s, est. speed input: 41146.37 toks/s, output: 40.18 toks/s]
Processed prompts:  79%|███████▉  | 810/1024 [00:20<00:05, 37.97it/s, est. speed input: 41122.66 toks/s, output: 40.16 toks/s]
Processed prompts:  80%|███████▉  | 818/1024 [00:20<00:05, 37.96it/s, est. speed input: 41099.16 toks/s, output: 40.14 toks/s]
Processed prompts:  81%|████████  | 826/1024 [00:20<00:05, 37.97it/s, est. speed input: 41076.68 toks/s, output: 40.11 toks/s]
Processed prompts:  81%|████████▏ | 834/1024 [00:20<00:05, 37.95it/s, est. speed input: 41053.93 toks/s, output: 40.09 toks/s]
Processed prompts:  82%|████████▏ | 842/1024 [00:21<00:04, 37.95it/s, est. speed input: 41031.98 toks/s, output: 40.07 toks/s]
Processed prompts:  83%|████████▎ | 850/1024 [00:21<00:04, 37.94it/s, est. speed input: 41010.09 toks/s, output: 40.05 toks/s]
Processed prompts:  84%|████████▍ | 858/1024 [00:21<00:04, 37.94it/s, est. speed input: 40988.80 toks/s, output: 40.03 toks/s]
Processed prompts:  85%|████████▍ | 866/1024 [00:21<00:04, 37.95it/s, est. speed input: 40968.20 toks/s, output: 40.01 toks/s]
Processed prompts:  85%|████████▌ | 874/1024 [00:21<00:03, 37.94it/s, est. speed input: 40947.73 toks/s, output: 39.99 toks/s]
Processed prompts:  86%|████████▌ | 882/1024 [00:22<00:03, 37.95it/s, est. speed input: 40927.94 toks/s, output: 39.97 toks/s]
Processed prompts:  87%|████████▋ | 890/1024 [00:22<00:03, 37.97it/s, est. speed input: 40908.98 toks/s, output: 39.95 toks/s]
Processed prompts:  88%|████████▊ | 898/1024 [00:22<00:03, 37.96it/s, est. speed input: 40889.65 toks/s, output: 39.93 toks/s]
Processed prompts:  88%|████████▊ | 906/1024 [00:22<00:03, 37.95it/s, est. speed input: 40870.78 toks/s, output: 39.91 toks/s]
Processed prompts:  89%|████████▉ | 914/1024 [00:22<00:02, 37.96it/s, est. speed input: 40852.44 toks/s, output: 39.89 toks/s]
Processed prompts:  90%|█████████ | 922/1024 [00:23<00:02, 37.95it/s, est. speed input: 40834.16 toks/s, output: 39.88 toks/s]
Processed prompts:  91%|█████████ | 930/1024 [00:23<00:02, 37.95it/s, est. speed input: 40816.22 toks/s, output: 39.86 toks/s]
Processed prompts:  92%|█████████▏| 938/1024 [00:23<00:02, 37.93it/s, est. speed input: 40798.26 toks/s, output: 39.84 toks/s]
Processed prompts:  92%|█████████▏| 946/1024 [00:23<00:02, 37.95it/s, est. speed input: 40781.26 toks/s, output: 39.83 toks/s]
Processed prompts:  93%|█████████▎| 954/1024 [00:23<00:01, 37.95it/s, est. speed input: 40764.42 toks/s, output: 39.81 toks/s]
Processed prompts:  94%|█████████▍| 962/1024 [00:24<00:01, 37.93it/s, est. speed input: 40747.31 toks/s, output: 39.79 toks/s]
Processed prompts:  95%|█████████▍| 970/1024 [00:24<00:01, 37.91it/s, est. speed input: 40730.26 toks/s, output: 39.78 toks/s]
Processed prompts:  96%|█████████▌| 978/1024 [00:24<00:01, 37.94it/s, est. speed input: 40714.64 toks/s, output: 39.76 toks/s]
Processed prompts:  96%|█████████▋| 986/1024 [00:24<00:01, 37.93it/s, est. speed input: 40698.53 toks/s, output: 39.74 toks/s]
Processed prompts:  97%|█████████▋| 994/1024 [00:25<00:00, 37.93it/s, est. speed input: 40683.02 toks/s, output: 39.73 toks/s]
Processed prompts:  98%|█████████▊| 1002/1024 [00:25<00:00, 37.93it/s, est. speed input: 40667.47 toks/s, output: 39.71 toks/s]
Processed prompts:  99%|█████████▊| 1010/1024 [00:25<00:00, 37.91it/s, est. speed input: 40651.74 toks/s, output: 39.70 toks/s]
Processed prompts:  99%|█████████▉| 1018/1024 [00:25<00:00, 39.34it/s, est. speed input: 40677.26 toks/s, output: 39.72 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:25<00:00, 39.34it/s, est. speed input: 40916.85 toks/s, output: 39.96 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:25<00:00, 39.96it/s, est. speed input: 40916.85 toks/s, output: 39.96 toks/s]
[rank0]:[W125 19:42:26.971954024 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 51.5s

测试结果:
  Requests/s:   37.92
  Tokens/s:     38871.93
  Total Reqs:   1024
  Elapsed:      27.00s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     38834.00

============================================================
[6/8] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-3B-INT8                                │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 19:42:36 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=390619) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=390619) WARNING 01-25 19:42:46 [backends.py:609] Failed to read file <frozen os>
Throughput: 37.56 requests/s, 38498.86 total tokens/s, 37.56 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-25 19:42:36] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:42:36] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:42:36] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:42:36] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:42:36] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:42:36] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:42:36] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:42:36] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:42:36] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:42:36] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:42:36] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:42:36] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:42:36] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:42:36] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 19:42:40] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:42:40] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:42:40] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:42:40] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:42:40] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:42:40] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:42:40] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:42:40] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:42:40] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:42:40] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:42:40] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:42:40] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:42:40] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:42:40] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=390619) [2026-01-25 19:42:41] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=390619) [2026-01-25 19:42:41] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=390619) [2026-01-25 19:42:41] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=390619) [2026-01-25 19:42:41] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=390619) [2026-01-25 19:42:41] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=390619) [2026-01-25 19:42:41] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=390619) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=390619) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.07it/s]
(EngineCore_DP0 pid=390619) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.07it/s]
(EngineCore_DP0 pid=390619) 
(EngineCore_DP0 pid=390619) [2026-01-25 19:42:41] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 4608] -> 1D uint8
(EngineCore_DP0 pid=390619) [2026-01-25 19:42:42] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 17694720 bytes
(EngineCore_DP0 pid=390619) [2026-01-25 19:42:42] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 4608] -> 1D uint8
(EngineCore_DP0 pid=390619) [2026-01-25 19:42:42] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 10616832 bytes
(EngineCore_DP0 pid=390619) [2026-01-25 19:42:42] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 4608] -> 1D uint8
(EngineCore_DP0 pid=390619) [2026-01-25 19:42:42] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 56623104 bytes
(EngineCore_DP0 pid=390619) [2026-01-25 19:42:42] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 12288] -> 1D uint8
(EngineCore_DP0 pid=390619) [2026-01-25 19:42:42] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 28311552 bytes
(EngineCore_DP0 pid=390619) 2026-01-25 19:42:52,217 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=390619) 2026-01-25 19:42:52,232 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=390619) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 1/7 [00:00<00:01,  5.13it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00, 15.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 20.26it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 17.24it/s]
(EngineCore_DP0 pid=390619) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 1/5 [00:00<00:00,  7.53it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 21.91it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 19.65it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   3%|▎         | 70/2048 [00:00<00:02, 699.29it/s]
Adding requests:   7%|▋         | 144/2048 [00:00<00:02, 720.71it/s]
Adding requests:  11%|█         | 217/2048 [00:00<00:02, 724.51it/s]
Adding requests:  14%|█▍        | 293/2048 [00:00<00:02, 734.79it/s]
Adding requests:  18%|█▊        | 368/2048 [00:00<00:02, 737.84it/s]
Adding requests:  22%|██▏       | 442/2048 [00:00<00:02, 736.46it/s]
Adding requests:  25%|██▌       | 516/2048 [00:00<00:02, 710.77it/s]
Adding requests:  29%|██▉       | 590/2048 [00:00<00:02, 717.83it/s]
Adding requests:  32%|███▏      | 665/2048 [00:00<00:01, 724.96it/s]
Adding requests:  36%|███▋      | 743/2048 [00:01<00:01, 739.01it/s]
Adding requests:  40%|███▉      | 817/2048 [00:01<00:01, 718.62it/s]
Adding requests:  44%|████▎     | 894/2048 [00:01<00:01, 731.07it/s]
Adding requests:  47%|████▋     | 972/2048 [00:01<00:01, 743.87it/s]
Adding requests:  51%|█████▏    | 1050/2048 [00:01<00:01, 751.63it/s]
Adding requests:  55%|█████▍    | 1126/2048 [00:01<00:01, 748.27it/s]
Adding requests:  59%|█████▉    | 1204/2048 [00:01<00:01, 757.08it/s]
Adding requests:  62%|██████▎   | 1280/2048 [00:01<00:01, 749.03it/s]
Adding requests:  66%|██████▋   | 1358/2048 [00:01<00:00, 755.75it/s]
Adding requests:  70%|███████   | 1436/2048 [00:01<00:00, 759.89it/s]
Adding requests:  74%|███████▍  | 1517/2048 [00:02<00:00, 771.82it/s]
Adding requests:  78%|███████▊  | 1597/2048 [00:02<00:00, 779.02it/s]
Adding requests:  82%|████████▏ | 1675/2048 [00:02<00:00, 777.11it/s]
Adding requests:  86%|████████▌ | 1753/2048 [00:02<00:00, 770.27it/s]
Adding requests:  89%|████████▉ | 1831/2048 [00:02<00:00, 761.99it/s]
Adding requests:  93%|█████████▎| 1908/2048 [00:02<00:00, 763.54it/s]
Adding requests:  97%|█████████▋| 1985/2048 [00:02<00:00, 748.53it/s]
Adding requests: 100%|██████████| 2048/2048 [00:02<00:00, 746.60it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   5%|▍         | 98/2048 [00:00<00:06, 283.49it/s, est. speed input: 290306.78 toks/s, output: 283.50 toks/s]
Processed prompts:   6%|▌         | 127/2048 [00:00<00:13, 146.61it/s, est. speed input: 169025.35 toks/s, output: 165.06 toks/s]
Processed prompts:   7%|▋         | 143/2048 [00:01<00:20, 94.72it/s, est. speed input: 122679.50 toks/s, output: 119.80 toks/s] 
Processed prompts:   8%|▊         | 154/2048 [00:01<00:28, 66.90it/s, est. speed input: 97510.99 toks/s, output: 95.23 toks/s]  
Processed prompts:   8%|▊         | 162/2048 [00:02<00:38, 49.30it/s, est. speed input: 81273.37 toks/s, output: 79.37 toks/s]
Processed prompts:   9%|▊         | 178/2048 [00:02<00:41, 45.31it/s, est. speed input: 73932.66 toks/s, output: 72.20 toks/s]
Processed prompts:   9%|▉         | 194/2048 [00:02<00:43, 42.82it/s, est. speed input: 68752.32 toks/s, output: 67.14 toks/s]
Processed prompts:  10%|█         | 210/2048 [00:03<00:44, 41.18it/s, est. speed input: 64894.32 toks/s, output: 63.37 toks/s]
Processed prompts:  11%|█         | 226/2048 [00:03<00:45, 40.10it/s, est. speed input: 61913.27 toks/s, output: 60.46 toks/s]
Processed prompts:  12%|█▏        | 242/2048 [00:04<00:45, 39.35it/s, est. speed input: 59534.76 toks/s, output: 58.14 toks/s]
Processed prompts:  13%|█▎        | 258/2048 [00:04<00:46, 38.85it/s, est. speed input: 57601.71 toks/s, output: 56.25 toks/s]
Processed prompts:  13%|█▎        | 274/2048 [00:05<00:46, 38.51it/s, est. speed input: 55995.96 toks/s, output: 54.68 toks/s]
Processed prompts:  14%|█▍        | 290/2048 [00:05<00:45, 38.26it/s, est. speed input: 54636.91 toks/s, output: 53.36 toks/s]
Processed prompts:  15%|█▍        | 306/2048 [00:05<00:45, 38.01it/s, est. speed input: 53449.62 toks/s, output: 52.20 toks/s]
Processed prompts:  16%|█▌        | 322/2048 [00:06<00:45, 37.92it/s, est. speed input: 52447.33 toks/s, output: 51.22 toks/s]
Processed prompts:  17%|█▋        | 338/2048 [00:06<00:45, 37.86it/s, est. speed input: 51573.60 toks/s, output: 50.36 toks/s]
Processed prompts:  17%|█▋        | 354/2048 [00:07<00:44, 37.81it/s, est. speed input: 50801.53 toks/s, output: 49.61 toks/s]
Processed prompts:  18%|█▊        | 370/2048 [00:07<00:44, 37.78it/s, est. speed input: 50118.70 toks/s, output: 48.94 toks/s]
Processed prompts:  19%|█▉        | 386/2048 [00:07<00:44, 37.76it/s, est. speed input: 49506.31 toks/s, output: 48.35 toks/s]
Processed prompts:  20%|█▉        | 402/2048 [00:08<00:43, 37.73it/s, est. speed input: 48953.17 toks/s, output: 47.81 toks/s]
Processed prompts:  20%|██        | 418/2048 [00:08<00:43, 37.71it/s, est. speed input: 48454.51 toks/s, output: 47.32 toks/s]
Processed prompts:  21%|██        | 434/2048 [00:09<00:42, 37.70it/s, est. speed input: 48000.79 toks/s, output: 46.88 toks/s]
Processed prompts:  22%|██▏       | 450/2048 [00:09<00:42, 37.69it/s, est. speed input: 47588.36 toks/s, output: 46.47 toks/s]
Processed prompts:  23%|██▎       | 466/2048 [00:10<00:41, 37.69it/s, est. speed input: 47209.35 toks/s, output: 46.10 toks/s]
Processed prompts:  24%|██▎       | 482/2048 [00:10<00:41, 37.68it/s, est. speed input: 46861.51 toks/s, output: 45.76 toks/s]
Processed prompts:  24%|██▍       | 498/2048 [00:10<00:41, 37.67it/s, est. speed input: 46538.92 toks/s, output: 45.45 toks/s]
Processed prompts:  25%|██▌       | 514/2048 [00:11<00:40, 37.66it/s, est. speed input: 46240.82 toks/s, output: 45.16 toks/s]
Processed prompts:  26%|██▌       | 530/2048 [00:11<00:40, 37.68it/s, est. speed input: 45966.33 toks/s, output: 44.89 toks/s]
Processed prompts:  27%|██▋       | 546/2048 [00:12<00:39, 37.67it/s, est. speed input: 45708.57 toks/s, output: 44.64 toks/s]
Processed prompts:  27%|██▋       | 562/2048 [00:12<00:39, 37.66it/s, est. speed input: 45468.79 toks/s, output: 44.40 toks/s]
Processed prompts:  28%|██▊       | 578/2048 [00:13<00:39, 37.65it/s, est. speed input: 45243.26 toks/s, output: 44.18 toks/s]
Processed prompts:  29%|██▉       | 594/2048 [00:13<00:38, 37.65it/s, est. speed input: 45032.51 toks/s, output: 43.98 toks/s]
Processed prompts:  30%|██▉       | 610/2048 [00:13<00:38, 37.66it/s, est. speed input: 44836.17 toks/s, output: 43.79 toks/s]
Processed prompts:  31%|███       | 626/2048 [00:14<00:37, 37.65it/s, est. speed input: 44649.14 toks/s, output: 43.60 toks/s]
Processed prompts:  31%|███▏      | 642/2048 [00:14<00:37, 37.65it/s, est. speed input: 44473.85 toks/s, output: 43.43 toks/s]
Processed prompts:  32%|███▏      | 658/2048 [00:15<00:36, 37.64it/s, est. speed input: 44307.50 toks/s, output: 43.27 toks/s]
Processed prompts:  33%|███▎      | 674/2048 [00:15<00:36, 37.65it/s, est. speed input: 44151.62 toks/s, output: 43.12 toks/s]
Processed prompts:  34%|███▎      | 690/2048 [00:16<00:36, 37.64it/s, est. speed input: 44002.05 toks/s, output: 42.97 toks/s]
Processed prompts:  34%|███▍      | 706/2048 [00:16<00:35, 37.63it/s, est. speed input: 43860.37 toks/s, output: 42.83 toks/s]
Processed prompts:  35%|███▌      | 722/2048 [00:16<00:35, 37.63it/s, est. speed input: 43726.97 toks/s, output: 42.70 toks/s]
Processed prompts:  36%|███▌      | 738/2048 [00:17<00:34, 37.63it/s, est. speed input: 43598.82 toks/s, output: 42.58 toks/s]
Processed prompts:  37%|███▋      | 754/2048 [00:17<00:34, 37.63it/s, est. speed input: 43478.16 toks/s, output: 42.46 toks/s]
Processed prompts:  38%|███▊      | 770/2048 [00:18<00:33, 37.63it/s, est. speed input: 43362.34 toks/s, output: 42.35 toks/s]
Processed prompts:  38%|███▊      | 786/2048 [00:18<00:33, 37.63it/s, est. speed input: 43252.10 toks/s, output: 42.24 toks/s]
Processed prompts:  39%|███▉      | 802/2048 [00:19<00:33, 37.64it/s, est. speed input: 43147.23 toks/s, output: 42.14 toks/s]
Processed prompts:  40%|███▉      | 818/2048 [00:19<00:32, 37.62it/s, est. speed input: 43044.76 toks/s, output: 42.04 toks/s]
Processed prompts:  41%|████      | 834/2048 [00:19<00:32, 37.62it/s, est. speed input: 42948.57 toks/s, output: 41.94 toks/s]
Processed prompts:  42%|████▏     | 850/2048 [00:20<00:31, 37.61it/s, est. speed input: 42855.05 toks/s, output: 41.85 toks/s]
Processed prompts:  42%|████▏     | 866/2048 [00:20<00:31, 37.61it/s, est. speed input: 42766.09 toks/s, output: 41.76 toks/s]
Processed prompts:  43%|████▎     | 882/2048 [00:21<00:31, 37.61it/s, est. speed input: 42680.42 toks/s, output: 41.68 toks/s]
Processed prompts:  44%|████▍     | 898/2048 [00:21<00:30, 37.61it/s, est. speed input: 42598.34 toks/s, output: 41.60 toks/s]
Processed prompts:  45%|████▍     | 914/2048 [00:22<00:30, 37.61it/s, est. speed input: 42519.63 toks/s, output: 41.52 toks/s]
Processed prompts:  45%|████▌     | 930/2048 [00:22<00:29, 37.61it/s, est. speed input: 42443.15 toks/s, output: 41.45 toks/s]
Processed prompts:  46%|████▌     | 946/2048 [00:22<00:29, 37.61it/s, est. speed input: 42370.25 toks/s, output: 41.38 toks/s]
Processed prompts:  47%|████▋     | 962/2048 [00:23<00:28, 37.61it/s, est. speed input: 42299.86 toks/s, output: 41.31 toks/s]
Processed prompts:  48%|████▊     | 978/2048 [00:23<00:28, 37.62it/s, est. speed input: 42232.19 toks/s, output: 41.24 toks/s]
Processed prompts:  49%|████▊     | 994/2048 [00:24<00:28, 37.61it/s, est. speed input: 42166.22 toks/s, output: 41.18 toks/s]
Processed prompts:  49%|████▉     | 1010/2048 [00:24<00:27, 37.61it/s, est. speed input: 42102.99 toks/s, output: 41.12 toks/s]
Processed prompts:  50%|█████     | 1026/2048 [00:24<00:27, 37.60it/s, est. speed input: 42041.48 toks/s, output: 41.06 toks/s]
Processed prompts:  51%|█████     | 1042/2048 [00:25<00:26, 37.60it/s, est. speed input: 41982.11 toks/s, output: 41.00 toks/s]
Processed prompts:  52%|█████▏    | 1058/2048 [00:25<00:26, 37.60it/s, est. speed input: 41925.08 toks/s, output: 40.94 toks/s]
Processed prompts:  52%|█████▏    | 1074/2048 [00:26<00:25, 37.60it/s, est. speed input: 41869.32 toks/s, output: 40.89 toks/s]
Processed prompts:  53%|█████▎    | 1090/2048 [00:26<00:25, 37.59it/s, est. speed input: 41815.40 toks/s, output: 40.84 toks/s]
Processed prompts:  54%|█████▍    | 1106/2048 [00:27<00:25, 37.59it/s, est. speed input: 41763.09 toks/s, output: 40.78 toks/s]
Processed prompts:  55%|█████▍    | 1122/2048 [00:27<00:24, 37.59it/s, est. speed input: 41712.23 toks/s, output: 40.73 toks/s]
Processed prompts:  56%|█████▌    | 1138/2048 [00:27<00:24, 37.59it/s, est. speed input: 41663.40 toks/s, output: 40.69 toks/s]
Processed prompts:  56%|█████▋    | 1154/2048 [00:28<00:23, 37.59it/s, est. speed input: 41615.89 toks/s, output: 40.64 toks/s]
Processed prompts:  57%|█████▋    | 1170/2048 [00:28<00:23, 37.59it/s, est. speed input: 41569.71 toks/s, output: 40.60 toks/s]
Processed prompts:  58%|█████▊    | 1186/2048 [00:29<00:22, 37.58it/s, est. speed input: 41524.56 toks/s, output: 40.55 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [00:29<00:22, 37.59it/s, est. speed input: 41481.17 toks/s, output: 40.51 toks/s]
Processed prompts:  59%|█████▉    | 1218/2048 [00:30<00:22, 37.59it/s, est. speed input: 41438.95 toks/s, output: 40.47 toks/s]
Processed prompts:  60%|██████    | 1234/2048 [00:30<00:21, 37.59it/s, est. speed input: 41397.82 toks/s, output: 40.43 toks/s]
Processed prompts:  61%|██████    | 1250/2048 [00:30<00:21, 37.59it/s, est. speed input: 41357.71 toks/s, output: 40.39 toks/s]
Processed prompts:  62%|██████▏   | 1266/2048 [00:31<00:20, 37.58it/s, est. speed input: 41318.35 toks/s, output: 40.35 toks/s]
Processed prompts:  63%|██████▎   | 1282/2048 [00:31<00:20, 37.58it/s, est. speed input: 41280.34 toks/s, output: 40.31 toks/s]
Processed prompts:  63%|██████▎   | 1298/2048 [00:32<00:19, 37.57it/s, est. speed input: 41243.15 toks/s, output: 40.28 toks/s]
Processed prompts:  64%|██████▍   | 1314/2048 [00:32<00:19, 37.58it/s, est. speed input: 41207.27 toks/s, output: 40.24 toks/s]
Processed prompts:  65%|██████▍   | 1330/2048 [00:33<00:19, 37.58it/s, est. speed input: 41172.25 toks/s, output: 40.21 toks/s]
Processed prompts:  66%|██████▌   | 1346/2048 [00:33<00:18, 37.57it/s, est. speed input: 41137.79 toks/s, output: 40.17 toks/s]
Processed prompts:  67%|██████▋   | 1362/2048 [00:33<00:18, 37.57it/s, est. speed input: 41104.03 toks/s, output: 40.14 toks/s]
Processed prompts:  67%|██████▋   | 1378/2048 [00:34<00:17, 37.56it/s, est. speed input: 41070.85 toks/s, output: 40.11 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [00:34<00:17, 37.57it/s, est. speed input: 41039.39 toks/s, output: 40.08 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [00:35<00:16, 37.56it/s, est. speed input: 41008.12 toks/s, output: 40.05 toks/s]
Processed prompts:  70%|██████▉   | 1426/2048 [00:35<00:16, 37.57it/s, est. speed input: 40978.01 toks/s, output: 40.02 toks/s]
Processed prompts:  70%|███████   | 1442/2048 [00:36<00:16, 37.57it/s, est. speed input: 40948.30 toks/s, output: 39.99 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [00:36<00:15, 37.57it/s, est. speed input: 40919.41 toks/s, output: 39.96 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [00:36<00:15, 37.57it/s, est. speed input: 40891.10 toks/s, output: 39.93 toks/s]
Processed prompts:  73%|███████▎  | 1490/2048 [00:37<00:14, 37.56it/s, est. speed input: 40863.09 toks/s, output: 39.91 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [00:37<00:14, 37.56it/s, est. speed input: 40835.99 toks/s, output: 39.88 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [00:38<00:14, 37.55it/s, est. speed input: 40809.15 toks/s, output: 39.85 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [00:38<00:13, 37.56it/s, est. speed input: 40783.36 toks/s, output: 39.83 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [00:39<00:13, 37.57it/s, est. speed input: 40758.37 toks/s, output: 39.80 toks/s]
Processed prompts:  77%|███████▋  | 1570/2048 [00:39<00:12, 37.56it/s, est. speed input: 40733.37 toks/s, output: 39.78 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [00:39<00:12, 37.56it/s, est. speed input: 40709.16 toks/s, output: 39.76 toks/s]
Processed prompts:  78%|███████▊  | 1602/2048 [00:40<00:11, 37.56it/s, est. speed input: 40685.24 toks/s, output: 39.73 toks/s]
Processed prompts:  79%|███████▉  | 1618/2048 [00:40<00:11, 37.55it/s, est. speed input: 40661.91 toks/s, output: 39.71 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [00:41<00:11, 37.55it/s, est. speed input: 40638.78 toks/s, output: 39.69 toks/s]
Processed prompts:  81%|████████  | 1650/2048 [00:41<00:10, 37.54it/s, est. speed input: 40616.09 toks/s, output: 39.66 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [00:42<00:10, 37.55it/s, est. speed input: 40594.24 toks/s, output: 39.64 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [00:42<00:09, 37.54it/s, est. speed input: 40572.31 toks/s, output: 39.62 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [00:42<00:09, 37.54it/s, est. speed input: 40551.26 toks/s, output: 39.60 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [00:43<00:08, 37.54it/s, est. speed input: 40530.42 toks/s, output: 39.58 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [00:43<00:08, 37.54it/s, est. speed input: 40510.26 toks/s, output: 39.56 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [00:44<00:08, 37.54it/s, est. speed input: 40490.01 toks/s, output: 39.54 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [00:44<00:07, 37.54it/s, est. speed input: 40470.64 toks/s, output: 39.52 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [00:45<00:07, 37.55it/s, est. speed input: 40451.63 toks/s, output: 39.50 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [00:45<00:06, 37.54it/s, est. speed input: 40432.58 toks/s, output: 39.48 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [00:45<00:06, 37.53it/s, est. speed input: 40413.59 toks/s, output: 39.47 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [00:46<00:05, 37.54it/s, est. speed input: 40395.60 toks/s, output: 39.45 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [00:46<00:05, 37.54it/s, est. speed input: 40377.93 toks/s, output: 39.43 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [00:47<00:05, 37.54it/s, est. speed input: 40360.26 toks/s, output: 39.41 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [00:47<00:04, 38.15it/s, est. speed input: 40362.37 toks/s, output: 39.42 toks/s]
Processed prompts:  92%|█████████▏| 1890/2048 [00:47<00:04, 37.97it/s, est. speed input: 40345.45 toks/s, output: 39.40 toks/s]
Processed prompts:  93%|█████████▎| 1906/2048 [00:48<00:03, 37.83it/s, est. speed input: 40328.54 toks/s, output: 39.38 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [00:48<00:03, 37.74it/s, est. speed input: 40311.78 toks/s, output: 39.37 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [00:49<00:02, 37.67it/s, est. speed input: 40295.35 toks/s, output: 39.35 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [00:49<00:02, 37.63it/s, est. speed input: 40279.55 toks/s, output: 39.34 toks/s]
Processed prompts:  96%|█████████▌| 1970/2048 [00:50<00:02, 37.60it/s, est. speed input: 40263.80 toks/s, output: 39.32 toks/s]
Processed prompts:  97%|█████████▋| 1986/2048 [00:50<00:01, 37.58it/s, est. speed input: 40248.26 toks/s, output: 39.30 toks/s]
Processed prompts:  98%|█████████▊| 2002/2048 [00:50<00:01, 37.56it/s, est. speed input: 40232.94 toks/s, output: 39.29 toks/s]
Processed prompts:  99%|█████████▊| 2018/2048 [00:51<00:00, 37.56it/s, est. speed input: 40218.26 toks/s, output: 39.28 toks/s]
Processed prompts:  99%|█████████▉| 2034/2048 [00:51<00:00, 38.22it/s, est. speed input: 40222.97 toks/s, output: 39.28 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:51<00:00, 38.22it/s, est. speed input: 40499.63 toks/s, output: 39.55 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:51<00:00, 39.55it/s, est. speed input: 40499.63 toks/s, output: 39.55 toks/s]
[rank0]:[W125 19:43:48.362957018 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 82.4s

测试结果:
  Requests/s:   37.56
  Tokens/s:     38498.86
  Total Reqs:   2048
  Elapsed:      54.53s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     38461.30

============================================================
[7/8] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-3B-INT8                                │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 19:44:04 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=392108) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=392108) WARNING 01-25 19:44:14 [backends.py:609] Failed to read file <frozen os>
Throughput: 37.65 requests/s, 38594.95 total tokens/s, 37.65 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-25 19:44:04] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:44:04] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:44:04] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:44:04] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:44:04] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:44:04] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:44:04] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:44:04] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:44:04] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:44:04] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:44:04] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:44:04] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:44:04] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:44:04] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 19:44:08] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:44:08] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:44:08] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:44:08] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:44:08] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:44:08] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:44:08] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:44:08] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:44:08] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:44:08] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:44:08] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:44:08] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:44:08] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:44:08] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=392108) [2026-01-25 19:44:09] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=392108) [2026-01-25 19:44:09] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=392108) [2026-01-25 19:44:09] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=392108) [2026-01-25 19:44:09] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=392108) [2026-01-25 19:44:09] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=392108) [2026-01-25 19:44:09] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=392108) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=392108) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.03it/s]
(EngineCore_DP0 pid=392108) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.03it/s]
(EngineCore_DP0 pid=392108) 
(EngineCore_DP0 pid=392108) [2026-01-25 19:44:10] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 4608] -> 1D uint8
(EngineCore_DP0 pid=392108) [2026-01-25 19:44:10] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 17694720 bytes
(EngineCore_DP0 pid=392108) [2026-01-25 19:44:10] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 4608] -> 1D uint8
(EngineCore_DP0 pid=392108) [2026-01-25 19:44:10] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 10616832 bytes
(EngineCore_DP0 pid=392108) [2026-01-25 19:44:10] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 4608] -> 1D uint8
(EngineCore_DP0 pid=392108) [2026-01-25 19:44:10] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 56623104 bytes
(EngineCore_DP0 pid=392108) [2026-01-25 19:44:10] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 12288] -> 1D uint8
(EngineCore_DP0 pid=392108) [2026-01-25 19:44:10] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 28311552 bytes
(EngineCore_DP0 pid=392108) [rank0]:W0125 19:44:16.985000 392108 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=392108) [rank0]:W0125 19:44:17.036000 392108 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=392108) [rank0]:W0125 19:44:17.665000 392108 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=392108) [rank0]:W0125 19:44:17.738000 392108 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=392108) 2026-01-25 19:44:20,025 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=392108) 2026-01-25 19:44:20,041 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=392108) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▉         | 1/11 [00:00<00:05,  1.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:00<00:00,  7.35it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▎   | 7/11 [00:00<00:00, 11.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 10/11 [00:00<00:00, 14.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00, 10.96it/s]
(EngineCore_DP0 pid=392108) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 1/7 [00:00<00:01,  5.79it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00, 15.95it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 18.64it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 71/4096 [00:00<00:05, 709.25it/s]
Adding requests:   4%|▎         | 146/4096 [00:00<00:05, 728.27it/s]
Adding requests:   5%|▌         | 222/4096 [00:00<00:05, 740.93it/s]
Adding requests:   7%|▋         | 298/4096 [00:00<00:05, 747.25it/s]
Adding requests:   9%|▉         | 373/4096 [00:00<00:05, 740.15it/s]
Adding requests:  11%|█         | 449/4096 [00:00<00:04, 744.41it/s]
Adding requests:  13%|█▎        | 524/4096 [00:00<00:04, 738.67it/s]
Adding requests:  15%|█▍        | 602/4096 [00:00<00:04, 749.74it/s]
Adding requests:  17%|█▋        | 683/4096 [00:00<00:04, 765.51it/s]
Adding requests:  19%|█▊        | 760/4096 [00:01<00:04, 761.72it/s]
Adding requests:  20%|██        | 837/4096 [00:01<00:04, 747.78it/s]
Adding requests:  22%|██▏       | 917/4096 [00:01<00:04, 761.02it/s]
Adding requests:  24%|██▍       | 995/4096 [00:01<00:04, 766.63it/s]
Adding requests:  26%|██▌       | 1072/4096 [00:01<00:04, 744.96it/s]
Adding requests:  28%|██▊       | 1147/4096 [00:01<00:03, 741.58it/s]
Adding requests:  30%|██▉       | 1225/4096 [00:01<00:03, 752.76it/s]
Adding requests:  32%|███▏      | 1301/4096 [00:01<00:03, 751.84it/s]
Adding requests:  34%|███▎      | 1379/4096 [00:01<00:03, 757.29it/s]
Adding requests:  36%|███▌      | 1457/4096 [00:01<00:03, 762.51it/s]
Adding requests:  37%|███▋      | 1535/4096 [00:02<00:03, 767.51it/s]
Adding requests:  39%|███▉      | 1612/4096 [00:02<00:03, 764.12it/s]
Adding requests:  41%|████      | 1689/4096 [00:02<00:03, 757.57it/s]
Adding requests:  43%|████▎     | 1766/4096 [00:02<00:03, 760.39it/s]
Adding requests:  45%|████▌     | 1845/4096 [00:02<00:02, 766.69it/s]
Adding requests:  47%|████▋     | 1922/4096 [00:02<00:02, 760.94it/s]
Adding requests:  49%|████▉     | 1999/4096 [00:02<00:02, 761.18it/s]
Adding requests:  51%|█████     | 2076/4096 [00:02<00:02, 756.03it/s]
Adding requests:  53%|█████▎    | 2152/4096 [00:02<00:02, 745.84it/s]
Adding requests:  54%|█████▍    | 2227/4096 [00:02<00:02, 734.48it/s]
Adding requests:  56%|█████▋    | 2305/4096 [00:03<00:02, 745.99it/s]
Adding requests:  58%|█████▊    | 2382/4096 [00:03<00:02, 751.99it/s]
Adding requests:  60%|██████    | 2459/4096 [00:03<00:02, 756.75it/s]
Adding requests:  62%|██████▏   | 2535/4096 [00:03<00:02, 756.56it/s]
Adding requests:  64%|██████▎   | 2611/4096 [00:03<00:01, 755.09it/s]
Adding requests:  66%|██████▌   | 2687/4096 [00:03<00:01, 756.48it/s]
Adding requests:  67%|██████▋   | 2763/4096 [00:03<00:01, 753.44it/s]
Adding requests:  69%|██████▉   | 2839/4096 [00:03<00:01, 754.34it/s]
Adding requests:  71%|███████   | 2917/4096 [00:03<00:01, 761.94it/s]
Adding requests:  73%|███████▎  | 2994/4096 [00:03<00:01, 763.01it/s]
Adding requests:  75%|███████▍  | 3071/4096 [00:04<00:01, 759.78it/s]
Adding requests:  77%|███████▋  | 3147/4096 [00:04<00:01, 756.27it/s]
Adding requests:  79%|███████▊  | 3224/4096 [00:04<00:01, 760.13it/s]
Adding requests:  81%|████████  | 3301/4096 [00:04<00:01, 762.23it/s]
Adding requests:  82%|████████▏ | 3379/4096 [00:04<00:00, 766.66it/s]
Adding requests:  84%|████████▍ | 3456/4096 [00:04<00:00, 754.27it/s]
Adding requests:  86%|████████▌ | 3532/4096 [00:04<00:00, 748.95it/s]
Adding requests:  88%|████████▊ | 3607/4096 [00:04<00:00, 738.34it/s]
Adding requests:  90%|████████▉ | 3683/4096 [00:04<00:00, 742.07it/s]
Adding requests:  92%|█████████▏| 3761/4096 [00:04<00:00, 752.55it/s]
Adding requests:  94%|█████████▍| 3841/4096 [00:05<00:00, 763.97it/s]
Adding requests:  96%|█████████▌| 3918/4096 [00:05<00:00, 760.67it/s]
Adding requests:  98%|█████████▊| 3995/4096 [00:05<00:00, 760.82it/s]
Adding requests:  99%|█████████▉| 4073/4096 [00:05<00:00, 762.37it/s]
Adding requests: 100%|██████████| 4096/4096 [00:05<00:00, 755.08it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   5%|▍         | 194/4096 [00:00<00:11, 335.23it/s, est. speed input: 343282.70 toks/s, output: 335.23 toks/s]
Processed prompts:   6%|▌         | 228/4096 [00:01<00:28, 135.73it/s, est. speed input: 163878.36 toks/s, output: 160.04 toks/s]
Processed prompts:   6%|▋         | 258/4096 [00:02<00:44, 86.42it/s, est. speed input: 116309.94 toks/s, output: 113.58 toks/s] 
Processed prompts:   7%|▋         | 290/4096 [00:03<00:57, 66.36it/s, est. speed input: 95238.24 toks/s, output: 93.01 toks/s]  
Processed prompts:   8%|▊         | 322/4096 [00:03<01:07, 55.77it/s, est. speed input: 83167.72 toks/s, output: 81.22 toks/s]
Processed prompts:   9%|▊         | 354/4096 [00:04<01:15, 49.55it/s, est. speed input: 75345.34 toks/s, output: 73.58 toks/s]
Processed prompts:   9%|▉         | 386/4096 [00:05<01:21, 45.65it/s, est. speed input: 69854.92 toks/s, output: 68.22 toks/s]
Processed prompts:  10%|█         | 418/4096 [00:06<01:25, 43.12it/s, est. speed input: 65795.77 toks/s, output: 64.25 toks/s]
Processed prompts:  11%|█         | 450/4096 [00:07<01:27, 41.44it/s, est. speed input: 62672.48 toks/s, output: 61.20 toks/s]
Processed prompts:  12%|█▏        | 482/4096 [00:08<01:29, 40.30it/s, est. speed input: 60191.79 toks/s, output: 58.78 toks/s]
Processed prompts:  13%|█▎        | 514/4096 [00:09<01:30, 39.52it/s, est. speed input: 58174.74 toks/s, output: 56.81 toks/s]
Processed prompts:  13%|█▎        | 546/4096 [00:09<01:31, 38.99it/s, est. speed input: 56505.36 toks/s, output: 55.18 toks/s]
Processed prompts:  14%|█▍        | 578/4096 [00:10<01:31, 38.61it/s, est. speed input: 55096.98 toks/s, output: 53.81 toks/s]
Processed prompts:  15%|█▍        | 610/4096 [00:11<01:30, 38.36it/s, est. speed input: 53897.32 toks/s, output: 52.63 toks/s]
Processed prompts:  16%|█▌        | 642/4096 [00:12<01:30, 38.17it/s, est. speed input: 52856.05 toks/s, output: 51.62 toks/s]
Processed prompts:  16%|█▋        | 674/4096 [00:13<01:29, 38.04it/s, est. speed input: 51949.52 toks/s, output: 50.73 toks/s]
Processed prompts:  17%|█▋        | 706/4096 [00:14<01:29, 37.96it/s, est. speed input: 51153.13 toks/s, output: 49.95 toks/s]
Processed prompts:  18%|█▊        | 738/4096 [00:14<01:28, 37.88it/s, est. speed input: 50441.44 toks/s, output: 49.26 toks/s]
Processed prompts:  19%|█▉        | 770/4096 [00:15<01:27, 37.84it/s, est. speed input: 49810.05 toks/s, output: 48.64 toks/s]
Processed prompts:  20%|█▉        | 802/4096 [00:16<01:27, 37.81it/s, est. speed input: 49242.06 toks/s, output: 48.09 toks/s]
Processed prompts:  20%|██        | 834/4096 [00:17<01:26, 37.78it/s, est. speed input: 48727.20 toks/s, output: 47.59 toks/s]
Processed prompts:  21%|██        | 866/4096 [00:18<01:25, 37.76it/s, est. speed input: 48261.81 toks/s, output: 47.13 toks/s]
Processed prompts:  22%|██▏       | 898/4096 [00:19<01:24, 37.75it/s, est. speed input: 47836.55 toks/s, output: 46.72 toks/s]
Processed prompts:  23%|██▎       | 930/4096 [00:20<01:23, 37.74it/s, est. speed input: 47447.00 toks/s, output: 46.33 toks/s]
Processed prompts:  23%|██▎       | 962/4096 [00:20<01:23, 37.73it/s, est. speed input: 47088.74 toks/s, output: 45.99 toks/s]
Processed prompts:  24%|██▍       | 994/4096 [00:21<01:22, 37.73it/s, est. speed input: 46759.26 toks/s, output: 45.66 toks/s]
Processed prompts:  25%|██▌       | 1026/4096 [00:22<01:21, 37.72it/s, est. speed input: 46453.34 toks/s, output: 45.36 toks/s]
Processed prompts:  26%|██▌       | 1058/4096 [00:23<01:20, 37.72it/s, est. speed input: 46169.56 toks/s, output: 45.09 toks/s]
Processed prompts:  27%|██▋       | 1090/4096 [00:24<01:19, 37.71it/s, est. speed input: 45905.42 toks/s, output: 44.83 toks/s]
Processed prompts:  27%|██▋       | 1122/4096 [00:25<01:18, 37.68it/s, est. speed input: 45655.16 toks/s, output: 44.59 toks/s]
Processed prompts:  28%|██▊       | 1154/4096 [00:26<01:18, 37.69it/s, est. speed input: 45425.37 toks/s, output: 44.36 toks/s]
Processed prompts:  29%|██▉       | 1186/4096 [00:26<01:17, 37.69it/s, est. speed input: 45209.78 toks/s, output: 44.15 toks/s]
Processed prompts:  30%|██▉       | 1218/4096 [00:27<01:16, 37.69it/s, est. speed input: 45007.23 toks/s, output: 43.95 toks/s]
Processed prompts:  31%|███       | 1250/4096 [00:28<01:15, 37.69it/s, est. speed input: 44816.55 toks/s, output: 43.77 toks/s]
Processed prompts:  31%|███▏      | 1282/4096 [00:29<01:14, 37.69it/s, est. speed input: 44636.49 toks/s, output: 43.59 toks/s]
Processed prompts:  32%|███▏      | 1314/4096 [00:30<01:13, 37.68it/s, est. speed input: 44466.61 toks/s, output: 43.42 toks/s]
Processed prompts:  33%|███▎      | 1346/4096 [00:31<01:12, 37.68it/s, est. speed input: 44306.22 toks/s, output: 43.27 toks/s]
Processed prompts:  34%|███▎      | 1378/4096 [00:31<01:12, 37.68it/s, est. speed input: 44153.90 toks/s, output: 43.12 toks/s]
Processed prompts:  34%|███▍      | 1410/4096 [00:32<01:11, 37.68it/s, est. speed input: 44009.90 toks/s, output: 42.98 toks/s]
Processed prompts:  35%|███▌      | 1442/4096 [00:33<01:10, 37.68it/s, est. speed input: 43873.20 toks/s, output: 42.84 toks/s]
Processed prompts:  36%|███▌      | 1474/4096 [00:34<01:09, 37.67it/s, est. speed input: 43742.27 toks/s, output: 42.72 toks/s]
Processed prompts:  37%|███▋      | 1506/4096 [00:35<01:08, 37.67it/s, est. speed input: 43617.82 toks/s, output: 42.60 toks/s]
Processed prompts:  38%|███▊      | 1538/4096 [00:36<01:07, 37.67it/s, est. speed input: 43499.36 toks/s, output: 42.48 toks/s]
Processed prompts:  38%|███▊      | 1570/4096 [00:37<01:07, 37.67it/s, est. speed input: 43386.18 toks/s, output: 42.37 toks/s]
Processed prompts:  39%|███▉      | 1602/4096 [00:37<01:06, 37.67it/s, est. speed input: 43278.32 toks/s, output: 42.26 toks/s]
Processed prompts:  40%|███▉      | 1634/4096 [00:38<01:05, 37.67it/s, est. speed input: 43174.95 toks/s, output: 42.16 toks/s]
Processed prompts:  41%|████      | 1666/4096 [00:39<01:04, 37.66it/s, est. speed input: 43076.07 toks/s, output: 42.07 toks/s]
Processed prompts:  41%|████▏     | 1698/4096 [00:40<01:03, 37.66it/s, est. speed input: 42981.29 toks/s, output: 41.97 toks/s]
Processed prompts:  42%|████▏     | 1730/4096 [00:41<01:02, 37.66it/s, est. speed input: 42890.27 toks/s, output: 41.89 toks/s]
Processed prompts:  43%|████▎     | 1762/4096 [00:42<01:01, 37.66it/s, est. speed input: 42802.99 toks/s, output: 41.80 toks/s]
Processed prompts:  44%|████▍     | 1794/4096 [00:43<01:01, 37.66it/s, est. speed input: 42719.15 toks/s, output: 41.72 toks/s]
Processed prompts:  45%|████▍     | 1826/4096 [00:43<01:00, 37.65it/s, est. speed input: 42638.34 toks/s, output: 41.64 toks/s]
Processed prompts:  45%|████▌     | 1858/4096 [00:44<00:58, 37.97it/s, est. speed input: 42582.91 toks/s, output: 41.58 toks/s]
Processed prompts:  46%|████▌     | 1890/4096 [00:45<00:58, 37.87it/s, est. speed input: 42507.60 toks/s, output: 41.51 toks/s]
Processed prompts:  47%|████▋     | 1922/4096 [00:46<00:57, 37.80it/s, est. speed input: 42435.05 toks/s, output: 41.44 toks/s]
Processed prompts:  48%|████▊     | 1954/4096 [00:47<00:56, 37.75it/s, est. speed input: 42365.03 toks/s, output: 41.37 toks/s]
Processed prompts:  48%|████▊     | 1986/4096 [00:48<00:55, 37.71it/s, est. speed input: 42297.12 toks/s, output: 41.31 toks/s]
Processed prompts:  49%|████▉     | 2018/4096 [00:48<00:55, 37.69it/s, est. speed input: 42231.96 toks/s, output: 41.24 toks/s]
Processed prompts:  50%|█████     | 2050/4096 [00:49<00:54, 37.68it/s, est. speed input: 42169.14 toks/s, output: 41.18 toks/s]
Processed prompts:  51%|█████     | 2082/4096 [00:50<00:53, 37.66it/s, est. speed input: 42108.02 toks/s, output: 41.12 toks/s]
Processed prompts:  52%|█████▏    | 2114/4096 [00:51<00:52, 37.65it/s, est. speed input: 42048.61 toks/s, output: 41.06 toks/s]
Processed prompts:  52%|█████▏    | 2146/4096 [00:52<00:51, 37.64it/s, est. speed input: 41991.33 toks/s, output: 41.01 toks/s]
Processed prompts:  53%|█████▎    | 2178/4096 [00:53<00:50, 37.63it/s, est. speed input: 41935.87 toks/s, output: 40.95 toks/s]
Processed prompts:  54%|█████▍    | 2210/4096 [00:54<00:50, 37.63it/s, est. speed input: 41882.26 toks/s, output: 40.90 toks/s]
Processed prompts:  55%|█████▍    | 2242/4096 [00:54<00:49, 37.64it/s, est. speed input: 41830.61 toks/s, output: 40.85 toks/s]
Processed prompts:  56%|█████▌    | 2274/4096 [00:55<00:48, 37.63it/s, est. speed input: 41780.07 toks/s, output: 40.80 toks/s]
Processed prompts:  56%|█████▋    | 2306/4096 [00:56<00:47, 37.62it/s, est. speed input: 41730.91 toks/s, output: 40.75 toks/s]
Processed prompts:  57%|█████▋    | 2338/4096 [00:57<00:46, 37.62it/s, est. speed input: 41683.50 toks/s, output: 40.71 toks/s]
Processed prompts:  58%|█████▊    | 2370/4096 [00:58<00:45, 37.62it/s, est. speed input: 41637.32 toks/s, output: 40.66 toks/s]
Processed prompts:  59%|█████▊    | 2402/4096 [00:59<00:45, 37.62it/s, est. speed input: 41592.65 toks/s, output: 40.62 toks/s]
Processed prompts:  59%|█████▉    | 2434/4096 [00:59<00:44, 37.62it/s, est. speed input: 41549.01 toks/s, output: 40.58 toks/s]
Processed prompts:  60%|██████    | 2466/4096 [01:00<00:43, 37.62it/s, est. speed input: 41506.76 toks/s, output: 40.53 toks/s]
Processed prompts:  61%|██████    | 2498/4096 [01:01<00:42, 37.62it/s, est. speed input: 41465.43 toks/s, output: 40.49 toks/s]
Processed prompts:  62%|██████▏   | 2530/4096 [01:02<00:41, 37.61it/s, est. speed input: 41425.20 toks/s, output: 40.45 toks/s]
Processed prompts:  63%|██████▎   | 2562/4096 [01:03<00:40, 37.61it/s, est. speed input: 41385.98 toks/s, output: 40.42 toks/s]
Processed prompts:  63%|██████▎   | 2594/4096 [01:04<00:39, 37.61it/s, est. speed input: 41347.85 toks/s, output: 40.38 toks/s]
Processed prompts:  64%|██████▍   | 2626/4096 [01:05<00:39, 37.61it/s, est. speed input: 41310.61 toks/s, output: 40.34 toks/s]
Processed prompts:  65%|██████▍   | 2658/4096 [01:05<00:38, 37.60it/s, est. speed input: 41274.18 toks/s, output: 40.31 toks/s]
Processed prompts:  66%|██████▌   | 2690/4096 [01:06<00:37, 37.61it/s, est. speed input: 41239.22 toks/s, output: 40.27 toks/s]
Processed prompts:  66%|██████▋   | 2722/4096 [01:07<00:36, 37.61it/s, est. speed input: 41204.94 toks/s, output: 40.24 toks/s]
Processed prompts:  67%|██████▋   | 2754/4096 [01:08<00:35, 37.61it/s, est. speed input: 41171.39 toks/s, output: 40.21 toks/s]
Processed prompts:  68%|██████▊   | 2786/4096 [01:09<00:34, 37.61it/s, est. speed input: 41138.69 toks/s, output: 40.17 toks/s]
Processed prompts:  69%|██████▉   | 2818/4096 [01:10<00:33, 37.61it/s, est. speed input: 41106.87 toks/s, output: 40.14 toks/s]
Processed prompts:  70%|██████▉   | 2850/4096 [01:11<00:33, 37.60it/s, est. speed input: 41075.59 toks/s, output: 40.11 toks/s]
Processed prompts:  70%|███████   | 2882/4096 [01:11<00:32, 37.60it/s, est. speed input: 41045.24 toks/s, output: 40.08 toks/s]
Processed prompts:  71%|███████   | 2914/4096 [01:12<00:31, 37.60it/s, est. speed input: 41015.29 toks/s, output: 40.05 toks/s]
Processed prompts:  72%|███████▏  | 2946/4096 [01:13<00:30, 37.60it/s, est. speed input: 40986.20 toks/s, output: 40.03 toks/s]
Processed prompts:  73%|███████▎  | 2978/4096 [01:14<00:29, 37.60it/s, est. speed input: 40957.73 toks/s, output: 40.00 toks/s]
Processed prompts:  73%|███████▎  | 3010/4096 [01:15<00:28, 37.59it/s, est. speed input: 40929.77 toks/s, output: 39.97 toks/s]
Processed prompts:  74%|███████▍  | 3042/4096 [01:16<00:28, 37.60it/s, est. speed input: 40902.85 toks/s, output: 39.94 toks/s]
Processed prompts:  75%|███████▌  | 3074/4096 [01:17<00:27, 37.60it/s, est. speed input: 40876.20 toks/s, output: 39.92 toks/s]
Processed prompts:  76%|███████▌  | 3106/4096 [01:17<00:26, 37.60it/s, est. speed input: 40850.20 toks/s, output: 39.89 toks/s]
Processed prompts:  77%|███████▋  | 3138/4096 [01:18<00:25, 37.60it/s, est. speed input: 40824.85 toks/s, output: 39.87 toks/s]
Processed prompts:  77%|███████▋  | 3170/4096 [01:19<00:24, 37.59it/s, est. speed input: 40799.69 toks/s, output: 39.84 toks/s]
Processed prompts:  78%|███████▊  | 3202/4096 [01:20<00:23, 37.59it/s, est. speed input: 40775.17 toks/s, output: 39.82 toks/s]
Processed prompts:  79%|███████▉  | 3234/4096 [01:21<00:22, 37.59it/s, est. speed input: 40751.24 toks/s, output: 39.80 toks/s]
Processed prompts:  80%|███████▉  | 3266/4096 [01:22<00:22, 37.59it/s, est. speed input: 40727.77 toks/s, output: 39.77 toks/s]
Processed prompts:  81%|████████  | 3298/4096 [01:22<00:21, 37.59it/s, est. speed input: 40705.00 toks/s, output: 39.75 toks/s]
Processed prompts:  81%|████████▏ | 3330/4096 [01:23<00:20, 37.58it/s, est. speed input: 40682.21 toks/s, output: 39.73 toks/s]
Processed prompts:  82%|████████▏ | 3362/4096 [01:24<00:19, 37.59it/s, est. speed input: 40660.20 toks/s, output: 39.71 toks/s]
Processed prompts:  83%|████████▎ | 3394/4096 [01:25<00:18, 37.59it/s, est. speed input: 40638.55 toks/s, output: 39.69 toks/s]
Processed prompts:  84%|████████▎ | 3426/4096 [01:26<00:17, 37.58it/s, est. speed input: 40617.09 toks/s, output: 39.67 toks/s]
Processed prompts:  84%|████████▍ | 3458/4096 [01:27<00:16, 37.58it/s, est. speed input: 40596.34 toks/s, output: 39.64 toks/s]
Processed prompts:  85%|████████▌ | 3490/4096 [01:28<00:16, 37.58it/s, est. speed input: 40575.87 toks/s, output: 39.62 toks/s]
Processed prompts:  86%|████████▌ | 3522/4096 [01:28<00:15, 37.58it/s, est. speed input: 40555.77 toks/s, output: 39.61 toks/s]
Processed prompts:  87%|████████▋ | 3554/4096 [01:29<00:14, 37.58it/s, est. speed input: 40536.15 toks/s, output: 39.59 toks/s]
Processed prompts:  88%|████████▊ | 3586/4096 [01:30<00:13, 37.58it/s, est. speed input: 40516.72 toks/s, output: 39.57 toks/s]
Processed prompts:  88%|████████▊ | 3618/4096 [01:31<00:12, 37.58it/s, est. speed input: 40497.95 toks/s, output: 39.55 toks/s]
Processed prompts:  89%|████████▉ | 3650/4096 [01:32<00:11, 37.58it/s, est. speed input: 40479.18 toks/s, output: 39.53 toks/s]
Processed prompts:  90%|████████▉ | 3682/4096 [01:33<00:11, 37.57it/s, est. speed input: 40460.81 toks/s, output: 39.51 toks/s]
Processed prompts:  91%|█████████ | 3714/4096 [01:34<00:10, 37.57it/s, est. speed input: 40442.77 toks/s, output: 39.49 toks/s]
Processed prompts:  91%|█████████▏| 3746/4096 [01:34<00:09, 37.58it/s, est. speed input: 40425.24 toks/s, output: 39.48 toks/s]
Processed prompts:  92%|█████████▏| 3778/4096 [01:35<00:08, 37.57it/s, est. speed input: 40407.87 toks/s, output: 39.46 toks/s]
Processed prompts:  93%|█████████▎| 3810/4096 [01:36<00:07, 37.57it/s, est. speed input: 40390.77 toks/s, output: 39.44 toks/s]
Processed prompts:  94%|█████████▍| 3842/4096 [01:37<00:06, 37.57it/s, est. speed input: 40373.96 toks/s, output: 39.43 toks/s]
Processed prompts:  95%|█████████▍| 3874/4096 [01:38<00:05, 37.57it/s, est. speed input: 40357.37 toks/s, output: 39.41 toks/s]
Processed prompts:  95%|█████████▌| 3906/4096 [01:39<00:05, 37.57it/s, est. speed input: 40341.21 toks/s, output: 39.40 toks/s]
Processed prompts:  96%|█████████▌| 3938/4096 [01:39<00:04, 37.57it/s, est. speed input: 40325.34 toks/s, output: 39.38 toks/s]
Processed prompts:  97%|█████████▋| 3970/4096 [01:40<00:03, 37.57it/s, est. speed input: 40309.79 toks/s, output: 39.37 toks/s]
Processed prompts:  98%|█████████▊| 4002/4096 [01:41<00:02, 37.57it/s, est. speed input: 40294.30 toks/s, output: 39.35 toks/s]
Processed prompts:  98%|█████████▊| 4034/4096 [01:42<00:01, 37.89it/s, est. speed input: 40288.69 toks/s, output: 39.34 toks/s]
Processed prompts:  99%|█████████▉| 4066/4096 [01:43<00:00, 38.16it/s, est. speed input: 40284.30 toks/s, output: 39.34 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [01:43<00:00, 38.16it/s, est. speed input: 40581.41 toks/s, output: 39.63 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [01:43<00:00, 39.63it/s, est. speed input: 40581.41 toks/s, output: 39.63 toks/s]
[rank0]:[W125 19:46:11.185601884 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 142.8s

测试结果:
  Requests/s:   37.65
  Tokens/s:     38594.95
  Total Reqs:   4096
  Elapsed:      108.78s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     38557.30

============================================================
[8/8] 测试 M=65536
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-3B-INT8                                │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 65536
│   M_prefill     = 65536 (= 64 x 1024)
│   M_decode      = 64
│   batched_tokens = 65536 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 8192
│   --max-num-seqs           = 64
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 65536
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 19:46:39 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=394523) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=394523) WARNING 01-25 19:46:48 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 256, in _initialize_kv_caches
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]     self.model_executor.initialize_from_config(kv_cache_configs)
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 116, in initialize_from_config
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]     self.collective_rpc("compile_or_warm_up_model")
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 455, in compile_or_warm_up_model
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]     kernel_warmup(self)
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]   File "/root/vllmbench/vllm/model_executor/warmup/kernel_warmup.py", line 41, in kernel_warmup
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]     flashinfer_autotune(worker.model_runner)
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]   File "/root/vllmbench/vllm/model_executor/warmup/kernel_warmup.py", line 94, in flashinfer_autotune
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]     runner._dummy_run(
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 623, in forward
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]     model_output = self.model(
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]                    ^^^^^^^^^^^
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 439, in __call__
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]     return TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 223, in __call__
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 412, in forward
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]     def forward(
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]     raise e
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]   File "<eval_with_key>.58", line 304, in forward
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]     submod_4 = self.submod_4(getitem_8, s72, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_, getitem_9, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_8 = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_ = getitem_9 = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = None
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]     return range_entry.runnable(*args)
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]     return self._compiled_fn(*args)
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]     return compiled_fn(full_args)
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]                             ^^^^^^^
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]     return self.current_callable(inputs)
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]     out = model(new_inputs)
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]   File "/tmp/torchinductor_root/ny/cnyjsheme5xgwvzrgnkiqzyhpeyqpaahe6gnhetkrzuieuovx4gl.py", line 983, in call
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]     buf14 = torch.ops.slidesparse.dequant_bias.default(reinterpret_tensor(buf13, (s72, 16384), (16384, 1), 0), reinterpret_tensor(buf11, (s72, ), (1, ), 0), arg7_1, None, 'bfloat16', 'Llama3.2-3B-INT8')
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 483, in _dequant_bias_impl
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]     return fn(gemm_output, scale_a, scale_b, bias, out_dtype)
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]   File "/root/vllmbench/slidesparse/csrc/fused_dequant_bias_triton/build/RTX5080_cc120_py312_cu129_x86_64/dequant_bias_tuned_Llama3.2-3B.py", line 112, in dequant_bias_triton
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]     output = torch.empty((M, N), dtype=torch.bfloat16, device=gemm_output.device)
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=394523) ERROR 01-25 19:46:55 [core.py:866] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 15.46 GiB of which 1.80 GiB is free. Including non-PyTorch memory, this process has 13.44 GiB memory in use. Of the allocated memory 10.68 GiB is allocated by PyTorch, and 2.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


─── STDERR ───
[2026-01-25 19:46:38] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:46:39] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:46:39] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:46:39] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:46:39] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:46:39] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:46:39] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:46:39] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:46:39] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:46:39] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:46:39] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:46:39] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:46:39] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:46:39] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 19:46:42] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:46:42] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:46:42] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:46:42] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:46:42] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:46:42] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:46:42] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:46:42] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:46:42] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:46:42] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:46:42] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:46:42] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:46:42] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:46:42] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=394523) [2026-01-25 19:46:43] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=394523) [2026-01-25 19:46:43] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=394523) [2026-01-25 19:46:43] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=394523) [2026-01-25 19:46:43] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=394523) [2026-01-25 19:46:43] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=394523) [2026-01-25 19:46:43] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=394523) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=394523) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.05it/s]
(EngineCore_DP0 pid=394523) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.05it/s]
(EngineCore_DP0 pid=394523) 
(EngineCore_DP0 pid=394523) [2026-01-25 19:46:44] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 4608] -> 1D uint8
(EngineCore_DP0 pid=394523) [2026-01-25 19:46:44] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 17694720 bytes
(EngineCore_DP0 pid=394523) [2026-01-25 19:46:44] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 4608] -> 1D uint8
(EngineCore_DP0 pid=394523) [2026-01-25 19:46:44] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 10616832 bytes
(EngineCore_DP0 pid=394523) [2026-01-25 19:46:44] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 4608] -> 1D uint8
(EngineCore_DP0 pid=394523) [2026-01-25 19:46:44] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 56623104 bytes
(EngineCore_DP0 pid=394523) [2026-01-25 19:46:44] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 12288] -> 1D uint8
(EngineCore_DP0 pid=394523) [2026-01-25 19:46:44] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 28311552 bytes
(EngineCore_DP0 pid=394523) [rank0]:W0125 19:46:51.280000 394523 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=394523) [rank0]:W0125 19:46:51.911000 394523 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=394523) 2026-01-25 19:46:55,009 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=394523) 2026-01-25 19:46:55,102 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=394523) Process EngineCore_DP0:
(EngineCore_DP0 pid=394523) Traceback (most recent call last):
(EngineCore_DP0 pid=394523)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=394523)     self.run()
(EngineCore_DP0 pid=394523)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=394523)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=394523)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=394523)     raise e
(EngineCore_DP0 pid=394523)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=394523)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=394523)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=394523)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=394523)     super().__init__(
(EngineCore_DP0 pid=394523)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=394523)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=394523)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=394523)   File "/root/vllmbench/vllm/v1/engine/core.py", line 256, in _initialize_kv_caches
(EngineCore_DP0 pid=394523)     self.model_executor.initialize_from_config(kv_cache_configs)
(EngineCore_DP0 pid=394523)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 116, in initialize_from_config
(EngineCore_DP0 pid=394523)     self.collective_rpc("compile_or_warm_up_model")
(EngineCore_DP0 pid=394523)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=394523)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=394523)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=394523)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=394523)     return func(*args, **kwargs)
(EngineCore_DP0 pid=394523)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=394523)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 455, in compile_or_warm_up_model
(EngineCore_DP0 pid=394523)     kernel_warmup(self)
(EngineCore_DP0 pid=394523)   File "/root/vllmbench/vllm/model_executor/warmup/kernel_warmup.py", line 41, in kernel_warmup
(EngineCore_DP0 pid=394523)     flashinfer_autotune(worker.model_runner)
(EngineCore_DP0 pid=394523)   File "/root/vllmbench/vllm/model_executor/warmup/kernel_warmup.py", line 94, in flashinfer_autotune
(EngineCore_DP0 pid=394523)     runner._dummy_run(
(EngineCore_DP0 pid=394523)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=394523)     return func(*args, **kwargs)
(EngineCore_DP0 pid=394523)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=394523)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=394523)     outputs = self.model(
(EngineCore_DP0 pid=394523)               ^^^^^^^^^^^
(EngineCore_DP0 pid=394523)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=394523)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=394523)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=394523)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=394523)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=394523)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=394523)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=394523)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=394523)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=394523)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 623, in forward
(EngineCore_DP0 pid=394523)     model_output = self.model(
(EngineCore_DP0 pid=394523)                    ^^^^^^^^^^^
(EngineCore_DP0 pid=394523)   File "/root/vllmbench/vllm/compilation/decorators.py", line 439, in __call__
(EngineCore_DP0 pid=394523)     return TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=394523)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=394523)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 223, in __call__
(EngineCore_DP0 pid=394523)     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=394523)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=394523)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=394523)     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=394523)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=394523)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 412, in forward
(EngineCore_DP0 pid=394523)     def forward(
(EngineCore_DP0 pid=394523)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=394523)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=394523)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=394523)   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=394523)     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=394523)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=394523)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=394523)     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=394523)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=394523)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=394523)     raise e
(EngineCore_DP0 pid=394523)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=394523)     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=394523)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=394523)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=394523)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=394523)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=394523)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=394523)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=394523)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=394523)   File "<eval_with_key>.58", line 304, in forward
(EngineCore_DP0 pid=394523)     submod_4 = self.submod_4(getitem_8, s72, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_, getitem_9, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_8 = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_ = getitem_9 = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = None
(EngineCore_DP0 pid=394523)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=394523)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=394523)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=394523)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=394523)   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=394523)     return range_entry.runnable(*args)
(EngineCore_DP0 pid=394523)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=394523)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=394523)     return self._compiled_fn(*args)
(EngineCore_DP0 pid=394523)            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=394523)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=394523)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=394523)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=394523)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=394523)     return compiled_fn(full_args)
(EngineCore_DP0 pid=394523)            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=394523)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=394523)     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=394523)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=394523)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=394523)     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=394523)                             ^^^^^^^
(EngineCore_DP0 pid=394523)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=394523)     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=394523)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=394523)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=394523)     return self.current_callable(inputs)
(EngineCore_DP0 pid=394523)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=394523)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=394523)     out = model(new_inputs)
(EngineCore_DP0 pid=394523)           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=394523)   File "/tmp/torchinductor_root/ny/cnyjsheme5xgwvzrgnkiqzyhpeyqpaahe6gnhetkrzuieuovx4gl.py", line 983, in call
(EngineCore_DP0 pid=394523)     buf14 = torch.ops.slidesparse.dequant_bias.default(reinterpret_tensor(buf13, (s72, 16384), (16384, 1), 0), reinterpret_tensor(buf11, (s72, ), (1, ), 0), arg7_1, None, 'bfloat16', 'Llama3.2-3B-INT8')
(EngineCore_DP0 pid=394523)             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=394523)   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
(EngineCore_DP0 pid=394523)     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=394523)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=394523)   File "/root/vllmbench/slidesparse/core/kernels.py", line 483, in _dequant_bias_impl
(EngineCore_DP0 pid=394523)     return fn(gemm_output, scale_a, scale_b, bias, out_dtype)
(EngineCore_DP0 pid=394523)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=394523)   File "/root/vllmbench/slidesparse/csrc/fused_dequant_bias_triton/build/RTX5080_cc120_py312_cu129_x86_64/dequant_bias_tuned_Llama3.2-3B.py", line 112, in dequant_bias_triton
(EngineCore_DP0 pid=394523)     output = torch.empty((M, N), dtype=torch.bfloat16, device=gemm_output.device)
(EngineCore_DP0 pid=394523)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=394523) torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 15.46 GiB of which 1.80 GiB is free. Including non-PyTorch memory, this process has 13.44 GiB memory in use. Of the allocated memory 10.68 GiB is allocated by PyTorch, and 2.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W125 19:46:55.399339327 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[ERROR] 测试失败: M=65536 (exit code: 1)


------------------------------------------------------------
  生成 CSV: Llama3.2-3B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_8/Llama3.2-3B-INT8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,55.3971,28418.7119,2.3106
1024,1024,1,128,128,35.8919,36789.1560,3.5663
2048,1024,2,256,128,40.7153,41733.2307,6.2876
4096,1024,4,512,128,39.0286,40004.3416,13.1186
8192,1024,8,1024,128,37.9238,38871.9253,27.0015
16384,1024,16,2048,128,37.5599,38498.8569,54.5263
32768,1024,32,4096,128,37.6536,38594.9523,108.7811
65536,1024,64,8192,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[INFO] 完成: 7 成功, 1 失败

============================================================
  Llama3.2-3B-INT8 | cuSPARSELt (2_10) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-INT8-SlideSparse-2_10
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_10

============================================================
[1/8] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-3B-INT8                                │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 19:47:01 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=395087) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=395087) WARNING 01-25 19:47:11 [backends.py:609] Failed to read file <frozen os>
Throughput: 56.23 requests/s, 28846.14 total tokens/s, 56.23 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-25 19:47:01] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:47:01] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:47:01] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:47:01] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:47:01] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:47:01] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:47:01] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:47:01] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:47:01] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:47:01] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:47:01] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:47:01] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:47:01] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:47:01] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 19:47:04] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:47:05] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:47:05] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:47:05] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:47:05] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:47:05] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:47:05] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:47:05] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:47:05] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:47:05] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:47:05] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:47:05] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:47:05] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:47:05] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=395087) [2026-01-25 19:47:05] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=395087) [2026-01-25 19:47:05] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=395087) [2026-01-25 19:47:05] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=395087) [2026-01-25 19:47:05] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=395087) [2026-01-25 19:47:05] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=395087) [2026-01-25 19:47:05] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=395087) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=395087) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.53s/it]
(EngineCore_DP0 pid=395087) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:01<00:00,  1.53s/it]
(EngineCore_DP0 pid=395087) 
(EngineCore_DP0 pid=395087) [2026-01-25 19:47:07] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 4928] -> 1D uint8
(EngineCore_DP0 pid=395087) [2026-01-25 19:47:07] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19005440 bytes
(EngineCore_DP0 pid=395087) [2026-01-25 19:47:07] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 4928] -> 1D uint8
(EngineCore_DP0 pid=395087) [2026-01-25 19:47:07] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11403264 bytes
(EngineCore_DP0 pid=395087) [2026-01-25 19:47:07] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 4928] -> 1D uint8
(EngineCore_DP0 pid=395087) [2026-01-25 19:47:07] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 60817408 bytes
(EngineCore_DP0 pid=395087) [2026-01-25 19:47:07] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 13120] -> 1D uint8
(EngineCore_DP0 pid=395087) [2026-01-25 19:47:07] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 30277632 bytes
(EngineCore_DP0 pid=395087) 2026-01-25 19:47:17,373 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=395087) 2026-01-25 19:47:17,390 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=395087) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  4.13it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  4.02it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  4.04it/s]
(EngineCore_DP0 pid=395087) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.07it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.07it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  84%|████████▍ | 108/128 [00:00<00:00, 1072.29it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 1099.02it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:18,  6.71it/s, est. speed input: 3438.38 toks/s, output: 6.72 toks/s]
Processed prompts:   6%|▋         | 8/128 [00:00<00:03, 35.47it/s, est. speed input: 15650.05 toks/s, output: 30.57 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:00<00:02, 47.24it/s, est. speed input: 20640.94 toks/s, output: 40.31 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:00<00:01, 53.11it/s, est. speed input: 23292.45 toks/s, output: 45.49 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:00<00:01, 56.57it/s, est. speed input: 24967.38 toks/s, output: 48.76 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:00<00:01, 58.74it/s, est. speed input: 26121.78 toks/s, output: 51.02 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:00<00:01, 60.26it/s, est. speed input: 26982.34 toks/s, output: 52.70 toks/s]
Processed prompts:  39%|███▉      | 50/128 [00:00<00:01, 61.41it/s, est. speed input: 27662.96 toks/s, output: 54.03 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:01<00:01, 62.16it/s, est. speed input: 28195.14 toks/s, output: 55.07 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:01<00:01, 62.52it/s, est. speed input: 28603.64 toks/s, output: 55.87 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:01<00:00, 62.72it/s, est. speed input: 28933.38 toks/s, output: 56.51 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:01<00:00, 62.91it/s, est. speed input: 29216.13 toks/s, output: 57.06 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:01<00:00, 63.05it/s, est. speed input: 29457.86 toks/s, output: 57.53 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:01<00:00, 63.04it/s, est. speed input: 29654.26 toks/s, output: 57.92 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:01<00:00, 63.19it/s, est. speed input: 29841.09 toks/s, output: 58.28 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:01<00:00, 62.39it/s, est. speed input: 29916.84 toks/s, output: 58.43 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:01<00:00, 62.77it/s, est. speed input: 30070.38 toks/s, output: 58.73 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:02<00:00, 63.24it/s, est. speed input: 30223.97 toks/s, output: 59.03 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:02<00:00, 63.46it/s, est. speed input: 30353.33 toks/s, output: 59.28 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 63.46it/s, est. speed input: 30372.59 toks/s, output: 59.32 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 59.32it/s, est. speed input: 30372.59 toks/s, output: 59.32 toks/s]
[rank0]:[W125 19:47:21.204084032 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 25.5s

测试结果:
  Requests/s:   56.23
  Tokens/s:     28846.14
  Total Reqs:   128
  Elapsed:      2.28s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     28789.91

============================================================
[2/8] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-3B-INT8                                │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 19:47:27 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=395790) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=395790) WARNING 01-25 19:47:36 [backends.py:609] Failed to read file <frozen os>
Throughput: 34.83 requests/s, 35697.82 total tokens/s, 34.83 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-25 19:47:26] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:47:27] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:47:27] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:47:27] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:47:27] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:47:27] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:47:27] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:47:27] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:47:27] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:47:27] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:47:27] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:47:27] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:47:27] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:47:27] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 19:47:31] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:47:31] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:47:31] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:47:31] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:47:31] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:47:31] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:47:31] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:47:31] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:47:31] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:47:31] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:47:31] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:47:31] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:47:31] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:47:31] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=395790) [2026-01-25 19:47:31] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=395790) [2026-01-25 19:47:31] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=395790) [2026-01-25 19:47:31] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=395790) [2026-01-25 19:47:31] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=395790) [2026-01-25 19:47:31] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=395790) [2026-01-25 19:47:31] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=395790) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=395790) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.04it/s]
(EngineCore_DP0 pid=395790) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.04it/s]
(EngineCore_DP0 pid=395790) 
(EngineCore_DP0 pid=395790) [2026-01-25 19:47:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 4928] -> 1D uint8
(EngineCore_DP0 pid=395790) [2026-01-25 19:47:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19005440 bytes
(EngineCore_DP0 pid=395790) [2026-01-25 19:47:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 4928] -> 1D uint8
(EngineCore_DP0 pid=395790) [2026-01-25 19:47:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11403264 bytes
(EngineCore_DP0 pid=395790) [2026-01-25 19:47:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 4928] -> 1D uint8
(EngineCore_DP0 pid=395790) [2026-01-25 19:47:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 60817408 bytes
(EngineCore_DP0 pid=395790) [2026-01-25 19:47:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 13120] -> 1D uint8
(EngineCore_DP0 pid=395790) [2026-01-25 19:47:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 30277632 bytes
(EngineCore_DP0 pid=395790) 2026-01-25 19:47:42,275 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=395790) 2026-01-25 19:47:42,301 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=395790) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 19.59it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 19.57it/s]
(EngineCore_DP0 pid=395790) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.50it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.50it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  50%|█████     | 64/128 [00:00<00:00, 634.03it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 677.75it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 2/128 [00:00<00:07, 17.99it/s, est. speed input: 18425.51 toks/s, output: 17.99 toks/s]
Processed prompts:   5%|▍         | 6/128 [00:00<00:04, 29.25it/s, est. speed input: 28188.37 toks/s, output: 27.53 toks/s]
Processed prompts:   8%|▊         | 10/128 [00:00<00:03, 32.91it/s, est. speed input: 31492.12 toks/s, output: 30.75 toks/s]
Processed prompts:  11%|█         | 14/128 [00:00<00:03, 34.48it/s, est. speed input: 33060.50 toks/s, output: 32.29 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:00<00:03, 35.51it/s, est. speed input: 34090.50 toks/s, output: 33.29 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:00<00:02, 36.14it/s, est. speed input: 34779.73 toks/s, output: 33.96 toks/s]
Processed prompts:  20%|██        | 26/128 [00:00<00:02, 36.55it/s, est. speed input: 35276.37 toks/s, output: 34.45 toks/s]
Processed prompts:  23%|██▎       | 30/128 [00:00<00:02, 36.82it/s, est. speed input: 35651.89 toks/s, output: 34.82 toks/s]
Processed prompts:  27%|██▋       | 34/128 [00:00<00:02, 36.99it/s, est. speed input: 35941.49 toks/s, output: 35.10 toks/s]
Processed prompts:  30%|██▉       | 38/128 [00:01<00:02, 37.11it/s, est. speed input: 36174.13 toks/s, output: 35.33 toks/s]
Processed prompts:  33%|███▎      | 42/128 [00:01<00:02, 37.20it/s, est. speed input: 36364.80 toks/s, output: 35.51 toks/s]
Processed prompts:  36%|███▌      | 46/128 [00:01<00:02, 37.24it/s, est. speed input: 36521.14 toks/s, output: 35.67 toks/s]
Processed prompts:  39%|███▉      | 50/128 [00:01<00:02, 37.32it/s, est. speed input: 36663.73 toks/s, output: 35.80 toks/s]
Processed prompts:  42%|████▏     | 54/128 [00:01<00:01, 37.27it/s, est. speed input: 36763.10 toks/s, output: 35.90 toks/s]
Processed prompts:  45%|████▌     | 58/128 [00:01<00:01, 37.32it/s, est. speed input: 36867.74 toks/s, output: 36.00 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:01<00:01, 37.36it/s, est. speed input: 36960.24 toks/s, output: 36.09 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:01<00:01, 37.33it/s, est. speed input: 37030.44 toks/s, output: 36.16 toks/s]
Processed prompts:  55%|█████▍    | 70/128 [00:01<00:01, 37.31it/s, est. speed input: 37093.88 toks/s, output: 36.22 toks/s]
Processed prompts:  58%|█████▊    | 74/128 [00:02<00:01, 37.34it/s, est. speed input: 37156.29 toks/s, output: 36.29 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:02<00:01, 37.29it/s, est. speed input: 37202.39 toks/s, output: 36.33 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:02<00:01, 37.30it/s, est. speed input: 37249.95 toks/s, output: 36.38 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:02<00:01, 37.29it/s, est. speed input: 37291.72 toks/s, output: 36.42 toks/s]
Processed prompts:  70%|███████   | 90/128 [00:02<00:01, 37.29it/s, est. speed input: 37330.38 toks/s, output: 36.46 toks/s]
Processed prompts:  73%|███████▎  | 94/128 [00:02<00:00, 37.37it/s, est. speed input: 37376.63 toks/s, output: 36.50 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:02<00:00, 37.33it/s, est. speed input: 37407.25 toks/s, output: 36.53 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:02<00:00, 37.34it/s, est. speed input: 37440.46 toks/s, output: 36.56 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:02<00:00, 37.38it/s, est. speed input: 37474.03 toks/s, output: 36.60 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:03<00:00, 37.41it/s, est. speed input: 37505.90 toks/s, output: 36.63 toks/s]
Processed prompts:  89%|████████▉ | 114/128 [00:03<00:00, 37.36it/s, est. speed input: 37528.02 toks/s, output: 36.65 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:03<00:00, 37.38it/s, est. speed input: 37554.55 toks/s, output: 36.67 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:03<00:00, 37.40it/s, est. speed input: 37579.85 toks/s, output: 36.70 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:03<00:00, 37.40it/s, est. speed input: 37602.37 toks/s, output: 36.72 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 37.40it/s, est. speed input: 37614.86 toks/s, output: 36.73 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 36.73it/s, est. speed input: 37614.86 toks/s, output: 36.73 toks/s]
[rank0]:[W125 19:47:47.101472086 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 25.7s

测试结果:
  Requests/s:   34.83
  Tokens/s:     35697.82
  Total Reqs:   128
  Elapsed:      3.68s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     35662.99

============================================================
[3/8] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-3B-INT8                                │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 19:47:52 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=396438) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=396438) WARNING 01-25 19:48:02 [backends.py:609] Failed to read file <frozen os>
Throughput: 39.01 requests/s, 39989.49 total tokens/s, 39.01 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-25 19:47:52] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:47:52] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:47:52] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:47:52] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:47:52] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:47:52] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:47:52] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:47:52] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:47:52] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:47:52] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:47:52] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:47:52] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:47:52] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:47:52] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 19:47:56] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:47:56] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:47:56] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:47:56] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:47:56] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:47:56] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:47:56] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:47:56] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:47:56] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:47:56] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:47:56] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:47:56] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:47:56] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:47:56] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=396438) [2026-01-25 19:47:57] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=396438) [2026-01-25 19:47:57] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=396438) [2026-01-25 19:47:57] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=396438) [2026-01-25 19:47:57] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=396438) [2026-01-25 19:47:57] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=396438) [2026-01-25 19:47:57] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=396438) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=396438) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.05it/s]
(EngineCore_DP0 pid=396438) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.05it/s]
(EngineCore_DP0 pid=396438) 
(EngineCore_DP0 pid=396438) [2026-01-25 19:47:58] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 4928] -> 1D uint8
(EngineCore_DP0 pid=396438) [2026-01-25 19:47:58] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19005440 bytes
(EngineCore_DP0 pid=396438) [2026-01-25 19:47:58] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 4928] -> 1D uint8
(EngineCore_DP0 pid=396438) [2026-01-25 19:47:58] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11403264 bytes
(EngineCore_DP0 pid=396438) [2026-01-25 19:47:58] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 4928] -> 1D uint8
(EngineCore_DP0 pid=396438) [2026-01-25 19:47:58] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 60817408 bytes
(EngineCore_DP0 pid=396438) [2026-01-25 19:47:58] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 13120] -> 1D uint8
(EngineCore_DP0 pid=396438) [2026-01-25 19:47:58] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 30277632 bytes
(EngineCore_DP0 pid=396438) 2026-01-25 19:48:08,000 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=396438) 2026-01-25 19:48:08,014 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=396438) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 25.08it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 25.06it/s]
(EngineCore_DP0 pid=396438) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 1/2 [00:00<00:00,  7.67it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 12.47it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  25%|██▌       | 64/256 [00:00<00:00, 633.25it/s]
Adding requests:  54%|█████▍    | 138/256 [00:00<00:00, 693.46it/s]
Adding requests:  82%|████████▏ | 211/256 [00:00<00:00, 709.93it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 709.92it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   6%|▋         | 16/256 [00:00<00:02, 113.28it/s, est. speed input: 116002.21 toks/s, output: 113.28 toks/s]
Processed prompts:  11%|█         | 28/256 [00:00<00:03, 57.69it/s, est. speed input: 64503.04 toks/s, output: 62.99 toks/s]   
Processed prompts:  14%|█▎        | 35/256 [00:00<00:04, 53.61it/s, est. speed input: 60083.59 toks/s, output: 58.68 toks/s]
Processed prompts:  16%|█▌        | 41/256 [00:00<00:04, 48.93it/s, est. speed input: 56116.99 toks/s, output: 54.80 toks/s]
Processed prompts:  18%|█▊        | 47/256 [00:00<00:04, 45.90it/s, est. speed input: 53482.24 toks/s, output: 52.23 toks/s]
Processed prompts:  20%|██        | 52/256 [00:01<00:04, 41.82it/s, est. speed input: 50638.77 toks/s, output: 49.45 toks/s]
Processed prompts:  22%|██▏       | 57/256 [00:01<00:04, 43.55it/s, est. speed input: 50623.43 toks/s, output: 49.44 toks/s]
Processed prompts:  24%|██▍       | 62/256 [00:01<00:04, 40.05it/s, est. speed input: 48664.98 toks/s, output: 47.52 toks/s]
Processed prompts:  26%|██▌       | 67/256 [00:01<00:04, 42.32it/s, est. speed input: 48811.42 toks/s, output: 47.67 toks/s]
Processed prompts:  28%|██▊       | 72/256 [00:01<00:04, 39.15it/s, est. speed input: 47345.57 toks/s, output: 46.24 toks/s]
Processed prompts:  30%|███       | 77/256 [00:01<00:04, 41.64it/s, est. speed input: 47538.86 toks/s, output: 46.42 toks/s]
Processed prompts:  32%|███▏      | 82/256 [00:01<00:04, 38.59it/s, est. speed input: 46363.13 toks/s, output: 45.28 toks/s]
Processed prompts:  34%|███▎      | 86/256 [00:01<00:04, 38.81it/s, est. speed input: 46049.41 toks/s, output: 44.97 toks/s]
Processed prompts:  35%|███▌      | 90/256 [00:02<00:04, 38.97it/s, est. speed input: 45763.22 toks/s, output: 44.69 toks/s]
Processed prompts:  37%|███▋      | 94/256 [00:02<00:04, 39.06it/s, est. speed input: 45497.25 toks/s, output: 44.43 toks/s]
Processed prompts:  38%|███▊      | 98/256 [00:02<00:04, 39.15it/s, est. speed input: 45261.37 toks/s, output: 44.20 toks/s]
Processed prompts:  40%|███▉      | 102/256 [00:02<00:03, 39.25it/s, est. speed input: 45050.82 toks/s, output: 43.99 toks/s]
Processed prompts:  41%|████▏     | 106/256 [00:02<00:03, 39.27it/s, est. speed input: 44850.35 toks/s, output: 43.80 toks/s]
Processed prompts:  43%|████▎     | 110/256 [00:02<00:03, 39.38it/s, est. speed input: 44678.91 toks/s, output: 43.63 toks/s]
Processed prompts:  45%|████▍     | 114/256 [00:02<00:03, 39.37it/s, est. speed input: 44510.03 toks/s, output: 43.47 toks/s]
Processed prompts:  46%|████▌     | 118/256 [00:02<00:03, 39.37it/s, est. speed input: 44353.02 toks/s, output: 43.31 toks/s]
Processed prompts:  48%|████▊     | 122/256 [00:02<00:03, 39.39it/s, est. speed input: 44210.03 toks/s, output: 43.17 toks/s]
Processed prompts:  49%|████▉     | 126/256 [00:02<00:03, 39.35it/s, est. speed input: 44071.17 toks/s, output: 43.04 toks/s]
Processed prompts:  51%|█████     | 130/256 [00:03<00:03, 39.38it/s, est. speed input: 43948.09 toks/s, output: 42.92 toks/s]
Processed prompts:  52%|█████▏    | 134/256 [00:03<00:03, 39.42it/s, est. speed input: 43835.52 toks/s, output: 42.81 toks/s]
Processed prompts:  54%|█████▍    | 138/256 [00:03<00:02, 39.39it/s, est. speed input: 43723.38 toks/s, output: 42.70 toks/s]
Processed prompts:  55%|█████▌    | 142/256 [00:03<00:02, 39.42it/s, est. speed input: 43622.82 toks/s, output: 42.60 toks/s]
Processed prompts:  57%|█████▋    | 146/256 [00:03<00:02, 39.47it/s, est. speed input: 43531.97 toks/s, output: 42.51 toks/s]
Processed prompts:  59%|█████▊    | 150/256 [00:03<00:02, 39.48it/s, est. speed input: 43443.59 toks/s, output: 42.43 toks/s]
Processed prompts:  60%|██████    | 154/256 [00:03<00:02, 39.48it/s, est. speed input: 43360.12 toks/s, output: 42.34 toks/s]
Processed prompts:  62%|██████▏   | 158/256 [00:03<00:02, 39.46it/s, est. speed input: 43278.04 toks/s, output: 42.26 toks/s]
Processed prompts:  63%|██████▎   | 162/256 [00:03<00:02, 39.40it/s, est. speed input: 43196.68 toks/s, output: 42.18 toks/s]
Processed prompts:  65%|██████▍   | 166/256 [00:03<00:02, 39.39it/s, est. speed input: 43122.40 toks/s, output: 42.11 toks/s]
Processed prompts:  66%|██████▋   | 170/256 [00:04<00:02, 39.42it/s, est. speed input: 43055.10 toks/s, output: 42.05 toks/s]
Processed prompts:  68%|██████▊   | 174/256 [00:04<00:02, 39.39it/s, est. speed input: 42986.30 toks/s, output: 41.98 toks/s]
Processed prompts:  70%|██████▉   | 178/256 [00:04<00:01, 39.37it/s, est. speed input: 42920.93 toks/s, output: 41.91 toks/s]
Processed prompts:  71%|███████   | 182/256 [00:04<00:01, 39.42it/s, est. speed input: 42864.82 toks/s, output: 41.86 toks/s]
Processed prompts:  73%|███████▎  | 186/256 [00:04<00:01, 39.45it/s, est. speed input: 42810.37 toks/s, output: 41.81 toks/s]
Processed prompts:  74%|███████▍  | 190/256 [00:04<00:01, 39.44it/s, est. speed input: 42755.88 toks/s, output: 41.75 toks/s]
Processed prompts:  76%|███████▌  | 194/256 [00:04<00:01, 39.46it/s, est. speed input: 42705.60 toks/s, output: 41.70 toks/s]
Processed prompts:  77%|███████▋  | 198/256 [00:04<00:01, 39.47it/s, est. speed input: 42657.38 toks/s, output: 41.66 toks/s]
Processed prompts:  79%|███████▉  | 202/256 [00:04<00:01, 39.48it/s, est. speed input: 42611.63 toks/s, output: 41.61 toks/s]
Processed prompts:  80%|████████  | 206/256 [00:04<00:01, 39.48it/s, est. speed input: 42566.94 toks/s, output: 41.57 toks/s]
Processed prompts:  82%|████████▏ | 210/256 [00:05<00:01, 39.51it/s, est. speed input: 42526.02 toks/s, output: 41.53 toks/s]
Processed prompts:  84%|████████▎ | 214/256 [00:05<00:01, 39.50it/s, est. speed input: 42484.70 toks/s, output: 41.49 toks/s]
Processed prompts:  85%|████████▌ | 218/256 [00:05<00:00, 39.47it/s, est. speed input: 42443.66 toks/s, output: 41.45 toks/s]
Processed prompts:  87%|████████▋ | 222/256 [00:05<00:00, 39.40it/s, est. speed input: 42400.79 toks/s, output: 41.41 toks/s]
Processed prompts:  88%|████████▊ | 226/256 [00:05<00:00, 39.46it/s, est. speed input: 42366.13 toks/s, output: 41.37 toks/s]
Processed prompts:  90%|████████▉ | 230/256 [00:05<00:00, 39.46it/s, est. speed input: 42330.62 toks/s, output: 41.34 toks/s]
Processed prompts:  91%|█████████▏| 234/256 [00:05<00:00, 39.50it/s, est. speed input: 42298.48 toks/s, output: 41.31 toks/s]
Processed prompts:  93%|█████████▎| 238/256 [00:05<00:00, 39.52it/s, est. speed input: 42267.31 toks/s, output: 41.28 toks/s]
Processed prompts:  95%|█████████▍| 242/256 [00:05<00:00, 39.40it/s, est. speed input: 42229.17 toks/s, output: 41.24 toks/s]
Processed prompts:  96%|█████████▌| 246/256 [00:05<00:00, 39.41it/s, est. speed input: 42197.46 toks/s, output: 41.21 toks/s]
Processed prompts:  98%|█████████▊| 250/256 [00:06<00:00, 39.40it/s, est. speed input: 42165.91 toks/s, output: 41.18 toks/s]
Processed prompts:  99%|█████████▉| 254/256 [00:06<00:00, 39.36it/s, est. speed input: 42133.72 toks/s, output: 41.15 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:06<00:00, 39.36it/s, est. speed input: 42279.75 toks/s, output: 41.29 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:06<00:00, 41.29it/s, est. speed input: 42279.75 toks/s, output: 41.29 toks/s]
[rank0]:[W125 19:48:15.736266205 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 28.6s

测试结果:
  Requests/s:   39.01
  Tokens/s:     39989.49
  Total Reqs:   256
  Elapsed:      6.56s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     39950.48

============================================================
[4/8] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-3B-INT8                                │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 19:48:22 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=397132) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=397132) WARNING 01-25 19:48:31 [backends.py:609] Failed to read file <frozen os>
Throughput: 37.38 requests/s, 38314.36 total tokens/s, 37.38 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-25 19:48:22] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:48:22] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:48:22] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:48:22] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:48:22] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:48:22] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:48:22] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:48:22] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:48:22] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:48:22] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:48:22] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:48:22] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:48:22] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:48:22] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 19:48:26] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:48:26] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:48:26] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:48:26] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:48:26] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:48:26] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:48:26] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:48:26] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:48:26] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:48:26] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:48:26] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:48:26] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:48:26] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:48:26] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=397132) [2026-01-25 19:48:26] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=397132) [2026-01-25 19:48:26] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=397132) [2026-01-25 19:48:26] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=397132) [2026-01-25 19:48:26] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=397132) [2026-01-25 19:48:26] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=397132) [2026-01-25 19:48:26] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=397132) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=397132) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.97it/s]
(EngineCore_DP0 pid=397132) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.97it/s]
(EngineCore_DP0 pid=397132) 
(EngineCore_DP0 pid=397132) [2026-01-25 19:48:27] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 4928] -> 1D uint8
(EngineCore_DP0 pid=397132) [2026-01-25 19:48:27] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19005440 bytes
(EngineCore_DP0 pid=397132) [2026-01-25 19:48:27] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 4928] -> 1D uint8
(EngineCore_DP0 pid=397132) [2026-01-25 19:48:27] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11403264 bytes
(EngineCore_DP0 pid=397132) [2026-01-25 19:48:27] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 4928] -> 1D uint8
(EngineCore_DP0 pid=397132) [2026-01-25 19:48:27] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 60817408 bytes
(EngineCore_DP0 pid=397132) [2026-01-25 19:48:27] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 13120] -> 1D uint8
(EngineCore_DP0 pid=397132) [2026-01-25 19:48:27] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 30277632 bytes
(EngineCore_DP0 pid=397132) 2026-01-25 19:48:37,271 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=397132) 2026-01-25 19:48:37,285 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=397132) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 3/4 [00:00<00:00, 28.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 26.26it/s]
(EngineCore_DP0 pid=397132) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  33%|███▎      | 1/3 [00:00<00:00,  7.58it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 15.62it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:  13%|█▎        | 68/512 [00:00<00:00, 676.72it/s]
Adding requests:  28%|██▊       | 143/512 [00:00<00:00, 716.57it/s]
Adding requests:  42%|████▏     | 215/512 [00:00<00:00, 717.78it/s]
Adding requests:  56%|█████▋    | 289/512 [00:00<00:00, 724.00it/s]
Adding requests:  71%|███████▏  | 365/512 [00:00<00:00, 733.89it/s]
Adding requests:  86%|████████▌ | 439/512 [00:00<00:00, 732.17it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 726.66it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   5%|▌         | 26/512 [00:00<00:02, 213.30it/s, est. speed input: 218442.14 toks/s, output: 213.31 toks/s]
Processed prompts:   9%|▉         | 48/512 [00:00<00:07, 65.12it/s, est. speed input: 75166.18 toks/s, output: 73.40 toks/s]   
Processed prompts:  12%|█▏        | 60/512 [00:00<00:08, 53.47it/s, est. speed input: 63181.86 toks/s, output: 61.70 toks/s]
Processed prompts:  13%|█▎        | 68/512 [00:01<00:09, 48.90it/s, est. speed input: 58750.53 toks/s, output: 57.37 toks/s]
Processed prompts:  15%|█▍        | 75/512 [00:01<00:09, 44.27it/s, est. speed input: 54960.19 toks/s, output: 53.67 toks/s]
Processed prompts:  16%|█▌        | 81/512 [00:01<00:09, 46.35it/s, est. speed input: 55152.24 toks/s, output: 53.86 toks/s]
Processed prompts:  17%|█▋        | 87/512 [00:01<00:10, 40.34it/s, est. speed input: 51884.45 toks/s, output: 50.67 toks/s]
Processed prompts:  18%|█▊        | 92/512 [00:01<00:10, 41.59it/s, est. speed input: 51658.45 toks/s, output: 50.45 toks/s]
Processed prompts:  19%|█▉        | 97/512 [00:01<00:09, 42.74it/s, est. speed input: 51462.77 toks/s, output: 50.26 toks/s]
Processed prompts:  20%|█▉        | 102/512 [00:02<00:11, 35.42it/s, est. speed input: 48733.97 toks/s, output: 47.59 toks/s]
Processed prompts:  21%|██        | 106/512 [00:02<00:11, 35.88it/s, est. speed input: 48250.40 toks/s, output: 47.12 toks/s]
Processed prompts:  21%|██▏       | 110/512 [00:02<00:11, 36.28it/s, est. speed input: 47809.70 toks/s, output: 46.69 toks/s]
Processed prompts:  22%|██▏       | 114/512 [00:02<00:10, 36.60it/s, est. speed input: 47405.48 toks/s, output: 46.29 toks/s]
Processed prompts:  23%|██▎       | 118/512 [00:02<00:10, 36.87it/s, est. speed input: 47037.96 toks/s, output: 45.94 toks/s]
Processed prompts:  24%|██▍       | 122/512 [00:02<00:10, 37.05it/s, est. speed input: 46694.30 toks/s, output: 45.60 toks/s]
Processed prompts:  25%|██▍       | 126/512 [00:02<00:10, 37.18it/s, est. speed input: 46376.57 toks/s, output: 45.29 toks/s]
Processed prompts:  25%|██▌       | 130/512 [00:02<00:10, 37.32it/s, est. speed input: 46089.89 toks/s, output: 45.01 toks/s]
Processed prompts:  26%|██▌       | 134/512 [00:02<00:10, 37.40it/s, est. speed input: 45819.70 toks/s, output: 44.75 toks/s]
Processed prompts:  27%|██▋       | 138/512 [00:03<00:09, 37.46it/s, est. speed input: 45568.72 toks/s, output: 44.50 toks/s]
Processed prompts:  28%|██▊       | 142/512 [00:03<00:09, 37.47it/s, est. speed input: 45330.03 toks/s, output: 44.27 toks/s]
Processed prompts:  29%|██▊       | 146/512 [00:03<00:09, 37.50it/s, est. speed input: 45110.23 toks/s, output: 44.05 toks/s]
Processed prompts:  29%|██▉       | 150/512 [00:03<00:09, 37.53it/s, est. speed input: 44904.48 toks/s, output: 43.85 toks/s]
Processed prompts:  30%|███       | 154/512 [00:03<00:09, 37.50it/s, est. speed input: 44705.77 toks/s, output: 43.66 toks/s]
Processed prompts:  31%|███       | 158/512 [00:03<00:09, 37.52it/s, est. speed input: 44523.17 toks/s, output: 43.48 toks/s]
Processed prompts:  32%|███▏      | 162/512 [00:03<00:09, 37.54it/s, est. speed input: 44351.44 toks/s, output: 43.31 toks/s]
Processed prompts:  32%|███▏      | 166/512 [00:03<00:09, 37.52it/s, est. speed input: 44185.59 toks/s, output: 43.15 toks/s]
Processed prompts:  33%|███▎      | 170/512 [00:03<00:09, 37.51it/s, est. speed input: 44029.16 toks/s, output: 43.00 toks/s]
Processed prompts:  34%|███▍      | 174/512 [00:04<00:09, 37.52it/s, est. speed input: 43882.71 toks/s, output: 42.85 toks/s]
Processed prompts:  35%|███▍      | 178/512 [00:04<00:08, 37.49it/s, est. speed input: 43740.18 toks/s, output: 42.71 toks/s]
Processed prompts:  36%|███▌      | 182/512 [00:04<00:08, 37.52it/s, est. speed input: 43609.50 toks/s, output: 42.59 toks/s]
Processed prompts:  36%|███▋      | 186/512 [00:04<00:08, 37.54it/s, est. speed input: 43485.16 toks/s, output: 42.47 toks/s]
Processed prompts:  37%|███▋      | 190/512 [00:04<00:08, 37.54it/s, est. speed input: 43364.99 toks/s, output: 42.35 toks/s]
Processed prompts:  38%|███▊      | 194/512 [00:04<00:08, 37.55it/s, est. speed input: 43251.43 toks/s, output: 42.24 toks/s]
Processed prompts:  39%|███▊      | 198/512 [00:04<00:08, 37.55it/s, est. speed input: 43143.15 toks/s, output: 42.13 toks/s]
Processed prompts:  39%|███▉      | 202/512 [00:04<00:08, 37.58it/s, est. speed input: 43041.63 toks/s, output: 42.03 toks/s]
Processed prompts:  40%|████      | 206/512 [00:04<00:08, 37.60it/s, est. speed input: 42944.48 toks/s, output: 41.94 toks/s]
Processed prompts:  41%|████      | 210/512 [00:05<00:08, 37.62it/s, est. speed input: 42851.76 toks/s, output: 41.85 toks/s]
Processed prompts:  42%|████▏     | 214/512 [00:05<00:07, 37.63it/s, est. speed input: 42762.80 toks/s, output: 41.76 toks/s]
Processed prompts:  43%|████▎     | 218/512 [00:05<00:07, 37.64it/s, est. speed input: 42677.31 toks/s, output: 41.68 toks/s]
Processed prompts:  43%|████▎     | 222/512 [00:05<00:07, 37.61it/s, est. speed input: 42592.86 toks/s, output: 41.59 toks/s]
Processed prompts:  44%|████▍     | 226/512 [00:05<00:07, 37.59it/s, est. speed input: 42512.03 toks/s, output: 41.52 toks/s]
Processed prompts:  45%|████▍     | 230/512 [00:05<00:07, 37.59it/s, est. speed input: 42434.93 toks/s, output: 41.44 toks/s]
Processed prompts:  46%|████▌     | 234/512 [00:05<00:07, 37.56it/s, est. speed input: 42358.80 toks/s, output: 41.37 toks/s]
Processed prompts:  46%|████▋     | 238/512 [00:05<00:07, 37.52it/s, est. speed input: 42284.06 toks/s, output: 41.29 toks/s]
Processed prompts:  47%|████▋     | 242/512 [00:05<00:07, 37.52it/s, est. speed input: 42213.60 toks/s, output: 41.22 toks/s]
Processed prompts:  48%|████▊     | 246/512 [00:05<00:07, 37.51it/s, est. speed input: 42145.36 toks/s, output: 41.16 toks/s]
Processed prompts:  49%|████▉     | 250/512 [00:06<00:06, 37.51it/s, est. speed input: 42079.87 toks/s, output: 41.09 toks/s]
Processed prompts:  50%|████▉     | 254/512 [00:06<00:06, 37.51it/s, est. speed input: 42016.76 toks/s, output: 41.03 toks/s]
Processed prompts:  50%|█████     | 258/512 [00:06<00:06, 37.50it/s, est. speed input: 41955.16 toks/s, output: 40.97 toks/s]
Processed prompts:  51%|█████     | 262/512 [00:06<00:06, 37.54it/s, est. speed input: 41898.18 toks/s, output: 40.92 toks/s]
Processed prompts:  52%|█████▏    | 266/512 [00:06<00:06, 37.51it/s, est. speed input: 41839.53 toks/s, output: 40.86 toks/s]
Processed prompts:  53%|█████▎    | 270/512 [00:06<00:06, 37.54it/s, est. speed input: 41786.14 toks/s, output: 40.81 toks/s]
Processed prompts:  54%|█████▎    | 274/512 [00:06<00:06, 37.53it/s, est. speed input: 41732.71 toks/s, output: 40.75 toks/s]
Processed prompts:  54%|█████▍    | 278/512 [00:06<00:06, 37.55it/s, est. speed input: 41682.49 toks/s, output: 40.71 toks/s]
Processed prompts:  55%|█████▌    | 282/512 [00:06<00:06, 37.56it/s, est. speed input: 41633.34 toks/s, output: 40.66 toks/s]
Processed prompts:  56%|█████▌    | 286/512 [00:07<00:06, 37.59it/s, est. speed input: 41586.72 toks/s, output: 40.61 toks/s]
Processed prompts:  57%|█████▋    | 290/512 [00:07<00:05, 37.61it/s, est. speed input: 41541.72 toks/s, output: 40.57 toks/s]
Processed prompts:  57%|█████▋    | 294/512 [00:07<00:05, 37.60it/s, est. speed input: 41497.03 toks/s, output: 40.52 toks/s]
Processed prompts:  58%|█████▊    | 298/512 [00:07<00:05, 37.61it/s, est. speed input: 41454.03 toks/s, output: 40.48 toks/s]
Processed prompts:  59%|█████▉    | 302/512 [00:07<00:05, 37.57it/s, est. speed input: 41410.25 toks/s, output: 40.44 toks/s]
Processed prompts:  60%|█████▉    | 306/512 [00:07<00:05, 37.57it/s, est. speed input: 41368.74 toks/s, output: 40.40 toks/s]
Processed prompts:  61%|██████    | 310/512 [00:07<00:05, 37.57it/s, est. speed input: 41328.87 toks/s, output: 40.36 toks/s]
Processed prompts:  61%|██████▏   | 314/512 [00:07<00:05, 37.57it/s, est. speed input: 41289.91 toks/s, output: 40.32 toks/s]
Processed prompts:  62%|██████▏   | 318/512 [00:07<00:05, 37.55it/s, est. speed input: 41250.51 toks/s, output: 40.28 toks/s]
Processed prompts:  63%|██████▎   | 322/512 [00:08<00:05, 37.51it/s, est. speed input: 41211.51 toks/s, output: 40.25 toks/s]
Processed prompts:  64%|██████▎   | 326/512 [00:08<00:04, 37.49it/s, est. speed input: 41173.57 toks/s, output: 40.21 toks/s]
Processed prompts:  64%|██████▍   | 330/512 [00:08<00:04, 37.52it/s, est. speed input: 41138.74 toks/s, output: 40.17 toks/s]
Processed prompts:  65%|██████▌   | 334/512 [00:08<00:04, 37.53it/s, est. speed input: 41104.62 toks/s, output: 40.14 toks/s]
Processed prompts:  66%|██████▌   | 338/512 [00:08<00:04, 37.53it/s, est. speed input: 41070.87 toks/s, output: 40.11 toks/s]
Processed prompts:  67%|██████▋   | 342/512 [00:08<00:04, 37.51it/s, est. speed input: 41036.83 toks/s, output: 40.07 toks/s]
Processed prompts:  68%|██████▊   | 346/512 [00:08<00:04, 37.47it/s, est. speed input: 41002.69 toks/s, output: 40.04 toks/s]
Processed prompts:  68%|██████▊   | 350/512 [00:08<00:04, 37.47it/s, est. speed input: 40970.60 toks/s, output: 40.01 toks/s]
Processed prompts:  69%|██████▉   | 354/512 [00:08<00:04, 37.52it/s, est. speed input: 40941.37 toks/s, output: 39.98 toks/s]
Processed prompts:  70%|██████▉   | 358/512 [00:08<00:04, 37.54it/s, est. speed input: 40912.04 toks/s, output: 39.95 toks/s]
Processed prompts:  71%|███████   | 362/512 [00:09<00:03, 37.56it/s, est. speed input: 40884.05 toks/s, output: 39.93 toks/s]
Processed prompts:  71%|███████▏  | 366/512 [00:09<00:03, 37.54it/s, est. speed input: 40854.86 toks/s, output: 39.90 toks/s]
Processed prompts:  72%|███████▏  | 370/512 [00:09<00:03, 37.51it/s, est. speed input: 40825.90 toks/s, output: 39.87 toks/s]
Processed prompts:  73%|███████▎  | 374/512 [00:09<00:03, 37.53it/s, est. speed input: 40799.53 toks/s, output: 39.84 toks/s]
Processed prompts:  74%|███████▍  | 378/512 [00:09<00:03, 37.53it/s, est. speed input: 40772.73 toks/s, output: 39.82 toks/s]
Processed prompts:  75%|███████▍  | 382/512 [00:09<00:03, 37.53it/s, est. speed input: 40746.77 toks/s, output: 39.79 toks/s]
Processed prompts:  75%|███████▌  | 386/512 [00:09<00:03, 37.53it/s, est. speed input: 40721.52 toks/s, output: 39.77 toks/s]
Processed prompts:  76%|███████▌  | 390/512 [00:09<00:03, 37.56it/s, est. speed input: 40697.55 toks/s, output: 39.74 toks/s]
Processed prompts:  77%|███████▋  | 394/512 [00:09<00:03, 37.56it/s, est. speed input: 40673.61 toks/s, output: 39.72 toks/s]
Processed prompts:  78%|███████▊  | 398/512 [00:10<00:03, 37.52it/s, est. speed input: 40648.67 toks/s, output: 39.70 toks/s]
Processed prompts:  79%|███████▊  | 402/512 [00:10<00:02, 37.50it/s, est. speed input: 40624.59 toks/s, output: 39.67 toks/s]
Processed prompts:  79%|███████▉  | 406/512 [00:10<00:02, 37.54it/s, est. speed input: 40602.68 toks/s, output: 39.65 toks/s]
Processed prompts:  80%|████████  | 410/512 [00:10<00:02, 37.50it/s, est. speed input: 40578.90 toks/s, output: 39.63 toks/s]
Processed prompts:  81%|████████  | 414/512 [00:10<00:02, 37.46it/s, est. speed input: 40555.22 toks/s, output: 39.60 toks/s]
Processed prompts:  82%|████████▏ | 418/512 [00:10<00:02, 37.50it/s, est. speed input: 40534.67 toks/s, output: 39.58 toks/s]
Processed prompts:  82%|████████▏ | 422/512 [00:10<00:02, 37.50it/s, est. speed input: 40513.39 toks/s, output: 39.56 toks/s]
Processed prompts:  83%|████████▎ | 426/512 [00:10<00:02, 37.48it/s, est. speed input: 40491.75 toks/s, output: 39.54 toks/s]
Processed prompts:  84%|████████▍ | 430/512 [00:10<00:02, 37.55it/s, est. speed input: 40473.34 toks/s, output: 39.52 toks/s]
Processed prompts:  85%|████████▍ | 434/512 [00:10<00:02, 37.53it/s, est. speed input: 40453.17 toks/s, output: 39.51 toks/s]
Processed prompts:  86%|████████▌ | 438/512 [00:11<00:01, 37.58it/s, est. speed input: 40435.48 toks/s, output: 39.49 toks/s]
Processed prompts:  86%|████████▋ | 442/512 [00:11<00:01, 37.57it/s, est. speed input: 40416.41 toks/s, output: 39.47 toks/s]
Processed prompts:  87%|████████▋ | 446/512 [00:11<00:01, 37.58it/s, est. speed input: 40398.55 toks/s, output: 39.45 toks/s]
Processed prompts:  88%|████████▊ | 450/512 [00:11<00:01, 37.59it/s, est. speed input: 40381.05 toks/s, output: 39.43 toks/s]
Processed prompts:  89%|████████▊ | 454/512 [00:11<00:01, 37.56it/s, est. speed input: 40362.66 toks/s, output: 39.42 toks/s]
Processed prompts:  89%|████████▉ | 458/512 [00:11<00:01, 37.54it/s, est. speed input: 40344.38 toks/s, output: 39.40 toks/s]
Processed prompts:  90%|█████████ | 462/512 [00:11<00:01, 37.50it/s, est. speed input: 40326.05 toks/s, output: 39.38 toks/s]
Processed prompts:  91%|█████████ | 466/512 [00:11<00:01, 37.53it/s, est. speed input: 40309.44 toks/s, output: 39.36 toks/s]
Processed prompts:  92%|█████████▏| 470/512 [00:11<00:01, 37.51it/s, est. speed input: 40292.02 toks/s, output: 39.35 toks/s]
Processed prompts:  93%|█████████▎| 474/512 [00:12<00:01, 37.49it/s, est. speed input: 40274.71 toks/s, output: 39.33 toks/s]
Processed prompts:  93%|█████████▎| 478/512 [00:12<00:00, 37.46it/s, est. speed input: 40257.32 toks/s, output: 39.31 toks/s]
Processed prompts:  94%|█████████▍| 482/512 [00:12<00:00, 37.43it/s, est. speed input: 40239.82 toks/s, output: 39.30 toks/s]
Processed prompts:  95%|█████████▍| 486/512 [00:12<00:00, 37.44it/s, est. speed input: 40223.67 toks/s, output: 39.28 toks/s]
Processed prompts:  96%|█████████▌| 490/512 [00:12<00:00, 37.42it/s, est. speed input: 40206.84 toks/s, output: 39.26 toks/s]
Processed prompts:  96%|█████████▋| 494/512 [00:12<00:00, 37.43it/s, est. speed input: 40191.15 toks/s, output: 39.25 toks/s]
Processed prompts:  97%|█████████▋| 498/512 [00:12<00:00, 37.45it/s, est. speed input: 40176.17 toks/s, output: 39.23 toks/s]
Processed prompts:  98%|█████████▊| 502/512 [00:12<00:00, 37.46it/s, est. speed input: 40161.14 toks/s, output: 39.22 toks/s]
Processed prompts:  99%|█████████▉| 506/512 [00:12<00:00, 37.49it/s, est. speed input: 40147.15 toks/s, output: 39.21 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:12<00:00, 37.49it/s, est. speed input: 40359.25 toks/s, output: 39.41 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:12<00:00, 39.41it/s, est. speed input: 40359.25 toks/s, output: 39.41 toks/s]
[rank0]:[W125 19:48:52.229484942 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 36.5s

测试结果:
  Requests/s:   37.38
  Tokens/s:     38314.36
  Total Reqs:   512
  Elapsed:      13.70s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     38276.98

============================================================
[5/8] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-3B-INT8                                │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 19:49:00 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=397919) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=397919) WARNING 01-25 19:49:09 [backends.py:609] Failed to read file <frozen os>
Throughput: 36.31 requests/s, 37220.32 total tokens/s, 36.31 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-25 19:49:00] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:49:00] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:49:00] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:49:00] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:49:00] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:49:00] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:49:00] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:49:00] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:49:00] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:49:00] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:49:00] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:49:00] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:49:00] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:49:00] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 19:49:03] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:49:04] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:49:04] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:49:04] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:49:04] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:49:04] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:49:04] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:49:04] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:49:04] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:49:04] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:49:04] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:49:04] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:49:04] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:49:04] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=397919) [2026-01-25 19:49:04] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=397919) [2026-01-25 19:49:04] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=397919) [2026-01-25 19:49:04] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=397919) [2026-01-25 19:49:04] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=397919) [2026-01-25 19:49:04] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=397919) [2026-01-25 19:49:04] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=397919) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=397919) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.01it/s]
(EngineCore_DP0 pid=397919) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.01it/s]
(EngineCore_DP0 pid=397919) 
(EngineCore_DP0 pid=397919) [2026-01-25 19:49:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 4928] -> 1D uint8
(EngineCore_DP0 pid=397919) [2026-01-25 19:49:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19005440 bytes
(EngineCore_DP0 pid=397919) [2026-01-25 19:49:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 4928] -> 1D uint8
(EngineCore_DP0 pid=397919) [2026-01-25 19:49:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11403264 bytes
(EngineCore_DP0 pid=397919) [2026-01-25 19:49:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 4928] -> 1D uint8
(EngineCore_DP0 pid=397919) [2026-01-25 19:49:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 60817408 bytes
(EngineCore_DP0 pid=397919) [2026-01-25 19:49:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 13120] -> 1D uint8
(EngineCore_DP0 pid=397919) [2026-01-25 19:49:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 30277632 bytes
(EngineCore_DP0 pid=397919) 2026-01-25 19:49:15,018 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=397919) 2026-01-25 19:49:15,040 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=397919) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 1/5 [00:00<00:00,  5.55it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 3/5 [00:00<00:00, 10.65it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 12.93it/s]
(EngineCore_DP0 pid=397919) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 1/4 [00:00<00:00,  7.62it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 17.94it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   7%|▋         | 73/1024 [00:00<00:01, 728.75it/s]
Adding requests:  14%|█▍        | 148/1024 [00:00<00:01, 737.57it/s]
Adding requests:  22%|██▏       | 224/1024 [00:00<00:01, 747.66it/s]
Adding requests:  29%|██▉       | 300/1024 [00:00<00:00, 750.14it/s]
Adding requests:  37%|███▋      | 376/1024 [00:00<00:00, 751.91it/s]
Adding requests:  44%|████▍     | 452/1024 [00:00<00:00, 742.63it/s]
Adding requests:  51%|█████▏    | 527/1024 [00:00<00:00, 734.56it/s]
Adding requests:  59%|█████▉    | 606/1024 [00:00<00:00, 749.52it/s]
Adding requests:  67%|██████▋   | 687/1024 [00:00<00:00, 765.31it/s]
Adding requests:  75%|███████▍  | 766/1024 [00:01<00:00, 769.98it/s]
Adding requests:  82%|████████▏ | 844/1024 [00:01<00:00, 753.47it/s]
Adding requests:  90%|█████████ | 922/1024 [00:01<00:00, 758.01it/s]
Adding requests:  98%|█████████▊| 999/1024 [00:01<00:00, 761.02it/s]
Adding requests: 100%|██████████| 1024/1024 [00:01<00:00, 754.76it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   5%|▍         | 50/1024 [00:00<00:05, 193.96it/s, est. speed input: 198620.76 toks/s, output: 193.96 toks/s]
Processed prompts:   7%|▋         | 70/1024 [00:00<00:10, 88.82it/s, est. speed input: 102901.60 toks/s, output: 100.49 toks/s] 
Processed prompts:   8%|▊         | 81/1024 [00:00<00:12, 75.82it/s, est. speed input: 90551.62 toks/s, output: 88.43 toks/s]  
Processed prompts:   9%|▉         | 90/1024 [00:01<00:19, 48.74it/s, est. speed input: 68039.39 toks/s, output: 66.44 toks/s]
Processed prompts:  10%|▉         | 98/1024 [00:01<00:20, 45.56it/s, est. speed input: 63758.91 toks/s, output: 62.26 toks/s]
Processed prompts:  10%|█         | 106/1024 [00:01<00:21, 43.11it/s, est. speed input: 60530.48 toks/s, output: 59.11 toks/s]
Processed prompts:  11%|█         | 114/1024 [00:02<00:22, 41.28it/s, est. speed input: 58015.13 toks/s, output: 56.66 toks/s]
Processed prompts:  12%|█▏        | 122/1024 [00:02<00:22, 39.91it/s, est. speed input: 55984.84 toks/s, output: 54.67 toks/s]
Processed prompts:  13%|█▎        | 130/1024 [00:02<00:22, 38.91it/s, est. speed input: 54315.37 toks/s, output: 53.04 toks/s]
Processed prompts:  13%|█▎        | 138/1024 [00:02<00:23, 38.19it/s, est. speed input: 52916.26 toks/s, output: 51.68 toks/s]
Processed prompts:  14%|█▍        | 146/1024 [00:02<00:23, 37.68it/s, est. speed input: 51734.96 toks/s, output: 50.52 toks/s]
Processed prompts:  15%|█▌        | 154/1024 [00:03<00:23, 37.31it/s, est. speed input: 50715.73 toks/s, output: 49.53 toks/s]
Processed prompts:  16%|█▌        | 162/1024 [00:03<00:23, 37.06it/s, est. speed input: 49834.82 toks/s, output: 48.67 toks/s]
Processed prompts:  17%|█▋        | 170/1024 [00:03<00:23, 36.88it/s, est. speed input: 49060.14 toks/s, output: 47.91 toks/s]
Processed prompts:  17%|█▋        | 178/1024 [00:03<00:23, 36.74it/s, est. speed input: 48375.06 toks/s, output: 47.24 toks/s]
Processed prompts:  18%|█▊        | 186/1024 [00:03<00:22, 36.66it/s, est. speed input: 47767.78 toks/s, output: 46.65 toks/s]
Processed prompts:  19%|█▉        | 194/1024 [00:04<00:22, 36.61it/s, est. speed input: 47226.33 toks/s, output: 46.12 toks/s]
Processed prompts:  20%|█▉        | 202/1024 [00:04<00:22, 36.56it/s, est. speed input: 46733.95 toks/s, output: 45.64 toks/s]
Processed prompts:  21%|██        | 210/1024 [00:04<00:22, 36.52it/s, est. speed input: 46289.23 toks/s, output: 45.20 toks/s]
Processed prompts:  21%|██▏       | 218/1024 [00:04<00:22, 36.49it/s, est. speed input: 45883.05 toks/s, output: 44.81 toks/s]
Processed prompts:  22%|██▏       | 226/1024 [00:05<00:21, 36.48it/s, est. speed input: 45513.20 toks/s, output: 44.45 toks/s]
Processed prompts:  23%|██▎       | 234/1024 [00:05<00:21, 36.48it/s, est. speed input: 45175.55 toks/s, output: 44.12 toks/s]
Processed prompts:  24%|██▎       | 242/1024 [00:05<00:21, 36.47it/s, est. speed input: 44863.21 toks/s, output: 43.81 toks/s]
Processed prompts:  24%|██▍       | 250/1024 [00:05<00:21, 36.45it/s, est. speed input: 44572.84 toks/s, output: 43.53 toks/s]
Processed prompts:  25%|██▌       | 258/1024 [00:05<00:21, 36.45it/s, est. speed input: 44307.14 toks/s, output: 43.27 toks/s]
Processed prompts:  26%|██▌       | 266/1024 [00:06<00:20, 36.45it/s, est. speed input: 44058.37 toks/s, output: 43.03 toks/s]
Processed prompts:  27%|██▋       | 274/1024 [00:06<00:20, 36.44it/s, est. speed input: 43825.97 toks/s, output: 42.80 toks/s]
Processed prompts:  28%|██▊       | 282/1024 [00:06<00:20, 36.43it/s, est. speed input: 43608.97 toks/s, output: 42.59 toks/s]
Processed prompts:  28%|██▊       | 290/1024 [00:06<00:20, 36.42it/s, est. speed input: 43405.15 toks/s, output: 42.39 toks/s]
Processed prompts:  29%|██▉       | 298/1024 [00:07<00:19, 36.42it/s, est. speed input: 43215.52 toks/s, output: 42.20 toks/s]
Processed prompts:  30%|██▉       | 306/1024 [00:07<00:19, 36.41it/s, est. speed input: 43035.00 toks/s, output: 42.03 toks/s]
Processed prompts:  31%|███       | 314/1024 [00:07<00:19, 36.42it/s, est. speed input: 42867.47 toks/s, output: 41.86 toks/s]
Processed prompts:  31%|███▏      | 322/1024 [00:07<00:19, 36.39it/s, est. speed input: 42706.35 toks/s, output: 41.71 toks/s]
Processed prompts:  32%|███▏      | 330/1024 [00:07<00:19, 36.41it/s, est. speed input: 42557.10 toks/s, output: 41.56 toks/s]
Processed prompts:  33%|███▎      | 338/1024 [00:08<00:18, 36.42it/s, est. speed input: 42416.56 toks/s, output: 41.42 toks/s]
Processed prompts:  34%|███▍      | 346/1024 [00:08<00:18, 36.43it/s, est. speed input: 42282.95 toks/s, output: 41.29 toks/s]
Processed prompts:  35%|███▍      | 354/1024 [00:08<00:18, 36.42it/s, est. speed input: 42155.41 toks/s, output: 41.17 toks/s]
Processed prompts:  35%|███▌      | 362/1024 [00:08<00:18, 36.40it/s, est. speed input: 42032.55 toks/s, output: 41.05 toks/s]
Processed prompts:  36%|███▌      | 370/1024 [00:09<00:17, 36.41it/s, est. speed input: 41917.51 toks/s, output: 40.94 toks/s]
Processed prompts:  37%|███▋      | 378/1024 [00:09<00:17, 36.41it/s, est. speed input: 41807.69 toks/s, output: 40.83 toks/s]
Processed prompts:  38%|███▊      | 386/1024 [00:09<00:17, 36.40it/s, est. speed input: 41701.98 toks/s, output: 40.72 toks/s]
Processed prompts:  38%|███▊      | 394/1024 [00:09<00:17, 36.38it/s, est. speed input: 41599.53 toks/s, output: 40.62 toks/s]
Processed prompts:  39%|███▉      | 402/1024 [00:09<00:17, 36.40it/s, est. speed input: 41505.13 toks/s, output: 40.53 toks/s]
Processed prompts:  40%|████      | 410/1024 [00:10<00:16, 36.40it/s, est. speed input: 41413.49 toks/s, output: 40.44 toks/s]
Processed prompts:  41%|████      | 418/1024 [00:10<00:16, 36.41it/s, est. speed input: 41326.49 toks/s, output: 40.36 toks/s]
Processed prompts:  42%|████▏     | 426/1024 [00:10<00:16, 36.41it/s, est. speed input: 41242.34 toks/s, output: 40.28 toks/s]
Processed prompts:  42%|████▏     | 434/1024 [00:10<00:16, 36.41it/s, est. speed input: 41161.65 toks/s, output: 40.20 toks/s]
Processed prompts:  43%|████▎     | 442/1024 [00:11<00:15, 36.40it/s, est. speed input: 41083.61 toks/s, output: 40.12 toks/s]
Processed prompts:  44%|████▍     | 450/1024 [00:11<00:15, 36.40it/s, est. speed input: 41009.08 toks/s, output: 40.05 toks/s]
Processed prompts:  45%|████▍     | 458/1024 [00:11<00:15, 36.38it/s, est. speed input: 40936.00 toks/s, output: 39.98 toks/s]
Processed prompts:  46%|████▌     | 466/1024 [00:11<00:15, 36.36it/s, est. speed input: 40865.47 toks/s, output: 39.91 toks/s]
Processed prompts:  46%|████▋     | 474/1024 [00:11<00:15, 36.38it/s, est. speed input: 40799.17 toks/s, output: 39.84 toks/s]
Processed prompts:  47%|████▋     | 482/1024 [00:12<00:14, 36.36it/s, est. speed input: 40733.74 toks/s, output: 39.78 toks/s]
Processed prompts:  48%|████▊     | 490/1024 [00:12<00:14, 36.37it/s, est. speed input: 40671.86 toks/s, output: 39.72 toks/s]
Processed prompts:  49%|████▊     | 498/1024 [00:12<00:14, 36.37it/s, est. speed input: 40611.79 toks/s, output: 39.66 toks/s]
Processed prompts:  49%|████▉     | 506/1024 [00:12<00:14, 36.34it/s, est. speed input: 40551.65 toks/s, output: 39.60 toks/s]
Processed prompts:  50%|█████     | 514/1024 [00:12<00:14, 36.36it/s, est. speed input: 40496.67 toks/s, output: 39.55 toks/s]
Processed prompts:  51%|█████     | 522/1024 [00:13<00:13, 36.38it/s, est. speed input: 40443.60 toks/s, output: 39.50 toks/s]
Processed prompts:  52%|█████▏    | 530/1024 [00:13<00:13, 36.37it/s, est. speed input: 40390.81 toks/s, output: 39.44 toks/s]
Processed prompts:  53%|█████▎    | 538/1024 [00:13<00:13, 36.38it/s, est. speed input: 40340.51 toks/s, output: 39.39 toks/s]
Processed prompts:  53%|█████▎    | 546/1024 [00:13<00:13, 36.36it/s, est. speed input: 40290.42 toks/s, output: 39.35 toks/s]
Processed prompts:  54%|█████▍    | 554/1024 [00:14<00:12, 36.37it/s, est. speed input: 40243.09 toks/s, output: 39.30 toks/s]
Processed prompts:  55%|█████▍    | 562/1024 [00:14<00:12, 36.38it/s, est. speed input: 40197.79 toks/s, output: 39.26 toks/s]
Processed prompts:  56%|█████▌    | 570/1024 [00:14<00:12, 36.37it/s, est. speed input: 40152.71 toks/s, output: 39.21 toks/s]
Processed prompts:  56%|█████▋    | 578/1024 [00:14<00:12, 36.36it/s, est. speed input: 40108.72 toks/s, output: 39.17 toks/s]
Processed prompts:  57%|█████▋    | 586/1024 [00:14<00:12, 36.36it/s, est. speed input: 40066.52 toks/s, output: 39.13 toks/s]
Processed prompts:  58%|█████▊    | 594/1024 [00:15<00:11, 36.35it/s, est. speed input: 40024.98 toks/s, output: 39.09 toks/s]
Processed prompts:  59%|█████▉    | 602/1024 [00:15<00:11, 36.36it/s, est. speed input: 39985.70 toks/s, output: 39.05 toks/s]
Processed prompts:  60%|█████▉    | 610/1024 [00:15<00:11, 36.36it/s, est. speed input: 39946.60 toks/s, output: 39.01 toks/s]
Processed prompts:  60%|██████    | 618/1024 [00:15<00:11, 36.35it/s, est. speed input: 39908.33 toks/s, output: 38.97 toks/s]
Processed prompts:  61%|██████    | 626/1024 [00:16<00:10, 36.35it/s, est. speed input: 39871.86 toks/s, output: 38.94 toks/s]
Processed prompts:  62%|██████▏   | 634/1024 [00:16<00:10, 36.36it/s, est. speed input: 39836.46 toks/s, output: 38.90 toks/s]
Processed prompts:  63%|██████▎   | 642/1024 [00:16<00:10, 36.36it/s, est. speed input: 39801.91 toks/s, output: 38.87 toks/s]
Processed prompts:  63%|██████▎   | 650/1024 [00:16<00:10, 36.34it/s, est. speed input: 39767.24 toks/s, output: 38.84 toks/s]
Processed prompts:  64%|██████▍   | 658/1024 [00:16<00:10, 36.35it/s, est. speed input: 39734.38 toks/s, output: 38.80 toks/s]
Processed prompts:  65%|██████▌   | 666/1024 [00:17<00:09, 36.37it/s, est. speed input: 39703.33 toks/s, output: 38.77 toks/s]
Processed prompts:  66%|██████▌   | 674/1024 [00:17<00:09, 36.38it/s, est. speed input: 39672.47 toks/s, output: 38.74 toks/s]
Processed prompts:  67%|██████▋   | 682/1024 [00:17<00:09, 36.36it/s, est. speed input: 39641.52 toks/s, output: 38.71 toks/s]
Processed prompts:  67%|██████▋   | 690/1024 [00:17<00:09, 36.33it/s, est. speed input: 39610.43 toks/s, output: 38.68 toks/s]
Processed prompts:  68%|██████▊   | 698/1024 [00:18<00:08, 36.34it/s, est. speed input: 39581.42 toks/s, output: 38.65 toks/s]
Processed prompts:  69%|██████▉   | 706/1024 [00:18<00:08, 36.33it/s, est. speed input: 39552.37 toks/s, output: 38.63 toks/s]
Processed prompts:  70%|██████▉   | 714/1024 [00:18<00:08, 36.32it/s, est. speed input: 39524.05 toks/s, output: 38.60 toks/s]
Processed prompts:  71%|███████   | 722/1024 [00:18<00:08, 36.31it/s, est. speed input: 39496.11 toks/s, output: 38.57 toks/s]
Processed prompts:  71%|███████▏  | 730/1024 [00:18<00:08, 36.32it/s, est. speed input: 39469.62 toks/s, output: 38.54 toks/s]
Processed prompts:  72%|███████▏  | 738/1024 [00:19<00:07, 36.33it/s, est. speed input: 39443.99 toks/s, output: 38.52 toks/s]
Processed prompts:  73%|███████▎  | 746/1024 [00:19<00:07, 36.36it/s, est. speed input: 39419.68 toks/s, output: 38.50 toks/s]
Processed prompts:  74%|███████▎  | 754/1024 [00:19<00:07, 36.35it/s, est. speed input: 39394.76 toks/s, output: 38.47 toks/s]
Processed prompts:  74%|███████▍  | 762/1024 [00:19<00:07, 36.33it/s, est. speed input: 39369.79 toks/s, output: 38.45 toks/s]
Processed prompts:  75%|███████▌  | 770/1024 [00:20<00:06, 36.33it/s, est. speed input: 39346.07 toks/s, output: 38.42 toks/s]
Processed prompts:  76%|███████▌  | 778/1024 [00:20<00:06, 36.32it/s, est. speed input: 39322.47 toks/s, output: 38.40 toks/s]
Processed prompts:  77%|███████▋  | 786/1024 [00:20<00:06, 36.33it/s, est. speed input: 39299.93 toks/s, output: 38.38 toks/s]
Processed prompts:  78%|███████▊  | 794/1024 [00:20<00:06, 36.32it/s, est. speed input: 39277.22 toks/s, output: 38.36 toks/s]
Processed prompts:  78%|███████▊  | 802/1024 [00:20<00:06, 36.33it/s, est. speed input: 39255.68 toks/s, output: 38.34 toks/s]
Processed prompts:  79%|███████▉  | 810/1024 [00:21<00:05, 36.33it/s, est. speed input: 39234.33 toks/s, output: 38.31 toks/s]
Processed prompts:  80%|███████▉  | 818/1024 [00:21<00:05, 36.34it/s, est. speed input: 39213.76 toks/s, output: 38.29 toks/s]
Processed prompts:  81%|████████  | 826/1024 [00:21<00:05, 36.34it/s, est. speed input: 39193.21 toks/s, output: 38.27 toks/s]
Processed prompts:  81%|████████▏ | 834/1024 [00:21<00:05, 36.31it/s, est. speed input: 39172.34 toks/s, output: 38.25 toks/s]
Processed prompts:  82%|████████▏ | 842/1024 [00:22<00:05, 36.31it/s, est. speed input: 39152.36 toks/s, output: 38.23 toks/s]
Processed prompts:  83%|████████▎ | 850/1024 [00:22<00:04, 36.32it/s, est. speed input: 39133.20 toks/s, output: 38.22 toks/s]
Processed prompts:  84%|████████▍ | 858/1024 [00:22<00:04, 36.32it/s, est. speed input: 39114.19 toks/s, output: 38.20 toks/s]
Processed prompts:  85%|████████▍ | 866/1024 [00:22<00:04, 36.32it/s, est. speed input: 39095.40 toks/s, output: 38.18 toks/s]
Processed prompts:  85%|████████▌ | 874/1024 [00:22<00:04, 36.32it/s, est. speed input: 39077.09 toks/s, output: 38.16 toks/s]
Processed prompts:  86%|████████▌ | 882/1024 [00:23<00:03, 36.33it/s, est. speed input: 39059.64 toks/s, output: 38.14 toks/s]
Processed prompts:  87%|████████▋ | 890/1024 [00:23<00:03, 36.35it/s, est. speed input: 39042.64 toks/s, output: 38.13 toks/s]
Processed prompts:  88%|████████▊ | 898/1024 [00:23<00:03, 36.34it/s, est. speed input: 39025.44 toks/s, output: 38.11 toks/s]
Processed prompts:  88%|████████▊ | 906/1024 [00:23<00:03, 36.33it/s, est. speed input: 39008.16 toks/s, output: 38.09 toks/s]
Processed prompts:  89%|████████▉ | 914/1024 [00:24<00:03, 36.32it/s, est. speed input: 38991.19 toks/s, output: 38.08 toks/s]
Processed prompts:  90%|█████████ | 922/1024 [00:24<00:02, 36.32it/s, est. speed input: 38975.01 toks/s, output: 38.06 toks/s]
Processed prompts:  91%|█████████ | 930/1024 [00:24<00:02, 36.31it/s, est. speed input: 38958.40 toks/s, output: 38.05 toks/s]
Processed prompts:  92%|█████████▏| 938/1024 [00:24<00:02, 36.29it/s, est. speed input: 38941.90 toks/s, output: 38.03 toks/s]
Processed prompts:  92%|█████████▏| 946/1024 [00:24<00:02, 36.29it/s, est. speed input: 38926.34 toks/s, output: 38.01 toks/s]
Processed prompts:  93%|█████████▎| 954/1024 [00:25<00:01, 36.31it/s, est. speed input: 38911.51 toks/s, output: 38.00 toks/s]
Processed prompts:  94%|█████████▍| 962/1024 [00:25<00:01, 36.32it/s, est. speed input: 38896.64 toks/s, output: 37.98 toks/s]
Processed prompts:  95%|█████████▍| 970/1024 [00:25<00:01, 36.34it/s, est. speed input: 38882.43 toks/s, output: 37.97 toks/s]
Processed prompts:  96%|█████████▌| 978/1024 [00:25<00:01, 36.33it/s, est. speed input: 38867.86 toks/s, output: 37.96 toks/s]
Processed prompts:  96%|█████████▋| 986/1024 [00:25<00:01, 36.32it/s, est. speed input: 38853.44 toks/s, output: 37.94 toks/s]
Processed prompts:  97%|█████████▋| 994/1024 [00:26<00:00, 36.33it/s, est. speed input: 38839.69 toks/s, output: 37.93 toks/s]
Processed prompts:  98%|█████████▊| 1002/1024 [00:26<00:00, 36.31it/s, est. speed input: 38825.68 toks/s, output: 37.92 toks/s]
Processed prompts:  99%|█████████▊| 1010/1024 [00:26<00:00, 36.32it/s, est. speed input: 38812.39 toks/s, output: 37.90 toks/s]
Processed prompts:  99%|█████████▉| 1018/1024 [00:26<00:00, 37.64it/s, est. speed input: 38836.27 toks/s, output: 37.93 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:26<00:00, 37.64it/s, est. speed input: 39064.84 toks/s, output: 38.15 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:26<00:00, 38.15it/s, est. speed input: 39064.84 toks/s, output: 38.15 toks/s]
[rank0]:[W125 19:49:45.879743707 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 52.8s

测试结果:
  Requests/s:   36.31
  Tokens/s:     37220.32
  Total Reqs:   1024
  Elapsed:      28.20s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     37184.01

============================================================
[6/8] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-3B-INT8                                │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 19:49:55 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=398979) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=398979) WARNING 01-25 19:50:05 [backends.py:609] Failed to read file <frozen os>
Throughput: 35.97 requests/s, 36868.91 total tokens/s, 35.97 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-25 19:49:55] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:49:55] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:49:55] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:49:55] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:49:55] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:49:55] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:49:55] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:49:55] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:49:55] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:49:55] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:49:55] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:49:55] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:49:55] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:49:55] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 19:49:59] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:49:59] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:49:59] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:49:59] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:49:59] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:49:59] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:49:59] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:49:59] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:49:59] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:49:59] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:49:59] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:49:59] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:49:59] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:49:59] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=398979) [2026-01-25 19:50:00] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=398979) [2026-01-25 19:50:00] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=398979) [2026-01-25 19:50:00] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=398979) [2026-01-25 19:50:00] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=398979) [2026-01-25 19:50:00] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=398979) [2026-01-25 19:50:00] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=398979) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=398979) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.99it/s]
(EngineCore_DP0 pid=398979) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.99it/s]
(EngineCore_DP0 pid=398979) 
(EngineCore_DP0 pid=398979) [2026-01-25 19:50:01] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 4928] -> 1D uint8
(EngineCore_DP0 pid=398979) [2026-01-25 19:50:01] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19005440 bytes
(EngineCore_DP0 pid=398979) [2026-01-25 19:50:01] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 4928] -> 1D uint8
(EngineCore_DP0 pid=398979) [2026-01-25 19:50:01] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11403264 bytes
(EngineCore_DP0 pid=398979) [2026-01-25 19:50:01] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 4928] -> 1D uint8
(EngineCore_DP0 pid=398979) [2026-01-25 19:50:01] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 60817408 bytes
(EngineCore_DP0 pid=398979) [2026-01-25 19:50:01] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 13120] -> 1D uint8
(EngineCore_DP0 pid=398979) [2026-01-25 19:50:01] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 30277632 bytes
(EngineCore_DP0 pid=398979) 2026-01-25 19:50:11,029 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=398979) 2026-01-25 19:50:11,044 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=398979) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 1/7 [00:00<00:01,  4.68it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00, 13.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 18.81it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 15.83it/s]
(EngineCore_DP0 pid=398979) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 1/5 [00:00<00:00,  7.55it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 21.90it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 19.65it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   4%|▎         | 73/2048 [00:00<00:02, 729.11it/s]
Adding requests:   7%|▋         | 148/2048 [00:00<00:02, 739.87it/s]
Adding requests:  11%|█         | 225/2048 [00:00<00:02, 750.80it/s]
Adding requests:  15%|█▍        | 302/2048 [00:00<00:02, 757.42it/s]
Adding requests:  19%|█▊        | 380/2048 [00:00<00:02, 764.50it/s]
Adding requests:  22%|██▏       | 458/2048 [00:00<00:02, 765.86it/s]
Adding requests:  26%|██▌       | 535/2048 [00:00<00:02, 756.34it/s]
Adding requests:  30%|██▉       | 614/2048 [00:00<00:01, 763.91it/s]
Adding requests:  34%|███▍      | 693/2048 [00:00<00:01, 771.21it/s]
Adding requests:  38%|███▊      | 771/2048 [00:01<00:01, 756.32it/s]
Adding requests:  41%|████▏     | 847/2048 [00:01<00:01, 740.84it/s]
Adding requests:  45%|████▌     | 927/2048 [00:01<00:01, 755.81it/s]
Adding requests:  49%|████▉     | 1006/2048 [00:01<00:01, 763.07it/s]
Adding requests:  53%|█████▎    | 1083/2048 [00:01<00:01, 759.02it/s]
Adding requests:  57%|█████▋    | 1160/2048 [00:01<00:01, 762.26it/s]
Adding requests:  61%|██████    | 1240/2048 [00:01<00:01, 772.83it/s]
Adding requests:  64%|██████▍   | 1318/2048 [00:01<00:00, 769.95it/s]
Adding requests:  68%|██████▊   | 1398/2048 [00:01<00:00, 778.68it/s]
Adding requests:  72%|███████▏  | 1476/2048 [00:01<00:00, 773.55it/s]
Adding requests:  76%|███████▌  | 1555/2048 [00:02<00:00, 775.77it/s]
Adding requests:  80%|███████▉  | 1636/2048 [00:02<00:00, 785.53it/s]
Adding requests:  84%|████████▎ | 1715/2048 [00:02<00:00, 774.58it/s]
Adding requests:  88%|████████▊ | 1793/2048 [00:02<00:00, 768.66it/s]
Adding requests:  91%|█████████▏| 1870/2048 [00:02<00:00, 766.29it/s]
Adding requests:  95%|█████████▌| 1947/2048 [00:02<00:00, 754.55it/s]
Adding requests:  99%|█████████▉| 2025/2048 [00:02<00:00, 760.50it/s]
Adding requests: 100%|██████████| 2048/2048 [00:02<00:00, 763.35it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   5%|▍         | 98/2048 [00:00<00:09, 198.39it/s, est. speed input: 203157.01 toks/s, output: 198.39 toks/s]
Processed prompts:   6%|▌         | 118/2048 [00:00<00:17, 112.45it/s, est. speed input: 129079.83 toks/s, output: 126.05 toks/s]
Processed prompts:   6%|▋         | 130/2048 [00:01<00:25, 74.47it/s, est. speed input: 96566.58 toks/s, output: 94.30 toks/s]   
Processed prompts:   7%|▋         | 146/2048 [00:01<00:31, 59.59it/s, est. speed input: 82112.59 toks/s, output: 80.19 toks/s]
Processed prompts:   8%|▊         | 162/2048 [00:02<00:36, 51.24it/s, est. speed input: 73307.81 toks/s, output: 71.59 toks/s]
Processed prompts:   9%|▊         | 178/2048 [00:02<00:40, 46.13it/s, est. speed input: 67366.87 toks/s, output: 65.79 toks/s]
Processed prompts:   9%|▉         | 194/2048 [00:03<00:43, 42.89it/s, est. speed input: 63102.44 toks/s, output: 61.62 toks/s]
Processed prompts:  10%|█         | 210/2048 [00:03<00:45, 40.74it/s, est. speed input: 59883.74 toks/s, output: 58.48 toks/s]
Processed prompts:  11%|█         | 226/2048 [00:04<00:46, 39.32it/s, est. speed input: 57378.41 toks/s, output: 56.03 toks/s]
Processed prompts:  12%|█▏        | 242/2048 [00:04<00:47, 38.34it/s, est. speed input: 55364.34 toks/s, output: 54.07 toks/s]
Processed prompts:  13%|█▎        | 258/2048 [00:04<00:47, 37.66it/s, est. speed input: 53708.44 toks/s, output: 52.45 toks/s]
Processed prompts:  13%|█▎        | 274/2048 [00:05<00:47, 37.19it/s, est. speed input: 52328.11 toks/s, output: 51.10 toks/s]
Processed prompts:  14%|█▍        | 290/2048 [00:05<00:47, 36.80it/s, est. speed input: 51132.71 toks/s, output: 49.93 toks/s]
Processed prompts:  15%|█▍        | 306/2048 [00:06<00:47, 36.59it/s, est. speed input: 50128.43 toks/s, output: 48.95 toks/s]
Processed prompts:  16%|█▌        | 322/2048 [00:06<00:47, 36.44it/s, est. speed input: 49255.47 toks/s, output: 48.10 toks/s]
Processed prompts:  17%|█▋        | 338/2048 [00:07<00:47, 36.33it/s, est. speed input: 48490.89 toks/s, output: 47.35 toks/s]
Processed prompts:  17%|█▋        | 354/2048 [00:07<00:46, 36.26it/s, est. speed input: 47816.19 toks/s, output: 46.70 toks/s]
Processed prompts:  18%|█▊        | 370/2048 [00:08<00:46, 36.21it/s, est. speed input: 47217.88 toks/s, output: 46.11 toks/s]
Processed prompts:  19%|█▉        | 386/2048 [00:08<00:45, 36.18it/s, est. speed input: 46680.54 toks/s, output: 45.59 toks/s]
Processed prompts:  20%|█▉        | 402/2048 [00:08<00:45, 36.15it/s, est. speed input: 46197.62 toks/s, output: 45.11 toks/s]
Processed prompts:  20%|██        | 418/2048 [00:09<00:45, 36.14it/s, est. speed input: 45761.37 toks/s, output: 44.69 toks/s]
Processed prompts:  21%|██        | 434/2048 [00:09<00:44, 36.12it/s, est. speed input: 45361.77 toks/s, output: 44.30 toks/s]
Processed prompts:  22%|██▏       | 450/2048 [00:10<00:44, 36.13it/s, est. speed input: 45000.73 toks/s, output: 43.95 toks/s]
Processed prompts:  23%|██▎       | 466/2048 [00:10<00:43, 36.11it/s, est. speed input: 44665.96 toks/s, output: 43.62 toks/s]
Processed prompts:  24%|██▎       | 482/2048 [00:11<00:43, 36.10it/s, est. speed input: 44358.63 toks/s, output: 43.32 toks/s]
Processed prompts:  24%|██▍       | 498/2048 [00:11<00:42, 36.08it/s, est. speed input: 44072.70 toks/s, output: 43.04 toks/s]
Processed prompts:  25%|██▌       | 514/2048 [00:12<00:42, 36.09it/s, est. speed input: 43810.78 toks/s, output: 42.78 toks/s]
Processed prompts:  26%|██▌       | 530/2048 [00:12<00:42, 36.08it/s, est. speed input: 43565.49 toks/s, output: 42.54 toks/s]
Processed prompts:  27%|██▋       | 546/2048 [00:12<00:41, 36.08it/s, est. speed input: 43337.91 toks/s, output: 42.32 toks/s]
Processed prompts:  27%|██▋       | 562/2048 [00:13<00:41, 36.08it/s, est. speed input: 43125.90 toks/s, output: 42.12 toks/s]
Processed prompts:  28%|██▊       | 578/2048 [00:13<00:40, 36.08it/s, est. speed input: 42926.61 toks/s, output: 41.92 toks/s]
Processed prompts:  29%|██▉       | 594/2048 [00:14<00:40, 36.08it/s, est. speed input: 42740.36 toks/s, output: 41.74 toks/s]
Processed prompts:  30%|██▉       | 610/2048 [00:14<00:39, 36.07it/s, est. speed input: 42563.76 toks/s, output: 41.57 toks/s]
Processed prompts:  31%|███       | 626/2048 [00:15<00:39, 36.06it/s, est. speed input: 42398.15 toks/s, output: 41.40 toks/s]
Processed prompts:  31%|███▏      | 642/2048 [00:15<00:38, 36.06it/s, est. speed input: 42241.63 toks/s, output: 41.25 toks/s]
Processed prompts:  32%|███▏      | 658/2048 [00:16<00:38, 36.06it/s, est. speed input: 42093.86 toks/s, output: 41.11 toks/s]
Processed prompts:  33%|███▎      | 674/2048 [00:16<00:38, 36.07it/s, est. speed input: 41955.49 toks/s, output: 40.97 toks/s]
Processed prompts:  34%|███▎      | 690/2048 [00:16<00:37, 36.07it/s, est. speed input: 41823.86 toks/s, output: 40.84 toks/s]
Processed prompts:  34%|███▍      | 706/2048 [00:17<00:37, 36.07it/s, est. speed input: 41698.54 toks/s, output: 40.72 toks/s]
Processed prompts:  35%|███▌      | 722/2048 [00:17<00:36, 36.06it/s, est. speed input: 41578.63 toks/s, output: 40.60 toks/s]
Processed prompts:  36%|███▌      | 738/2048 [00:18<00:36, 36.06it/s, est. speed input: 41465.27 toks/s, output: 40.49 toks/s]
Processed prompts:  37%|███▋      | 754/2048 [00:18<00:35, 36.05it/s, est. speed input: 41356.31 toks/s, output: 40.39 toks/s]
Processed prompts:  38%|███▊      | 770/2048 [00:19<00:35, 36.05it/s, est. speed input: 41253.36 toks/s, output: 40.29 toks/s]
Processed prompts:  38%|███▊      | 786/2048 [00:19<00:35, 36.04it/s, est. speed input: 41154.36 toks/s, output: 40.19 toks/s]
Processed prompts:  39%|███▉      | 802/2048 [00:20<00:34, 36.04it/s, est. speed input: 41060.19 toks/s, output: 40.10 toks/s]
Processed prompts:  40%|███▉      | 818/2048 [00:20<00:34, 36.04it/s, est. speed input: 40969.74 toks/s, output: 40.01 toks/s]
Processed prompts:  41%|████      | 834/2048 [00:20<00:33, 36.04it/s, est. speed input: 40883.13 toks/s, output: 39.92 toks/s]
Processed prompts:  42%|████▏     | 850/2048 [00:21<00:33, 36.04it/s, est. speed input: 40800.40 toks/s, output: 39.84 toks/s]
Processed prompts:  42%|████▏     | 866/2048 [00:21<00:32, 36.03it/s, est. speed input: 40720.51 toks/s, output: 39.77 toks/s]
Processed prompts:  43%|████▎     | 882/2048 [00:22<00:32, 36.04it/s, est. speed input: 40644.50 toks/s, output: 39.69 toks/s]
Processed prompts:  44%|████▍     | 898/2048 [00:22<00:31, 36.04it/s, est. speed input: 40571.08 toks/s, output: 39.62 toks/s]
Processed prompts:  45%|████▍     | 914/2048 [00:23<00:31, 36.04it/s, est. speed input: 40500.99 toks/s, output: 39.55 toks/s]
Processed prompts:  45%|████▌     | 930/2048 [00:23<00:31, 36.03it/s, est. speed input: 40432.68 toks/s, output: 39.49 toks/s]
Processed prompts:  46%|████▌     | 946/2048 [00:23<00:30, 36.04it/s, est. speed input: 40367.69 toks/s, output: 39.42 toks/s]
Processed prompts:  47%|████▋     | 962/2048 [00:24<00:30, 36.03it/s, est. speed input: 40303.93 toks/s, output: 39.36 toks/s]
Processed prompts:  48%|████▊     | 978/2048 [00:24<00:29, 36.03it/s, est. speed input: 40243.22 toks/s, output: 39.30 toks/s]
Processed prompts:  49%|████▊     | 994/2048 [00:25<00:29, 36.03it/s, est. speed input: 40184.46 toks/s, output: 39.24 toks/s]
Processed prompts:  49%|████▉     | 1010/2048 [00:25<00:28, 36.02it/s, est. speed input: 40127.31 toks/s, output: 39.19 toks/s]
Processed prompts:  50%|█████     | 1026/2048 [00:26<00:28, 36.03it/s, est. speed input: 40072.83 toks/s, output: 39.13 toks/s]
Processed prompts:  51%|█████     | 1042/2048 [00:26<00:27, 36.03it/s, est. speed input: 40019.77 toks/s, output: 39.08 toks/s]
Processed prompts:  52%|█████▏    | 1058/2048 [00:27<00:27, 36.02it/s, est. speed input: 39968.28 toks/s, output: 39.03 toks/s]
Processed prompts:  52%|█████▏    | 1074/2048 [00:27<00:27, 36.02it/s, est. speed input: 39918.35 toks/s, output: 38.98 toks/s]
Processed prompts:  53%|█████▎    | 1090/2048 [00:27<00:26, 36.02it/s, est. speed input: 39870.25 toks/s, output: 38.94 toks/s]
Processed prompts:  54%|█████▍    | 1106/2048 [00:28<00:26, 36.02it/s, est. speed input: 39823.63 toks/s, output: 38.89 toks/s]
Processed prompts:  55%|█████▍    | 1122/2048 [00:28<00:25, 36.02it/s, est. speed input: 39778.49 toks/s, output: 38.85 toks/s]
Processed prompts:  56%|█████▌    | 1138/2048 [00:29<00:25, 36.02it/s, est. speed input: 39734.55 toks/s, output: 38.80 toks/s]
Processed prompts:  56%|█████▋    | 1154/2048 [00:29<00:24, 36.00it/s, est. speed input: 39691.06 toks/s, output: 38.76 toks/s]
Processed prompts:  57%|█████▋    | 1170/2048 [00:30<00:24, 36.01it/s, est. speed input: 39649.95 toks/s, output: 38.72 toks/s]
Processed prompts:  58%|█████▊    | 1186/2048 [00:30<00:23, 36.01it/s, est. speed input: 39609.54 toks/s, output: 38.68 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [00:31<00:23, 36.01it/s, est. speed input: 39570.59 toks/s, output: 38.64 toks/s]
Processed prompts:  59%|█████▉    | 1218/2048 [00:31<00:23, 36.00it/s, est. speed input: 39532.28 toks/s, output: 38.61 toks/s]
Processed prompts:  60%|██████    | 1234/2048 [00:31<00:22, 36.01it/s, est. speed input: 39495.54 toks/s, output: 38.57 toks/s]
Processed prompts:  61%|██████    | 1250/2048 [00:32<00:22, 36.00it/s, est. speed input: 39459.15 toks/s, output: 38.53 toks/s]
Processed prompts:  62%|██████▏   | 1266/2048 [00:32<00:21, 36.00it/s, est. speed input: 39424.25 toks/s, output: 38.50 toks/s]
Processed prompts:  63%|██████▎   | 1282/2048 [00:33<00:21, 36.00it/s, est. speed input: 39389.97 toks/s, output: 38.47 toks/s]
Processed prompts:  63%|██████▎   | 1298/2048 [00:33<00:20, 36.00it/s, est. speed input: 39356.71 toks/s, output: 38.43 toks/s]
Processed prompts:  64%|██████▍   | 1314/2048 [00:34<00:20, 35.99it/s, est. speed input: 39324.09 toks/s, output: 38.40 toks/s]
Processed prompts:  65%|██████▍   | 1330/2048 [00:34<00:19, 35.99it/s, est. speed input: 39292.48 toks/s, output: 38.37 toks/s]
Processed prompts:  66%|██████▌   | 1346/2048 [00:35<00:19, 36.00it/s, est. speed input: 39261.89 toks/s, output: 38.34 toks/s]
Processed prompts:  67%|██████▋   | 1362/2048 [00:35<00:19, 35.99it/s, est. speed input: 39231.51 toks/s, output: 38.31 toks/s]
Processed prompts:  67%|██████▋   | 1378/2048 [00:35<00:18, 35.99it/s, est. speed input: 39202.23 toks/s, output: 38.28 toks/s]
Processed prompts:  68%|██████▊   | 1394/2048 [00:36<00:18, 35.98it/s, est. speed input: 39173.05 toks/s, output: 38.25 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [00:36<00:17, 35.99it/s, est. speed input: 39145.32 toks/s, output: 38.23 toks/s]
Processed prompts:  70%|██████▉   | 1426/2048 [00:37<00:17, 35.99it/s, est. speed input: 39118.13 toks/s, output: 38.20 toks/s]
Processed prompts:  70%|███████   | 1442/2048 [00:37<00:16, 35.98it/s, est. speed input: 39090.93 toks/s, output: 38.17 toks/s]
Processed prompts:  71%|███████   | 1458/2048 [00:38<00:16, 35.98it/s, est. speed input: 39064.94 toks/s, output: 38.15 toks/s]
Processed prompts:  72%|███████▏  | 1474/2048 [00:38<00:15, 35.98it/s, est. speed input: 39039.23 toks/s, output: 38.12 toks/s]
Processed prompts:  73%|███████▎  | 1490/2048 [00:39<00:15, 35.98it/s, est. speed input: 39014.39 toks/s, output: 38.10 toks/s]
Processed prompts:  74%|███████▎  | 1506/2048 [00:39<00:15, 35.97it/s, est. speed input: 38989.59 toks/s, output: 38.08 toks/s]
Processed prompts:  74%|███████▍  | 1522/2048 [00:39<00:14, 35.97it/s, est. speed input: 38965.52 toks/s, output: 38.05 toks/s]
Processed prompts:  75%|███████▌  | 1538/2048 [00:40<00:14, 35.97it/s, est. speed input: 38941.92 toks/s, output: 38.03 toks/s]
Processed prompts:  76%|███████▌  | 1554/2048 [00:40<00:13, 35.97it/s, est. speed input: 38919.26 toks/s, output: 38.01 toks/s]
Processed prompts:  77%|███████▋  | 1570/2048 [00:41<00:13, 35.98it/s, est. speed input: 38897.03 toks/s, output: 37.99 toks/s]
Processed prompts:  77%|███████▋  | 1586/2048 [00:41<00:12, 35.97it/s, est. speed input: 38874.99 toks/s, output: 37.96 toks/s]
Processed prompts:  78%|███████▊  | 1602/2048 [00:42<00:12, 35.98it/s, est. speed input: 38853.67 toks/s, output: 37.94 toks/s]
Processed prompts:  79%|███████▉  | 1618/2048 [00:42<00:11, 35.97it/s, est. speed input: 38832.55 toks/s, output: 37.92 toks/s]
Processed prompts:  80%|███████▉  | 1634/2048 [00:43<00:11, 35.97it/s, est. speed input: 38811.98 toks/s, output: 37.90 toks/s]
Processed prompts:  81%|████████  | 1650/2048 [00:43<00:11, 35.97it/s, est. speed input: 38791.56 toks/s, output: 37.88 toks/s]
Processed prompts:  81%|████████▏ | 1666/2048 [00:44<00:10, 35.97it/s, est. speed input: 38771.82 toks/s, output: 37.86 toks/s]
Processed prompts:  82%|████████▏ | 1682/2048 [00:44<00:10, 35.97it/s, est. speed input: 38752.31 toks/s, output: 37.84 toks/s]
Processed prompts:  83%|████████▎ | 1698/2048 [00:44<00:09, 35.96it/s, est. speed input: 38733.14 toks/s, output: 37.83 toks/s]
Processed prompts:  84%|████████▎ | 1714/2048 [00:45<00:09, 35.97it/s, est. speed input: 38714.51 toks/s, output: 37.81 toks/s]
Processed prompts:  84%|████████▍ | 1730/2048 [00:45<00:08, 35.96it/s, est. speed input: 38695.95 toks/s, output: 37.79 toks/s]
Processed prompts:  85%|████████▌ | 1746/2048 [00:46<00:08, 35.97it/s, est. speed input: 38678.16 toks/s, output: 37.77 toks/s]
Processed prompts:  86%|████████▌ | 1762/2048 [00:46<00:07, 35.96it/s, est. speed input: 38660.20 toks/s, output: 37.75 toks/s]
Processed prompts:  87%|████████▋ | 1778/2048 [00:47<00:07, 35.96it/s, est. speed input: 38642.80 toks/s, output: 37.74 toks/s]
Processed prompts:  88%|████████▊ | 1794/2048 [00:47<00:07, 35.95it/s, est. speed input: 38625.62 toks/s, output: 37.72 toks/s]
Processed prompts:  88%|████████▊ | 1810/2048 [00:48<00:06, 35.96it/s, est. speed input: 38608.98 toks/s, output: 37.70 toks/s]
Processed prompts:  89%|████████▉ | 1826/2048 [00:48<00:06, 35.95it/s, est. speed input: 38592.36 toks/s, output: 37.69 toks/s]
Processed prompts:  90%|████████▉ | 1842/2048 [00:48<00:05, 35.96it/s, est. speed input: 38576.35 toks/s, output: 37.67 toks/s]
Processed prompts:  91%|█████████ | 1858/2048 [00:49<00:05, 35.96it/s, est. speed input: 38560.49 toks/s, output: 37.66 toks/s]
Processed prompts:  92%|█████████▏| 1874/2048 [00:49<00:04, 36.55it/s, est. speed input: 38563.52 toks/s, output: 37.66 toks/s]
Processed prompts:  92%|█████████▏| 1890/2048 [00:50<00:04, 36.37it/s, est. speed input: 38547.97 toks/s, output: 37.64 toks/s]
Processed prompts:  93%|█████████▎| 1906/2048 [00:50<00:03, 36.24it/s, est. speed input: 38532.62 toks/s, output: 37.63 toks/s]
Processed prompts:  94%|█████████▍| 1922/2048 [00:51<00:03, 36.15it/s, est. speed input: 38517.78 toks/s, output: 37.62 toks/s]
Processed prompts:  95%|█████████▍| 1938/2048 [00:51<00:03, 36.09it/s, est. speed input: 38503.08 toks/s, output: 37.60 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [00:51<00:02, 36.05it/s, est. speed input: 38488.68 toks/s, output: 37.59 toks/s]
Processed prompts:  96%|█████████▌| 1970/2048 [00:52<00:02, 36.02it/s, est. speed input: 38474.37 toks/s, output: 37.57 toks/s]
Processed prompts:  97%|█████████▋| 1986/2048 [00:52<00:01, 36.00it/s, est. speed input: 38460.43 toks/s, output: 37.56 toks/s]
Processed prompts:  98%|█████████▊| 2002/2048 [00:53<00:01, 35.98it/s, est. speed input: 38446.63 toks/s, output: 37.55 toks/s]
Processed prompts:  99%|█████████▊| 2018/2048 [00:53<00:00, 35.97it/s, est. speed input: 38433.09 toks/s, output: 37.53 toks/s]
Processed prompts:  99%|█████████▉| 2034/2048 [00:54<00:00, 35.01it/s, est. speed input: 38390.97 toks/s, output: 37.49 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:54<00:00, 35.01it/s, est. speed input: 38655.08 toks/s, output: 37.75 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:54<00:00, 37.75it/s, est. speed input: 38655.08 toks/s, output: 37.75 toks/s]
[rank0]:[W125 19:51:09.606747983 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 84.6s

测试结果:
  Requests/s:   35.97
  Tokens/s:     36868.91
  Total Reqs:   2048
  Elapsed:      56.94s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     36832.94

============================================================
[7/8] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-3B-INT8                                │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 19:51:26 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=400502) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=400502) WARNING 01-25 19:51:35 [backends.py:609] Failed to read file <frozen os>
Throughput: 36.06 requests/s, 36963.29 total tokens/s, 36.06 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-25 19:51:26] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:51:26] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:51:26] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:51:26] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:51:26] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:51:26] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:51:26] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:51:26] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:51:26] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:51:26] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:51:26] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:51:26] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:51:26] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:51:26] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 19:51:29] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:51:30] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:51:30] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:51:30] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:51:30] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:51:30] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:51:30] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:51:30] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:51:30] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:51:30] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:51:30] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:51:30] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:51:30] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:51:30] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=400502) [2026-01-25 19:51:30] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=400502) [2026-01-25 19:51:30] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=400502) [2026-01-25 19:51:30] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=400502) [2026-01-25 19:51:30] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=400502) [2026-01-25 19:51:30] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=400502) [2026-01-25 19:51:30] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=400502) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=400502) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.00it/s]
(EngineCore_DP0 pid=400502) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.00it/s]
(EngineCore_DP0 pid=400502) 
(EngineCore_DP0 pid=400502) [2026-01-25 19:51:31] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 4928] -> 1D uint8
(EngineCore_DP0 pid=400502) [2026-01-25 19:51:31] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19005440 bytes
(EngineCore_DP0 pid=400502) [2026-01-25 19:51:31] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 4928] -> 1D uint8
(EngineCore_DP0 pid=400502) [2026-01-25 19:51:31] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11403264 bytes
(EngineCore_DP0 pid=400502) [2026-01-25 19:51:31] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 4928] -> 1D uint8
(EngineCore_DP0 pid=400502) [2026-01-25 19:51:31] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 60817408 bytes
(EngineCore_DP0 pid=400502) [2026-01-25 19:51:31] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 13120] -> 1D uint8
(EngineCore_DP0 pid=400502) [2026-01-25 19:51:31] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 30277632 bytes
(EngineCore_DP0 pid=400502) [rank0]:W0125 19:51:38.358000 400502 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=400502) [rank0]:W0125 19:51:38.410000 400502 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=400502) [rank0]:W0125 19:51:39.030000 400502 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=400502) [rank0]:W0125 19:51:39.101000 400502 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=400502) 2026-01-25 19:51:41,484 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=400502) 2026-01-25 19:51:41,500 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=400502) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▉         | 1/11 [00:00<00:07,  1.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 3/11 [00:00<00:01,  4.01it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 6/11 [00:00<00:00,  8.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 9/11 [00:01<00:00, 12.55it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:01<00:00,  9.37it/s]
(EngineCore_DP0 pid=400502) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 1/7 [00:00<00:00,  7.55it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 5/7 [00:00<00:00, 21.72it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 22.08it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 69/4096 [00:00<00:05, 684.02it/s]
Adding requests:   4%|▎         | 145/4096 [00:00<00:05, 727.11it/s]
Adding requests:   5%|▌         | 222/4096 [00:00<00:05, 743.28it/s]
Adding requests:   7%|▋         | 300/4096 [00:00<00:05, 751.77it/s]
Adding requests:   9%|▉         | 378/4096 [00:00<00:04, 758.94it/s]
Adding requests:  11%|█         | 455/4096 [00:00<00:04, 761.37it/s]
Adding requests:  13%|█▎        | 532/4096 [00:00<00:04, 748.96it/s]
Adding requests:  15%|█▍        | 610/4096 [00:00<00:04, 757.65it/s]
Adding requests:  17%|█▋        | 690/4096 [00:00<00:04, 769.01it/s]
Adding requests:  19%|█▊        | 767/4096 [00:01<00:04, 756.07it/s]
Adding requests:  21%|██        | 843/4096 [00:01<00:04, 740.63it/s]
Adding requests:  23%|██▎       | 922/4096 [00:01<00:04, 753.76it/s]
Adding requests:  24%|██▍       | 998/4096 [00:01<00:04, 753.70it/s]
Adding requests:  26%|██▌       | 1074/4096 [00:01<00:04, 754.82it/s]
Adding requests:  28%|██▊       | 1150/4096 [00:01<00:04, 733.95it/s]
Adding requests:  30%|██▉       | 1228/4096 [00:01<00:03, 746.08it/s]
Adding requests:  32%|███▏      | 1303/4096 [00:01<00:03, 741.73it/s]
Adding requests:  34%|███▎      | 1380/4096 [00:01<00:03, 749.34it/s]
Adding requests:  36%|███▌      | 1457/4096 [00:01<00:03, 752.76it/s]
Adding requests:  37%|███▋      | 1535/4096 [00:02<00:03, 760.13it/s]
Adding requests:  39%|███▉      | 1613/4096 [00:02<00:03, 764.74it/s]
Adding requests:  41%|████▏     | 1690/4096 [00:02<00:03, 755.73it/s]
Adding requests:  43%|████▎     | 1768/4096 [00:02<00:03, 761.17it/s]
Adding requests:  45%|████▌     | 1847/4096 [00:02<00:02, 768.88it/s]
Adding requests:  47%|████▋     | 1925/4096 [00:02<00:02, 769.04it/s]
Adding requests:  49%|████▉     | 2004/4096 [00:02<00:02, 773.23it/s]
Adding requests:  51%|█████     | 2083/4096 [00:02<00:02, 776.32it/s]
Adding requests:  53%|█████▎    | 2161/4096 [00:02<00:02, 763.93it/s]
Adding requests:  55%|█████▍    | 2238/4096 [00:02<00:02, 765.58it/s]
Adding requests:  57%|█████▋    | 2316/4096 [00:03<00:02, 756.86it/s]
Adding requests:  58%|█████▊    | 2392/4096 [00:03<00:02, 752.12it/s]
Adding requests:  60%|██████    | 2470/4096 [00:03<00:02, 759.75it/s]
Adding requests:  62%|██████▏   | 2547/4096 [00:03<00:02, 760.92it/s]
Adding requests:  64%|██████▍   | 2625/4096 [00:03<00:01, 766.54it/s]
Adding requests:  66%|██████▌   | 2702/4096 [00:03<00:01, 764.93it/s]
Adding requests:  68%|██████▊   | 2779/4096 [00:03<00:01, 761.66it/s]
Adding requests:  70%|██████▉   | 2856/4096 [00:03<00:01, 755.87it/s]
Adding requests:  72%|███████▏  | 2932/4096 [00:03<00:01, 753.83it/s]
Adding requests:  73%|███████▎  | 3008/4096 [00:03<00:01, 752.29it/s]
Adding requests:  75%|███████▌  | 3084/4096 [00:04<00:01, 749.07it/s]
Adding requests:  77%|███████▋  | 3162/4096 [00:04<00:01, 755.69it/s]
Adding requests:  79%|███████▉  | 3238/4096 [00:04<00:01, 733.79it/s]
Adding requests:  81%|████████  | 3315/4096 [00:04<00:01, 742.15it/s]
Adding requests:  83%|████████▎ | 3392/4096 [00:04<00:00, 748.03it/s]
Adding requests:  85%|████████▍ | 3467/4096 [00:04<00:00, 743.54it/s]
Adding requests:  87%|████████▋ | 3544/4096 [00:04<00:00, 750.39it/s]
Adding requests:  88%|████████▊ | 3621/4096 [00:04<00:00, 754.10it/s]
Adding requests:  90%|█████████ | 3697/4096 [00:04<00:00, 749.25it/s]
Adding requests:  92%|█████████▏| 3777/4096 [00:05<00:00, 762.17it/s]
Adding requests:  94%|█████████▍| 3856/4096 [00:05<00:00, 769.81it/s]
Adding requests:  96%|█████████▌| 3934/4096 [00:05<00:00, 769.96it/s]
Adding requests:  98%|█████████▊| 4013/4096 [00:05<00:00, 775.60it/s]
Adding requests: 100%|█████████▉| 4091/4096 [00:05<00:00, 770.26it/s]
Adding requests: 100%|██████████| 4096/4096 [00:05<00:00, 756.68it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   5%|▍         | 194/4096 [00:00<00:17, 225.64it/s, est. speed input: 231063.41 toks/s, output: 225.65 toks/s]
Processed prompts:   6%|▌         | 226/4096 [00:01<00:34, 112.98it/s, est. speed input: 132763.08 toks/s, output: 129.65 toks/s]
Processed prompts:   6%|▋         | 258/4096 [00:02<00:49, 77.70it/s, est. speed input: 100567.12 toks/s, output: 98.21 toks/s]  
Processed prompts:   7%|▋         | 290/4096 [00:03<01:02, 61.25it/s, est. speed input: 84580.71 toks/s, output: 82.60 toks/s] 
Processed prompts:   8%|▊         | 322/4096 [00:04<01:12, 52.22it/s, est. speed input: 75042.50 toks/s, output: 73.28 toks/s]
Processed prompts:   9%|▊         | 354/4096 [00:05<01:20, 46.75it/s, est. speed input: 68674.52 toks/s, output: 67.06 toks/s]
Processed prompts:   9%|▉         | 386/4096 [00:06<01:25, 43.29it/s, est. speed input: 64137.96 toks/s, output: 62.63 toks/s]
Processed prompts:  10%|█         | 418/4096 [00:07<01:29, 41.03it/s, est. speed input: 60739.09 toks/s, output: 59.32 toks/s]
Processed prompts:  11%|█         | 450/4096 [00:07<01:32, 39.51it/s, est. speed input: 58096.87 toks/s, output: 56.74 toks/s]
Processed prompts:  12%|█▏        | 482/4096 [00:08<01:33, 38.48it/s, est. speed input: 55982.79 toks/s, output: 54.67 toks/s]
Processed prompts:  13%|█▎        | 514/4096 [00:09<01:34, 37.77it/s, est. speed input: 54254.25 toks/s, output: 52.98 toks/s]
Processed prompts:  13%|█▎        | 546/4096 [00:10<01:35, 37.28it/s, est. speed input: 52816.04 toks/s, output: 51.58 toks/s]
Processed prompts:  14%|█▍        | 578/4096 [00:11<01:35, 36.94it/s, est. speed input: 51596.15 toks/s, output: 50.39 toks/s]
Processed prompts:  15%|█▍        | 610/4096 [00:12<01:34, 36.71it/s, est. speed input: 50554.03 toks/s, output: 49.37 toks/s]
Processed prompts:  16%|█▌        | 642/4096 [00:13<01:34, 36.54it/s, est. speed input: 49649.91 toks/s, output: 48.49 toks/s]
Processed prompts:  16%|█▋        | 674/4096 [00:14<01:33, 36.43it/s, est. speed input: 48858.96 toks/s, output: 47.71 toks/s]
Processed prompts:  17%|█▋        | 706/4096 [00:15<01:33, 36.35it/s, est. speed input: 48161.79 toks/s, output: 47.03 toks/s]
Processed prompts:  18%|█▊        | 738/4096 [00:15<01:32, 36.28it/s, est. speed input: 47540.41 toks/s, output: 46.43 toks/s]
Processed prompts:  19%|█▉        | 770/4096 [00:16<01:31, 36.23it/s, est. speed input: 46982.55 toks/s, output: 45.88 toks/s]
Processed prompts:  20%|█▉        | 802/4096 [00:17<01:30, 36.21it/s, est. speed input: 46482.84 toks/s, output: 45.39 toks/s]
Processed prompts:  20%|██        | 834/4096 [00:18<01:30, 36.18it/s, est. speed input: 46030.13 toks/s, output: 44.95 toks/s]
Processed prompts:  21%|██        | 866/4096 [00:19<01:29, 36.17it/s, est. speed input: 45618.32 toks/s, output: 44.55 toks/s]
Processed prompts:  22%|██▏       | 898/4096 [00:20<01:28, 36.16it/s, est. speed input: 45242.91 toks/s, output: 44.18 toks/s]
Processed prompts:  23%|██▎       | 930/4096 [00:21<01:27, 36.15it/s, est. speed input: 44898.98 toks/s, output: 43.85 toks/s]
Processed prompts:  23%|██▎       | 962/4096 [00:22<01:26, 36.14it/s, est. speed input: 44582.20 toks/s, output: 43.54 toks/s]
Processed prompts:  24%|██▍       | 994/4096 [00:22<01:25, 36.14it/s, est. speed input: 44289.16 toks/s, output: 43.25 toks/s]
Processed prompts:  25%|██▌       | 1026/4096 [00:23<01:24, 36.13it/s, est. speed input: 44018.43 toks/s, output: 42.99 toks/s]
Processed prompts:  26%|██▌       | 1058/4096 [00:24<01:24, 36.12it/s, est. speed input: 43765.72 toks/s, output: 42.74 toks/s]
Processed prompts:  27%|██▋       | 1090/4096 [00:25<01:23, 36.12it/s, est. speed input: 43531.61 toks/s, output: 42.51 toks/s]
Processed prompts:  27%|██▋       | 1122/4096 [00:26<01:22, 36.12it/s, est. speed input: 43312.89 toks/s, output: 42.30 toks/s]
Processed prompts:  28%|██▊       | 1154/4096 [00:27<01:21, 36.12it/s, est. speed input: 43108.10 toks/s, output: 42.10 toks/s]
Processed prompts:  29%|██▉       | 1186/4096 [00:28<01:20, 36.12it/s, est. speed input: 42916.24 toks/s, output: 41.91 toks/s]
Processed prompts:  30%|██▉       | 1218/4096 [00:29<01:19, 36.09it/s, est. speed input: 42732.48 toks/s, output: 41.73 toks/s]
Processed prompts:  31%|███       | 1250/4096 [00:30<01:18, 36.09it/s, est. speed input: 42562.43 toks/s, output: 41.56 toks/s]
Processed prompts:  31%|███▏      | 1282/4096 [00:30<01:17, 36.09it/s, est. speed input: 42401.90 toks/s, output: 41.41 toks/s]
Processed prompts:  32%|███▏      | 1314/4096 [00:31<01:17, 36.09it/s, est. speed input: 42250.42 toks/s, output: 41.26 toks/s]
Processed prompts:  33%|███▎      | 1346/4096 [00:32<01:16, 36.09it/s, est. speed input: 42106.80 toks/s, output: 41.12 toks/s]
Processed prompts:  34%|███▎      | 1378/4096 [00:33<01:15, 36.09it/s, est. speed input: 41970.87 toks/s, output: 40.99 toks/s]
Processed prompts:  34%|███▍      | 1410/4096 [00:34<01:14, 36.09it/s, est. speed input: 41842.04 toks/s, output: 40.86 toks/s]
Processed prompts:  35%|███▌      | 1442/4096 [00:35<01:13, 36.09it/s, est. speed input: 41719.52 toks/s, output: 40.74 toks/s]
Processed prompts:  36%|███▌      | 1474/4096 [00:36<01:12, 36.09it/s, est. speed input: 41602.97 toks/s, output: 40.63 toks/s]
Processed prompts:  37%|███▋      | 1506/4096 [00:37<01:11, 36.09it/s, est. speed input: 41492.37 toks/s, output: 40.52 toks/s]
Processed prompts:  38%|███▊      | 1538/4096 [00:38<01:10, 36.08it/s, est. speed input: 41385.64 toks/s, output: 40.42 toks/s]
Processed prompts:  38%|███▊      | 1570/4096 [00:38<01:10, 36.08it/s, est. speed input: 41284.24 toks/s, output: 40.32 toks/s]
Processed prompts:  39%|███▉      | 1602/4096 [00:39<01:09, 36.07it/s, est. speed input: 41187.18 toks/s, output: 40.22 toks/s]
Processed prompts:  40%|███▉      | 1634/4096 [00:40<01:08, 36.07it/s, est. speed input: 41094.81 toks/s, output: 40.13 toks/s]
Processed prompts:  41%|████      | 1666/4096 [00:41<01:07, 36.07it/s, est. speed input: 41005.96 toks/s, output: 40.04 toks/s]
Processed prompts:  41%|████▏     | 1698/4096 [00:42<01:06, 36.07it/s, est. speed input: 40921.23 toks/s, output: 39.96 toks/s]
Processed prompts:  42%|████▏     | 1730/4096 [00:43<01:05, 36.07it/s, est. speed input: 40839.78 toks/s, output: 39.88 toks/s]
Processed prompts:  43%|████▎     | 1762/4096 [00:44<01:04, 36.07it/s, est. speed input: 40761.46 toks/s, output: 39.81 toks/s]
Processed prompts:  44%|████▍     | 1794/4096 [00:45<01:03, 36.06it/s, est. speed input: 40685.51 toks/s, output: 39.73 toks/s]
Processed prompts:  45%|████▍     | 1826/4096 [00:46<01:02, 36.06it/s, est. speed input: 40612.78 toks/s, output: 39.66 toks/s]
Processed prompts:  45%|████▌     | 1858/4096 [00:46<01:01, 36.35it/s, est. speed input: 40563.87 toks/s, output: 39.61 toks/s]
Processed prompts:  46%|████▌     | 1890/4096 [00:47<01:00, 36.26it/s, est. speed input: 40496.14 toks/s, output: 39.55 toks/s]
Processed prompts:  47%|████▋     | 1922/4096 [00:48<01:00, 36.20it/s, est. speed input: 40431.12 toks/s, output: 39.48 toks/s]
Processed prompts:  48%|████▊     | 1954/4096 [00:49<00:59, 36.15it/s, est. speed input: 40367.94 toks/s, output: 39.42 toks/s]
Processed prompts:  48%|████▊     | 1986/4096 [00:50<00:58, 36.12it/s, est. speed input: 40307.01 toks/s, output: 39.36 toks/s]
Processed prompts:  49%|████▉     | 2018/4096 [00:51<00:57, 36.10it/s, est. speed input: 40248.46 toks/s, output: 39.31 toks/s]
Processed prompts:  50%|█████     | 2050/4096 [00:52<00:56, 36.08it/s, est. speed input: 40191.65 toks/s, output: 39.25 toks/s]
Processed prompts:  51%|█████     | 2082/4096 [00:53<00:55, 36.07it/s, est. speed input: 40136.60 toks/s, output: 39.20 toks/s]
Processed prompts:  52%|█████▏    | 2114/4096 [00:54<00:54, 36.06it/s, est. speed input: 40083.61 toks/s, output: 39.14 toks/s]
Processed prompts:  52%|█████▏    | 2146/4096 [00:54<00:54, 36.05it/s, est. speed input: 40031.92 toks/s, output: 39.09 toks/s]
Processed prompts:  53%|█████▎    | 2178/4096 [00:55<00:53, 36.04it/s, est. speed input: 39981.94 toks/s, output: 39.04 toks/s]
Processed prompts:  54%|█████▍    | 2210/4096 [00:56<00:52, 36.04it/s, est. speed input: 39933.42 toks/s, output: 39.00 toks/s]
Processed prompts:  55%|█████▍    | 2242/4096 [00:57<00:51, 36.04it/s, est. speed input: 39886.78 toks/s, output: 38.95 toks/s]
Processed prompts:  56%|█████▌    | 2274/4096 [00:58<00:50, 36.05it/s, est. speed input: 39841.83 toks/s, output: 38.91 toks/s]
Processed prompts:  56%|█████▋    | 2306/4096 [00:59<00:49, 36.04it/s, est. speed input: 39797.49 toks/s, output: 38.86 toks/s]
Processed prompts:  57%|█████▋    | 2338/4096 [01:00<00:48, 36.03it/s, est. speed input: 39754.57 toks/s, output: 38.82 toks/s]
Processed prompts:  58%|█████▊    | 2370/4096 [01:01<00:47, 36.03it/s, est. speed input: 39712.80 toks/s, output: 38.78 toks/s]
Processed prompts:  59%|█████▊    | 2402/4096 [01:01<00:47, 36.03it/s, est. speed input: 39672.39 toks/s, output: 38.74 toks/s]
Processed prompts:  59%|█████▉    | 2434/4096 [01:02<00:46, 36.03it/s, est. speed input: 39633.03 toks/s, output: 38.70 toks/s]
Processed prompts:  60%|██████    | 2466/4096 [01:03<00:45, 36.03it/s, est. speed input: 39594.81 toks/s, output: 38.67 toks/s]
Processed prompts:  61%|██████    | 2498/4096 [01:04<00:44, 36.03it/s, est. speed input: 39557.96 toks/s, output: 38.63 toks/s]
Processed prompts:  62%|██████▏   | 2530/4096 [01:05<00:43, 36.01it/s, est. speed input: 39521.07 toks/s, output: 38.59 toks/s]
Processed prompts:  63%|██████▎   | 2562/4096 [01:06<00:42, 36.01it/s, est. speed input: 39485.73 toks/s, output: 38.56 toks/s]
Processed prompts:  63%|██████▎   | 2594/4096 [01:07<00:41, 36.02it/s, est. speed input: 39451.64 toks/s, output: 38.53 toks/s]
Processed prompts:  64%|██████▍   | 2626/4096 [01:08<00:40, 36.02it/s, est. speed input: 39417.96 toks/s, output: 38.49 toks/s]
Processed prompts:  65%|██████▍   | 2658/4096 [01:09<00:39, 36.01it/s, est. speed input: 39385.17 toks/s, output: 38.46 toks/s]
Processed prompts:  66%|██████▌   | 2690/4096 [01:09<00:39, 36.01it/s, est. speed input: 39353.27 toks/s, output: 38.43 toks/s]
Processed prompts:  66%|██████▋   | 2722/4096 [01:10<00:38, 36.01it/s, est. speed input: 39322.25 toks/s, output: 38.40 toks/s]
Processed prompts:  67%|██████▋   | 2754/4096 [01:11<00:37, 36.01it/s, est. speed input: 39291.88 toks/s, output: 38.37 toks/s]
Processed prompts:  68%|██████▊   | 2786/4096 [01:12<00:36, 36.01it/s, est. speed input: 39262.30 toks/s, output: 38.34 toks/s]
Processed prompts:  69%|██████▉   | 2818/4096 [01:13<00:35, 36.01it/s, est. speed input: 39233.35 toks/s, output: 38.31 toks/s]
Processed prompts:  70%|██████▉   | 2850/4096 [01:14<00:34, 36.01it/s, est. speed input: 39205.10 toks/s, output: 38.29 toks/s]
Processed prompts:  70%|███████   | 2882/4096 [01:15<00:33, 36.01it/s, est. speed input: 39177.62 toks/s, output: 38.26 toks/s]
Processed prompts:  71%|███████   | 2914/4096 [01:16<00:32, 36.01it/s, est. speed input: 39150.63 toks/s, output: 38.23 toks/s]
Processed prompts:  72%|███████▏  | 2946/4096 [01:17<00:31, 36.00it/s, est. speed input: 39124.28 toks/s, output: 38.21 toks/s]
Processed prompts:  73%|███████▎  | 2978/4096 [01:17<00:31, 36.00it/s, est. speed input: 39098.38 toks/s, output: 38.18 toks/s]
Processed prompts:  73%|███████▎  | 3010/4096 [01:18<00:30, 36.00it/s, est. speed input: 39073.18 toks/s, output: 38.16 toks/s]
Processed prompts:  74%|███████▍  | 3042/4096 [01:19<00:29, 36.00it/s, est. speed input: 39048.63 toks/s, output: 38.13 toks/s]
Processed prompts:  75%|███████▌  | 3074/4096 [01:20<00:28, 36.00it/s, est. speed input: 39024.52 toks/s, output: 38.11 toks/s]
Processed prompts:  76%|███████▌  | 3106/4096 [01:21<00:27, 36.00it/s, est. speed input: 39000.95 toks/s, output: 38.09 toks/s]
Processed prompts:  77%|███████▋  | 3138/4096 [01:22<00:26, 35.99it/s, est. speed input: 38977.71 toks/s, output: 38.06 toks/s]
Processed prompts:  77%|███████▋  | 3170/4096 [01:23<00:25, 36.00it/s, est. speed input: 38955.29 toks/s, output: 38.04 toks/s]
Processed prompts:  78%|███████▊  | 3202/4096 [01:24<00:24, 36.00it/s, est. speed input: 38933.39 toks/s, output: 38.02 toks/s]
Processed prompts:  79%|███████▉  | 3234/4096 [01:25<00:23, 36.00it/s, est. speed input: 38911.84 toks/s, output: 38.00 toks/s]
Processed prompts:  80%|███████▉  | 3266/4096 [01:25<00:23, 35.99it/s, est. speed input: 38890.33 toks/s, output: 37.98 toks/s]
Processed prompts:  81%|████████  | 3298/4096 [01:26<00:22, 36.00it/s, est. speed input: 38869.59 toks/s, output: 37.96 toks/s]
Processed prompts:  81%|████████▏ | 3330/4096 [01:27<00:21, 36.00it/s, est. speed input: 38849.39 toks/s, output: 37.94 toks/s]
Processed prompts:  82%|████████▏ | 3362/4096 [01:28<00:20, 36.00it/s, est. speed input: 38829.44 toks/s, output: 37.92 toks/s]
Processed prompts:  83%|████████▎ | 3394/4096 [01:29<00:19, 35.99it/s, est. speed input: 38809.70 toks/s, output: 37.90 toks/s]
Processed prompts:  84%|████████▎ | 3426/4096 [01:30<00:18, 36.00it/s, est. speed input: 38790.59 toks/s, output: 37.88 toks/s]
Processed prompts:  84%|████████▍ | 3458/4096 [01:31<00:17, 36.00it/s, est. speed input: 38771.93 toks/s, output: 37.86 toks/s]
Processed prompts:  85%|████████▌ | 3490/4096 [01:32<00:16, 36.00it/s, est. speed input: 38753.39 toks/s, output: 37.85 toks/s]
Processed prompts:  86%|████████▌ | 3522/4096 [01:33<00:15, 35.99it/s, est. speed input: 38735.17 toks/s, output: 37.83 toks/s]
Processed prompts:  87%|████████▋ | 3554/4096 [01:33<00:15, 35.99it/s, est. speed input: 38717.36 toks/s, output: 37.81 toks/s]
Processed prompts:  88%|████████▊ | 3586/4096 [01:34<00:14, 35.99it/s, est. speed input: 38699.82 toks/s, output: 37.79 toks/s]
Processed prompts:  88%|████████▊ | 3618/4096 [01:35<00:13, 35.99it/s, est. speed input: 38682.86 toks/s, output: 37.78 toks/s]
Processed prompts:  89%|████████▉ | 3650/4096 [01:36<00:12, 35.99it/s, est. speed input: 38666.00 toks/s, output: 37.76 toks/s]
Processed prompts:  90%|████████▉ | 3682/4096 [01:37<00:11, 35.99it/s, est. speed input: 38649.43 toks/s, output: 37.74 toks/s]
Processed prompts:  91%|█████████ | 3714/4096 [01:38<00:10, 35.99it/s, est. speed input: 38633.12 toks/s, output: 37.73 toks/s]
Processed prompts:  91%|█████████▏| 3746/4096 [01:39<00:09, 35.99it/s, est. speed input: 38617.32 toks/s, output: 37.71 toks/s]
Processed prompts:  92%|█████████▏| 3778/4096 [01:40<00:08, 35.99it/s, est. speed input: 38601.71 toks/s, output: 37.70 toks/s]
Processed prompts:  93%|█████████▎| 3810/4096 [01:41<00:07, 35.99it/s, est. speed input: 38586.28 toks/s, output: 37.68 toks/s]
Processed prompts:  94%|█████████▍| 3842/4096 [01:41<00:07, 35.99it/s, est. speed input: 38571.22 toks/s, output: 37.67 toks/s]
Processed prompts:  95%|█████████▍| 3874/4096 [01:42<00:06, 35.99it/s, est. speed input: 38556.30 toks/s, output: 37.65 toks/s]
Processed prompts:  95%|█████████▌| 3906/4096 [01:43<00:05, 35.99it/s, est. speed input: 38541.71 toks/s, output: 37.64 toks/s]
Processed prompts:  96%|█████████▌| 3938/4096 [01:44<00:04, 35.99it/s, est. speed input: 38527.37 toks/s, output: 37.62 toks/s]
Processed prompts:  97%|█████████▋| 3970/4096 [01:45<00:03, 35.99it/s, est. speed input: 38513.30 toks/s, output: 37.61 toks/s]
Processed prompts:  98%|█████████▊| 4002/4096 [01:46<00:02, 35.99it/s, est. speed input: 38499.51 toks/s, output: 37.60 toks/s]
Processed prompts:  98%|█████████▊| 4034/4096 [01:47<00:01, 36.30it/s, est. speed input: 38494.92 toks/s, output: 37.59 toks/s]
Processed prompts:  99%|█████████▉| 4066/4096 [01:48<00:00, 36.57it/s, est. speed input: 38491.70 toks/s, output: 37.59 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [01:48<00:00, 36.57it/s, est. speed input: 38775.60 toks/s, output: 37.87 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [01:48<00:00, 37.87it/s, est. speed input: 38775.60 toks/s, output: 37.87 toks/s]
[rank0]:[W125 19:53:37.529191755 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 148.0s

测试结果:
  Requests/s:   36.06
  Tokens/s:     36963.29
  Total Reqs:   4096
  Elapsed:      113.58s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     36927.23

============================================================
[8/8] 测试 M=65536
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-3B-INT8                                │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 65536
│   M_prefill     = 65536 (= 64 x 1024)
│   M_decode      = 64
│   batched_tokens = 65536 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 8192
│   --max-num-seqs           = 64
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 65536
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 19:54:04 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=402974) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=402974) WARNING 01-25 19:54:14 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 256, in _initialize_kv_caches
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]     self.model_executor.initialize_from_config(kv_cache_configs)
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 116, in initialize_from_config
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]     self.collective_rpc("compile_or_warm_up_model")
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 455, in compile_or_warm_up_model
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]     kernel_warmup(self)
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]   File "/root/vllmbench/vllm/model_executor/warmup/kernel_warmup.py", line 41, in kernel_warmup
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]     flashinfer_autotune(worker.model_runner)
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]   File "/root/vllmbench/vllm/model_executor/warmup/kernel_warmup.py", line 94, in flashinfer_autotune
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]     runner._dummy_run(
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]     outputs = self.model(
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]               ^^^^^^^^^^^
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 623, in forward
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]     model_output = self.model(
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]                    ^^^^^^^^^^^
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]   File "/root/vllmbench/vllm/compilation/decorators.py", line 439, in __call__
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]     return TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 223, in __call__
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 412, in forward
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]     def forward(
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]     raise e
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]   File "<eval_with_key>.58", line 304, in forward
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]     submod_4 = self.submod_4(getitem_8, s72, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_, getitem_9, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_8 = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_ = getitem_9 = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = None
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]     return range_entry.runnable(*args)
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]     return self._compiled_fn(*args)
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]     return fn(*args, **kwargs)
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]     return compiled_fn(full_args)
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]                             ^^^^^^^
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]     return self.current_callable(inputs)
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]     out = model(new_inputs)
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]   File "/tmp/torchinductor_root/ol/colmjan5lexazrkhwcmc5v6scbv5hrk5eaeilrxtnkx3o7x2m2tx.py", line 983, in call
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]     buf14 = torch.ops.slidesparse.dequant_bias.default(reinterpret_tensor(buf13, (s72, 16384), (16384, 1), 0), reinterpret_tensor(buf11, (s72, ), (1, ), 0), arg7_1, None, 'bfloat16', 'Llama3.2-3B-INT8')
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]   File "/root/vllmbench/slidesparse/core/kernels.py", line 483, in _dequant_bias_impl
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]     return fn(gemm_output, scale_a, scale_b, bias, out_dtype)
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]   File "/root/vllmbench/slidesparse/csrc/fused_dequant_bias_triton/build/RTX5080_cc120_py312_cu129_x86_64/dequant_bias_tuned_Llama3.2-3B.py", line 112, in dequant_bias_triton
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]     output = torch.empty((M, N), dtype=torch.bfloat16, device=gemm_output.device)
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=402974) ERROR 01-25 19:54:20 [core.py:866] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 15.46 GiB of which 1.97 GiB is free. Including non-PyTorch memory, this process has 13.27 GiB memory in use. Of the allocated memory 10.68 GiB is allocated by PyTorch, and 2.21 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)


─── STDERR ───
[2026-01-25 19:54:04] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:54:04] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:54:04] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:54:04] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:54:04] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:54:04] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:54:04] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:54:04] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:54:04] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:54:04] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:54:04] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:54:04] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:54:04] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:54:04] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 19:54:08] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 19:54:08] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-INT8'
[2026-01-25 19:54:08] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-INT8
[2026-01-25 19:54:08] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:54:08] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:54:08] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:54:08] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:54:08] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-INT8
[2026-01-25 19:54:08] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-INT8'
[2026-01-25 19:54:08] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 19:54:08] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 19:54:08] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 19:54:08] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 19:54:08] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=402974) [2026-01-25 19:54:09] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=402974) [2026-01-25 19:54:09] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=402974) [2026-01-25 19:54:09] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=402974) [2026-01-25 19:54:09] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=402974) [2026-01-25 19:54:09] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-3B-INT8
(EngineCore_DP0 pid=402974) [2026-01-25 19:54:09] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=402974) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=402974) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.00it/s]
(EngineCore_DP0 pid=402974) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.00it/s]
(EngineCore_DP0 pid=402974) 
(EngineCore_DP0 pid=402974) [2026-01-25 19:54:10] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 4928] -> 1D uint8
(EngineCore_DP0 pid=402974) [2026-01-25 19:54:10] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19005440 bytes
(EngineCore_DP0 pid=402974) [2026-01-25 19:54:10] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 4928] -> 1D uint8
(EngineCore_DP0 pid=402974) [2026-01-25 19:54:10] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 11403264 bytes
(EngineCore_DP0 pid=402974) [2026-01-25 19:54:10] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 4928] -> 1D uint8
(EngineCore_DP0 pid=402974) [2026-01-25 19:54:10] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 60817408 bytes
(EngineCore_DP0 pid=402974) [2026-01-25 19:54:10] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 13120] -> 1D uint8
(EngineCore_DP0 pid=402974) [2026-01-25 19:54:10] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 30277632 bytes
(EngineCore_DP0 pid=402974) [rank0]:W0125 19:54:17.129000 402974 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=402974) [rank0]:W0125 19:54:17.727000 402974 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 2 large pointwise nodes are separated
(EngineCore_DP0 pid=402974) 2026-01-25 19:54:20,865 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=402974) 2026-01-25 19:54:20,962 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=402974) Process EngineCore_DP0:
(EngineCore_DP0 pid=402974) Traceback (most recent call last):
(EngineCore_DP0 pid=402974)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=402974)     self.run()
(EngineCore_DP0 pid=402974)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=402974)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=402974)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=402974)     raise e
(EngineCore_DP0 pid=402974)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=402974)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=402974)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=402974)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=402974)     super().__init__(
(EngineCore_DP0 pid=402974)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=402974)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=402974)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=402974)   File "/root/vllmbench/vllm/v1/engine/core.py", line 256, in _initialize_kv_caches
(EngineCore_DP0 pid=402974)     self.model_executor.initialize_from_config(kv_cache_configs)
(EngineCore_DP0 pid=402974)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 116, in initialize_from_config
(EngineCore_DP0 pid=402974)     self.collective_rpc("compile_or_warm_up_model")
(EngineCore_DP0 pid=402974)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=402974)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=402974)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=402974)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=402974)     return func(*args, **kwargs)
(EngineCore_DP0 pid=402974)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=402974)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 455, in compile_or_warm_up_model
(EngineCore_DP0 pid=402974)     kernel_warmup(self)
(EngineCore_DP0 pid=402974)   File "/root/vllmbench/vllm/model_executor/warmup/kernel_warmup.py", line 41, in kernel_warmup
(EngineCore_DP0 pid=402974)     flashinfer_autotune(worker.model_runner)
(EngineCore_DP0 pid=402974)   File "/root/vllmbench/vllm/model_executor/warmup/kernel_warmup.py", line 94, in flashinfer_autotune
(EngineCore_DP0 pid=402974)     runner._dummy_run(
(EngineCore_DP0 pid=402974)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=402974)     return func(*args, **kwargs)
(EngineCore_DP0 pid=402974)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=402974)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4198, in _dummy_run
(EngineCore_DP0 pid=402974)     outputs = self.model(
(EngineCore_DP0 pid=402974)               ^^^^^^^^^^^
(EngineCore_DP0 pid=402974)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=402974)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=402974)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=402974)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=402974)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=402974)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=402974)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=402974)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=402974)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=402974)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 623, in forward
(EngineCore_DP0 pid=402974)     model_output = self.model(
(EngineCore_DP0 pid=402974)                    ^^^^^^^^^^^
(EngineCore_DP0 pid=402974)   File "/root/vllmbench/vllm/compilation/decorators.py", line 439, in __call__
(EngineCore_DP0 pid=402974)     return TorchCompileWithNoGuardsWrapper.__call__(self, *args, **kwargs)
(EngineCore_DP0 pid=402974)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=402974)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 223, in __call__
(EngineCore_DP0 pid=402974)     return self._call_with_optional_nvtx_range(
(EngineCore_DP0 pid=402974)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=402974)   File "/root/vllmbench/vllm/compilation/wrapper.py", line 109, in _call_with_optional_nvtx_range
(EngineCore_DP0 pid=402974)     return callable_fn(*args, **kwargs)
(EngineCore_DP0 pid=402974)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=402974)   File "/root/vllmbench/vllm/model_executor/models/llama.py", line 412, in forward
(EngineCore_DP0 pid=402974)     def forward(
(EngineCore_DP0 pid=402974)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=402974)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=402974)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=402974)   File "/root/vllmbench/vllm/compilation/caching.py", line 54, in __call__
(EngineCore_DP0 pid=402974)     return self.optimized_call(*args, **kwargs)
(EngineCore_DP0 pid=402974)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=402974)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 837, in call_wrapped
(EngineCore_DP0 pid=402974)     return self._wrapped_call(self, *args, **kwargs)
(EngineCore_DP0 pid=402974)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=402974)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 413, in __call__
(EngineCore_DP0 pid=402974)     raise e
(EngineCore_DP0 pid=402974)   File "/usr/local/lib/python3.12/dist-packages/torch/fx/graph_module.py", line 400, in __call__
(EngineCore_DP0 pid=402974)     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
(EngineCore_DP0 pid=402974)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=402974)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=402974)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=402974)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=402974)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=402974)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=402974)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=402974)   File "<eval_with_key>.58", line 304, in forward
(EngineCore_DP0 pid=402974)     submod_4 = self.submod_4(getitem_8, s72, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_, getitem_9, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_, l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_, l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_, l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_, l_positions_, l_self_modules_layers_modules_0_modules_self_attn_modules_rotary_emb_buffers_cos_sin_cache_);  getitem_8 = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_self_attn_modules_o_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_post_attention_layernorm_parameters_weight_ = getitem_9 = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_gate_up_proj_parameters_weight_scale_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_ = l_self_modules_layers_modules_1_modules_mlp_modules_down_proj_parameters_weight_scale_ = l_self_modules_layers_modules_2_modules_input_layernorm_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_ = l_self_modules_layers_modules_2_modules_self_attn_modules_qkv_proj_parameters_weight_scale_ = None
(EngineCore_DP0 pid=402974)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=402974)   File "/root/vllmbench/vllm/compilation/cuda_graph.py", line 220, in __call__
(EngineCore_DP0 pid=402974)     return self.runnable(*args, **kwargs)
(EngineCore_DP0 pid=402974)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=402974)   File "/root/vllmbench/vllm/compilation/piecewise_backend.py", line 178, in __call__
(EngineCore_DP0 pid=402974)     return range_entry.runnable(*args)
(EngineCore_DP0 pid=402974)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=402974)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/standalone_compile.py", line 63, in __call__
(EngineCore_DP0 pid=402974)     return self._compiled_fn(*args)
(EngineCore_DP0 pid=402974)            ^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=402974)   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 1044, in _fn
(EngineCore_DP0 pid=402974)     return fn(*args, **kwargs)
(EngineCore_DP0 pid=402974)            ^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=402974)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py", line 1130, in forward
(EngineCore_DP0 pid=402974)     return compiled_fn(full_args)
(EngineCore_DP0 pid=402974)            ^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=402974)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 353, in runtime_wrapper
(EngineCore_DP0 pid=402974)     all_outs = call_func_at_runtime_with_args(
(EngineCore_DP0 pid=402974)                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=402974)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py", line 129, in call_func_at_runtime_with_args
(EngineCore_DP0 pid=402974)     out = normalize_as_list(f(args))
(EngineCore_DP0 pid=402974)                             ^^^^^^^
(EngineCore_DP0 pid=402974)   File "/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py", line 526, in wrapper
(EngineCore_DP0 pid=402974)     return compiled_fn(runtime_args)
(EngineCore_DP0 pid=402974)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=402974)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py", line 613, in __call__
(EngineCore_DP0 pid=402974)     return self.current_callable(inputs)
(EngineCore_DP0 pid=402974)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=402974)   File "/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py", line 2962, in run
(EngineCore_DP0 pid=402974)     out = model(new_inputs)
(EngineCore_DP0 pid=402974)           ^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=402974)   File "/tmp/torchinductor_root/ol/colmjan5lexazrkhwcmc5v6scbv5hrk5eaeilrxtnkx3o7x2m2tx.py", line 983, in call
(EngineCore_DP0 pid=402974)     buf14 = torch.ops.slidesparse.dequant_bias.default(reinterpret_tensor(buf13, (s72, 16384), (16384, 1), 0), reinterpret_tensor(buf11, (s72, ), (1, ), 0), arg7_1, None, 'bfloat16', 'Llama3.2-3B-INT8')
(EngineCore_DP0 pid=402974)             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=402974)   File "/usr/local/lib/python3.12/dist-packages/torch/_ops.py", line 841, in __call__
(EngineCore_DP0 pid=402974)     return self._op(*args, **kwargs)
(EngineCore_DP0 pid=402974)            ^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=402974)   File "/root/vllmbench/slidesparse/core/kernels.py", line 483, in _dequant_bias_impl
(EngineCore_DP0 pid=402974)     return fn(gemm_output, scale_a, scale_b, bias, out_dtype)
(EngineCore_DP0 pid=402974)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=402974)   File "/root/vllmbench/slidesparse/csrc/fused_dequant_bias_triton/build/RTX5080_cc120_py312_cu129_x86_64/dequant_bias_tuned_Llama3.2-3B.py", line 112, in dequant_bias_triton
(EngineCore_DP0 pid=402974)     output = torch.empty((M, N), dtype=torch.bfloat16, device=gemm_output.device)
(EngineCore_DP0 pid=402974)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=402974) torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 15.46 GiB of which 1.97 GiB is free. Including non-PyTorch memory, this process has 13.27 GiB memory in use. Of the allocated memory 10.68 GiB is allocated by PyTorch, and 2.21 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]:[W125 19:54:21.272342844 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[ERROR] 测试失败: M=65536 (exit code: 1)


------------------------------------------------------------
  生成 CSV: Llama3.2-3B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_10/Llama3.2-3B-INT8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,56.2303,28846.1388,2.2764
1024,1024,1,128,128,34.8271,35697.8186,3.6753
2048,1024,2,256,128,39.0141,39989.4934,6.5617
4096,1024,4,512,128,37.3799,38314.3625,13.6972
8192,1024,8,1024,128,36.3125,37220.3188,28.1997
16384,1024,16,2048,128,35.9697,36868.9109,56.9369
32768,1024,32,4096,128,36.0617,36963.2895,113.5830
65536,1024,64,8192,128,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[INFO] 完成: 7 成功, 1 失败


============================================================
  Benchmark 完成!
============================================================


总计: 36 成功, 4 失败
============================================================
