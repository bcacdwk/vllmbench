======================================================================
SlideSparse vLLM Throughput Benchmark Log
Created: 2026-01-25 22:55:06
======================================================================

原始命令:
  /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-int8 --backend cublaslt,cusparselt --stage decode --sparsity 2_4,2_6,2_8,2_10 --M 64,128,256,512

命令行参数:
  --model: qwen2.5-7b-int8
  --backend: cublaslt,cusparselt
  --sparsity: 2_4,2_6,2_8,2_10
  --stage: decode
  --M: 64,128,256,512
  --N: None
  --inner-32: False
  --eager: False
  --gpu-id: 0
  --gpu-mem: 0.8
  --dry-run: False
  --list-models: False

硬件信息:
  GPU: RTX5080
  Compute Capability: cc120
  VRAM: 15.5 GB
  CUDA: 12.9
  Python: py312

Backend 环境变量 (初始状态):
  DISABLE_SLIDESPARSE: 未设置
  USE_CUBLASLT: 未设置
  USE_CUSPARSELT: 未设置
  SPARSITY: 未设置
  INNER_DTYPE_32: 未设置

======================================================================


============================================================
  Qwen2.5-7B-INT8 | cuBLASLt | decode
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints/Qwen2.5-7B-INT8
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_INT8_py312_cu129_x86_64/cublaslt

============================================================
[1/4] 测试 M=64
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 64
│   M_prefill     = 1024 (= 64 x 16)
│   M_decode      = 64
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 64
│   --max-num-seqs           = 64
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 22:55:10 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=631042) WARNING 01-25 22:55:22 [backends.py:609] Failed to read file <frozen os>
Throughput: 16.50 requests/s, 4489.28 total tokens/s, 4225.21 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384


─── STDERR ───
[2026-01-25 22:55:10] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:55:10] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-25 22:55:10] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-25 22:55:10] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:55:10] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:55:10] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:55:10] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:55:10] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:55:10] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-25 22:55:10] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:55:10] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:55:10] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:55:10] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:55:10] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 22:55:14] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:55:14] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-25 22:55:14] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-25 22:55:14] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:55:14] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:55:14] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:55:14] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:55:14] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:55:14] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-25 22:55:14] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:55:14] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:55:14] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:55:14] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:55:14] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=631042) [2026-01-25 22:55:14] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=631042) [2026-01-25 22:55:14] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=631042) [2026-01-25 22:55:14] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=631042) [2026-01-25 22:55:14] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=631042) [2026-01-25 22:55:14] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=631042) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=631042) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.38s/it]
(EngineCore_DP0 pid=631042) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:03<00:00,  1.84s/it]
(EngineCore_DP0 pid=631042) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:03<00:00,  1.77s/it]
(EngineCore_DP0 pid=631042) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=631042) 2026-01-25 22:55:28,541 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=631042) 2026-01-25 22:55:28,561 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=631042) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   5%|▌         | 1/19 [00:00<00:03,  5.70it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  21%|██        | 4/19 [00:00<00:00, 15.65it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 7/19 [00:00<00:00, 21.02it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 10/19 [00:00<00:00, 23.96it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▎  | 14/19 [00:00<00:00, 26.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 17/19 [00:00<00:00, 27.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:00<00:00, 22.71it/s]
(EngineCore_DP0 pid=631042) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   9%|▉         | 1/11 [00:00<00:01,  7.28it/s]
Capturing CUDA graphs (decode, FULL):  36%|███▋      | 4/11 [00:00<00:00, 18.79it/s]
Capturing CUDA graphs (decode, FULL):  64%|██████▎   | 7/11 [00:00<00:00, 23.47it/s]
Capturing CUDA graphs (decode, FULL):  91%|█████████ | 10/11 [00:00<00:00, 22.63it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:00<00:00, 21.37it/s]

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 64/64 [00:00<00:00, 2779.09it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 1/64 [00:03<03:58,  3.79s/it, est. speed input: 4.23 toks/s, output: 67.62 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:03<00:00,  3.79s/it, est. speed input: 265.78 toks/s, output: 4252.45 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:03<00:00, 16.61it/s, est. speed input: 265.78 toks/s, output: 4252.45 toks/s]
[rank0]:[W125 22:55:34.792227911 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 29.8s

测试结果:
  Requests/s:   16.50
  Tokens/s:     4489.28
  Total Reqs:   64
  Elapsed:      3.88s

  [Decode 分析]
  Total Decode Tokens:  16384
  Decode Tokens/s:      4225.21

============================================================
[2/4] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 2048 (= 128 x 16)
│   M_decode      = 128
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 128
│   --max-num-seqs           = 128
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 22:55:40 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=631752) WARNING 01-25 22:55:49 [backends.py:609] Failed to read file <frozen os>
Throughput: 27.59 requests/s, 7504.65 total tokens/s, 7063.20 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768


─── STDERR ───
[2026-01-25 22:55:40] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:55:40] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-25 22:55:40] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-25 22:55:40] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:55:40] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:55:40] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:55:40] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:55:40] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:55:40] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-25 22:55:40] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:55:40] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:55:40] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:55:40] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:55:40] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 22:55:43] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:55:43] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-25 22:55:43] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-25 22:55:43] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:55:43] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:55:43] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:55:43] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:55:43] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:55:43] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-25 22:55:43] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:55:43] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:55:43] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:55:43] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:55:43] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=631752) [2026-01-25 22:55:44] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=631752) [2026-01-25 22:55:44] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=631752) [2026-01-25 22:55:44] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=631752) [2026-01-25 22:55:44] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=631752) [2026-01-25 22:55:44] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=631752) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=631752) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  3.04it/s]
(EngineCore_DP0 pid=631752) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.25it/s]
(EngineCore_DP0 pid=631752) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.34it/s]
(EngineCore_DP0 pid=631752) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=631752) 2026-01-25 22:55:53,283 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=631752) 2026-01-25 22:55:53,305 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=631752) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 2/35 [00:00<00:03, 10.05it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 5/35 [00:00<00:01, 16.88it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  23%|██▎       | 8/35 [00:00<00:01, 20.32it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 11/35 [00:00<00:01, 20.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 14/35 [00:00<00:01, 18.79it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  46%|████▌     | 16/35 [00:00<00:01, 14.82it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  54%|█████▍    | 19/35 [00:01<00:00, 16.85it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 22/35 [00:01<00:00, 19.74it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 25/35 [00:01<00:00, 22.16it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 28/35 [00:01<00:00, 24.04it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▊ | 31/35 [00:01<00:00, 25.60it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  97%|█████████▋| 34/35 [00:01<00:00, 26.63it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:01<00:00, 20.92it/s]
(EngineCore_DP0 pid=631752) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   5%|▌         | 1/19 [00:00<00:02,  7.32it/s]
Capturing CUDA graphs (decode, FULL):  26%|██▋       | 5/19 [00:00<00:00, 20.50it/s]
Capturing CUDA graphs (decode, FULL):  42%|████▏     | 8/19 [00:00<00:00, 22.40it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 12/19 [00:00<00:00, 25.55it/s]
Capturing CUDA graphs (decode, FULL):  84%|████████▍ | 16/19 [00:00<00:00, 27.20it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:00<00:00, 25.16it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 4214.98it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:04<09:24,  4.45s/it, est. speed input: 3.60 toks/s, output: 57.58 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:04<00:01, 26.89it/s, est. speed input: 305.43 toks/s, output: 4886.90 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:04<00:00, 26.89it/s, est. speed input: 444.53 toks/s, output: 7112.53 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:04<00:00, 27.78it/s, est. speed input: 444.53 toks/s, output: 7112.53 toks/s]
[rank0]:[W125 22:56:01.285094874 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 26.5s

测试结果:
  Requests/s:   27.59
  Tokens/s:     7504.65
  Total Reqs:   128
  Elapsed:      4.64s

  [Decode 分析]
  Total Decode Tokens:  32768
  Decode Tokens/s:      7063.20

============================================================
[3/4] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 4096 (= 256 x 16)
│   M_decode      = 256
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 256
│   --max-num-seqs           = 256
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 22:56:06 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=632395) WARNING 01-25 22:56:15 [backends.py:609] Failed to read file <frozen os>
Throughput: 24.81 requests/s, 6747.48 total tokens/s, 6350.57 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536


─── STDERR ───
[2026-01-25 22:56:06] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:56:06] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-25 22:56:06] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-25 22:56:06] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:56:06] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:56:06] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:56:06] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:56:06] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:56:06] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-25 22:56:06] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:56:06] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:56:06] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:56:06] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:56:06] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 22:56:10] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:56:10] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-25 22:56:10] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-25 22:56:10] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:56:10] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:56:10] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:56:10] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:56:10] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:56:10] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-25 22:56:10] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:56:10] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:56:10] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:56:10] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:56:10] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=632395) [2026-01-25 22:56:11] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=632395) [2026-01-25 22:56:11] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=632395) [2026-01-25 22:56:11] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=632395) [2026-01-25 22:56:11] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=632395) [2026-01-25 22:56:11] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=632395) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=632395) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  3.07it/s]
(EngineCore_DP0 pid=632395) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.27it/s]
(EngineCore_DP0 pid=632395) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.36it/s]
(EngineCore_DP0 pid=632395) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=632395) 2026-01-25 22:56:19,875 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=632395) 2026-01-25 22:56:19,891 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=632395) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/36 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 3/36 [00:00<00:01, 22.10it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 6/36 [00:00<00:01, 21.02it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 9/36 [00:00<00:01, 22.85it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 12/36 [00:00<00:00, 24.24it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 15/36 [00:00<00:00, 25.05it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 18/36 [00:00<00:00, 25.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 21/36 [00:00<00:00, 26.54it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 24/36 [00:00<00:00, 26.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 27/36 [00:01<00:00, 22.92it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  83%|████████▎ | 30/36 [00:01<00:00, 18.91it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 33/36 [00:01<00:00, 18.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:01<00:00, 20.29it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:01<00:00, 22.11it/s]
(EngineCore_DP0 pid=632395) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   3%|▎         | 1/35 [00:00<00:04,  7.07it/s]
Capturing CUDA graphs (decode, FULL):  11%|█▏        | 4/35 [00:00<00:01, 16.98it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 7/35 [00:00<00:01, 21.03it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 10/35 [00:00<00:01, 23.28it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 13/35 [00:00<00:00, 24.85it/s]
Capturing CUDA graphs (decode, FULL):  46%|████▌     | 16/35 [00:00<00:00, 25.98it/s]
Capturing CUDA graphs (decode, FULL):  54%|█████▍    | 19/35 [00:00<00:00, 27.20it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 22/35 [00:00<00:00, 26.60it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 25/35 [00:01<00:00, 27.33it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 28/35 [00:01<00:00, 27.93it/s]
Capturing CUDA graphs (decode, FULL):  91%|█████████▏| 32/35 [00:01<00:00, 28.64it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:01<00:00, 25.83it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 6677.58it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:07<29:57,  7.05s/it, est. speed input: 2.27 toks/s, output: 36.31 toks/s]
Processed prompts:  24%|██▍       | 62/256 [00:07<00:15, 12.23it/s, est. speed input: 138.52 toks/s, output: 2216.29 toks/s]
Processed prompts:  53%|█████▎    | 136/256 [00:07<00:03, 32.16it/s, est. speed input: 299.20 toks/s, output: 4787.27 toks/s]
Processed prompts:  72%|███████▏  | 184/256 [00:07<00:01, 48.77it/s, est. speed input: 398.45 toks/s, output: 6375.28 toks/s]
Processed prompts:  90%|████████▉ | 230/256 [00:09<00:00, 34.56it/s, est. speed input: 387.92 toks/s, output: 6206.64 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:10<00:00, 34.56it/s, est. speed input: 398.47 toks/s, output: 6375.56 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:10<00:00, 24.90it/s, est. speed input: 398.47 toks/s, output: 6375.56 toks/s]
[rank0]:[W125 22:56:34.042737658 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 32.7s

测试结果:
  Requests/s:   24.81
  Tokens/s:     6747.48
  Total Reqs:   256
  Elapsed:      10.32s

  [Decode 分析]
  Total Decode Tokens:  65536
  Decode Tokens/s:      6350.57

============================================================
[4/4] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 8192 (= 512 x 16)
│   M_decode      = 512
│   batched_tokens = 512 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 512
│   --max-num-seqs           = 512
│   --max-model-len          = 272
│   --max-num-batched-tokens = 512
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 22:56:39 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=633104) WARNING 01-25 22:56:48 [backends.py:609] Failed to read file <frozen os>
Throughput: 23.27 requests/s, 6330.05 total tokens/s, 5957.70 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072


─── STDERR ───
[2026-01-25 22:56:39] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:56:39] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-25 22:56:39] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-25 22:56:39] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:56:39] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:56:39] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:56:39] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:56:39] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:56:39] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-25 22:56:39] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:56:39] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:56:39] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:56:39] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:56:39] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 22:56:43] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:56:43] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-25 22:56:43] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-25 22:56:43] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:56:43] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:56:43] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:56:43] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:56:43] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:56:43] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-25 22:56:43] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:56:43] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:56:43] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:56:43] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:56:43] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=633104) [2026-01-25 22:56:43] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=633104) [2026-01-25 22:56:43] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=633104) [2026-01-25 22:56:43] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=633104) [2026-01-25 22:56:43] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=633104) [2026-01-25 22:56:43] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=633104) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=633104) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  3.04it/s]
(EngineCore_DP0 pid=633104) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.27it/s]
(EngineCore_DP0 pid=633104) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.36it/s]
(EngineCore_DP0 pid=633104) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=633104) 2026-01-25 22:56:54,687 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=633104) 2026-01-25 22:56:54,702 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=633104) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 3/51 [00:00<00:02, 20.41it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|█▏        | 6/51 [00:00<00:02, 20.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 9/51 [00:00<00:01, 21.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|██▎       | 12/51 [00:00<00:01, 22.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▉       | 15/51 [00:00<00:01, 24.04it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  35%|███▌      | 18/51 [00:00<00:01, 23.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|████      | 21/51 [00:00<00:01, 23.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 24/51 [00:01<00:01, 24.34it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 27/51 [00:01<00:00, 25.00it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  59%|█████▉    | 30/51 [00:01<00:00, 24.36it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  65%|██████▍   | 33/51 [00:01<00:00, 25.18it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████   | 36/51 [00:01<00:00, 25.90it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|███████▋  | 39/51 [00:01<00:00, 26.63it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 42/51 [00:01<00:00, 27.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  88%|████████▊ | 45/51 [00:01<00:00, 27.77it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 48/51 [00:01<00:00, 28.25it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:02<00:00, 27.15it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:02<00:00, 25.17it/s]
(EngineCore_DP0 pid=633104) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   2%|▏         | 1/51 [00:00<00:08,  5.98it/s]
Capturing CUDA graphs (decode, FULL):   6%|▌         | 3/51 [00:00<00:04,  9.63it/s]
Capturing CUDA graphs (decode, FULL):  10%|▉         | 5/51 [00:00<00:04, 10.19it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▎        | 7/51 [00:00<00:03, 12.48it/s]
Capturing CUDA graphs (decode, FULL):  20%|█▉        | 10/51 [00:00<00:02, 16.13it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 13/51 [00:00<00:01, 19.01it/s]
Capturing CUDA graphs (decode, FULL):  31%|███▏      | 16/51 [00:00<00:01, 21.23it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 19/51 [00:01<00:01, 22.47it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 22/51 [00:01<00:01, 23.48it/s]
Capturing CUDA graphs (decode, FULL):  49%|████▉     | 25/51 [00:01<00:01, 24.46it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 28/51 [00:01<00:00, 24.84it/s]
Capturing CUDA graphs (decode, FULL):  61%|██████    | 31/51 [00:01<00:00, 25.90it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 34/51 [00:01<00:00, 26.75it/s]
Capturing CUDA graphs (decode, FULL):  73%|███████▎  | 37/51 [00:01<00:00, 27.55it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 41/51 [00:01<00:00, 28.47it/s]
Capturing CUDA graphs (decode, FULL):  88%|████████▊ | 45/51 [00:02<00:00, 29.03it/s]
Capturing CUDA graphs (decode, FULL):  96%|█████████▌| 49/51 [00:02<00:00, 29.42it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:02<00:00, 22.97it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 8246.10it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:06<56:33,  6.64s/it, est. speed input: 2.41 toks/s, output: 38.55 toks/s]
Processed prompts:  18%|█▊        | 91/512 [00:06<00:22, 19.10it/s, est. speed input: 215.96 toks/s, output: 3455.40 toks/s]
Processed prompts:  28%|██▊       | 144/512 [00:10<00:22, 16.31it/s, est. speed input: 218.18 toks/s, output: 3490.90 toks/s]
Processed prompts:  34%|███▍      | 174/512 [00:11<00:18, 18.22it/s, est. speed input: 238.31 toks/s, output: 3812.89 toks/s]
Processed prompts:  38%|███▊      | 193/512 [00:12<00:15, 20.35it/s, est. speed input: 253.79 toks/s, output: 4060.70 toks/s]
Processed prompts:  40%|████      | 207/512 [00:12<00:14, 21.47it/s, est. speed input: 261.91 toks/s, output: 4190.61 toks/s]
Processed prompts:  42%|████▏     | 217/512 [00:12<00:12, 22.84it/s, est. speed input: 268.67 toks/s, output: 4298.68 toks/s]
Processed prompts:  44%|████▍     | 225/512 [00:13<00:12, 22.85it/s, est. speed input: 271.24 toks/s, output: 4339.82 toks/s]
Processed prompts:  45%|████▌     | 232/512 [00:13<00:11, 24.78it/s, est. speed input: 276.53 toks/s, output: 4424.54 toks/s]
Processed prompts:  46%|████▋     | 238/512 [00:13<00:10, 26.69it/s, est. speed input: 280.91 toks/s, output: 4494.61 toks/s]
Processed prompts:  48%|████▊     | 244/512 [00:13<00:09, 28.61it/s, est. speed input: 284.99 toks/s, output: 4559.91 toks/s]
Processed prompts:  49%|████▉     | 251/512 [00:13<00:08, 30.94it/s, est. speed input: 289.65 toks/s, output: 4634.44 toks/s]
Processed prompts:  50%|█████     | 256/512 [00:14<00:09, 26.26it/s, est. speed input: 288.95 toks/s, output: 4623.26 toks/s]
Processed prompts:  51%|█████     | 260/512 [00:14<00:13, 18.60it/s, est. speed input: 283.49 toks/s, output: 4535.89 toks/s]
Processed prompts:  51%|█████▏    | 263/512 [00:15<00:19, 12.98it/s, est. speed input: 275.66 toks/s, output: 4410.57 toks/s]
Processed prompts:  52%|█████▏    | 266/512 [00:15<00:19, 12.38it/s, est. speed input: 273.60 toks/s, output: 4377.61 toks/s]
Processed prompts:  52%|█████▏    | 268/512 [00:15<00:19, 12.32it/s, est. speed input: 272.73 toks/s, output: 4363.71 toks/s]
Processed prompts:  53%|█████▎    | 270/512 [00:15<00:19, 12.49it/s, est. speed input: 272.17 toks/s, output: 4354.68 toks/s]
Processed prompts:  53%|█████▎    | 273/512 [00:16<00:17, 13.56it/s, est. speed input: 272.25 toks/s, output: 4356.06 toks/s]
Processed prompts:  54%|█████▍    | 277/512 [00:16<00:14, 16.75it/s, est. speed input: 273.98 toks/s, output: 4383.75 toks/s]
Processed prompts:  55%|█████▍    | 280/512 [00:16<00:12, 17.85it/s, est. speed input: 274.61 toks/s, output: 4393.70 toks/s]
Processed prompts:  55%|█████▌    | 283/512 [00:16<00:11, 19.48it/s, est. speed input: 275.56 toks/s, output: 4409.00 toks/s]
Processed prompts:  56%|█████▌    | 286/512 [00:16<00:10, 20.87it/s, est. speed input: 276.50 toks/s, output: 4423.98 toks/s]
Processed prompts:  57%|█████▋    | 290/512 [00:16<00:09, 24.07it/s, est. speed input: 278.36 toks/s, output: 4453.76 toks/s]
Processed prompts:  58%|█████▊    | 296/512 [00:16<00:06, 31.83it/s, est. speed input: 282.29 toks/s, output: 4516.72 toks/s]
Processed prompts:  59%|█████▊    | 300/512 [00:16<00:07, 29.52it/s, est. speed input: 283.41 toks/s, output: 4534.61 toks/s]
Processed prompts:  59%|█████▉    | 304/512 [00:17<00:07, 28.45it/s, est. speed input: 284.62 toks/s, output: 4553.99 toks/s]
Processed prompts:  60%|██████    | 308/512 [00:17<00:07, 27.78it/s, est. speed input: 285.83 toks/s, output: 4573.22 toks/s]
Processed prompts:  61%|██████    | 313/512 [00:17<00:06, 29.77it/s, est. speed input: 288.04 toks/s, output: 4608.65 toks/s]
Processed prompts:  62%|██████▏   | 317/512 [00:17<00:06, 30.61it/s, est. speed input: 289.70 toks/s, output: 4635.21 toks/s]
Processed prompts:  63%|██████▎   | 323/512 [00:17<00:05, 35.71it/s, est. speed input: 293.14 toks/s, output: 4690.30 toks/s]
Processed prompts:  64%|██████▍   | 327/512 [00:17<00:05, 36.15it/s, est. speed input: 294.99 toks/s, output: 4719.76 toks/s]
Processed prompts:  65%|██████▍   | 331/512 [00:17<00:05, 36.07it/s, est. speed input: 296.73 toks/s, output: 4747.64 toks/s]
Processed prompts:  66%|██████▌   | 337/512 [00:17<00:04, 41.02it/s, est. speed input: 300.22 toks/s, output: 4803.53 toks/s]
Processed prompts:  67%|██████▋   | 342/512 [00:18<00:04, 40.07it/s, est. speed input: 302.46 toks/s, output: 4839.41 toks/s]
Processed prompts:  68%|██████▊   | 347/512 [00:18<00:04, 39.53it/s, est. speed input: 304.69 toks/s, output: 4875.01 toks/s]
Processed prompts:  69%|██████▉   | 352/512 [00:18<00:04, 37.35it/s, est. speed input: 306.54 toks/s, output: 4904.67 toks/s]
Processed prompts:  70%|███████   | 359/512 [00:18<00:03, 44.85it/s, est. speed input: 310.87 toks/s, output: 4973.96 toks/s]
Processed prompts:  72%|███████▏  | 368/512 [00:18<00:02, 55.96it/s, est. speed input: 316.89 toks/s, output: 5070.29 toks/s]
Processed prompts:  75%|███████▌  | 384/512 [00:18<00:01, 80.00it/s, est. speed input: 328.60 toks/s, output: 5257.54 toks/s]
Processed prompts:  77%|███████▋  | 393/512 [00:19<00:03, 32.29it/s, est. speed input: 324.42 toks/s, output: 5190.68 toks/s]
Processed prompts:  78%|███████▊  | 400/512 [00:19<00:04, 24.45it/s, est. speed input: 321.87 toks/s, output: 5149.86 toks/s]
Processed prompts:  79%|███████▉  | 405/512 [00:20<00:04, 25.03it/s, est. speed input: 322.95 toks/s, output: 5167.27 toks/s]
Processed prompts:  80%|████████  | 410/512 [00:20<00:03, 26.65it/s, est. speed input: 324.60 toks/s, output: 5193.53 toks/s]
Processed prompts:  81%|████████  | 414/512 [00:20<00:03, 25.77it/s, est. speed input: 324.94 toks/s, output: 5199.08 toks/s]
Processed prompts:  82%|████████▏ | 418/512 [00:20<00:03, 23.63it/s, est. speed input: 324.61 toks/s, output: 5193.74 toks/s]
Processed prompts:  82%|████████▏ | 422/512 [00:20<00:03, 24.70it/s, est. speed input: 325.52 toks/s, output: 5208.31 toks/s]
Processed prompts:  83%|████████▎ | 426/512 [00:20<00:03, 24.39it/s, est. speed input: 325.93 toks/s, output: 5214.93 toks/s]
Processed prompts:  84%|████████▍ | 431/512 [00:21<00:02, 28.70it/s, est. speed input: 328.08 toks/s, output: 5249.27 toks/s]
Processed prompts:  85%|████████▌ | 436/512 [00:21<00:02, 31.81it/s, est. speed input: 330.01 toks/s, output: 5280.19 toks/s]
Processed prompts:  86%|████████▋ | 442/512 [00:21<00:01, 35.42it/s, est. speed input: 332.46 toks/s, output: 5319.31 toks/s]
Processed prompts:  88%|████████▊ | 448/512 [00:21<00:01, 40.84it/s, est. speed input: 335.37 toks/s, output: 5365.91 toks/s]
Processed prompts:  89%|████████▉ | 456/512 [00:21<00:01, 48.73it/s, est. speed input: 339.55 toks/s, output: 5432.76 toks/s]
Processed prompts:  91%|█████████ | 464/512 [00:21<00:00, 54.88it/s, est. speed input: 343.71 toks/s, output: 5499.28 toks/s]
Processed prompts:  92%|█████████▏| 470/512 [00:21<00:00, 55.01it/s, est. speed input: 346.41 toks/s, output: 5542.58 toks/s]
Processed prompts:  95%|█████████▍| 484/512 [00:21<00:00, 76.37it/s, est. speed input: 355.00 toks/s, output: 5679.94 toks/s]
Processed prompts:  99%|█████████▉| 506/512 [00:21<00:00, 111.89it/s, est. speed input: 369.24 toks/s, output: 5907.89 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:21<00:00, 111.89it/s, est. speed input: 373.42 toks/s, output: 5974.78 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:21<00:00, 23.34it/s, est. speed input: 373.42 toks/s, output: 5974.78 toks/s] 
[rank0]:[W125 22:57:22.871635571 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 47.8s

测试结果:
  Requests/s:   23.27
  Tokens/s:     6330.05
  Total Reqs:   512
  Elapsed:      22.00s

  [Decode 分析]
  Total Decode Tokens:  131072
  Decode Tokens/s:      5957.70


------------------------------------------------------------
  生成 CSV: Qwen2.5-7B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_INT8_py312_cu129_x86_64/cublaslt/Qwen2.5-7B-INT8_decode.csv

预览:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
64,16,64,64,256,256,16.5047,4489.2828,3.8777
128,16,128,128,256,256,27.5906,7504.6504,4.6393
256,16,256,256,256,256,24.8069,6747.4791,10.3197
512,16,512,512,256,256,23.2722,6330.0516,22.0005

------------------------------------------------------------

[INFO] 完成: 4 成功, 0 失败

============================================================
  Qwen2.5-7B-INT8 | cuSPARSELt (2_4) | decode
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_4
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_4

============================================================
[1/4] 测试 M=64
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 64
│   M_prefill     = 1024 (= 64 x 16)
│   M_decode      = 64
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 64
│   --max-num-seqs           = 64
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 22:57:27 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=634039) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=634039) WARNING 01-25 22:57:38 [backends.py:609] Failed to read file <frozen os>
Throughput: 22.83 requests/s, 6210.67 total tokens/s, 5845.34 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384


─── STDERR ───
[2026-01-25 22:57:27] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:57:27] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-25 22:57:27] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-25 22:57:27] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:57:27] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:57:27] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:57:27] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:57:27] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:57:27] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-25 22:57:27] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:57:27] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:57:27] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:57:27] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:57:27] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 22:57:31] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:57:31] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-25 22:57:31] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-25 22:57:31] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:57:31] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:57:31] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:57:31] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:57:31] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:57:31] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-25 22:57:31] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:57:31] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:57:31] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:57:31] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:57:31] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=634039) [2026-01-25 22:57:31] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=634039) [2026-01-25 22:57:31] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=634039) [2026-01-25 22:57:31] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=634039) [2026-01-25 22:57:31] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=634039) [2026-01-25 22:57:31] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=634039) [2026-01-25 22:57:31] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=634039) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=634039) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.12s/it]
(EngineCore_DP0 pid=634039) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.31s/it]
(EngineCore_DP0 pid=634039) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.28s/it]
(EngineCore_DP0 pid=634039) 
(EngineCore_DP0 pid=634039) [2026-01-25 22:57:34] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 3584] -> 1D uint8
(EngineCore_DP0 pid=634039) [2026-01-25 22:57:34] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12386304 bytes
(EngineCore_DP0 pid=634039) [2026-01-25 22:57:34] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 3584] -> 1D uint8
(EngineCore_DP0 pid=634039) [2026-01-25 22:57:34] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9633792 bytes
(EngineCore_DP0 pid=634039) [2026-01-25 22:57:34] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 3584] -> 1D uint8
(EngineCore_DP0 pid=634039) [2026-01-25 22:57:34] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 101842944 bytes
(EngineCore_DP0 pid=634039) [2026-01-25 22:57:34] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 18944] -> 1D uint8
(EngineCore_DP0 pid=634039) [2026-01-25 22:57:34] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 50921472 bytes
(EngineCore_DP0 pid=634039) 2026-01-25 22:57:44,879 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=634039) 2026-01-25 22:57:44,893 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=634039) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   5%|▌         | 1/19 [00:00<00:03,  5.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 3/19 [00:00<00:01,  8.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  32%|███▏      | 6/19 [00:00<00:00, 15.05it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 9/19 [00:00<00:00, 18.91it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 12/19 [00:00<00:00, 22.12it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  79%|███████▉  | 15/19 [00:00<00:00, 24.14it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:00<00:00, 24.86it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:00<00:00, 20.03it/s]
(EngineCore_DP0 pid=634039) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   9%|▉         | 1/11 [00:00<00:01,  7.07it/s]
Capturing CUDA graphs (decode, FULL):  45%|████▌     | 5/11 [00:00<00:00, 21.07it/s]
Capturing CUDA graphs (decode, FULL):  82%|████████▏ | 9/11 [00:00<00:00, 26.24it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:00<00:00, 24.51it/s]

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 64/64 [00:00<00:00, 2825.19it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 1/64 [00:02<02:52,  2.73s/it, est. speed input: 5.86 toks/s, output: 93.70 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:02<00:00,  2.73s/it, est. speed input: 368.54 toks/s, output: 5896.58 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:02<00:00, 23.03it/s, est. speed input: 368.54 toks/s, output: 5896.58 toks/s]
[rank0]:[W125 22:57:50.031185555 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 28.2s

测试结果:
  Requests/s:   22.83
  Tokens/s:     6210.67
  Total Reqs:   64
  Elapsed:      2.80s

  [Decode 分析]
  Total Decode Tokens:  16384
  Decode Tokens/s:      5845.34

============================================================
[2/4] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 2048 (= 128 x 16)
│   M_decode      = 128
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 128
│   --max-num-seqs           = 128
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 22:57:55 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=634739) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=634739) WARNING 01-25 22:58:05 [backends.py:609] Failed to read file <frozen os>
Throughput: 39.57 requests/s, 10761.78 total tokens/s, 10128.74 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768


─── STDERR ───
[2026-01-25 22:57:55] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:57:55] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-25 22:57:55] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-25 22:57:55] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:57:55] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:57:55] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:57:55] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:57:55] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:57:55] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-25 22:57:55] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:57:55] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:57:55] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:57:55] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:57:55] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 22:57:59] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:57:59] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-25 22:57:59] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-25 22:57:59] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:57:59] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:57:59] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:57:59] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:57:59] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:57:59] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-25 22:57:59] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:57:59] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:57:59] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:57:59] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:57:59] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=634739) [2026-01-25 22:57:59] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=634739) [2026-01-25 22:57:59] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=634739) [2026-01-25 22:57:59] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=634739) [2026-01-25 22:57:59] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=634739) [2026-01-25 22:57:59] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=634739) [2026-01-25 22:57:59] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=634739) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=634739) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.78it/s]
(EngineCore_DP0 pid=634739) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.09it/s]
(EngineCore_DP0 pid=634739) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.17it/s]
(EngineCore_DP0 pid=634739) 
(EngineCore_DP0 pid=634739) [2026-01-25 22:58:00] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 3584] -> 1D uint8
(EngineCore_DP0 pid=634739) [2026-01-25 22:58:00] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12386304 bytes
(EngineCore_DP0 pid=634739) [2026-01-25 22:58:00] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 3584] -> 1D uint8
(EngineCore_DP0 pid=634739) [2026-01-25 22:58:00] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9633792 bytes
(EngineCore_DP0 pid=634739) [2026-01-25 22:58:00] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 3584] -> 1D uint8
(EngineCore_DP0 pid=634739) [2026-01-25 22:58:00] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 101842944 bytes
(EngineCore_DP0 pid=634739) [2026-01-25 22:58:00] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 18944] -> 1D uint8
(EngineCore_DP0 pid=634739) [2026-01-25 22:58:00] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 50921472 bytes
(EngineCore_DP0 pid=634739) 2026-01-25 22:58:09,096 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=634739) 2026-01-25 22:58:09,110 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=634739) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|▎         | 1/35 [00:00<00:04,  7.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 2/35 [00:00<00:04,  7.36it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 5/35 [00:00<00:01, 15.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▌       | 9/35 [00:00<00:01, 21.37it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  34%|███▍      | 12/35 [00:00<00:00, 23.74it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 15/35 [00:00<00:00, 24.27it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████▏    | 18/35 [00:00<00:00, 25.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 21/35 [00:00<00:00, 26.10it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 24/35 [00:01<00:00, 27.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  77%|███████▋  | 27/35 [00:01<00:00, 27.70it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▊ | 31/35 [00:01<00:00, 28.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:01<00:00, 28.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:01<00:00, 24.50it/s]
(EngineCore_DP0 pid=634739) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   5%|▌         | 1/19 [00:00<00:02,  7.47it/s]
Capturing CUDA graphs (decode, FULL):  26%|██▋       | 5/19 [00:00<00:00, 20.77it/s]
Capturing CUDA graphs (decode, FULL):  47%|████▋     | 9/19 [00:00<00:00, 25.92it/s]
Capturing CUDA graphs (decode, FULL):  68%|██████▊   | 13/19 [00:00<00:00, 28.44it/s]
Capturing CUDA graphs (decode, FULL):  89%|████████▉ | 17/19 [00:00<00:00, 30.13it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:00<00:00, 27.46it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 7162.48it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:03<06:34,  3.10s/it, est. speed input: 5.16 toks/s, output: 82.49 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:03<00:00, 51.68it/s, est. speed input: 588.45 toks/s, output: 9415.08 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 51.68it/s, est. speed input: 636.73 toks/s, output: 10187.60 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 39.79it/s, est. speed input: 636.73 toks/s, output: 10187.60 toks/s]
[rank0]:[W125 22:58:15.313261963 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 25.3s

测试结果:
  Requests/s:   39.57
  Tokens/s:     10761.78
  Total Reqs:   128
  Elapsed:      3.24s

  [Decode 分析]
  Total Decode Tokens:  32768
  Decode Tokens/s:      10128.74

============================================================
[3/4] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 4096 (= 256 x 16)
│   M_decode      = 256
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 256
│   --max-num-seqs           = 256
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 22:58:20 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=635373) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=635373) WARNING 01-25 22:58:30 [backends.py:609] Failed to read file <frozen os>
Throughput: 44.70 requests/s, 12158.48 total tokens/s, 11443.28 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536


─── STDERR ───
[2026-01-25 22:58:20] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:58:20] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-25 22:58:20] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-25 22:58:20] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:58:20] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:58:20] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:58:20] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:58:20] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:58:20] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-25 22:58:20] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:58:20] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:58:20] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:58:20] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:58:20] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 22:58:24] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:58:24] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-25 22:58:24] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-25 22:58:24] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:58:24] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:58:24] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:58:24] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:58:24] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:58:24] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-25 22:58:24] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:58:24] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:58:24] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:58:24] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:58:24] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=635373) [2026-01-25 22:58:25] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=635373) [2026-01-25 22:58:25] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=635373) [2026-01-25 22:58:25] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=635373) [2026-01-25 22:58:25] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=635373) [2026-01-25 22:58:25] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=635373) [2026-01-25 22:58:25] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=635373) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=635373) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.91it/s]
(EngineCore_DP0 pid=635373) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.18it/s]
(EngineCore_DP0 pid=635373) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.26it/s]
(EngineCore_DP0 pid=635373) 
(EngineCore_DP0 pid=635373) [2026-01-25 22:58:26] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 3584] -> 1D uint8
(EngineCore_DP0 pid=635373) [2026-01-25 22:58:26] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12386304 bytes
(EngineCore_DP0 pid=635373) [2026-01-25 22:58:26] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 3584] -> 1D uint8
(EngineCore_DP0 pid=635373) [2026-01-25 22:58:26] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9633792 bytes
(EngineCore_DP0 pid=635373) [2026-01-25 22:58:26] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 3584] -> 1D uint8
(EngineCore_DP0 pid=635373) [2026-01-25 22:58:26] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 101842944 bytes
(EngineCore_DP0 pid=635373) [2026-01-25 22:58:26] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 18944] -> 1D uint8
(EngineCore_DP0 pid=635373) [2026-01-25 22:58:26] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 50921472 bytes
(EngineCore_DP0 pid=635373) 2026-01-25 22:58:34,440 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=635373) 2026-01-25 22:58:34,453 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=635373) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/36 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 3/36 [00:00<00:01, 24.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 6/36 [00:00<00:01, 26.65it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  28%|██▊       | 10/36 [00:00<00:00, 28.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|███▉      | 14/36 [00:00<00:00, 28.86it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 18/36 [00:00<00:00, 28.88it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 21/36 [00:00<00:00, 28.60it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 24/36 [00:00<00:00, 28.80it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 28/36 [00:00<00:00, 29.15it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 31/36 [00:01<00:00, 29.36it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 34/36 [00:01<00:00, 27.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:01<00:00, 27.92it/s]
(EngineCore_DP0 pid=635373) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   3%|▎         | 1/35 [00:00<00:05,  6.05it/s]
Capturing CUDA graphs (decode, FULL):  11%|█▏        | 4/35 [00:00<00:02, 13.07it/s]
Capturing CUDA graphs (decode, FULL):  17%|█▋        | 6/35 [00:00<00:01, 15.02it/s]
Capturing CUDA graphs (decode, FULL):  23%|██▎       | 8/35 [00:00<00:01, 15.00it/s]
Capturing CUDA graphs (decode, FULL):  34%|███▍      | 12/35 [00:00<00:01, 20.90it/s]
Capturing CUDA graphs (decode, FULL):  46%|████▌     | 16/35 [00:00<00:00, 24.60it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 20/35 [00:00<00:00, 26.79it/s]
Capturing CUDA graphs (decode, FULL):  69%|██████▊   | 24/35 [00:01<00:00, 28.25it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 28/35 [00:01<00:00, 29.58it/s]
Capturing CUDA graphs (decode, FULL):  91%|█████████▏| 32/35 [00:01<00:00, 30.70it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:01<00:00, 24.84it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 5664.33it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:05<21:41,  5.11s/it, est. speed input: 3.13 toks/s, output: 50.14 toks/s]
Processed prompts:  29%|██▉       | 75/256 [00:05<00:08, 20.25it/s, est. speed input: 229.88 toks/s, output: 3678.15 toks/s]
Processed prompts:  56%|█████▋    | 144/256 [00:05<00:02, 45.12it/s, est. speed input: 432.59 toks/s, output: 6921.37 toks/s]
Processed prompts:  79%|███████▉  | 202/256 [00:05<00:00, 71.81it/s, est. speed input: 594.72 toks/s, output: 9515.51 toks/s]
Processed prompts:  99%|█████████▉| 254/256 [00:05<00:00, 93.70it/s, est. speed input: 717.51 toks/s, output: 11480.14 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:05<00:00, 93.70it/s, est. speed input: 721.12 toks/s, output: 11537.95 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:05<00:00, 45.07it/s, est. speed input: 721.12 toks/s, output: 11537.95 toks/s]
[rank0]:[W125 22:58:43.727066600 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 28.4s

测试结果:
  Requests/s:   44.70
  Tokens/s:     12158.48
  Total Reqs:   256
  Elapsed:      5.73s

  [Decode 分析]
  Total Decode Tokens:  65536
  Decode Tokens/s:      11443.28

============================================================
[4/4] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 8192 (= 512 x 16)
│   M_decode      = 512
│   batched_tokens = 512 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 512
│   --max-num-seqs           = 512
│   --max-model-len          = 272
│   --max-num-batched-tokens = 512
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 22:58:49 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=636033) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=636033) WARNING 01-25 22:58:58 [backends.py:609] Failed to read file <frozen os>
Throughput: 35.33 requests/s, 9610.38 total tokens/s, 9045.07 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072


─── STDERR ───
[2026-01-25 22:58:48] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:58:49] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-25 22:58:49] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-25 22:58:49] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:58:49] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:58:49] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:58:49] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:58:49] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:58:49] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-25 22:58:49] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:58:49] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:58:49] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:58:49] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:58:49] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 22:58:52] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:58:52] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-25 22:58:52] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-25 22:58:52] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:58:52] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:58:52] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:58:52] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:58:52] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:58:52] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-25 22:58:52] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:58:52] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:58:52] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:58:52] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:58:52] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=636033) [2026-01-25 22:58:53] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=636033) [2026-01-25 22:58:53] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=636033) [2026-01-25 22:58:53] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=636033) [2026-01-25 22:58:53] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=636033) [2026-01-25 22:58:53] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=636033) [2026-01-25 22:58:53] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=636033) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=636033) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.85it/s]
(EngineCore_DP0 pid=636033) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.14it/s]
(EngineCore_DP0 pid=636033) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.22it/s]
(EngineCore_DP0 pid=636033) 
(EngineCore_DP0 pid=636033) [2026-01-25 22:58:54] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 3584] -> 1D uint8
(EngineCore_DP0 pid=636033) [2026-01-25 22:58:54] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12386304 bytes
(EngineCore_DP0 pid=636033) [2026-01-25 22:58:54] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 3584] -> 1D uint8
(EngineCore_DP0 pid=636033) [2026-01-25 22:58:54] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 9633792 bytes
(EngineCore_DP0 pid=636033) [2026-01-25 22:58:54] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 3584] -> 1D uint8
(EngineCore_DP0 pid=636033) [2026-01-25 22:58:54] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 101842944 bytes
(EngineCore_DP0 pid=636033) [2026-01-25 22:58:54] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 18944] -> 1D uint8
(EngineCore_DP0 pid=636033) [2026-01-25 22:58:54] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 50921472 bytes
(EngineCore_DP0 pid=636033) 2026-01-25 22:59:04,223 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=636033) 2026-01-25 22:59:04,238 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=636033) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 3/51 [00:00<00:01, 24.05it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|█▏        | 6/51 [00:00<00:01, 24.42it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 9/51 [00:00<00:01, 24.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|██▎       | 12/51 [00:00<00:01, 23.84it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▉       | 15/51 [00:00<00:01, 24.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  35%|███▌      | 18/51 [00:00<00:01, 25.24it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|████      | 21/51 [00:00<00:01, 25.93it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 24/51 [00:00<00:00, 27.06it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 27/51 [00:01<00:00, 27.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  59%|█████▉    | 30/51 [00:01<00:00, 28.06it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  65%|██████▍   | 33/51 [00:01<00:00, 27.87it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████   | 36/51 [00:01<00:00, 27.91it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|███████▋  | 39/51 [00:01<00:00, 27.79it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 42/51 [00:01<00:00, 25.67it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  88%|████████▊ | 45/51 [00:01<00:00, 19.14it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 48/51 [00:02<00:00, 17.66it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:02<00:00, 19.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:02<00:00, 23.46it/s]
(EngineCore_DP0 pid=636033) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   2%|▏         | 1/51 [00:00<00:07,  7.07it/s]
Capturing CUDA graphs (decode, FULL):   8%|▊         | 4/51 [00:00<00:02, 17.45it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▎        | 7/51 [00:00<00:02, 21.81it/s]
Capturing CUDA graphs (decode, FULL):  20%|█▉        | 10/51 [00:00<00:01, 23.50it/s]
Capturing CUDA graphs (decode, FULL):  27%|██▋       | 14/51 [00:00<00:01, 26.72it/s]
Capturing CUDA graphs (decode, FULL):  35%|███▌      | 18/51 [00:00<00:01, 28.70it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 22/51 [00:00<00:00, 29.76it/s]
Capturing CUDA graphs (decode, FULL):  51%|█████     | 26/51 [00:00<00:00, 30.57it/s]
Capturing CUDA graphs (decode, FULL):  59%|█████▉    | 30/51 [00:01<00:00, 31.06it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 34/51 [00:01<00:00, 31.44it/s]
Capturing CUDA graphs (decode, FULL):  75%|███████▍  | 38/51 [00:01<00:00, 31.60it/s]
Capturing CUDA graphs (decode, FULL):  82%|████████▏ | 42/51 [00:01<00:00, 31.80it/s]
Capturing CUDA graphs (decode, FULL):  90%|█████████ | 46/51 [00:01<00:00, 31.23it/s]
Capturing CUDA graphs (decode, FULL):  98%|█████████▊| 50/51 [00:01<00:00, 31.65it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:01<00:00, 29.00it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 8452.86it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:07<1:03:44,  7.48s/it, est. speed input: 2.14 toks/s, output: 34.21 toks/s]
Processed prompts:  12%|█▏        | 63/512 [00:07<00:38, 11.69it/s, est. speed input: 132.48 toks/s, output: 2119.71 toks/s]
Processed prompts:  36%|███▌      | 185/512 [00:07<00:07, 42.95it/s, est. speed input: 383.57 toks/s, output: 6137.18 toks/s]
Processed prompts:  49%|████▉     | 253/512 [00:10<00:07, 35.19it/s, est. speed input: 395.09 toks/s, output: 6321.38 toks/s]
Processed prompts:  58%|█████▊    | 295/512 [00:11<00:06, 33.37it/s, est. speed input: 403.28 toks/s, output: 6452.43 toks/s]
Processed prompts:  63%|██████▎   | 323/512 [00:12<00:05, 35.83it/s, est. speed input: 422.24 toks/s, output: 6755.85 toks/s]
Processed prompts:  67%|██████▋   | 344/512 [00:12<00:04, 38.30it/s, est. speed input: 436.78 toks/s, output: 6988.52 toks/s]
Processed prompts:  70%|███████   | 360/512 [00:12<00:03, 41.37it/s, est. speed input: 449.31 toks/s, output: 7188.88 toks/s]
Processed prompts:  73%|███████▎  | 373/512 [00:12<00:03, 45.30it/s, est. speed input: 460.66 toks/s, output: 7370.60 toks/s]
Processed prompts:  75%|███████▌  | 385/512 [00:13<00:02, 48.23it/s, est. speed input: 469.37 toks/s, output: 7509.84 toks/s]
Processed prompts:  78%|███████▊  | 397/512 [00:13<00:02, 54.11it/s, est. speed input: 480.01 toks/s, output: 7680.13 toks/s]
Processed prompts:  80%|███████▉  | 409/512 [00:13<00:01, 61.06it/s, est. speed input: 490.62 toks/s, output: 7849.93 toks/s]
Processed prompts:  83%|████████▎ | 423/512 [00:13<00:01, 70.66it/s, est. speed input: 503.20 toks/s, output: 8051.24 toks/s]
Processed prompts:  85%|████████▌ | 436/512 [00:13<00:00, 78.11it/s, est. speed input: 514.21 toks/s, output: 8227.39 toks/s]
Processed prompts:  88%|████████▊ | 448/512 [00:13<00:00, 85.55it/s, est. speed input: 524.45 toks/s, output: 8391.16 toks/s]
Processed prompts:  90%|████████▉ | 460/512 [00:13<00:00, 91.44it/s, est. speed input: 534.33 toks/s, output: 8549.34 toks/s]
Processed prompts:  92%|█████████▏| 472/512 [00:13<00:00, 97.43it/s, est. speed input: 544.24 toks/s, output: 8707.84 toks/s]
Processed prompts:  95%|█████████▍| 485/512 [00:13<00:00, 103.56it/s, est. speed input: 554.95 toks/s, output: 8879.16 toks/s]
Processed prompts:  97%|█████████▋| 497/512 [00:14<00:00, 96.64it/s, est. speed input: 562.84 toks/s, output: 9005.37 toks/s] 
Processed prompts:  99%|█████████▉| 508/512 [00:14<00:00, 67.77it/s, est. speed input: 563.60 toks/s, output: 9017.65 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:14<00:00, 67.77it/s, est. speed input: 567.72 toks/s, output: 9083.59 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:14<00:00, 35.48it/s, est. speed input: 567.72 toks/s, output: 9083.59 toks/s]
[rank0]:[W125 22:59:23.717922743 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 40.0s

测试结果:
  Requests/s:   35.33
  Tokens/s:     9610.38
  Total Reqs:   512
  Elapsed:      14.49s

  [Decode 分析]
  Total Decode Tokens:  131072
  Decode Tokens/s:      9045.07


------------------------------------------------------------
  生成 CSV: Qwen2.5-7B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_4/Qwen2.5-7B-INT8_decode.csv

预览:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
64,16,64,64,256,256,22.8334,6210.6722,2.8029
128,16,128,128,256,256,39.5654,10761.7837,3.2352
256,16,256,256,256,256,44.7003,12158.4798,5.7270
512,16,512,512,256,256,35.3323,9610.3827,14.4910

------------------------------------------------------------

[INFO] 完成: 4 成功, 0 失败

============================================================
  Qwen2.5-7B-INT8 | cuSPARSELt (2_6) | decode
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_6
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_6

============================================================
[1/4] 测试 M=64
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 64
│   M_prefill     = 1024 (= 64 x 16)
│   M_decode      = 64
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 64
│   --max-num-seqs           = 64
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 22:59:28 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=636898) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=636898) WARNING 01-25 22:59:41 [backends.py:609] Failed to read file <frozen os>
Throughput: 20.47 requests/s, 5567.31 total tokens/s, 5239.82 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384


─── STDERR ───
[2026-01-25 22:59:28] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:59:28] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-25 22:59:28] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-25 22:59:28] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:59:28] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:59:28] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:59:28] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:59:28] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:59:28] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-25 22:59:28] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:59:28] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:59:28] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:59:28] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:59:28] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 22:59:32] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:59:32] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-25 22:59:32] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-25 22:59:32] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:59:32] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:59:32] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:59:32] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:59:32] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:59:32] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-25 22:59:32] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:59:32] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:59:32] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:59:32] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:59:32] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=636898) [2026-01-25 22:59:33] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=636898) [2026-01-25 22:59:33] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=636898) [2026-01-25 22:59:33] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=636898) [2026-01-25 22:59:33] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=636898) [2026-01-25 22:59:33] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=636898) [2026-01-25 22:59:33] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=636898) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=636898) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.09s/it]
(EngineCore_DP0 pid=636898) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:03<00:00,  1.76s/it]
(EngineCore_DP0 pid=636898) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:03<00:00,  1.66s/it]
(EngineCore_DP0 pid=636898) 
(EngineCore_DP0 pid=636898) [2026-01-25 22:59:37] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 4800] -> 1D uint8
(EngineCore_DP0 pid=636898) [2026-01-25 22:59:37] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16662528 bytes
(EngineCore_DP0 pid=636898) [2026-01-25 22:59:37] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 4800] -> 1D uint8
(EngineCore_DP0 pid=636898) [2026-01-25 22:59:37] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12959744 bytes
(EngineCore_DP0 pid=636898) [2026-01-25 22:59:37] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 4800] -> 1D uint8
(EngineCore_DP0 pid=636898) [2026-01-25 22:59:37] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 137003008 bytes
(EngineCore_DP0 pid=636898) [2026-01-25 22:59:37] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 25280] -> 1D uint8
(EngineCore_DP0 pid=636898) [2026-01-25 22:59:37] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 68009984 bytes
(EngineCore_DP0 pid=636898) 2026-01-25 22:59:47,374 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=636898) 2026-01-25 22:59:47,394 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=636898) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 3/19 [00:00<00:00, 23.94it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  32%|███▏      | 6/19 [00:00<00:00, 26.88it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 9/19 [00:00<00:00, 26.63it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 12/19 [00:00<00:00, 24.26it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  79%|███████▉  | 15/19 [00:00<00:00, 20.60it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  95%|█████████▍| 18/19 [00:00<00:00, 17.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:00<00:00, 20.09it/s]
(EngineCore_DP0 pid=636898) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   9%|▉         | 1/11 [00:00<00:01,  7.49it/s]
Capturing CUDA graphs (decode, FULL):  45%|████▌     | 5/11 [00:00<00:00, 21.68it/s]
Capturing CUDA graphs (decode, FULL):  82%|████████▏ | 9/11 [00:00<00:00, 26.49it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:00<00:00, 25.01it/s]

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 64/64 [00:00<00:00, 5496.67it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 1/64 [00:03<03:12,  3.06s/it, est. speed input: 5.23 toks/s, output: 83.65 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:03<00:00,  3.06s/it, est. speed input: 328.79 toks/s, output: 5260.66 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:03<00:00, 20.55it/s, est. speed input: 328.79 toks/s, output: 5260.66 toks/s]
[rank0]:[W125 22:59:53.829773386 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 29.1s

测试结果:
  Requests/s:   20.47
  Tokens/s:     5567.31
  Total Reqs:   64
  Elapsed:      3.13s

  [Decode 分析]
  Total Decode Tokens:  16384
  Decode Tokens/s:      5239.82

============================================================
[2/4] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 2048 (= 128 x 16)
│   M_decode      = 128
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 128
│   --max-num-seqs           = 128
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 22:59:58 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=637583) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=637583) WARNING 01-25 23:00:08 [backends.py:609] Failed to read file <frozen os>
Throughput: 34.19 requests/s, 9299.42 total tokens/s, 8752.40 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768


─── STDERR ───
[2026-01-25 22:59:58] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 22:59:58] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-25 22:59:58] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-25 22:59:58] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:59:58] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:59:58] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:59:58] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:59:58] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 22:59:58] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-25 22:59:58] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 22:59:58] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 22:59:58] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 22:59:58] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 22:59:58] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 23:00:01] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 23:00:02] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-25 23:00:02] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-25 23:00:02] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:00:02] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:00:02] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:00:02] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:00:02] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:00:02] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-25 23:00:02] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 23:00:02] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 23:00:02] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 23:00:02] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 23:00:02] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=637583) [2026-01-25 23:00:02] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=637583) [2026-01-25 23:00:02] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=637583) [2026-01-25 23:00:02] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=637583) [2026-01-25 23:00:02] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=637583) [2026-01-25 23:00:02] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=637583) [2026-01-25 23:00:02] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=637583) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=637583) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.48it/s]
(EngineCore_DP0 pid=637583) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.80it/s]
(EngineCore_DP0 pid=637583) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.87it/s]
(EngineCore_DP0 pid=637583) 
(EngineCore_DP0 pid=637583) [2026-01-25 23:00:03] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 4800] -> 1D uint8
(EngineCore_DP0 pid=637583) [2026-01-25 23:00:03] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16662528 bytes
(EngineCore_DP0 pid=637583) [2026-01-25 23:00:03] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 4800] -> 1D uint8
(EngineCore_DP0 pid=637583) [2026-01-25 23:00:03] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12959744 bytes
(EngineCore_DP0 pid=637583) [2026-01-25 23:00:03] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 4800] -> 1D uint8
(EngineCore_DP0 pid=637583) [2026-01-25 23:00:03] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 137003008 bytes
(EngineCore_DP0 pid=637583) [2026-01-25 23:00:03] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 25280] -> 1D uint8
(EngineCore_DP0 pid=637583) [2026-01-25 23:00:03] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 68009984 bytes
(EngineCore_DP0 pid=637583) 2026-01-25 23:00:12,213 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=637583) 2026-01-25 23:00:12,226 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=637583) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|▎         | 1/35 [00:00<00:03,  9.88it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█▏        | 4/35 [00:00<00:01, 20.91it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 7/35 [00:00<00:01, 24.60it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 11/35 [00:00<00:00, 27.07it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 15/35 [00:00<00:00, 28.16it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████▏    | 18/35 [00:00<00:00, 28.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 21/35 [00:00<00:00, 27.17it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 25/35 [00:00<00:00, 28.14it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 28/35 [00:01<00:00, 28.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▊ | 31/35 [00:01<00:00, 28.79it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:01<00:00, 28.82it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:01<00:00, 27.39it/s]
(EngineCore_DP0 pid=637583) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   5%|▌         | 1/19 [00:00<00:02,  7.52it/s]
Capturing CUDA graphs (decode, FULL):  26%|██▋       | 5/19 [00:00<00:00, 21.86it/s]
Capturing CUDA graphs (decode, FULL):  47%|████▋     | 9/19 [00:00<00:00, 26.81it/s]
Capturing CUDA graphs (decode, FULL):  68%|██████▊   | 13/19 [00:00<00:00, 29.17it/s]
Capturing CUDA graphs (decode, FULL):  89%|████████▉ | 17/19 [00:00<00:00, 29.93it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:00<00:00, 27.69it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 5930.64it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:03<07:35,  3.59s/it, est. speed input: 4.46 toks/s, output: 71.34 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:03<00:00, 37.37it/s, est. speed input: 424.79 toks/s, output: 6796.65 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 37.37it/s, est. speed input: 550.32 toks/s, output: 8805.14 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 34.39it/s, est. speed input: 550.32 toks/s, output: 8805.14 toks/s]
[rank0]:[W125 23:00:18.775821183 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 26.0s

测试结果:
  Requests/s:   34.19
  Tokens/s:     9299.42
  Total Reqs:   128
  Elapsed:      3.74s

  [Decode 分析]
  Total Decode Tokens:  32768
  Decode Tokens/s:      8752.40

============================================================
[3/4] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 4096 (= 256 x 16)
│   M_decode      = 256
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 256
│   --max-num-seqs           = 256
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 23:00:24 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=638211) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=638211) WARNING 01-25 23:00:34 [backends.py:609] Failed to read file <frozen os>
Throughput: 30.95 requests/s, 8417.30 total tokens/s, 7922.16 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536


─── STDERR ───
[2026-01-25 23:00:24] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 23:00:24] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-25 23:00:24] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-25 23:00:24] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:00:24] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:00:24] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:00:24] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:00:24] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:00:24] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-25 23:00:24] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 23:00:24] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 23:00:24] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 23:00:24] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 23:00:24] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 23:00:27] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 23:00:27] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-25 23:00:27] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-25 23:00:27] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:00:27] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:00:27] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:00:27] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:00:27] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:00:27] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-25 23:00:27] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 23:00:27] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 23:00:27] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 23:00:27] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 23:00:27] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=638211) [2026-01-25 23:00:28] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=638211) [2026-01-25 23:00:28] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=638211) [2026-01-25 23:00:28] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=638211) [2026-01-25 23:00:28] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=638211) [2026-01-25 23:00:28] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=638211) [2026-01-25 23:00:28] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=638211) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=638211) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.36it/s]
(EngineCore_DP0 pid=638211) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.74it/s]
(EngineCore_DP0 pid=638211) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.81it/s]
(EngineCore_DP0 pid=638211) 
(EngineCore_DP0 pid=638211) [2026-01-25 23:00:29] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 4800] -> 1D uint8
(EngineCore_DP0 pid=638211) [2026-01-25 23:00:29] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16662528 bytes
(EngineCore_DP0 pid=638211) [2026-01-25 23:00:29] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 4800] -> 1D uint8
(EngineCore_DP0 pid=638211) [2026-01-25 23:00:29] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12959744 bytes
(EngineCore_DP0 pid=638211) [2026-01-25 23:00:29] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 4800] -> 1D uint8
(EngineCore_DP0 pid=638211) [2026-01-25 23:00:29] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 137003008 bytes
(EngineCore_DP0 pid=638211) [2026-01-25 23:00:29] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 25280] -> 1D uint8
(EngineCore_DP0 pid=638211) [2026-01-25 23:00:29] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 68009984 bytes
(EngineCore_DP0 pid=638211) 2026-01-25 23:00:38,311 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=638211) 2026-01-25 23:00:38,325 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=638211) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/36 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 3/36 [00:00<00:01, 28.05it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 6/36 [00:00<00:01, 27.97it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  28%|██▊       | 10/36 [00:00<00:00, 28.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▌      | 13/36 [00:00<00:00, 28.93it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  44%|████▍     | 16/36 [00:00<00:00, 21.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 19/36 [00:00<00:01, 16.66it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 21/36 [00:01<00:00, 17.01it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 24/36 [00:01<00:00, 19.66it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 28/36 [00:01<00:00, 22.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 31/36 [00:01<00:00, 24.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  97%|█████████▋| 35/36 [00:01<00:00, 26.35it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:01<00:00, 23.16it/s]
(EngineCore_DP0 pid=638211) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   3%|▎         | 1/35 [00:00<00:04,  6.92it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 5/35 [00:00<00:01, 20.74it/s]
Capturing CUDA graphs (decode, FULL):  26%|██▌       | 9/35 [00:00<00:01, 25.64it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 13/35 [00:00<00:00, 28.36it/s]
Capturing CUDA graphs (decode, FULL):  49%|████▊     | 17/35 [00:00<00:00, 29.74it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 21/35 [00:00<00:00, 30.63it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 25/35 [00:00<00:00, 31.19it/s]
Capturing CUDA graphs (decode, FULL):  83%|████████▎ | 29/35 [00:01<00:00, 31.15it/s]
Capturing CUDA graphs (decode, FULL):  94%|█████████▍| 33/35 [00:01<00:00, 31.68it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:01<00:00, 28.62it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  38%|███▊      | 98/256 [00:00<00:00, 645.88it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 1497.44it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:05<23:18,  5.48s/it, est. speed input: 2.92 toks/s, output: 46.69 toks/s]
Processed prompts:  29%|██▉       | 75/256 [00:05<00:09, 18.94it/s, est. speed input: 214.70 toks/s, output: 3435.19 toks/s]
Processed prompts:  62%|██████▏   | 159/256 [00:05<00:02, 47.49it/s, est. speed input: 446.46 toks/s, output: 7143.30 toks/s]
Processed prompts:  85%|████████▍ | 217/256 [00:07<00:00, 44.14it/s, est. speed input: 484.61 toks/s, output: 7753.80 toks/s]
Processed prompts:  99%|█████████▉| 254/256 [00:08<00:00, 43.46it/s, est. speed input: 504.60 toks/s, output: 8073.56 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:08<00:00, 43.46it/s, est. speed input: 505.63 toks/s, output: 8090.03 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:08<00:00, 31.60it/s, est. speed input: 505.63 toks/s, output: 8090.03 toks/s]
[rank0]:[W125 23:00:50.232060488 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 31.4s

测试结果:
  Requests/s:   30.95
  Tokens/s:     8417.30
  Total Reqs:   256
  Elapsed:      8.27s

  [Decode 分析]
  Total Decode Tokens:  65536
  Decode Tokens/s:      7922.16

============================================================
[4/4] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 8192 (= 512 x 16)
│   M_decode      = 512
│   batched_tokens = 512 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 512
│   --max-num-seqs           = 512
│   --max-model-len          = 272
│   --max-num-batched-tokens = 512
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 23:00:55 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=638912) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=638912) WARNING 01-25 23:01:05 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=638912) ERROR 01-25 23:01:15 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=638912) ERROR 01-25 23:01:15 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=638912) ERROR 01-25 23:01:15 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4302, in _dummy_sampler_run
(EngineCore_DP0 pid=638912) ERROR 01-25 23:01:15 [core.py:866]     sampler_output = self.sampler(
(EngineCore_DP0 pid=638912) ERROR 01-25 23:01:15 [core.py:866]                      ^^^^^^^^^^^^^
(EngineCore_DP0 pid=638912) ERROR 01-25 23:01:15 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=638912) ERROR 01-25 23:01:15 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=638912) ERROR 01-25 23:01:15 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=638912) ERROR 01-25 23:01:15 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=638912) ERROR 01-25 23:01:15 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=638912) ERROR 01-25 23:01:15 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=638912) ERROR 01-25 23:01:15 [core.py:866]   File "/root/vllmbench/vllm/v1/sample/sampler.py", line 96, in forward
(EngineCore_DP0 pid=638912) ERROR 01-25 23:01:15 [core.py:866]     sampled, processed_logprobs = self.sample(logits, sampling_metadata)
(EngineCore_DP0 pid=638912) ERROR 01-25 23:01:15 [core.py:866]                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=638912) ERROR 01-25 23:01:15 [core.py:866]   File "/root/vllmbench/vllm/v1/sample/sampler.py", line 187, in sample
(EngineCore_DP0 pid=638912) ERROR 01-25 23:01:15 [core.py:866]     random_sampled, processed_logprobs = self.topk_topp_sampler(
(EngineCore_DP0 pid=638912) ERROR 01-25 23:01:15 [core.py:866]                                          ^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=638912) ERROR 01-25 23:01:15 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=638912) ERROR 01-25 23:01:15 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=638912) ERROR 01-25 23:01:15 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=638912) ERROR 01-25 23:01:15 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=638912) ERROR 01-25 23:01:15 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=638912) ERROR 01-25 23:01:15 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=638912) ERROR 01-25 23:01:15 [core.py:866]   File "/root/vllmbench/vllm/v1/sample/ops/topk_topp_sampler.py", line 104, in forward_native
(EngineCore_DP0 pid=638912) ERROR 01-25 23:01:15 [core.py:866]     logits = self.apply_top_k_top_p(logits, k, p)
(EngineCore_DP0 pid=638912) ERROR 01-25 23:01:15 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=638912) ERROR 01-25 23:01:15 [core.py:866]   File "/root/vllmbench/vllm/v1/sample/ops/topk_topp_sampler.py", line 258, in apply_top_k_top_p
(EngineCore_DP0 pid=638912) ERROR 01-25 23:01:15 [core.py:866]     logits_sort, logits_idx = logits.sort(dim=-1, descending=False)
(EngineCore_DP0 pid=638912) ERROR 01-25 23:01:15 [core.py:866]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=638912) ERROR 01-25 23:01:15 [core.py:866] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 892.00 MiB. GPU 0 has a total capacity of 15.46 GiB of which 848.94 MiB is free. Including non-PyTorch memory, this process has 14.41 GiB memory in use. Of the allocated memory 11.59 GiB is allocated by PyTorch, with 142.00 MiB allocated in private pools (e.g., CUDA Graphs), and 2.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
(EngineCore_DP0 pid=638912) ERROR 01-25 23:01:15 [core.py:866] 
(EngineCore_DP0 pid=638912) ERROR 01-25 23:01:15 [core.py:866] The above exception was the direct cause of the following exception:
(EngineCore_DP0 pid=638912) ERROR 01-25 23:01:15 [core.py:866] 
(EngineCore_DP0 pid=638912) ERROR 01-25 23:01:15 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=638912) ERROR 01-25 23:01:15 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=638912) ERROR 01-25 23:01:15 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=638912) ERROR 01-25 23:01:15 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=638912) ERROR 01-25 23:01:15 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=638912) ERROR 01-25 23:01:15 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=638912) ERROR 01-25 23:01:15 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=638912) ERROR 01-25 23:01:15 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=638912) ERROR 01-25 23:01:15 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=638912) ERROR 01-25 23:01:15 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 256, in _initialize_kv_caches
(EngineCore_DP0 pid=638912) ERROR 01-25 23:01:15 [core.py:866]     self.model_executor.initialize_from_config(kv_cache_configs)
(EngineCore_DP0 pid=638912) ERROR 01-25 23:01:15 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 116, in initialize_from_config
(EngineCore_DP0 pid=638912) ERROR 01-25 23:01:15 [core.py:866]     self.collective_rpc("compile_or_warm_up_model")
(EngineCore_DP0 pid=638912) ERROR 01-25 23:01:15 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=638912) ERROR 01-25 23:01:15 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=638912) ERROR 01-25 23:01:15 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=638912) ERROR 01-25 23:01:15 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=638912) ERROR 01-25 23:01:15 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=638912) ERROR 01-25 23:01:15 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=638912) ERROR 01-25 23:01:15 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 538, in compile_or_warm_up_model
(EngineCore_DP0 pid=638912) ERROR 01-25 23:01:15 [core.py:866]     self.model_runner._dummy_sampler_run(hidden_states=last_hidden_states)
(EngineCore_DP0 pid=638912) ERROR 01-25 23:01:15 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=638912) ERROR 01-25 23:01:15 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=638912) ERROR 01-25 23:01:15 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=638912) ERROR 01-25 23:01:15 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4307, in _dummy_sampler_run
(EngineCore_DP0 pid=638912) ERROR 01-25 23:01:15 [core.py:866]     raise RuntimeError(
(EngineCore_DP0 pid=638912) ERROR 01-25 23:01:15 [core.py:866] RuntimeError: CUDA out of memory occurred when warming up sampler with 512 dummy requests. Please try lowering `max_num_seqs` or `gpu_memory_utilization` when initializing the engine.


─── STDERR ───
[2026-01-25 23:00:55] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 23:00:55] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-25 23:00:55] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-25 23:00:55] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:00:55] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:00:55] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:00:55] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:00:55] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:00:55] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-25 23:00:55] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 23:00:55] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 23:00:55] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 23:00:55] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 23:00:55] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 23:00:59] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 23:00:59] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-25 23:00:59] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-25 23:00:59] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:00:59] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:00:59] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:00:59] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:00:59] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:00:59] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-25 23:00:59] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 23:00:59] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 23:00:59] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 23:00:59] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 23:00:59] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=638912) [2026-01-25 23:00:59] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=638912) [2026-01-25 23:00:59] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=638912) [2026-01-25 23:00:59] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=638912) [2026-01-25 23:00:59] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=638912) [2026-01-25 23:00:59] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=638912) [2026-01-25 23:00:59] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=638912) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=638912) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.41it/s]
(EngineCore_DP0 pid=638912) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.75it/s]
(EngineCore_DP0 pid=638912) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.83it/s]
(EngineCore_DP0 pid=638912) 
(EngineCore_DP0 pid=638912) [2026-01-25 23:01:01] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 4800] -> 1D uint8
(EngineCore_DP0 pid=638912) [2026-01-25 23:01:01] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16662528 bytes
(EngineCore_DP0 pid=638912) [2026-01-25 23:01:01] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 4800] -> 1D uint8
(EngineCore_DP0 pid=638912) [2026-01-25 23:01:01] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12959744 bytes
(EngineCore_DP0 pid=638912) [2026-01-25 23:01:01] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 4800] -> 1D uint8
(EngineCore_DP0 pid=638912) [2026-01-25 23:01:01] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 137003008 bytes
(EngineCore_DP0 pid=638912) [2026-01-25 23:01:01] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 25280] -> 1D uint8
(EngineCore_DP0 pid=638912) [2026-01-25 23:01:01] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 68009984 bytes
(EngineCore_DP0 pid=638912) 2026-01-25 23:01:11,115 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=638912) 2026-01-25 23:01:11,129 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=638912) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 3/51 [00:00<00:02, 21.99it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  12%|█▏        | 6/51 [00:00<00:02, 21.08it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 9/51 [00:00<00:01, 21.86it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  24%|██▎       | 12/51 [00:00<00:01, 23.19it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▉       | 15/51 [00:00<00:01, 24.14it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  35%|███▌      | 18/51 [00:00<00:01, 19.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  41%|████      | 21/51 [00:01<00:01, 16.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 24/51 [00:01<00:01, 19.07it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 27/51 [00:01<00:01, 21.10it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  59%|█████▉    | 30/51 [00:01<00:00, 22.59it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  65%|██████▍   | 33/51 [00:01<00:00, 24.00it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████   | 36/51 [00:01<00:00, 25.15it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|███████▋  | 39/51 [00:01<00:00, 26.12it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|████████▍ | 43/51 [00:01<00:00, 27.24it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 47/51 [00:02<00:00, 27.92it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:02<00:00, 27.99it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:02<00:00, 23.68it/s]
(EngineCore_DP0 pid=638912) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   2%|▏         | 1/51 [00:00<00:07,  6.92it/s]
Capturing CUDA graphs (decode, FULL):   8%|▊         | 4/51 [00:00<00:03, 15.50it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▎        | 7/51 [00:00<00:02, 19.44it/s]
Capturing CUDA graphs (decode, FULL):  20%|█▉        | 10/51 [00:00<00:01, 22.37it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 13/51 [00:00<00:01, 24.61it/s]
Capturing CUDA graphs (decode, FULL):  33%|███▎      | 17/51 [00:00<00:01, 26.76it/s]
Capturing CUDA graphs (decode, FULL):  41%|████      | 21/51 [00:00<00:01, 28.60it/s]
Capturing CUDA graphs (decode, FULL):  49%|████▉     | 25/51 [00:00<00:00, 29.95it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 29/51 [00:01<00:00, 30.69it/s]
Capturing CUDA graphs (decode, FULL):  65%|██████▍   | 33/51 [00:01<00:00, 31.16it/s]
Capturing CUDA graphs (decode, FULL):  73%|███████▎  | 37/51 [00:01<00:00, 29.50it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 41/51 [00:01<00:00, 30.46it/s]
Capturing CUDA graphs (decode, FULL):  88%|████████▊ | 45/51 [00:01<00:00, 26.19it/s]
Capturing CUDA graphs (decode, FULL):  94%|█████████▍| 48/51 [00:01<00:00, 24.74it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:02<00:00, 23.07it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:02<00:00, 25.39it/s]
(EngineCore_DP0 pid=638912) Process EngineCore_DP0:
(EngineCore_DP0 pid=638912) Traceback (most recent call last):
(EngineCore_DP0 pid=638912)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4302, in _dummy_sampler_run
(EngineCore_DP0 pid=638912)     sampler_output = self.sampler(
(EngineCore_DP0 pid=638912)                      ^^^^^^^^^^^^^
(EngineCore_DP0 pid=638912)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=638912)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=638912)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=638912)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=638912)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=638912)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=638912)   File "/root/vllmbench/vllm/v1/sample/sampler.py", line 96, in forward
(EngineCore_DP0 pid=638912)     sampled, processed_logprobs = self.sample(logits, sampling_metadata)
(EngineCore_DP0 pid=638912)                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=638912)   File "/root/vllmbench/vllm/v1/sample/sampler.py", line 187, in sample
(EngineCore_DP0 pid=638912)     random_sampled, processed_logprobs = self.topk_topp_sampler(
(EngineCore_DP0 pid=638912)                                          ^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=638912)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=638912)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=638912)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=638912)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=638912)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=638912)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=638912)   File "/root/vllmbench/vllm/v1/sample/ops/topk_topp_sampler.py", line 104, in forward_native
(EngineCore_DP0 pid=638912)     logits = self.apply_top_k_top_p(logits, k, p)
(EngineCore_DP0 pid=638912)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=638912)   File "/root/vllmbench/vllm/v1/sample/ops/topk_topp_sampler.py", line 258, in apply_top_k_top_p
(EngineCore_DP0 pid=638912)     logits_sort, logits_idx = logits.sort(dim=-1, descending=False)
(EngineCore_DP0 pid=638912)                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=638912) torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 892.00 MiB. GPU 0 has a total capacity of 15.46 GiB of which 848.94 MiB is free. Including non-PyTorch memory, this process has 14.41 GiB memory in use. Of the allocated memory 11.59 GiB is allocated by PyTorch, with 142.00 MiB allocated in private pools (e.g., CUDA Graphs), and 2.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
(EngineCore_DP0 pid=638912) 
(EngineCore_DP0 pid=638912) The above exception was the direct cause of the following exception:
(EngineCore_DP0 pid=638912) 
(EngineCore_DP0 pid=638912) Traceback (most recent call last):
(EngineCore_DP0 pid=638912)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=638912)     self.run()
(EngineCore_DP0 pid=638912)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=638912)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=638912)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=638912)     raise e
(EngineCore_DP0 pid=638912)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=638912)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=638912)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=638912)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=638912)     super().__init__(
(EngineCore_DP0 pid=638912)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=638912)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=638912)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=638912)   File "/root/vllmbench/vllm/v1/engine/core.py", line 256, in _initialize_kv_caches
(EngineCore_DP0 pid=638912)     self.model_executor.initialize_from_config(kv_cache_configs)
(EngineCore_DP0 pid=638912)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 116, in initialize_from_config
(EngineCore_DP0 pid=638912)     self.collective_rpc("compile_or_warm_up_model")
(EngineCore_DP0 pid=638912)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=638912)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=638912)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=638912)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=638912)     return func(*args, **kwargs)
(EngineCore_DP0 pid=638912)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=638912)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 538, in compile_or_warm_up_model
(EngineCore_DP0 pid=638912)     self.model_runner._dummy_sampler_run(hidden_states=last_hidden_states)
(EngineCore_DP0 pid=638912)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=638912)     return func(*args, **kwargs)
(EngineCore_DP0 pid=638912)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=638912)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4307, in _dummy_sampler_run
(EngineCore_DP0 pid=638912)     raise RuntimeError(
(EngineCore_DP0 pid=638912) RuntimeError: CUDA out of memory occurred when warming up sampler with 512 dummy requests. Please try lowering `max_num_seqs` or `gpu_memory_utilization` when initializing the engine.
[rank0]:[W125 23:01:16.097661992 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[ERROR] 测试失败: M=512 (exit code: 1)


------------------------------------------------------------
  生成 CSV: Qwen2.5-7B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_6/Qwen2.5-7B-INT8_decode.csv

预览:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
64,16,64,64,256,256,20.4680,5567.3073,3.1268
128,16,128,128,256,256,34.1891,9299.4244,3.7439
256,16,256,256,256,256,30.9459,8417.2964,8.2725
512,16,512,512,256,256,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[INFO] 完成: 3 成功, 1 失败

============================================================
  Qwen2.5-7B-INT8 | cuSPARSELt (2_8) | decode
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_8
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_8

============================================================
[1/4] 测试 M=64
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 64
│   M_prefill     = 1024 (= 64 x 16)
│   M_decode      = 64
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 64
│   --max-num-seqs           = 64
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 23:01:21 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=639543) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=639543) WARNING 01-25 23:01:34 [backends.py:609] Failed to read file <frozen os>
Throughput: 19.29 requests/s, 5245.85 total tokens/s, 4937.27 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384


─── STDERR ───
[2026-01-25 23:01:21] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 23:01:21] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-25 23:01:21] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-25 23:01:21] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:01:21] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:01:21] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:01:21] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:01:21] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:01:21] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-25 23:01:21] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 23:01:21] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 23:01:21] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 23:01:21] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 23:01:21] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 23:01:25] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 23:01:25] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-25 23:01:25] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-25 23:01:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:01:25] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:01:25] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:01:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:01:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:01:25] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-25 23:01:25] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 23:01:25] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 23:01:25] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 23:01:25] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 23:01:25] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=639543) [2026-01-25 23:01:26] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=639543) [2026-01-25 23:01:26] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=639543) [2026-01-25 23:01:26] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=639543) [2026-01-25 23:01:26] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=639543) [2026-01-25 23:01:26] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=639543) [2026-01-25 23:01:26] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=639543) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=639543) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.33s/it]
(EngineCore_DP0 pid=639543) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:03<00:00,  1.88s/it]
(EngineCore_DP0 pid=639543) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:03<00:00,  1.80s/it]
(EngineCore_DP0 pid=639543) 
(EngineCore_DP0 pid=639543) [2026-01-25 23:01:30] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5376] -> 1D uint8
(EngineCore_DP0 pid=639543) [2026-01-25 23:01:30] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 18579456 bytes
(EngineCore_DP0 pid=639543) [2026-01-25 23:01:30] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5376] -> 1D uint8
(EngineCore_DP0 pid=639543) [2026-01-25 23:01:30] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 14450688 bytes
(EngineCore_DP0 pid=639543) [2026-01-25 23:01:30] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5376] -> 1D uint8
(EngineCore_DP0 pid=639543) [2026-01-25 23:01:30] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 152764416 bytes
(EngineCore_DP0 pid=639543) [2026-01-25 23:01:30] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 28416] -> 1D uint8
(EngineCore_DP0 pid=639543) [2026-01-25 23:01:30] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 76382208 bytes
(EngineCore_DP0 pid=639543) 2026-01-25 23:01:40,545 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=639543) 2026-01-25 23:01:40,559 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=639543) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 2/19 [00:00<00:01, 16.55it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▋       | 5/19 [00:00<00:00, 19.34it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 8/19 [00:00<00:00, 20.74it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 11/19 [00:00<00:00, 20.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  79%|███████▉  | 15/19 [00:00<00:00, 24.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:00<00:00, 25.12it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:00<00:00, 23.14it/s]
(EngineCore_DP0 pid=639543) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   9%|▉         | 1/11 [00:00<00:01,  7.34it/s]
Capturing CUDA graphs (decode, FULL):  45%|████▌     | 5/11 [00:00<00:00, 21.76it/s]
Capturing CUDA graphs (decode, FULL):  73%|███████▎  | 8/11 [00:00<00:00, 23.38it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:00<00:00, 23.39it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:00<00:00, 21.87it/s]

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 64/64 [00:00<00:00, 6682.32it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 1/64 [00:03<03:24,  3.25s/it, est. speed input: 4.92 toks/s, output: 78.78 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:03<00:00,  3.25s/it, est. speed input: 309.56 toks/s, output: 4952.90 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:03<00:00, 19.35it/s, est. speed input: 309.56 toks/s, output: 4952.90 toks/s]
[rank0]:[W125 23:01:46.155998777 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 29.5s

测试结果:
  Requests/s:   19.29
  Tokens/s:     5245.85
  Total Reqs:   64
  Elapsed:      3.32s

  [Decode 分析]
  Total Decode Tokens:  16384
  Decode Tokens/s:      4937.27

============================================================
[2/4] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 2048 (= 128 x 16)
│   M_decode      = 128
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 128
│   --max-num-seqs           = 128
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 23:01:51 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=640228) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=640228) WARNING 01-25 23:02:01 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=640228) ERROR 01-25 23:02:08 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=640228) ERROR 01-25 23:02:08 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=640228) ERROR 01-25 23:02:08 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4302, in _dummy_sampler_run
(EngineCore_DP0 pid=640228) ERROR 01-25 23:02:08 [core.py:866]     sampler_output = self.sampler(
(EngineCore_DP0 pid=640228) ERROR 01-25 23:02:08 [core.py:866]                      ^^^^^^^^^^^^^
(EngineCore_DP0 pid=640228) ERROR 01-25 23:02:08 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=640228) ERROR 01-25 23:02:08 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=640228) ERROR 01-25 23:02:08 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=640228) ERROR 01-25 23:02:08 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=640228) ERROR 01-25 23:02:08 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=640228) ERROR 01-25 23:02:08 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=640228) ERROR 01-25 23:02:08 [core.py:866]   File "/root/vllmbench/vllm/v1/sample/sampler.py", line 96, in forward
(EngineCore_DP0 pid=640228) ERROR 01-25 23:02:08 [core.py:866]     sampled, processed_logprobs = self.sample(logits, sampling_metadata)
(EngineCore_DP0 pid=640228) ERROR 01-25 23:02:08 [core.py:866]                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=640228) ERROR 01-25 23:02:08 [core.py:866]   File "/root/vllmbench/vllm/v1/sample/sampler.py", line 187, in sample
(EngineCore_DP0 pid=640228) ERROR 01-25 23:02:08 [core.py:866]     random_sampled, processed_logprobs = self.topk_topp_sampler(
(EngineCore_DP0 pid=640228) ERROR 01-25 23:02:08 [core.py:866]                                          ^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=640228) ERROR 01-25 23:02:08 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=640228) ERROR 01-25 23:02:08 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=640228) ERROR 01-25 23:02:08 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=640228) ERROR 01-25 23:02:08 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=640228) ERROR 01-25 23:02:08 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=640228) ERROR 01-25 23:02:08 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=640228) ERROR 01-25 23:02:08 [core.py:866]   File "/root/vllmbench/vllm/v1/sample/ops/topk_topp_sampler.py", line 104, in forward_native
(EngineCore_DP0 pid=640228) ERROR 01-25 23:02:08 [core.py:866]     logits = self.apply_top_k_top_p(logits, k, p)
(EngineCore_DP0 pid=640228) ERROR 01-25 23:02:08 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=640228) ERROR 01-25 23:02:08 [core.py:866]   File "/root/vllmbench/vllm/v1/sample/ops/topk_topp_sampler.py", line 258, in apply_top_k_top_p
(EngineCore_DP0 pid=640228) ERROR 01-25 23:02:08 [core.py:866]     logits_sort, logits_idx = logits.sort(dim=-1, descending=False)
(EngineCore_DP0 pid=640228) ERROR 01-25 23:02:08 [core.py:866]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=640228) ERROR 01-25 23:02:08 [core.py:866] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacity of 15.46 GiB of which 172.94 MiB is free. Including non-PyTorch memory, this process has 15.07 GiB memory in use. Of the allocated memory 12.14 GiB is allocated by PyTorch, with 68.00 MiB allocated in private pools (e.g., CUDA Graphs), and 2.34 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
(EngineCore_DP0 pid=640228) ERROR 01-25 23:02:08 [core.py:866] 
(EngineCore_DP0 pid=640228) ERROR 01-25 23:02:08 [core.py:866] The above exception was the direct cause of the following exception:
(EngineCore_DP0 pid=640228) ERROR 01-25 23:02:08 [core.py:866] 
(EngineCore_DP0 pid=640228) ERROR 01-25 23:02:08 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=640228) ERROR 01-25 23:02:08 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=640228) ERROR 01-25 23:02:08 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=640228) ERROR 01-25 23:02:08 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=640228) ERROR 01-25 23:02:08 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=640228) ERROR 01-25 23:02:08 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=640228) ERROR 01-25 23:02:08 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=640228) ERROR 01-25 23:02:08 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=640228) ERROR 01-25 23:02:08 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=640228) ERROR 01-25 23:02:08 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 256, in _initialize_kv_caches
(EngineCore_DP0 pid=640228) ERROR 01-25 23:02:08 [core.py:866]     self.model_executor.initialize_from_config(kv_cache_configs)
(EngineCore_DP0 pid=640228) ERROR 01-25 23:02:08 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 116, in initialize_from_config
(EngineCore_DP0 pid=640228) ERROR 01-25 23:02:08 [core.py:866]     self.collective_rpc("compile_or_warm_up_model")
(EngineCore_DP0 pid=640228) ERROR 01-25 23:02:08 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=640228) ERROR 01-25 23:02:08 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=640228) ERROR 01-25 23:02:08 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=640228) ERROR 01-25 23:02:08 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=640228) ERROR 01-25 23:02:08 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=640228) ERROR 01-25 23:02:08 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=640228) ERROR 01-25 23:02:08 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 538, in compile_or_warm_up_model
(EngineCore_DP0 pid=640228) ERROR 01-25 23:02:08 [core.py:866]     self.model_runner._dummy_sampler_run(hidden_states=last_hidden_states)
(EngineCore_DP0 pid=640228) ERROR 01-25 23:02:08 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=640228) ERROR 01-25 23:02:08 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=640228) ERROR 01-25 23:02:08 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=640228) ERROR 01-25 23:02:08 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4307, in _dummy_sampler_run
(EngineCore_DP0 pid=640228) ERROR 01-25 23:02:08 [core.py:866]     raise RuntimeError(
(EngineCore_DP0 pid=640228) ERROR 01-25 23:02:08 [core.py:866] RuntimeError: CUDA out of memory occurred when warming up sampler with 128 dummy requests. Please try lowering `max_num_seqs` or `gpu_memory_utilization` when initializing the engine.


─── STDERR ───
[2026-01-25 23:01:51] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 23:01:51] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-25 23:01:51] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-25 23:01:51] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:01:51] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:01:51] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:01:51] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:01:51] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:01:51] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-25 23:01:51] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 23:01:51] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 23:01:51] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 23:01:51] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 23:01:51] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 23:01:55] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 23:01:55] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-25 23:01:55] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-25 23:01:55] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:01:55] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:01:55] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:01:55] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:01:55] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:01:55] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-25 23:01:55] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 23:01:55] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 23:01:55] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 23:01:55] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 23:01:55] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=640228) [2026-01-25 23:01:56] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=640228) [2026-01-25 23:01:56] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=640228) [2026-01-25 23:01:56] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=640228) [2026-01-25 23:01:56] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=640228) [2026-01-25 23:01:56] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=640228) [2026-01-25 23:01:56] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=640228) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=640228) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.25it/s]
(EngineCore_DP0 pid=640228) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.67it/s]
(EngineCore_DP0 pid=640228) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.74it/s]
(EngineCore_DP0 pid=640228) 
(EngineCore_DP0 pid=640228) [2026-01-25 23:01:57] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5376] -> 1D uint8
(EngineCore_DP0 pid=640228) [2026-01-25 23:01:57] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 18579456 bytes
(EngineCore_DP0 pid=640228) [2026-01-25 23:01:57] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5376] -> 1D uint8
(EngineCore_DP0 pid=640228) [2026-01-25 23:01:57] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 14450688 bytes
(EngineCore_DP0 pid=640228) [2026-01-25 23:01:57] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5376] -> 1D uint8
(EngineCore_DP0 pid=640228) [2026-01-25 23:01:57] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 152764416 bytes
(EngineCore_DP0 pid=640228) [2026-01-25 23:01:57] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 28416] -> 1D uint8
(EngineCore_DP0 pid=640228) [2026-01-25 23:01:57] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 76382208 bytes
(EngineCore_DP0 pid=640228) 2026-01-25 23:02:05,834 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=640228) 2026-01-25 23:02:05,848 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=640228) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 2/35 [00:00<00:02, 13.07it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█▏        | 4/35 [00:00<00:02, 11.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 6/35 [00:00<00:02, 12.96it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▌       | 9/35 [00:00<00:01, 17.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  34%|███▍      | 12/35 [00:00<00:01, 19.85it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 15/35 [00:00<00:00, 22.36it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████▏    | 18/35 [00:00<00:00, 23.70it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 21/35 [00:01<00:00, 24.93it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 25/35 [00:01<00:00, 26.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  83%|████████▎ | 29/35 [00:01<00:00, 27.72it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 33/35 [00:01<00:00, 28.90it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:01<00:00, 23.35it/s]
(EngineCore_DP0 pid=640228) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   5%|▌         | 1/19 [00:00<00:02,  7.40it/s]
Capturing CUDA graphs (decode, FULL):  21%|██        | 4/19 [00:00<00:00, 17.86it/s]
Capturing CUDA graphs (decode, FULL):  42%|████▏     | 8/19 [00:00<00:00, 24.49it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 12/19 [00:00<00:00, 27.92it/s]
Capturing CUDA graphs (decode, FULL):  84%|████████▍ | 16/19 [00:00<00:00, 29.81it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:00<00:00, 27.06it/s]
(EngineCore_DP0 pid=640228) Process EngineCore_DP0:
(EngineCore_DP0 pid=640228) Traceback (most recent call last):
(EngineCore_DP0 pid=640228)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4302, in _dummy_sampler_run
(EngineCore_DP0 pid=640228)     sampler_output = self.sampler(
(EngineCore_DP0 pid=640228)                      ^^^^^^^^^^^^^
(EngineCore_DP0 pid=640228)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=640228)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=640228)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=640228)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=640228)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=640228)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=640228)   File "/root/vllmbench/vllm/v1/sample/sampler.py", line 96, in forward
(EngineCore_DP0 pid=640228)     sampled, processed_logprobs = self.sample(logits, sampling_metadata)
(EngineCore_DP0 pid=640228)                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=640228)   File "/root/vllmbench/vllm/v1/sample/sampler.py", line 187, in sample
(EngineCore_DP0 pid=640228)     random_sampled, processed_logprobs = self.topk_topp_sampler(
(EngineCore_DP0 pid=640228)                                          ^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=640228)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=640228)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=640228)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=640228)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=640228)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=640228)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=640228)   File "/root/vllmbench/vllm/v1/sample/ops/topk_topp_sampler.py", line 104, in forward_native
(EngineCore_DP0 pid=640228)     logits = self.apply_top_k_top_p(logits, k, p)
(EngineCore_DP0 pid=640228)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=640228)   File "/root/vllmbench/vllm/v1/sample/ops/topk_topp_sampler.py", line 258, in apply_top_k_top_p
(EngineCore_DP0 pid=640228)     logits_sort, logits_idx = logits.sort(dim=-1, descending=False)
(EngineCore_DP0 pid=640228)                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=640228) torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacity of 15.46 GiB of which 172.94 MiB is free. Including non-PyTorch memory, this process has 15.07 GiB memory in use. Of the allocated memory 12.14 GiB is allocated by PyTorch, with 68.00 MiB allocated in private pools (e.g., CUDA Graphs), and 2.34 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
(EngineCore_DP0 pid=640228) 
(EngineCore_DP0 pid=640228) The above exception was the direct cause of the following exception:
(EngineCore_DP0 pid=640228) 
(EngineCore_DP0 pid=640228) Traceback (most recent call last):
(EngineCore_DP0 pid=640228)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=640228)     self.run()
(EngineCore_DP0 pid=640228)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=640228)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=640228)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=640228)     raise e
(EngineCore_DP0 pid=640228)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=640228)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=640228)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=640228)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=640228)     super().__init__(
(EngineCore_DP0 pid=640228)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=640228)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=640228)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=640228)   File "/root/vllmbench/vllm/v1/engine/core.py", line 256, in _initialize_kv_caches
(EngineCore_DP0 pid=640228)     self.model_executor.initialize_from_config(kv_cache_configs)
(EngineCore_DP0 pid=640228)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 116, in initialize_from_config
(EngineCore_DP0 pid=640228)     self.collective_rpc("compile_or_warm_up_model")
(EngineCore_DP0 pid=640228)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=640228)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=640228)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=640228)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=640228)     return func(*args, **kwargs)
(EngineCore_DP0 pid=640228)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=640228)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 538, in compile_or_warm_up_model
(EngineCore_DP0 pid=640228)     self.model_runner._dummy_sampler_run(hidden_states=last_hidden_states)
(EngineCore_DP0 pid=640228)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=640228)     return func(*args, **kwargs)
(EngineCore_DP0 pid=640228)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=640228)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4307, in _dummy_sampler_run
(EngineCore_DP0 pid=640228)     raise RuntimeError(
(EngineCore_DP0 pid=640228) RuntimeError: CUDA out of memory occurred when warming up sampler with 128 dummy requests. Please try lowering `max_num_seqs` or `gpu_memory_utilization` when initializing the engine.
[rank0]:[W125 23:02:08.711876616 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[ERROR] 测试失败: M=128 (exit code: 1)

============================================================
[3/4] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 4096 (= 256 x 16)
│   M_decode      = 256
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 256
│   --max-num-seqs           = 256
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 23:02:14 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=640797) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=640797) WARNING 01-25 23:02:25 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=640797) ERROR 01-25 23:02:32 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=640797) ERROR 01-25 23:02:32 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=640797) ERROR 01-25 23:02:32 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4302, in _dummy_sampler_run
(EngineCore_DP0 pid=640797) ERROR 01-25 23:02:32 [core.py:866]     sampler_output = self.sampler(
(EngineCore_DP0 pid=640797) ERROR 01-25 23:02:32 [core.py:866]                      ^^^^^^^^^^^^^
(EngineCore_DP0 pid=640797) ERROR 01-25 23:02:32 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=640797) ERROR 01-25 23:02:32 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=640797) ERROR 01-25 23:02:32 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=640797) ERROR 01-25 23:02:32 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=640797) ERROR 01-25 23:02:32 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=640797) ERROR 01-25 23:02:32 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=640797) ERROR 01-25 23:02:32 [core.py:866]   File "/root/vllmbench/vllm/v1/sample/sampler.py", line 96, in forward
(EngineCore_DP0 pid=640797) ERROR 01-25 23:02:32 [core.py:866]     sampled, processed_logprobs = self.sample(logits, sampling_metadata)
(EngineCore_DP0 pid=640797) ERROR 01-25 23:02:32 [core.py:866]                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=640797) ERROR 01-25 23:02:32 [core.py:866]   File "/root/vllmbench/vllm/v1/sample/sampler.py", line 187, in sample
(EngineCore_DP0 pid=640797) ERROR 01-25 23:02:32 [core.py:866]     random_sampled, processed_logprobs = self.topk_topp_sampler(
(EngineCore_DP0 pid=640797) ERROR 01-25 23:02:32 [core.py:866]                                          ^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=640797) ERROR 01-25 23:02:32 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=640797) ERROR 01-25 23:02:32 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=640797) ERROR 01-25 23:02:32 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=640797) ERROR 01-25 23:02:32 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=640797) ERROR 01-25 23:02:32 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=640797) ERROR 01-25 23:02:32 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=640797) ERROR 01-25 23:02:32 [core.py:866]   File "/root/vllmbench/vllm/v1/sample/ops/topk_topp_sampler.py", line 104, in forward_native
(EngineCore_DP0 pid=640797) ERROR 01-25 23:02:32 [core.py:866]     logits = self.apply_top_k_top_p(logits, k, p)
(EngineCore_DP0 pid=640797) ERROR 01-25 23:02:32 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=640797) ERROR 01-25 23:02:32 [core.py:866]   File "/root/vllmbench/vllm/v1/sample/ops/topk_topp_sampler.py", line 258, in apply_top_k_top_p
(EngineCore_DP0 pid=640797) ERROR 01-25 23:02:32 [core.py:866]     logits_sort, logits_idx = logits.sort(dim=-1, descending=False)
(EngineCore_DP0 pid=640797) ERROR 01-25 23:02:32 [core.py:866]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=640797) ERROR 01-25 23:02:32 [core.py:866] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 446.00 MiB. GPU 0 has a total capacity of 15.46 GiB of which 336.94 MiB is free. Including non-PyTorch memory, this process has 14.91 GiB memory in use. Of the allocated memory 11.94 GiB is allocated by PyTorch, with 68.00 MiB allocated in private pools (e.g., CUDA Graphs), and 2.33 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
(EngineCore_DP0 pid=640797) ERROR 01-25 23:02:32 [core.py:866] 
(EngineCore_DP0 pid=640797) ERROR 01-25 23:02:32 [core.py:866] The above exception was the direct cause of the following exception:
(EngineCore_DP0 pid=640797) ERROR 01-25 23:02:32 [core.py:866] 
(EngineCore_DP0 pid=640797) ERROR 01-25 23:02:32 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=640797) ERROR 01-25 23:02:32 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=640797) ERROR 01-25 23:02:32 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=640797) ERROR 01-25 23:02:32 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=640797) ERROR 01-25 23:02:32 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=640797) ERROR 01-25 23:02:32 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=640797) ERROR 01-25 23:02:32 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=640797) ERROR 01-25 23:02:32 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=640797) ERROR 01-25 23:02:32 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=640797) ERROR 01-25 23:02:32 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 256, in _initialize_kv_caches
(EngineCore_DP0 pid=640797) ERROR 01-25 23:02:32 [core.py:866]     self.model_executor.initialize_from_config(kv_cache_configs)
(EngineCore_DP0 pid=640797) ERROR 01-25 23:02:32 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 116, in initialize_from_config
(EngineCore_DP0 pid=640797) ERROR 01-25 23:02:32 [core.py:866]     self.collective_rpc("compile_or_warm_up_model")
(EngineCore_DP0 pid=640797) ERROR 01-25 23:02:32 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=640797) ERROR 01-25 23:02:32 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=640797) ERROR 01-25 23:02:32 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=640797) ERROR 01-25 23:02:32 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=640797) ERROR 01-25 23:02:32 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=640797) ERROR 01-25 23:02:32 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=640797) ERROR 01-25 23:02:32 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 538, in compile_or_warm_up_model
(EngineCore_DP0 pid=640797) ERROR 01-25 23:02:32 [core.py:866]     self.model_runner._dummy_sampler_run(hidden_states=last_hidden_states)
(EngineCore_DP0 pid=640797) ERROR 01-25 23:02:32 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=640797) ERROR 01-25 23:02:32 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=640797) ERROR 01-25 23:02:32 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=640797) ERROR 01-25 23:02:32 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4307, in _dummy_sampler_run
(EngineCore_DP0 pid=640797) ERROR 01-25 23:02:32 [core.py:866]     raise RuntimeError(
(EngineCore_DP0 pid=640797) ERROR 01-25 23:02:32 [core.py:866] RuntimeError: CUDA out of memory occurred when warming up sampler with 256 dummy requests. Please try lowering `max_num_seqs` or `gpu_memory_utilization` when initializing the engine.


─── STDERR ───
[2026-01-25 23:02:14] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 23:02:14] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-25 23:02:14] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-25 23:02:14] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:02:14] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:02:14] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:02:14] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:02:14] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:02:14] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-25 23:02:14] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 23:02:14] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 23:02:14] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 23:02:14] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 23:02:14] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 23:02:18] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 23:02:18] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-25 23:02:18] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-25 23:02:18] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:02:18] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:02:18] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:02:18] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:02:18] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:02:18] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-25 23:02:18] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 23:02:18] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 23:02:18] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 23:02:18] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 23:02:18] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=640797) [2026-01-25 23:02:19] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=640797) [2026-01-25 23:02:19] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=640797) [2026-01-25 23:02:19] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=640797) [2026-01-25 23:02:19] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=640797) [2026-01-25 23:02:19] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=640797) [2026-01-25 23:02:19] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=640797) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=640797) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.25it/s]
(EngineCore_DP0 pid=640797) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.67it/s]
(EngineCore_DP0 pid=640797) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.74it/s]
(EngineCore_DP0 pid=640797) 
(EngineCore_DP0 pid=640797) [2026-01-25 23:02:20] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5376] -> 1D uint8
(EngineCore_DP0 pid=640797) [2026-01-25 23:02:20] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 18579456 bytes
(EngineCore_DP0 pid=640797) [2026-01-25 23:02:20] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5376] -> 1D uint8
(EngineCore_DP0 pid=640797) [2026-01-25 23:02:20] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 14450688 bytes
(EngineCore_DP0 pid=640797) [2026-01-25 23:02:20] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5376] -> 1D uint8
(EngineCore_DP0 pid=640797) [2026-01-25 23:02:20] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 152764416 bytes
(EngineCore_DP0 pid=640797) [2026-01-25 23:02:20] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 28416] -> 1D uint8
(EngineCore_DP0 pid=640797) [2026-01-25 23:02:20] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 76382208 bytes
(EngineCore_DP0 pid=640797) 2026-01-25 23:02:29,103 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=640797) 2026-01-25 23:02:29,117 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=640797) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/36 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 3/36 [00:00<00:01, 25.16it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 6/36 [00:00<00:01, 24.42it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 9/36 [00:00<00:01, 26.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 12/36 [00:00<00:00, 27.10it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 15/36 [00:00<00:00, 27.93it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 18/36 [00:00<00:00, 27.88it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 21/36 [00:00<00:00, 24.02it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 24/36 [00:01<00:00, 18.87it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 27/36 [00:01<00:00, 17.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  83%|████████▎ | 30/36 [00:01<00:00, 20.06it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 34/36 [00:01<00:00, 22.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:01<00:00, 23.00it/s]
(EngineCore_DP0 pid=640797) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   3%|▎         | 1/35 [00:00<00:04,  7.26it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 5/35 [00:00<00:01, 20.76it/s]
Capturing CUDA graphs (decode, FULL):  26%|██▌       | 9/35 [00:00<00:01, 25.65it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 13/35 [00:00<00:00, 28.19it/s]
Capturing CUDA graphs (decode, FULL):  49%|████▊     | 17/35 [00:00<00:00, 29.71it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 20/35 [00:00<00:00, 29.09it/s]
Capturing CUDA graphs (decode, FULL):  69%|██████▊   | 24/35 [00:00<00:00, 30.35it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 28/35 [00:00<00:00, 31.20it/s]
Capturing CUDA graphs (decode, FULL):  91%|█████████▏| 32/35 [00:01<00:00, 31.61it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:01<00:00, 29.01it/s]
(EngineCore_DP0 pid=640797) Process EngineCore_DP0:
(EngineCore_DP0 pid=640797) Traceback (most recent call last):
(EngineCore_DP0 pid=640797)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4302, in _dummy_sampler_run
(EngineCore_DP0 pid=640797)     sampler_output = self.sampler(
(EngineCore_DP0 pid=640797)                      ^^^^^^^^^^^^^
(EngineCore_DP0 pid=640797)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=640797)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=640797)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=640797)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=640797)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=640797)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=640797)   File "/root/vllmbench/vllm/v1/sample/sampler.py", line 96, in forward
(EngineCore_DP0 pid=640797)     sampled, processed_logprobs = self.sample(logits, sampling_metadata)
(EngineCore_DP0 pid=640797)                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=640797)   File "/root/vllmbench/vllm/v1/sample/sampler.py", line 187, in sample
(EngineCore_DP0 pid=640797)     random_sampled, processed_logprobs = self.topk_topp_sampler(
(EngineCore_DP0 pid=640797)                                          ^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=640797)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=640797)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=640797)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=640797)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=640797)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=640797)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=640797)   File "/root/vllmbench/vllm/v1/sample/ops/topk_topp_sampler.py", line 104, in forward_native
(EngineCore_DP0 pid=640797)     logits = self.apply_top_k_top_p(logits, k, p)
(EngineCore_DP0 pid=640797)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=640797)   File "/root/vllmbench/vllm/v1/sample/ops/topk_topp_sampler.py", line 258, in apply_top_k_top_p
(EngineCore_DP0 pid=640797)     logits_sort, logits_idx = logits.sort(dim=-1, descending=False)
(EngineCore_DP0 pid=640797)                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=640797) torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 446.00 MiB. GPU 0 has a total capacity of 15.46 GiB of which 336.94 MiB is free. Including non-PyTorch memory, this process has 14.91 GiB memory in use. Of the allocated memory 11.94 GiB is allocated by PyTorch, with 68.00 MiB allocated in private pools (e.g., CUDA Graphs), and 2.33 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
(EngineCore_DP0 pid=640797) 
(EngineCore_DP0 pid=640797) The above exception was the direct cause of the following exception:
(EngineCore_DP0 pid=640797) 
(EngineCore_DP0 pid=640797) Traceback (most recent call last):
(EngineCore_DP0 pid=640797)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=640797)     self.run()
(EngineCore_DP0 pid=640797)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=640797)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=640797)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=640797)     raise e
(EngineCore_DP0 pid=640797)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=640797)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=640797)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=640797)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=640797)     super().__init__(
(EngineCore_DP0 pid=640797)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=640797)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=640797)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=640797)   File "/root/vllmbench/vllm/v1/engine/core.py", line 256, in _initialize_kv_caches
(EngineCore_DP0 pid=640797)     self.model_executor.initialize_from_config(kv_cache_configs)
(EngineCore_DP0 pid=640797)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 116, in initialize_from_config
(EngineCore_DP0 pid=640797)     self.collective_rpc("compile_or_warm_up_model")
(EngineCore_DP0 pid=640797)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=640797)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=640797)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=640797)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=640797)     return func(*args, **kwargs)
(EngineCore_DP0 pid=640797)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=640797)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 538, in compile_or_warm_up_model
(EngineCore_DP0 pid=640797)     self.model_runner._dummy_sampler_run(hidden_states=last_hidden_states)
(EngineCore_DP0 pid=640797)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=640797)     return func(*args, **kwargs)
(EngineCore_DP0 pid=640797)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=640797)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4307, in _dummy_sampler_run
(EngineCore_DP0 pid=640797)     raise RuntimeError(
(EngineCore_DP0 pid=640797) RuntimeError: CUDA out of memory occurred when warming up sampler with 256 dummy requests. Please try lowering `max_num_seqs` or `gpu_memory_utilization` when initializing the engine.
[rank0]:[W125 23:02:32.583675284 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[ERROR] 测试失败: M=256 (exit code: 1)

============================================================
[4/4] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 8192 (= 512 x 16)
│   M_decode      = 512
│   batched_tokens = 512 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 512
│   --max-num-seqs           = 512
│   --max-model-len          = 272
│   --max-num-batched-tokens = 512
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 23:02:38 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=641387) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=641387) WARNING 01-25 23:02:49 [backends.py:609] Failed to read file <frozen os>
Throughput: 18.02 requests/s, 4902.18 total tokens/s, 4613.81 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072


─── STDERR ───
[2026-01-25 23:02:38] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 23:02:38] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-25 23:02:38] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-25 23:02:38] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:02:38] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:02:38] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:02:38] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:02:38] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:02:38] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-25 23:02:38] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 23:02:38] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 23:02:38] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 23:02:38] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 23:02:38] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 23:02:42] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 23:02:42] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-25 23:02:42] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-25 23:02:42] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:02:42] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:02:42] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:02:42] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:02:42] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:02:42] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-25 23:02:42] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 23:02:42] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 23:02:42] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 23:02:42] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 23:02:42] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=641387) [2026-01-25 23:02:43] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=641387) [2026-01-25 23:02:43] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=641387) [2026-01-25 23:02:43] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=641387) [2026-01-25 23:02:43] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=641387) [2026-01-25 23:02:43] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=641387) [2026-01-25 23:02:43] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=641387) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=641387) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.25it/s]
(EngineCore_DP0 pid=641387) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.68it/s]
(EngineCore_DP0 pid=641387) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.74it/s]
(EngineCore_DP0 pid=641387) 
(EngineCore_DP0 pid=641387) [2026-01-25 23:02:44] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5376] -> 1D uint8
(EngineCore_DP0 pid=641387) [2026-01-25 23:02:44] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 18579456 bytes
(EngineCore_DP0 pid=641387) [2026-01-25 23:02:44] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5376] -> 1D uint8
(EngineCore_DP0 pid=641387) [2026-01-25 23:02:44] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 14450688 bytes
(EngineCore_DP0 pid=641387) [2026-01-25 23:02:44] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5376] -> 1D uint8
(EngineCore_DP0 pid=641387) [2026-01-25 23:02:44] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 152764416 bytes
(EngineCore_DP0 pid=641387) [2026-01-25 23:02:44] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 28416] -> 1D uint8
(EngineCore_DP0 pid=641387) [2026-01-25 23:02:44] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 76382208 bytes
(EngineCore_DP0 pid=641387) 2026-01-25 23:02:54,716 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=641387) 2026-01-25 23:02:54,730 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=641387) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|▍         | 2/51 [00:00<00:02, 19.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  10%|▉         | 5/51 [00:00<00:02, 21.04it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 8/51 [00:00<00:01, 21.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 11/51 [00:00<00:01, 22.68it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 14/51 [00:00<00:01, 22.70it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 17/51 [00:00<00:01, 23.14it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|███▉      | 20/51 [00:00<00:01, 24.24it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 23/51 [00:00<00:01, 25.08it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████     | 26/51 [00:01<00:00, 26.22it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 29/51 [00:01<00:00, 26.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 32/51 [00:01<00:00, 27.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  69%|██████▊   | 35/51 [00:01<00:00, 26.58it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▍  | 38/51 [00:01<00:00, 27.45it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 41/51 [00:01<00:00, 27.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▋ | 44/51 [00:01<00:00, 26.48it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  92%|█████████▏| 47/51 [00:01<00:00, 26.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:01<00:00, 27.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:01<00:00, 25.60it/s]
(EngineCore_DP0 pid=641387) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/51 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   2%|▏         | 1/51 [00:00<00:08,  5.79it/s]
Capturing CUDA graphs (decode, FULL):   6%|▌         | 3/51 [00:00<00:04,  9.88it/s]
Capturing CUDA graphs (decode, FULL):  10%|▉         | 5/51 [00:00<00:04, 11.33it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▎        | 7/51 [00:00<00:03, 12.61it/s]
Capturing CUDA graphs (decode, FULL):  20%|█▉        | 10/51 [00:00<00:02, 16.68it/s]
Capturing CUDA graphs (decode, FULL):  25%|██▌       | 13/51 [00:00<00:01, 19.37it/s]
Capturing CUDA graphs (decode, FULL):  31%|███▏      | 16/51 [00:00<00:01, 22.07it/s]
Capturing CUDA graphs (decode, FULL):  39%|███▉      | 20/51 [00:01<00:01, 24.91it/s]
Capturing CUDA graphs (decode, FULL):  47%|████▋     | 24/51 [00:01<00:01, 26.97it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 28/51 [00:01<00:00, 28.61it/s]
Capturing CUDA graphs (decode, FULL):  63%|██████▎   | 32/51 [00:01<00:00, 29.82it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████   | 36/51 [00:01<00:00, 30.70it/s]
Capturing CUDA graphs (decode, FULL):  78%|███████▊  | 40/51 [00:01<00:00, 31.15it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▋ | 44/51 [00:01<00:00, 30.54it/s]
Capturing CUDA graphs (decode, FULL):  94%|█████████▍| 48/51 [00:01<00:00, 31.13it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:02<00:00, 24.89it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 7761.45it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:03<33:06,  3.89s/it, est. speed input: 4.12 toks/s, output: 65.86 toks/s]
Processed prompts:   8%|▊         | 40/512 [00:04<00:37, 12.74it/s, est. speed input: 149.48 toks/s, output: 2391.69 toks/s]
Processed prompts:   9%|▉         | 46/512 [00:04<00:38, 12.21it/s, est. speed input: 151.16 toks/s, output: 2418.53 toks/s]
Processed prompts:  10%|▉         | 50/512 [00:05<00:36, 12.70it/s, est. speed input: 156.62 toks/s, output: 2505.88 toks/s]
Processed prompts:  10%|█         | 53/512 [00:05<00:36, 12.65it/s, est. speed input: 158.47 toks/s, output: 2535.46 toks/s]
Processed prompts:  11%|█         | 56/512 [00:05<00:35, 12.86it/s, est. speed input: 161.01 toks/s, output: 2576.19 toks/s]
Processed prompts:  12%|█▏        | 61/512 [00:05<00:32, 13.99it/s, est. speed input: 167.13 toks/s, output: 2674.11 toks/s]
Processed prompts:  13%|█▎        | 67/512 [00:06<00:28, 15.76it/s, est. speed input: 175.20 toks/s, output: 2803.12 toks/s]
Processed prompts:  14%|█▍        | 74/512 [00:06<00:24, 18.24it/s, est. speed input: 185.28 toks/s, output: 2964.44 toks/s]
Processed prompts:  16%|█▌        | 82/512 [00:06<00:17, 24.66it/s, est. speed input: 201.40 toks/s, output: 3222.44 toks/s]
Processed prompts:  17%|█▋        | 86/512 [00:06<00:19, 22.34it/s, est. speed input: 203.60 toks/s, output: 3257.59 toks/s]
Processed prompts:  18%|█▊        | 94/512 [00:06<00:13, 29.93it/s, est. speed input: 218.88 toks/s, output: 3502.10 toks/s]
Processed prompts:  19%|█▉        | 99/512 [00:07<00:15, 27.06it/s, est. speed input: 222.80 toks/s, output: 3564.75 toks/s]
Processed prompts:  20%|██        | 103/512 [00:07<00:21, 18.68it/s, est. speed input: 218.07 toks/s, output: 3489.04 toks/s]
Processed prompts:  21%|██        | 106/512 [00:08<00:30, 13.38it/s, est. speed input: 210.68 toks/s, output: 3370.84 toks/s]
Processed prompts:  21%|██▏       | 109/512 [00:08<00:31, 12.75it/s, est. speed input: 209.48 toks/s, output: 3351.63 toks/s]
Processed prompts:  22%|██▏       | 111/512 [00:08<00:32, 12.18it/s, est. speed input: 208.31 toks/s, output: 3333.00 toks/s]
Processed prompts:  22%|██▏       | 114/512 [00:08<00:27, 14.35it/s, est. speed input: 211.30 toks/s, output: 3380.72 toks/s]
Processed prompts:  23%|██▎       | 116/512 [00:08<00:27, 14.45it/s, est. speed input: 211.71 toks/s, output: 3387.29 toks/s]
Processed prompts:  23%|██▎       | 120/512 [00:08<00:21, 18.60it/s, est. speed input: 216.33 toks/s, output: 3461.29 toks/s]
Processed prompts:  24%|██▍       | 124/512 [00:08<00:17, 22.10it/s, est. speed input: 220.65 toks/s, output: 3530.48 toks/s]
Processed prompts:  25%|██▌       | 128/512 [00:09<00:16, 23.81it/s, est. speed input: 224.25 toks/s, output: 3587.94 toks/s]
Processed prompts:  26%|██▌       | 133/512 [00:09<00:17, 21.62it/s, est. speed input: 226.38 toks/s, output: 3622.04 toks/s]
Processed prompts:  27%|██▋       | 139/512 [00:09<00:13, 28.07it/s, est. speed input: 233.77 toks/s, output: 3740.30 toks/s]
Processed prompts:  28%|██▊       | 145/512 [00:09<00:11, 33.30it/s, est. speed input: 240.83 toks/s, output: 3853.34 toks/s]
Processed prompts:  29%|██▉       | 149/512 [00:09<00:11, 31.80it/s, est. speed input: 243.87 toks/s, output: 3901.93 toks/s]
Processed prompts:  30%|██▉       | 153/512 [00:09<00:10, 33.21it/s, est. speed input: 247.74 toks/s, output: 3963.79 toks/s]
Processed prompts:  31%|███       | 157/512 [00:10<00:11, 31.07it/s, est. speed input: 250.40 toks/s, output: 4006.41 toks/s]
Processed prompts:  31%|███▏      | 161/512 [00:10<00:27, 12.82it/s, est. speed input: 238.29 toks/s, output: 3812.70 toks/s]
Processed prompts:  32%|███▏      | 164/512 [00:11<00:26, 12.91it/s, est. speed input: 237.75 toks/s, output: 3803.97 toks/s]
Processed prompts:  33%|███▎      | 167/512 [00:11<00:28, 12.26it/s, est. speed input: 236.06 toks/s, output: 3777.02 toks/s]
Processed prompts:  33%|███▎      | 169/512 [00:11<00:27, 12.50it/s, est. speed input: 235.83 toks/s, output: 3773.26 toks/s]
Processed prompts:  34%|███▎      | 172/512 [00:11<00:23, 14.44it/s, est. speed input: 237.34 toks/s, output: 3797.42 toks/s]
Processed prompts:  34%|███▍      | 175/512 [00:11<00:20, 16.08it/s, est. speed input: 238.69 toks/s, output: 3819.04 toks/s]
Processed prompts:  35%|███▍      | 178/512 [00:11<00:18, 17.66it/s, est. speed input: 240.12 toks/s, output: 3841.93 toks/s]
Processed prompts:  35%|███▌      | 181/512 [00:12<00:18, 18.18it/s, est. speed input: 241.05 toks/s, output: 3856.77 toks/s]
Processed prompts:  36%|███▌      | 184/512 [00:12<00:19, 17.10it/s, est. speed input: 241.03 toks/s, output: 3856.48 toks/s]
Processed prompts:  37%|███▋      | 188/512 [00:12<00:16, 19.27it/s, est. speed input: 243.06 toks/s, output: 3888.99 toks/s]
Processed prompts:  37%|███▋      | 191/512 [00:12<00:16, 19.69it/s, est. speed input: 244.10 toks/s, output: 3905.67 toks/s]
Processed prompts:  38%|███▊      | 194/512 [00:12<00:15, 20.58it/s, est. speed input: 245.41 toks/s, output: 3926.48 toks/s]
Processed prompts:  39%|███▊      | 198/512 [00:12<00:12, 24.41it/s, est. speed input: 248.35 toks/s, output: 3973.58 toks/s]
Processed prompts:  40%|████      | 205/512 [00:12<00:09, 33.41it/s, est. speed input: 254.70 toks/s, output: 4075.17 toks/s]
Processed prompts:  43%|████▎     | 218/512 [00:13<00:06, 47.53it/s, est. speed input: 267.11 toks/s, output: 4273.71 toks/s]
Processed prompts:  44%|████▎     | 223/512 [00:14<00:18, 15.61it/s, est. speed input: 253.31 toks/s, output: 4052.90 toks/s]
Processed prompts:  44%|████▍     | 227/512 [00:14<00:19, 14.36it/s, est. speed input: 251.39 toks/s, output: 4022.31 toks/s]
Processed prompts:  45%|████▍     | 230/512 [00:14<00:19, 14.26it/s, est. speed input: 250.94 toks/s, output: 4015.04 toks/s]
Processed prompts:  46%|████▌     | 233/512 [00:15<00:22, 12.60it/s, est. speed input: 248.40 toks/s, output: 3974.43 toks/s]
Processed prompts:  46%|████▋     | 237/512 [00:15<00:18, 15.13it/s, est. speed input: 250.48 toks/s, output: 4007.72 toks/s]
Processed prompts:  47%|████▋     | 240/512 [00:15<00:18, 14.70it/s, est. speed input: 249.97 toks/s, output: 3999.47 toks/s]
Processed prompts:  47%|████▋     | 243/512 [00:15<00:17, 15.62it/s, est. speed input: 250.55 toks/s, output: 4008.75 toks/s]
Processed prompts:  48%|████▊     | 245/512 [00:15<00:17, 15.01it/s, est. speed input: 250.11 toks/s, output: 4001.82 toks/s]
Processed prompts:  48%|████▊     | 248/512 [00:15<00:16, 15.82it/s, est. speed input: 250.54 toks/s, output: 4008.68 toks/s]
Processed prompts:  49%|████▉     | 252/512 [00:15<00:13, 19.55it/s, est. speed input: 252.70 toks/s, output: 4043.21 toks/s]
Processed prompts:  50%|█████     | 257/512 [00:16<00:10, 24.84it/s, est. speed input: 255.86 toks/s, output: 4093.72 toks/s]
Processed prompts:  51%|█████     | 262/512 [00:16<00:08, 28.53it/s, est. speed input: 258.77 toks/s, output: 4140.28 toks/s]
Processed prompts:  52%|█████▏    | 267/512 [00:16<00:07, 32.12it/s, est. speed input: 261.81 toks/s, output: 4188.96 toks/s]
Processed prompts:  53%|█████▎    | 271/512 [00:16<00:07, 33.85it/s, est. speed input: 264.09 toks/s, output: 4225.38 toks/s]
Processed prompts:  54%|█████▍    | 277/512 [00:16<00:06, 38.09it/s, est. speed input: 267.90 toks/s, output: 4286.42 toks/s]
Processed prompts:  55%|█████▌    | 282/512 [00:16<00:05, 40.66it/s, est. speed input: 271.03 toks/s, output: 4336.45 toks/s]
Processed prompts:  56%|█████▌    | 287/512 [00:17<00:15, 14.54it/s, est. speed input: 262.50 toks/s, output: 4200.02 toks/s]
Processed prompts:  57%|█████▋    | 291/512 [00:18<00:22,  9.83it/s, est. speed input: 254.73 toks/s, output: 4075.65 toks/s]
Processed prompts:  57%|█████▋    | 294/512 [00:18<00:20, 10.39it/s, est. speed input: 254.18 toks/s, output: 4066.82 toks/s]
Processed prompts:  58%|█████▊    | 297/512 [00:18<00:17, 11.96it/s, est. speed input: 254.99 toks/s, output: 4079.80 toks/s]
Processed prompts:  59%|█████▊    | 300/512 [00:18<00:15, 13.98it/s, est. speed input: 256.09 toks/s, output: 4097.42 toks/s]
Processed prompts:  59%|█████▉    | 303/512 [00:18<00:13, 15.82it/s, est. speed input: 257.00 toks/s, output: 4112.00 toks/s]
Processed prompts:  60%|██████    | 308/512 [00:19<00:10, 20.10it/s, est. speed input: 259.29 toks/s, output: 4148.56 toks/s]
Processed prompts:  61%|██████    | 313/512 [00:19<00:08, 24.40it/s, est. speed input: 261.76 toks/s, output: 4188.22 toks/s]
Processed prompts:  62%|██████▏   | 318/512 [00:19<00:07, 27.19it/s, est. speed input: 263.98 toks/s, output: 4223.65 toks/s]
Processed prompts:  63%|██████▎   | 322/512 [00:19<00:06, 29.54it/s, est. speed input: 265.85 toks/s, output: 4253.61 toks/s]
Processed prompts:  64%|██████▎   | 326/512 [00:19<00:05, 31.87it/s, est. speed input: 267.76 toks/s, output: 4284.20 toks/s]
Processed prompts:  65%|██████▍   | 331/512 [00:19<00:05, 34.40it/s, est. speed input: 270.17 toks/s, output: 4322.69 toks/s]
Processed prompts:  66%|██████▌   | 338/512 [00:19<00:05, 31.56it/s, est. speed input: 272.42 toks/s, output: 4358.75 toks/s]
Processed prompts:  67%|██████▋   | 342/512 [00:20<00:10, 16.06it/s, est. speed input: 267.28 toks/s, output: 4276.45 toks/s]
Processed prompts:  67%|██████▋   | 345/512 [00:20<00:11, 14.41it/s, est. speed input: 265.87 toks/s, output: 4253.93 toks/s]
Processed prompts:  68%|██████▊   | 348/512 [00:21<00:12, 13.14it/s, est. speed input: 264.42 toks/s, output: 4230.77 toks/s]
Processed prompts:  68%|██████▊   | 350/512 [00:21<00:11, 13.95it/s, est. speed input: 264.65 toks/s, output: 4234.34 toks/s]
Processed prompts:  69%|██████▉   | 353/512 [00:21<00:10, 15.16it/s, est. speed input: 264.99 toks/s, output: 4239.90 toks/s]
Processed prompts:  70%|██████▉   | 356/512 [00:21<00:10, 15.55it/s, est. speed input: 265.00 toks/s, output: 4240.00 toks/s]
Processed prompts:  70%|██████▉   | 358/512 [00:21<00:10, 15.40it/s, est. speed input: 264.83 toks/s, output: 4237.31 toks/s]
Processed prompts:  71%|███████   | 361/512 [00:21<00:08, 18.03it/s, est. speed input: 265.77 toks/s, output: 4252.33 toks/s]
Processed prompts:  71%|███████▏  | 365/512 [00:21<00:06, 21.32it/s, est. speed input: 267.11 toks/s, output: 4273.83 toks/s]
Processed prompts:  72%|███████▏  | 368/512 [00:21<00:06, 21.75it/s, est. speed input: 267.71 toks/s, output: 4283.32 toks/s]
Processed prompts:  72%|███████▏  | 371/512 [00:22<00:06, 22.11it/s, est. speed input: 268.30 toks/s, output: 4292.82 toks/s]
Processed prompts:  73%|███████▎  | 375/512 [00:22<00:05, 25.86it/s, est. speed input: 269.89 toks/s, output: 4318.17 toks/s]
Processed prompts:  74%|███████▍  | 378/512 [00:22<00:05, 25.53it/s, est. speed input: 270.57 toks/s, output: 4329.08 toks/s]
Processed prompts:  74%|███████▍  | 381/512 [00:22<00:05, 24.22it/s, est. speed input: 271.02 toks/s, output: 4336.28 toks/s]
Processed prompts:  75%|███████▌  | 385/512 [00:22<00:04, 26.67it/s, est. speed input: 272.39 toks/s, output: 4358.20 toks/s]
Processed prompts:  76%|███████▌  | 389/512 [00:22<00:04, 28.91it/s, est. speed input: 273.82 toks/s, output: 4381.17 toks/s]
Processed prompts:  77%|███████▋  | 393/512 [00:22<00:03, 30.83it/s, est. speed input: 275.29 toks/s, output: 4404.59 toks/s]
Processed prompts:  78%|███████▊  | 399/512 [00:22<00:02, 38.16it/s, est. speed input: 278.23 toks/s, output: 4451.72 toks/s]
Processed prompts:  79%|███████▊  | 403/512 [00:23<00:07, 14.77it/s, est. speed input: 272.74 toks/s, output: 4363.77 toks/s]
Processed prompts:  79%|███████▉  | 406/512 [00:24<00:09, 11.39it/s, est. speed input: 269.47 toks/s, output: 4311.53 toks/s]
Processed prompts:  80%|███████▉  | 409/512 [00:24<00:09, 11.20it/s, est. speed input: 268.34 toks/s, output: 4293.39 toks/s]
Processed prompts:  80%|████████  | 411/512 [00:24<00:08, 11.93it/s, est. speed input: 268.31 toks/s, output: 4293.00 toks/s]
Processed prompts:  81%|████████  | 415/512 [00:24<00:06, 14.77it/s, est. speed input: 269.25 toks/s, output: 4307.95 toks/s]
Processed prompts:  82%|████████▏ | 418/512 [00:24<00:05, 16.04it/s, est. speed input: 269.61 toks/s, output: 4313.75 toks/s]
Processed prompts:  82%|████████▏ | 421/512 [00:24<00:05, 17.80it/s, est. speed input: 270.23 toks/s, output: 4323.61 toks/s]
Processed prompts:  83%|████████▎ | 424/512 [00:25<00:04, 19.56it/s, est. speed input: 270.89 toks/s, output: 4334.26 toks/s]
Processed prompts:  83%|████████▎ | 427/512 [00:25<00:04, 19.13it/s, est. speed input: 271.02 toks/s, output: 4336.33 toks/s]
Processed prompts:  84%|████████▍ | 430/512 [00:25<00:04, 19.62it/s, est. speed input: 271.38 toks/s, output: 4342.08 toks/s]
Processed prompts:  85%|████████▍ | 434/512 [00:25<00:03, 23.10it/s, est. speed input: 272.63 toks/s, output: 4362.13 toks/s]
Processed prompts:  86%|████████▌ | 440/512 [00:25<00:02, 29.95it/s, est. speed input: 275.07 toks/s, output: 4401.17 toks/s]
Processed prompts:  88%|████████▊ | 451/512 [00:25<00:01, 47.75it/s, est. speed input: 280.74 toks/s, output: 4491.89 toks/s]
Processed prompts:  89%|████████▉ | 457/512 [00:25<00:01, 39.72it/s, est. speed input: 282.15 toks/s, output: 4514.37 toks/s]
Processed prompts:  90%|█████████ | 462/512 [00:26<00:02, 19.80it/s, est. speed input: 278.62 toks/s, output: 4457.91 toks/s]
Processed prompts:  91%|█████████ | 466/512 [00:26<00:02, 17.49it/s, est. speed input: 277.69 toks/s, output: 4443.05 toks/s]
Processed prompts:  92%|█████████▏| 469/512 [00:27<00:02, 16.32it/s, est. speed input: 277.06 toks/s, output: 4433.01 toks/s]
Processed prompts:  92%|█████████▏| 472/512 [00:27<00:02, 16.52it/s, est. speed input: 277.06 toks/s, output: 4432.92 toks/s]
Processed prompts:  93%|█████████▎| 475/512 [00:27<00:02, 16.47it/s, est. speed input: 276.95 toks/s, output: 4431.23 toks/s]
Processed prompts:  93%|█████████▎| 477/512 [00:27<00:02, 15.54it/s, est. speed input: 276.50 toks/s, output: 4423.98 toks/s]
Processed prompts:  94%|█████████▍| 480/512 [00:27<00:01, 16.91it/s, est. speed input: 276.85 toks/s, output: 4429.64 toks/s]
Processed prompts:  94%|█████████▍| 482/512 [00:27<00:01, 17.05it/s, est. speed input: 276.87 toks/s, output: 4429.96 toks/s]
Processed prompts:  95%|█████████▌| 488/512 [00:27<00:00, 25.22it/s, est. speed input: 279.19 toks/s, output: 4466.97 toks/s]
Processed prompts:  96%|█████████▌| 491/512 [00:28<00:00, 26.27it/s, est. speed input: 279.90 toks/s, output: 4478.39 toks/s]
Processed prompts:  98%|█████████▊| 501/512 [00:28<00:00, 43.15it/s, est. speed input: 284.50 toks/s, output: 4551.97 toks/s]
Processed prompts:  99%|█████████▉| 508/512 [00:28<00:00, 47.08it/s, est. speed input: 287.21 toks/s, output: 4595.36 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:28<00:00, 47.08it/s, est. speed input: 289.05 toks/s, output: 4624.74 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:28<00:00, 18.07it/s, est. speed input: 289.05 toks/s, output: 4624.74 toks/s]
[rank0]:[W125 23:03:28.120022609 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 55.1s

测试结果:
  Requests/s:   18.02
  Tokens/s:     4902.18
  Total Reqs:   512
  Elapsed:      28.41s

  [Decode 分析]
  Total Decode Tokens:  131072
  Decode Tokens/s:      4613.81


------------------------------------------------------------
  生成 CSV: Qwen2.5-7B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_8/Qwen2.5-7B-INT8_decode.csv

预览:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
64,16,64,64,256,256,19.2862,5245.8485,3.3184
128,16,128,128,256,256,-1.0000,-1.0000,-1.0000
256,16,256,256,256,256,-1.0000,-1.0000,-1.0000
512,16,512,512,256,256,18.0227,4902.1778,28.4086

------------------------------------------------------------

[INFO] 完成: 2 成功, 2 失败

============================================================
  Qwen2.5-7B-INT8 | cuSPARSELt (2_10) | decode
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-INT8-SlideSparse-2_10
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_10

============================================================
[1/4] 测试 M=64
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 64
│   M_prefill     = 1024 (= 64 x 16)
│   M_decode      = 64
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 64
│   --max-num-seqs           = 64
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 23:03:33 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=642417) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=642417) WARNING 01-25 23:03:46 [backends.py:609] Failed to read file <frozen os>
Throughput: 17.97 requests/s, 4887.25 total tokens/s, 4599.76 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384


─── STDERR ───
[2026-01-25 23:03:33] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 23:03:33] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-25 23:03:33] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-25 23:03:33] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:03:33] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:03:33] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:03:33] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:03:33] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:03:33] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-25 23:03:33] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 23:03:33] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 23:03:33] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 23:03:33] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 23:03:33] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 23:03:37] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 23:03:37] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-25 23:03:37] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-25 23:03:37] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:03:37] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:03:37] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:03:37] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:03:37] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:03:37] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-25 23:03:37] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 23:03:37] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 23:03:37] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 23:03:37] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 23:03:37] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=642417) [2026-01-25 23:03:38] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=642417) [2026-01-25 23:03:38] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=642417) [2026-01-25 23:03:38] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=642417) [2026-01-25 23:03:38] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=642417) [2026-01-25 23:03:38] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=642417) [2026-01-25 23:03:38] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=642417) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=642417) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.76s/it]
(EngineCore_DP0 pid=642417) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:03<00:00,  1.83s/it]
(EngineCore_DP0 pid=642417) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:03<00:00,  1.82s/it]
(EngineCore_DP0 pid=642417) 
(EngineCore_DP0 pid=642417) [2026-01-25 23:03:41] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=642417) [2026-01-25 23:03:41] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19906560 bytes
(EngineCore_DP0 pid=642417) [2026-01-25 23:03:41] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=642417) [2026-01-25 23:03:41] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15482880 bytes
(EngineCore_DP0 pid=642417) [2026-01-25 23:03:41] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=642417) [2026-01-25 23:03:42] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 163676160 bytes
(EngineCore_DP0 pid=642417) [2026-01-25 23:03:42] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=642417) [2026-01-25 23:03:42] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 81543168 bytes
(EngineCore_DP0 pid=642417) 2026-01-25 23:03:52,200 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=642417) 2026-01-25 23:03:52,220 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=642417) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|█▌        | 3/19 [00:00<00:00, 24.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  32%|███▏      | 6/19 [00:00<00:00, 27.02it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 9/19 [00:00<00:00, 26.88it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|██████▎   | 12/19 [00:00<00:00, 27.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  79%|███████▉  | 15/19 [00:00<00:00, 28.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:00<00:00, 28.11it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:00<00:00, 27.72it/s]
(EngineCore_DP0 pid=642417) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   9%|▉         | 1/11 [00:00<00:01,  7.14it/s]
Capturing CUDA graphs (decode, FULL):  45%|████▌     | 5/11 [00:00<00:00, 20.76it/s]
Capturing CUDA graphs (decode, FULL):  82%|████████▏ | 9/11 [00:00<00:00, 26.26it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:00<00:00, 24.58it/s]

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 64/64 [00:00<00:00, 4488.59it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 1/64 [00:03<03:39,  3.48s/it, est. speed input: 4.59 toks/s, output: 73.48 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:03<00:00,  3.48s/it, est. speed input: 288.74 toks/s, output: 4619.76 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:03<00:00, 18.05it/s, est. speed input: 288.74 toks/s, output: 4619.76 toks/s]
[rank0]:[W125 23:03:58.854928194 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 29.7s

测试结果:
  Requests/s:   17.97
  Tokens/s:     4887.25
  Total Reqs:   64
  Elapsed:      3.56s

  [Decode 分析]
  Total Decode Tokens:  16384
  Decode Tokens/s:      4599.76

============================================================
[2/4] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 2048 (= 128 x 16)
│   M_decode      = 128
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 128
│   --max-num-seqs           = 128
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 23:04:03 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=643108) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=643108) WARNING 01-25 23:04:13 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=643108) ERROR 01-25 23:04:20 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=643108) ERROR 01-25 23:04:20 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=643108) ERROR 01-25 23:04:20 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4302, in _dummy_sampler_run
(EngineCore_DP0 pid=643108) ERROR 01-25 23:04:20 [core.py:866]     sampler_output = self.sampler(
(EngineCore_DP0 pid=643108) ERROR 01-25 23:04:20 [core.py:866]                      ^^^^^^^^^^^^^
(EngineCore_DP0 pid=643108) ERROR 01-25 23:04:20 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=643108) ERROR 01-25 23:04:20 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=643108) ERROR 01-25 23:04:20 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=643108) ERROR 01-25 23:04:20 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=643108) ERROR 01-25 23:04:20 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=643108) ERROR 01-25 23:04:20 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=643108) ERROR 01-25 23:04:20 [core.py:866]   File "/root/vllmbench/vllm/v1/sample/sampler.py", line 96, in forward
(EngineCore_DP0 pid=643108) ERROR 01-25 23:04:20 [core.py:866]     sampled, processed_logprobs = self.sample(logits, sampling_metadata)
(EngineCore_DP0 pid=643108) ERROR 01-25 23:04:20 [core.py:866]                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=643108) ERROR 01-25 23:04:20 [core.py:866]   File "/root/vllmbench/vllm/v1/sample/sampler.py", line 187, in sample
(EngineCore_DP0 pid=643108) ERROR 01-25 23:04:20 [core.py:866]     random_sampled, processed_logprobs = self.topk_topp_sampler(
(EngineCore_DP0 pid=643108) ERROR 01-25 23:04:20 [core.py:866]                                          ^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=643108) ERROR 01-25 23:04:20 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=643108) ERROR 01-25 23:04:20 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=643108) ERROR 01-25 23:04:20 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=643108) ERROR 01-25 23:04:20 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=643108) ERROR 01-25 23:04:20 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=643108) ERROR 01-25 23:04:20 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=643108) ERROR 01-25 23:04:20 [core.py:866]   File "/root/vllmbench/vllm/v1/sample/ops/topk_topp_sampler.py", line 104, in forward_native
(EngineCore_DP0 pid=643108) ERROR 01-25 23:04:20 [core.py:866]     logits = self.apply_top_k_top_p(logits, k, p)
(EngineCore_DP0 pid=643108) ERROR 01-25 23:04:20 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=643108) ERROR 01-25 23:04:20 [core.py:866]   File "/root/vllmbench/vllm/v1/sample/ops/topk_topp_sampler.py", line 258, in apply_top_k_top_p
(EngineCore_DP0 pid=643108) ERROR 01-25 23:04:20 [core.py:866]     logits_sort, logits_idx = logits.sort(dim=-1, descending=False)
(EngineCore_DP0 pid=643108) ERROR 01-25 23:04:20 [core.py:866]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=643108) ERROR 01-25 23:04:20 [core.py:866] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacity of 15.46 GiB of which 56.94 MiB is free. Including non-PyTorch memory, this process has 15.19 GiB memory in use. Of the allocated memory 12.16 GiB is allocated by PyTorch, with 68.00 MiB allocated in private pools (e.g., CUDA Graphs), and 2.43 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
(EngineCore_DP0 pid=643108) ERROR 01-25 23:04:20 [core.py:866] 
(EngineCore_DP0 pid=643108) ERROR 01-25 23:04:20 [core.py:866] The above exception was the direct cause of the following exception:
(EngineCore_DP0 pid=643108) ERROR 01-25 23:04:20 [core.py:866] 
(EngineCore_DP0 pid=643108) ERROR 01-25 23:04:20 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=643108) ERROR 01-25 23:04:20 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=643108) ERROR 01-25 23:04:20 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=643108) ERROR 01-25 23:04:20 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=643108) ERROR 01-25 23:04:20 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=643108) ERROR 01-25 23:04:20 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=643108) ERROR 01-25 23:04:20 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=643108) ERROR 01-25 23:04:20 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=643108) ERROR 01-25 23:04:20 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=643108) ERROR 01-25 23:04:20 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 256, in _initialize_kv_caches
(EngineCore_DP0 pid=643108) ERROR 01-25 23:04:20 [core.py:866]     self.model_executor.initialize_from_config(kv_cache_configs)
(EngineCore_DP0 pid=643108) ERROR 01-25 23:04:20 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 116, in initialize_from_config
(EngineCore_DP0 pid=643108) ERROR 01-25 23:04:20 [core.py:866]     self.collective_rpc("compile_or_warm_up_model")
(EngineCore_DP0 pid=643108) ERROR 01-25 23:04:20 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=643108) ERROR 01-25 23:04:20 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=643108) ERROR 01-25 23:04:20 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=643108) ERROR 01-25 23:04:20 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=643108) ERROR 01-25 23:04:20 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=643108) ERROR 01-25 23:04:20 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=643108) ERROR 01-25 23:04:20 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 538, in compile_or_warm_up_model
(EngineCore_DP0 pid=643108) ERROR 01-25 23:04:20 [core.py:866]     self.model_runner._dummy_sampler_run(hidden_states=last_hidden_states)
(EngineCore_DP0 pid=643108) ERROR 01-25 23:04:20 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=643108) ERROR 01-25 23:04:20 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=643108) ERROR 01-25 23:04:20 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=643108) ERROR 01-25 23:04:20 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4307, in _dummy_sampler_run
(EngineCore_DP0 pid=643108) ERROR 01-25 23:04:20 [core.py:866]     raise RuntimeError(
(EngineCore_DP0 pid=643108) ERROR 01-25 23:04:20 [core.py:866] RuntimeError: CUDA out of memory occurred when warming up sampler with 128 dummy requests. Please try lowering `max_num_seqs` or `gpu_memory_utilization` when initializing the engine.


─── STDERR ───
[2026-01-25 23:04:03] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 23:04:03] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-25 23:04:03] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-25 23:04:03] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:04:03] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:04:03] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:04:03] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:04:03] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:04:03] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-25 23:04:03] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 23:04:03] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 23:04:03] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 23:04:03] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 23:04:03] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 23:04:07] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 23:04:07] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-25 23:04:07] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-25 23:04:07] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:04:07] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:04:07] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:04:07] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:04:07] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:04:07] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-25 23:04:07] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 23:04:07] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 23:04:07] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 23:04:07] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 23:04:07] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=643108) [2026-01-25 23:04:07] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=643108) [2026-01-25 23:04:07] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=643108) [2026-01-25 23:04:07] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=643108) [2026-01-25 23:04:07] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=643108) [2026-01-25 23:04:07] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=643108) [2026-01-25 23:04:07] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=643108) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=643108) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.14it/s]
(EngineCore_DP0 pid=643108) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.58it/s]
(EngineCore_DP0 pid=643108) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.64it/s]
(EngineCore_DP0 pid=643108) 
(EngineCore_DP0 pid=643108) [2026-01-25 23:04:09] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=643108) [2026-01-25 23:04:09] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19906560 bytes
(EngineCore_DP0 pid=643108) [2026-01-25 23:04:09] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=643108) [2026-01-25 23:04:09] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15482880 bytes
(EngineCore_DP0 pid=643108) [2026-01-25 23:04:09] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=643108) [2026-01-25 23:04:09] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 163676160 bytes
(EngineCore_DP0 pid=643108) [2026-01-25 23:04:09] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=643108) [2026-01-25 23:04:09] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 81543168 bytes
(EngineCore_DP0 pid=643108) 2026-01-25 23:04:17,593 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=643108) 2026-01-25 23:04:17,607 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=643108) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▊         | 3/35 [00:00<00:01, 24.74it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 6/35 [00:00<00:01, 25.48it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▌       | 9/35 [00:00<00:01, 16.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 11/35 [00:00<00:01, 14.90it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 14/35 [00:00<00:01, 18.32it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|████▊     | 17/35 [00:00<00:00, 20.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 20/35 [00:00<00:00, 22.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  66%|██████▌   | 23/35 [00:01<00:00, 24.48it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  77%|███████▋  | 27/35 [00:01<00:00, 26.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▊ | 31/35 [00:01<00:00, 27.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:01<00:00, 27.42it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:01<00:00, 23.34it/s]
(EngineCore_DP0 pid=643108) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   5%|▌         | 1/19 [00:00<00:02,  7.08it/s]
Capturing CUDA graphs (decode, FULL):  26%|██▋       | 5/19 [00:00<00:00, 21.15it/s]
Capturing CUDA graphs (decode, FULL):  47%|████▋     | 9/19 [00:00<00:00, 26.12it/s]
Capturing CUDA graphs (decode, FULL):  68%|██████▊   | 13/19 [00:00<00:00, 28.81it/s]
Capturing CUDA graphs (decode, FULL):  89%|████████▉ | 17/19 [00:00<00:00, 30.32it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:00<00:00, 27.50it/s]
(EngineCore_DP0 pid=643108) Process EngineCore_DP0:
(EngineCore_DP0 pid=643108) Traceback (most recent call last):
(EngineCore_DP0 pid=643108)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4302, in _dummy_sampler_run
(EngineCore_DP0 pid=643108)     sampler_output = self.sampler(
(EngineCore_DP0 pid=643108)                      ^^^^^^^^^^^^^
(EngineCore_DP0 pid=643108)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=643108)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=643108)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=643108)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=643108)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=643108)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=643108)   File "/root/vllmbench/vllm/v1/sample/sampler.py", line 96, in forward
(EngineCore_DP0 pid=643108)     sampled, processed_logprobs = self.sample(logits, sampling_metadata)
(EngineCore_DP0 pid=643108)                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=643108)   File "/root/vllmbench/vllm/v1/sample/sampler.py", line 187, in sample
(EngineCore_DP0 pid=643108)     random_sampled, processed_logprobs = self.topk_topp_sampler(
(EngineCore_DP0 pid=643108)                                          ^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=643108)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=643108)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=643108)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=643108)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=643108)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=643108)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=643108)   File "/root/vllmbench/vllm/v1/sample/ops/topk_topp_sampler.py", line 104, in forward_native
(EngineCore_DP0 pid=643108)     logits = self.apply_top_k_top_p(logits, k, p)
(EngineCore_DP0 pid=643108)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=643108)   File "/root/vllmbench/vllm/v1/sample/ops/topk_topp_sampler.py", line 258, in apply_top_k_top_p
(EngineCore_DP0 pid=643108)     logits_sort, logits_idx = logits.sort(dim=-1, descending=False)
(EngineCore_DP0 pid=643108)                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=643108) torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacity of 15.46 GiB of which 56.94 MiB is free. Including non-PyTorch memory, this process has 15.19 GiB memory in use. Of the allocated memory 12.16 GiB is allocated by PyTorch, with 68.00 MiB allocated in private pools (e.g., CUDA Graphs), and 2.43 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
(EngineCore_DP0 pid=643108) 
(EngineCore_DP0 pid=643108) The above exception was the direct cause of the following exception:
(EngineCore_DP0 pid=643108) 
(EngineCore_DP0 pid=643108) Traceback (most recent call last):
(EngineCore_DP0 pid=643108)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=643108)     self.run()
(EngineCore_DP0 pid=643108)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=643108)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=643108)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=643108)     raise e
(EngineCore_DP0 pid=643108)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=643108)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=643108)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=643108)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=643108)     super().__init__(
(EngineCore_DP0 pid=643108)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=643108)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=643108)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=643108)   File "/root/vllmbench/vllm/v1/engine/core.py", line 256, in _initialize_kv_caches
(EngineCore_DP0 pid=643108)     self.model_executor.initialize_from_config(kv_cache_configs)
(EngineCore_DP0 pid=643108)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 116, in initialize_from_config
(EngineCore_DP0 pid=643108)     self.collective_rpc("compile_or_warm_up_model")
(EngineCore_DP0 pid=643108)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=643108)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=643108)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=643108)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=643108)     return func(*args, **kwargs)
(EngineCore_DP0 pid=643108)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=643108)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 538, in compile_or_warm_up_model
(EngineCore_DP0 pid=643108)     self.model_runner._dummy_sampler_run(hidden_states=last_hidden_states)
(EngineCore_DP0 pid=643108)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=643108)     return func(*args, **kwargs)
(EngineCore_DP0 pid=643108)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=643108)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4307, in _dummy_sampler_run
(EngineCore_DP0 pid=643108)     raise RuntimeError(
(EngineCore_DP0 pid=643108) RuntimeError: CUDA out of memory occurred when warming up sampler with 128 dummy requests. Please try lowering `max_num_seqs` or `gpu_memory_utilization` when initializing the engine.
[rank0]:[W125 23:04:20.472786998 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[ERROR] 测试失败: M=128 (exit code: 1)

============================================================
[3/4] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 4096 (= 256 x 16)
│   M_decode      = 256
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 256
│   --max-num-seqs           = 256
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 23:04:26 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=643676) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=643676) WARNING 01-25 23:04:37 [backends.py:609] Failed to read file <frozen os>
Throughput: 23.13 requests/s, 6291.55 total tokens/s, 5921.46 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536


─── STDERR ───
[2026-01-25 23:04:26] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 23:04:26] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-25 23:04:26] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-25 23:04:26] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:04:26] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:04:26] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:04:26] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:04:26] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:04:26] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-25 23:04:26] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 23:04:26] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 23:04:26] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 23:04:26] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 23:04:26] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 23:04:30] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 23:04:30] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-25 23:04:30] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-25 23:04:30] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:04:30] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:04:30] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:04:30] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:04:30] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:04:30] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-25 23:04:30] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 23:04:30] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 23:04:30] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 23:04:30] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 23:04:30] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=643676) [2026-01-25 23:04:31] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=643676) [2026-01-25 23:04:31] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=643676) [2026-01-25 23:04:31] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=643676) [2026-01-25 23:04:31] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=643676) [2026-01-25 23:04:31] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=643676) [2026-01-25 23:04:31] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=643676) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=643676) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.12it/s]
(EngineCore_DP0 pid=643676) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.57it/s]
(EngineCore_DP0 pid=643676) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.63it/s]
(EngineCore_DP0 pid=643676) 
(EngineCore_DP0 pid=643676) [2026-01-25 23:04:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=643676) [2026-01-25 23:04:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19906560 bytes
(EngineCore_DP0 pid=643676) [2026-01-25 23:04:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=643676) [2026-01-25 23:04:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15482880 bytes
(EngineCore_DP0 pid=643676) [2026-01-25 23:04:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=643676) [2026-01-25 23:04:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 163676160 bytes
(EngineCore_DP0 pid=643676) [2026-01-25 23:04:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=643676) [2026-01-25 23:04:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 81543168 bytes
(EngineCore_DP0 pid=643676) 2026-01-25 23:04:41,309 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=643676) 2026-01-25 23:04:41,323 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=643676) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/36 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 3/36 [00:00<00:01, 26.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 6/36 [00:00<00:01, 26.54it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 9/36 [00:00<00:00, 27.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 12/36 [00:00<00:00, 27.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 15/36 [00:00<00:00, 21.08it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 18/36 [00:00<00:01, 14.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 21/36 [00:01<00:00, 16.84it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 24/36 [00:01<00:00, 19.45it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 28/36 [00:01<00:00, 22.34it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 32/36 [00:01<00:00, 24.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:01<00:00, 25.15it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:01<00:00, 22.31it/s]
(EngineCore_DP0 pid=643676) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   3%|▎         | 1/35 [00:00<00:04,  7.31it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 5/35 [00:00<00:01, 20.58it/s]
Capturing CUDA graphs (decode, FULL):  26%|██▌       | 9/35 [00:00<00:01, 25.10it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 13/35 [00:00<00:00, 27.52it/s]
Capturing CUDA graphs (decode, FULL):  49%|████▊     | 17/35 [00:00<00:00, 29.08it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 21/35 [00:00<00:00, 30.08it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 25/35 [00:00<00:00, 30.80it/s]
Capturing CUDA graphs (decode, FULL):  83%|████████▎ | 29/35 [00:01<00:00, 30.47it/s]
Capturing CUDA graphs (decode, FULL):  94%|█████████▍| 33/35 [00:01<00:00, 30.52it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:01<00:00, 28.53it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 7932.43it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:05<22:56,  5.40s/it, est. speed input: 2.96 toks/s, output: 47.42 toks/s]
Processed prompts:  34%|███▍      | 87/256 [00:05<00:07, 22.28it/s, est. speed input: 252.59 toks/s, output: 4041.45 toks/s]
Processed prompts:  52%|█████▏    | 133/256 [00:07<00:05, 22.10it/s, est. speed input: 279.46 toks/s, output: 4471.39 toks/s]
Processed prompts:  62%|██████▏   | 159/256 [00:08<00:04, 22.17it/s, est. speed input: 289.89 toks/s, output: 4638.18 toks/s]
Processed prompts:  69%|██████▉   | 176/256 [00:09<00:03, 23.80it/s, est. speed input: 303.66 toks/s, output: 4858.60 toks/s]
Processed prompts:  73%|███████▎  | 188/256 [00:09<00:02, 25.37it/s, est. speed input: 313.77 toks/s, output: 5020.25 toks/s]
Processed prompts:  77%|███████▋  | 198/256 [00:09<00:02, 26.83it/s, est. speed input: 321.63 toks/s, output: 5146.11 toks/s]
Processed prompts:  80%|████████  | 206/256 [00:10<00:01, 28.43it/s, est. speed input: 328.25 toks/s, output: 5252.05 toks/s]
Processed prompts:  83%|████████▎ | 213/256 [00:10<00:01, 30.36it/s, est. speed input: 334.38 toks/s, output: 5350.02 toks/s]
Processed prompts:  86%|████████▌ | 219/256 [00:10<00:01, 32.41it/s, est. speed input: 339.70 toks/s, output: 5435.20 toks/s]
Processed prompts:  88%|████████▊ | 225/256 [00:10<00:00, 33.80it/s, est. speed input: 344.14 toks/s, output: 5506.29 toks/s]
Processed prompts:  90%|█████████ | 231/256 [00:10<00:00, 36.36it/s, est. speed input: 349.33 toks/s, output: 5589.25 toks/s]
Processed prompts:  93%|█████████▎| 237/256 [00:10<00:00, 39.64it/s, est. speed input: 354.81 toks/s, output: 5676.90 toks/s]
Processed prompts:  95%|█████████▍| 243/256 [00:10<00:00, 43.06it/s, est. speed input: 360.29 toks/s, output: 5764.68 toks/s]
Processed prompts:  97%|█████████▋| 249/256 [00:10<00:00, 45.44it/s, est. speed input: 365.41 toks/s, output: 5846.48 toks/s]
Processed prompts: 100%|█████████▉| 255/256 [00:11<00:00, 46.57it/s, est. speed input: 370.12 toks/s, output: 5921.96 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:11<00:00, 46.57it/s, est. speed input: 371.21 toks/s, output: 5939.28 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:11<00:00, 23.20it/s, est. speed input: 371.21 toks/s, output: 5939.28 toks/s]
[rank0]:[W125 23:04:56.097582121 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 35.0s

测试结果:
  Requests/s:   23.13
  Tokens/s:     6291.55
  Total Reqs:   256
  Elapsed:      11.07s

  [Decode 分析]
  Total Decode Tokens:  65536
  Decode Tokens/s:      5921.46

============================================================
[4/4] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-INT8                                 │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 8192 (= 512 x 16)
│   M_decode      = 512
│   batched_tokens = 512 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 512
│   --max-num-seqs           = 512
│   --max-model-len          = 272
│   --max-num-batched-tokens = 512
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 23:05:01 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=644435) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=644435) WARNING 01-25 23:05:11 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=644435) ERROR 01-25 23:05:17 [core.py:866] EngineCore failed to start.
(EngineCore_DP0 pid=644435) ERROR 01-25 23:05:17 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=644435) ERROR 01-25 23:05:17 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4302, in _dummy_sampler_run
(EngineCore_DP0 pid=644435) ERROR 01-25 23:05:17 [core.py:866]     sampler_output = self.sampler(
(EngineCore_DP0 pid=644435) ERROR 01-25 23:05:17 [core.py:866]                      ^^^^^^^^^^^^^
(EngineCore_DP0 pid=644435) ERROR 01-25 23:05:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=644435) ERROR 01-25 23:05:17 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=644435) ERROR 01-25 23:05:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=644435) ERROR 01-25 23:05:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=644435) ERROR 01-25 23:05:17 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=644435) ERROR 01-25 23:05:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=644435) ERROR 01-25 23:05:17 [core.py:866]   File "/root/vllmbench/vllm/v1/sample/sampler.py", line 96, in forward
(EngineCore_DP0 pid=644435) ERROR 01-25 23:05:17 [core.py:866]     sampled, processed_logprobs = self.sample(logits, sampling_metadata)
(EngineCore_DP0 pid=644435) ERROR 01-25 23:05:17 [core.py:866]                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=644435) ERROR 01-25 23:05:17 [core.py:866]   File "/root/vllmbench/vllm/v1/sample/sampler.py", line 187, in sample
(EngineCore_DP0 pid=644435) ERROR 01-25 23:05:17 [core.py:866]     random_sampled, processed_logprobs = self.topk_topp_sampler(
(EngineCore_DP0 pid=644435) ERROR 01-25 23:05:17 [core.py:866]                                          ^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=644435) ERROR 01-25 23:05:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=644435) ERROR 01-25 23:05:17 [core.py:866]     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=644435) ERROR 01-25 23:05:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=644435) ERROR 01-25 23:05:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=644435) ERROR 01-25 23:05:17 [core.py:866]     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=644435) ERROR 01-25 23:05:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=644435) ERROR 01-25 23:05:17 [core.py:866]   File "/root/vllmbench/vllm/v1/sample/ops/topk_topp_sampler.py", line 104, in forward_native
(EngineCore_DP0 pid=644435) ERROR 01-25 23:05:17 [core.py:866]     logits = self.apply_top_k_top_p(logits, k, p)
(EngineCore_DP0 pid=644435) ERROR 01-25 23:05:17 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=644435) ERROR 01-25 23:05:17 [core.py:866]   File "/root/vllmbench/vllm/v1/sample/ops/topk_topp_sampler.py", line 258, in apply_top_k_top_p
(EngineCore_DP0 pid=644435) ERROR 01-25 23:05:17 [core.py:866]     logits_sort, logits_idx = logits.sort(dim=-1, descending=False)
(EngineCore_DP0 pid=644435) ERROR 01-25 23:05:17 [core.py:866]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=644435) ERROR 01-25 23:05:17 [core.py:866] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 892.00 MiB. GPU 0 has a total capacity of 15.46 GiB of which 814.94 MiB is free. Including non-PyTorch memory, this process has 14.45 GiB memory in use. Of the allocated memory 11.33 GiB is allocated by PyTorch, and 2.74 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
(EngineCore_DP0 pid=644435) ERROR 01-25 23:05:17 [core.py:866] 
(EngineCore_DP0 pid=644435) ERROR 01-25 23:05:17 [core.py:866] The above exception was the direct cause of the following exception:
(EngineCore_DP0 pid=644435) ERROR 01-25 23:05:17 [core.py:866] 
(EngineCore_DP0 pid=644435) ERROR 01-25 23:05:17 [core.py:866] Traceback (most recent call last):
(EngineCore_DP0 pid=644435) ERROR 01-25 23:05:17 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=644435) ERROR 01-25 23:05:17 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=644435) ERROR 01-25 23:05:17 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=644435) ERROR 01-25 23:05:17 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=644435) ERROR 01-25 23:05:17 [core.py:866]     super().__init__(
(EngineCore_DP0 pid=644435) ERROR 01-25 23:05:17 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=644435) ERROR 01-25 23:05:17 [core.py:866]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=644435) ERROR 01-25 23:05:17 [core.py:866]                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=644435) ERROR 01-25 23:05:17 [core.py:866]   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=644435) ERROR 01-25 23:05:17 [core.py:866]     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=644435) ERROR 01-25 23:05:17 [core.py:866]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=644435) ERROR 01-25 23:05:17 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=644435) ERROR 01-25 23:05:17 [core.py:866]     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=644435) ERROR 01-25 23:05:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=644435) ERROR 01-25 23:05:17 [core.py:866]   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=644435) ERROR 01-25 23:05:17 [core.py:866]     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=644435) ERROR 01-25 23:05:17 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=644435) ERROR 01-25 23:05:17 [core.py:866]   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=644435) ERROR 01-25 23:05:17 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=644435) ERROR 01-25 23:05:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=644435) ERROR 01-25 23:05:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=644435) ERROR 01-25 23:05:17 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=644435) ERROR 01-25 23:05:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=644435) ERROR 01-25 23:05:17 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=644435) ERROR 01-25 23:05:17 [core.py:866]     self.model_runner.profile_run()
(EngineCore_DP0 pid=644435) ERROR 01-25 23:05:17 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4481, in profile_run
(EngineCore_DP0 pid=644435) ERROR 01-25 23:05:17 [core.py:866]     output = self._dummy_sampler_run(last_hidden_states)
(EngineCore_DP0 pid=644435) ERROR 01-25 23:05:17 [core.py:866]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=644435) ERROR 01-25 23:05:17 [core.py:866]   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=644435) ERROR 01-25 23:05:17 [core.py:866]     return func(*args, **kwargs)
(EngineCore_DP0 pid=644435) ERROR 01-25 23:05:17 [core.py:866]            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=644435) ERROR 01-25 23:05:17 [core.py:866]   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4307, in _dummy_sampler_run
(EngineCore_DP0 pid=644435) ERROR 01-25 23:05:17 [core.py:866]     raise RuntimeError(
(EngineCore_DP0 pid=644435) ERROR 01-25 23:05:17 [core.py:866] RuntimeError: CUDA out of memory occurred when warming up sampler with 512 dummy requests. Please try lowering `max_num_seqs` or `gpu_memory_utilization` when initializing the engine.


─── STDERR ───
[2026-01-25 23:05:01] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 23:05:01] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-25 23:05:01] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-25 23:05:01] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:05:01] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:05:01] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:05:01] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:05:01] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:05:01] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-25 23:05:01] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 23:05:01] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 23:05:01] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 23:05:01] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 23:05:01] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 23:05:05] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-25 23:05:05] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-INT8'
[2026-01-25 23:05:05] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-INT8
[2026-01-25 23:05:05] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:05:05] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:05:05] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:05:05] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:05:05] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-INT8
[2026-01-25 23:05:05] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-INT8'
[2026-01-25 23:05:05] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-25 23:05:05] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 23:05:05] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 23:05:05] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 23:05:05] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=644435) [2026-01-25 23:05:05] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=644435) [2026-01-25 23:05:05] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=644435) [2026-01-25 23:05:05] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=644435) [2026-01-25 23:05:05] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=644435) [2026-01-25 23:05:05] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-7B-INT8
(EngineCore_DP0 pid=644435) [2026-01-25 23:05:05] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=644435) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=644435) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.10it/s]
(EngineCore_DP0 pid=644435) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.56it/s]
(EngineCore_DP0 pid=644435) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.62it/s]
(EngineCore_DP0 pid=644435) 
(EngineCore_DP0 pid=644435) [2026-01-25 23:05:07] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=644435) [2026-01-25 23:05:07] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19906560 bytes
(EngineCore_DP0 pid=644435) [2026-01-25 23:05:07] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=644435) [2026-01-25 23:05:07] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15482880 bytes
(EngineCore_DP0 pid=644435) [2026-01-25 23:05:07] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=644435) [2026-01-25 23:05:07] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 163676160 bytes
(EngineCore_DP0 pid=644435) [2026-01-25 23:05:07] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=644435) [2026-01-25 23:05:07] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 81543168 bytes
(EngineCore_DP0 pid=644435) Process EngineCore_DP0:
(EngineCore_DP0 pid=644435) Traceback (most recent call last):
(EngineCore_DP0 pid=644435)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4302, in _dummy_sampler_run
(EngineCore_DP0 pid=644435)     sampler_output = self.sampler(
(EngineCore_DP0 pid=644435)                      ^^^^^^^^^^^^^
(EngineCore_DP0 pid=644435)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=644435)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=644435)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=644435)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=644435)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=644435)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=644435)   File "/root/vllmbench/vllm/v1/sample/sampler.py", line 96, in forward
(EngineCore_DP0 pid=644435)     sampled, processed_logprobs = self.sample(logits, sampling_metadata)
(EngineCore_DP0 pid=644435)                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=644435)   File "/root/vllmbench/vllm/v1/sample/sampler.py", line 187, in sample
(EngineCore_DP0 pid=644435)     random_sampled, processed_logprobs = self.topk_topp_sampler(
(EngineCore_DP0 pid=644435)                                          ^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=644435)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
(EngineCore_DP0 pid=644435)     return self._call_impl(*args, **kwargs)
(EngineCore_DP0 pid=644435)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=644435)   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1786, in _call_impl
(EngineCore_DP0 pid=644435)     return forward_call(*args, **kwargs)
(EngineCore_DP0 pid=644435)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=644435)   File "/root/vllmbench/vllm/v1/sample/ops/topk_topp_sampler.py", line 104, in forward_native
(EngineCore_DP0 pid=644435)     logits = self.apply_top_k_top_p(logits, k, p)
(EngineCore_DP0 pid=644435)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=644435)   File "/root/vllmbench/vllm/v1/sample/ops/topk_topp_sampler.py", line 258, in apply_top_k_top_p
(EngineCore_DP0 pid=644435)     logits_sort, logits_idx = logits.sort(dim=-1, descending=False)
(EngineCore_DP0 pid=644435)                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=644435) torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 892.00 MiB. GPU 0 has a total capacity of 15.46 GiB of which 814.94 MiB is free. Including non-PyTorch memory, this process has 14.45 GiB memory in use. Of the allocated memory 11.33 GiB is allocated by PyTorch, and 2.74 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
(EngineCore_DP0 pid=644435) 
(EngineCore_DP0 pid=644435) The above exception was the direct cause of the following exception:
(EngineCore_DP0 pid=644435) 
(EngineCore_DP0 pid=644435) Traceback (most recent call last):
(EngineCore_DP0 pid=644435)   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
(EngineCore_DP0 pid=644435)     self.run()
(EngineCore_DP0 pid=644435)   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
(EngineCore_DP0 pid=644435)     self._target(*self._args, **self._kwargs)
(EngineCore_DP0 pid=644435)   File "/root/vllmbench/vllm/v1/engine/core.py", line 870, in run_engine_core
(EngineCore_DP0 pid=644435)     raise e
(EngineCore_DP0 pid=644435)   File "/root/vllmbench/vllm/v1/engine/core.py", line 857, in run_engine_core
(EngineCore_DP0 pid=644435)     engine_core = EngineCoreProc(*args, **kwargs)
(EngineCore_DP0 pid=644435)                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=644435)   File "/root/vllmbench/vllm/v1/engine/core.py", line 637, in __init__
(EngineCore_DP0 pid=644435)     super().__init__(
(EngineCore_DP0 pid=644435)   File "/root/vllmbench/vllm/v1/engine/core.py", line 109, in __init__
(EngineCore_DP0 pid=644435)     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
(EngineCore_DP0 pid=644435)                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=644435)   File "/root/vllmbench/vllm/v1/engine/core.py", line 240, in _initialize_kv_caches
(EngineCore_DP0 pid=644435)     available_gpu_memory = self.model_executor.determine_available_memory()
(EngineCore_DP0 pid=644435)                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=644435)   File "/root/vllmbench/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
(EngineCore_DP0 pid=644435)     return self.collective_rpc("determine_available_memory")
(EngineCore_DP0 pid=644435)            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=644435)   File "/root/vllmbench/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
(EngineCore_DP0 pid=644435)     result = run_method(self.driver_worker, method, args, kwargs)
(EngineCore_DP0 pid=644435)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=644435)   File "/root/vllmbench/vllm/v1/serial_utils.py", line 461, in run_method
(EngineCore_DP0 pid=644435)     return func(*args, **kwargs)
(EngineCore_DP0 pid=644435)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=644435)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=644435)     return func(*args, **kwargs)
(EngineCore_DP0 pid=644435)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=644435)   File "/root/vllmbench/vllm/v1/worker/gpu_worker.py", line 340, in determine_available_memory
(EngineCore_DP0 pid=644435)     self.model_runner.profile_run()
(EngineCore_DP0 pid=644435)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4481, in profile_run
(EngineCore_DP0 pid=644435)     output = self._dummy_sampler_run(last_hidden_states)
(EngineCore_DP0 pid=644435)              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=644435)   File "/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py", line 120, in decorate_context
(EngineCore_DP0 pid=644435)     return func(*args, **kwargs)
(EngineCore_DP0 pid=644435)            ^^^^^^^^^^^^^^^^^^^^^
(EngineCore_DP0 pid=644435)   File "/root/vllmbench/vllm/v1/worker/gpu_model_runner.py", line 4307, in _dummy_sampler_run
(EngineCore_DP0 pid=644435)     raise RuntimeError(
(EngineCore_DP0 pid=644435) RuntimeError: CUDA out of memory occurred when warming up sampler with 512 dummy requests. Please try lowering `max_num_seqs` or `gpu_memory_utilization` when initializing the engine.
[rank0]:[W125 23:05:17.558032947 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
Traceback (most recent call last):
  File "/usr/local/bin/vllm", line 7, in <module>
    sys.exit(main())
             ^^^^^^
  File "/root/vllmbench/vllm/entrypoints/cli/main.py", line 73, in main
    args.dispatch_function(args)
  File "/root/vllmbench/vllm/entrypoints/cli/benchmark/throughput.py", line 21, in cmd
    main(args)
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 730, in main
    elapsed_time, request_outputs = run_vllm(
                                    ^^^^^^^^^
  File "/root/vllmbench/vllm/benchmarks/throughput.py", line 51, in run_vllm
    llm = LLM(**dataclasses.asdict(engine_args))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/entrypoints/llm.py", line 351, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 183, in from_engine_args
    return cls(
           ^^^^
  File "/root/vllmbench/vllm/v1/engine/llm_engine.py", line 109, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 648, in __init__
    super().__init__(
  File "/root/vllmbench/vllm/v1/engine/core_client.py", line 477, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 903, in launch_core_engines
    wait_for_engine_startup(
  File "/root/vllmbench/vllm/v1/engine/utils.py", line 960, in wait_for_engine_startup
    raise RuntimeError(
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}

[ERROR] 测试失败: M=512 (exit code: 1)


------------------------------------------------------------
  生成 CSV: Qwen2.5-7B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_INT8_py312_cu129_x86_64/cusparselt/2_10/Qwen2.5-7B-INT8_decode.csv

预览:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
64,16,64,64,256,256,17.9678,4887.2456,3.5619
128,16,128,128,256,256,-1.0000,-1.0000,-1.0000
256,16,256,256,256,256,23.1307,6291.5465,11.0675
512,16,512,512,256,256,-1.0000,-1.0000,-1.0000

------------------------------------------------------------

[INFO] 完成: 2 成功, 2 失败


============================================================
  Benchmark 完成!
============================================================


总计: 15 成功, 5 失败
============================================================
