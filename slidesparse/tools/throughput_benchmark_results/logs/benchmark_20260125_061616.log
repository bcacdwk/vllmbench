======================================================================
SlideSparse vLLM Throughput Benchmark Log
Created: 2026-01-25 06:16:16
======================================================================

原始命令:
  throughput_benchmark.py --model qwen2.5-0.5b-fp8 --M quick

命令行参数:
  --model: qwen2.5-0.5b-fp8
  --backend: None
  --sparsity: None
  --stage: None
  --M: quick
  --N: None
  --inner-32: False
  --eager: False
  --gpu-id: 0
  --gpu-mem: 0.8
  --dry-run: False
  --list-models: False

硬件信息:
  GPU: RTX5080
  Compute Capability: cc120
  VRAM: 15.5 GB
  CUDA: 12.9
  Python: py312

Backend 环境变量 (初始状态):
  DISABLE_SLIDESPARSE: 未设置
  USE_CUBLASLT: 未设置
  USE_CUSPARSELT: 未设置
  SPARSITY: 未设置
  INNER_DTYPE_32: 未设置

======================================================================


============================================================
  Qwen2.5-0.5B-FP8 | CUTLASS | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints/Qwen2.5-0.5B-FP8
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cutlass

============================================================
[1/3] 测试 M=16
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-0.5B-FP8                                │
│ Backend:  CUTLASS (SlideSparse fallback)                  │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16
│   M_prefill     = 16 (= 1 x 16)
│   M_decode      = 1
│   batched_tokens = 17 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 17
│   --max-num-batched-tokens = 17
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 06:16:19 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
Throughput: 218.33 requests/s, 3711.54 total tokens/s, 218.33 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  128


─── STDERR ───
[2026-01-25 06:16:19] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:16:19] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 06:16:19] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:16:19] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 06:16:19] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:16:19] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:16:19] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:16:19] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:16:19] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:16:19] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:16:19] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:16:19] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:16:19] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:16:19] INFO kernels.py:743: Preloaded 10 Triton kernels from RTX5080_cc120_py312_cu129_x86_64
[2026-01-25 06:16:19] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=2, cuSPARSELt=2 models
[2026-01-25 06:16:19] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:16:19] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:16:19] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:16:19] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 06:16:23] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:16:23] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 06:16:23] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:16:23] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 06:16:23] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:16:23] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:16:23] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:16:23] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:16:23] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:16:23] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:16:23] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:16:23] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:16:23] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:16:23] INFO kernels.py:743: Preloaded 10 Triton kernels from RTX5080_cc120_py312_cu129_x86_64
[2026-01-25 06:16:23] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=2, cuSPARSELt=2 models
[2026-01-25 06:16:23] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:16:23] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:16:23] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:16:23] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1094382) [2026-01-25 06:16:24] INFO SlideSparseLinearMethod_FP8.py:730: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (CUTLASS)
(EngineCore_DP0 pid=1094382) [2026-01-25 06:16:24] INFO SlideSparseLinearMethod_FP8.py:388: SlideSparseFp8LinearOp initialized (kernel=CUTLASS)
(EngineCore_DP0 pid=1094382) [2026-01-25 06:16:24] INFO SlideSparseLinearMethod_FP8.py:556: SlideSparseFp8LinearMethod initialized, kernel=CUTLASS
(EngineCore_DP0 pid=1094382) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1094382) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 14.29it/s]
(EngineCore_DP0 pid=1094382) 
(EngineCore_DP0 pid=1094382) 2026-01-25 06:16:35,628 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1094382) 2026-01-25 06:16:35,633 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1094382) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 87.70it/s]
(EngineCore_DP0 pid=1094382) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.81it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.80it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 3513.21it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:00<00:00, 187.49it/s, est. speed input: 3000.10 toks/s, output: 187.49 toks/s]
Processed prompts:  34%|███▍      | 44/128 [00:00<00:00, 221.59it/s, est. speed input: 3463.86 toks/s, output: 216.49 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:00<00:00, 231.90it/s, est. speed input: 3611.82 toks/s, output: 225.74 toks/s]
Processed prompts:  73%|███████▎  | 94/128 [00:00<00:00, 236.04it/s, est. speed input: 3678.81 toks/s, output: 229.92 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:00<00:00, 239.13it/s, est. speed input: 3725.78 toks/s, output: 232.86 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 239.13it/s, est. speed input: 3736.63 toks/s, output: 233.54 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 233.50it/s, est. speed input: 3736.63 toks/s, output: 233.54 toks/s]
[rank0]:[W125 06:16:37.349198465 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 22.5s

测试结果:
  Requests/s:   218.33
  Tokens/s:     3711.54
  Total Reqs:   128
  Elapsed:      0.59s

  [Prefill 分析]
  Total Prefill Tokens: 2048
  Prefill Tokens/s:     3493.21

============================================================
[2/3] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-0.5B-FP8                                │
│ Backend:  CUTLASS (SlideSparse fallback)                  │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 128 (= 1 x 128)
│   M_decode      = 1
│   batched_tokens = 129 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 128
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 129
│   --max-num-batched-tokens = 129
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 06:16:42 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
Throughput: 215.20 requests/s, 27761.12 total tokens/s, 215.20 output tokens/s
Total num prompt tokens:  16384
Total num output tokens:  128


─── STDERR ───
[2026-01-25 06:16:42] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:16:42] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 06:16:42] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:16:42] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 06:16:42] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:16:42] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:16:42] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:16:42] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:16:42] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:16:42] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:16:42] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:16:42] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:16:42] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:16:42] INFO kernels.py:743: Preloaded 10 Triton kernels from RTX5080_cc120_py312_cu129_x86_64
[2026-01-25 06:16:42] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=2, cuSPARSELt=2 models
[2026-01-25 06:16:42] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:16:42] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:16:42] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:16:42] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 06:16:46] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:16:46] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 06:16:46] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:16:46] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 06:16:46] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:16:46] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:16:46] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:16:46] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:16:46] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:16:46] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:16:46] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:16:46] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:16:46] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:16:46] INFO kernels.py:743: Preloaded 10 Triton kernels from RTX5080_cc120_py312_cu129_x86_64
[2026-01-25 06:16:46] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=2, cuSPARSELt=2 models
[2026-01-25 06:16:46] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:16:46] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:16:46] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:16:46] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1094838) [2026-01-25 06:16:47] INFO SlideSparseLinearMethod_FP8.py:730: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (CUTLASS)
(EngineCore_DP0 pid=1094838) [2026-01-25 06:16:47] INFO SlideSparseLinearMethod_FP8.py:388: SlideSparseFp8LinearOp initialized (kernel=CUTLASS)
(EngineCore_DP0 pid=1094838) [2026-01-25 06:16:47] INFO SlideSparseLinearMethod_FP8.py:556: SlideSparseFp8LinearMethod initialized, kernel=CUTLASS
(EngineCore_DP0 pid=1094838) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1094838) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 14.18it/s]
(EngineCore_DP0 pid=1094838) 
(EngineCore_DP0 pid=1094838) 2026-01-25 06:16:56,986 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1094838) 2026-01-25 06:16:56,990 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1094838) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 38.14it/s]
(EngineCore_DP0 pid=1094838) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.01it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.00it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 2106.34it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  19%|█▉        | 24/128 [00:00<00:00, 233.67it/s, est. speed input: 29912.92 toks/s, output: 233.68 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:00<00:00, 238.90it/s, est. speed input: 30479.18 toks/s, output: 238.11 toks/s]
Processed prompts:  58%|█████▊    | 74/128 [00:00<00:00, 240.81it/s, est. speed input: 30690.76 toks/s, output: 239.77 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:00<00:00, 241.16it/s, est. speed input: 30752.39 toks/s, output: 240.25 toks/s]
Processed prompts:  97%|█████████▋| 124/128 [00:00<00:00, 241.20it/s, est. speed input: 30778.84 toks/s, output: 240.46 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 241.20it/s, est. speed input: 30786.08 toks/s, output: 240.51 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 240.49it/s, est. speed input: 30786.08 toks/s, output: 240.51 toks/s]
[rank0]:[W125 06:16:58.689252419 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 21.4s

测试结果:
  Requests/s:   215.20
  Tokens/s:     27761.12
  Total Reqs:   128
  Elapsed:      0.59s

  [Prefill 分析]
  Total Prefill Tokens: 16384
  Prefill Tokens/s:     27545.92

============================================================
[3/3] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-0.5B-FP8                                │
│ Backend:  CUTLASS (SlideSparse fallback)                  │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 256 (= 1 x 256)
│   M_decode      = 1
│   batched_tokens = 257 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 256
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 257
│   --max-num-batched-tokens = 257
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 06:17:04 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
Throughput: 201.56 requests/s, 51801.60 total tokens/s, 201.56 output tokens/s
Total num prompt tokens:  32768
Total num output tokens:  128


─── STDERR ───
[2026-01-25 06:17:03] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:17:03] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 06:17:03] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:17:03] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 06:17:03] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:17:03] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:17:03] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:17:03] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:17:03] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:17:03] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:17:03] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:17:04] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:17:04] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:17:04] INFO kernels.py:743: Preloaded 10 Triton kernels from RTX5080_cc120_py312_cu129_x86_64
[2026-01-25 06:17:04] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=2, cuSPARSELt=2 models
[2026-01-25 06:17:04] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:17:04] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:17:04] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:17:04] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 06:17:07] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:17:07] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 06:17:07] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:17:07] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 06:17:07] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:17:07] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:17:07] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:17:07] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:17:07] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:17:07] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:17:07] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:17:07] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:17:07] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:17:07] INFO kernels.py:743: Preloaded 10 Triton kernels from RTX5080_cc120_py312_cu129_x86_64
[2026-01-25 06:17:07] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=2, cuSPARSELt=2 models
[2026-01-25 06:17:07] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:17:07] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:17:07] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:17:07] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1095188) [2026-01-25 06:17:08] INFO SlideSparseLinearMethod_FP8.py:730: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (CUTLASS)
(EngineCore_DP0 pid=1095188) [2026-01-25 06:17:08] INFO SlideSparseLinearMethod_FP8.py:388: SlideSparseFp8LinearOp initialized (kernel=CUTLASS)
(EngineCore_DP0 pid=1095188) [2026-01-25 06:17:08] INFO SlideSparseLinearMethod_FP8.py:556: SlideSparseFp8LinearMethod initialized, kernel=CUTLASS
(EngineCore_DP0 pid=1095188) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1095188) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 14.31it/s]
(EngineCore_DP0 pid=1095188) 
(EngineCore_DP0 pid=1095188) 2026-01-25 06:17:16,234 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1095188) 2026-01-25 06:17:16,238 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1095188) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 87.55it/s]
(EngineCore_DP0 pid=1095188) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.80it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.80it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 1538.17it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:00<00:00, 279.54it/s, est. speed input: 71565.18 toks/s, output: 279.54 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:00<00:00, 243.43it/s, est. speed input: 63573.76 toks/s, output: 248.33 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:00<00:00, 233.93it/s, est. speed input: 61373.23 toks/s, output: 239.74 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:00<00:00, 229.56it/s, est. speed input: 60309.69 toks/s, output: 235.58 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 229.56it/s, est. speed input: 59582.35 toks/s, output: 232.74 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:00<00:00, 232.70it/s, est. speed input: 59582.35 toks/s, output: 232.74 toks/s]
[rank0]:[W125 06:17:18.876034537 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 19.3s

测试结果:
  Requests/s:   201.56
  Tokens/s:     51801.60
  Total Reqs:   128
  Elapsed:      0.64s

  [Prefill 分析]
  Total Prefill Tokens: 32768
  Prefill Tokens/s:     51600.03


------------------------------------------------------------
  生成 CSV: Qwen2.5-0.5B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cutlass/Qwen2.5-0.5B-FP8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
16,16,1,128,128,218.3258,3711.5385,0.5863
128,128,1,128,128,215.2025,27761.1227,0.5948
256,256,1,128,128,201.5626,51801.5951,0.6350

------------------------------------------------------------

[INFO] 完成: 3 成功, 0 失败

============================================================
  Qwen2.5-0.5B-FP8 | CUTLASS | decode
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints/Qwen2.5-0.5B-FP8
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cutlass

============================================================
[1/3] 测试 M=16
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-0.5B-FP8                                │
│ Backend:  CUTLASS (SlideSparse fallback)                  │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16
│   M_prefill     = 256 (= 16 x 16)
│   M_decode      = 16
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 16
│   --max-num-seqs           = 16
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 06:17:23 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
Throughput: 16.21 requests/s, 4407.85 total tokens/s, 4148.57 output tokens/s
Total num prompt tokens:  256
Total num output tokens:  4096


─── STDERR ───
[2026-01-25 06:17:23] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:17:23] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 06:17:23] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:17:23] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 06:17:23] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:17:23] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:17:23] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:17:23] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:17:23] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:17:23] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:17:23] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:17:23] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:17:23] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:17:23] INFO kernels.py:743: Preloaded 10 Triton kernels from RTX5080_cc120_py312_cu129_x86_64
[2026-01-25 06:17:23] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=2, cuSPARSELt=2 models
[2026-01-25 06:17:23] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:17:23] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:17:23] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:17:23] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 06:17:27] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:17:27] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 06:17:27] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:17:27] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 06:17:27] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:17:27] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:17:27] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:17:27] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:17:27] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:17:27] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:17:27] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:17:27] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:17:27] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:17:27] INFO kernels.py:743: Preloaded 10 Triton kernels from RTX5080_cc120_py312_cu129_x86_64
[2026-01-25 06:17:27] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=2, cuSPARSELt=2 models
[2026-01-25 06:17:27] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:17:27] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:17:27] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:17:27] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1095575) [2026-01-25 06:17:28] INFO SlideSparseLinearMethod_FP8.py:730: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (CUTLASS)
(EngineCore_DP0 pid=1095575) [2026-01-25 06:17:28] INFO SlideSparseLinearMethod_FP8.py:388: SlideSparseFp8LinearOp initialized (kernel=CUTLASS)
(EngineCore_DP0 pid=1095575) [2026-01-25 06:17:28] INFO SlideSparseLinearMethod_FP8.py:556: SlideSparseFp8LinearMethod initialized, kernel=CUTLASS
(EngineCore_DP0 pid=1095575) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1095575) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 14.43it/s]
(EngineCore_DP0 pid=1095575) 
(EngineCore_DP0 pid=1095575) 2026-01-25 06:17:38,087 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1095575) 2026-01-25 06:17:38,092 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1095575) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 78.35it/s]
(EngineCore_DP0 pid=1095575) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 1/5 [00:00<00:00,  7.87it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 30.77it/s]

Adding requests:   0%|          | 0/16 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 16/16 [00:00<00:00, 3991.49it/s]

Processed prompts:   0%|          | 0/16 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   6%|▋         | 1/16 [00:00<00:14,  1.02it/s, est. speed input: 16.36 toks/s, output: 261.72 toks/s]
Processed prompts: 100%|██████████| 16/16 [00:00<00:00,  1.02it/s, est. speed input: 260.67 toks/s, output: 4170.66 toks/s]
Processed prompts: 100%|██████████| 16/16 [00:00<00:00, 16.29it/s, est. speed input: 260.67 toks/s, output: 4170.66 toks/s]
[rank0]:[W125 06:17:40.239300618 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 22.3s

测试结果:
  Requests/s:   16.21
  Tokens/s:     4407.85
  Total Reqs:   16
  Elapsed:      0.99s

  [Decode 分析]
  Total Decode Tokens:  4096
  Decode Tokens/s:      4148.57

============================================================
[2/3] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-0.5B-FP8                                │
│ Backend:  CUTLASS (SlideSparse fallback)                  │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 2048 (= 128 x 16)
│   M_decode      = 128
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 128
│   --max-num-seqs           = 128
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 06:17:45 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
Throughput: 81.41 requests/s, 22144.76 total tokens/s, 20842.13 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768


─── STDERR ───
[2026-01-25 06:17:45] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:17:45] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 06:17:45] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:17:45] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 06:17:45] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:17:45] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:17:45] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:17:45] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:17:45] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:17:45] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:17:45] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:17:45] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:17:45] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:17:45] INFO kernels.py:743: Preloaded 10 Triton kernels from RTX5080_cc120_py312_cu129_x86_64
[2026-01-25 06:17:45] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=2, cuSPARSELt=2 models
[2026-01-25 06:17:45] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:17:45] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:17:45] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:17:45] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 06:17:49] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:17:49] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 06:17:49] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:17:49] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 06:17:49] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:17:49] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:17:49] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:17:49] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:17:49] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:17:49] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:17:49] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:17:49] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:17:49] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:17:49] INFO kernels.py:743: Preloaded 10 Triton kernels from RTX5080_cc120_py312_cu129_x86_64
[2026-01-25 06:17:49] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=2, cuSPARSELt=2 models
[2026-01-25 06:17:49] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:17:49] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:17:49] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:17:49] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1095988) [2026-01-25 06:17:50] INFO SlideSparseLinearMethod_FP8.py:730: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (CUTLASS)
(EngineCore_DP0 pid=1095988) [2026-01-25 06:17:50] INFO SlideSparseLinearMethod_FP8.py:388: SlideSparseFp8LinearOp initialized (kernel=CUTLASS)
(EngineCore_DP0 pid=1095988) [2026-01-25 06:17:50] INFO SlideSparseLinearMethod_FP8.py:556: SlideSparseFp8LinearMethod initialized, kernel=CUTLASS
(EngineCore_DP0 pid=1095988) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1095988) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 14.11it/s]
(EngineCore_DP0 pid=1095988) 
(EngineCore_DP0 pid=1095988) 2026-01-25 06:17:57,857 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1095988) 2026-01-25 06:17:57,862 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1095988) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▌       | 9/35 [00:00<00:00, 88.22it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████▏    | 18/35 [00:00<00:00, 88.79it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 28/35 [00:00<00:00, 89.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:00<00:00, 87.25it/s]
(EngineCore_DP0 pid=1095988) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   5%|▌         | 1/19 [00:00<00:02,  7.89it/s]
Capturing CUDA graphs (decode, FULL):  68%|██████▊   | 13/19 [00:00<00:00, 64.43it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:00<00:00, 66.10it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 4585.74it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:01<03:09,  1.49s/it, est. speed input: 10.73 toks/s, output: 171.68 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00,  1.49s/it, est. speed input: 1327.75 toks/s, output: 21243.96 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 82.98it/s, est. speed input: 1327.75 toks/s, output: 21243.96 toks/s]
[rank0]:[W125 06:18:01.982122061 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 20.7s

测试结果:
  Requests/s:   81.41
  Tokens/s:     22144.76
  Total Reqs:   128
  Elapsed:      1.57s

  [Decode 分析]
  Total Decode Tokens:  32768
  Decode Tokens/s:      20842.13

============================================================
[3/3] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-0.5B-FP8                                │
│ Backend:  CUTLASS (SlideSparse fallback)                  │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 4096 (= 256 x 16)
│   M_decode      = 256
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 256
│   --max-num-seqs           = 256
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 06:18:06 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
Throughput: 97.45 requests/s, 26507.17 total tokens/s, 24947.92 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536


─── STDERR ───
[2026-01-25 06:18:06] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:18:06] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 06:18:06] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:18:06] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 06:18:06] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:18:06] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:18:06] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:18:06] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:18:06] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:18:06] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:18:06] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:18:06] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:18:06] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:18:06] INFO kernels.py:743: Preloaded 10 Triton kernels from RTX5080_cc120_py312_cu129_x86_64
[2026-01-25 06:18:06] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=2, cuSPARSELt=2 models
[2026-01-25 06:18:06] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:18:06] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:18:06] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:18:06] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 06:18:10] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:18:10] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-FP8
[2026-01-25 06:18:10] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:18:10] INFO kernels.py:134: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-25 06:18:10] INFO kernels.py:180: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:18:10] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:18:10] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:18:10] INFO kernels.py:249: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:18:10] INFO kernels.py:373: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:18:10] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:18:10] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-25 06:18:10] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:18:10] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Llama3.2-1B-FP8
[2026-01-25 06:18:10] INFO kernels.py:743: Preloaded 10 Triton kernels from RTX5080_cc120_py312_cu129_x86_64
[2026-01-25 06:18:10] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=2, cuSPARSELt=2 models
[2026-01-25 06:18:10] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:18:10] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:18:10] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:18:10] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1096404) [2026-01-25 06:18:10] INFO SlideSparseLinearMethod_FP8.py:730: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (CUTLASS)
(EngineCore_DP0 pid=1096404) [2026-01-25 06:18:10] INFO SlideSparseLinearMethod_FP8.py:388: SlideSparseFp8LinearOp initialized (kernel=CUTLASS)
(EngineCore_DP0 pid=1096404) [2026-01-25 06:18:10] INFO SlideSparseLinearMethod_FP8.py:556: SlideSparseFp8LinearMethod initialized, kernel=CUTLASS
(EngineCore_DP0 pid=1096404) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1096404) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 14.37it/s]
(EngineCore_DP0 pid=1096404) 
(EngineCore_DP0 pid=1096404) 2026-01-25 06:18:18,562 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1096404) 2026-01-25 06:18:18,567 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1096404) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/36 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 9/36 [00:00<00:00, 89.12it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 19/36 [00:00<00:00, 89.92it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  81%|████████  | 29/36 [00:00<00:00, 90.94it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:00<00:00, 88.15it/s]
(EngineCore_DP0 pid=1096404) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   3%|▎         | 1/35 [00:00<00:04,  7.85it/s]
Capturing CUDA graphs (decode, FULL):  34%|███▍      | 12/35 [00:00<00:00, 60.32it/s]
Capturing CUDA graphs (decode, FULL):  66%|██████▌   | 23/35 [00:00<00:00, 81.07it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:00<00:00, 93.40it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:00<00:00, 79.58it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 6630.94it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:02<09:54,  2.33s/it, est. speed input: 6.87 toks/s, output: 109.87 toks/s]
Processed prompts:  59%|█████▉    | 152/256 [00:02<00:01, 87.53it/s, est. speed input: 999.92 toks/s, output: 15998.65 toks/s]
Processed prompts:  95%|█████████▍| 242/256 [00:02<00:00, 149.04it/s, est. speed input: 1527.24 toks/s, output: 24435.76 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 149.04it/s, est. speed input: 1583.63 toks/s, output: 25338.03 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 98.97it/s, est. speed input: 1583.63 toks/s, output: 25338.03 toks/s] 
[rank0]:[W125 06:18:23.916154082 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 22.0s

测试结果:
  Requests/s:   97.45
  Tokens/s:     26507.17
  Total Reqs:   256
  Elapsed:      2.63s

  [Decode 分析]
  Total Decode Tokens:  65536
  Decode Tokens/s:      24947.92


------------------------------------------------------------
  生成 CSV: Qwen2.5-0.5B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cutlass/Qwen2.5-0.5B-FP8_decode.csv

预览:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
16,16,16,16,256,256,16.2053,4407.8504,0.9873
128,16,128,128,256,256,81.4146,22144.7649,1.5722
256,16,256,256,256,256,97.4528,26507.1698,2.6269

------------------------------------------------------------

[INFO] 完成: 3 成功, 0 失败

============================================================
  Qwen2.5-0.5B-FP8 | cuBLASLt | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints/Qwen2.5-0.5B-FP8
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cublaslt

============================================================
[1/3] 测试 M=16
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-0.5B-FP8                                │
│ Backend:  cuBLASLt                                        │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16
│   M_prefill     = 16 (= 1 x 16)
│   M_decode      = 1
│   batched_tokens = 17 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 17
│   --max-num-batched-tokens = 17
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 06:18:28 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1096758) WARNING 01-25 06:18:35 [backends.py:609] Failed to read file <frozen os>
Throughput: 61.09 requests/s, 1038.47 total tokens/s, 61.09 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  128


─── STDERR ───
[2026-01-25 06:18:28] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:18:28] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:18:28] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:18:28] INFO kernels.py:97: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 06:18:28] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:18:28] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:18:28] INFO kernels.py:97: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 06:18:28] INFO kernels.py:249: FP8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:18:28] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:18:28] INFO kernels.py:373: INT8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:18:28] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:18:28] INFO kernels.py:97: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 06:18:28] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:18:28] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:18:28] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:18:28] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:18:28] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 06:18:28] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:18:28] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:18:28] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:18:28] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 06:18:31] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:18:31] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:18:31] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:18:31] INFO kernels.py:97: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 06:18:31] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:18:31] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:18:31] INFO kernels.py:97: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 06:18:31] INFO kernels.py:249: FP8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:18:31] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:18:31] INFO kernels.py:373: INT8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:18:31] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:18:31] INFO kernels.py:97: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 06:18:31] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:18:31] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:18:31] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:18:31] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:18:31] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 06:18:31] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:18:31] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:18:31] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:18:31] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1096758) [2026-01-25 06:18:32] INFO SlideSparseLinearMethod_FP8.py:730: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=1096758) [2026-01-25 06:18:32] INFO gemm_wrapper.py:872: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1096758) [2026-01-25 06:18:32] INFO SlideSparseLinearMethod_FP8.py:388: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=1096758) [2026-01-25 06:18:32] INFO SlideSparseLinearMethod_FP8.py:552: Preloaded FP8 Triton kernels for model: Qwen2.5-0.5B-FP8
(EngineCore_DP0 pid=1096758) [2026-01-25 06:18:32] INFO SlideSparseLinearMethod_FP8.py:556: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=1096758) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1096758) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 14.15it/s]
(EngineCore_DP0 pid=1096758) 
(EngineCore_DP0 pid=1096758) [2026-01-25 06:18:38] WARNING gemm_wrapper.py:337: No cuBLASLt config for model 'Qwen2.5-0.5B-FP8', using default algorithm
(EngineCore_DP0 pid=1096758) 2026-01-25 06:18:41,969 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1096758) 2026-01-25 06:18:41,983 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1096758) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  4.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  4.62it/s]
(EngineCore_DP0 pid=1096758) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.75it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.74it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 7983.69it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:42,  3.02it/s, est. speed input: 48.31 toks/s, output: 3.02 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:04, 25.45it/s, est. speed input: 326.38 toks/s, output: 20.40 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:02, 40.30it/s, est. speed input: 493.14 toks/s, output: 30.82 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:00<00:02, 50.50it/s, est. speed input: 605.58 toks/s, output: 37.85 toks/s]
Processed prompts:  25%|██▌       | 32/128 [00:00<00:01, 55.73it/s, est. speed input: 671.11 toks/s, output: 41.94 toks/s]
Processed prompts:  31%|███▏      | 40/128 [00:00<00:01, 60.72it/s, est. speed input: 731.83 toks/s, output: 45.74 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:00<00:01, 64.42it/s, est. speed input: 780.21 toks/s, output: 48.76 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:01<00:01, 67.43it/s, est. speed input: 820.68 toks/s, output: 51.29 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:01<00:00, 68.89it/s, est. speed input: 851.51 toks/s, output: 53.22 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:01<00:00, 70.52it/s, est. speed input: 879.28 toks/s, output: 54.95 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:01<00:00, 71.87it/s, est. speed input: 903.48 toks/s, output: 56.47 toks/s]
Processed prompts:  69%|██████▉   | 88/128 [00:01<00:00, 72.74it/s, est. speed input: 924.07 toks/s, output: 57.75 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:01<00:00, 72.93it/s, est. speed input: 940.74 toks/s, output: 58.80 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:01<00:00, 72.00it/s, est. speed input: 952.44 toks/s, output: 59.53 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:01<00:00, 72.54it/s, est. speed input: 965.78 toks/s, output: 60.36 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:01<00:00, 72.05it/s, est. speed input: 975.47 toks/s, output: 60.97 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 72.22it/s, est. speed input: 985.36 toks/s, output: 61.58 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 72.22it/s, est. speed input: 985.36 toks/s, output: 61.58 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 61.58it/s, est. speed input: 985.36 toks/s, output: 61.58 toks/s]
[rank0]:[W125 06:18:45.735614685 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 22.8s

测试结果:
  Requests/s:   61.09
  Tokens/s:     1038.47
  Total Reqs:   128
  Elapsed:      2.10s

  [Prefill 分析]
  Total Prefill Tokens: 2048
  Prefill Tokens/s:     977.38

============================================================
[2/3] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-0.5B-FP8                                │
│ Backend:  cuBLASLt                                        │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 128 (= 1 x 128)
│   M_decode      = 1
│   batched_tokens = 129 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 128
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 129
│   --max-num-batched-tokens = 129
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 06:18:51 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1097262) WARNING 01-25 06:18:58 [backends.py:609] Failed to read file <frozen os>
Throughput: 70.04 requests/s, 9035.03 total tokens/s, 70.04 output tokens/s
Total num prompt tokens:  16384
Total num output tokens:  128


─── STDERR ───
[2026-01-25 06:18:50] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:18:51] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:18:51] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:18:51] INFO kernels.py:97: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 06:18:51] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:18:51] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:18:51] INFO kernels.py:97: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 06:18:51] INFO kernels.py:249: FP8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:18:51] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:18:51] INFO kernels.py:373: INT8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:18:51] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:18:51] INFO kernels.py:97: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 06:18:51] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:18:51] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:18:51] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:18:51] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:18:51] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 06:18:51] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:18:51] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:18:51] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:18:51] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 06:18:54] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:18:54] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:18:54] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:18:54] INFO kernels.py:97: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 06:18:54] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:18:54] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:18:54] INFO kernels.py:97: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 06:18:54] INFO kernels.py:249: FP8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:18:54] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:18:54] INFO kernels.py:373: INT8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:18:54] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:18:54] INFO kernels.py:97: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 06:18:54] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:18:54] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:18:54] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:18:54] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:18:54] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 06:18:54] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:18:54] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:18:54] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:18:54] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1097262) [2026-01-25 06:18:55] INFO SlideSparseLinearMethod_FP8.py:730: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=1097262) [2026-01-25 06:18:55] INFO gemm_wrapper.py:872: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1097262) [2026-01-25 06:18:55] INFO SlideSparseLinearMethod_FP8.py:388: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=1097262) [2026-01-25 06:18:55] INFO SlideSparseLinearMethod_FP8.py:552: Preloaded FP8 Triton kernels for model: Qwen2.5-0.5B-FP8
(EngineCore_DP0 pid=1097262) [2026-01-25 06:18:55] INFO SlideSparseLinearMethod_FP8.py:556: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=1097262) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1097262) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 14.33it/s]
(EngineCore_DP0 pid=1097262) 
(EngineCore_DP0 pid=1097262) [2026-01-25 06:19:01] WARNING gemm_wrapper.py:337: No cuBLASLt config for model 'Qwen2.5-0.5B-FP8', using default algorithm
(EngineCore_DP0 pid=1097262) 2026-01-25 06:19:04,368 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1097262) 2026-01-25 06:19:04,382 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1097262) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 28.84it/s]
(EngineCore_DP0 pid=1097262) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.86it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.85it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 2105.76it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   5%|▌         | 7/128 [00:00<00:01, 67.90it/s, est. speed input: 8691.93 toks/s, output: 67.90 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:00<00:01, 70.41it/s, est. speed input: 8966.27 toks/s, output: 70.05 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:00<00:01, 70.13it/s, est. speed input: 8956.20 toks/s, output: 69.97 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:00<00:01, 71.76it/s, est. speed input: 9093.94 toks/s, output: 71.05 toks/s]
Processed prompts:  30%|███       | 39/128 [00:00<00:01, 72.43it/s, est. speed input: 9159.66 toks/s, output: 71.56 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:00<00:01, 72.55it/s, est. speed input: 9186.11 toks/s, output: 71.77 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:00<00:01, 72.56it/s, est. speed input: 9201.06 toks/s, output: 71.88 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:00<00:00, 72.71it/s, est. speed input: 9219.32 toks/s, output: 72.03 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:00<00:00, 72.92it/s, est. speed input: 9238.71 toks/s, output: 72.18 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:01<00:00, 73.08it/s, est. speed input: 9254.80 toks/s, output: 72.30 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:01<00:00, 73.27it/s, est. speed input: 9271.11 toks/s, output: 72.43 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:01<00:00, 73.36it/s, est. speed input: 9283.26 toks/s, output: 72.52 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:01<00:00, 73.45it/s, est. speed input: 9294.35 toks/s, output: 72.61 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:01<00:00, 73.27it/s, est. speed input: 9296.46 toks/s, output: 72.63 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:01<00:00, 72.72it/s, est. speed input: 9286.45 toks/s, output: 72.55 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:01<00:00, 72.58it/s, est. speed input: 9284.20 toks/s, output: 72.53 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 72.58it/s, est. speed input: 9282.67 toks/s, output: 72.52 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 72.52it/s, est. speed input: 9282.67 toks/s, output: 72.52 toks/s]
[rank0]:[W125 06:19:07.328359973 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 21.6s

测试结果:
  Requests/s:   70.04
  Tokens/s:     9035.03
  Total Reqs:   128
  Elapsed:      1.83s

  [Prefill 分析]
  Total Prefill Tokens: 16384
  Prefill Tokens/s:     8964.99

============================================================
[3/3] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-0.5B-FP8                                │
│ Backend:  cuBLASLt                                        │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 256 (= 1 x 256)
│   M_decode      = 1
│   batched_tokens = 257 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 256
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 257
│   --max-num-batched-tokens = 257
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 06:19:12 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1097682) WARNING 01-25 06:19:20 [backends.py:609] Failed to read file <frozen os>
Throughput: 66.01 requests/s, 16965.59 total tokens/s, 66.01 output tokens/s
Total num prompt tokens:  32768
Total num output tokens:  128


─── STDERR ───
[2026-01-25 06:19:12] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:19:12] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:19:12] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:19:12] INFO kernels.py:97: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 06:19:12] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:19:12] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:19:12] INFO kernels.py:97: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 06:19:12] INFO kernels.py:249: FP8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:19:12] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:19:12] INFO kernels.py:373: INT8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:19:12] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:19:12] INFO kernels.py:97: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 06:19:12] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:19:12] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:19:12] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:19:12] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:19:12] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 06:19:12] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:19:12] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:19:12] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:19:12] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 06:19:16] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:19:16] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:19:16] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:19:16] INFO kernels.py:97: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 06:19:16] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:19:16] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:19:16] INFO kernels.py:97: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 06:19:16] INFO kernels.py:249: FP8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:19:16] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:19:16] INFO kernels.py:373: INT8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:19:16] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:19:16] INFO kernels.py:97: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 06:19:16] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:19:16] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:19:16] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:19:16] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:19:16] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 06:19:16] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:19:16] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:19:16] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:19:16] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1097682) [2026-01-25 06:19:16] INFO SlideSparseLinearMethod_FP8.py:730: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=1097682) [2026-01-25 06:19:17] INFO gemm_wrapper.py:872: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1097682) [2026-01-25 06:19:17] INFO SlideSparseLinearMethod_FP8.py:388: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=1097682) [2026-01-25 06:19:17] INFO SlideSparseLinearMethod_FP8.py:552: Preloaded FP8 Triton kernels for model: Qwen2.5-0.5B-FP8
(EngineCore_DP0 pid=1097682) [2026-01-25 06:19:17] INFO SlideSparseLinearMethod_FP8.py:556: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=1097682) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1097682) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 13.64it/s]
(EngineCore_DP0 pid=1097682) 
(EngineCore_DP0 pid=1097682) [2026-01-25 06:19:23] WARNING gemm_wrapper.py:337: No cuBLASLt config for model 'Qwen2.5-0.5B-FP8', using default algorithm
(EngineCore_DP0 pid=1097682) 2026-01-25 06:19:25,792 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1097682) 2026-01-25 06:19:25,806 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1097682) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 27.69it/s]
(EngineCore_DP0 pid=1097682) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.75it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.74it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 1677.38it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:14,  8.71it/s, est. speed input: 2229.00 toks/s, output: 8.71 toks/s]
Processed prompts:   6%|▋         | 8/128 [00:00<00:02, 42.25it/s, est. speed input: 9451.39 toks/s, output: 36.92 toks/s]
Processed prompts:  12%|█▎        | 16/128 [00:00<00:01, 56.06it/s, est. speed input: 12489.35 toks/s, output: 48.79 toks/s]
Processed prompts:  19%|█▉        | 24/128 [00:00<00:01, 62.62it/s, est. speed input: 14020.44 toks/s, output: 54.77 toks/s]
Processed prompts:  25%|██▌       | 32/128 [00:00<00:01, 66.42it/s, est. speed input: 14960.83 toks/s, output: 58.44 toks/s]
Processed prompts:  31%|███▏      | 40/128 [00:00<00:01, 68.85it/s, est. speed input: 15603.92 toks/s, output: 60.95 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:00<00:01, 70.13it/s, est. speed input: 16037.02 toks/s, output: 62.64 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:00<00:01, 71.06it/s, est. speed input: 16370.15 toks/s, output: 63.95 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:00<00:00, 71.75it/s, est. speed input: 16634.55 toks/s, output: 64.98 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:01<00:00, 72.20it/s, est. speed input: 16845.38 toks/s, output: 65.80 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:01<00:00, 72.63it/s, est. speed input: 17026.02 toks/s, output: 66.51 toks/s]
Processed prompts:  69%|██████▉   | 88/128 [00:01<00:00, 72.48it/s, est. speed input: 17147.41 toks/s, output: 66.98 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:01<00:00, 72.79it/s, est. speed input: 17275.49 toks/s, output: 67.48 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:01<00:00, 72.94it/s, est. speed input: 17381.57 toks/s, output: 67.90 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:01<00:00, 73.08it/s, est. speed input: 17474.93 toks/s, output: 68.26 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:01<00:00, 72.76it/s, est. speed input: 17536.43 toks/s, output: 68.50 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 72.76it/s, est. speed input: 17600.41 toks/s, output: 68.75 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 72.76it/s, est. speed input: 17600.41 toks/s, output: 68.75 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 68.75it/s, est. speed input: 17600.41 toks/s, output: 68.75 toks/s]
[rank0]:[W125 06:19:29.018487368 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 21.7s

测试结果:
  Requests/s:   66.01
  Tokens/s:     16965.59
  Total Reqs:   128
  Elapsed:      1.94s

  [Prefill 分析]
  Total Prefill Tokens: 32768
  Prefill Tokens/s:     16899.58


------------------------------------------------------------
  生成 CSV: Qwen2.5-0.5B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cublaslt/Qwen2.5-0.5B-FP8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
16,16,1,128,128,61.0865,1038.4701,2.0954
128,128,1,128,128,70.0390,9035.0308,1.8276
256,256,1,128,128,66.0140,16965.5938,1.9390

------------------------------------------------------------

[INFO] 完成: 3 成功, 0 失败

============================================================
  Qwen2.5-0.5B-FP8 | cuBLASLt | decode
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints/Qwen2.5-0.5B-FP8
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cublaslt

============================================================
[1/3] 测试 M=16
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-0.5B-FP8                                │
│ Backend:  cuBLASLt                                        │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16
│   M_prefill     = 256 (= 16 x 16)
│   M_decode      = 16
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 16
│   --max-num-seqs           = 16
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 06:19:34 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1098093) WARNING 01-25 06:19:42 [backends.py:609] Failed to read file <frozen os>
Throughput: 21.70 requests/s, 5903.70 total tokens/s, 5556.42 output tokens/s
Total num prompt tokens:  256
Total num output tokens:  4096


─── STDERR ───
[2026-01-25 06:19:34] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:19:34] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:19:34] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:19:34] INFO kernels.py:97: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 06:19:34] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:19:34] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:19:34] INFO kernels.py:97: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 06:19:34] INFO kernels.py:249: FP8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:19:34] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:19:34] INFO kernels.py:373: INT8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:19:34] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:19:34] INFO kernels.py:97: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 06:19:34] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:19:34] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:19:34] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:19:34] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:19:34] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 06:19:34] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:19:34] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:19:34] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:19:34] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 06:19:38] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:19:38] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:19:38] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:19:38] INFO kernels.py:97: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 06:19:38] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:19:38] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:19:38] INFO kernels.py:97: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 06:19:38] INFO kernels.py:249: FP8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:19:38] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:19:38] INFO kernels.py:373: INT8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:19:38] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:19:38] INFO kernels.py:97: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 06:19:38] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:19:38] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:19:38] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:19:38] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:19:38] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 06:19:38] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:19:38] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:19:38] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:19:38] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1098093) [2026-01-25 06:19:38] INFO SlideSparseLinearMethod_FP8.py:730: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=1098093) [2026-01-25 06:19:38] INFO gemm_wrapper.py:872: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1098093) [2026-01-25 06:19:38] INFO SlideSparseLinearMethod_FP8.py:388: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=1098093) [2026-01-25 06:19:38] INFO SlideSparseLinearMethod_FP8.py:552: Preloaded FP8 Triton kernels for model: Qwen2.5-0.5B-FP8
(EngineCore_DP0 pid=1098093) [2026-01-25 06:19:38] INFO SlideSparseLinearMethod_FP8.py:556: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=1098093) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1098093) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 14.42it/s]
(EngineCore_DP0 pid=1098093) 
(EngineCore_DP0 pid=1098093) [2026-01-25 06:19:44] WARNING gemm_wrapper.py:337: No cuBLASLt config for model 'Qwen2.5-0.5B-FP8', using default algorithm
(EngineCore_DP0 pid=1098093) 2026-01-25 06:19:47,240 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1098093) 2026-01-25 06:19:47,254 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1098093) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00, 30.88it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 29.49it/s]
(EngineCore_DP0 pid=1098093) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 1/5 [00:00<00:00,  6.76it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 21.25it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 18.82it/s]

Adding requests:   0%|          | 0/16 [00:00<?, ?it/s]
Adding requests:  62%|██████▎   | 10/16 [00:00<00:00, 68.95it/s]
Adding requests: 100%|██████████| 16/16 [00:00<00:00, 109.29it/s]

Processed prompts:   0%|          | 0/16 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   6%|▋         | 1/16 [00:00<00:07,  1.91it/s, est. speed input: 30.55 toks/s, output: 488.83 toks/s]
Processed prompts: 100%|██████████| 16/16 [00:00<00:00,  1.91it/s, est. speed input: 434.67 toks/s, output: 6954.63 toks/s]
Processed prompts: 100%|██████████| 16/16 [00:00<00:00, 27.16it/s, est. speed input: 434.67 toks/s, output: 6954.63 toks/s]
[rank0]:[W125 06:19:49.401834894 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 20.3s

测试结果:
  Requests/s:   21.70
  Tokens/s:     5903.70
  Total Reqs:   16
  Elapsed:      0.74s

  [Decode 分析]
  Total Decode Tokens:  4096
  Decode Tokens/s:      5556.42

============================================================
[2/3] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-0.5B-FP8                                │
│ Backend:  cuBLASLt                                        │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 2048 (= 128 x 16)
│   M_decode      = 128
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 128
│   --max-num-seqs           = 128
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 06:19:54 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1098472) WARNING 01-25 06:20:02 [backends.py:609] Failed to read file <frozen os>
Throughput: 97.92 requests/s, 26634.21 total tokens/s, 25067.49 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768


─── STDERR ───
[2026-01-25 06:19:54] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:19:54] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:19:54] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:19:54] INFO kernels.py:97: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 06:19:54] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:19:54] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:19:54] INFO kernels.py:97: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 06:19:54] INFO kernels.py:249: FP8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:19:54] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:19:54] INFO kernels.py:373: INT8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:19:54] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:19:54] INFO kernels.py:97: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 06:19:54] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:19:54] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:19:54] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:19:54] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:19:54] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 06:19:54] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:19:54] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:19:54] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:19:54] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 06:19:58] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:19:58] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:19:58] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:19:58] INFO kernels.py:97: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 06:19:58] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:19:58] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:19:58] INFO kernels.py:97: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 06:19:58] INFO kernels.py:249: FP8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:19:58] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:19:58] INFO kernels.py:373: INT8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:19:58] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:19:58] INFO kernels.py:97: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 06:19:58] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:19:58] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:19:58] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:19:58] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:19:58] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 06:19:58] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:19:58] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:19:58] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:19:58] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1098472) [2026-01-25 06:19:59] INFO SlideSparseLinearMethod_FP8.py:730: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=1098472) [2026-01-25 06:19:59] INFO gemm_wrapper.py:872: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1098472) [2026-01-25 06:19:59] INFO SlideSparseLinearMethod_FP8.py:388: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=1098472) [2026-01-25 06:19:59] INFO SlideSparseLinearMethod_FP8.py:552: Preloaded FP8 Triton kernels for model: Qwen2.5-0.5B-FP8
(EngineCore_DP0 pid=1098472) [2026-01-25 06:19:59] INFO SlideSparseLinearMethod_FP8.py:556: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=1098472) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1098472) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 13.99it/s]
(EngineCore_DP0 pid=1098472) 
(EngineCore_DP0 pid=1098472) [2026-01-25 06:20:04] WARNING gemm_wrapper.py:337: No cuBLASLt config for model 'Qwen2.5-0.5B-FP8', using default algorithm
(EngineCore_DP0 pid=1098472) 2026-01-25 06:20:06,099 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1098472) 2026-01-25 06:20:06,113 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1098472) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|▎         | 1/35 [00:00<00:03,  9.55it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▊         | 3/35 [00:00<00:02, 13.72it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 7/35 [00:00<00:01, 22.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 11/35 [00:00<00:00, 26.27it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 15/35 [00:00<00:00, 28.42it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  54%|█████▍    | 19/35 [00:00<00:00, 29.39it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  66%|██████▌   | 23/35 [00:00<00:00, 30.35it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  77%|███████▋  | 27/35 [00:00<00:00, 30.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▊ | 31/35 [00:01<00:00, 30.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:01<00:00, 30.44it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:01<00:00, 28.07it/s]
(EngineCore_DP0 pid=1098472) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   5%|▌         | 1/19 [00:00<00:02,  6.82it/s]
Capturing CUDA graphs (decode, FULL):  26%|██▋       | 5/19 [00:00<00:00, 21.40it/s]
Capturing CUDA graphs (decode, FULL):  47%|████▋     | 9/19 [00:00<00:00, 26.96it/s]
Capturing CUDA graphs (decode, FULL):  68%|██████▊   | 13/19 [00:00<00:00, 29.72it/s]
Capturing CUDA graphs (decode, FULL):  89%|████████▉ | 17/19 [00:00<00:00, 31.43it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:00<00:00, 28.34it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 4645.54it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:01<02:37,  1.24s/it, est. speed input: 12.89 toks/s, output: 206.22 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00,  1.24s/it, est. speed input: 1601.64 toks/s, output: 25626.26 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 100.10it/s, est. speed input: 1601.64 toks/s, output: 25626.26 toks/s]
[rank0]:[W125 06:20:10.171282142 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 20.8s

测试结果:
  Requests/s:   97.92
  Tokens/s:     26634.21
  Total Reqs:   128
  Elapsed:      1.31s

  [Decode 分析]
  Total Decode Tokens:  32768
  Decode Tokens/s:      25067.49

============================================================
[3/3] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-0.5B-FP8                                │
│ Backend:  cuBLASLt                                        │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 4096 (= 256 x 16)
│   M_decode      = 256
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 256
│   --max-num-seqs           = 256
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 06:20:15 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1098843) WARNING 01-25 06:20:23 [backends.py:609] Failed to read file <frozen os>
Throughput: 112.44 requests/s, 30584.09 total tokens/s, 28785.02 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536


─── STDERR ───
[2026-01-25 06:20:15] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:20:15] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:20:15] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:20:15] INFO kernels.py:97: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 06:20:15] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:20:15] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:20:15] INFO kernels.py:97: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 06:20:15] INFO kernels.py:249: FP8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:20:15] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:20:15] INFO kernels.py:373: INT8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:20:15] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:20:15] INFO kernels.py:97: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 06:20:15] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:20:15] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:20:15] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:20:15] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:20:15] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 06:20:15] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:20:15] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:20:15] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:20:15] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 06:20:19] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:20:19] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:20:19] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:20:19] INFO kernels.py:97: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 06:20:19] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:20:19] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:20:19] INFO kernels.py:97: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 06:20:19] INFO kernels.py:249: FP8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:20:19] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:20:19] INFO kernels.py:373: INT8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:20:19] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:20:19] INFO kernels.py:97: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 06:20:19] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:20:19] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:20:19] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:20:19] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:20:19] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 06:20:19] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:20:19] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:20:19] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:20:19] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1098843) [2026-01-25 06:20:20] INFO SlideSparseLinearMethod_FP8.py:730: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=1098843) [2026-01-25 06:20:20] INFO gemm_wrapper.py:872: cublaslt GEMM extension loaded: cublaslt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1098843) [2026-01-25 06:20:20] INFO SlideSparseLinearMethod_FP8.py:388: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=1098843) [2026-01-25 06:20:20] INFO SlideSparseLinearMethod_FP8.py:552: Preloaded FP8 Triton kernels for model: Qwen2.5-0.5B-FP8
(EngineCore_DP0 pid=1098843) [2026-01-25 06:20:20] INFO SlideSparseLinearMethod_FP8.py:556: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=1098843) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1098843) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 14.26it/s]
(EngineCore_DP0 pid=1098843) 
(EngineCore_DP0 pid=1098843) [2026-01-25 06:20:25] WARNING gemm_wrapper.py:337: No cuBLASLt config for model 'Qwen2.5-0.5B-FP8', using default algorithm
(EngineCore_DP0 pid=1098843) 2026-01-25 06:20:27,247 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1098843) 2026-01-25 06:20:27,264 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1098843) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/36 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 3/36 [00:00<00:01, 24.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  19%|█▉        | 7/36 [00:00<00:00, 29.00it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███       | 11/36 [00:00<00:00, 30.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 15/36 [00:00<00:00, 31.00it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 19/36 [00:00<00:00, 31.05it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▍   | 23/36 [00:00<00:00, 31.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 27/36 [00:00<00:00, 31.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 31/36 [00:01<00:00, 31.66it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  97%|█████████▋| 35/36 [00:01<00:00, 31.92it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:01<00:00, 30.71it/s]
(EngineCore_DP0 pid=1098843) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   3%|▎         | 1/35 [00:00<00:04,  6.82it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 5/35 [00:00<00:01, 21.20it/s]
Capturing CUDA graphs (decode, FULL):  26%|██▌       | 9/35 [00:00<00:00, 26.67it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 13/35 [00:00<00:00, 29.26it/s]
Capturing CUDA graphs (decode, FULL):  49%|████▊     | 17/35 [00:00<00:00, 30.84it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 21/35 [00:00<00:00, 31.73it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 25/35 [00:00<00:00, 32.57it/s]
Capturing CUDA graphs (decode, FULL):  83%|████████▎ | 29/35 [00:00<00:00, 32.89it/s]
Capturing CUDA graphs (decode, FULL):  94%|█████████▍| 33/35 [00:01<00:00, 33.29it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:01<00:00, 30.36it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 5585.57it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:02<08:39,  2.04s/it, est. speed input: 7.85 toks/s, output: 125.61 toks/s]
Processed prompts:  70%|██████▉   | 178/256 [00:02<00:00, 116.19it/s, est. speed input: 1330.37 toks/s, output: 21285.92 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 116.19it/s, est. speed input: 1837.47 toks/s, output: 29399.46 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 114.84it/s, est. speed input: 1837.47 toks/s, output: 29399.46 toks/s]
[rank0]:[W125 06:20:32.698455360 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 22.5s

测试结果:
  Requests/s:   112.44
  Tokens/s:     30584.09
  Total Reqs:   256
  Elapsed:      2.28s

  [Decode 分析]
  Total Decode Tokens:  65536
  Decode Tokens/s:      28785.02


------------------------------------------------------------
  生成 CSV: Qwen2.5-0.5B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cublaslt/Qwen2.5-0.5B-FP8_decode.csv

预览:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
16,16,16,16,256,256,21.7048,5903.6989,0.7372
128,16,128,128,256,256,97.9199,26634.2132,1.3072
256,16,256,256,256,256,112.4415,30584.0871,2.2767

------------------------------------------------------------

[INFO] 完成: 3 成功, 0 失败

======================================================================
[SlideSparse] 未找到 Qwen2.5-0.5B-FP8-SlideSparse-2_4 checkpoint
[SlideSparse] 发现基础模型: Qwen2.5-0.5B-FP8
[SlideSparse] 自动转换中...
[SlideSparse] 开始转换: Qwen2.5-0.5B-FP8 -> SlideSparse-2_4
======================================================================

[SlideSparse] 转换成功: Qwen2.5-0.5B-FP8-SlideSparse-2_4
======================================================================


============================================================
  Qwen2.5-0.5B-FP8 | cuSPARSELt (2_4) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_4
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_4

============================================================
[1/3] 测试 M=16
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-0.5B-FP8                                │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16
│   M_prefill     = 16 (= 1 x 16)
│   M_decode      = 1
│   batched_tokens = 17 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 17
│   --max-num-batched-tokens = 17
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 06:20:58 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1099454) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1099454) WARNING 01-25 06:21:07 [backends.py:609] Failed to read file <frozen os>
Throughput: 68.17 requests/s, 1158.81 total tokens/s, 68.17 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  128


─── STDERR ───
[2026-01-25 06:20:58] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:20:58] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:20:58] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:20:58] INFO kernels.py:97: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 06:20:58] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:20:58] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:20:58] INFO kernels.py:97: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 06:20:58] INFO kernels.py:249: FP8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:20:58] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:20:58] INFO kernels.py:373: INT8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:20:58] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:20:58] INFO kernels.py:97: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 06:20:58] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:20:58] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:20:58] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:20:58] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:20:58] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 06:20:58] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:20:58] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:20:58] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:20:58] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 06:21:02] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:21:02] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:21:02] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:21:02] INFO kernels.py:97: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 06:21:02] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:21:02] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:21:02] INFO kernels.py:97: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 06:21:02] INFO kernels.py:249: FP8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:21:02] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:21:02] INFO kernels.py:373: INT8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:21:02] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:21:02] INFO kernels.py:97: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 06:21:02] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:21:02] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:21:02] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:21:02] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:21:02] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 06:21:02] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:21:02] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:21:02] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:21:02] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1099454) [2026-01-25 06:21:03] INFO SlideSparseLinearMethod_FP8.py:730: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1099454) [2026-01-25 06:21:03] INFO gemm_wrapper.py:872: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1099454) [2026-01-25 06:21:03] INFO SlideSparseLinearMethod_FP8.py:388: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1099454) [2026-01-25 06:21:03] INFO SlideSparseLinearMethod_FP8.py:535: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=1099454) [2026-01-25 06:21:03] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
(EngineCore_DP0 pid=1099454) [2026-01-25 06:21:03] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8-SlideSparse-2_4
(EngineCore_DP0 pid=1099454) [2026-01-25 06:21:03] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
(EngineCore_DP0 pid=1099454) [2026-01-25 06:21:03] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8-SlideSparse-2_4
(EngineCore_DP0 pid=1099454) [2026-01-25 06:21:03] INFO SlideSparseLinearMethod_FP8.py:552: Preloaded FP8 Triton kernels for model: Qwen2.5-0.5B-FP8-SlideSparse-2_4
(EngineCore_DP0 pid=1099454) [2026-01-25 06:21:03] INFO SlideSparseLinearMethod_FP8.py:556: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=1099454) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1099454) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 13.99it/s]
(EngineCore_DP0 pid=1099454) 
(EngineCore_DP0 pid=1099454) [2026-01-25 06:21:03] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [1152, 896] -> 1D uint8
(EngineCore_DP0 pid=1099454) [2026-01-25 06:21:03] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 645120 bytes
(EngineCore_DP0 pid=1099454) [2026-01-25 06:21:03] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [896, 896] -> 1D uint8
(EngineCore_DP0 pid=1099454) [2026-01-25 06:21:03] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 501760 bytes
(EngineCore_DP0 pid=1099454) [2026-01-25 06:21:03] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [9728, 896] -> 1D uint8
(EngineCore_DP0 pid=1099454) [2026-01-25 06:21:03] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 5447680 bytes
(EngineCore_DP0 pid=1099454) [2026-01-25 06:21:03] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [896, 4864] -> 1D uint8
(EngineCore_DP0 pid=1099454) [2026-01-25 06:21:03] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 2723840 bytes
(EngineCore_DP0 pid=1099454) [2026-01-25 06:21:10] WARNING gemm_wrapper.py:393: No cuSPARSELt config for model 'Qwen2.5-0.5B-FP8', using default algorithm
(EngineCore_DP0 pid=1099454) 2026-01-25 06:21:12,771 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1099454) 2026-01-25 06:21:12,784 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1099454) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  8.34it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  8.34it/s]
(EngineCore_DP0 pid=1099454) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.82it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.82it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 4949.90it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:28,  4.41it/s, est. speed input: 70.51 toks/s, output: 4.41 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:03, 33.16it/s, est. speed input: 435.74 toks/s, output: 27.23 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:02, 49.30it/s, est. speed input: 628.76 toks/s, output: 39.30 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:00<00:01, 58.92it/s, est. speed input: 746.68 toks/s, output: 46.67 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:00<00:01, 65.21it/s, est. speed input: 827.70 toks/s, output: 51.73 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:00<00:01, 68.61it/s, est. speed input: 882.16 toks/s, output: 55.13 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:00<00:01, 71.75it/s, est. speed input: 927.66 toks/s, output: 57.98 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:00<00:00, 74.13it/s, est. speed input: 964.41 toks/s, output: 60.28 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:01<00:00, 75.45it/s, est. speed input: 992.74 toks/s, output: 62.05 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:01<00:00, 76.73it/s, est. speed input: 1017.46 toks/s, output: 63.59 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:01<00:00, 76.68it/s, est. speed input: 1034.78 toks/s, output: 64.67 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:01<00:00, 77.27it/s, est. speed input: 1051.55 toks/s, output: 65.72 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:01<00:00, 77.77it/s, est. speed input: 1066.30 toks/s, output: 66.64 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:01<00:00, 78.00it/s, est. speed input: 1078.74 toks/s, output: 67.42 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:01<00:00, 78.16it/s, est. speed input: 1089.64 toks/s, output: 68.10 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:01<00:00, 78.28it/s, est. speed input: 1099.32 toks/s, output: 68.71 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 78.28it/s, est. speed input: 1106.95 toks/s, output: 69.18 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 69.18it/s, est. speed input: 1106.95 toks/s, output: 69.18 toks/s]
[rank0]:[W125 06:21:16.979914349 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 22.2s

测试结果:
  Requests/s:   68.17
  Tokens/s:     1158.81
  Total Reqs:   128
  Elapsed:      1.88s

  [Prefill 分析]
  Total Prefill Tokens: 2048
  Prefill Tokens/s:     1090.64

============================================================
[2/3] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-0.5B-FP8                                │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 128 (= 1 x 128)
│   M_decode      = 1
│   batched_tokens = 129 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 128
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 129
│   --max-num-batched-tokens = 129
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 06:21:21 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1099909) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1099909) WARNING 01-25 06:21:29 [backends.py:609] Failed to read file <frozen os>
Throughput: 68.40 requests/s, 8823.62 total tokens/s, 68.40 output tokens/s
Total num prompt tokens:  16384
Total num output tokens:  128


─── STDERR ───
[2026-01-25 06:21:21] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:21:21] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:21:21] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:21:21] INFO kernels.py:97: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 06:21:21] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:21:21] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:21:21] INFO kernels.py:97: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 06:21:21] INFO kernels.py:249: FP8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:21:21] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:21:21] INFO kernels.py:373: INT8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:21:21] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:21:21] INFO kernels.py:97: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 06:21:21] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:21:21] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:21:21] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:21:21] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:21:21] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 06:21:21] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:21:21] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:21:21] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:21:21] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 06:21:25] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:21:25] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:21:25] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:21:25] INFO kernels.py:97: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 06:21:25] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:21:25] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:21:25] INFO kernels.py:97: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 06:21:25] INFO kernels.py:249: FP8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:21:25] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:21:25] INFO kernels.py:373: INT8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:21:25] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:21:25] INFO kernels.py:97: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 06:21:25] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:21:25] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:21:25] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:21:25] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:21:25] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 06:21:25] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:21:25] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:21:25] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:21:25] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1099909) [2026-01-25 06:21:25] INFO SlideSparseLinearMethod_FP8.py:730: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1099909) [2026-01-25 06:21:25] INFO gemm_wrapper.py:872: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1099909) [2026-01-25 06:21:25] INFO SlideSparseLinearMethod_FP8.py:388: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1099909) [2026-01-25 06:21:25] INFO SlideSparseLinearMethod_FP8.py:535: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=1099909) [2026-01-25 06:21:25] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
(EngineCore_DP0 pid=1099909) [2026-01-25 06:21:25] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8-SlideSparse-2_4
(EngineCore_DP0 pid=1099909) [2026-01-25 06:21:25] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
(EngineCore_DP0 pid=1099909) [2026-01-25 06:21:25] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8-SlideSparse-2_4
(EngineCore_DP0 pid=1099909) [2026-01-25 06:21:25] INFO SlideSparseLinearMethod_FP8.py:552: Preloaded FP8 Triton kernels for model: Qwen2.5-0.5B-FP8-SlideSparse-2_4
(EngineCore_DP0 pid=1099909) [2026-01-25 06:21:25] INFO SlideSparseLinearMethod_FP8.py:556: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=1099909) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1099909) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 14.27it/s]
(EngineCore_DP0 pid=1099909) 
(EngineCore_DP0 pid=1099909) [2026-01-25 06:21:25] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [1152, 896] -> 1D uint8
(EngineCore_DP0 pid=1099909) [2026-01-25 06:21:26] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 645120 bytes
(EngineCore_DP0 pid=1099909) [2026-01-25 06:21:26] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [896, 896] -> 1D uint8
(EngineCore_DP0 pid=1099909) [2026-01-25 06:21:26] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 501760 bytes
(EngineCore_DP0 pid=1099909) [2026-01-25 06:21:26] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [9728, 896] -> 1D uint8
(EngineCore_DP0 pid=1099909) [2026-01-25 06:21:26] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 5447680 bytes
(EngineCore_DP0 pid=1099909) [2026-01-25 06:21:26] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [896, 4864] -> 1D uint8
(EngineCore_DP0 pid=1099909) [2026-01-25 06:21:26] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 2723840 bytes
(EngineCore_DP0 pid=1099909) [2026-01-25 06:21:32] WARNING gemm_wrapper.py:393: No cuSPARSELt config for model 'Qwen2.5-0.5B-FP8', using default algorithm
(EngineCore_DP0 pid=1099909) 2026-01-25 06:21:34,505 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1099909) 2026-01-25 06:21:34,518 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1099909) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 27.09it/s]
(EngineCore_DP0 pid=1099909) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.91it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.90it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 2726.98it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:25,  4.94it/s, est. speed input: 632.95 toks/s, output: 4.94 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:03, 35.39it/s, est. speed input: 3758.80 toks/s, output: 29.37 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:02, 50.92it/s, est. speed input: 5292.85 toks/s, output: 41.35 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:00<00:01, 59.53it/s, est. speed input: 6184.01 toks/s, output: 48.31 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:00<00:01, 65.90it/s, est. speed input: 6828.72 toks/s, output: 53.35 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:00<00:01, 69.96it/s, est. speed input: 7285.90 toks/s, output: 56.92 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:00<00:01, 72.65it/s, est. speed input: 7628.96 toks/s, output: 59.60 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:00<00:00, 74.60it/s, est. speed input: 7901.08 toks/s, output: 61.73 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:01<00:00, 76.02it/s, est. speed input: 8122.42 toks/s, output: 63.46 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:01<00:00, 76.88it/s, est. speed input: 8299.84 toks/s, output: 64.84 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:01<00:00, 77.24it/s, est. speed input: 8441.02 toks/s, output: 65.95 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:01<00:00, 77.74it/s, est. speed input: 8567.51 toks/s, output: 66.93 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:01<00:00, 78.01it/s, est. speed input: 8673.84 toks/s, output: 67.76 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:01<00:00, 78.33it/s, est. speed input: 8769.58 toks/s, output: 68.51 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:01<00:00, 78.74it/s, est. speed input: 8857.48 toks/s, output: 69.20 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:01<00:00, 78.86it/s, est. speed input: 8931.68 toks/s, output: 69.78 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 78.86it/s, est. speed input: 8984.85 toks/s, output: 70.19 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 70.19it/s, est. speed input: 8984.85 toks/s, output: 70.19 toks/s]
[rank0]:[W125 06:21:37.675279312 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 21.7s

测试结果:
  Requests/s:   68.40
  Tokens/s:     8823.62
  Total Reqs:   128
  Elapsed:      1.87s

  [Prefill 分析]
  Total Prefill Tokens: 16384
  Prefill Tokens/s:     8755.22

============================================================
[3/3] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-0.5B-FP8                                │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 256 (= 1 x 256)
│   M_decode      = 1
│   batched_tokens = 257 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 256
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 257
│   --max-num-batched-tokens = 257
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 06:21:42 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1100331) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1100331) WARNING 01-25 06:21:50 [backends.py:609] Failed to read file <frozen os>
Throughput: 73.23 requests/s, 18819.35 total tokens/s, 73.23 output tokens/s
Total num prompt tokens:  32768
Total num output tokens:  128


─── STDERR ───
[2026-01-25 06:21:42] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:21:42] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:21:42] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:21:42] INFO kernels.py:97: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 06:21:42] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:21:42] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:21:42] INFO kernels.py:97: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 06:21:42] INFO kernels.py:249: FP8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:21:42] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:21:42] INFO kernels.py:373: INT8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:21:42] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:21:42] INFO kernels.py:97: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 06:21:42] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:21:42] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:21:42] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:21:42] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:21:42] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 06:21:42] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:21:42] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:21:42] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:21:42] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 06:21:46] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:21:46] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:21:46] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:21:46] INFO kernels.py:97: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 06:21:46] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:21:46] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:21:46] INFO kernels.py:97: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 06:21:46] INFO kernels.py:249: FP8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:21:46] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:21:46] INFO kernels.py:373: INT8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:21:46] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:21:46] INFO kernels.py:97: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 06:21:46] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:21:46] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:21:46] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:21:46] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:21:46] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 06:21:46] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:21:46] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:21:46] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:21:46] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1100331) [2026-01-25 06:21:47] INFO SlideSparseLinearMethod_FP8.py:730: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1100331) [2026-01-25 06:21:47] INFO gemm_wrapper.py:872: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1100331) [2026-01-25 06:21:47] INFO SlideSparseLinearMethod_FP8.py:388: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1100331) [2026-01-25 06:21:47] INFO SlideSparseLinearMethod_FP8.py:535: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=1100331) [2026-01-25 06:21:47] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
(EngineCore_DP0 pid=1100331) [2026-01-25 06:21:47] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8-SlideSparse-2_4
(EngineCore_DP0 pid=1100331) [2026-01-25 06:21:47] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
(EngineCore_DP0 pid=1100331) [2026-01-25 06:21:47] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8-SlideSparse-2_4
(EngineCore_DP0 pid=1100331) [2026-01-25 06:21:47] INFO SlideSparseLinearMethod_FP8.py:552: Preloaded FP8 Triton kernels for model: Qwen2.5-0.5B-FP8-SlideSparse-2_4
(EngineCore_DP0 pid=1100331) [2026-01-25 06:21:47] INFO SlideSparseLinearMethod_FP8.py:556: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=1100331) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1100331) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 14.25it/s]
(EngineCore_DP0 pid=1100331) 
(EngineCore_DP0 pid=1100331) [2026-01-25 06:21:47] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [1152, 896] -> 1D uint8
(EngineCore_DP0 pid=1100331) [2026-01-25 06:21:47] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 645120 bytes
(EngineCore_DP0 pid=1100331) [2026-01-25 06:21:47] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [896, 896] -> 1D uint8
(EngineCore_DP0 pid=1100331) [2026-01-25 06:21:47] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 501760 bytes
(EngineCore_DP0 pid=1100331) [2026-01-25 06:21:47] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [9728, 896] -> 1D uint8
(EngineCore_DP0 pid=1100331) [2026-01-25 06:21:47] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 5447680 bytes
(EngineCore_DP0 pid=1100331) [2026-01-25 06:21:47] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [896, 4864] -> 1D uint8
(EngineCore_DP0 pid=1100331) [2026-01-25 06:21:47] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 2723840 bytes
(EngineCore_DP0 pid=1100331) [2026-01-25 06:21:53] WARNING gemm_wrapper.py:393: No cuSPARSELt config for model 'Qwen2.5-0.5B-FP8', using default algorithm
(EngineCore_DP0 pid=1100331) 2026-01-25 06:21:56,054 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1100331) 2026-01-25 06:21:56,067 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1100331) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 27.34it/s]
(EngineCore_DP0 pid=1100331) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.96it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.95it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 1489.40it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   3%|▎         | 4/128 [00:00<00:03, 35.50it/s, est. speed input: 9087.36 toks/s, output: 35.50 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:01, 61.82it/s, est. speed input: 14813.36 toks/s, output: 57.86 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:01, 69.56it/s, est. speed input: 16551.29 toks/s, output: 64.65 toks/s]
Processed prompts:  23%|██▎       | 30/128 [00:00<00:01, 73.85it/s, est. speed input: 17566.95 toks/s, output: 68.62 toks/s]
Processed prompts:  30%|███       | 39/128 [00:00<00:01, 76.25it/s, est. speed input: 18182.37 toks/s, output: 71.02 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:00<00:01, 77.38it/s, est. speed input: 18528.53 toks/s, output: 72.38 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:00<00:00, 78.38it/s, est. speed input: 18828.13 toks/s, output: 73.55 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:00<00:00, 78.51it/s, est. speed input: 18986.56 toks/s, output: 74.17 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:00<00:00, 79.10it/s, est. speed input: 19167.89 toks/s, output: 74.87 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:01<00:00, 79.55it/s, est. speed input: 19316.47 toks/s, output: 75.45 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:01<00:00, 79.78it/s, est. speed input: 19432.34 toks/s, output: 75.91 toks/s]
Processed prompts:  78%|███████▊  | 100/128 [00:01<00:00, 80.09it/s, est. speed input: 19538.70 toks/s, output: 76.32 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:01<00:00, 80.17it/s, est. speed input: 19619.91 toks/s, output: 76.64 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:01<00:00, 80.05it/s, est. speed input: 19679.19 toks/s, output: 76.87 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:01<00:00, 79.91it/s, est. speed input: 19726.65 toks/s, output: 77.06 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 79.91it/s, est. speed input: 19730.82 toks/s, output: 77.07 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 77.07it/s, est. speed input: 19730.82 toks/s, output: 77.07 toks/s]
[rank0]:[W125 06:21:59.927078920 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 21.3s

测试结果:
  Requests/s:   73.23
  Tokens/s:     18819.35
  Total Reqs:   128
  Elapsed:      1.75s

  [Prefill 分析]
  Total Prefill Tokens: 32768
  Prefill Tokens/s:     18746.12


------------------------------------------------------------
  生成 CSV: Qwen2.5-0.5B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_4/Qwen2.5-0.5B-FP8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
16,16,1,128,128,68.1653,1158.8101,1.8778
128,128,1,128,128,68.4002,8823.6229,1.8713
256,256,1,128,128,73.2270,18819.3474,1.7480

------------------------------------------------------------

[INFO] 完成: 3 成功, 0 失败

============================================================
  Qwen2.5-0.5B-FP8 | cuSPARSELt (2_4) | decode
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_4
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_4

============================================================
[1/3] 测试 M=16
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-0.5B-FP8                                │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16
│   M_prefill     = 256 (= 16 x 16)
│   M_decode      = 16
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 16
│   --max-num-seqs           = 16
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 06:22:04 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1100713) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1100713) WARNING 01-25 06:22:12 [backends.py:609] Failed to read file <frozen os>
Throughput: 17.86 requests/s, 4857.03 total tokens/s, 4571.32 output tokens/s
Total num prompt tokens:  256
Total num output tokens:  4096


─── STDERR ───
[2026-01-25 06:22:04] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:22:04] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:22:04] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:22:04] INFO kernels.py:97: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 06:22:04] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:22:04] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:22:04] INFO kernels.py:97: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 06:22:04] INFO kernels.py:249: FP8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:22:04] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:22:04] INFO kernels.py:373: INT8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:22:04] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:22:04] INFO kernels.py:97: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 06:22:04] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:22:04] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:22:04] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:22:04] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:22:04] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 06:22:04] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:22:04] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:22:04] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:22:04] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 06:22:07] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:22:07] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:22:07] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:22:07] INFO kernels.py:97: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 06:22:07] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:22:07] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:22:07] INFO kernels.py:97: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 06:22:07] INFO kernels.py:249: FP8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:22:07] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:22:07] INFO kernels.py:373: INT8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:22:07] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:22:07] INFO kernels.py:97: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 06:22:07] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:22:07] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:22:07] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:22:07] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:22:07] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 06:22:07] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:22:07] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:22:07] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:22:07] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1100713) [2026-01-25 06:22:08] INFO SlideSparseLinearMethod_FP8.py:730: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1100713) [2026-01-25 06:22:08] INFO gemm_wrapper.py:872: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1100713) [2026-01-25 06:22:08] INFO SlideSparseLinearMethod_FP8.py:388: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1100713) [2026-01-25 06:22:08] INFO SlideSparseLinearMethod_FP8.py:535: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=1100713) [2026-01-25 06:22:08] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
(EngineCore_DP0 pid=1100713) [2026-01-25 06:22:08] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8-SlideSparse-2_4
(EngineCore_DP0 pid=1100713) [2026-01-25 06:22:08] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
(EngineCore_DP0 pid=1100713) [2026-01-25 06:22:08] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8-SlideSparse-2_4
(EngineCore_DP0 pid=1100713) [2026-01-25 06:22:08] INFO SlideSparseLinearMethod_FP8.py:552: Preloaded FP8 Triton kernels for model: Qwen2.5-0.5B-FP8-SlideSparse-2_4
(EngineCore_DP0 pid=1100713) [2026-01-25 06:22:08] INFO SlideSparseLinearMethod_FP8.py:556: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=1100713) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1100713) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 14.06it/s]
(EngineCore_DP0 pid=1100713) 
(EngineCore_DP0 pid=1100713) [2026-01-25 06:22:08] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [1152, 896] -> 1D uint8
(EngineCore_DP0 pid=1100713) [2026-01-25 06:22:08] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 645120 bytes
(EngineCore_DP0 pid=1100713) [2026-01-25 06:22:08] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [896, 896] -> 1D uint8
(EngineCore_DP0 pid=1100713) [2026-01-25 06:22:08] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 501760 bytes
(EngineCore_DP0 pid=1100713) [2026-01-25 06:22:08] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [9728, 896] -> 1D uint8
(EngineCore_DP0 pid=1100713) [2026-01-25 06:22:08] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 5447680 bytes
(EngineCore_DP0 pid=1100713) [2026-01-25 06:22:08] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [896, 4864] -> 1D uint8
(EngineCore_DP0 pid=1100713) [2026-01-25 06:22:08] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 2723840 bytes
(EngineCore_DP0 pid=1100713) [2026-01-25 06:22:14] WARNING gemm_wrapper.py:393: No cuSPARSELt config for model 'Qwen2.5-0.5B-FP8', using default algorithm
(EngineCore_DP0 pid=1100713) 2026-01-25 06:22:16,985 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1100713) 2026-01-25 06:22:16,997 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1100713) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 3/7 [00:00<00:00, 27.13it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 29.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 29.07it/s]
(EngineCore_DP0 pid=1100713) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 1/5 [00:00<00:00,  6.81it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 22.32it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 19.63it/s]

Adding requests:   0%|          | 0/16 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 16/16 [00:00<00:00, 1177.10it/s]

Processed prompts:   0%|          | 0/16 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   6%|▋         | 1/16 [00:00<00:13,  1.14it/s, est. speed input: 18.25 toks/s, output: 292.00 toks/s]
Processed prompts: 100%|██████████| 16/16 [00:00<00:00,  1.14it/s, est. speed input: 290.82 toks/s, output: 4653.12 toks/s]
Processed prompts: 100%|██████████| 16/16 [00:00<00:00, 18.17it/s, est. speed input: 290.82 toks/s, output: 4653.12 toks/s]
[rank0]:[W125 06:22:19.291250696 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 20.5s

测试结果:
  Requests/s:   17.86
  Tokens/s:     4857.03
  Total Reqs:   16
  Elapsed:      0.90s

  [Decode 分析]
  Total Decode Tokens:  4096
  Decode Tokens/s:      4571.32

============================================================
[2/3] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-0.5B-FP8                                │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 2048 (= 128 x 16)
│   M_decode      = 128
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 128
│   --max-num-seqs           = 128
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 06:22:24 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1101118) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1101118) WARNING 01-25 06:22:32 [backends.py:609] Failed to read file <frozen os>
Throughput: 85.58 requests/s, 23278.56 total tokens/s, 21909.24 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768


─── STDERR ───
[2026-01-25 06:22:24] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:22:24] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:22:24] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:22:24] INFO kernels.py:97: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 06:22:24] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:22:24] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:22:24] INFO kernels.py:97: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 06:22:24] INFO kernels.py:249: FP8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:22:24] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:22:24] INFO kernels.py:373: INT8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:22:24] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:22:24] INFO kernels.py:97: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 06:22:24] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:22:24] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:22:24] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:22:24] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:22:24] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 06:22:24] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:22:24] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:22:24] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:22:24] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 06:22:28] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:22:28] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:22:28] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:22:28] INFO kernels.py:97: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 06:22:28] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:22:28] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:22:28] INFO kernels.py:97: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 06:22:28] INFO kernels.py:249: FP8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:22:28] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:22:28] INFO kernels.py:373: INT8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:22:28] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:22:28] INFO kernels.py:97: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 06:22:28] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:22:28] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:22:28] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:22:28] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:22:28] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 06:22:28] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:22:28] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:22:28] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:22:28] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1101118) [2026-01-25 06:22:28] INFO SlideSparseLinearMethod_FP8.py:730: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1101118) [2026-01-25 06:22:28] INFO gemm_wrapper.py:872: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1101118) [2026-01-25 06:22:28] INFO SlideSparseLinearMethod_FP8.py:388: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1101118) [2026-01-25 06:22:28] INFO SlideSparseLinearMethod_FP8.py:535: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=1101118) [2026-01-25 06:22:28] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
(EngineCore_DP0 pid=1101118) [2026-01-25 06:22:28] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8-SlideSparse-2_4
(EngineCore_DP0 pid=1101118) [2026-01-25 06:22:28] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
(EngineCore_DP0 pid=1101118) [2026-01-25 06:22:28] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8-SlideSparse-2_4
(EngineCore_DP0 pid=1101118) [2026-01-25 06:22:28] INFO SlideSparseLinearMethod_FP8.py:552: Preloaded FP8 Triton kernels for model: Qwen2.5-0.5B-FP8-SlideSparse-2_4
(EngineCore_DP0 pid=1101118) [2026-01-25 06:22:28] INFO SlideSparseLinearMethod_FP8.py:556: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=1101118) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1101118) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 13.98it/s]
(EngineCore_DP0 pid=1101118) 
(EngineCore_DP0 pid=1101118) [2026-01-25 06:22:29] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [1152, 896] -> 1D uint8
(EngineCore_DP0 pid=1101118) [2026-01-25 06:22:29] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 645120 bytes
(EngineCore_DP0 pid=1101118) [2026-01-25 06:22:29] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [896, 896] -> 1D uint8
(EngineCore_DP0 pid=1101118) [2026-01-25 06:22:29] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 501760 bytes
(EngineCore_DP0 pid=1101118) [2026-01-25 06:22:29] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [9728, 896] -> 1D uint8
(EngineCore_DP0 pid=1101118) [2026-01-25 06:22:29] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 5447680 bytes
(EngineCore_DP0 pid=1101118) [2026-01-25 06:22:29] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [896, 4864] -> 1D uint8
(EngineCore_DP0 pid=1101118) [2026-01-25 06:22:29] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 2723840 bytes
(EngineCore_DP0 pid=1101118) [2026-01-25 06:22:34] WARNING gemm_wrapper.py:393: No cuSPARSELt config for model 'Qwen2.5-0.5B-FP8', using default algorithm
(EngineCore_DP0 pid=1101118) 2026-01-25 06:22:36,106 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1101118) 2026-01-25 06:22:36,119 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1101118) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|▎         | 1/35 [00:00<00:04,  7.72it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█▏        | 4/35 [00:00<00:01, 16.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 7/35 [00:00<00:01, 21.87it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 11/35 [00:00<00:00, 25.44it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 15/35 [00:00<00:00, 27.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  54%|█████▍    | 19/35 [00:00<00:00, 28.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  66%|██████▌   | 23/35 [00:00<00:00, 29.02it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  77%|███████▋  | 27/35 [00:01<00:00, 29.17it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▊ | 31/35 [00:01<00:00, 29.59it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:01<00:00, 29.90it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:01<00:00, 27.23it/s]
(EngineCore_DP0 pid=1101118) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   5%|▌         | 1/19 [00:00<00:02,  6.91it/s]
Capturing CUDA graphs (decode, FULL):  26%|██▋       | 5/19 [00:00<00:00, 22.20it/s]
Capturing CUDA graphs (decode, FULL):  47%|████▋     | 9/19 [00:00<00:00, 28.15it/s]
Capturing CUDA graphs (decode, FULL):  68%|██████▊   | 13/19 [00:00<00:00, 31.21it/s]
Capturing CUDA graphs (decode, FULL):  89%|████████▉ | 17/19 [00:00<00:00, 33.05it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:00<00:00, 29.65it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  83%|████████▎ | 106/128 [00:00<00:00, 694.65it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 821.25it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:01<02:44,  1.29s/it, est. speed input: 12.37 toks/s, output: 197.99 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00,  1.29s/it, est. speed input: 1529.54 toks/s, output: 24472.65 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 95.59it/s, est. speed input: 1529.54 toks/s, output: 24472.65 toks/s]
[rank0]:[W125 06:22:40.377439219 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 21.0s

测试结果:
  Requests/s:   85.58
  Tokens/s:     23278.56
  Total Reqs:   128
  Elapsed:      1.50s

  [Decode 分析]
  Total Decode Tokens:  32768
  Decode Tokens/s:      21909.24

============================================================
[3/3] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-0.5B-FP8                                │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 4096 (= 256 x 16)
│   M_decode      = 256
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 256
│   --max-num-seqs           = 256
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 06:22:45 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1101486) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1101486) WARNING 01-25 06:22:53 [backends.py:609] Failed to read file <frozen os>
Throughput: 105.25 requests/s, 28628.63 total tokens/s, 26944.59 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536


─── STDERR ───
[2026-01-25 06:22:45] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:22:45] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:22:45] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:22:45] INFO kernels.py:97: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 06:22:45] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:22:45] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:22:45] INFO kernels.py:97: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 06:22:45] INFO kernels.py:249: FP8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:22:45] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:22:45] INFO kernels.py:373: INT8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:22:45] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:22:45] INFO kernels.py:97: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 06:22:45] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:22:45] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:22:45] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:22:45] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:22:45] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 06:22:45] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:22:45] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:22:45] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:22:45] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 06:22:49] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:22:49] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:22:49] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:22:49] INFO kernels.py:97: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 06:22:49] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:22:49] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:22:49] INFO kernels.py:97: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 06:22:49] INFO kernels.py:249: FP8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:22:49] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:22:49] INFO kernels.py:373: INT8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:22:49] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:22:49] INFO kernels.py:97: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 06:22:49] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:22:49] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:22:49] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:22:49] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:22:49] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 06:22:49] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:22:49] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:22:49] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:22:49] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1101486) [2026-01-25 06:22:49] INFO SlideSparseLinearMethod_FP8.py:730: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1101486) [2026-01-25 06:22:49] INFO gemm_wrapper.py:872: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1101486) [2026-01-25 06:22:49] INFO SlideSparseLinearMethod_FP8.py:388: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1101486) [2026-01-25 06:22:49] INFO SlideSparseLinearMethod_FP8.py:535: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=1101486) [2026-01-25 06:22:49] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
(EngineCore_DP0 pid=1101486) [2026-01-25 06:22:49] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8-SlideSparse-2_4
(EngineCore_DP0 pid=1101486) [2026-01-25 06:22:49] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
(EngineCore_DP0 pid=1101486) [2026-01-25 06:22:49] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8-SlideSparse-2_4
(EngineCore_DP0 pid=1101486) [2026-01-25 06:22:49] INFO SlideSparseLinearMethod_FP8.py:552: Preloaded FP8 Triton kernels for model: Qwen2.5-0.5B-FP8-SlideSparse-2_4
(EngineCore_DP0 pid=1101486) [2026-01-25 06:22:49] INFO SlideSparseLinearMethod_FP8.py:556: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=1101486) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1101486) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 14.23it/s]
(EngineCore_DP0 pid=1101486) 
(EngineCore_DP0 pid=1101486) [2026-01-25 06:22:50] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [1152, 896] -> 1D uint8
(EngineCore_DP0 pid=1101486) [2026-01-25 06:22:50] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 645120 bytes
(EngineCore_DP0 pid=1101486) [2026-01-25 06:22:50] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [896, 896] -> 1D uint8
(EngineCore_DP0 pid=1101486) [2026-01-25 06:22:50] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 501760 bytes
(EngineCore_DP0 pid=1101486) [2026-01-25 06:22:50] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [9728, 896] -> 1D uint8
(EngineCore_DP0 pid=1101486) [2026-01-25 06:22:50] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 5447680 bytes
(EngineCore_DP0 pid=1101486) [2026-01-25 06:22:50] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [896, 4864] -> 1D uint8
(EngineCore_DP0 pid=1101486) [2026-01-25 06:22:50] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 2723840 bytes
(EngineCore_DP0 pid=1101486) [2026-01-25 06:22:55] WARNING gemm_wrapper.py:393: No cuSPARSELt config for model 'Qwen2.5-0.5B-FP8', using default algorithm
(EngineCore_DP0 pid=1101486) 2026-01-25 06:22:57,195 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1101486) 2026-01-25 06:22:57,207 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1101486) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/36 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|▎         | 1/36 [00:00<00:04,  7.87it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 4/36 [00:00<00:01, 18.67it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 8/36 [00:00<00:01, 24.43it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 12/36 [00:00<00:00, 26.82it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  44%|████▍     | 16/36 [00:00<00:00, 28.29it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  56%|█████▌    | 20/36 [00:00<00:00, 28.79it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 24/36 [00:00<00:00, 29.42it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 27/36 [00:00<00:00, 29.46it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  83%|████████▎ | 30/36 [00:01<00:00, 29.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 34/36 [00:01<00:00, 30.42it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:01<00:00, 27.82it/s]
(EngineCore_DP0 pid=1101486) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   3%|▎         | 1/35 [00:00<00:04,  6.85it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 5/35 [00:00<00:01, 22.16it/s]
Capturing CUDA graphs (decode, FULL):  26%|██▌       | 9/35 [00:00<00:00, 28.07it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 13/35 [00:00<00:00, 31.15it/s]
Capturing CUDA graphs (decode, FULL):  49%|████▊     | 17/35 [00:00<00:00, 33.05it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 21/35 [00:00<00:00, 34.18it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 25/35 [00:00<00:00, 34.99it/s]
Capturing CUDA graphs (decode, FULL):  83%|████████▎ | 29/35 [00:00<00:00, 35.71it/s]
Capturing CUDA graphs (decode, FULL):  94%|█████████▍| 33/35 [00:01<00:00, 36.04it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:01<00:00, 32.55it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 8253.14it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:02<09:12,  2.17s/it, est. speed input: 7.38 toks/s, output: 118.13 toks/s]
Processed prompts:  62%|██████▏   | 159/256 [00:02<00:00, 98.07it/s, est. speed input: 1121.51 toks/s, output: 17944.11 toks/s]
Processed prompts:  98%|█████████▊| 251/256 [00:02<00:00, 162.90it/s, est. speed input: 1685.78 toks/s, output: 26972.46 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 162.90it/s, est. speed input: 1706.45 toks/s, output: 27303.24 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 106.65it/s, est. speed input: 1706.45 toks/s, output: 27303.24 toks/s]
[rank0]:[W125 06:23:03.987593069 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 22.6s

测试结果:
  Requests/s:   105.25
  Tokens/s:     28628.63
  Total Reqs:   256
  Elapsed:      2.43s

  [Decode 分析]
  Total Decode Tokens:  65536
  Decode Tokens/s:      26944.59


------------------------------------------------------------
  生成 CSV: Qwen2.5-0.5B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_4/Qwen2.5-0.5B-FP8_decode.csv

预览:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
16,16,16,16,256,256,17.8567,4857.0274,0.8960
128,16,128,128,256,256,85.5830,23278.5640,1.4956
256,16,256,256,256,256,105.2523,28628.6292,2.4323

------------------------------------------------------------

[INFO] 完成: 3 成功, 0 失败

======================================================================
[SlideSparse] 未找到 Qwen2.5-0.5B-FP8-SlideSparse-2_6 checkpoint
[SlideSparse] 发现基础模型: Qwen2.5-0.5B-FP8
[SlideSparse] 自动转换中...
[SlideSparse] 开始转换: Qwen2.5-0.5B-FP8 -> SlideSparse-2_6
======================================================================

[SlideSparse] 转换成功: Qwen2.5-0.5B-FP8-SlideSparse-2_6
======================================================================


============================================================
  Qwen2.5-0.5B-FP8 | cuSPARSELt (2_6) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6

============================================================
[1/3] 测试 M=16
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-0.5B-FP8                                │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16
│   M_prefill     = 16 (= 1 x 16)
│   M_decode      = 1
│   batched_tokens = 17 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 17
│   --max-num-batched-tokens = 17
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 06:23:41 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1102207) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1102207) WARNING 01-25 06:23:49 [backends.py:609] Failed to read file <frozen os>
Throughput: 70.01 requests/s, 1190.23 total tokens/s, 70.01 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  128


─── STDERR ───
[2026-01-25 06:23:41] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:23:41] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:23:41] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:23:41] INFO kernels.py:97: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 06:23:41] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:23:41] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:23:41] INFO kernels.py:97: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 06:23:41] INFO kernels.py:249: FP8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:23:41] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:23:41] INFO kernels.py:373: INT8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:23:41] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:23:41] INFO kernels.py:97: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 06:23:41] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:23:41] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:23:41] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:23:41] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:23:41] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 06:23:41] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:23:41] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:23:41] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:23:41] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 06:23:44] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:23:44] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:23:44] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:23:44] INFO kernels.py:97: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 06:23:44] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:23:44] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:23:44] INFO kernels.py:97: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 06:23:44] INFO kernels.py:249: FP8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:23:44] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:23:44] INFO kernels.py:373: INT8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:23:44] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:23:44] INFO kernels.py:97: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 06:23:44] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:23:44] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:23:44] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:23:44] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:23:44] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 06:23:44] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:23:44] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:23:44] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:23:44] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1102207) [2026-01-25 06:23:45] INFO SlideSparseLinearMethod_FP8.py:730: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1102207) [2026-01-25 06:23:45] INFO gemm_wrapper.py:872: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1102207) [2026-01-25 06:23:45] INFO SlideSparseLinearMethod_FP8.py:388: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1102207) [2026-01-25 06:23:45] INFO SlideSparseLinearMethod_FP8.py:535: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=1102207) [2026-01-25 06:23:45] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
(EngineCore_DP0 pid=1102207) [2026-01-25 06:23:45] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8-SlideSparse-2_6
(EngineCore_DP0 pid=1102207) [2026-01-25 06:23:45] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
(EngineCore_DP0 pid=1102207) [2026-01-25 06:23:45] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8-SlideSparse-2_6
(EngineCore_DP0 pid=1102207) [2026-01-25 06:23:45] INFO SlideSparseLinearMethod_FP8.py:552: Preloaded FP8 Triton kernels for model: Qwen2.5-0.5B-FP8-SlideSparse-2_6
(EngineCore_DP0 pid=1102207) [2026-01-25 06:23:45] INFO SlideSparseLinearMethod_FP8.py:556: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=1102207) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1102207) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 12.80it/s]
(EngineCore_DP0 pid=1102207) 
(EngineCore_DP0 pid=1102207) [2026-01-25 06:23:45] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [1152, 1216] -> 1D uint8
(EngineCore_DP0 pid=1102207) [2026-01-25 06:23:45] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 884736 bytes
(EngineCore_DP0 pid=1102207) [2026-01-25 06:23:45] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [896, 1216] -> 1D uint8
(EngineCore_DP0 pid=1102207) [2026-01-25 06:23:45] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 688128 bytes
(EngineCore_DP0 pid=1102207) [2026-01-25 06:23:45] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [9728, 1216] -> 1D uint8
(EngineCore_DP0 pid=1102207) [2026-01-25 06:23:45] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 7471104 bytes
(EngineCore_DP0 pid=1102207) [2026-01-25 06:23:45] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [896, 6496] -> 1D uint8
(EngineCore_DP0 pid=1102207) [2026-01-25 06:23:45] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 3641344 bytes
(EngineCore_DP0 pid=1102207) [2026-01-25 06:23:51] WARNING gemm_wrapper.py:393: No cuSPARSELt config for model 'Qwen2.5-0.5B-FP8', using default algorithm
(EngineCore_DP0 pid=1102207) 2026-01-25 06:23:54,466 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1102207) 2026-01-25 06:23:54,478 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1102207) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  9.01it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  9.00it/s]
(EngineCore_DP0 pid=1102207) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.78it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.77it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 7563.05it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:28,  4.48it/s, est. speed input: 71.67 toks/s, output: 4.48 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:03, 33.79it/s, est. speed input: 443.87 toks/s, output: 27.74 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:00<00:02, 51.59it/s, est. speed input: 659.50 toks/s, output: 41.22 toks/s]
Processed prompts:  21%|██        | 27/128 [00:00<00:01, 61.74it/s, est. speed input: 787.16 toks/s, output: 49.20 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:00<00:01, 68.08it/s, est. speed input: 872.24 toks/s, output: 54.51 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:00<00:01, 72.04it/s, est. speed input: 932.10 toks/s, output: 58.25 toks/s]
Processed prompts:  42%|████▏     | 54/128 [00:00<00:00, 74.64it/s, est. speed input: 976.69 toks/s, output: 61.04 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:00<00:00, 76.04it/s, est. speed input: 1006.81 toks/s, output: 62.93 toks/s]
Processed prompts:  55%|█████▍    | 70/128 [00:01<00:00, 77.08it/s, est. speed input: 1031.52 toks/s, output: 64.47 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:01<00:00, 78.08it/s, est. speed input: 1055.16 toks/s, output: 65.95 toks/s]
Processed prompts:  69%|██████▉   | 88/128 [00:01<00:00, 79.10it/s, est. speed input: 1076.01 toks/s, output: 67.25 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:01<00:00, 78.57it/s, est. speed input: 1087.80 toks/s, output: 67.99 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:01<00:00, 78.85it/s, est. speed input: 1100.07 toks/s, output: 68.75 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:01<00:00, 79.30it/s, est. speed input: 1112.79 toks/s, output: 69.55 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:01<00:00, 79.73it/s, est. speed input: 1124.23 toks/s, output: 70.26 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 79.73it/s, est. speed input: 1131.28 toks/s, output: 70.71 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 70.70it/s, est. speed input: 1131.28 toks/s, output: 70.71 toks/s]
[rank0]:[W125 06:23:57.756577707 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 21.8s

测试结果:
  Requests/s:   70.01
  Tokens/s:     1190.23
  Total Reqs:   128
  Elapsed:      1.83s

  [Prefill 分析]
  Total Prefill Tokens: 2048
  Prefill Tokens/s:     1120.22

============================================================
[2/3] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-0.5B-FP8                                │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 128 (= 1 x 128)
│   M_decode      = 1
│   batched_tokens = 129 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 128
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 129
│   --max-num-batched-tokens = 129
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 06:24:03 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1102576) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1102576) WARNING 01-25 06:24:10 [backends.py:609] Failed to read file <frozen os>
Throughput: 69.94 requests/s, 9022.54 total tokens/s, 69.94 output tokens/s
Total num prompt tokens:  16384
Total num output tokens:  128


─── STDERR ───
[2026-01-25 06:24:03] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:24:03] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:24:03] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:24:03] INFO kernels.py:97: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 06:24:03] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:24:03] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:24:03] INFO kernels.py:97: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 06:24:03] INFO kernels.py:249: FP8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:24:03] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:24:03] INFO kernels.py:373: INT8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:24:03] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:24:03] INFO kernels.py:97: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 06:24:03] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:24:03] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:24:03] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:24:03] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:24:03] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 06:24:03] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:24:03] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:24:03] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:24:03] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 06:24:06] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:24:06] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:24:06] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:24:06] INFO kernels.py:97: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 06:24:06] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:24:06] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:24:06] INFO kernels.py:97: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 06:24:06] INFO kernels.py:249: FP8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:24:06] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:24:06] INFO kernels.py:373: INT8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:24:06] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:24:06] INFO kernels.py:97: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 06:24:06] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:24:06] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:24:06] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:24:06] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:24:06] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 06:24:06] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:24:06] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:24:06] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:24:06] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1102576) [2026-01-25 06:24:07] INFO SlideSparseLinearMethod_FP8.py:730: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1102576) [2026-01-25 06:24:07] INFO gemm_wrapper.py:872: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1102576) [2026-01-25 06:24:07] INFO SlideSparseLinearMethod_FP8.py:388: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1102576) [2026-01-25 06:24:07] INFO SlideSparseLinearMethod_FP8.py:535: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=1102576) [2026-01-25 06:24:07] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
(EngineCore_DP0 pid=1102576) [2026-01-25 06:24:07] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8-SlideSparse-2_6
(EngineCore_DP0 pid=1102576) [2026-01-25 06:24:07] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
(EngineCore_DP0 pid=1102576) [2026-01-25 06:24:07] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8-SlideSparse-2_6
(EngineCore_DP0 pid=1102576) [2026-01-25 06:24:07] INFO SlideSparseLinearMethod_FP8.py:552: Preloaded FP8 Triton kernels for model: Qwen2.5-0.5B-FP8-SlideSparse-2_6
(EngineCore_DP0 pid=1102576) [2026-01-25 06:24:07] INFO SlideSparseLinearMethod_FP8.py:556: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=1102576) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1102576) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 12.86it/s]
(EngineCore_DP0 pid=1102576) 
(EngineCore_DP0 pid=1102576) [2026-01-25 06:24:07] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [1152, 1216] -> 1D uint8
(EngineCore_DP0 pid=1102576) [2026-01-25 06:24:07] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 884736 bytes
(EngineCore_DP0 pid=1102576) [2026-01-25 06:24:07] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [896, 1216] -> 1D uint8
(EngineCore_DP0 pid=1102576) [2026-01-25 06:24:07] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 688128 bytes
(EngineCore_DP0 pid=1102576) [2026-01-25 06:24:07] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [9728, 1216] -> 1D uint8
(EngineCore_DP0 pid=1102576) [2026-01-25 06:24:07] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 7471104 bytes
(EngineCore_DP0 pid=1102576) [2026-01-25 06:24:07] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [896, 6496] -> 1D uint8
(EngineCore_DP0 pid=1102576) [2026-01-25 06:24:07] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 3641344 bytes
(EngineCore_DP0 pid=1102576) [2026-01-25 06:24:13] WARNING gemm_wrapper.py:393: No cuSPARSELt config for model 'Qwen2.5-0.5B-FP8', using default algorithm
(EngineCore_DP0 pid=1102576) 2026-01-25 06:24:16,079 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1102576) 2026-01-25 06:24:16,091 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1102576) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 26.78it/s]
(EngineCore_DP0 pid=1102576) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.00it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  7.00it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  83%|████████▎ | 106/128 [00:00<00:00, 546.13it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 630.23it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   5%|▌         | 7/128 [00:00<00:01, 69.97it/s, est. speed input: 8957.21 toks/s, output: 69.97 toks/s]
Processed prompts:  12%|█▎        | 16/128 [00:00<00:01, 76.14it/s, est. speed input: 9635.23 toks/s, output: 75.27 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:00<00:01, 78.06it/s, est. speed input: 9855.38 toks/s, output: 76.99 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:00<00:01, 78.51it/s, est. speed input: 9924.02 toks/s, output: 77.53 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:00<00:01, 78.76it/s, est. speed input: 9965.78 toks/s, output: 77.86 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:00<00:01, 78.71it/s, est. speed input: 9981.20 toks/s, output: 77.98 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:00<00:00, 78.51it/s, est. speed input: 9983.67 toks/s, output: 78.00 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:00<00:00, 77.90it/s, est. speed input: 9961.38 toks/s, output: 77.82 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:00<00:00, 78.41it/s, est. speed input: 9985.36 toks/s, output: 78.01 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:01<00:00, 78.38it/s, est. speed input: 9989.06 toks/s, output: 78.04 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:01<00:00, 78.80it/s, est. speed input: 10008.68 toks/s, output: 78.19 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:01<00:00, 79.31it/s, est. speed input: 10033.64 toks/s, output: 78.39 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:01<00:00, 79.66it/s, est. speed input: 10054.97 toks/s, output: 78.55 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:01<00:00, 79.66it/s, est. speed input: 10064.65 toks/s, output: 78.63 toks/s]
Processed prompts:  97%|█████████▋| 124/128 [00:01<00:00, 79.71it/s, est. speed input: 10075.50 toks/s, output: 78.71 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 79.71it/s, est. speed input: 10081.99 toks/s, output: 78.77 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 78.76it/s, est. speed input: 10081.99 toks/s, output: 78.77 toks/s]
[rank0]:[W125 06:24:19.039144826 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 21.3s

测试结果:
  Requests/s:   69.94
  Tokens/s:     9022.54
  Total Reqs:   128
  Elapsed:      1.83s

  [Prefill 分析]
  Total Prefill Tokens: 16384
  Prefill Tokens/s:     8952.59

============================================================
[3/3] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-0.5B-FP8                                │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 256 (= 1 x 256)
│   M_decode      = 1
│   batched_tokens = 257 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 256
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 257
│   --max-num-batched-tokens = 257
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 06:24:24 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1102958) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1102958) WARNING 01-25 06:24:32 [backends.py:609] Failed to read file <frozen os>
Throughput: 74.48 requests/s, 19141.93 total tokens/s, 74.48 output tokens/s
Total num prompt tokens:  32768
Total num output tokens:  128


─── STDERR ───
[2026-01-25 06:24:24] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:24:24] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:24:24] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:24:24] INFO kernels.py:97: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 06:24:24] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:24:24] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:24:24] INFO kernels.py:97: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 06:24:24] INFO kernels.py:249: FP8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:24:24] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:24:24] INFO kernels.py:373: INT8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:24:24] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:24:24] INFO kernels.py:97: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 06:24:24] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:24:24] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:24:24] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:24:24] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:24:24] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 06:24:24] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:24:24] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:24:24] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:24:24] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 06:24:28] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:24:28] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:24:28] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:24:28] INFO kernels.py:97: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 06:24:28] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:24:28] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:24:28] INFO kernels.py:97: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 06:24:28] INFO kernels.py:249: FP8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:24:28] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:24:28] INFO kernels.py:373: INT8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:24:28] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:24:28] INFO kernels.py:97: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 06:24:28] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:24:28] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:24:28] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:24:28] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:24:28] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 06:24:28] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:24:28] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:24:28] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:24:28] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1102958) [2026-01-25 06:24:28] INFO SlideSparseLinearMethod_FP8.py:730: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1102958) [2026-01-25 06:24:28] INFO gemm_wrapper.py:872: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1102958) [2026-01-25 06:24:28] INFO SlideSparseLinearMethod_FP8.py:388: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1102958) [2026-01-25 06:24:28] INFO SlideSparseLinearMethod_FP8.py:535: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=1102958) [2026-01-25 06:24:28] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
(EngineCore_DP0 pid=1102958) [2026-01-25 06:24:28] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8-SlideSparse-2_6
(EngineCore_DP0 pid=1102958) [2026-01-25 06:24:28] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
(EngineCore_DP0 pid=1102958) [2026-01-25 06:24:28] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8-SlideSparse-2_6
(EngineCore_DP0 pid=1102958) [2026-01-25 06:24:28] INFO SlideSparseLinearMethod_FP8.py:552: Preloaded FP8 Triton kernels for model: Qwen2.5-0.5B-FP8-SlideSparse-2_6
(EngineCore_DP0 pid=1102958) [2026-01-25 06:24:28] INFO SlideSparseLinearMethod_FP8.py:556: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=1102958) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1102958) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 12.89it/s]
(EngineCore_DP0 pid=1102958) 
(EngineCore_DP0 pid=1102958) [2026-01-25 06:24:28] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [1152, 1216] -> 1D uint8
(EngineCore_DP0 pid=1102958) [2026-01-25 06:24:29] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 884736 bytes
(EngineCore_DP0 pid=1102958) [2026-01-25 06:24:29] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [896, 1216] -> 1D uint8
(EngineCore_DP0 pid=1102958) [2026-01-25 06:24:29] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 688128 bytes
(EngineCore_DP0 pid=1102958) [2026-01-25 06:24:29] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [9728, 1216] -> 1D uint8
(EngineCore_DP0 pid=1102958) [2026-01-25 06:24:29] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 7471104 bytes
(EngineCore_DP0 pid=1102958) [2026-01-25 06:24:29] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [896, 6496] -> 1D uint8
(EngineCore_DP0 pid=1102958) [2026-01-25 06:24:29] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 3641344 bytes
(EngineCore_DP0 pid=1102958) [2026-01-25 06:24:34] WARNING gemm_wrapper.py:393: No cuSPARSELt config for model 'Qwen2.5-0.5B-FP8', using default algorithm
(EngineCore_DP0 pid=1102958) 2026-01-25 06:24:37,158 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1102958) 2026-01-25 06:24:37,171 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1102958) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 26.68it/s]
(EngineCore_DP0 pid=1102958) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.93it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.93it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 1703.38it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:01, 86.25it/s, est. speed input: 22081.14 toks/s, output: 86.25 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:00<00:01, 80.39it/s, est. speed input: 20793.21 toks/s, output: 81.22 toks/s]
Processed prompts:  21%|██        | 27/128 [00:00<00:01, 77.65it/s, est. speed input: 20198.25 toks/s, output: 78.90 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:00<00:01, 77.30it/s, est. speed input: 20067.81 toks/s, output: 78.39 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:00<00:01, 77.65it/s, est. speed input: 20064.26 toks/s, output: 78.37 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:00<00:00, 77.73it/s, est. speed input: 20044.95 toks/s, output: 78.30 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:00<00:00, 77.86it/s, est. speed input: 20038.59 toks/s, output: 78.27 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:00<00:00, 77.28it/s, est. speed input: 19969.18 toks/s, output: 78.00 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:00<00:00, 77.40it/s, est. speed input: 19960.22 toks/s, output: 77.97 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:01<00:00, 77.81it/s, est. speed input: 19979.53 toks/s, output: 78.04 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:01<00:00, 77.72it/s, est. speed input: 19967.38 toks/s, output: 78.00 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:01<00:00, 77.80it/s, est. speed input: 19966.73 toks/s, output: 77.99 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:01<00:00, 77.91it/s, est. speed input: 19970.18 toks/s, output: 78.01 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:01<00:00, 77.81it/s, est. speed input: 19963.18 toks/s, output: 77.98 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:01<00:00, 77.78it/s, est. speed input: 19957.79 toks/s, output: 77.96 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 77.78it/s, est. speed input: 19950.77 toks/s, output: 77.93 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 77.93it/s, est. speed input: 19950.77 toks/s, output: 77.93 toks/s]
[rank0]:[W125 06:24:40.170262971 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 21.1s

测试结果:
  Requests/s:   74.48
  Tokens/s:     19141.93
  Total Reqs:   128
  Elapsed:      1.72s

  [Prefill 分析]
  Total Prefill Tokens: 32768
  Prefill Tokens/s:     19067.44


------------------------------------------------------------
  生成 CSV: Qwen2.5-0.5B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/Qwen2.5-0.5B-FP8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
16,16,1,128,128,70.0138,1190.2348,1.8282
128,128,1,128,128,69.9421,9022.5357,1.8301
256,256,1,128,128,74.4822,19141.9261,1.7185

------------------------------------------------------------

[INFO] 完成: 3 成功, 0 失败

============================================================
  Qwen2.5-0.5B-FP8 | cuSPARSELt (2_6) | decode
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_6
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6

============================================================
[1/3] 测试 M=16
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-0.5B-FP8                                │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16
│   M_prefill     = 256 (= 16 x 16)
│   M_decode      = 16
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 16
│   --max-num-seqs           = 16
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 06:24:45 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1103314) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1103314) WARNING 01-25 06:24:53 [backends.py:609] Failed to read file <frozen os>
Throughput: 13.89 requests/s, 3776.89 total tokens/s, 3554.72 output tokens/s
Total num prompt tokens:  256
Total num output tokens:  4096


─── STDERR ───
[2026-01-25 06:24:45] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:24:45] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:24:45] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:24:45] INFO kernels.py:97: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 06:24:45] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:24:45] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:24:45] INFO kernels.py:97: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 06:24:45] INFO kernels.py:249: FP8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:24:45] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:24:45] INFO kernels.py:373: INT8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:24:45] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:24:45] INFO kernels.py:97: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 06:24:45] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:24:45] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:24:45] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:24:45] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:24:45] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 06:24:45] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:24:45] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:24:45] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:24:45] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 06:24:49] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:24:49] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:24:49] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:24:49] INFO kernels.py:97: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 06:24:49] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:24:49] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:24:49] INFO kernels.py:97: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 06:24:49] INFO kernels.py:249: FP8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:24:49] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:24:49] INFO kernels.py:373: INT8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:24:49] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:24:49] INFO kernels.py:97: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 06:24:49] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:24:49] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:24:49] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:24:49] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:24:49] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 06:24:49] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:24:49] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:24:49] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:24:49] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1103314) [2026-01-25 06:24:49] INFO SlideSparseLinearMethod_FP8.py:730: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1103314) [2026-01-25 06:24:49] INFO gemm_wrapper.py:872: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1103314) [2026-01-25 06:24:49] INFO SlideSparseLinearMethod_FP8.py:388: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1103314) [2026-01-25 06:24:49] INFO SlideSparseLinearMethod_FP8.py:535: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=1103314) [2026-01-25 06:24:49] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
(EngineCore_DP0 pid=1103314) [2026-01-25 06:24:49] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8-SlideSparse-2_6
(EngineCore_DP0 pid=1103314) [2026-01-25 06:24:49] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
(EngineCore_DP0 pid=1103314) [2026-01-25 06:24:49] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8-SlideSparse-2_6
(EngineCore_DP0 pid=1103314) [2026-01-25 06:24:49] INFO SlideSparseLinearMethod_FP8.py:552: Preloaded FP8 Triton kernels for model: Qwen2.5-0.5B-FP8-SlideSparse-2_6
(EngineCore_DP0 pid=1103314) [2026-01-25 06:24:49] INFO SlideSparseLinearMethod_FP8.py:556: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=1103314) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1103314) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 12.81it/s]
(EngineCore_DP0 pid=1103314) 
(EngineCore_DP0 pid=1103314) [2026-01-25 06:24:50] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [1152, 1216] -> 1D uint8
(EngineCore_DP0 pid=1103314) [2026-01-25 06:24:50] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 884736 bytes
(EngineCore_DP0 pid=1103314) [2026-01-25 06:24:50] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [896, 1216] -> 1D uint8
(EngineCore_DP0 pid=1103314) [2026-01-25 06:24:50] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 688128 bytes
(EngineCore_DP0 pid=1103314) [2026-01-25 06:24:50] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [9728, 1216] -> 1D uint8
(EngineCore_DP0 pid=1103314) [2026-01-25 06:24:50] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 7471104 bytes
(EngineCore_DP0 pid=1103314) [2026-01-25 06:24:50] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [896, 6496] -> 1D uint8
(EngineCore_DP0 pid=1103314) [2026-01-25 06:24:50] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 3641344 bytes
(EngineCore_DP0 pid=1103314) [2026-01-25 06:24:56] WARNING gemm_wrapper.py:393: No cuSPARSELt config for model 'Qwen2.5-0.5B-FP8', using default algorithm
(EngineCore_DP0 pid=1103314) 2026-01-25 06:24:58,280 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1103314) 2026-01-25 06:24:58,293 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1103314) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 3/7 [00:00<00:00, 27.86it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 29.66it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 29.40it/s]
(EngineCore_DP0 pid=1103314) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 1/5 [00:00<00:00,  6.89it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 22.24it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 19.62it/s]

Adding requests:   0%|          | 0/16 [00:00<?, ?it/s]
Adding requests:  75%|███████▌  | 12/16 [00:00<00:00, 79.41it/s]
Adding requests: 100%|██████████| 16/16 [00:00<00:00, 105.19it/s]

Processed prompts:   0%|          | 0/16 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   6%|▋         | 1/16 [00:00<00:13,  1.08it/s, est. speed input: 17.22 toks/s, output: 275.47 toks/s]
Processed prompts: 100%|██████████| 16/16 [00:00<00:00,  1.08it/s, est. speed input: 256.43 toks/s, output: 4102.87 toks/s]
Processed prompts: 100%|██████████| 16/16 [00:00<00:00, 16.03it/s, est. speed input: 256.43 toks/s, output: 4102.87 toks/s]
[rank0]:[W125 06:25:01.842392464 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 20.7s

测试结果:
  Requests/s:   13.89
  Tokens/s:     3776.89
  Total Reqs:   16
  Elapsed:      1.15s

  [Decode 分析]
  Total Decode Tokens:  4096
  Decode Tokens/s:      3554.72

============================================================
[2/3] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-0.5B-FP8                                │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 2048 (= 128 x 16)
│   M_decode      = 128
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 128
│   --max-num-seqs           = 128
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 06:25:06 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1103664) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1103664) WARNING 01-25 06:25:14 [backends.py:609] Failed to read file <frozen os>
Throughput: 79.65 requests/s, 21665.53 total tokens/s, 20391.09 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768


─── STDERR ───
[2026-01-25 06:25:05] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:25:06] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:25:06] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:25:06] INFO kernels.py:97: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 06:25:06] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:25:06] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:25:06] INFO kernels.py:97: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 06:25:06] INFO kernels.py:249: FP8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:25:06] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:25:06] INFO kernels.py:373: INT8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:25:06] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:25:06] INFO kernels.py:97: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 06:25:06] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:25:06] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:25:06] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:25:06] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:25:06] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 06:25:06] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:25:06] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:25:06] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:25:06] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 06:25:09] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:25:10] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:25:10] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:25:10] INFO kernels.py:97: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 06:25:10] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:25:10] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:25:10] INFO kernels.py:97: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 06:25:10] INFO kernels.py:249: FP8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:25:10] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:25:10] INFO kernels.py:373: INT8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:25:10] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:25:10] INFO kernels.py:97: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 06:25:10] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:25:10] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:25:10] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:25:10] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:25:10] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 06:25:10] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:25:10] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:25:10] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:25:10] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1103664) [2026-01-25 06:25:10] INFO SlideSparseLinearMethod_FP8.py:730: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1103664) [2026-01-25 06:25:10] INFO gemm_wrapper.py:872: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1103664) [2026-01-25 06:25:10] INFO SlideSparseLinearMethod_FP8.py:388: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1103664) [2026-01-25 06:25:10] INFO SlideSparseLinearMethod_FP8.py:535: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=1103664) [2026-01-25 06:25:10] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
(EngineCore_DP0 pid=1103664) [2026-01-25 06:25:10] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8-SlideSparse-2_6
(EngineCore_DP0 pid=1103664) [2026-01-25 06:25:10] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
(EngineCore_DP0 pid=1103664) [2026-01-25 06:25:10] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8-SlideSparse-2_6
(EngineCore_DP0 pid=1103664) [2026-01-25 06:25:10] INFO SlideSparseLinearMethod_FP8.py:552: Preloaded FP8 Triton kernels for model: Qwen2.5-0.5B-FP8-SlideSparse-2_6
(EngineCore_DP0 pid=1103664) [2026-01-25 06:25:10] INFO SlideSparseLinearMethod_FP8.py:556: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=1103664) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1103664) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 12.69it/s]
(EngineCore_DP0 pid=1103664) 
(EngineCore_DP0 pid=1103664) [2026-01-25 06:25:10] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [1152, 1216] -> 1D uint8
(EngineCore_DP0 pid=1103664) [2026-01-25 06:25:10] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 884736 bytes
(EngineCore_DP0 pid=1103664) [2026-01-25 06:25:10] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [896, 1216] -> 1D uint8
(EngineCore_DP0 pid=1103664) [2026-01-25 06:25:10] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 688128 bytes
(EngineCore_DP0 pid=1103664) [2026-01-25 06:25:10] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [9728, 1216] -> 1D uint8
(EngineCore_DP0 pid=1103664) [2026-01-25 06:25:10] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 7471104 bytes
(EngineCore_DP0 pid=1103664) [2026-01-25 06:25:10] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [896, 6496] -> 1D uint8
(EngineCore_DP0 pid=1103664) [2026-01-25 06:25:10] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 3641344 bytes
(EngineCore_DP0 pid=1103664) [2026-01-25 06:25:16] WARNING gemm_wrapper.py:393: No cuSPARSELt config for model 'Qwen2.5-0.5B-FP8', using default algorithm
(EngineCore_DP0 pid=1103664) 2026-01-25 06:25:17,733 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1103664) 2026-01-25 06:25:17,746 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1103664) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▊         | 3/35 [00:00<00:01, 22.79it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 6/35 [00:00<00:01, 19.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▌       | 9/35 [00:00<00:01, 16.63it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  34%|███▍      | 12/35 [00:00<00:01, 19.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 15/35 [00:00<00:00, 22.16it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████▏    | 18/35 [00:00<00:00, 24.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 21/35 [00:00<00:00, 25.72it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 25/35 [00:01<00:00, 27.01it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  83%|████████▎ | 29/35 [00:01<00:00, 27.90it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 33/35 [00:01<00:00, 29.18it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:01<00:00, 24.99it/s]
(EngineCore_DP0 pid=1103664) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   5%|▌         | 1/19 [00:00<00:02,  6.82it/s]
Capturing CUDA graphs (decode, FULL):  26%|██▋       | 5/19 [00:00<00:00, 22.01it/s]
Capturing CUDA graphs (decode, FULL):  47%|████▋     | 9/19 [00:00<00:00, 28.07it/s]
Capturing CUDA graphs (decode, FULL):  68%|██████▊   | 13/19 [00:00<00:00, 31.28it/s]
Capturing CUDA graphs (decode, FULL):  89%|████████▉ | 17/19 [00:00<00:00, 33.24it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:00<00:00, 29.68it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 4073.50it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:01<03:13,  1.52s/it, est. speed input: 10.50 toks/s, output: 168.07 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00,  1.52s/it, est. speed input: 1301.32 toks/s, output: 20821.06 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 81.33it/s, est. speed input: 1301.32 toks/s, output: 20821.06 toks/s]
[rank0]:[W125 06:25:22.239221988 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 21.4s

测试结果:
  Requests/s:   79.65
  Tokens/s:     21665.53
  Total Reqs:   128
  Elapsed:      1.61s

  [Decode 分析]
  Total Decode Tokens:  32768
  Decode Tokens/s:      20391.09

============================================================
[3/3] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-0.5B-FP8                                │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 4096 (= 256 x 16)
│   M_decode      = 256
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 256
│   --max-num-seqs           = 256
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 06:25:27 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1104035) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1104035) WARNING 01-25 06:25:35 [backends.py:609] Failed to read file <frozen os>
Throughput: 99.07 requests/s, 26946.38 total tokens/s, 25361.30 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536


─── STDERR ───
[2026-01-25 06:25:27] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:25:27] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:25:27] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:25:27] INFO kernels.py:97: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 06:25:27] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:25:27] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:25:27] INFO kernels.py:97: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 06:25:27] INFO kernels.py:249: FP8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:25:27] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:25:27] INFO kernels.py:373: INT8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:25:27] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:25:27] INFO kernels.py:97: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 06:25:27] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:25:27] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:25:27] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:25:27] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:25:27] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 06:25:27] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:25:27] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:25:27] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:25:27] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 06:25:31] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:25:31] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:25:31] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:25:31] INFO kernels.py:97: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 06:25:31] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:25:31] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:25:31] INFO kernels.py:97: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 06:25:31] INFO kernels.py:249: FP8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:25:31] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:25:31] INFO kernels.py:373: INT8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:25:31] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:25:31] INFO kernels.py:97: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 06:25:31] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:25:31] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:25:31] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:25:31] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:25:31] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 06:25:31] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:25:31] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:25:31] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:25:31] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1104035) [2026-01-25 06:25:31] INFO SlideSparseLinearMethod_FP8.py:730: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1104035) [2026-01-25 06:25:31] INFO gemm_wrapper.py:872: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1104035) [2026-01-25 06:25:31] INFO SlideSparseLinearMethod_FP8.py:388: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1104035) [2026-01-25 06:25:31] INFO SlideSparseLinearMethod_FP8.py:535: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=1104035) [2026-01-25 06:25:31] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
(EngineCore_DP0 pid=1104035) [2026-01-25 06:25:31] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8-SlideSparse-2_6
(EngineCore_DP0 pid=1104035) [2026-01-25 06:25:31] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
(EngineCore_DP0 pid=1104035) [2026-01-25 06:25:31] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8-SlideSparse-2_6
(EngineCore_DP0 pid=1104035) [2026-01-25 06:25:31] INFO SlideSparseLinearMethod_FP8.py:552: Preloaded FP8 Triton kernels for model: Qwen2.5-0.5B-FP8-SlideSparse-2_6
(EngineCore_DP0 pid=1104035) [2026-01-25 06:25:31] INFO SlideSparseLinearMethod_FP8.py:556: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=1104035) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1104035) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 12.70it/s]
(EngineCore_DP0 pid=1104035) 
(EngineCore_DP0 pid=1104035) [2026-01-25 06:25:32] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [1152, 1216] -> 1D uint8
(EngineCore_DP0 pid=1104035) [2026-01-25 06:25:32] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 884736 bytes
(EngineCore_DP0 pid=1104035) [2026-01-25 06:25:32] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [896, 1216] -> 1D uint8
(EngineCore_DP0 pid=1104035) [2026-01-25 06:25:32] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 688128 bytes
(EngineCore_DP0 pid=1104035) [2026-01-25 06:25:32] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [9728, 1216] -> 1D uint8
(EngineCore_DP0 pid=1104035) [2026-01-25 06:25:32] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 7471104 bytes
(EngineCore_DP0 pid=1104035) [2026-01-25 06:25:32] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [896, 6496] -> 1D uint8
(EngineCore_DP0 pid=1104035) [2026-01-25 06:25:32] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 3641344 bytes
(EngineCore_DP0 pid=1104035) [2026-01-25 06:25:37] WARNING gemm_wrapper.py:393: No cuSPARSELt config for model 'Qwen2.5-0.5B-FP8', using default algorithm
(EngineCore_DP0 pid=1104035) 2026-01-25 06:25:39,262 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1104035) 2026-01-25 06:25:39,276 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1104035) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/36 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 3/36 [00:00<00:01, 26.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 6/36 [00:00<00:01, 26.92it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  25%|██▌       | 9/36 [00:00<00:00, 28.07it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 12/36 [00:00<00:00, 28.09it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 15/36 [00:00<00:00, 28.22it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 18/36 [00:00<00:00, 27.99it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 21/36 [00:00<00:00, 28.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 24/36 [00:00<00:00, 28.24it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 27/36 [00:00<00:00, 28.39it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  83%|████████▎ | 30/36 [00:01<00:00, 28.36it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 34/36 [00:01<00:00, 29.06it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:01<00:00, 28.21it/s]
(EngineCore_DP0 pid=1104035) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   3%|▎         | 1/35 [00:00<00:04,  6.82it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 5/35 [00:00<00:01, 21.33it/s]
Capturing CUDA graphs (decode, FULL):  26%|██▌       | 9/35 [00:00<00:00, 26.77it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 13/35 [00:00<00:00, 29.68it/s]
Capturing CUDA graphs (decode, FULL):  49%|████▊     | 17/35 [00:00<00:00, 31.28it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 21/35 [00:00<00:00, 32.39it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 25/35 [00:00<00:00, 33.05it/s]
Capturing CUDA graphs (decode, FULL):  83%|████████▎ | 29/35 [00:00<00:00, 33.59it/s]
Capturing CUDA graphs (decode, FULL):  94%|█████████▍| 33/35 [00:01<00:00, 34.08it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:01<00:00, 30.93it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 6044.41it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:02<09:42,  2.29s/it, est. speed input: 7.00 toks/s, output: 111.99 toks/s]
Processed prompts:  59%|█████▉    | 152/256 [00:02<00:01, 89.12it/s, est. speed input: 1018.44 toks/s, output: 16294.88 toks/s]
Processed prompts:  95%|█████████▍| 242/256 [00:02<00:00, 151.72it/s, est. speed input: 1555.32 toks/s, output: 24885.02 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 151.72it/s, est. speed input: 1612.71 toks/s, output: 25803.39 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 100.79it/s, est. speed input: 1612.71 toks/s, output: 25803.39 toks/s]
[rank0]:[W125 06:25:45.133886563 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 22.9s

测试结果:
  Requests/s:   99.07
  Tokens/s:     26946.38
  Total Reqs:   256
  Elapsed:      2.58s

  [Decode 分析]
  Total Decode Tokens:  65536
  Decode Tokens/s:      25361.30


------------------------------------------------------------
  生成 CSV: Qwen2.5-0.5B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/Qwen2.5-0.5B-FP8_decode.csv

预览:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
16,16,16,16,256,256,13.8856,3776.8916,1.1523
128,16,128,128,256,256,79.6527,21665.5309,1.6070
256,16,256,256,256,256,99.0676,26946.3827,2.5841

------------------------------------------------------------

[INFO] 完成: 3 成功, 0 失败

======================================================================
[SlideSparse] 未找到 Qwen2.5-0.5B-FP8-SlideSparse-2_8 checkpoint
[SlideSparse] 发现基础模型: Qwen2.5-0.5B-FP8
[SlideSparse] 自动转换中...
[SlideSparse] 开始转换: Qwen2.5-0.5B-FP8 -> SlideSparse-2_8
======================================================================

[SlideSparse] 转换成功: Qwen2.5-0.5B-FP8-SlideSparse-2_8
======================================================================


============================================================
  Qwen2.5-0.5B-FP8 | cuSPARSELt (2_8) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_8
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8

============================================================
[1/3] 测试 M=16
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-0.5B-FP8                                │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16
│   M_prefill     = 16 (= 1 x 16)
│   M_decode      = 1
│   batched_tokens = 17 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 17
│   --max-num-batched-tokens = 17
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 06:26:20 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1104747) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1104747) WARNING 01-25 06:26:28 [backends.py:609] Failed to read file <frozen os>
Throughput: 69.30 requests/s, 1178.03 total tokens/s, 69.30 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  128


─── STDERR ───
[2026-01-25 06:26:20] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:26:20] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:26:20] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:26:20] INFO kernels.py:97: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 06:26:20] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:26:20] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:26:20] INFO kernels.py:97: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 06:26:20] INFO kernels.py:249: FP8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:26:20] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:26:20] INFO kernels.py:373: INT8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:26:20] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:26:20] INFO kernels.py:97: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 06:26:20] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:26:20] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:26:20] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:26:20] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:26:20] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 06:26:20] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:26:20] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:26:20] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:26:20] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 06:26:24] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:26:24] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:26:24] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:26:24] INFO kernels.py:97: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 06:26:24] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:26:24] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:26:24] INFO kernels.py:97: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 06:26:24] INFO kernels.py:249: FP8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:26:24] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:26:24] INFO kernels.py:373: INT8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:26:24] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:26:24] INFO kernels.py:97: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 06:26:24] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:26:24] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:26:24] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:26:24] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:26:24] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 06:26:24] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:26:24] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:26:24] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:26:24] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1104747) [2026-01-25 06:26:24] INFO SlideSparseLinearMethod_FP8.py:730: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1104747) [2026-01-25 06:26:25] INFO gemm_wrapper.py:872: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1104747) [2026-01-25 06:26:25] INFO SlideSparseLinearMethod_FP8.py:388: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1104747) [2026-01-25 06:26:25] INFO SlideSparseLinearMethod_FP8.py:535: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=1104747) [2026-01-25 06:26:25] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
(EngineCore_DP0 pid=1104747) [2026-01-25 06:26:25] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8-SlideSparse-2_8
(EngineCore_DP0 pid=1104747) [2026-01-25 06:26:25] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
(EngineCore_DP0 pid=1104747) [2026-01-25 06:26:25] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8-SlideSparse-2_8
(EngineCore_DP0 pid=1104747) [2026-01-25 06:26:25] INFO SlideSparseLinearMethod_FP8.py:552: Preloaded FP8 Triton kernels for model: Qwen2.5-0.5B-FP8-SlideSparse-2_8
(EngineCore_DP0 pid=1104747) [2026-01-25 06:26:25] INFO SlideSparseLinearMethod_FP8.py:556: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=1104747) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1104747) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 11.03it/s]
(EngineCore_DP0 pid=1104747) 
(EngineCore_DP0 pid=1104747) [2026-01-25 06:26:25] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [1152, 1344] -> 1D uint8
(EngineCore_DP0 pid=1104747) [2026-01-25 06:26:25] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 976896 bytes
(EngineCore_DP0 pid=1104747) [2026-01-25 06:26:25] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [896, 1344] -> 1D uint8
(EngineCore_DP0 pid=1104747) [2026-01-25 06:26:25] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 759808 bytes
(EngineCore_DP0 pid=1104747) [2026-01-25 06:26:25] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [9728, 1344] -> 1D uint8
(EngineCore_DP0 pid=1104747) [2026-01-25 06:26:25] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 8249344 bytes
(EngineCore_DP0 pid=1104747) [2026-01-25 06:26:25] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [896, 7296] -> 1D uint8
(EngineCore_DP0 pid=1104747) [2026-01-25 06:26:25] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 4085760 bytes
(EngineCore_DP0 pid=1104747) [2026-01-25 06:26:31] WARNING gemm_wrapper.py:393: No cuSPARSELt config for model 'Qwen2.5-0.5B-FP8', using default algorithm
(EngineCore_DP0 pid=1104747) 2026-01-25 06:26:33,786 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1104747) 2026-01-25 06:26:33,799 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1104747) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  7.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  7.40it/s]
(EngineCore_DP0 pid=1104747) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.86it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.86it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 6758.62it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:27,  4.60it/s, est. speed input: 73.56 toks/s, output: 4.60 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:03, 34.18it/s, est. speed input: 450.31 toks/s, output: 28.14 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:02, 50.33it/s, est. speed input: 645.23 toks/s, output: 40.33 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:00<00:01, 60.17it/s, est. speed input: 765.63 toks/s, output: 47.85 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:00<00:01, 66.38it/s, est. speed input: 846.78 toks/s, output: 52.92 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:00<00:01, 69.37it/s, est. speed input: 899.16 toks/s, output: 56.20 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:00<00:01, 72.56it/s, est. speed input: 944.68 toks/s, output: 59.04 toks/s]
Processed prompts:  45%|████▌     | 58/128 [00:00<00:00, 75.10it/s, est. speed input: 985.13 toks/s, output: 61.57 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:01<00:00, 76.46it/s, est. speed input: 1013.04 toks/s, output: 63.31 toks/s]
Processed prompts:  58%|█████▊    | 74/128 [00:01<00:00, 77.50it/s, est. speed input: 1036.38 toks/s, output: 64.77 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:01<00:00, 77.42it/s, est. speed input: 1052.94 toks/s, output: 65.81 toks/s]
Processed prompts:  70%|███████   | 90/128 [00:01<00:00, 78.12it/s, est. speed input: 1069.57 toks/s, output: 66.85 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:01<00:00, 78.41it/s, est. speed input: 1083.27 toks/s, output: 67.70 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:01<00:00, 78.28it/s, est. speed input: 1094.16 toks/s, output: 68.38 toks/s]
Processed prompts:  89%|████████▉ | 114/128 [00:01<00:00, 78.55it/s, est. speed input: 1104.72 toks/s, output: 69.04 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:01<00:00, 78.84it/s, est. speed input: 1114.36 toks/s, output: 69.65 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 78.84it/s, est. speed input: 1120.68 toks/s, output: 70.04 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 70.04it/s, est. speed input: 1120.68 toks/s, output: 70.04 toks/s]
[rank0]:[W125 06:26:37.967741307 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 21.7s

测试结果:
  Requests/s:   69.30
  Tokens/s:     1178.03
  Total Reqs:   128
  Elapsed:      1.85s

  [Prefill 分析]
  Total Prefill Tokens: 2048
  Prefill Tokens/s:     1108.74

============================================================
[2/3] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-0.5B-FP8                                │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 128 (= 1 x 128)
│   M_decode      = 1
│   batched_tokens = 129 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 128
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 129
│   --max-num-batched-tokens = 129
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 06:26:42 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1105157) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1105157) WARNING 01-25 06:26:50 [backends.py:609] Failed to read file <frozen os>
Throughput: 69.01 requests/s, 8901.87 total tokens/s, 69.01 output tokens/s
Total num prompt tokens:  16384
Total num output tokens:  128


─── STDERR ───
[2026-01-25 06:26:42] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:26:42] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:26:42] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:26:42] INFO kernels.py:97: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 06:26:42] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:26:42] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:26:42] INFO kernels.py:97: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 06:26:42] INFO kernels.py:249: FP8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:26:42] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:26:42] INFO kernels.py:373: INT8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:26:42] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:26:42] INFO kernels.py:97: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 06:26:42] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:26:42] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:26:42] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:26:42] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:26:42] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 06:26:42] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:26:42] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:26:42] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:26:42] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 06:26:46] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:26:46] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:26:46] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:26:46] INFO kernels.py:97: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 06:26:46] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:26:46] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:26:46] INFO kernels.py:97: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 06:26:46] INFO kernels.py:249: FP8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:26:46] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:26:46] INFO kernels.py:373: INT8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:26:46] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:26:46] INFO kernels.py:97: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 06:26:46] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:26:46] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:26:46] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:26:46] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:26:46] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 06:26:46] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:26:46] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:26:46] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:26:46] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1105157) [2026-01-25 06:26:46] INFO SlideSparseLinearMethod_FP8.py:730: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1105157) [2026-01-25 06:26:46] INFO gemm_wrapper.py:872: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1105157) [2026-01-25 06:26:46] INFO SlideSparseLinearMethod_FP8.py:388: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1105157) [2026-01-25 06:26:46] INFO SlideSparseLinearMethod_FP8.py:535: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=1105157) [2026-01-25 06:26:46] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
(EngineCore_DP0 pid=1105157) [2026-01-25 06:26:46] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8-SlideSparse-2_8
(EngineCore_DP0 pid=1105157) [2026-01-25 06:26:46] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
(EngineCore_DP0 pid=1105157) [2026-01-25 06:26:46] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8-SlideSparse-2_8
(EngineCore_DP0 pid=1105157) [2026-01-25 06:26:46] INFO SlideSparseLinearMethod_FP8.py:552: Preloaded FP8 Triton kernels for model: Qwen2.5-0.5B-FP8-SlideSparse-2_8
(EngineCore_DP0 pid=1105157) [2026-01-25 06:26:46] INFO SlideSparseLinearMethod_FP8.py:556: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=1105157) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1105157) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 11.28it/s]
(EngineCore_DP0 pid=1105157) 
(EngineCore_DP0 pid=1105157) [2026-01-25 06:26:46] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [1152, 1344] -> 1D uint8
(EngineCore_DP0 pid=1105157) [2026-01-25 06:26:46] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 976896 bytes
(EngineCore_DP0 pid=1105157) [2026-01-25 06:26:46] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [896, 1344] -> 1D uint8
(EngineCore_DP0 pid=1105157) [2026-01-25 06:26:46] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 759808 bytes
(EngineCore_DP0 pid=1105157) [2026-01-25 06:26:46] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [9728, 1344] -> 1D uint8
(EngineCore_DP0 pid=1105157) [2026-01-25 06:26:46] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 8249344 bytes
(EngineCore_DP0 pid=1105157) [2026-01-25 06:26:46] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [896, 7296] -> 1D uint8
(EngineCore_DP0 pid=1105157) [2026-01-25 06:26:46] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 4085760 bytes
(EngineCore_DP0 pid=1105157) [2026-01-25 06:26:53] WARNING gemm_wrapper.py:393: No cuSPARSELt config for model 'Qwen2.5-0.5B-FP8', using default algorithm
(EngineCore_DP0 pid=1105157) 2026-01-25 06:26:55,304 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1105157) 2026-01-25 06:26:55,316 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1105157) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 27.55it/s]
(EngineCore_DP0 pid=1105157) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.98it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.98it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 2784.15it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:23,  5.32it/s, est. speed input: 681.12 toks/s, output: 5.32 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:03, 36.81it/s, est. speed input: 3935.49 toks/s, output: 30.75 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:02, 52.75it/s, est. speed input: 5517.39 toks/s, output: 43.10 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:00<00:01, 62.01it/s, est. speed input: 6458.47 toks/s, output: 50.46 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:00<00:01, 67.81it/s, est. speed input: 7084.75 toks/s, output: 55.35 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:00<00:01, 71.39it/s, est. speed input: 7522.13 toks/s, output: 58.77 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:00<00:01, 73.53it/s, est. speed input: 7839.36 toks/s, output: 61.24 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:00<00:00, 75.18it/s, est. speed input: 8092.83 toks/s, output: 63.22 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:01<00:00, 76.24it/s, est. speed input: 8292.66 toks/s, output: 64.79 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:01<00:00, 77.07it/s, est. speed input: 8458.95 toks/s, output: 66.09 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:01<00:00, 77.89it/s, est. speed input: 8605.01 toks/s, output: 67.23 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:01<00:00, 78.24it/s, est. speed input: 8722.27 toks/s, output: 68.14 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:01<00:00, 78.34it/s, est. speed input: 8818.67 toks/s, output: 68.90 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:01<00:00, 78.13it/s, est. speed input: 8895.28 toks/s, output: 69.49 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:01<00:00, 78.05it/s, est. speed input: 8963.50 toks/s, output: 70.03 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:01<00:00, 77.74it/s, est. speed input: 9017.66 toks/s, output: 70.45 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 77.74it/s, est. speed input: 9062.01 toks/s, output: 70.80 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 70.79it/s, est. speed input: 9062.01 toks/s, output: 70.80 toks/s]
[rank0]:[W125 06:26:58.426865710 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 21.5s

测试结果:
  Requests/s:   69.01
  Tokens/s:     8901.87
  Total Reqs:   128
  Elapsed:      1.85s

  [Prefill 分析]
  Total Prefill Tokens: 16384
  Prefill Tokens/s:     8832.87

============================================================
[3/3] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-0.5B-FP8                                │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 256 (= 1 x 256)
│   M_decode      = 1
│   batched_tokens = 257 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 256
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 257
│   --max-num-batched-tokens = 257
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 06:27:03 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1105558) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1105558) WARNING 01-25 06:27:11 [backends.py:609] Failed to read file <frozen os>
Throughput: 72.59 requests/s, 18655.66 total tokens/s, 72.59 output tokens/s
Total num prompt tokens:  32768
Total num output tokens:  128


─── STDERR ───
[2026-01-25 06:27:03] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:27:03] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:27:03] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:27:03] INFO kernels.py:97: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 06:27:03] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:27:03] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:27:03] INFO kernels.py:97: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 06:27:03] INFO kernels.py:249: FP8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:27:03] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:27:03] INFO kernels.py:373: INT8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:27:03] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:27:03] INFO kernels.py:97: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 06:27:03] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:27:03] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:27:03] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:27:03] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:27:03] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 06:27:03] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:27:03] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:27:03] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:27:03] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 06:27:07] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:27:07] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:27:07] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:27:07] INFO kernels.py:97: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 06:27:07] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:27:07] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:27:07] INFO kernels.py:97: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 06:27:07] INFO kernels.py:249: FP8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:27:07] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:27:07] INFO kernels.py:373: INT8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:27:07] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:27:07] INFO kernels.py:97: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 06:27:07] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:27:07] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:27:07] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:27:07] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:27:07] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 06:27:07] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:27:07] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:27:07] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:27:07] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1105558) [2026-01-25 06:27:08] INFO SlideSparseLinearMethod_FP8.py:730: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1105558) [2026-01-25 06:27:08] INFO gemm_wrapper.py:872: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1105558) [2026-01-25 06:27:08] INFO SlideSparseLinearMethod_FP8.py:388: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1105558) [2026-01-25 06:27:08] INFO SlideSparseLinearMethod_FP8.py:535: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=1105558) [2026-01-25 06:27:08] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
(EngineCore_DP0 pid=1105558) [2026-01-25 06:27:08] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8-SlideSparse-2_8
(EngineCore_DP0 pid=1105558) [2026-01-25 06:27:08] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
(EngineCore_DP0 pid=1105558) [2026-01-25 06:27:08] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8-SlideSparse-2_8
(EngineCore_DP0 pid=1105558) [2026-01-25 06:27:08] INFO SlideSparseLinearMethod_FP8.py:552: Preloaded FP8 Triton kernels for model: Qwen2.5-0.5B-FP8-SlideSparse-2_8
(EngineCore_DP0 pid=1105558) [2026-01-25 06:27:08] INFO SlideSparseLinearMethod_FP8.py:556: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=1105558) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1105558) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 11.12it/s]
(EngineCore_DP0 pid=1105558) 
(EngineCore_DP0 pid=1105558) [2026-01-25 06:27:08] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [1152, 1344] -> 1D uint8
(EngineCore_DP0 pid=1105558) [2026-01-25 06:27:08] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 976896 bytes
(EngineCore_DP0 pid=1105558) [2026-01-25 06:27:08] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [896, 1344] -> 1D uint8
(EngineCore_DP0 pid=1105558) [2026-01-25 06:27:08] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 759808 bytes
(EngineCore_DP0 pid=1105558) [2026-01-25 06:27:08] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [9728, 1344] -> 1D uint8
(EngineCore_DP0 pid=1105558) [2026-01-25 06:27:08] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 8249344 bytes
(EngineCore_DP0 pid=1105558) [2026-01-25 06:27:08] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [896, 7296] -> 1D uint8
(EngineCore_DP0 pid=1105558) [2026-01-25 06:27:08] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 4085760 bytes
(EngineCore_DP0 pid=1105558) [2026-01-25 06:27:14] WARNING gemm_wrapper.py:393: No cuSPARSELt config for model 'Qwen2.5-0.5B-FP8', using default algorithm
(EngineCore_DP0 pid=1105558) 2026-01-25 06:27:16,544 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1105558) 2026-01-25 06:27:16,557 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1105558) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 27.46it/s]
(EngineCore_DP0 pid=1105558) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.97it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.97it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 1674.71it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:02, 44.44it/s, est. speed input: 11378.96 toks/s, output: 44.44 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:01, 63.77it/s, est. speed input: 15545.60 toks/s, output: 60.72 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:01, 70.17it/s, est. speed input: 16983.19 toks/s, output: 66.34 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:00<00:01, 73.39it/s, est. speed input: 17737.13 toks/s, output: 69.28 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:00<00:01, 74.19it/s, est. speed input: 18062.05 toks/s, output: 70.55 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:00<00:01, 75.40it/s, est. speed input: 18365.20 toks/s, output: 71.74 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:00<00:00, 76.13it/s, est. speed input: 18578.59 toks/s, output: 72.57 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:00<00:00, 76.82it/s, est. speed input: 18759.58 toks/s, output: 73.28 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:00<00:00, 77.47it/s, est. speed input: 18916.38 toks/s, output: 73.89 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:01<00:00, 77.60it/s, est. speed input: 19017.85 toks/s, output: 74.29 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:01<00:00, 77.81it/s, est. speed input: 19109.87 toks/s, output: 74.65 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:01<00:00, 78.01it/s, est. speed input: 19190.28 toks/s, output: 74.96 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:01<00:00, 78.03it/s, est. speed input: 19250.78 toks/s, output: 75.20 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:01<00:00, 78.19it/s, est. speed input: 19311.68 toks/s, output: 75.44 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:01<00:00, 78.36it/s, est. speed input: 19367.68 toks/s, output: 75.65 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:01<00:00, 78.53it/s, est. speed input: 19418.87 toks/s, output: 75.85 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 78.53it/s, est. speed input: 19435.86 toks/s, output: 75.92 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 75.92it/s, est. speed input: 19435.86 toks/s, output: 75.92 toks/s]
[rank0]:[W125 06:27:19.569545837 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 21.1s

测试结果:
  Requests/s:   72.59
  Tokens/s:     18655.66
  Total Reqs:   128
  Elapsed:      1.76s

  [Prefill 分析]
  Total Prefill Tokens: 32768
  Prefill Tokens/s:     18583.07


------------------------------------------------------------
  生成 CSV: Qwen2.5-0.5B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/Qwen2.5-0.5B-FP8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
16,16,1,128,128,69.2961,1178.0344,1.8471
128,128,1,128,128,69.0068,8901.8725,1.8549
256,256,1,128,128,72.5901,18655.6650,1.7633

------------------------------------------------------------

[INFO] 完成: 3 成功, 0 失败

============================================================
  Qwen2.5-0.5B-FP8 | cuSPARSELt (2_8) | decode
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_8
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8

============================================================
[1/3] 测试 M=16
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-0.5B-FP8                                │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16
│   M_prefill     = 256 (= 16 x 16)
│   M_decode      = 16
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 16
│   --max-num-seqs           = 16
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 06:27:24 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1105938) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1105938) WARNING 01-25 06:27:32 [backends.py:609] Failed to read file <frozen os>
Throughput: 13.41 requests/s, 3646.33 total tokens/s, 3431.84 output tokens/s
Total num prompt tokens:  256
Total num output tokens:  4096


─── STDERR ───
[2026-01-25 06:27:24] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:27:24] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:27:24] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:27:24] INFO kernels.py:97: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 06:27:24] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:27:24] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:27:24] INFO kernels.py:97: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 06:27:24] INFO kernels.py:249: FP8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:27:24] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:27:24] INFO kernels.py:373: INT8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:27:24] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:27:24] INFO kernels.py:97: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 06:27:24] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:27:24] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:27:24] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:27:24] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:27:24] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 06:27:24] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:27:24] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:27:24] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:27:24] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 06:27:28] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:27:28] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:27:28] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:27:28] INFO kernels.py:97: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 06:27:28] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:27:28] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:27:28] INFO kernels.py:97: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 06:27:28] INFO kernels.py:249: FP8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:27:28] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:27:28] INFO kernels.py:373: INT8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:27:28] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:27:28] INFO kernels.py:97: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 06:27:28] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:27:28] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:27:28] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:27:28] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:27:28] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 06:27:28] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:27:28] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:27:28] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:27:28] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1105938) [2026-01-25 06:27:28] INFO SlideSparseLinearMethod_FP8.py:730: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1105938) [2026-01-25 06:27:28] INFO gemm_wrapper.py:872: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1105938) [2026-01-25 06:27:28] INFO SlideSparseLinearMethod_FP8.py:388: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1105938) [2026-01-25 06:27:28] INFO SlideSparseLinearMethod_FP8.py:535: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=1105938) [2026-01-25 06:27:28] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
(EngineCore_DP0 pid=1105938) [2026-01-25 06:27:28] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8-SlideSparse-2_8
(EngineCore_DP0 pid=1105938) [2026-01-25 06:27:28] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
(EngineCore_DP0 pid=1105938) [2026-01-25 06:27:28] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8-SlideSparse-2_8
(EngineCore_DP0 pid=1105938) [2026-01-25 06:27:28] INFO SlideSparseLinearMethod_FP8.py:552: Preloaded FP8 Triton kernels for model: Qwen2.5-0.5B-FP8-SlideSparse-2_8
(EngineCore_DP0 pid=1105938) [2026-01-25 06:27:28] INFO SlideSparseLinearMethod_FP8.py:556: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=1105938) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1105938) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 11.10it/s]
(EngineCore_DP0 pid=1105938) 
(EngineCore_DP0 pid=1105938) [2026-01-25 06:27:29] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [1152, 1344] -> 1D uint8
(EngineCore_DP0 pid=1105938) [2026-01-25 06:27:29] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 976896 bytes
(EngineCore_DP0 pid=1105938) [2026-01-25 06:27:29] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [896, 1344] -> 1D uint8
(EngineCore_DP0 pid=1105938) [2026-01-25 06:27:29] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 759808 bytes
(EngineCore_DP0 pid=1105938) [2026-01-25 06:27:29] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [9728, 1344] -> 1D uint8
(EngineCore_DP0 pid=1105938) [2026-01-25 06:27:29] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 8249344 bytes
(EngineCore_DP0 pid=1105938) [2026-01-25 06:27:29] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [896, 7296] -> 1D uint8
(EngineCore_DP0 pid=1105938) [2026-01-25 06:27:29] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 4085760 bytes
(EngineCore_DP0 pid=1105938) [2026-01-25 06:27:35] WARNING gemm_wrapper.py:393: No cuSPARSELt config for model 'Qwen2.5-0.5B-FP8', using default algorithm
(EngineCore_DP0 pid=1105938) 2026-01-25 06:27:37,432 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1105938) 2026-01-25 06:27:37,445 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1105938) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 3/7 [00:00<00:00, 27.35it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 29.37it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 29.08it/s]
(EngineCore_DP0 pid=1105938) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 1/5 [00:00<00:00,  6.90it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 22.49it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 19.80it/s]

Adding requests:   0%|          | 0/16 [00:00<?, ?it/s]
Adding requests:  62%|██████▎   | 10/16 [00:00<00:00, 70.69it/s]
Adding requests: 100%|██████████| 16/16 [00:00<00:00, 111.94it/s]

Processed prompts:   0%|          | 0/16 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   6%|▋         | 1/16 [00:01<00:15,  1.01s/it, est. speed input: 15.81 toks/s, output: 252.91 toks/s]
Processed prompts: 100%|██████████| 16/16 [00:01<00:00,  1.01s/it, est. speed input: 243.91 toks/s, output: 3902.51 toks/s]
Processed prompts: 100%|██████████| 16/16 [00:01<00:00, 15.24it/s, est. speed input: 243.91 toks/s, output: 3902.51 toks/s]
[rank0]:[W125 06:27:40.027619711 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 20.5s

测试结果:
  Requests/s:   13.41
  Tokens/s:     3646.33
  Total Reqs:   16
  Elapsed:      1.19s

  [Decode 分析]
  Total Decode Tokens:  4096
  Decode Tokens/s:      3431.84

============================================================
[2/3] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-0.5B-FP8                                │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 2048 (= 128 x 16)
│   M_decode      = 128
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 128
│   --max-num-seqs           = 128
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 06:27:45 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1106294) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1106294) WARNING 01-25 06:27:53 [backends.py:609] Failed to read file <frozen os>
Throughput: 77.21 requests/s, 21000.48 total tokens/s, 19765.15 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768


─── STDERR ───
[2026-01-25 06:27:45] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:27:45] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:27:45] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:27:45] INFO kernels.py:97: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 06:27:45] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:27:45] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:27:45] INFO kernels.py:97: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 06:27:45] INFO kernels.py:249: FP8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:27:45] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:27:45] INFO kernels.py:373: INT8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:27:45] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:27:45] INFO kernels.py:97: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 06:27:45] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:27:45] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:27:45] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:27:45] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:27:45] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 06:27:45] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:27:45] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:27:45] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:27:45] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 06:27:49] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:27:49] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:27:49] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:27:49] INFO kernels.py:97: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 06:27:49] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:27:49] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:27:49] INFO kernels.py:97: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 06:27:49] INFO kernels.py:249: FP8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:27:49] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:27:49] INFO kernels.py:373: INT8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:27:49] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:27:49] INFO kernels.py:97: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 06:27:49] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:27:49] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:27:49] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:27:49] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:27:49] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 06:27:49] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:27:49] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:27:49] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:27:49] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1106294) [2026-01-25 06:27:49] INFO SlideSparseLinearMethod_FP8.py:730: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1106294) [2026-01-25 06:27:49] INFO gemm_wrapper.py:872: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1106294) [2026-01-25 06:27:49] INFO SlideSparseLinearMethod_FP8.py:388: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1106294) [2026-01-25 06:27:49] INFO SlideSparseLinearMethod_FP8.py:535: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=1106294) [2026-01-25 06:27:49] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
(EngineCore_DP0 pid=1106294) [2026-01-25 06:27:49] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8-SlideSparse-2_8
(EngineCore_DP0 pid=1106294) [2026-01-25 06:27:49] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
(EngineCore_DP0 pid=1106294) [2026-01-25 06:27:49] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8-SlideSparse-2_8
(EngineCore_DP0 pid=1106294) [2026-01-25 06:27:49] INFO SlideSparseLinearMethod_FP8.py:552: Preloaded FP8 Triton kernels for model: Qwen2.5-0.5B-FP8-SlideSparse-2_8
(EngineCore_DP0 pid=1106294) [2026-01-25 06:27:49] INFO SlideSparseLinearMethod_FP8.py:556: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=1106294) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1106294) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 11.11it/s]
(EngineCore_DP0 pid=1106294) 
(EngineCore_DP0 pid=1106294) [2026-01-25 06:27:50] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [1152, 1344] -> 1D uint8
(EngineCore_DP0 pid=1106294) [2026-01-25 06:27:50] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 976896 bytes
(EngineCore_DP0 pid=1106294) [2026-01-25 06:27:50] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [896, 1344] -> 1D uint8
(EngineCore_DP0 pid=1106294) [2026-01-25 06:27:50] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 759808 bytes
(EngineCore_DP0 pid=1106294) [2026-01-25 06:27:50] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [9728, 1344] -> 1D uint8
(EngineCore_DP0 pid=1106294) [2026-01-25 06:27:50] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 8249344 bytes
(EngineCore_DP0 pid=1106294) [2026-01-25 06:27:50] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [896, 7296] -> 1D uint8
(EngineCore_DP0 pid=1106294) [2026-01-25 06:27:50] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 4085760 bytes
(EngineCore_DP0 pid=1106294) [2026-01-25 06:27:55] WARNING gemm_wrapper.py:393: No cuSPARSELt config for model 'Qwen2.5-0.5B-FP8', using default algorithm
(EngineCore_DP0 pid=1106294) 2026-01-25 06:27:57,090 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1106294) 2026-01-25 06:27:57,112 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1106294) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|▎         | 1/35 [00:00<00:04,  7.48it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█▏        | 4/35 [00:00<00:01, 18.84it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 7/35 [00:00<00:01, 23.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 11/35 [00:00<00:00, 26.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 15/35 [00:00<00:00, 28.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  54%|█████▍    | 19/35 [00:00<00:00, 29.05it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  66%|██████▌   | 23/35 [00:00<00:00, 29.70it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  77%|███████▋  | 27/35 [00:00<00:00, 29.42it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▊ | 31/35 [00:01<00:00, 29.77it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:01<00:00, 29.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:01<00:00, 27.84it/s]
(EngineCore_DP0 pid=1106294) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   5%|▌         | 1/19 [00:00<00:02,  6.94it/s]
Capturing CUDA graphs (decode, FULL):  26%|██▋       | 5/19 [00:00<00:00, 22.46it/s]
Capturing CUDA graphs (decode, FULL):  47%|████▋     | 9/19 [00:00<00:00, 28.60it/s]
Capturing CUDA graphs (decode, FULL):  68%|██████▊   | 13/19 [00:00<00:00, 31.70it/s]
Capturing CUDA graphs (decode, FULL):  89%|████████▉ | 17/19 [00:00<00:00, 33.65it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:00<00:00, 30.07it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 6545.45it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:01<03:21,  1.58s/it, est. speed input: 10.10 toks/s, output: 161.59 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00,  1.58s/it, est. speed input: 1250.64 toks/s, output: 20010.17 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 78.16it/s, est. speed input: 1250.64 toks/s, output: 20010.17 toks/s]
[rank0]:[W125 06:28:01.494202459 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 21.5s

测试结果:
  Requests/s:   77.21
  Tokens/s:     21000.48
  Total Reqs:   128
  Elapsed:      1.66s

  [Decode 分析]
  Total Decode Tokens:  32768
  Decode Tokens/s:      19765.15

============================================================
[3/3] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-0.5B-FP8                                │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 4096 (= 256 x 16)
│   M_decode      = 256
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 256
│   --max-num-seqs           = 256
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 06:28:06 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1106643) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1106643) WARNING 01-25 06:28:14 [backends.py:609] Failed to read file <frozen os>
Throughput: 96.62 requests/s, 26279.32 total tokens/s, 24733.48 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536


─── STDERR ───
[2026-01-25 06:28:06] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:28:06] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:28:06] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:28:06] INFO kernels.py:97: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 06:28:06] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:28:06] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:28:06] INFO kernels.py:97: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 06:28:06] INFO kernels.py:249: FP8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:28:06] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:28:06] INFO kernels.py:373: INT8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:28:06] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:28:06] INFO kernels.py:97: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 06:28:06] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:28:06] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:28:06] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:28:06] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:28:06] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 06:28:06] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:28:06] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:28:06] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:28:06] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 06:28:10] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:28:10] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:28:10] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:28:10] INFO kernels.py:97: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 06:28:10] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:28:10] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:28:10] INFO kernels.py:97: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 06:28:10] INFO kernels.py:249: FP8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:28:10] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:28:10] INFO kernels.py:373: INT8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:28:10] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:28:10] INFO kernels.py:97: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 06:28:10] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:28:10] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:28:10] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:28:10] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:28:10] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 06:28:10] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:28:10] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:28:10] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:28:10] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1106643) [2026-01-25 06:28:11] INFO SlideSparseLinearMethod_FP8.py:730: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1106643) [2026-01-25 06:28:11] INFO gemm_wrapper.py:872: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1106643) [2026-01-25 06:28:11] INFO SlideSparseLinearMethod_FP8.py:388: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1106643) [2026-01-25 06:28:11] INFO SlideSparseLinearMethod_FP8.py:535: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=1106643) [2026-01-25 06:28:11] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
(EngineCore_DP0 pid=1106643) [2026-01-25 06:28:11] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8-SlideSparse-2_8
(EngineCore_DP0 pid=1106643) [2026-01-25 06:28:11] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
(EngineCore_DP0 pid=1106643) [2026-01-25 06:28:11] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8-SlideSparse-2_8
(EngineCore_DP0 pid=1106643) [2026-01-25 06:28:11] INFO SlideSparseLinearMethod_FP8.py:552: Preloaded FP8 Triton kernels for model: Qwen2.5-0.5B-FP8-SlideSparse-2_8
(EngineCore_DP0 pid=1106643) [2026-01-25 06:28:11] INFO SlideSparseLinearMethod_FP8.py:556: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=1106643) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1106643) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 10.87it/s]
(EngineCore_DP0 pid=1106643) 
(EngineCore_DP0 pid=1106643) [2026-01-25 06:28:11] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [1152, 1344] -> 1D uint8
(EngineCore_DP0 pid=1106643) [2026-01-25 06:28:11] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 976896 bytes
(EngineCore_DP0 pid=1106643) [2026-01-25 06:28:11] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [896, 1344] -> 1D uint8
(EngineCore_DP0 pid=1106643) [2026-01-25 06:28:11] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 759808 bytes
(EngineCore_DP0 pid=1106643) [2026-01-25 06:28:11] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [9728, 1344] -> 1D uint8
(EngineCore_DP0 pid=1106643) [2026-01-25 06:28:11] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 8249344 bytes
(EngineCore_DP0 pid=1106643) [2026-01-25 06:28:11] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [896, 7296] -> 1D uint8
(EngineCore_DP0 pid=1106643) [2026-01-25 06:28:11] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 4085760 bytes
(EngineCore_DP0 pid=1106643) [2026-01-25 06:28:16] WARNING gemm_wrapper.py:393: No cuSPARSELt config for model 'Qwen2.5-0.5B-FP8', using default algorithm
(EngineCore_DP0 pid=1106643) 2026-01-25 06:28:18,428 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1106643) 2026-01-25 06:28:18,458 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1106643) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/36 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 3/36 [00:00<00:01, 29.86it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  19%|█▉        | 7/36 [00:00<00:00, 30.58it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███       | 11/36 [00:00<00:00, 30.87it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 15/36 [00:00<00:00, 30.94it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|█████▎    | 19/36 [00:00<00:00, 30.67it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▍   | 23/36 [00:00<00:00, 30.72it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 27/36 [00:00<00:00, 30.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 31/36 [00:01<00:00, 30.73it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  97%|█████████▋| 35/36 [00:01<00:00, 31.42it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:01<00:00, 30.63it/s]
(EngineCore_DP0 pid=1106643) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   3%|▎         | 1/35 [00:00<00:04,  6.91it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 5/35 [00:00<00:01, 22.25it/s]
Capturing CUDA graphs (decode, FULL):  26%|██▌       | 9/35 [00:00<00:00, 28.22it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 13/35 [00:00<00:00, 31.36it/s]
Capturing CUDA graphs (decode, FULL):  49%|████▊     | 17/35 [00:00<00:00, 32.90it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 21/35 [00:00<00:00, 34.13it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 25/35 [00:00<00:00, 34.82it/s]
Capturing CUDA graphs (decode, FULL):  83%|████████▎ | 29/35 [00:00<00:00, 35.39it/s]
Capturing CUDA graphs (decode, FULL):  94%|█████████▍| 33/35 [00:01<00:00, 35.92it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:01<00:00, 32.49it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 8339.55it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:02<10:00,  2.35s/it, est. speed input: 6.80 toks/s, output: 108.76 toks/s]
Processed prompts:  59%|█████▉    | 152/256 [00:02<00:01, 86.56it/s, est. speed input: 989.11 toks/s, output: 15825.69 toks/s]
Processed prompts:  94%|█████████▍| 240/256 [00:02<00:00, 146.09it/s, est. speed input: 1499.28 toks/s, output: 23988.39 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 146.09it/s, est. speed input: 1564.48 toks/s, output: 25031.62 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 97.77it/s, est. speed input: 1564.48 toks/s, output: 25031.62 toks/s] 
[rank0]:[W125 06:28:24.206438840 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 22.7s

测试结果:
  Requests/s:   96.62
  Tokens/s:     26279.32
  Total Reqs:   256
  Elapsed:      2.65s

  [Decode 分析]
  Total Decode Tokens:  65536
  Decode Tokens/s:      24733.48


------------------------------------------------------------
  生成 CSV: Qwen2.5-0.5B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/Qwen2.5-0.5B-FP8_decode.csv

预览:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
16,16,16,16,256,256,13.4056,3646.3318,1.1935
128,16,128,128,256,256,77.2076,21000.4764,1.6579
256,16,256,256,256,256,96.6152,26279.3219,2.6497

------------------------------------------------------------

[INFO] 完成: 3 成功, 0 失败

======================================================================
[SlideSparse] 未找到 Qwen2.5-0.5B-FP8-SlideSparse-2_10 checkpoint
[SlideSparse] 发现基础模型: Qwen2.5-0.5B-FP8
[SlideSparse] 自动转换中...
[SlideSparse] 开始转换: Qwen2.5-0.5B-FP8 -> SlideSparse-2_10
======================================================================

[SlideSparse] 转换成功: Qwen2.5-0.5B-FP8-SlideSparse-2_10
======================================================================


============================================================
  Qwen2.5-0.5B-FP8 | cuSPARSELt (2_10) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_10
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10

============================================================
[1/3] 测试 M=16
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-0.5B-FP8                                │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16
│   M_prefill     = 16 (= 1 x 16)
│   M_decode      = 1
│   batched_tokens = 17 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 17
│   --max-num-batched-tokens = 17
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 06:29:03 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1107315) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1107315) WARNING 01-25 06:29:11 [backends.py:609] Failed to read file <frozen os>
Throughput: 70.63 requests/s, 1200.65 total tokens/s, 70.63 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  128


─── STDERR ───
[2026-01-25 06:29:03] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:29:03] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:29:03] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:29:03] INFO kernels.py:97: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 06:29:03] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:29:03] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:29:03] INFO kernels.py:97: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 06:29:03] INFO kernels.py:249: FP8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:29:03] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:29:03] INFO kernels.py:373: INT8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:29:03] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:29:03] INFO kernels.py:97: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 06:29:03] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:29:03] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:29:03] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:29:03] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:29:03] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 06:29:03] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:29:03] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:29:03] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:29:03] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 06:29:07] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:29:07] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:29:07] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:29:07] INFO kernels.py:97: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 06:29:07] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:29:07] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:29:07] INFO kernels.py:97: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 06:29:07] INFO kernels.py:249: FP8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:29:07] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:29:07] INFO kernels.py:373: INT8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:29:07] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:29:07] INFO kernels.py:97: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 06:29:07] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:29:07] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:29:07] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:29:07] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:29:07] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 06:29:07] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:29:07] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:29:07] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:29:07] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1107315) [2026-01-25 06:29:08] INFO SlideSparseLinearMethod_FP8.py:730: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1107315) [2026-01-25 06:29:08] INFO gemm_wrapper.py:872: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1107315) [2026-01-25 06:29:08] INFO SlideSparseLinearMethod_FP8.py:388: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1107315) [2026-01-25 06:29:08] INFO SlideSparseLinearMethod_FP8.py:535: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=1107315) [2026-01-25 06:29:08] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
(EngineCore_DP0 pid=1107315) [2026-01-25 06:29:08] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8-SlideSparse-2_10
(EngineCore_DP0 pid=1107315) [2026-01-25 06:29:08] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
(EngineCore_DP0 pid=1107315) [2026-01-25 06:29:08] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8-SlideSparse-2_10
(EngineCore_DP0 pid=1107315) [2026-01-25 06:29:08] INFO SlideSparseLinearMethod_FP8.py:552: Preloaded FP8 Triton kernels for model: Qwen2.5-0.5B-FP8-SlideSparse-2_10
(EngineCore_DP0 pid=1107315) [2026-01-25 06:29:08] INFO SlideSparseLinearMethod_FP8.py:556: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=1107315) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1107315) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 11.05it/s]
(EngineCore_DP0 pid=1107315) 
(EngineCore_DP0 pid=1107315) [2026-01-25 06:29:08] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [1152, 1440] -> 1D uint8
(EngineCore_DP0 pid=1107315) [2026-01-25 06:29:08] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 1050624 bytes
(EngineCore_DP0 pid=1107315) [2026-01-25 06:29:08] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [896, 1440] -> 1D uint8
(EngineCore_DP0 pid=1107315) [2026-01-25 06:29:08] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 817152 bytes
(EngineCore_DP0 pid=1107315) [2026-01-25 06:29:08] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [9728, 1440] -> 1D uint8
(EngineCore_DP0 pid=1107315) [2026-01-25 06:29:08] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 8871936 bytes
(EngineCore_DP0 pid=1107315) [2026-01-25 06:29:08] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [896, 7808] -> 1D uint8
(EngineCore_DP0 pid=1107315) [2026-01-25 06:29:08] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 4372480 bytes
(EngineCore_DP0 pid=1107315) [2026-01-25 06:29:14] WARNING gemm_wrapper.py:393: No cuSPARSELt config for model 'Qwen2.5-0.5B-FP8', using default algorithm
(EngineCore_DP0 pid=1107315) 2026-01-25 06:29:16,936 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1107315) 2026-01-25 06:29:16,949 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1107315) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  7.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  7.50it/s]
(EngineCore_DP0 pid=1107315) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.89it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.89it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 8039.28it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:26,  4.73it/s, est. speed input: 75.66 toks/s, output: 4.73 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:03, 34.90it/s, est. speed input: 460.52 toks/s, output: 28.78 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:02, 51.02it/s, est. speed input: 656.46 toks/s, output: 41.03 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:00<00:01, 60.71it/s, est. speed input: 776.31 toks/s, output: 48.52 toks/s]
Processed prompts:  27%|██▋       | 34/128 [00:00<00:01, 67.80it/s, est. speed input: 868.36 toks/s, output: 54.27 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:00<00:01, 72.07it/s, est. speed input: 931.97 toks/s, output: 58.25 toks/s]
Processed prompts:  41%|████      | 52/128 [00:00<00:01, 74.82it/s, est. speed input: 978.95 toks/s, output: 61.18 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:00<00:00, 76.70it/s, est. speed input: 1015.26 toks/s, output: 63.45 toks/s]
Processed prompts:  55%|█████▍    | 70/128 [00:01<00:00, 78.18it/s, est. speed input: 1045.01 toks/s, output: 65.31 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:01<00:00, 78.98it/s, est. speed input: 1068.29 toks/s, output: 66.77 toks/s]
Processed prompts:  69%|██████▉   | 88/128 [00:01<00:00, 79.60it/s, est. speed input: 1087.86 toks/s, output: 67.99 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:01<00:00, 79.76it/s, est. speed input: 1103.37 toks/s, output: 68.96 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:01<00:00, 79.78it/s, est. speed input: 1116.27 toks/s, output: 69.77 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:01<00:00, 80.12it/s, est. speed input: 1128.44 toks/s, output: 70.53 toks/s]
Processed prompts:  97%|█████████▋| 124/128 [00:01<00:00, 79.58it/s, est. speed input: 1136.70 toks/s, output: 71.04 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 79.58it/s, est. speed input: 1140.60 toks/s, output: 71.29 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 71.28it/s, est. speed input: 1140.60 toks/s, output: 71.29 toks/s]
[rank0]:[W125 06:29:20.258703378 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 21.5s

测试结果:
  Requests/s:   70.63
  Tokens/s:     1200.65
  Total Reqs:   128
  Elapsed:      1.81s

  [Prefill 分析]
  Total Prefill Tokens: 2048
  Prefill Tokens/s:     1130.03

============================================================
[2/3] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-0.5B-FP8                                │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 128 (= 1 x 128)
│   M_decode      = 1
│   batched_tokens = 129 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 128
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 129
│   --max-num-batched-tokens = 129
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 06:29:25 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1107710) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1107710) WARNING 01-25 06:29:33 [backends.py:609] Failed to read file <frozen os>
Throughput: 68.99 requests/s, 8899.97 total tokens/s, 68.99 output tokens/s
Total num prompt tokens:  16384
Total num output tokens:  128


─── STDERR ───
[2026-01-25 06:29:25] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:29:25] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:29:25] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:29:25] INFO kernels.py:97: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 06:29:25] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:29:25] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:29:25] INFO kernels.py:97: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 06:29:25] INFO kernels.py:249: FP8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:29:25] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:29:25] INFO kernels.py:373: INT8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:29:25] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:29:25] INFO kernels.py:97: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 06:29:25] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:29:25] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:29:25] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:29:25] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:29:25] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 06:29:25] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:29:25] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:29:25] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:29:25] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 06:29:29] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:29:29] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:29:29] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:29:29] INFO kernels.py:97: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 06:29:29] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:29:29] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:29:29] INFO kernels.py:97: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 06:29:29] INFO kernels.py:249: FP8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:29:29] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:29:29] INFO kernels.py:373: INT8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:29:29] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:29:29] INFO kernels.py:97: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 06:29:29] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:29:29] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:29:29] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:29:29] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:29:29] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 06:29:29] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:29:29] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:29:29] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:29:29] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1107710) [2026-01-25 06:29:29] INFO SlideSparseLinearMethod_FP8.py:730: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1107710) [2026-01-25 06:29:29] INFO gemm_wrapper.py:872: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1107710) [2026-01-25 06:29:29] INFO SlideSparseLinearMethod_FP8.py:388: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1107710) [2026-01-25 06:29:29] INFO SlideSparseLinearMethod_FP8.py:535: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=1107710) [2026-01-25 06:29:29] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
(EngineCore_DP0 pid=1107710) [2026-01-25 06:29:29] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8-SlideSparse-2_10
(EngineCore_DP0 pid=1107710) [2026-01-25 06:29:29] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
(EngineCore_DP0 pid=1107710) [2026-01-25 06:29:29] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8-SlideSparse-2_10
(EngineCore_DP0 pid=1107710) [2026-01-25 06:29:29] INFO SlideSparseLinearMethod_FP8.py:552: Preloaded FP8 Triton kernels for model: Qwen2.5-0.5B-FP8-SlideSparse-2_10
(EngineCore_DP0 pid=1107710) [2026-01-25 06:29:29] INFO SlideSparseLinearMethod_FP8.py:556: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=1107710) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1107710) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 10.76it/s]
(EngineCore_DP0 pid=1107710) 
(EngineCore_DP0 pid=1107710) [2026-01-25 06:29:30] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [1152, 1440] -> 1D uint8
(EngineCore_DP0 pid=1107710) [2026-01-25 06:29:30] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 1050624 bytes
(EngineCore_DP0 pid=1107710) [2026-01-25 06:29:30] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [896, 1440] -> 1D uint8
(EngineCore_DP0 pid=1107710) [2026-01-25 06:29:30] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 817152 bytes
(EngineCore_DP0 pid=1107710) [2026-01-25 06:29:30] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [9728, 1440] -> 1D uint8
(EngineCore_DP0 pid=1107710) [2026-01-25 06:29:30] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 8871936 bytes
(EngineCore_DP0 pid=1107710) [2026-01-25 06:29:30] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [896, 7808] -> 1D uint8
(EngineCore_DP0 pid=1107710) [2026-01-25 06:29:30] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 4372480 bytes
(EngineCore_DP0 pid=1107710) [2026-01-25 06:29:36] WARNING gemm_wrapper.py:393: No cuSPARSELt config for model 'Qwen2.5-0.5B-FP8', using default algorithm
(EngineCore_DP0 pid=1107710) 2026-01-25 06:29:38,819 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1107710) 2026-01-25 06:29:38,833 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1107710) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  9.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 12.68it/s]
(EngineCore_DP0 pid=1107710) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.29it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.29it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 2589.43it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:23,  5.39it/s, est. speed input: 690.01 toks/s, output: 5.39 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:03, 37.61it/s, est. speed input: 4014.57 toks/s, output: 31.36 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:02, 53.79it/s, est. speed input: 5622.73 toks/s, output: 43.93 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:00<00:01, 62.94it/s, est. speed input: 6564.82 toks/s, output: 51.29 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:00<00:01, 68.40it/s, est. speed input: 7177.23 toks/s, output: 56.07 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:00<00:01, 71.55it/s, est. speed input: 7593.36 toks/s, output: 59.32 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:00<00:01, 74.18it/s, est. speed input: 7926.99 toks/s, output: 61.93 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:00<00:00, 75.86it/s, est. speed input: 8182.04 toks/s, output: 63.92 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:00<00:00, 76.96it/s, est. speed input: 8383.58 toks/s, output: 65.50 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:01<00:00, 77.74it/s, est. speed input: 8548.57 toks/s, output: 66.79 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:01<00:00, 78.05it/s, est. speed input: 8678.92 toks/s, output: 67.80 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:01<00:00, 77.12it/s, est. speed input: 8755.32 toks/s, output: 68.40 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:01<00:00, 77.26it/s, est. speed input: 8841.62 toks/s, output: 69.07 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:01<00:00, 77.35it/s, est. speed input: 8915.78 toks/s, output: 69.65 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:01<00:00, 76.99it/s, est. speed input: 8970.22 toks/s, output: 70.08 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:01<00:00, 77.38it/s, est. speed input: 9032.89 toks/s, output: 70.57 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 77.38it/s, est. speed input: 9077.43 toks/s, output: 70.92 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 70.91it/s, est. speed input: 9077.43 toks/s, output: 70.92 toks/s]
[rank0]:[W125 06:29:42.096369888 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 21.9s

测试结果:
  Requests/s:   68.99
  Tokens/s:     8899.97
  Total Reqs:   128
  Elapsed:      1.86s

  [Prefill 分析]
  Total Prefill Tokens: 16384
  Prefill Tokens/s:     8830.98

============================================================
[3/3] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-0.5B-FP8                                │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 256 (= 1 x 256)
│   M_decode      = 1
│   batched_tokens = 257 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 256
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 257
│   --max-num-batched-tokens = 257
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 06:29:47 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1108146) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1108146) WARNING 01-25 06:29:55 [backends.py:609] Failed to read file <frozen os>
Throughput: 73.83 requests/s, 18975.39 total tokens/s, 73.83 output tokens/s
Total num prompt tokens:  32768
Total num output tokens:  128


─── STDERR ───
[2026-01-25 06:29:47] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:29:47] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:29:47] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:29:47] INFO kernels.py:97: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 06:29:47] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:29:47] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:29:47] INFO kernels.py:97: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 06:29:47] INFO kernels.py:249: FP8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:29:47] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:29:47] INFO kernels.py:373: INT8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:29:47] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:29:47] INFO kernels.py:97: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 06:29:47] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:29:47] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:29:47] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:29:47] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:29:47] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 06:29:47] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:29:47] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:29:47] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:29:47] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 06:29:51] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:29:51] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:29:51] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:29:51] INFO kernels.py:97: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 06:29:51] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:29:51] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:29:51] INFO kernels.py:97: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 06:29:51] INFO kernels.py:249: FP8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:29:51] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:29:51] INFO kernels.py:373: INT8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:29:51] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:29:51] INFO kernels.py:97: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 06:29:51] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:29:51] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:29:51] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:29:51] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:29:51] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 06:29:51] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:29:51] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:29:51] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:29:51] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1108146) [2026-01-25 06:29:52] INFO SlideSparseLinearMethod_FP8.py:730: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1108146) [2026-01-25 06:29:52] INFO gemm_wrapper.py:872: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1108146) [2026-01-25 06:29:52] INFO SlideSparseLinearMethod_FP8.py:388: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1108146) [2026-01-25 06:29:52] INFO SlideSparseLinearMethod_FP8.py:535: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=1108146) [2026-01-25 06:29:52] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
(EngineCore_DP0 pid=1108146) [2026-01-25 06:29:52] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8-SlideSparse-2_10
(EngineCore_DP0 pid=1108146) [2026-01-25 06:29:52] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
(EngineCore_DP0 pid=1108146) [2026-01-25 06:29:52] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8-SlideSparse-2_10
(EngineCore_DP0 pid=1108146) [2026-01-25 06:29:52] INFO SlideSparseLinearMethod_FP8.py:552: Preloaded FP8 Triton kernels for model: Qwen2.5-0.5B-FP8-SlideSparse-2_10
(EngineCore_DP0 pid=1108146) [2026-01-25 06:29:52] INFO SlideSparseLinearMethod_FP8.py:556: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=1108146) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1108146) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 10.93it/s]
(EngineCore_DP0 pid=1108146) 
(EngineCore_DP0 pid=1108146) [2026-01-25 06:29:52] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [1152, 1440] -> 1D uint8
(EngineCore_DP0 pid=1108146) [2026-01-25 06:29:52] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 1050624 bytes
(EngineCore_DP0 pid=1108146) [2026-01-25 06:29:52] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [896, 1440] -> 1D uint8
(EngineCore_DP0 pid=1108146) [2026-01-25 06:29:52] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 817152 bytes
(EngineCore_DP0 pid=1108146) [2026-01-25 06:29:52] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [9728, 1440] -> 1D uint8
(EngineCore_DP0 pid=1108146) [2026-01-25 06:29:52] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 8871936 bytes
(EngineCore_DP0 pid=1108146) [2026-01-25 06:29:52] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [896, 7808] -> 1D uint8
(EngineCore_DP0 pid=1108146) [2026-01-25 06:29:52] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 4372480 bytes
(EngineCore_DP0 pid=1108146) [2026-01-25 06:29:58] WARNING gemm_wrapper.py:393: No cuSPARSELt config for model 'Qwen2.5-0.5B-FP8', using default algorithm
(EngineCore_DP0 pid=1108146) 2026-01-25 06:30:00,495 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1108146) 2026-01-25 06:30:00,508 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1108146) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 27.36it/s]
(EngineCore_DP0 pid=1108146) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.91it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.90it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 1538.90it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:01, 82.86it/s, est. speed input: 21215.02 toks/s, output: 82.86 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:00<00:01, 80.21it/s, est. speed input: 20635.68 toks/s, output: 80.60 toks/s]
Processed prompts:  21%|██        | 27/128 [00:00<00:01, 78.56it/s, est. speed input: 20289.07 toks/s, output: 79.25 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:00<00:01, 77.51it/s, est. speed input: 20078.19 toks/s, output: 78.43 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:00<00:01, 77.32it/s, est. speed input: 20007.35 toks/s, output: 78.15 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:00<00:00, 77.15it/s, est. speed input: 19951.75 toks/s, output: 77.93 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:00<00:00, 76.98it/s, est. speed input: 19905.96 toks/s, output: 77.76 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:00<00:00, 76.93it/s, est. speed input: 19876.53 toks/s, output: 77.64 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:00<00:00, 76.97it/s, est. speed input: 19860.47 toks/s, output: 77.58 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:01<00:00, 77.20it/s, est. speed input: 19864.45 toks/s, output: 77.59 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:01<00:00, 77.12it/s, est. speed input: 19849.14 toks/s, output: 77.54 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:01<00:00, 77.43it/s, est. speed input: 19861.73 toks/s, output: 77.58 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:01<00:00, 77.87it/s, est. speed input: 19886.36 toks/s, output: 77.68 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:01<00:00, 77.86it/s, est. speed input: 19889.10 toks/s, output: 77.69 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:01<00:00, 78.09it/s, est. speed input: 19904.49 toks/s, output: 77.75 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 78.09it/s, est. speed input: 19876.45 toks/s, output: 77.64 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 77.64it/s, est. speed input: 19876.45 toks/s, output: 77.64 toks/s]
[rank0]:[W125 06:30:03.366702577 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 21.2s

测试结果:
  Requests/s:   73.83
  Tokens/s:     18975.39
  Total Reqs:   128
  Elapsed:      1.73s

  [Prefill 分析]
  Total Prefill Tokens: 32768
  Prefill Tokens/s:     18901.56


------------------------------------------------------------
  生成 CSV: Qwen2.5-0.5B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/Qwen2.5-0.5B-FP8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
16,16,1,128,128,70.6268,1200.6549,1.8123
128,128,1,128,128,68.9920,8899.9686,1.8553
256,256,1,128,128,73.8342,18975.3929,1.7336

------------------------------------------------------------

[INFO] 完成: 3 成功, 0 失败

============================================================
  Qwen2.5-0.5B-FP8 | cuSPARSELt (2_10) | decode
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_10
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10

============================================================
[1/3] 测试 M=16
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-0.5B-FP8                                │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16
│   M_prefill     = 256 (= 16 x 16)
│   M_decode      = 16
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 16
│   --max-num-seqs           = 16
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 06:30:08 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1108522) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1108522) WARNING 01-25 06:30:16 [backends.py:609] Failed to read file <frozen os>
Throughput: 14.76 requests/s, 4015.47 total tokens/s, 3779.26 output tokens/s
Total num prompt tokens:  256
Total num output tokens:  4096


─── STDERR ───
[2026-01-25 06:30:08] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:30:08] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:30:08] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:30:08] INFO kernels.py:97: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 06:30:08] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:30:08] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:30:08] INFO kernels.py:97: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 06:30:08] INFO kernels.py:249: FP8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:30:08] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:30:08] INFO kernels.py:373: INT8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:30:08] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:30:08] INFO kernels.py:97: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 06:30:08] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:30:08] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:30:08] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:30:08] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:30:08] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 06:30:08] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:30:08] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:30:08] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:30:08] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 06:30:12] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:30:12] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:30:12] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:30:12] INFO kernels.py:97: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 06:30:12] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:30:12] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:30:12] INFO kernels.py:97: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 06:30:12] INFO kernels.py:249: FP8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:30:12] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:30:12] INFO kernels.py:373: INT8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:30:12] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:30:12] INFO kernels.py:97: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 06:30:12] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:30:12] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:30:12] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:30:12] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:30:12] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 06:30:12] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:30:12] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:30:12] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:30:12] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1108522) [2026-01-25 06:30:13] INFO SlideSparseLinearMethod_FP8.py:730: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1108522) [2026-01-25 06:30:13] INFO gemm_wrapper.py:872: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1108522) [2026-01-25 06:30:13] INFO SlideSparseLinearMethod_FP8.py:388: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1108522) [2026-01-25 06:30:13] INFO SlideSparseLinearMethod_FP8.py:535: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=1108522) [2026-01-25 06:30:13] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
(EngineCore_DP0 pid=1108522) [2026-01-25 06:30:13] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8-SlideSparse-2_10
(EngineCore_DP0 pid=1108522) [2026-01-25 06:30:13] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
(EngineCore_DP0 pid=1108522) [2026-01-25 06:30:13] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8-SlideSparse-2_10
(EngineCore_DP0 pid=1108522) [2026-01-25 06:30:13] INFO SlideSparseLinearMethod_FP8.py:552: Preloaded FP8 Triton kernels for model: Qwen2.5-0.5B-FP8-SlideSparse-2_10
(EngineCore_DP0 pid=1108522) [2026-01-25 06:30:13] INFO SlideSparseLinearMethod_FP8.py:556: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=1108522) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1108522) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 10.94it/s]
(EngineCore_DP0 pid=1108522) 
(EngineCore_DP0 pid=1108522) [2026-01-25 06:30:13] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [1152, 1440] -> 1D uint8
(EngineCore_DP0 pid=1108522) [2026-01-25 06:30:13] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 1050624 bytes
(EngineCore_DP0 pid=1108522) [2026-01-25 06:30:13] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [896, 1440] -> 1D uint8
(EngineCore_DP0 pid=1108522) [2026-01-25 06:30:13] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 817152 bytes
(EngineCore_DP0 pid=1108522) [2026-01-25 06:30:13] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [9728, 1440] -> 1D uint8
(EngineCore_DP0 pid=1108522) [2026-01-25 06:30:13] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 8871936 bytes
(EngineCore_DP0 pid=1108522) [2026-01-25 06:30:13] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [896, 7808] -> 1D uint8
(EngineCore_DP0 pid=1108522) [2026-01-25 06:30:13] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 4372480 bytes
(EngineCore_DP0 pid=1108522) [2026-01-25 06:30:19] WARNING gemm_wrapper.py:393: No cuSPARSELt config for model 'Qwen2.5-0.5B-FP8', using default algorithm
(EngineCore_DP0 pid=1108522) 2026-01-25 06:30:21,558 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1108522) 2026-01-25 06:30:21,571 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1108522) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 3/7 [00:00<00:00, 27.45it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 29.50it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 29.21it/s]
(EngineCore_DP0 pid=1108522) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 1/5 [00:00<00:00,  6.94it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 22.43it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 19.78it/s]

Adding requests:   0%|          | 0/16 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 16/16 [00:00<00:00, 1179.89it/s]

Processed prompts:   0%|          | 0/16 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   6%|▋         | 1/16 [00:01<00:15,  1.06s/it, est. speed input: 15.04 toks/s, output: 240.59 toks/s]
Processed prompts: 100%|██████████| 16/16 [00:01<00:00,  1.06s/it, est. speed input: 239.67 toks/s, output: 3834.69 toks/s]
Processed prompts: 100%|██████████| 16/16 [00:01<00:00, 14.98it/s, est. speed input: 239.67 toks/s, output: 3834.69 toks/s]
[rank0]:[W125 06:30:24.058945618 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 20.9s

测试结果:
  Requests/s:   14.76
  Tokens/s:     4015.47
  Total Reqs:   16
  Elapsed:      1.08s

  [Decode 分析]
  Total Decode Tokens:  4096
  Decode Tokens/s:      3779.26

============================================================
[2/3] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-0.5B-FP8                                │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 2048 (= 128 x 16)
│   M_decode      = 128
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 128
│   --max-num-seqs           = 128
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 06:30:29 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1108909) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1108909) WARNING 01-25 06:30:37 [backends.py:609] Failed to read file <frozen os>
Throughput: 76.02 requests/s, 20676.81 total tokens/s, 19460.53 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768


─── STDERR ───
[2026-01-25 06:30:29] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:30:29] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:30:29] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:30:29] INFO kernels.py:97: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 06:30:29] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:30:29] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:30:29] INFO kernels.py:97: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 06:30:29] INFO kernels.py:249: FP8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:30:29] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:30:29] INFO kernels.py:373: INT8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:30:29] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:30:29] INFO kernels.py:97: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 06:30:29] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:30:29] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:30:29] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:30:29] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:30:29] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 06:30:29] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:30:29] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:30:29] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:30:29] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 06:30:33] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:30:33] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:30:33] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:30:33] INFO kernels.py:97: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 06:30:33] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:30:33] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:30:33] INFO kernels.py:97: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 06:30:33] INFO kernels.py:249: FP8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:30:33] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:30:33] INFO kernels.py:373: INT8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:30:33] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:30:33] INFO kernels.py:97: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 06:30:33] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:30:33] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:30:33] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:30:33] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:30:33] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 06:30:33] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:30:33] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:30:33] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:30:33] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1108909) [2026-01-25 06:30:34] INFO SlideSparseLinearMethod_FP8.py:730: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1108909) [2026-01-25 06:30:34] INFO gemm_wrapper.py:872: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1108909) [2026-01-25 06:30:34] INFO SlideSparseLinearMethod_FP8.py:388: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1108909) [2026-01-25 06:30:34] INFO SlideSparseLinearMethod_FP8.py:535: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=1108909) [2026-01-25 06:30:34] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
(EngineCore_DP0 pid=1108909) [2026-01-25 06:30:34] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8-SlideSparse-2_10
(EngineCore_DP0 pid=1108909) [2026-01-25 06:30:34] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
(EngineCore_DP0 pid=1108909) [2026-01-25 06:30:34] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8-SlideSparse-2_10
(EngineCore_DP0 pid=1108909) [2026-01-25 06:30:34] INFO SlideSparseLinearMethod_FP8.py:552: Preloaded FP8 Triton kernels for model: Qwen2.5-0.5B-FP8-SlideSparse-2_10
(EngineCore_DP0 pid=1108909) [2026-01-25 06:30:34] INFO SlideSparseLinearMethod_FP8.py:556: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=1108909) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1108909) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 10.70it/s]
(EngineCore_DP0 pid=1108909) 
(EngineCore_DP0 pid=1108909) [2026-01-25 06:30:34] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [1152, 1440] -> 1D uint8
(EngineCore_DP0 pid=1108909) [2026-01-25 06:30:34] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 1050624 bytes
(EngineCore_DP0 pid=1108909) [2026-01-25 06:30:34] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [896, 1440] -> 1D uint8
(EngineCore_DP0 pid=1108909) [2026-01-25 06:30:34] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 817152 bytes
(EngineCore_DP0 pid=1108909) [2026-01-25 06:30:34] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [9728, 1440] -> 1D uint8
(EngineCore_DP0 pid=1108909) [2026-01-25 06:30:34] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 8871936 bytes
(EngineCore_DP0 pid=1108909) [2026-01-25 06:30:34] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [896, 7808] -> 1D uint8
(EngineCore_DP0 pid=1108909) [2026-01-25 06:30:34] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 4372480 bytes
(EngineCore_DP0 pid=1108909) [2026-01-25 06:30:40] WARNING gemm_wrapper.py:393: No cuSPARSELt config for model 'Qwen2.5-0.5B-FP8', using default algorithm
(EngineCore_DP0 pid=1108909) 2026-01-25 06:30:41,460 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1108909) 2026-01-25 06:30:41,473 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1108909) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▊         | 3/35 [00:00<00:01, 27.25it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 6/35 [00:00<00:01, 27.59it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▌       | 9/35 [00:00<00:00, 28.42it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  34%|███▍      | 12/35 [00:00<00:00, 26.07it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 15/35 [00:00<00:00, 22.86it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  51%|█████▏    | 18/35 [00:00<00:00, 19.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 21/35 [00:00<00:00, 22.11it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 25/35 [00:01<00:00, 24.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  83%|████████▎ | 29/35 [00:01<00:00, 26.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████▏| 32/35 [00:01<00:00, 27.09it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:01<00:00, 27.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:01<00:00, 25.44it/s]
(EngineCore_DP0 pid=1108909) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   5%|▌         | 1/19 [00:00<00:02,  6.86it/s]
Capturing CUDA graphs (decode, FULL):  26%|██▋       | 5/19 [00:00<00:00, 21.96it/s]
Capturing CUDA graphs (decode, FULL):  47%|████▋     | 9/19 [00:00<00:00, 27.68it/s]
Capturing CUDA graphs (decode, FULL):  68%|██████▊   | 13/19 [00:00<00:00, 30.75it/s]
Capturing CUDA graphs (decode, FULL):  89%|████████▉ | 17/19 [00:00<00:00, 32.69it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:00<00:00, 29.33it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 4038.26it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:01<03:22,  1.60s/it, est. speed input: 10.03 toks/s, output: 160.41 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00,  1.60s/it, est. speed input: 1240.89 toks/s, output: 19854.28 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 77.55it/s, est. speed input: 1240.89 toks/s, output: 19854.28 toks/s]
[rank0]:[W125 06:30:46.085257528 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 21.7s

测试结果:
  Requests/s:   76.02
  Tokens/s:     20676.81
  Total Reqs:   128
  Elapsed:      1.68s

  [Decode 分析]
  Total Decode Tokens:  32768
  Decode Tokens/s:      19460.53

============================================================
[3/3] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-0.5B-FP8                                │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 4096 (= 256 x 16)
│   M_decode      = 256
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 256
│   --max-num-seqs           = 256
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 06:30:51 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1109284) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1109284) WARNING 01-25 06:30:59 [backends.py:609] Failed to read file <frozen os>
Throughput: 95.04 requests/s, 25850.97 total tokens/s, 24330.32 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536


─── STDERR ───
[2026-01-25 06:30:51] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:30:51] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:30:51] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:30:51] INFO kernels.py:97: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 06:30:51] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:30:51] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:30:51] INFO kernels.py:97: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 06:30:51] INFO kernels.py:249: FP8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:30:51] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:30:51] INFO kernels.py:373: INT8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:30:51] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:30:51] INFO kernels.py:97: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 06:30:51] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:30:51] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:30:51] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:30:51] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:30:51] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 06:30:51] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:30:51] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:30:51] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:30:51] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 06:30:55] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:30:55] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:30:55] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:30:55] INFO kernels.py:97: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 06:30:55] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:30:55] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:30:55] INFO kernels.py:97: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 06:30:55] INFO kernels.py:249: FP8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:30:55] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:30:55] INFO kernels.py:373: INT8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:30:55] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:30:55] INFO kernels.py:97: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 06:30:55] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:30:55] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:30:55] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:30:55] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:30:55] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 06:30:55] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:30:55] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:30:55] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:30:55] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1109284) [2026-01-25 06:30:55] INFO SlideSparseLinearMethod_FP8.py:730: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1109284) [2026-01-25 06:30:55] INFO gemm_wrapper.py:872: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1109284) [2026-01-25 06:30:55] INFO SlideSparseLinearMethod_FP8.py:388: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1109284) [2026-01-25 06:30:55] INFO SlideSparseLinearMethod_FP8.py:535: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=1109284) [2026-01-25 06:30:55] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
(EngineCore_DP0 pid=1109284) [2026-01-25 06:30:55] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8-SlideSparse-2_10
(EngineCore_DP0 pid=1109284) [2026-01-25 06:30:55] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
(EngineCore_DP0 pid=1109284) [2026-01-25 06:30:55] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8-SlideSparse-2_10
(EngineCore_DP0 pid=1109284) [2026-01-25 06:30:55] INFO SlideSparseLinearMethod_FP8.py:552: Preloaded FP8 Triton kernels for model: Qwen2.5-0.5B-FP8-SlideSparse-2_10
(EngineCore_DP0 pid=1109284) [2026-01-25 06:30:55] INFO SlideSparseLinearMethod_FP8.py:556: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=1109284) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1109284) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 11.00it/s]
(EngineCore_DP0 pid=1109284) 
(EngineCore_DP0 pid=1109284) [2026-01-25 06:30:56] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [1152, 1440] -> 1D uint8
(EngineCore_DP0 pid=1109284) [2026-01-25 06:30:56] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 1050624 bytes
(EngineCore_DP0 pid=1109284) [2026-01-25 06:30:56] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [896, 1440] -> 1D uint8
(EngineCore_DP0 pid=1109284) [2026-01-25 06:30:56] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 817152 bytes
(EngineCore_DP0 pid=1109284) [2026-01-25 06:30:56] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [9728, 1440] -> 1D uint8
(EngineCore_DP0 pid=1109284) [2026-01-25 06:30:56] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 8871936 bytes
(EngineCore_DP0 pid=1109284) [2026-01-25 06:30:56] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [896, 7808] -> 1D uint8
(EngineCore_DP0 pid=1109284) [2026-01-25 06:30:56] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 4372480 bytes
(EngineCore_DP0 pid=1109284) [2026-01-25 06:31:01] WARNING gemm_wrapper.py:393: No cuSPARSELt config for model 'Qwen2.5-0.5B-FP8', using default algorithm
(EngineCore_DP0 pid=1109284) 2026-01-25 06:31:03,104 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1109284) 2026-01-25 06:31:03,119 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1109284) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/36 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   3%|▎         | 1/36 [00:00<00:04,  8.37it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 4/36 [00:00<00:01, 18.13it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|██▏       | 8/36 [00:00<00:01, 24.10it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  33%|███▎      | 12/36 [00:00<00:00, 26.86it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  44%|████▍     | 16/36 [00:00<00:00, 28.45it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  56%|█████▌    | 20/36 [00:00<00:00, 29.11it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 24/36 [00:00<00:00, 29.72it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  78%|███████▊  | 28/36 [00:01<00:00, 29.79it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 32/36 [00:01<00:00, 30.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:01<00:00, 30.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:01<00:00, 28.14it/s]
(EngineCore_DP0 pid=1109284) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   3%|▎         | 1/35 [00:00<00:04,  6.89it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 5/35 [00:00<00:01, 22.29it/s]
Capturing CUDA graphs (decode, FULL):  26%|██▌       | 9/35 [00:00<00:00, 28.39it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 13/35 [00:00<00:00, 31.49it/s]
Capturing CUDA graphs (decode, FULL):  49%|████▊     | 17/35 [00:00<00:00, 33.33it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 21/35 [00:00<00:00, 34.58it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 25/35 [00:00<00:00, 35.08it/s]
Capturing CUDA graphs (decode, FULL):  83%|████████▎ | 29/35 [00:00<00:00, 32.30it/s]
Capturing CUDA graphs (decode, FULL):  94%|█████████▍| 33/35 [00:01<00:00, 30.58it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:01<00:00, 30.41it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 6308.15it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:02<10:06,  2.38s/it, est. speed input: 6.73 toks/s, output: 107.61 toks/s]
Processed prompts:  56%|█████▋    | 144/256 [00:02<00:01, 81.26it/s, est. speed input: 928.23 toks/s, output: 14851.64 toks/s]
Processed prompts:  92%|█████████▏| 236/256 [00:02<00:00, 143.68it/s, est. speed input: 1460.96 toks/s, output: 23375.21 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 143.68it/s, est. speed input: 1545.04 toks/s, output: 24720.62 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 96.56it/s, est. speed input: 1545.04 toks/s, output: 24720.62 toks/s] 
[rank0]:[W125 06:31:09.079171200 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 23.1s

测试结果:
  Requests/s:   95.04
  Tokens/s:     25850.97
  Total Reqs:   256
  Elapsed:      2.69s

  [Decode 分析]
  Total Decode Tokens:  65536
  Decode Tokens/s:      24330.32


------------------------------------------------------------
  生成 CSV: Qwen2.5-0.5B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/Qwen2.5-0.5B-FP8_decode.csv

预览:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
16,16,16,16,256,256,14.7627,4015.4657,1.0838
128,16,128,128,256,256,76.0177,20676.8104,1.6838
256,16,256,256,256,256,95.0403,25850.9692,2.6936

------------------------------------------------------------

[INFO] 完成: 3 成功, 0 失败

======================================================================
[SlideSparse] 未找到 Qwen2.5-0.5B-FP8-SlideSparse-2_12 checkpoint
[SlideSparse] 发现基础模型: Qwen2.5-0.5B-FP8
[SlideSparse] 自动转换中...
[SlideSparse] 开始转换: Qwen2.5-0.5B-FP8 -> SlideSparse-2_12
======================================================================

[SlideSparse] 转换成功: Qwen2.5-0.5B-FP8-SlideSparse-2_12
======================================================================


============================================================
  Qwen2.5-0.5B-FP8 | cuSPARSELt (2_12) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_12
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_12

============================================================
[1/3] 测试 M=16
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-0.5B-FP8                                │
│ Backend:  cuSPARSELt (2:12)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16
│   M_prefill     = 16 (= 1 x 16)
│   M_decode      = 1
│   batched_tokens = 17 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 17
│   --max-num-batched-tokens = 17
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 06:31:40 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1109913) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1109913) WARNING 01-25 06:31:48 [backends.py:609] Failed to read file <frozen os>
Throughput: 72.00 requests/s, 1223.92 total tokens/s, 72.00 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  128


─── STDERR ───
[2026-01-25 06:31:40] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:31:40] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:31:40] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:31:40] INFO kernels.py:97: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 06:31:40] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:31:40] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:31:40] INFO kernels.py:97: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 06:31:40] INFO kernels.py:249: FP8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:31:40] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:31:40] INFO kernels.py:373: INT8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:31:40] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:31:40] INFO kernels.py:97: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 06:31:40] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:31:40] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:31:40] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:31:40] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:31:40] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 06:31:40] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:31:40] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:31:40] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:31:40] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 06:31:44] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:31:44] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:31:44] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:31:44] INFO kernels.py:97: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 06:31:44] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:31:44] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:31:44] INFO kernels.py:97: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 06:31:44] INFO kernels.py:249: FP8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:31:44] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:31:44] INFO kernels.py:373: INT8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:31:44] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:31:44] INFO kernels.py:97: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 06:31:44] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:31:44] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:31:44] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:31:44] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:31:44] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 06:31:44] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:31:44] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:31:44] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:31:44] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1109913) [2026-01-25 06:31:45] INFO SlideSparseLinearMethod_FP8.py:730: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1109913) [2026-01-25 06:31:45] INFO gemm_wrapper.py:872: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1109913) [2026-01-25 06:31:45] INFO SlideSparseLinearMethod_FP8.py:388: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1109913) [2026-01-25 06:31:45] INFO SlideSparseLinearMethod_FP8.py:535: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:12, expand_ratio=1.200
(EngineCore_DP0 pid=1109913) [2026-01-25 06:31:45] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
(EngineCore_DP0 pid=1109913) [2026-01-25 06:31:45] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8-SlideSparse-2_12
(EngineCore_DP0 pid=1109913) [2026-01-25 06:31:45] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
(EngineCore_DP0 pid=1109913) [2026-01-25 06:31:45] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8-SlideSparse-2_12
(EngineCore_DP0 pid=1109913) [2026-01-25 06:31:45] INFO SlideSparseLinearMethod_FP8.py:552: Preloaded FP8 Triton kernels for model: Qwen2.5-0.5B-FP8-SlideSparse-2_12
(EngineCore_DP0 pid=1109913) [2026-01-25 06:31:45] INFO SlideSparseLinearMethod_FP8.py:556: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=1109913) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1109913) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 10.58it/s]
(EngineCore_DP0 pid=1109913) 
(EngineCore_DP0 pid=1109913) [2026-01-25 06:31:45] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [1152, 1504] -> 1D uint8
(EngineCore_DP0 pid=1109913) [2026-01-25 06:31:45] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 1087488 bytes
(EngineCore_DP0 pid=1109913) [2026-01-25 06:31:45] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [896, 1504] -> 1D uint8
(EngineCore_DP0 pid=1109913) [2026-01-25 06:31:45] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 845824 bytes
(EngineCore_DP0 pid=1109913) [2026-01-25 06:31:45] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [9728, 1504] -> 1D uint8
(EngineCore_DP0 pid=1109913) [2026-01-25 06:31:45] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 9183232 bytes
(EngineCore_DP0 pid=1109913) [2026-01-25 06:31:45] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [896, 8128] -> 1D uint8
(EngineCore_DP0 pid=1109913) [2026-01-25 06:31:45] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 4558848 bytes
(EngineCore_DP0 pid=1109913) [2026-01-25 06:31:51] WARNING gemm_wrapper.py:393: No cuSPARSELt config for model 'Qwen2.5-0.5B-FP8', using default algorithm
(EngineCore_DP0 pid=1109913) 2026-01-25 06:31:54,120 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1109913) 2026-01-25 06:31:54,133 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1109913) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  9.02it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  5.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  5.58it/s]
(EngineCore_DP0 pid=1109913) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.66it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.66it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 6833.90it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:23,  5.45it/s, est. speed input: 87.18 toks/s, output: 5.45 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:03, 37.82it/s, est. speed input: 505.12 toks/s, output: 31.57 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:00<00:01, 55.40it/s, est. speed input: 726.46 toks/s, output: 45.40 toks/s]
Processed prompts:  21%|██        | 27/128 [00:00<00:01, 65.16it/s, est. speed input: 853.74 toks/s, output: 53.36 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:00<00:01, 70.37it/s, est. speed input: 931.51 toks/s, output: 58.22 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:00<00:01, 74.01it/s, est. speed input: 987.87 toks/s, output: 61.74 toks/s]
Processed prompts:  42%|████▏     | 54/128 [00:00<00:00, 76.00it/s, est. speed input: 1027.23 toks/s, output: 64.20 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:00<00:00, 77.62it/s, est. speed input: 1058.86 toks/s, output: 66.18 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:01<00:00, 78.58it/s, est. speed input: 1083.24 toks/s, output: 67.70 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:01<00:00, 79.58it/s, est. speed input: 1104.48 toks/s, output: 69.03 toks/s]
Processed prompts:  70%|███████   | 90/128 [00:01<00:00, 79.50it/s, est. speed input: 1118.99 toks/s, output: 69.94 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:01<00:00, 79.69it/s, est. speed input: 1132.09 toks/s, output: 70.76 toks/s]
Processed prompts:  84%|████████▍ | 108/128 [00:01<00:00, 80.14it/s, est. speed input: 1144.33 toks/s, output: 71.52 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:01<00:00, 80.53it/s, est. speed input: 1155.14 toks/s, output: 72.20 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:01<00:00, 80.30it/s, est. speed input: 1163.06 toks/s, output: 72.69 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 80.30it/s, est. speed input: 1164.79 toks/s, output: 72.80 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 72.80it/s, est. speed input: 1164.79 toks/s, output: 72.80 toks/s]
[rank0]:[W125 06:31:57.322831946 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 21.7s

测试结果:
  Requests/s:   72.00
  Tokens/s:     1223.92
  Total Reqs:   128
  Elapsed:      1.78s

  [Prefill 分析]
  Total Prefill Tokens: 2048
  Prefill Tokens/s:     1151.93

============================================================
[2/3] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-0.5B-FP8                                │
│ Backend:  cuSPARSELt (2:12)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 128 (= 1 x 128)
│   M_decode      = 1
│   batched_tokens = 129 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 128
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 129
│   --max-num-batched-tokens = 129
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 06:32:02 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1110315) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1110315) WARNING 01-25 06:32:10 [backends.py:609] Failed to read file <frozen os>
Throughput: 68.77 requests/s, 8870.89 total tokens/s, 68.77 output tokens/s
Total num prompt tokens:  16384
Total num output tokens:  128


─── STDERR ───
[2026-01-25 06:32:02] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:32:02] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:32:02] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:32:02] INFO kernels.py:97: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 06:32:02] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:32:02] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:32:02] INFO kernels.py:97: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 06:32:02] INFO kernels.py:249: FP8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:32:02] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:32:02] INFO kernels.py:373: INT8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:32:02] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:32:02] INFO kernels.py:97: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 06:32:02] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:32:02] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:32:02] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:32:02] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:32:02] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 06:32:02] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:32:02] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:32:02] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:32:02] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 06:32:06] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:32:06] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:32:06] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:32:06] INFO kernels.py:97: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 06:32:06] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:32:06] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:32:06] INFO kernels.py:97: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 06:32:06] INFO kernels.py:249: FP8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:32:06] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:32:06] INFO kernels.py:373: INT8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:32:06] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:32:06] INFO kernels.py:97: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 06:32:06] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:32:06] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:32:06] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:32:06] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:32:06] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 06:32:06] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:32:06] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:32:06] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:32:06] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1110315) [2026-01-25 06:32:07] INFO SlideSparseLinearMethod_FP8.py:730: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1110315) [2026-01-25 06:32:07] INFO gemm_wrapper.py:872: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1110315) [2026-01-25 06:32:07] INFO SlideSparseLinearMethod_FP8.py:388: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1110315) [2026-01-25 06:32:07] INFO SlideSparseLinearMethod_FP8.py:535: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:12, expand_ratio=1.200
(EngineCore_DP0 pid=1110315) [2026-01-25 06:32:07] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
(EngineCore_DP0 pid=1110315) [2026-01-25 06:32:07] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8-SlideSparse-2_12
(EngineCore_DP0 pid=1110315) [2026-01-25 06:32:07] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
(EngineCore_DP0 pid=1110315) [2026-01-25 06:32:07] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8-SlideSparse-2_12
(EngineCore_DP0 pid=1110315) [2026-01-25 06:32:07] INFO SlideSparseLinearMethod_FP8.py:552: Preloaded FP8 Triton kernels for model: Qwen2.5-0.5B-FP8-SlideSparse-2_12
(EngineCore_DP0 pid=1110315) [2026-01-25 06:32:07] INFO SlideSparseLinearMethod_FP8.py:556: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=1110315) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1110315) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 10.29it/s]
(EngineCore_DP0 pid=1110315) 
(EngineCore_DP0 pid=1110315) [2026-01-25 06:32:07] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [1152, 1504] -> 1D uint8
(EngineCore_DP0 pid=1110315) [2026-01-25 06:32:07] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 1087488 bytes
(EngineCore_DP0 pid=1110315) [2026-01-25 06:32:07] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [896, 1504] -> 1D uint8
(EngineCore_DP0 pid=1110315) [2026-01-25 06:32:07] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 845824 bytes
(EngineCore_DP0 pid=1110315) [2026-01-25 06:32:07] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [9728, 1504] -> 1D uint8
(EngineCore_DP0 pid=1110315) [2026-01-25 06:32:07] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 9183232 bytes
(EngineCore_DP0 pid=1110315) [2026-01-25 06:32:07] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [896, 8128] -> 1D uint8
(EngineCore_DP0 pid=1110315) [2026-01-25 06:32:07] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 4558848 bytes
(EngineCore_DP0 pid=1110315) [2026-01-25 06:32:13] WARNING gemm_wrapper.py:393: No cuSPARSELt config for model 'Qwen2.5-0.5B-FP8', using default algorithm
(EngineCore_DP0 pid=1110315) 2026-01-25 06:32:15,750 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1110315) 2026-01-25 06:32:15,763 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1110315) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 27.11it/s]
(EngineCore_DP0 pid=1110315) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.79it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.78it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 2808.84it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:23,  5.41it/s, est. speed input: 692.23 toks/s, output: 5.41 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:03, 37.52it/s, est. speed input: 4009.29 toks/s, output: 31.32 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:02, 53.68it/s, est. speed input: 5615.04 toks/s, output: 43.87 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:00<00:01, 62.80it/s, est. speed input: 6554.13 toks/s, output: 51.20 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:00<00:01, 68.25it/s, est. speed input: 7165.11 toks/s, output: 55.98 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:00<00:01, 71.06it/s, est. speed input: 7564.55 toks/s, output: 59.09 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:00<00:01, 69.50it/s, est. speed input: 7706.50 toks/s, output: 60.21 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:00<00:00, 71.76it/s, est. speed input: 7949.22 toks/s, output: 62.10 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:01<00:00, 73.80it/s, est. speed input: 8160.13 toks/s, output: 63.75 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:01<00:00, 75.41it/s, est. speed input: 8338.55 toks/s, output: 65.14 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:01<00:00, 76.07it/s, est. speed input: 8472.72 toks/s, output: 66.19 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:01<00:00, 76.97it/s, est. speed input: 8598.91 toks/s, output: 67.18 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:01<00:00, 77.76it/s, est. speed input: 8711.18 toks/s, output: 68.06 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:01<00:00, 78.35it/s, est. speed input: 8809.59 toks/s, output: 68.82 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:01<00:00, 78.72it/s, est. speed input: 8894.61 toks/s, output: 69.49 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:01<00:00, 78.75it/s, est. speed input: 8964.91 toks/s, output: 70.04 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 78.75it/s, est. speed input: 9027.68 toks/s, output: 70.53 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 70.53it/s, est. speed input: 9027.68 toks/s, output: 70.53 toks/s]
[rank0]:[W125 06:32:19.971329240 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 21.7s

测试结果:
  Requests/s:   68.77
  Tokens/s:     8870.89
  Total Reqs:   128
  Elapsed:      1.86s

  [Prefill 分析]
  Total Prefill Tokens: 16384
  Prefill Tokens/s:     8802.12

============================================================
[3/3] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-0.5B-FP8                                │
│ Backend:  cuSPARSELt (2:12)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 256 (= 1 x 256)
│   M_decode      = 1
│   batched_tokens = 257 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 256
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 257
│   --max-num-batched-tokens = 257
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 06:32:24 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1110745) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1110745) WARNING 01-25 06:32:32 [backends.py:609] Failed to read file <frozen os>
Throughput: 74.63 requests/s, 19180.46 total tokens/s, 74.63 output tokens/s
Total num prompt tokens:  32768
Total num output tokens:  128


─── STDERR ───
[2026-01-25 06:32:24] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:32:24] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:32:24] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:32:24] INFO kernels.py:97: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 06:32:24] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:32:24] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:32:24] INFO kernels.py:97: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 06:32:24] INFO kernels.py:249: FP8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:32:24] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:32:24] INFO kernels.py:373: INT8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:32:24] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:32:24] INFO kernels.py:97: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 06:32:24] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:32:24] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:32:24] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:32:24] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:32:24] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 06:32:24] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:32:24] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:32:24] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:32:24] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 06:32:28] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:32:28] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:32:28] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:32:28] INFO kernels.py:97: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 06:32:28] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:32:28] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:32:28] INFO kernels.py:97: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 06:32:28] INFO kernels.py:249: FP8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:32:28] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:32:28] INFO kernels.py:373: INT8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:32:28] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:32:28] INFO kernels.py:97: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 06:32:28] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:32:28] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:32:28] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:32:28] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:32:28] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 06:32:28] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:32:28] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:32:28] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:32:28] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1110745) [2026-01-25 06:32:28] INFO SlideSparseLinearMethod_FP8.py:730: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1110745) [2026-01-25 06:32:28] INFO gemm_wrapper.py:872: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1110745) [2026-01-25 06:32:28] INFO SlideSparseLinearMethod_FP8.py:388: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1110745) [2026-01-25 06:32:28] INFO SlideSparseLinearMethod_FP8.py:535: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:12, expand_ratio=1.200
(EngineCore_DP0 pid=1110745) [2026-01-25 06:32:28] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
(EngineCore_DP0 pid=1110745) [2026-01-25 06:32:28] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8-SlideSparse-2_12
(EngineCore_DP0 pid=1110745) [2026-01-25 06:32:28] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
(EngineCore_DP0 pid=1110745) [2026-01-25 06:32:28] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8-SlideSparse-2_12
(EngineCore_DP0 pid=1110745) [2026-01-25 06:32:28] INFO SlideSparseLinearMethod_FP8.py:552: Preloaded FP8 Triton kernels for model: Qwen2.5-0.5B-FP8-SlideSparse-2_12
(EngineCore_DP0 pid=1110745) [2026-01-25 06:32:28] INFO SlideSparseLinearMethod_FP8.py:556: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=1110745) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1110745) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 10.59it/s]
(EngineCore_DP0 pid=1110745) 
(EngineCore_DP0 pid=1110745) [2026-01-25 06:32:29] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [1152, 1504] -> 1D uint8
(EngineCore_DP0 pid=1110745) [2026-01-25 06:32:29] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 1087488 bytes
(EngineCore_DP0 pid=1110745) [2026-01-25 06:32:29] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [896, 1504] -> 1D uint8
(EngineCore_DP0 pid=1110745) [2026-01-25 06:32:29] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 845824 bytes
(EngineCore_DP0 pid=1110745) [2026-01-25 06:32:29] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [9728, 1504] -> 1D uint8
(EngineCore_DP0 pid=1110745) [2026-01-25 06:32:29] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 9183232 bytes
(EngineCore_DP0 pid=1110745) [2026-01-25 06:32:29] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [896, 8128] -> 1D uint8
(EngineCore_DP0 pid=1110745) [2026-01-25 06:32:29] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 4558848 bytes
(EngineCore_DP0 pid=1110745) [2026-01-25 06:32:35] WARNING gemm_wrapper.py:393: No cuSPARSELt config for model 'Qwen2.5-0.5B-FP8', using default algorithm
(EngineCore_DP0 pid=1110745) 2026-01-25 06:32:37,426 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1110745) 2026-01-25 06:32:37,440 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1110745) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 26.79it/s]
(EngineCore_DP0 pid=1110745) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.88it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00,  6.88it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 1443.74it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:01, 88.68it/s, est. speed input: 22703.77 toks/s, output: 88.68 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:00<00:01, 81.29it/s, est. speed input: 21073.78 toks/s, output: 82.32 toks/s]
Processed prompts:  21%|██        | 27/128 [00:00<00:01, 80.23it/s, est. speed input: 20782.90 toks/s, output: 81.18 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:00<00:01, 79.89it/s, est. speed input: 20664.92 toks/s, output: 80.72 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:00<00:01, 79.60it/s, est. speed input: 20579.51 toks/s, output: 80.39 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:00<00:00, 79.47it/s, est. speed input: 20533.24 toks/s, output: 80.21 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:00<00:00, 78.45it/s, est. speed input: 20395.17 toks/s, output: 79.67 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:00<00:00, 78.25it/s, est. speed input: 20338.41 toks/s, output: 79.45 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:00<00:00, 78.07it/s, est. speed input: 20290.00 toks/s, output: 79.26 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:01<00:00, 78.13it/s, est. speed input: 20265.51 toks/s, output: 79.16 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:01<00:00, 77.82it/s, est. speed input: 20219.31 toks/s, output: 78.98 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:01<00:00, 78.01it/s, est. speed input: 20208.80 toks/s, output: 78.94 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:01<00:00, 77.93it/s, est. speed input: 20185.59 toks/s, output: 78.85 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:01<00:00, 78.13it/s, est. speed input: 20181.63 toks/s, output: 78.83 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:01<00:00, 78.08it/s, est. speed input: 20167.42 toks/s, output: 78.78 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 78.08it/s, est. speed input: 20169.17 toks/s, output: 78.79 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 78.78it/s, est. speed input: 20169.17 toks/s, output: 78.79 toks/s]
[rank0]:[W125 06:32:40.280585948 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 21.3s

测试结果:
  Requests/s:   74.63
  Tokens/s:     19180.46
  Total Reqs:   128
  Elapsed:      1.72s

  [Prefill 分析]
  Total Prefill Tokens: 32768
  Prefill Tokens/s:     19105.83


------------------------------------------------------------
  生成 CSV: Qwen2.5-0.5B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_12/Qwen2.5-0.5B-FP8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
16,16,1,128,128,71.9955,1223.9243,1.7779
128,128,1,128,128,68.7666,8870.8894,1.8614
256,256,1,128,128,74.6322,19180.4649,1.7151

------------------------------------------------------------

[INFO] 完成: 3 成功, 0 失败

============================================================
  Qwen2.5-0.5B-FP8 | cuSPARSELt (2_12) | decode
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-0.5B-FP8-SlideSparse-2_12
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_12

============================================================
[1/3] 测试 M=16
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-0.5B-FP8                                │
│ Backend:  cuSPARSELt (2:12)                               │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16
│   M_prefill     = 256 (= 16 x 16)
│   M_decode      = 16
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 16
│   --max-num-seqs           = 16
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 06:32:45 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1111169) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1111169) WARNING 01-25 06:32:53 [backends.py:609] Failed to read file <frozen os>
Throughput: 14.54 requests/s, 3954.55 total tokens/s, 3721.93 output tokens/s
Total num prompt tokens:  256
Total num output tokens:  4096


─── STDERR ───
[2026-01-25 06:32:45] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:32:45] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:32:45] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:32:45] INFO kernels.py:97: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 06:32:45] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:32:45] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:32:45] INFO kernels.py:97: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 06:32:45] INFO kernels.py:249: FP8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:32:45] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:32:45] INFO kernels.py:373: INT8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:32:45] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:32:45] INFO kernels.py:97: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 06:32:45] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:32:45] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:32:45] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:32:45] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:32:45] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 06:32:45] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:32:45] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:32:45] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:32:45] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 06:32:49] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:32:49] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:32:49] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:32:49] INFO kernels.py:97: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 06:32:49] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:32:49] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:32:49] INFO kernels.py:97: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 06:32:49] INFO kernels.py:249: FP8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:32:49] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:32:49] INFO kernels.py:373: INT8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:32:49] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:32:49] INFO kernels.py:97: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 06:32:49] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:32:49] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:32:49] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:32:49] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:32:49] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 06:32:49] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:32:49] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:32:49] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:32:49] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1111169) [2026-01-25 06:32:49] INFO SlideSparseLinearMethod_FP8.py:730: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1111169) [2026-01-25 06:32:49] INFO gemm_wrapper.py:872: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1111169) [2026-01-25 06:32:49] INFO SlideSparseLinearMethod_FP8.py:388: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1111169) [2026-01-25 06:32:49] INFO SlideSparseLinearMethod_FP8.py:535: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:12, expand_ratio=1.200
(EngineCore_DP0 pid=1111169) [2026-01-25 06:32:49] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
(EngineCore_DP0 pid=1111169) [2026-01-25 06:32:49] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8-SlideSparse-2_12
(EngineCore_DP0 pid=1111169) [2026-01-25 06:32:49] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
(EngineCore_DP0 pid=1111169) [2026-01-25 06:32:49] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8-SlideSparse-2_12
(EngineCore_DP0 pid=1111169) [2026-01-25 06:32:49] INFO SlideSparseLinearMethod_FP8.py:552: Preloaded FP8 Triton kernels for model: Qwen2.5-0.5B-FP8-SlideSparse-2_12
(EngineCore_DP0 pid=1111169) [2026-01-25 06:32:49] INFO SlideSparseLinearMethod_FP8.py:556: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=1111169) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1111169) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 10.52it/s]
(EngineCore_DP0 pid=1111169) 
(EngineCore_DP0 pid=1111169) [2026-01-25 06:32:50] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [1152, 1504] -> 1D uint8
(EngineCore_DP0 pid=1111169) [2026-01-25 06:32:50] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 1087488 bytes
(EngineCore_DP0 pid=1111169) [2026-01-25 06:32:50] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [896, 1504] -> 1D uint8
(EngineCore_DP0 pid=1111169) [2026-01-25 06:32:50] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 845824 bytes
(EngineCore_DP0 pid=1111169) [2026-01-25 06:32:50] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [9728, 1504] -> 1D uint8
(EngineCore_DP0 pid=1111169) [2026-01-25 06:32:50] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 9183232 bytes
(EngineCore_DP0 pid=1111169) [2026-01-25 06:32:50] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [896, 8128] -> 1D uint8
(EngineCore_DP0 pid=1111169) [2026-01-25 06:32:50] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 4558848 bytes
(EngineCore_DP0 pid=1111169) [2026-01-25 06:32:56] WARNING gemm_wrapper.py:393: No cuSPARSELt config for model 'Qwen2.5-0.5B-FP8', using default algorithm
(EngineCore_DP0 pid=1111169) 2026-01-25 06:32:58,399 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1111169) 2026-01-25 06:32:58,412 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1111169) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 3/7 [00:00<00:00, 27.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 29.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 29.21it/s]
(EngineCore_DP0 pid=1111169) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  20%|██        | 1/5 [00:00<00:00,  6.88it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 22.03it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 19.45it/s]

Adding requests:   0%|          | 0/16 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 16/16 [00:00<00:00, 2237.11it/s]

Processed prompts:   0%|          | 0/16 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   6%|▋         | 1/16 [00:01<00:16,  1.09s/it, est. speed input: 14.72 toks/s, output: 235.44 toks/s]
Processed prompts: 100%|██████████| 16/16 [00:01<00:00,  1.09s/it, est. speed input: 234.53 toks/s, output: 3752.43 toks/s]
Processed prompts: 100%|██████████| 16/16 [00:01<00:00, 14.66it/s, est. speed input: 234.53 toks/s, output: 3752.43 toks/s]
[rank0]:[W125 06:33:01.925573232 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 20.7s

测试结果:
  Requests/s:   14.54
  Tokens/s:     3954.55
  Total Reqs:   16
  Elapsed:      1.10s

  [Decode 分析]
  Total Decode Tokens:  4096
  Decode Tokens/s:      3721.93

============================================================
[2/3] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-0.5B-FP8                                │
│ Backend:  cuSPARSELt (2:12)                               │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 2048 (= 128 x 16)
│   M_decode      = 128
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 128
│   --max-num-seqs           = 128
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 06:33:06 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1111535) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1111535) WARNING 01-25 06:33:14 [backends.py:609] Failed to read file <frozen os>
Throughput: 76.41 requests/s, 20782.99 total tokens/s, 19560.46 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768


─── STDERR ───
[2026-01-25 06:33:06] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:33:06] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:33:06] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:33:06] INFO kernels.py:97: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 06:33:06] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:33:06] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:33:06] INFO kernels.py:97: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 06:33:06] INFO kernels.py:249: FP8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:33:06] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:33:06] INFO kernels.py:373: INT8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:33:06] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:33:06] INFO kernels.py:97: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 06:33:06] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:33:06] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:33:06] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:33:06] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:33:06] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 06:33:06] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:33:06] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:33:06] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:33:06] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 06:33:10] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:33:10] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:33:10] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:33:10] INFO kernels.py:97: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 06:33:10] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:33:10] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:33:10] INFO kernels.py:97: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 06:33:10] INFO kernels.py:249: FP8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:33:10] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:33:10] INFO kernels.py:373: INT8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:33:10] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:33:10] INFO kernels.py:97: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 06:33:10] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:33:10] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:33:10] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:33:10] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:33:10] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 06:33:10] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:33:10] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:33:10] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:33:10] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1111535) [2026-01-25 06:33:10] INFO SlideSparseLinearMethod_FP8.py:730: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1111535) [2026-01-25 06:33:10] INFO gemm_wrapper.py:872: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1111535) [2026-01-25 06:33:10] INFO SlideSparseLinearMethod_FP8.py:388: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1111535) [2026-01-25 06:33:10] INFO SlideSparseLinearMethod_FP8.py:535: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:12, expand_ratio=1.200
(EngineCore_DP0 pid=1111535) [2026-01-25 06:33:10] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
(EngineCore_DP0 pid=1111535) [2026-01-25 06:33:10] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8-SlideSparse-2_12
(EngineCore_DP0 pid=1111535) [2026-01-25 06:33:10] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
(EngineCore_DP0 pid=1111535) [2026-01-25 06:33:10] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8-SlideSparse-2_12
(EngineCore_DP0 pid=1111535) [2026-01-25 06:33:10] INFO SlideSparseLinearMethod_FP8.py:552: Preloaded FP8 Triton kernels for model: Qwen2.5-0.5B-FP8-SlideSparse-2_12
(EngineCore_DP0 pid=1111535) [2026-01-25 06:33:10] INFO SlideSparseLinearMethod_FP8.py:556: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=1111535) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1111535) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 10.62it/s]
(EngineCore_DP0 pid=1111535) 
(EngineCore_DP0 pid=1111535) [2026-01-25 06:33:10] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [1152, 1504] -> 1D uint8
(EngineCore_DP0 pid=1111535) [2026-01-25 06:33:11] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 1087488 bytes
(EngineCore_DP0 pid=1111535) [2026-01-25 06:33:11] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [896, 1504] -> 1D uint8
(EngineCore_DP0 pid=1111535) [2026-01-25 06:33:11] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 845824 bytes
(EngineCore_DP0 pid=1111535) [2026-01-25 06:33:11] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [9728, 1504] -> 1D uint8
(EngineCore_DP0 pid=1111535) [2026-01-25 06:33:11] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 9183232 bytes
(EngineCore_DP0 pid=1111535) [2026-01-25 06:33:11] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [896, 8128] -> 1D uint8
(EngineCore_DP0 pid=1111535) [2026-01-25 06:33:11] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 4558848 bytes
(EngineCore_DP0 pid=1111535) [2026-01-25 06:33:16] WARNING gemm_wrapper.py:393: No cuSPARSELt config for model 'Qwen2.5-0.5B-FP8', using default algorithm
(EngineCore_DP0 pid=1111535) 2026-01-25 06:33:17,952 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1111535) 2026-01-25 06:33:17,965 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1111535) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   6%|▌         | 2/35 [00:00<00:02, 12.00it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█▏        | 4/35 [00:00<00:02, 12.39it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 7/35 [00:00<00:01, 18.34it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|███▏      | 11/35 [00:00<00:01, 23.19it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 15/35 [00:00<00:00, 25.99it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  54%|█████▍    | 19/35 [00:00<00:00, 27.44it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  66%|██████▌   | 23/35 [00:00<00:00, 28.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  77%|███████▋  | 27/35 [00:01<00:00, 29.11it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▊ | 31/35 [00:01<00:00, 29.73it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:01<00:00, 30.11it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 35/35 [00:01<00:00, 26.21it/s]
(EngineCore_DP0 pid=1111535) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   5%|▌         | 1/19 [00:00<00:02,  6.91it/s]
Capturing CUDA graphs (decode, FULL):  26%|██▋       | 5/19 [00:00<00:00, 22.26it/s]
Capturing CUDA graphs (decode, FULL):  47%|████▋     | 9/19 [00:00<00:00, 28.33it/s]
Capturing CUDA graphs (decode, FULL):  68%|██████▊   | 13/19 [00:00<00:00, 31.60it/s]
Capturing CUDA graphs (decode, FULL):  89%|████████▉ | 17/19 [00:00<00:00, 33.40it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 19/19 [00:00<00:00, 29.92it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 8074.10it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:01<03:23,  1.60s/it, est. speed input: 9.98 toks/s, output: 159.61 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00,  1.60s/it, est. speed input: 1234.89 toks/s, output: 19758.22 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 77.18it/s, est. speed input: 1234.89 toks/s, output: 19758.22 toks/s]
[rank0]:[W125 06:33:22.625469281 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 21.6s

测试结果:
  Requests/s:   76.41
  Tokens/s:     20782.99
  Total Reqs:   128
  Elapsed:      1.68s

  [Decode 分析]
  Total Decode Tokens:  32768
  Decode Tokens/s:      19560.46

============================================================
[3/3] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-0.5B-FP8                                │
│ Backend:  cuSPARSELt (2:12)                               │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 4096 (= 256 x 16)
│   M_decode      = 256
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 256
│   --max-num-seqs           = 256
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-25 06:33:27 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=1111922) [INFO] Loading compress extension: cusparselt_compress_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1111922) WARNING 01-25 06:33:35 [backends.py:609] Failed to read file <frozen os>
Throughput: 95.27 requests/s, 25914.14 total tokens/s, 24389.78 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536


─── STDERR ───
[2026-01-25 06:33:27] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:33:27] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:33:27] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:33:27] INFO kernels.py:97: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 06:33:27] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:33:27] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:33:27] INFO kernels.py:97: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 06:33:27] INFO kernels.py:249: FP8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:33:27] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:33:27] INFO kernels.py:373: INT8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:33:27] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:33:27] INFO kernels.py:97: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 06:33:27] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:33:27] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:33:27] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:33:27] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:33:27] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 06:33:27] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:33:27] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:33:27] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:33:27] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-25 06:33:31] INFO kernels.py:603: Triton kernel custom ops registered
[2026-01-25 06:33:31] INFO kernels.py:649: Optimization enabled: Only preloading kernels for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:33:31] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:33:31] INFO kernels.py:97: Using basic kernel: basic_dequant_bias_triton.py
[2026-01-25 06:33:31] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:33:31] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:33:31] INFO kernels.py:97: Using basic kernel: basic_quant_only_triton.py
[2026-01-25 06:33:31] INFO kernels.py:249: FP8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:33:31] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:33:31] INFO kernels.py:373: INT8 quant kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:33:31] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:33:31] INFO kernels.py:97: Using basic kernel: basic_quant_slide_triton.py
[2026-01-25 06:33:31] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:33:31] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
[2026-01-25 06:33:31] INFO kernels.py:433: INT8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8
[2026-01-25 06:33:31] INFO gemm_wrapper.py:168: Optimization enabled: Only loading GEMM configs for base model 'Qwen2.5-0.5B-FP8'
[2026-01-25 06:33:31] INFO gemm_wrapper.py:183: Loaded algorithm configs: cuBLASLt=0, cuSPARSELt=0 models
[2026-01-25 06:33:31] INFO gemm_wrapper.py:946: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-25 06:33:31] INFO gemm_wrapper.py:1011: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-25 06:33:31] INFO gemm_wrapper.py:1077: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-25 06:33:31] INFO gemm_wrapper.py:1143: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=1111922) [2026-01-25 06:33:32] INFO SlideSparseLinearMethod_FP8.py:730: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=1111922) [2026-01-25 06:33:32] INFO gemm_wrapper.py:872: cusparselt GEMM extension loaded: cusparselt_gemm_RTX5080_cc120_py312_cu129_x86_64.so
(EngineCore_DP0 pid=1111922) [2026-01-25 06:33:32] INFO SlideSparseLinearMethod_FP8.py:388: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=1111922) [2026-01-25 06:33:32] INFO SlideSparseLinearMethod_FP8.py:535: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:12, expand_ratio=1.200
(EngineCore_DP0 pid=1111922) [2026-01-25 06:33:32] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
(EngineCore_DP0 pid=1111922) [2026-01-25 06:33:32] INFO kernels.py:180: Dequant+bias kernel loaded for model: Qwen2.5-0.5B-FP8-SlideSparse-2_12
(EngineCore_DP0 pid=1111922) [2026-01-25 06:33:32] WARNING kernels.py:140: Tuned kernel not found for Qwen2.5-0.5B-FP8, using basic kernel
(EngineCore_DP0 pid=1111922) [2026-01-25 06:33:32] INFO kernels.py:309: FP8 quant+slide kernel loaded for model: Qwen2.5-0.5B-FP8-SlideSparse-2_12
(EngineCore_DP0 pid=1111922) [2026-01-25 06:33:32] INFO SlideSparseLinearMethod_FP8.py:552: Preloaded FP8 Triton kernels for model: Qwen2.5-0.5B-FP8-SlideSparse-2_12
(EngineCore_DP0 pid=1111922) [2026-01-25 06:33:32] INFO SlideSparseLinearMethod_FP8.py:556: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=1111922) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=1111922) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00, 10.48it/s]
(EngineCore_DP0 pid=1111922) 
(EngineCore_DP0 pid=1111922) [2026-01-25 06:33:32] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [1152, 1504] -> 1D uint8
(EngineCore_DP0 pid=1111922) [2026-01-25 06:33:32] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 1087488 bytes
(EngineCore_DP0 pid=1111922) [2026-01-25 06:33:32] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [896, 1504] -> 1D uint8
(EngineCore_DP0 pid=1111922) [2026-01-25 06:33:32] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 845824 bytes
(EngineCore_DP0 pid=1111922) [2026-01-25 06:33:32] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [9728, 1504] -> 1D uint8
(EngineCore_DP0 pid=1111922) [2026-01-25 06:33:32] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 9183232 bytes
(EngineCore_DP0 pid=1111922) [2026-01-25 06:33:32] INFO SlideSparseLinearMethod_FP8.py:656: cuSPARSELt compression: [896, 8128] -> 1D uint8
(EngineCore_DP0 pid=1111922) [2026-01-25 06:33:32] INFO SlideSparseLinearMethod_FP8.py:666: cuSPARSELt compression done: 4558848 bytes
(EngineCore_DP0 pid=1111922) [2026-01-25 06:33:37] WARNING gemm_wrapper.py:393: No cuSPARSELt config for model 'Qwen2.5-0.5B-FP8', using default algorithm
(EngineCore_DP0 pid=1111922) 2026-01-25 06:33:39,489 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=1111922) 2026-01-25 06:33:39,503 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=1111922) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/36 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   8%|▊         | 3/36 [00:00<00:01, 29.41it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  17%|█▋        | 6/36 [00:00<00:01, 29.41it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  28%|██▊       | 10/36 [00:00<00:00, 30.06it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  39%|███▉      | 14/36 [00:00<00:00, 30.41it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 18/36 [00:00<00:00, 30.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  61%|██████    | 22/36 [00:00<00:00, 30.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  72%|███████▏  | 26/36 [00:00<00:00, 30.37it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  83%|████████▎ | 30/36 [00:00<00:00, 30.60it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|█████████▍| 34/36 [00:01<00:00, 31.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 36/36 [00:01<00:00, 30.44it/s]
(EngineCore_DP0 pid=1111922) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/35 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):   3%|▎         | 1/35 [00:00<00:04,  6.83it/s]
Capturing CUDA graphs (decode, FULL):  14%|█▍        | 5/35 [00:00<00:01, 22.10it/s]
Capturing CUDA graphs (decode, FULL):  26%|██▌       | 9/35 [00:00<00:00, 28.02it/s]
Capturing CUDA graphs (decode, FULL):  37%|███▋      | 13/35 [00:00<00:00, 31.00it/s]
Capturing CUDA graphs (decode, FULL):  49%|████▊     | 17/35 [00:00<00:00, 32.76it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 21/35 [00:00<00:00, 33.98it/s]
Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 25/35 [00:00<00:00, 34.73it/s]
Capturing CUDA graphs (decode, FULL):  83%|████████▎ | 29/35 [00:00<00:00, 35.17it/s]
Capturing CUDA graphs (decode, FULL):  94%|█████████▍| 33/35 [00:01<00:00, 35.72it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:01<00:00, 32.29it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 8384.74it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:02<10:07,  2.38s/it, est. speed input: 6.72 toks/s, output: 107.50 toks/s]
Processed prompts:  56%|█████▋    | 144/256 [00:02<00:01, 81.19it/s, est. speed input: 927.44 toks/s, output: 14838.94 toks/s]
Processed prompts:  91%|█████████▏| 234/256 [00:02<00:00, 142.32it/s, est. speed input: 1448.62 toks/s, output: 23177.91 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 142.32it/s, est. speed input: 1542.42 toks/s, output: 24678.71 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 96.40it/s, est. speed input: 1542.42 toks/s, output: 24678.71 toks/s] 
[rank0]:[W125 06:33:45.506433810 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 22.8s

测试结果:
  Requests/s:   95.27
  Tokens/s:     25914.14
  Total Reqs:   256
  Elapsed:      2.69s

  [Decode 分析]
  Total Decode Tokens:  65536
  Decode Tokens/s:      24389.78


------------------------------------------------------------
  生成 CSV: Qwen2.5-0.5B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/RTX5080_cc120_FP8E4M3_py312_cu129_x86_64/cusparselt/2_12/Qwen2.5-0.5B-FP8_decode.csv

预览:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
16,16,16,16,256,256,14.5388,3954.5485,1.1005
128,16,128,128,256,256,76.4081,20782.9940,1.6752
256,16,256,256,256,256,95.2726,25914.1414,2.6870

------------------------------------------------------------

[INFO] 完成: 3 成功, 0 失败


============================================================
  Benchmark 完成!
============================================================


总计: 42 成功, 0 失败
============================================================
