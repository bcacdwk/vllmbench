======================================================================
SlideSparse vLLM Throughput Benchmark Log
Created: 2026-01-27 17:33:44
======================================================================

原始命令:
  /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model llama3.2-3b-fp8 --backend cublaslt,cusparselt --stage decode --sparsity 2_4,2_6,2_8,2_10 --M 64,128,256,512

命令行参数:
  --model: llama3.2-3b-fp8
  --backend: cublaslt,cusparselt
  --sparsity: 2_4,2_6,2_8,2_10
  --stage: decode
  --M: 64,128,256,512
  --N: None
  --inner-32: False
  --eager: False
  --gpu-id: 0
  --gpu-mem: 0.8
  --dry-run: False
  --list-models: False

硬件信息:
  GPU: GB10
  Compute Capability: cc121
  VRAM: 119.7 GB
  CUDA: 12.9
  Python: py312

Backend 环境变量 (初始状态):
  DISABLE_SLIDESPARSE: 未设置
  USE_CUBLASLT: 未设置
  USE_CUSPARSELT: 未设置
  SPARSITY: 未设置
  INNER_DTYPE_32: 未设置

======================================================================


============================================================
  Llama3.2-3B-FP8 | cuBLASLt | decode
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints/Llama3.2-3B-FP8
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cublaslt

============================================================
[1/4] 测试 M=64
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-3B-FP8                                 │
│ Backend:  cuBLASLt                                        │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 64
│   M_prefill     = 1024 (= 64 x 16)
│   M_decode      = 64
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 64
│   --max-num-seqs           = 64
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:33:47 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:33:47 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2876337) WARNING 01-27 17:34:15 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 7.75 requests/s, 2107.83 total tokens/s, 1983.84 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384


─── STDERR ───
[2026-01-27 17:33:47] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:33:47] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-27 17:33:47] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-27 17:33:47] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:33:47] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:33:47] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:33:47] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:33:47] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:33:47] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-27 17:33:47] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:33:47] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:33:47] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:33:47] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:33:47] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:33:51] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:33:51] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-27 17:33:51] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-27 17:33:51] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:33:51] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:33:51] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:33:51] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:33:51] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:33:51] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-27 17:33:51] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:33:51] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:33:51] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:33:51] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:33:51] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2876337) [2026-01-27 17:33:52] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=2876337) [2026-01-27 17:33:52] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2876337) [2026-01-27 17:33:52] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=2876337) [2026-01-27 17:33:52] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=2876337) [2026-01-27 17:33:52] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=2876337) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2876337) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:16<00:00, 16.09s/it]
(EngineCore_DP0 pid=2876337) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:16<00:00, 16.09s/it]
(EngineCore_DP0 pid=2876337) 
(EngineCore_DP0 pid=2876337) 2026-01-27 17:34:14,563 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2876337) 2026-01-27 17:34:14,583 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 64/64 [00:00<00:00, 11840.48it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 1/64 [00:08<08:30,  8.11s/it, est. speed input: 1.97 toks/s, output: 31.56 toks/s]
Processed prompts:  78%|███████▊  | 50/64 [00:08<00:01,  8.57it/s, est. speed input: 97.20 toks/s, output: 1555.17 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:08<00:00,  8.57it/s, est. speed input: 124.09 toks/s, output: 1985.36 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:08<00:00,  7.76it/s, est. speed input: 124.09 toks/s, output: 1985.36 toks/s]
[rank0]:[W127 17:34:23.696517299 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 41.7s

测试结果:
  Requests/s:   7.75
  Tokens/s:     2107.83
  Total Reqs:   64
  Elapsed:      8.26s

  [Decode 分析]
  Total Decode Tokens:  16384
  Decode Tokens/s:      1983.84

============================================================
[2/4] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-3B-FP8                                 │
│ Backend:  cuBLASLt                                        │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 2048 (= 128 x 16)
│   M_decode      = 128
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 128
│   --max-num-seqs           = 128
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:34:29 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:34:29 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2877092) WARNING 01-27 17:34:55 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 11.58 requests/s, 3149.69 total tokens/s, 2964.41 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768


─── STDERR ───
[2026-01-27 17:34:29] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:34:29] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-27 17:34:29] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-27 17:34:29] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:34:29] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:34:29] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:34:29] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:34:29] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:34:29] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-27 17:34:29] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:34:29] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:34:29] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:34:29] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:34:29] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:34:33] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:34:33] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-27 17:34:33] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-27 17:34:33] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:34:33] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:34:33] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:34:33] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:34:33] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:34:33] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-27 17:34:33] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:34:33] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:34:33] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:34:33] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:34:33] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2877092) [2026-01-27 17:34:34] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=2877092) [2026-01-27 17:34:34] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2877092) [2026-01-27 17:34:34] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=2877092) [2026-01-27 17:34:34] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=2877092) [2026-01-27 17:34:34] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=2877092) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2877092) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:15<00:00, 15.53s/it]
(EngineCore_DP0 pid=2877092) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:15<00:00, 15.53s/it]
(EngineCore_DP0 pid=2877092) 
(EngineCore_DP0 pid=2877092) 2026-01-27 17:34:55,335 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2877092) 2026-01-27 17:34:55,350 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 12303.96it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:10<22:33, 10.66s/it, est. speed input: 1.50 toks/s, output: 24.02 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:10<00:22,  4.30it/s, est. speed input: 48.92 toks/s, output: 782.68 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:10<00:04, 11.95it/s, est. speed input: 110.04 toks/s, output: 1760.65 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:11<00:00, 22.59it/s, est. speed input: 171.34 toks/s, output: 2741.40 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:11<00:00, 22.59it/s, est. speed input: 185.47 toks/s, output: 2967.49 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:11<00:00, 11.59it/s, est. speed input: 185.47 toks/s, output: 2967.49 toks/s]
[rank0]:[W127 17:35:07.255895819 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 43.3s

测试结果:
  Requests/s:   11.58
  Tokens/s:     3149.69
  Total Reqs:   128
  Elapsed:      11.05s

  [Decode 分析]
  Total Decode Tokens:  32768
  Decode Tokens/s:      2964.41

============================================================
[3/4] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-3B-FP8                                 │
│ Backend:  cuBLASLt                                        │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 4096 (= 256 x 16)
│   M_decode      = 256
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 256
│   --max-num-seqs           = 256
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:35:13 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:35:13 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2877886) WARNING 01-27 17:35:39 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 14.78 requests/s, 4020.63 total tokens/s, 3784.12 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536


─── STDERR ───
[2026-01-27 17:35:13] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:35:13] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-27 17:35:13] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-27 17:35:13] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:35:13] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:35:13] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:35:13] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:35:13] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:35:13] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-27 17:35:13] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:35:13] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:35:13] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:35:13] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:35:13] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:35:16] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:35:16] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-27 17:35:16] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-27 17:35:16] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:35:16] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:35:16] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:35:16] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:35:16] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:35:16] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-27 17:35:16] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:35:16] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:35:16] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:35:16] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:35:16] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2877886) [2026-01-27 17:35:17] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=2877886) [2026-01-27 17:35:17] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2877886) [2026-01-27 17:35:17] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=2877886) [2026-01-27 17:35:17] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=2877886) [2026-01-27 17:35:17] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=2877886) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2877886) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:15<00:00, 15.42s/it]
(EngineCore_DP0 pid=2877886) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:15<00:00, 15.42s/it]
(EngineCore_DP0 pid=2877886) 
(EngineCore_DP0 pid=2877886) 2026-01-27 17:35:38,555 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2877886) 2026-01-27 17:35:38,570 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 4458.62it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:15<1:05:04, 15.31s/it, est. speed input: 1.04 toks/s, output: 16.72 toks/s]
Processed prompts:   7%|▋         | 18/256 [00:15<02:26,  1.63it/s, est. speed input: 18.63 toks/s, output: 298.01 toks/s]
Processed prompts:  19%|█▉        | 48/256 [00:15<00:37,  5.47it/s, est. speed input: 49.24 toks/s, output: 787.84 toks/s]
Processed prompts:  29%|██▉       | 75/256 [00:15<00:17, 10.22it/s, est. speed input: 76.33 toks/s, output: 1221.29 toks/s]
Processed prompts:  38%|███▊      | 98/256 [00:15<00:10, 15.69it/s, est. speed input: 99.02 toks/s, output: 1584.29 toks/s]
Processed prompts:  46%|████▌     | 118/256 [00:15<00:06, 22.02it/s, est. speed input: 118.45 toks/s, output: 1895.24 toks/s]
Processed prompts:  56%|█████▋    | 144/256 [00:16<00:03, 32.53it/s, est. speed input: 143.26 toks/s, output: 2292.18 toks/s]
Processed prompts:  65%|██████▍   | 166/256 [00:16<00:02, 43.30it/s, est. speed input: 163.83 toks/s, output: 2621.26 toks/s]
Processed prompts:  72%|███████▏  | 185/256 [00:16<00:01, 54.31it/s, est. speed input: 181.29 toks/s, output: 2900.59 toks/s]
Processed prompts:  79%|███████▉  | 203/256 [00:16<00:00, 63.98it/s, est. speed input: 197.15 toks/s, output: 3154.44 toks/s]
Processed prompts:  86%|████████▌ | 220/256 [00:16<00:00, 71.84it/s, est. speed input: 211.64 toks/s, output: 3386.28 toks/s]
Processed prompts:  92%|█████████▏| 235/256 [00:16<00:00, 75.50it/s, est. speed input: 223.80 toks/s, output: 3580.84 toks/s]
Processed prompts:  97%|█████████▋| 248/256 [00:17<00:00, 67.96it/s, est. speed input: 232.69 toks/s, output: 3722.99 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:17<00:00, 67.96it/s, est. speed input: 237.31 toks/s, output: 3796.99 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:17<00:00, 14.83it/s, est. speed input: 237.31 toks/s, output: 3796.99 toks/s]
[rank0]:[W127 17:35:56.803425052 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 49.6s

测试结果:
  Requests/s:   14.78
  Tokens/s:     4020.63
  Total Reqs:   256
  Elapsed:      17.32s

  [Decode 分析]
  Total Decode Tokens:  65536
  Decode Tokens/s:      3784.12

============================================================
[4/4] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-3B-FP8                                 │
│ Backend:  cuBLASLt                                        │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 8192 (= 512 x 16)
│   M_decode      = 512
│   batched_tokens = 512 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 512
│   --max-num-seqs           = 512
│   --max-model-len          = 272
│   --max-num-batched-tokens = 512
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:36:02 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:36:02 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2878764) WARNING 01-27 17:36:28 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 15.43 requests/s, 4196.29 total tokens/s, 3949.45 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072


─── STDERR ───
[2026-01-27 17:36:02] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:36:02] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-27 17:36:02] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-27 17:36:02] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:36:02] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:36:02] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:36:02] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:36:02] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:36:02] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-27 17:36:02] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:36:02] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:36:02] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:36:02] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:36:02] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:36:06] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:36:06] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-27 17:36:06] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-27 17:36:06] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:36:06] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:36:06] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:36:06] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:36:06] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:36:06] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-27 17:36:06] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:36:06] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:36:06] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:36:06] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:36:06] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2878764) [2026-01-27 17:36:07] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=2878764) [2026-01-27 17:36:07] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2878764) [2026-01-27 17:36:07] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=2878764) [2026-01-27 17:36:07] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=2878764) [2026-01-27 17:36:07] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=2878764) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2878764) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:15<00:00, 15.31s/it]
(EngineCore_DP0 pid=2878764) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:15<00:00, 15.31s/it]
(EngineCore_DP0 pid=2878764) 
(EngineCore_DP0 pid=2878764) 2026-01-27 17:36:28,075 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2878764) 2026-01-27 17:36:28,092 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 13749.97it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:24<3:30:30, 24.72s/it, est. speed input: 0.65 toks/s, output: 10.36 toks/s]
Processed prompts:   1%|          | 3/512 [00:24<55:09,  6.50s/it, est. speed input: 1.92 toks/s, output: 30.75 toks/s]  
Processed prompts:   7%|▋         | 34/512 [00:25<03:02,  2.62it/s, est. speed input: 21.57 toks/s, output: 345.20 toks/s]
Processed prompts:  12%|█▏        | 63/512 [00:25<01:17,  5.78it/s, est. speed input: 39.62 toks/s, output: 633.98 toks/s]
Processed prompts:  18%|█▊        | 91/512 [00:25<00:42,  9.90it/s, est. speed input: 56.76 toks/s, output: 908.20 toks/s]
Processed prompts:  23%|██▎       | 117/512 [00:25<00:26, 14.94it/s, est. speed input: 72.41 toks/s, output: 1158.62 toks/s]
Processed prompts:  32%|███▏      | 164/512 [00:26<00:12, 27.48it/s, est. speed input: 100.76 toks/s, output: 1612.11 toks/s]
Processed prompts:  40%|████      | 205/512 [00:26<00:07, 41.17it/s, est. speed input: 125.10 toks/s, output: 2001.60 toks/s]
Processed prompts:  47%|████▋     | 242/512 [00:26<00:04, 56.35it/s, est. speed input: 146.79 toks/s, output: 2348.68 toks/s]
Processed prompts:  53%|█████▎    | 273/512 [00:26<00:03, 70.95it/s, est. speed input: 164.67 toks/s, output: 2634.78 toks/s]
Processed prompts:  59%|█████▉    | 302/512 [00:26<00:02, 87.33it/s, est. speed input: 181.29 toks/s, output: 2900.64 toks/s]
Processed prompts:  64%|██████▍   | 327/512 [00:26<00:01, 102.26it/s, est. speed input: 195.42 toks/s, output: 3126.65 toks/s]
Processed prompts:  68%|██████▊   | 348/512 [00:26<00:01, 114.35it/s, est. speed input: 207.10 toks/s, output: 3313.61 toks/s]
Processed prompts:  72%|███████▏  | 369/512 [00:26<00:01, 128.21it/s, est. speed input: 218.75 toks/s, output: 3500.05 toks/s]
Processed prompts:  77%|███████▋  | 392/512 [00:27<00:00, 135.70it/s, est. speed input: 231.15 toks/s, output: 3698.46 toks/s]
Processed prompts:  80%|████████  | 412/512 [00:27<00:00, 138.57it/s, est. speed input: 241.74 toks/s, output: 3867.83 toks/s]
Processed prompts:  84%|████████▍ | 430/512 [00:27<00:00, 142.04it/s, est. speed input: 251.22 toks/s, output: 4019.54 toks/s]
Processed prompts:  87%|████████▋ | 447/512 [00:27<00:00, 135.14it/s, est. speed input: 259.79 toks/s, output: 4156.61 toks/s]
Processed prompts:  90%|█████████ | 463/512 [00:27<00:00, 122.52it/s, est. speed input: 267.47 toks/s, output: 4279.49 toks/s]
Processed prompts:  93%|█████████▎| 477/512 [00:27<00:00, 107.89it/s, est. speed input: 273.79 toks/s, output: 4380.61 toks/s]
Processed prompts:  96%|█████████▌| 489/512 [00:28<00:00, 81.48it/s, est. speed input: 277.99 toks/s, output: 4447.76 toks/s] 
Processed prompts:  97%|█████████▋| 499/512 [00:33<00:01,  8.72it/s, est. speed input: 241.36 toks/s, output: 3861.74 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:33<00:00,  8.72it/s, est. speed input: 247.13 toks/s, output: 3954.01 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:33<00:00, 15.45it/s, est. speed input: 247.13 toks/s, output: 3954.01 toks/s]
[rank0]:[W127 17:37:02.255581764 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 65.4s

测试结果:
  Requests/s:   15.43
  Tokens/s:     4196.29
  Total Reqs:   512
  Elapsed:      33.19s

  [Decode 分析]
  Total Decode Tokens:  131072
  Decode Tokens/s:      3949.45


------------------------------------------------------------
  生成 CSV: Llama3.2-3B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cublaslt/Llama3.2-3B-FP8_decode.csv

预览:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
64,16,64,64,256,256,7.7494,2107.8342,8.2587
128,16,128,128,256,256,11.5797,3149.6906,11.0538
256,16,256,256,256,256,14.7817,4020.6325,17.3187
512,16,512,512,256,256,15.4275,4196.2926,33.1874

------------------------------------------------------------

[INFO] 完成: 4 成功, 0 失败

============================================================
  Llama3.2-3B-FP8 | cuSPARSELt (2_4) | decode
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_4
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_4

============================================================
[1/4] 测试 M=64
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-3B-FP8                                 │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 64
│   M_prefill     = 1024 (= 64 x 16)
│   M_decode      = 64
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 64
│   --max-num-seqs           = 64
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:37:08 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:37:08 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2879857) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2879857) WARNING 01-27 17:37:35 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 9.14 requests/s, 2486.30 total tokens/s, 2340.04 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384


─── STDERR ───
[2026-01-27 17:37:07] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:37:08] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-27 17:37:08] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-27 17:37:08] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:37:08] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:37:08] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:37:08] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:37:08] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:37:08] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-27 17:37:08] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:37:08] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:37:08] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:37:08] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:37:08] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:37:11] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:37:11] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-27 17:37:11] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-27 17:37:11] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:37:11] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:37:11] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:37:11] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:37:11] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:37:11] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-27 17:37:11] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:37:11] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:37:11] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:37:11] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:37:11] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2879857) [2026-01-27 17:37:12] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2879857) [2026-01-27 17:37:12] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2879857) [2026-01-27 17:37:12] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2879857) [2026-01-27 17:37:12] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=2879857) [2026-01-27 17:37:12] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=2879857) [2026-01-27 17:37:12] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=2879857) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2879857) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:16<00:00, 16.06s/it]
(EngineCore_DP0 pid=2879857) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:16<00:00, 16.06s/it]
(EngineCore_DP0 pid=2879857) 
(EngineCore_DP0 pid=2879857) [2026-01-27 17:37:29] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 3072] -> 1D uint8
(EngineCore_DP0 pid=2879857) [2026-01-27 17:37:29] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9830400 bytes
(EngineCore_DP0 pid=2879857) [2026-01-27 17:37:29] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=2879857) [2026-01-27 17:37:29] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5898240 bytes
(EngineCore_DP0 pid=2879857) [2026-01-27 17:37:29] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=2879857) [2026-01-27 17:37:29] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 31457280 bytes
(EngineCore_DP0 pid=2879857) [2026-01-27 17:37:29] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 8192] -> 1D uint8
(EngineCore_DP0 pid=2879857) [2026-01-27 17:37:29] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15728640 bytes
(EngineCore_DP0 pid=2879857) 2026-01-27 17:37:35,220 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2879857) 2026-01-27 17:37:35,234 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 64/64 [00:00<00:00, 11988.54it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 1/64 [00:06<07:09,  6.82s/it, est. speed input: 2.35 toks/s, output: 37.55 toks/s]
Processed prompts:  53%|█████▎    | 34/64 [00:06<00:04,  6.89it/s, est. speed input: 78.40 toks/s, output: 1254.41 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:06<00:00,  6.89it/s, est. speed input: 146.38 toks/s, output: 2342.10 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:06<00:00,  9.15it/s, est. speed input: 146.38 toks/s, output: 2342.10 toks/s]
[rank0]:[W127 17:37:43.045853400 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 40.8s

测试结果:
  Requests/s:   9.14
  Tokens/s:     2486.30
  Total Reqs:   64
  Elapsed:      7.00s

  [Decode 分析]
  Total Decode Tokens:  16384
  Decode Tokens/s:      2340.04

============================================================
[2/4] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-3B-FP8                                 │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 2048 (= 128 x 16)
│   M_decode      = 128
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 128
│   --max-num-seqs           = 128
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:37:49 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:37:49 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2880607) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2880607) WARNING 01-27 17:38:15 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 13.54 requests/s, 3682.43 total tokens/s, 3465.82 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768


─── STDERR ───
[2026-01-27 17:37:49] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:37:49] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-27 17:37:49] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-27 17:37:49] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:37:49] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:37:49] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:37:49] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:37:49] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:37:49] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-27 17:37:49] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:37:49] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:37:49] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:37:49] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:37:49] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:37:52] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:37:52] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-27 17:37:52] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-27 17:37:52] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:37:52] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:37:52] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:37:52] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:37:52] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:37:52] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-27 17:37:52] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:37:52] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:37:52] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:37:52] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:37:52] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2880607) [2026-01-27 17:37:53] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2880607) [2026-01-27 17:37:53] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2880607) [2026-01-27 17:37:53] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2880607) [2026-01-27 17:37:53] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=2880607) [2026-01-27 17:37:53] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=2880607) [2026-01-27 17:37:53] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=2880607) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2880607) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:15<00:00, 15.24s/it]
(EngineCore_DP0 pid=2880607) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:15<00:00, 15.24s/it]
(EngineCore_DP0 pid=2880607) 
(EngineCore_DP0 pid=2880607) [2026-01-27 17:38:09] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 3072] -> 1D uint8
(EngineCore_DP0 pid=2880607) [2026-01-27 17:38:09] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9830400 bytes
(EngineCore_DP0 pid=2880607) [2026-01-27 17:38:09] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=2880607) [2026-01-27 17:38:09] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5898240 bytes
(EngineCore_DP0 pid=2880607) [2026-01-27 17:38:09] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=2880607) [2026-01-27 17:38:09] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 31457280 bytes
(EngineCore_DP0 pid=2880607) [2026-01-27 17:38:09] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 8192] -> 1D uint8
(EngineCore_DP0 pid=2880607) [2026-01-27 17:38:09] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15728640 bytes
(EngineCore_DP0 pid=2880607) 2026-01-27 17:38:14,722 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2880607) 2026-01-27 17:38:14,733 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 12876.15it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:08<18:52,  8.92s/it, est. speed input: 1.79 toks/s, output: 28.71 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:09<00:32,  3.26it/s, est. speed input: 37.26 toks/s, output: 596.09 toks/s]
Processed prompts:  39%|███▉      | 50/128 [00:09<00:08,  9.55it/s, est. speed input: 87.61 toks/s, output: 1401.83 toks/s]
Processed prompts:  69%|██████▉   | 88/128 [00:09<00:01, 20.76it/s, est. speed input: 152.38 toks/s, output: 2438.06 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:09<00:00, 32.72it/s, est. speed input: 203.76 toks/s, output: 3260.09 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:09<00:00, 32.72it/s, est. speed input: 216.86 toks/s, output: 3469.76 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:09<00:00, 13.55it/s, est. speed input: 216.86 toks/s, output: 3469.76 toks/s]
[rank0]:[W127 17:38:25.145103462 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 42.0s

测试结果:
  Requests/s:   13.54
  Tokens/s:     3682.43
  Total Reqs:   128
  Elapsed:      9.45s

  [Decode 分析]
  Total Decode Tokens:  32768
  Decode Tokens/s:      3465.82

============================================================
[3/4] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-3B-FP8                                 │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 4096 (= 256 x 16)
│   M_decode      = 256
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 256
│   --max-num-seqs           = 256
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:38:30 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:38:31 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2881393) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2881393) WARNING 01-27 17:38:57 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 16.45 requests/s, 4475.54 total tokens/s, 4212.27 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536


─── STDERR ───
[2026-01-27 17:38:30] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:38:30] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-27 17:38:30] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-27 17:38:30] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:38:30] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:38:30] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:38:30] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:38:30] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:38:30] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-27 17:38:30] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:38:30] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:38:30] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:38:30] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:38:30] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:38:34] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:38:34] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-27 17:38:34] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-27 17:38:34] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:38:34] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:38:34] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:38:34] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:38:34] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:38:34] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-27 17:38:34] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:38:34] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:38:34] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:38:34] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:38:34] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2881393) [2026-01-27 17:38:35] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2881393) [2026-01-27 17:38:35] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2881393) [2026-01-27 17:38:35] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2881393) [2026-01-27 17:38:35] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=2881393) [2026-01-27 17:38:35] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=2881393) [2026-01-27 17:38:35] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=2881393) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2881393) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:15<00:00, 15.38s/it]
(EngineCore_DP0 pid=2881393) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:15<00:00, 15.38s/it]
(EngineCore_DP0 pid=2881393) 
(EngineCore_DP0 pid=2881393) [2026-01-27 17:38:51] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 3072] -> 1D uint8
(EngineCore_DP0 pid=2881393) [2026-01-27 17:38:51] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9830400 bytes
(EngineCore_DP0 pid=2881393) [2026-01-27 17:38:51] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=2881393) [2026-01-27 17:38:51] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5898240 bytes
(EngineCore_DP0 pid=2881393) [2026-01-27 17:38:51] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=2881393) [2026-01-27 17:38:51] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 31457280 bytes
(EngineCore_DP0 pid=2881393) [2026-01-27 17:38:51] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 8192] -> 1D uint8
(EngineCore_DP0 pid=2881393) [2026-01-27 17:38:51] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15728640 bytes
(EngineCore_DP0 pid=2881393) 2026-01-27 17:38:56,638 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2881393) 2026-01-27 17:38:56,650 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 4619.20it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:13<57:30, 13.53s/it, est. speed input: 1.18 toks/s, output: 18.92 toks/s]
Processed prompts:   7%|▋         | 18/256 [00:13<02:09,  1.84it/s, est. speed input: 21.03 toks/s, output: 336.53 toks/s]
Processed prompts:  19%|█▉        | 48/256 [00:13<00:33,  6.15it/s, est. speed input: 55.50 toks/s, output: 887.98 toks/s]
Processed prompts:  29%|██▉       | 75/256 [00:13<00:15, 11.39it/s, est. speed input: 85.76 toks/s, output: 1372.11 toks/s]
Processed prompts:  36%|███▌      | 92/256 [00:14<00:10, 15.78it/s, est. speed input: 104.40 toks/s, output: 1670.38 toks/s]
Processed prompts:  43%|████▎     | 109/256 [00:14<00:06, 21.54it/s, est. speed input: 122.67 toks/s, output: 1962.65 toks/s]
Processed prompts:  50%|████▉     | 127/256 [00:14<00:04, 29.52it/s, est. speed input: 141.81 toks/s, output: 2268.95 toks/s]
Processed prompts:  56%|█████▋    | 144/256 [00:14<00:02, 38.94it/s, est. speed input: 159.61 toks/s, output: 2553.78 toks/s]
Processed prompts:  65%|██████▍   | 166/256 [00:14<00:01, 52.53it/s, est. speed input: 182.08 toks/s, output: 2913.30 toks/s]
Processed prompts:  71%|███████▏  | 183/256 [00:14<00:01, 63.84it/s, est. speed input: 199.10 toks/s, output: 3185.53 toks/s]
Processed prompts:  78%|███████▊  | 199/256 [00:14<00:00, 74.08it/s, est. speed input: 214.69 toks/s, output: 3435.02 toks/s]
Processed prompts:  84%|████████▎ | 214/256 [00:14<00:00, 82.19it/s, est. speed input: 228.89 toks/s, output: 3662.30 toks/s]
Processed prompts:  89%|████████▉ | 228/256 [00:15<00:00, 86.16it/s, est. speed input: 241.60 toks/s, output: 3865.58 toks/s]
Processed prompts:  94%|█████████▍| 241/256 [00:15<00:00, 89.77it/s, est. speed input: 253.24 toks/s, output: 4051.77 toks/s]
Processed prompts:  99%|█████████▉| 253/256 [00:15<00:00, 77.36it/s, est. speed input: 262.10 toks/s, output: 4193.52 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:15<00:00, 77.36it/s, est. speed input: 264.23 toks/s, output: 4227.65 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:15<00:00, 16.51it/s, est. speed input: 264.23 toks/s, output: 4227.65 toks/s]
[rank0]:[W127 17:39:13.119649523 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 47.9s

测试结果:
  Requests/s:   16.45
  Tokens/s:     4475.54
  Total Reqs:   256
  Elapsed:      15.56s

  [Decode 分析]
  Total Decode Tokens:  65536
  Decode Tokens/s:      4212.27

============================================================
[4/4] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-3B-FP8                                 │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 8192 (= 512 x 16)
│   M_decode      = 512
│   batched_tokens = 512 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 512
│   --max-num-seqs           = 512
│   --max-model-len          = 272
│   --max-num-batched-tokens = 512
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:39:18 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:39:18 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2882236) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2882236) WARNING 01-27 17:39:45 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 16.67 requests/s, 4534.54 total tokens/s, 4267.81 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072


─── STDERR ───
[2026-01-27 17:39:18] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:39:18] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-27 17:39:18] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-27 17:39:18] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:39:18] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:39:18] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:39:18] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:39:18] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:39:18] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-27 17:39:18] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:39:18] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:39:18] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:39:18] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:39:18] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:39:22] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:39:22] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-27 17:39:22] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-27 17:39:22] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:39:22] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:39:22] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:39:22] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:39:22] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:39:22] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-27 17:39:22] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:39:22] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:39:22] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:39:22] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:39:22] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2882236) [2026-01-27 17:39:23] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2882236) [2026-01-27 17:39:23] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2882236) [2026-01-27 17:39:23] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2882236) [2026-01-27 17:39:23] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=2882236) [2026-01-27 17:39:23] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=2882236) [2026-01-27 17:39:23] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=2882236) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2882236) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:15<00:00, 15.68s/it]
(EngineCore_DP0 pid=2882236) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:15<00:00, 15.68s/it]
(EngineCore_DP0 pid=2882236) 
(EngineCore_DP0 pid=2882236) [2026-01-27 17:39:39] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 3072] -> 1D uint8
(EngineCore_DP0 pid=2882236) [2026-01-27 17:39:39] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9830400 bytes
(EngineCore_DP0 pid=2882236) [2026-01-27 17:39:39] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=2882236) [2026-01-27 17:39:39] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5898240 bytes
(EngineCore_DP0 pid=2882236) [2026-01-27 17:39:39] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=2882236) [2026-01-27 17:39:39] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 31457280 bytes
(EngineCore_DP0 pid=2882236) [2026-01-27 17:39:39] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 8192] -> 1D uint8
(EngineCore_DP0 pid=2882236) [2026-01-27 17:39:39] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15728640 bytes
(EngineCore_DP0 pid=2882236) 2026-01-27 17:39:44,879 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2882236) 2026-01-27 17:39:44,890 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 13686.43it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:23<3:18:12, 23.27s/it, est. speed input: 0.69 toks/s, output: 11.00 toks/s]
Processed prompts:   1%|          | 6/512 [00:23<24:28,  2.90s/it, est. speed input: 4.08 toks/s, output: 65.31 toks/s]  
Processed prompts:   7%|▋         | 37/512 [00:23<02:40,  2.96it/s, est. speed input: 24.92 toks/s, output: 398.77 toks/s]
Processed prompts:  13%|█▎        | 66/512 [00:23<01:10,  6.30it/s, est. speed input: 44.05 toks/s, output: 704.82 toks/s]
Processed prompts:  18%|█▊        | 93/512 [00:24<00:39, 10.49it/s, est. speed input: 61.55 toks/s, output: 984.73 toks/s]
Processed prompts:  23%|██▎       | 119/512 [00:24<00:24, 15.74it/s, est. speed input: 78.05 toks/s, output: 1248.85 toks/s]
Processed prompts:  28%|██▊       | 143/512 [00:24<00:16, 22.51it/s, est. speed input: 93.36 toks/s, output: 1493.72 toks/s]
Processed prompts:  32%|███▏      | 166/512 [00:24<00:11, 30.91it/s, est. speed input: 107.84 toks/s, output: 1725.45 toks/s]
Processed prompts:  37%|███▋      | 187/512 [00:24<00:07, 40.79it/s, est. speed input: 120.97 toks/s, output: 1935.44 toks/s]
Processed prompts:  40%|████      | 207/512 [00:24<00:05, 52.50it/s, est. speed input: 133.36 toks/s, output: 2133.76 toks/s]
Processed prompts:  47%|████▋     | 243/512 [00:25<00:03, 75.42it/s, est. speed input: 155.37 toks/s, output: 2486.00 toks/s]
Processed prompts:  54%|█████▎    | 274/512 [00:25<00:02, 93.42it/s, est. speed input: 173.96 toks/s, output: 2783.31 toks/s]
Processed prompts:  59%|█████▉    | 302/512 [00:25<00:01, 112.05it/s, est. speed input: 190.70 toks/s, output: 3051.23 toks/s]
Processed prompts:  64%|██████▍   | 327/512 [00:25<00:01, 122.85it/s, est. speed input: 205.25 toks/s, output: 3283.97 toks/s]
Processed prompts:  68%|██████▊   | 348/512 [00:25<00:01, 132.18it/s, est. speed input: 217.38 toks/s, output: 3478.09 toks/s]
Processed prompts:  72%|███████▏  | 367/512 [00:25<00:01, 135.69it/s, est. speed input: 228.10 toks/s, output: 3649.66 toks/s]
Processed prompts:  75%|███████▌  | 384/512 [00:25<00:00, 140.28it/s, est. speed input: 237.67 toks/s, output: 3802.75 toks/s]
Processed prompts:  78%|███████▊  | 401/512 [00:25<00:00, 145.19it/s, est. speed input: 247.19 toks/s, output: 3955.02 toks/s]
Processed prompts:  82%|████████▏ | 418/512 [00:26<00:00, 133.41it/s, est. speed input: 256.13 toks/s, output: 4098.08 toks/s]
Processed prompts:  86%|████████▌ | 438/512 [00:26<00:00, 139.92it/s, est. speed input: 267.08 toks/s, output: 4273.32 toks/s]
Processed prompts:  89%|████████▊ | 454/512 [00:26<00:00, 131.65it/s, est. speed input: 275.35 toks/s, output: 4405.60 toks/s]
Processed prompts:  91%|█████████▏| 468/512 [00:26<00:00, 118.57it/s, est. speed input: 282.19 toks/s, output: 4515.10 toks/s]
Processed prompts:  94%|█████████▍| 481/512 [00:26<00:00, 112.11it/s, est. speed input: 288.56 toks/s, output: 4616.97 toks/s]
Processed prompts:  96%|█████████▋| 493/512 [00:26<00:00, 80.41it/s, est. speed input: 292.70 toks/s, output: 4683.13 toks/s] 
Processed prompts:  98%|█████████▊| 503/512 [00:30<00:00, 10.97it/s, est. speed input: 263.34 toks/s, output: 4213.51 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:30<00:00, 13.61it/s, est. speed input: 267.07 toks/s, output: 4273.14 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:30<00:00, 13.61it/s, est. speed input: 267.07 toks/s, output: 4273.14 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:30<00:00, 16.69it/s, est. speed input: 267.07 toks/s, output: 4273.14 toks/s]
[rank0]:[W127 17:40:16.565079079 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 63.5s

测试结果:
  Requests/s:   16.67
  Tokens/s:     4534.54
  Total Reqs:   512
  Elapsed:      30.71s

  [Decode 分析]
  Total Decode Tokens:  131072
  Decode Tokens/s:      4267.81


------------------------------------------------------------
  生成 CSV: Llama3.2-3B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_4/Llama3.2-3B-FP8_decode.csv

预览:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
64,16,64,64,256,256,9.1408,2486.2953,7.0016
128,16,128,128,256,256,13.5384,3682.4329,9.4546
256,16,256,256,256,256,16.4542,4475.5357,15.5584
512,16,512,512,256,256,16.6711,4534.5438,30.7118

------------------------------------------------------------

[INFO] 完成: 4 成功, 0 失败

============================================================
  Llama3.2-3B-FP8 | cuSPARSELt (2_6) | decode
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_6
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_6

============================================================
[1/4] 测试 M=64
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-3B-FP8                                 │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 64
│   M_prefill     = 1024 (= 64 x 16)
│   M_decode      = 64
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 64
│   --max-num-seqs           = 64
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:40:22 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:40:22 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2883319) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2883319) WARNING 01-27 17:40:55 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 7.89 requests/s, 2145.47 total tokens/s, 2019.26 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384


─── STDERR ───
[2026-01-27 17:40:22] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:40:22] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-27 17:40:22] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-27 17:40:22] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:40:22] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:40:22] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:40:22] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:40:22] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:40:22] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-27 17:40:22] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:40:22] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:40:22] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:40:22] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:40:22] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:40:25] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:40:25] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-27 17:40:25] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-27 17:40:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:40:25] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:40:25] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:40:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:40:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:40:25] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-27 17:40:25] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:40:25] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:40:25] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:40:25] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:40:25] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2883319) [2026-01-27 17:40:26] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2883319) [2026-01-27 17:40:26] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2883319) [2026-01-27 17:40:26] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2883319) [2026-01-27 17:40:26] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=2883319) [2026-01-27 17:40:26] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=2883319) [2026-01-27 17:40:26] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=2883319) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2883319) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:20<00:00, 20.97s/it]
(EngineCore_DP0 pid=2883319) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:20<00:00, 20.97s/it]
(EngineCore_DP0 pid=2883319) 
(EngineCore_DP0 pid=2883319) [2026-01-27 17:40:48] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 4096] -> 1D uint8
(EngineCore_DP0 pid=2883319) [2026-01-27 17:40:48] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 13107200 bytes
(EngineCore_DP0 pid=2883319) [2026-01-27 17:40:48] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 4096] -> 1D uint8
(EngineCore_DP0 pid=2883319) [2026-01-27 17:40:48] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 7864320 bytes
(EngineCore_DP0 pid=2883319) [2026-01-27 17:40:48] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 4096] -> 1D uint8
(EngineCore_DP0 pid=2883319) [2026-01-27 17:40:48] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 41943040 bytes
(EngineCore_DP0 pid=2883319) [2026-01-27 17:40:48] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 10944] -> 1D uint8
(EngineCore_DP0 pid=2883319) [2026-01-27 17:40:48] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 21037056 bytes
(EngineCore_DP0 pid=2883319) 2026-01-27 17:40:54,623 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2883319) 2026-01-27 17:40:54,636 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 64/64 [00:00<00:00, 11824.31it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 1/64 [00:07<08:12,  7.82s/it, est. speed input: 2.05 toks/s, output: 32.73 toks/s]
Processed prompts:  58%|█████▊    | 37/64 [00:07<00:04,  6.54it/s, est. speed input: 74.38 toks/s, output: 1190.10 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:08<00:00, 12.92it/s, est. speed input: 126.31 toks/s, output: 2020.89 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:08<00:00, 12.92it/s, est. speed input: 126.31 toks/s, output: 2020.89 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:08<00:00,  7.89it/s, est. speed input: 126.31 toks/s, output: 2020.89 toks/s]
[rank0]:[W127 17:41:03.588642224 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 47.1s

测试结果:
  Requests/s:   7.89
  Tokens/s:     2145.47
  Total Reqs:   64
  Elapsed:      8.11s

  [Decode 分析]
  Total Decode Tokens:  16384
  Decode Tokens/s:      2019.26

============================================================
[2/4] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-3B-FP8                                 │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 2048 (= 128 x 16)
│   M_decode      = 128
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 128
│   --max-num-seqs           = 128
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:41:09 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:41:09 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2884161) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2884161) WARNING 01-27 17:41:41 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 12.26 requests/s, 3334.67 total tokens/s, 3138.51 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768


─── STDERR ───
[2026-01-27 17:41:09] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:41:09] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-27 17:41:09] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-27 17:41:09] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:41:09] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:41:09] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:41:09] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:41:09] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:41:09] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-27 17:41:09] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:41:09] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:41:09] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:41:09] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:41:09] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:41:13] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:41:13] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-27 17:41:13] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-27 17:41:13] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:41:13] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:41:13] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:41:13] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:41:13] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:41:13] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-27 17:41:13] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:41:13] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:41:13] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:41:13] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:41:13] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2884161) [2026-01-27 17:41:14] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2884161) [2026-01-27 17:41:14] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2884161) [2026-01-27 17:41:14] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2884161) [2026-01-27 17:41:14] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=2884161) [2026-01-27 17:41:14] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=2884161) [2026-01-27 17:41:14] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=2884161) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2884161) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:20<00:00, 20.56s/it]
(EngineCore_DP0 pid=2884161) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:20<00:00, 20.56s/it]
(EngineCore_DP0 pid=2884161) 
(EngineCore_DP0 pid=2884161) [2026-01-27 17:41:35] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 4096] -> 1D uint8
(EngineCore_DP0 pid=2884161) [2026-01-27 17:41:35] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 13107200 bytes
(EngineCore_DP0 pid=2884161) [2026-01-27 17:41:35] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 4096] -> 1D uint8
(EngineCore_DP0 pid=2884161) [2026-01-27 17:41:35] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 7864320 bytes
(EngineCore_DP0 pid=2884161) [2026-01-27 17:41:35] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 4096] -> 1D uint8
(EngineCore_DP0 pid=2884161) [2026-01-27 17:41:35] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 41943040 bytes
(EngineCore_DP0 pid=2884161) [2026-01-27 17:41:35] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 10944] -> 1D uint8
(EngineCore_DP0 pid=2884161) [2026-01-27 17:41:35] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 21037056 bytes
(EngineCore_DP0 pid=2884161) 2026-01-27 17:41:40,625 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2884161) 2026-01-27 17:41:40,636 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 4229.76it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:09<20:57,  9.90s/it, est. speed input: 1.62 toks/s, output: 25.86 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:10<00:43,  2.51it/s, est. speed input: 28.73 toks/s, output: 459.62 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:10<00:05, 11.07it/s, est. speed input: 97.44 toks/s, output: 1559.06 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:10<00:02, 17.55it/s, est. speed input: 135.30 toks/s, output: 2164.85 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:10<00:00, 28.49it/s, est. speed input: 181.74 toks/s, output: 2907.78 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:10<00:00, 28.49it/s, est. speed input: 196.75 toks/s, output: 3148.04 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:10<00:00, 12.30it/s, est. speed input: 196.75 toks/s, output: 3148.04 toks/s]
[rank0]:[W127 17:41:51.950011894 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 48.3s

测试结果:
  Requests/s:   12.26
  Tokens/s:     3334.67
  Total Reqs:   128
  Elapsed:      10.44s

  [Decode 分析]
  Total Decode Tokens:  32768
  Decode Tokens/s:      3138.51

============================================================
[3/4] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-3B-FP8                                 │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 4096 (= 256 x 16)
│   M_decode      = 256
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 256
│   --max-num-seqs           = 256
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:41:57 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:41:57 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2885028) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2885028) WARNING 01-27 17:42:29 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 15.44 requests/s, 4200.83 total tokens/s, 3953.72 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536


─── STDERR ───
[2026-01-27 17:41:57] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:41:57] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-27 17:41:57] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-27 17:41:57] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:41:57] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:41:57] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:41:57] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:41:57] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:41:57] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-27 17:41:57] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:41:57] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:41:57] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:41:57] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:41:57] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:42:01] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:42:01] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-27 17:42:01] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-27 17:42:01] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:42:01] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:42:01] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:42:01] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:42:01] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:42:01] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-27 17:42:01] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:42:01] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:42:01] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:42:01] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:42:01] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2885028) [2026-01-27 17:42:02] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2885028) [2026-01-27 17:42:02] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2885028) [2026-01-27 17:42:02] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2885028) [2026-01-27 17:42:02] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=2885028) [2026-01-27 17:42:02] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=2885028) [2026-01-27 17:42:02] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=2885028) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2885028) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:20<00:00, 20.43s/it]
(EngineCore_DP0 pid=2885028) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:20<00:00, 20.43s/it]
(EngineCore_DP0 pid=2885028) 
(EngineCore_DP0 pid=2885028) [2026-01-27 17:42:23] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 4096] -> 1D uint8
(EngineCore_DP0 pid=2885028) [2026-01-27 17:42:23] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 13107200 bytes
(EngineCore_DP0 pid=2885028) [2026-01-27 17:42:23] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 4096] -> 1D uint8
(EngineCore_DP0 pid=2885028) [2026-01-27 17:42:23] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 7864320 bytes
(EngineCore_DP0 pid=2885028) [2026-01-27 17:42:23] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 4096] -> 1D uint8
(EngineCore_DP0 pid=2885028) [2026-01-27 17:42:23] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 41943040 bytes
(EngineCore_DP0 pid=2885028) [2026-01-27 17:42:23] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 10944] -> 1D uint8
(EngineCore_DP0 pid=2885028) [2026-01-27 17:42:23] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 21037056 bytes
(EngineCore_DP0 pid=2885028) 2026-01-27 17:42:28,617 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2885028) 2026-01-27 17:42:28,627 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 12457.99it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:14<1:01:24, 14.45s/it, est. speed input: 1.11 toks/s, output: 17.72 toks/s]
Processed prompts:   7%|▋         | 18/256 [00:14<02:18,  1.72it/s, est. speed input: 19.71 toks/s, output: 315.38 toks/s]
Processed prompts:  19%|█▉        | 48/256 [00:14<00:36,  5.77it/s, est. speed input: 52.01 toks/s, output: 832.18 toks/s]
Processed prompts:  29%|██▉       | 75/256 [00:14<00:16, 10.68it/s, est. speed input: 80.39 toks/s, output: 1286.28 toks/s]
Processed prompts:  38%|███▊      | 98/256 [00:15<00:09, 16.20it/s, est. speed input: 103.97 toks/s, output: 1663.45 toks/s]
Processed prompts:  46%|████▌     | 118/256 [00:15<00:06, 22.59it/s, est. speed input: 124.20 toks/s, output: 1987.19 toks/s]
Processed prompts:  53%|█████▎    | 136/256 [00:15<00:04, 29.94it/s, est. speed input: 142.07 toks/s, output: 2273.08 toks/s]
Processed prompts:  59%|█████▉    | 152/256 [00:15<00:02, 38.15it/s, est. speed input: 157.71 toks/s, output: 2523.41 toks/s]
Processed prompts:  67%|██████▋   | 172/256 [00:15<00:01, 50.16it/s, est. speed input: 176.91 toks/s, output: 2830.54 toks/s]
Processed prompts:  73%|███████▎  | 188/256 [00:15<00:01, 60.03it/s, est. speed input: 191.83 toks/s, output: 3069.30 toks/s]
Processed prompts:  79%|███████▉  | 203/256 [00:15<00:00, 69.24it/s, est. speed input: 205.50 toks/s, output: 3288.04 toks/s]
Processed prompts:  85%|████████▍ | 217/256 [00:15<00:00, 75.09it/s, est. speed input: 217.72 toks/s, output: 3483.53 toks/s]
Processed prompts:  90%|████████▉ | 230/256 [00:16<00:00, 77.04it/s, est. speed input: 228.53 toks/s, output: 3656.43 toks/s]
Processed prompts:  95%|█████████▍| 242/256 [00:16<00:00, 78.07it/s, est. speed input: 238.27 toks/s, output: 3812.24 toks/s]
Processed prompts:  99%|█████████▉| 253/256 [00:16<00:00, 66.69it/s, est. speed input: 245.51 toks/s, output: 3928.15 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:16<00:00, 66.69it/s, est. speed input: 247.43 toks/s, output: 3958.90 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:16<00:00, 15.46it/s, est. speed input: 247.43 toks/s, output: 3958.90 toks/s]
[rank0]:[W127 17:42:46.071983846 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 54.1s

测试结果:
  Requests/s:   15.44
  Tokens/s:     4200.83
  Total Reqs:   256
  Elapsed:      16.58s

  [Decode 分析]
  Total Decode Tokens:  65536
  Decode Tokens/s:      3953.72

============================================================
[4/4] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-3B-FP8                                 │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 8192 (= 512 x 16)
│   M_decode      = 512
│   batched_tokens = 512 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 512
│   --max-num-seqs           = 512
│   --max-model-len          = 272
│   --max-num-batched-tokens = 512
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:42:51 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:42:51 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2885969) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2885969) WARNING 01-27 17:43:23 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 15.78 requests/s, 4292.53 total tokens/s, 4040.02 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072


─── STDERR ───
[2026-01-27 17:42:51] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:42:51] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-27 17:42:51] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-27 17:42:51] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:42:51] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:42:51] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:42:51] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:42:51] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:42:51] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-27 17:42:51] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:42:51] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:42:51] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:42:51] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:42:51] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:42:55] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:42:55] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-27 17:42:55] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-27 17:42:55] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:42:55] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:42:55] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:42:55] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:42:55] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:42:55] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-27 17:42:55] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:42:55] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:42:55] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:42:55] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:42:55] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2885969) [2026-01-27 17:42:56] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2885969) [2026-01-27 17:42:56] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2885969) [2026-01-27 17:42:56] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2885969) [2026-01-27 17:42:56] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=2885969) [2026-01-27 17:42:56] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=2885969) [2026-01-27 17:42:56] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=2885969) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2885969) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:20<00:00, 20.58s/it]
(EngineCore_DP0 pid=2885969) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:20<00:00, 20.58s/it]
(EngineCore_DP0 pid=2885969) 
(EngineCore_DP0 pid=2885969) [2026-01-27 17:43:17] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 4096] -> 1D uint8
(EngineCore_DP0 pid=2885969) [2026-01-27 17:43:17] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 13107200 bytes
(EngineCore_DP0 pid=2885969) [2026-01-27 17:43:17] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 4096] -> 1D uint8
(EngineCore_DP0 pid=2885969) [2026-01-27 17:43:17] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 7864320 bytes
(EngineCore_DP0 pid=2885969) [2026-01-27 17:43:17] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 4096] -> 1D uint8
(EngineCore_DP0 pid=2885969) [2026-01-27 17:43:17] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 41943040 bytes
(EngineCore_DP0 pid=2885969) [2026-01-27 17:43:17] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 10944] -> 1D uint8
(EngineCore_DP0 pid=2885969) [2026-01-27 17:43:17] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 21037056 bytes
(EngineCore_DP0 pid=2885969) 2026-01-27 17:43:22,849 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2885969) 2026-01-27 17:43:22,859 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 6815.35it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:24<3:25:46, 24.16s/it, est. speed input: 0.66 toks/s, output: 10.60 toks/s]
Processed prompts:   0%|          | 2/512 [00:24<1:25:48, 10.10s/it, est. speed input: 1.31 toks/s, output: 20.97 toks/s]
Processed prompts:   6%|▋         | 33/512 [00:24<03:02,  2.63it/s, est. speed input: 21.42 toks/s, output: 342.67 toks/s]
Processed prompts:  12%|█▏        | 63/512 [00:24<01:15,  5.97it/s, est. speed input: 40.52 toks/s, output: 648.32 toks/s]
Processed prompts:  18%|█▊        | 91/512 [00:25<00:41, 10.16it/s, est. speed input: 58.04 toks/s, output: 928.60 toks/s]
Processed prompts:  23%|██▎       | 117/512 [00:25<00:25, 15.24it/s, est. speed input: 73.97 toks/s, output: 1183.55 toks/s]
Processed prompts:  28%|██▊       | 141/512 [00:25<00:17, 21.81it/s, est. speed input: 88.75 toks/s, output: 1419.92 toks/s]
Processed prompts:  32%|███▏      | 164/512 [00:25<00:11, 30.06it/s, est. speed input: 102.76 toks/s, output: 1644.20 toks/s]
Processed prompts:  36%|███▌      | 185/512 [00:25<00:08, 39.42it/s, est. speed input: 115.38 toks/s, output: 1846.10 toks/s]
Processed prompts:  40%|████      | 205/512 [00:25<00:06, 50.77it/s, est. speed input: 127.34 toks/s, output: 2037.51 toks/s]
Processed prompts:  44%|████▍     | 224/512 [00:25<00:04, 63.62it/s, est. speed input: 138.61 toks/s, output: 2217.74 toks/s]
Processed prompts:  50%|█████     | 258/512 [00:26<00:02, 86.67it/s, est. speed input: 158.48 toks/s, output: 2535.67 toks/s]
Processed prompts:  56%|█████▋    | 288/512 [00:26<00:02, 106.73it/s, est. speed input: 175.85 toks/s, output: 2813.65 toks/s]
Processed prompts:  62%|██████▏   | 315/512 [00:26<00:01, 119.42it/s, est. speed input: 191.14 toks/s, output: 3058.16 toks/s]
Processed prompts:  66%|██████▌   | 338/512 [00:26<00:01, 126.86it/s, est. speed input: 203.92 toks/s, output: 3262.78 toks/s]
Processed prompts:  70%|██████▉   | 358/512 [00:26<00:01, 132.38it/s, est. speed input: 214.92 toks/s, output: 3438.78 toks/s]
Processed prompts:  73%|███████▎  | 376/512 [00:26<00:00, 137.44it/s, est. speed input: 224.76 toks/s, output: 3596.09 toks/s]
Processed prompts:  77%|███████▋  | 393/512 [00:26<00:00, 140.71it/s, est. speed input: 233.94 toks/s, output: 3743.00 toks/s]
Processed prompts:  80%|███████▉  | 409/512 [00:26<00:00, 143.94it/s, est. speed input: 242.53 toks/s, output: 3880.45 toks/s]
Processed prompts:  83%|████████▎ | 425/512 [00:27<00:00, 136.31it/s, est. speed input: 250.76 toks/s, output: 4012.20 toks/s]
Processed prompts:  86%|████████▌ | 440/512 [00:27<00:00, 130.34it/s, est. speed input: 258.38 toks/s, output: 4134.09 toks/s]
Processed prompts:  89%|████████▊ | 454/512 [00:27<00:00, 117.84it/s, est. speed input: 265.13 toks/s, output: 4242.03 toks/s]
Processed prompts:  91%|█████████ | 467/512 [00:27<00:00, 110.66it/s, est. speed input: 271.35 toks/s, output: 4341.57 toks/s]
Processed prompts:  94%|█████████▎| 479/512 [00:27<00:00, 97.18it/s, est. speed input: 276.63 toks/s, output: 4426.09 toks/s] 
Processed prompts:  96%|█████████▌| 490/512 [00:27<00:00, 73.10it/s, est. speed input: 280.32 toks/s, output: 4485.05 toks/s]
Processed prompts:  97%|█████████▋| 499/512 [00:32<00:01,  8.42it/s, est. speed input: 247.15 toks/s, output: 3954.41 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:32<00:00,  8.42it/s, est. speed input: 253.10 toks/s, output: 4049.56 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:32<00:00, 15.82it/s, est. speed input: 253.10 toks/s, output: 4049.56 toks/s]
[rank0]:[W127 17:43:56.239466575 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 70.2s

测试结果:
  Requests/s:   15.78
  Tokens/s:     4292.53
  Total Reqs:   512
  Elapsed:      32.44s

  [Decode 分析]
  Total Decode Tokens:  131072
  Decode Tokens/s:      4040.02


------------------------------------------------------------
  生成 CSV: Llama3.2-3B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_6/Llama3.2-3B-FP8_decode.csv

预览:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
64,16,64,64,256,256,7.8878,2145.4681,8.1138
128,16,128,128,256,256,12.2598,3334.6667,10.4406
256,16,256,256,256,256,15.4442,4200.8323,16.5758
512,16,512,512,256,256,15.7813,4292.5262,32.4434

------------------------------------------------------------

[INFO] 完成: 4 成功, 0 失败

============================================================
  Llama3.2-3B-FP8 | cuSPARSELt (2_8) | decode
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_8
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_8

============================================================
[1/4] 测试 M=64
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-3B-FP8                                 │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 64
│   M_prefill     = 1024 (= 64 x 16)
│   M_decode      = 64
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 64
│   --max-num-seqs           = 64
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:44:02 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:44:02 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2887131) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2887131) WARNING 01-27 17:44:37 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 7.95 requests/s, 2163.66 total tokens/s, 2036.38 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384


─── STDERR ───
[2026-01-27 17:44:02] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:44:02] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-27 17:44:02] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-27 17:44:02] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:44:02] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:44:02] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:44:02] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:44:02] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:44:02] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-27 17:44:02] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:44:02] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:44:02] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:44:02] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:44:02] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:44:05] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:44:05] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-27 17:44:05] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-27 17:44:05] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:44:05] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:44:05] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:44:05] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:44:05] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:44:05] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-27 17:44:05] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:44:05] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:44:05] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:44:05] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:44:05] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2887131) [2026-01-27 17:44:06] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2887131) [2026-01-27 17:44:06] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2887131) [2026-01-27 17:44:06] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2887131) [2026-01-27 17:44:06] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=2887131) [2026-01-27 17:44:06] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=2887131) [2026-01-27 17:44:06] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=2887131) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2887131) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:23<00:00, 23.70s/it]
(EngineCore_DP0 pid=2887131) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:23<00:00, 23.70s/it]
(EngineCore_DP0 pid=2887131) 
(EngineCore_DP0 pid=2887131) [2026-01-27 17:44:30] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 4608] -> 1D uint8
(EngineCore_DP0 pid=2887131) [2026-01-27 17:44:30] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14745600 bytes
(EngineCore_DP0 pid=2887131) [2026-01-27 17:44:30] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 4608] -> 1D uint8
(EngineCore_DP0 pid=2887131) [2026-01-27 17:44:30] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8847360 bytes
(EngineCore_DP0 pid=2887131) [2026-01-27 17:44:30] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 4608] -> 1D uint8
(EngineCore_DP0 pid=2887131) [2026-01-27 17:44:30] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 47185920 bytes
(EngineCore_DP0 pid=2887131) [2026-01-27 17:44:30] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 12288] -> 1D uint8
(EngineCore_DP0 pid=2887131) [2026-01-27 17:44:31] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 23592960 bytes
(EngineCore_DP0 pid=2887131) 2026-01-27 17:44:36,702 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2887131) 2026-01-27 17:44:36,716 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 64/64 [00:00<00:00, 11860.36it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 1/64 [00:07<08:13,  7.84s/it, est. speed input: 2.04 toks/s, output: 32.67 toks/s]
Processed prompts:  28%|██▊       | 18/64 [00:07<00:14,  3.17it/s, est. speed input: 36.26 toks/s, output: 580.14 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:08<00:00,  3.17it/s, est. speed input: 127.37 toks/s, output: 2037.98 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:08<00:00,  7.96it/s, est. speed input: 127.37 toks/s, output: 2037.98 toks/s]
[rank0]:[W127 17:44:45.616577639 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 49.4s

测试结果:
  Requests/s:   7.95
  Tokens/s:     2163.66
  Total Reqs:   64
  Elapsed:      8.05s

  [Decode 分析]
  Total Decode Tokens:  16384
  Decode Tokens/s:      2036.38

============================================================
[2/4] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-3B-FP8                                 │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 2048 (= 128 x 16)
│   M_decode      = 128
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 128
│   --max-num-seqs           = 128
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:44:51 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:44:51 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2888018) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2888018) WARNING 01-27 17:45:25 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 12.11 requests/s, 3295.03 total tokens/s, 3101.21 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768


─── STDERR ───
[2026-01-27 17:44:51] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:44:51] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-27 17:44:51] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-27 17:44:51] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:44:51] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:44:51] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:44:51] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:44:51] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:44:51] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-27 17:44:51] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:44:51] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:44:51] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:44:51] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:44:51] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:44:55] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:44:55] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-27 17:44:55] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-27 17:44:55] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:44:55] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:44:55] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:44:55] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:44:55] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:44:55] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-27 17:44:55] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:44:55] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:44:55] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:44:55] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:44:55] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2888018) [2026-01-27 17:44:56] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2888018) [2026-01-27 17:44:56] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2888018) [2026-01-27 17:44:56] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2888018) [2026-01-27 17:44:56] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=2888018) [2026-01-27 17:44:56] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=2888018) [2026-01-27 17:44:56] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=2888018) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2888018) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:23<00:00, 23.03s/it]
(EngineCore_DP0 pid=2888018) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:23<00:00, 23.04s/it]
(EngineCore_DP0 pid=2888018) 
(EngineCore_DP0 pid=2888018) [2026-01-27 17:45:19] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 4608] -> 1D uint8
(EngineCore_DP0 pid=2888018) [2026-01-27 17:45:19] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14745600 bytes
(EngineCore_DP0 pid=2888018) [2026-01-27 17:45:19] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 4608] -> 1D uint8
(EngineCore_DP0 pid=2888018) [2026-01-27 17:45:19] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8847360 bytes
(EngineCore_DP0 pid=2888018) [2026-01-27 17:45:19] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 4608] -> 1D uint8
(EngineCore_DP0 pid=2888018) [2026-01-27 17:45:19] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 47185920 bytes
(EngineCore_DP0 pid=2888018) [2026-01-27 17:45:19] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 12288] -> 1D uint8
(EngineCore_DP0 pid=2888018) [2026-01-27 17:45:19] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 23592960 bytes
(EngineCore_DP0 pid=2888018) 2026-01-27 17:45:25,060 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2888018) 2026-01-27 17:45:25,071 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 4676.66it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:10<21:14, 10.03s/it, est. speed input: 1.59 toks/s, output: 25.51 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:10<00:44,  2.48it/s, est. speed input: 28.37 toks/s, output: 453.93 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:10<00:09,  8.31it/s, est. speed input: 74.91 toks/s, output: 1198.61 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:10<00:03, 15.39it/s, est. speed input: 115.76 toks/s, output: 1852.21 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:10<00:01, 23.77it/s, est. speed input: 151.35 toks/s, output: 2421.53 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:10<00:00, 23.77it/s, est. speed input: 194.36 toks/s, output: 3109.74 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:10<00:00, 12.15it/s, est. speed input: 194.36 toks/s, output: 3109.74 toks/s]
[rank0]:[W127 17:45:36.623849432 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 50.9s

测试结果:
  Requests/s:   12.11
  Tokens/s:     3295.03
  Total Reqs:   128
  Elapsed:      10.57s

  [Decode 分析]
  Total Decode Tokens:  32768
  Decode Tokens/s:      3101.21

============================================================
[3/4] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-3B-FP8                                 │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 4096 (= 256 x 16)
│   M_decode      = 256
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 256
│   --max-num-seqs           = 256
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:45:42 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:45:42 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2888909) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2888909) WARNING 01-27 17:46:16 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 15.16 requests/s, 4122.33 total tokens/s, 3879.84 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536


─── STDERR ───
[2026-01-27 17:45:42] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:45:42] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-27 17:45:42] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-27 17:45:42] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:45:42] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:45:42] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:45:42] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:45:42] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:45:42] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-27 17:45:42] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:45:42] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:45:42] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:45:42] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:45:42] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:45:45] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:45:45] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-27 17:45:45] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-27 17:45:45] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:45:45] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:45:45] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:45:45] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:45:45] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:45:45] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-27 17:45:45] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:45:45] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:45:45] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:45:45] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:45:45] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2888909) [2026-01-27 17:45:46] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2888909) [2026-01-27 17:45:46] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2888909) [2026-01-27 17:45:46] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2888909) [2026-01-27 17:45:46] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=2888909) [2026-01-27 17:45:46] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=2888909) [2026-01-27 17:45:46] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=2888909) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2888909) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:22<00:00, 22.83s/it]
(EngineCore_DP0 pid=2888909) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:22<00:00, 22.83s/it]
(EngineCore_DP0 pid=2888909) 
(EngineCore_DP0 pid=2888909) [2026-01-27 17:46:10] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 4608] -> 1D uint8
(EngineCore_DP0 pid=2888909) [2026-01-27 17:46:10] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14745600 bytes
(EngineCore_DP0 pid=2888909) [2026-01-27 17:46:10] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 4608] -> 1D uint8
(EngineCore_DP0 pid=2888909) [2026-01-27 17:46:10] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8847360 bytes
(EngineCore_DP0 pid=2888909) [2026-01-27 17:46:10] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 4608] -> 1D uint8
(EngineCore_DP0 pid=2888909) [2026-01-27 17:46:10] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 47185920 bytes
(EngineCore_DP0 pid=2888909) [2026-01-27 17:46:10] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 12288] -> 1D uint8
(EngineCore_DP0 pid=2888909) [2026-01-27 17:46:10] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 23592960 bytes
(EngineCore_DP0 pid=2888909) 2026-01-27 17:46:15,721 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2888909) 2026-01-27 17:46:15,731 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 4298.25it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:14<1:02:22, 14.67s/it, est. speed input: 1.09 toks/s, output: 17.44 toks/s]
Processed prompts:   7%|▋         | 18/256 [00:14<02:20,  1.70it/s, est. speed input: 19.41 toks/s, output: 310.52 toks/s]
Processed prompts:  19%|█▉        | 48/256 [00:14<00:36,  5.68it/s, est. speed input: 51.21 toks/s, output: 819.42 toks/s]
Processed prompts:  29%|██▉       | 75/256 [00:15<00:17, 10.53it/s, est. speed input: 79.19 toks/s, output: 1267.03 toks/s]
Processed prompts:  38%|███▊      | 98/256 [00:15<00:09, 15.97it/s, est. speed input: 102.40 toks/s, output: 1638.47 toks/s]
Processed prompts:  46%|████▌     | 118/256 [00:15<00:06, 22.27it/s, est. speed input: 122.34 toks/s, output: 1957.45 toks/s]
Processed prompts:  53%|█████▎    | 136/256 [00:15<00:04, 29.51it/s, est. speed input: 139.94 toks/s, output: 2238.97 toks/s]
Processed prompts:  59%|█████▉    | 152/256 [00:15<00:02, 37.57it/s, est. speed input: 155.33 toks/s, output: 2485.36 toks/s]
Processed prompts:  66%|██████▌   | 168/256 [00:15<00:01, 47.49it/s, est. speed input: 170.56 toks/s, output: 2728.90 toks/s]
Processed prompts:  72%|███████▏  | 184/256 [00:15<00:01, 57.17it/s, est. speed input: 185.21 toks/s, output: 2963.33 toks/s]
Processed prompts:  78%|███████▊  | 199/256 [00:16<00:00, 66.63it/s, est. speed input: 198.72 toks/s, output: 3179.54 toks/s]
Processed prompts:  83%|████████▎ | 213/256 [00:16<00:00, 72.77it/s, est. speed input: 210.81 toks/s, output: 3372.91 toks/s]
Processed prompts:  88%|████████▊ | 226/256 [00:16<00:00, 77.71it/s, est. speed input: 221.82 toks/s, output: 3549.06 toks/s]
Processed prompts:  93%|█████████▎| 238/256 [00:16<00:00, 78.26it/s, est. speed input: 231.46 toks/s, output: 3703.37 toks/s]
Processed prompts:  97%|█████████▋| 249/256 [00:16<00:00, 69.78it/s, est. speed input: 239.13 toks/s, output: 3826.09 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:16<00:00, 69.78it/s, est. speed input: 243.37 toks/s, output: 3893.92 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:16<00:00, 15.21it/s, est. speed input: 243.37 toks/s, output: 3893.92 toks/s]
[rank0]:[W127 17:46:33.503380054 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 56.9s

测试结果:
  Requests/s:   15.16
  Tokens/s:     4122.33
  Total Reqs:   256
  Elapsed:      16.89s

  [Decode 分析]
  Total Decode Tokens:  65536
  Decode Tokens/s:      3879.84

============================================================
[4/4] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-3B-FP8                                 │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 8192 (= 512 x 16)
│   M_decode      = 512
│   batched_tokens = 512 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 512
│   --max-num-seqs           = 512
│   --max-model-len          = 272
│   --max-num-batched-tokens = 512
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:46:39 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:46:39 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2889877) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2889877) WARNING 01-27 17:47:13 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 15.34 requests/s, 4172.46 total tokens/s, 3927.02 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072


─── STDERR ───
[2026-01-27 17:46:39] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:46:39] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-27 17:46:39] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-27 17:46:39] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:46:39] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:46:39] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:46:39] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:46:39] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:46:39] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-27 17:46:39] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:46:39] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:46:39] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:46:39] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:46:39] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:46:42] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:46:42] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-27 17:46:42] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-27 17:46:42] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:46:42] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:46:42] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:46:42] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:46:42] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:46:42] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-27 17:46:42] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:46:42] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:46:42] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:46:42] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:46:42] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2889877) [2026-01-27 17:46:43] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2889877) [2026-01-27 17:46:43] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2889877) [2026-01-27 17:46:43] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2889877) [2026-01-27 17:46:43] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=2889877) [2026-01-27 17:46:43] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=2889877) [2026-01-27 17:46:43] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=2889877) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2889877) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:22<00:00, 22.79s/it]
(EngineCore_DP0 pid=2889877) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:22<00:00, 22.79s/it]
(EngineCore_DP0 pid=2889877) 
(EngineCore_DP0 pid=2889877) [2026-01-27 17:47:07] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 4608] -> 1D uint8
(EngineCore_DP0 pid=2889877) [2026-01-27 17:47:07] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14745600 bytes
(EngineCore_DP0 pid=2889877) [2026-01-27 17:47:07] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 4608] -> 1D uint8
(EngineCore_DP0 pid=2889877) [2026-01-27 17:47:07] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8847360 bytes
(EngineCore_DP0 pid=2889877) [2026-01-27 17:47:07] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 4608] -> 1D uint8
(EngineCore_DP0 pid=2889877) [2026-01-27 17:47:07] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 47185920 bytes
(EngineCore_DP0 pid=2889877) [2026-01-27 17:47:07] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 12288] -> 1D uint8
(EngineCore_DP0 pid=2889877) [2026-01-27 17:47:07] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 23592960 bytes
(EngineCore_DP0 pid=2889877) 2026-01-27 17:47:12,631 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2889877) 2026-01-27 17:47:12,642 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 13692.11it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:24<3:31:29, 24.83s/it, est. speed input: 0.64 toks/s, output: 10.31 toks/s]
Processed prompts:   1%|          | 5/512 [00:25<31:41,  3.75s/it, est. speed input: 3.19 toks/s, output: 51.04 toks/s]  
Processed prompts:   7%|▋         | 36/512 [00:25<02:54,  2.73it/s, est. speed input: 22.75 toks/s, output: 363.99 toks/s]
Processed prompts:  13%|█▎        | 65/512 [00:25<01:16,  5.86it/s, est. speed input: 40.70 toks/s, output: 651.24 toks/s]
Processed prompts:  18%|█▊        | 93/512 [00:25<00:42,  9.96it/s, est. speed input: 57.76 toks/s, output: 924.12 toks/s]
Processed prompts:  23%|██▎       | 119/512 [00:25<00:26, 14.92it/s, est. speed input: 73.28 toks/s, output: 1172.47 toks/s]
Processed prompts:  28%|██▊       | 143/512 [00:26<00:17, 21.31it/s, est. speed input: 87.66 toks/s, output: 1402.48 toks/s]
Processed prompts:  32%|███▏      | 166/512 [00:26<00:11, 29.40it/s, est. speed input: 101.32 toks/s, output: 1621.08 toks/s]
Processed prompts:  37%|███▋      | 187/512 [00:26<00:08, 38.79it/s, est. speed input: 113.66 toks/s, output: 1818.53 toks/s]
Processed prompts:  40%|████      | 207/512 [00:26<00:06, 49.81it/s, est. speed input: 125.30 toks/s, output: 2004.76 toks/s]
Processed prompts:  44%|████▍     | 226/512 [00:26<00:04, 62.13it/s, est. speed input: 136.25 toks/s, output: 2179.96 toks/s]
Processed prompts:  48%|████▊     | 244/512 [00:26<00:03, 75.54it/s, est. speed input: 146.54 toks/s, output: 2344.72 toks/s]
Processed prompts:  54%|█████▎    | 274/512 [00:26<00:02, 94.93it/s, est. speed input: 163.39 toks/s, output: 2614.31 toks/s]
Processed prompts:  59%|█████▉    | 302/512 [00:26<00:01, 114.86it/s, est. speed input: 179.12 toks/s, output: 2865.87 toks/s]
Processed prompts:  64%|██████▍   | 327/512 [00:27<00:01, 125.32it/s, est. speed input: 192.81 toks/s, output: 3085.03 toks/s]
Processed prompts:  68%|██████▊   | 348/512 [00:27<00:01, 133.08it/s, est. speed input: 204.21 toks/s, output: 3267.37 toks/s]
Processed prompts:  72%|███████▏  | 367/512 [00:27<00:01, 137.59it/s, est. speed input: 214.38 toks/s, output: 3430.09 toks/s]
Processed prompts:  75%|███████▌  | 384/512 [00:27<00:00, 140.56it/s, est. speed input: 223.39 toks/s, output: 3574.23 toks/s]
Processed prompts:  78%|███████▊  | 400/512 [00:27<00:00, 139.31it/s, est. speed input: 231.70 toks/s, output: 3707.26 toks/s]
Processed prompts:  81%|████████▏ | 416/512 [00:27<00:00, 143.20it/s, est. speed input: 240.08 toks/s, output: 3841.23 toks/s]
Processed prompts:  84%|████████▍ | 432/512 [00:27<00:00, 135.92it/s, est. speed input: 248.11 toks/s, output: 3969.83 toks/s]
Processed prompts:  87%|████████▋ | 447/512 [00:28<00:00, 120.96it/s, est. speed input: 255.25 toks/s, output: 4084.02 toks/s]
Processed prompts:  90%|████████▉ | 460/512 [00:28<00:00, 115.84it/s, est. speed input: 261.49 toks/s, output: 4183.89 toks/s]
Processed prompts:  92%|█████████▏| 473/512 [00:28<00:00, 103.60it/s, est. speed input: 267.34 toks/s, output: 4277.40 toks/s]
Processed prompts:  95%|█████████▍| 484/512 [00:28<00:00, 84.32it/s, est. speed input: 271.57 toks/s, output: 4345.10 toks/s] 
Processed prompts:  96%|█████████▋| 494/512 [00:28<00:00, 67.47it/s, est. speed input: 274.84 toks/s, output: 4397.41 toks/s]
Processed prompts:  98%|█████████▊| 502/512 [00:33<00:01,  7.62it/s, est. speed input: 241.69 toks/s, output: 3867.06 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:33<00:00, 10.21it/s, est. speed input: 245.72 toks/s, output: 3931.55 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:33<00:00, 10.21it/s, est. speed input: 245.72 toks/s, output: 3931.55 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:33<00:00, 15.36it/s, est. speed input: 245.72 toks/s, output: 3931.55 toks/s]
[rank0]:[W127 17:47:46.942740286 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 73.4s

测试结果:
  Requests/s:   15.34
  Tokens/s:     4172.46
  Total Reqs:   512
  Elapsed:      33.38s

  [Decode 分析]
  Total Decode Tokens:  131072
  Decode Tokens/s:      3927.02


------------------------------------------------------------
  生成 CSV: Llama3.2-3B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_8/Llama3.2-3B-FP8_decode.csv

预览:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
64,16,64,64,256,256,7.9546,2163.6557,8.0456
128,16,128,128,256,256,12.1141,3295.0316,10.5662
256,16,256,256,256,256,15.1556,4122.3296,16.8914
512,16,512,512,256,256,15.3399,4172.4598,33.3770

------------------------------------------------------------

[INFO] 完成: 4 成功, 0 失败

============================================================
  Llama3.2-3B-FP8 | cuSPARSELt (2_10) | decode
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-3B-FP8-SlideSparse-2_10
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_10

============================================================
[1/4] 测试 M=64
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-3B-FP8                                 │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 64
│   M_prefill     = 1024 (= 64 x 16)
│   M_decode      = 64
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 64
│   --max-num-seqs           = 64
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:47:52 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:47:52 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2891099) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2891099) WARNING 01-27 17:48:29 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 7.20 requests/s, 1958.72 total tokens/s, 1843.50 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384


─── STDERR ───
[2026-01-27 17:47:52] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:47:52] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-27 17:47:52] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-27 17:47:52] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:47:52] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:47:52] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:47:52] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:47:52] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:47:52] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-27 17:47:52] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:47:52] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:47:52] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:47:52] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:47:52] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:47:56] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:47:56] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-27 17:47:56] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-27 17:47:56] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:47:56] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:47:56] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:47:56] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:47:56] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:47:56] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-27 17:47:56] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:47:56] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:47:56] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:47:56] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:47:56] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2891099) [2026-01-27 17:47:57] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2891099) [2026-01-27 17:47:57] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2891099) [2026-01-27 17:47:57] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2891099) [2026-01-27 17:47:57] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=2891099) [2026-01-27 17:47:57] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=2891099) [2026-01-27 17:47:57] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=2891099) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2891099) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:25<00:00, 25.31s/it]
(EngineCore_DP0 pid=2891099) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:25<00:00, 25.31s/it]
(EngineCore_DP0 pid=2891099) 
(EngineCore_DP0 pid=2891099) [2026-01-27 17:48:23] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 4928] -> 1D uint8
(EngineCore_DP0 pid=2891099) [2026-01-27 17:48:23] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15810560 bytes
(EngineCore_DP0 pid=2891099) [2026-01-27 17:48:23] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 4928] -> 1D uint8
(EngineCore_DP0 pid=2891099) [2026-01-27 17:48:23] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9486336 bytes
(EngineCore_DP0 pid=2891099) [2026-01-27 17:48:23] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 4928] -> 1D uint8
(EngineCore_DP0 pid=2891099) [2026-01-27 17:48:23] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 50593792 bytes
(EngineCore_DP0 pid=2891099) [2026-01-27 17:48:23] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 13120] -> 1D uint8
(EngineCore_DP0 pid=2891099) [2026-01-27 17:48:23] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 25214976 bytes
(EngineCore_DP0 pid=2891099) 2026-01-27 17:48:29,150 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2891099) 2026-01-27 17:48:29,177 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 64/64 [00:00<00:00, 11432.03it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 1/64 [00:08<09:06,  8.67s/it, est. speed input: 1.85 toks/s, output: 29.53 toks/s]
Processed prompts:  55%|█████▍    | 35/64 [00:08<00:05,  5.59it/s, est. speed input: 63.58 toks/s, output: 1017.32 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:08<00:00,  5.59it/s, est. speed input: 115.30 toks/s, output: 1844.87 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:08<00:00,  7.21it/s, est. speed input: 115.30 toks/s, output: 1844.87 toks/s]
[rank0]:[W127 17:48:38.905178331 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 52.0s

测试结果:
  Requests/s:   7.20
  Tokens/s:     1958.72
  Total Reqs:   64
  Elapsed:      8.89s

  [Decode 分析]
  Total Decode Tokens:  16384
  Decode Tokens/s:      1843.50

============================================================
[2/4] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-3B-FP8                                 │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 2048 (= 128 x 16)
│   M_decode      = 128
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 128
│   --max-num-seqs           = 128
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:48:44 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:48:44 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2892006) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2892006) WARNING 01-27 17:49:20 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 11.23 requests/s, 3054.20 total tokens/s, 2874.55 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768


─── STDERR ───
[2026-01-27 17:48:44] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:48:44] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-27 17:48:44] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-27 17:48:44] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:48:44] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:48:44] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:48:44] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:48:44] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:48:44] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-27 17:48:44] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:48:44] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:48:44] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:48:44] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:48:44] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:48:48] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:48:48] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-27 17:48:48] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-27 17:48:48] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:48:48] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:48:48] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:48:48] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:48:48] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:48:48] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-27 17:48:48] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:48:48] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:48:48] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:48:48] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:48:48] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2892006) [2026-01-27 17:48:49] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2892006) [2026-01-27 17:48:49] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2892006) [2026-01-27 17:48:49] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2892006) [2026-01-27 17:48:49] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=2892006) [2026-01-27 17:48:49] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=2892006) [2026-01-27 17:48:49] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=2892006) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2892006) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:24<00:00, 24.42s/it]
(EngineCore_DP0 pid=2892006) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:24<00:00, 24.42s/it]
(EngineCore_DP0 pid=2892006) 
(EngineCore_DP0 pid=2892006) [2026-01-27 17:49:14] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 4928] -> 1D uint8
(EngineCore_DP0 pid=2892006) [2026-01-27 17:49:14] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15810560 bytes
(EngineCore_DP0 pid=2892006) [2026-01-27 17:49:14] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 4928] -> 1D uint8
(EngineCore_DP0 pid=2892006) [2026-01-27 17:49:14] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9486336 bytes
(EngineCore_DP0 pid=2892006) [2026-01-27 17:49:14] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 4928] -> 1D uint8
(EngineCore_DP0 pid=2892006) [2026-01-27 17:49:14] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 50593792 bytes
(EngineCore_DP0 pid=2892006) [2026-01-27 17:49:14] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 13120] -> 1D uint8
(EngineCore_DP0 pid=2892006) [2026-01-27 17:49:14] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 25214976 bytes
(EngineCore_DP0 pid=2892006) 2026-01-27 17:49:19,641 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2892006) 2026-01-27 17:49:19,652 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 12491.47it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:10<22:59, 10.86s/it, est. speed input: 1.47 toks/s, output: 23.57 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:10<00:32,  3.20it/s, est. speed input: 36.43 toks/s, output: 582.93 toks/s]
Processed prompts:  42%|████▏     | 54/128 [00:11<00:08,  8.37it/s, est. speed input: 77.80 toks/s, output: 1244.81 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:11<00:03, 14.44it/s, est. speed input: 112.75 toks/s, output: 1804.05 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:11<00:01, 21.87it/s, est. speed input: 144.23 toks/s, output: 2307.64 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:11<00:00, 21.87it/s, est. speed input: 179.83 toks/s, output: 2877.34 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:11<00:00, 11.24it/s, est. speed input: 179.83 toks/s, output: 2877.34 toks/s]
[rank0]:[W127 17:49:32.061455539 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 53.1s

测试结果:
  Requests/s:   11.23
  Tokens/s:     3054.20
  Total Reqs:   128
  Elapsed:      11.40s

  [Decode 分析]
  Total Decode Tokens:  32768
  Decode Tokens/s:      2874.55

============================================================
[3/4] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-3B-FP8                                 │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 4096 (= 256 x 16)
│   M_decode      = 256
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 256
│   --max-num-seqs           = 256
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:49:37 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:49:37 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2892935) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2892935) WARNING 01-27 17:50:13 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 14.32 requests/s, 3894.99 total tokens/s, 3665.87 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536


─── STDERR ───
[2026-01-27 17:49:37] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:49:37] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-27 17:49:37] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-27 17:49:37] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:49:37] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:49:37] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:49:37] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:49:37] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:49:37] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-27 17:49:37] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:49:37] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:49:37] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:49:37] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:49:37] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:49:41] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:49:41] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-27 17:49:41] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-27 17:49:41] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:49:41] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:49:41] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:49:41] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:49:41] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:49:41] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-27 17:49:41] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:49:41] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:49:41] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:49:41] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:49:41] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2892935) [2026-01-27 17:49:42] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2892935) [2026-01-27 17:49:42] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2892935) [2026-01-27 17:49:42] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2892935) [2026-01-27 17:49:42] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=2892935) [2026-01-27 17:49:42] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=2892935) [2026-01-27 17:49:42] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=2892935) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2892935) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:24<00:00, 24.39s/it]
(EngineCore_DP0 pid=2892935) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:24<00:00, 24.39s/it]
(EngineCore_DP0 pid=2892935) 
(EngineCore_DP0 pid=2892935) [2026-01-27 17:50:07] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 4928] -> 1D uint8
(EngineCore_DP0 pid=2892935) [2026-01-27 17:50:07] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15810560 bytes
(EngineCore_DP0 pid=2892935) [2026-01-27 17:50:07] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 4928] -> 1D uint8
(EngineCore_DP0 pid=2892935) [2026-01-27 17:50:07] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9486336 bytes
(EngineCore_DP0 pid=2892935) [2026-01-27 17:50:07] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 4928] -> 1D uint8
(EngineCore_DP0 pid=2892935) [2026-01-27 17:50:07] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 50593792 bytes
(EngineCore_DP0 pid=2892935) [2026-01-27 17:50:07] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 13120] -> 1D uint8
(EngineCore_DP0 pid=2892935) [2026-01-27 17:50:07] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 25214976 bytes
(EngineCore_DP0 pid=2892935) 2026-01-27 17:50:12,458 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2892935) 2026-01-27 17:50:12,468 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 12805.97it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:15<1:05:47, 15.48s/it, est. speed input: 1.03 toks/s, output: 16.54 toks/s]
Processed prompts:   7%|▋         | 19/256 [00:15<02:19,  1.70it/s, est. speed input: 19.42 toks/s, output: 310.72 toks/s]
Processed prompts:  19%|█▉        | 48/256 [00:15<00:38,  5.35it/s, est. speed input: 48.58 toks/s, output: 777.23 toks/s]
Processed prompts:  29%|██▉       | 75/256 [00:15<00:18,  9.95it/s, est. speed input: 75.10 toks/s, output: 1201.61 toks/s]
Processed prompts:  38%|███▊      | 98/256 [00:16<00:10, 15.15it/s, est. speed input: 97.18 toks/s, output: 1554.81 toks/s]
Processed prompts:  46%|████▌     | 118/256 [00:16<00:06, 21.07it/s, est. speed input: 116.03 toks/s, output: 1856.43 toks/s]
Processed prompts:  53%|█████▎    | 136/256 [00:16<00:04, 27.93it/s, est. speed input: 132.71 toks/s, output: 2123.38 toks/s]
Processed prompts:  59%|█████▉    | 152/256 [00:16<00:02, 35.49it/s, est. speed input: 147.28 toks/s, output: 2356.48 toks/s]
Processed prompts:  65%|██████▌   | 167/256 [00:16<00:02, 44.21it/s, est. speed input: 160.77 toks/s, output: 2572.39 toks/s]
Processed prompts:  71%|███████   | 182/256 [00:16<00:01, 54.61it/s, est. speed input: 174.14 toks/s, output: 2786.19 toks/s]
Processed prompts:  77%|███████▋  | 197/256 [00:16<00:00, 63.81it/s, est. speed input: 186.99 toks/s, output: 2991.86 toks/s]
Processed prompts:  82%|████████▏ | 211/256 [00:17<00:00, 68.72it/s, est. speed input: 198.38 toks/s, output: 3174.01 toks/s]
Processed prompts:  87%|████████▋ | 223/256 [00:17<00:00, 73.50it/s, est. speed input: 208.08 toks/s, output: 3329.24 toks/s]
Processed prompts:  92%|█████████▏| 235/256 [00:17<00:00, 74.79it/s, est. speed input: 217.34 toks/s, output: 3477.45 toks/s]
Processed prompts:  96%|█████████▌| 246/256 [00:17<00:00, 64.86it/s, est. speed input: 224.47 toks/s, output: 3591.45 toks/s]
Processed prompts: 100%|█████████▉| 255/256 [00:17<00:00, 57.05it/s, est. speed input: 229.78 toks/s, output: 3676.50 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:17<00:00, 57.05it/s, est. speed input: 229.39 toks/s, output: 3670.25 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:17<00:00, 14.34it/s, est. speed input: 229.39 toks/s, output: 3670.25 toks/s]
[rank0]:[W127 17:50:31.214860511 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 59.2s

测试结果:
  Requests/s:   14.32
  Tokens/s:     3894.99
  Total Reqs:   256
  Elapsed:      17.88s

  [Decode 分析]
  Total Decode Tokens:  65536
  Decode Tokens/s:      3665.87

============================================================
[4/4] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-3B-FP8                                 │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 8192 (= 512 x 16)
│   M_decode      = 512
│   batched_tokens = 512 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 512
│   --max-num-seqs           = 512
│   --max-model-len          = 272
│   --max-num-batched-tokens = 512
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 17:50:37 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 17:50:37 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2893955) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2893955) WARNING 01-27 17:51:12 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 14.73 requests/s, 4007.27 total tokens/s, 3771.55 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072


─── STDERR ───
[2026-01-27 17:50:36] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:50:36] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-27 17:50:36] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-27 17:50:36] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:50:36] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:50:36] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:50:36] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:50:36] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:50:36] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-27 17:50:36] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:50:36] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:50:36] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:50:36] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:50:36] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 17:50:40] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 17:50:40] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-3B-FP8'
[2026-01-27 17:50:40] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-3B-FP8
[2026-01-27 17:50:40] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:50:40] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:50:40] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:50:40] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:50:40] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-3B-FP8
[2026-01-27 17:50:40] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-3B-FP8'
[2026-01-27 17:50:40] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 17:50:40] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 17:50:40] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 17:50:40] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 17:50:40] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2893955) [2026-01-27 17:50:41] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2893955) [2026-01-27 17:50:41] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2893955) [2026-01-27 17:50:41] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2893955) [2026-01-27 17:50:41] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=2893955) [2026-01-27 17:50:41] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Llama3.2-3B-FP8
(EngineCore_DP0 pid=2893955) [2026-01-27 17:50:41] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=2893955) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2893955) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:24<00:00, 24.64s/it]
(EngineCore_DP0 pid=2893955) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:24<00:00, 24.64s/it]
(EngineCore_DP0 pid=2893955) 
(EngineCore_DP0 pid=2893955) [2026-01-27 17:51:06] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 4928] -> 1D uint8
(EngineCore_DP0 pid=2893955) [2026-01-27 17:51:06] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15810560 bytes
(EngineCore_DP0 pid=2893955) [2026-01-27 17:51:06] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 4928] -> 1D uint8
(EngineCore_DP0 pid=2893955) [2026-01-27 17:51:06] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9486336 bytes
(EngineCore_DP0 pid=2893955) [2026-01-27 17:51:06] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [16384, 4928] -> 1D uint8
(EngineCore_DP0 pid=2893955) [2026-01-27 17:51:06] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 50593792 bytes
(EngineCore_DP0 pid=2893955) [2026-01-27 17:51:06] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3072, 13120] -> 1D uint8
(EngineCore_DP0 pid=2893955) [2026-01-27 17:51:06] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 25214976 bytes
(EngineCore_DP0 pid=2893955) 2026-01-27 17:51:12,104 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2893955) 2026-01-27 17:51:12,115 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 5949.30it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:25<3:36:58, 25.48s/it, est. speed input: 0.63 toks/s, output: 10.05 toks/s]
Processed prompts:   0%|          | 2/512 [00:25<1:30:25, 10.64s/it, est. speed input: 1.24 toks/s, output: 19.90 toks/s]
Processed prompts:   6%|▋         | 33/512 [00:25<03:12,  2.49it/s, est. speed input: 20.33 toks/s, output: 325.20 toks/s]
Processed prompts:  12%|█▏        | 63/512 [00:26<01:19,  5.67it/s, est. speed input: 38.46 toks/s, output: 615.39 toks/s]
Processed prompts:  18%|█▊        | 91/512 [00:26<00:43,  9.67it/s, est. speed input: 55.11 toks/s, output: 881.74 toks/s]
Processed prompts:  23%|██▎       | 117/512 [00:26<00:27, 14.51it/s, est. speed input: 70.25 toks/s, output: 1124.00 toks/s]
Processed prompts:  28%|██▊       | 141/512 [00:26<00:17, 20.74it/s, est. speed input: 84.27 toks/s, output: 1348.35 toks/s]
Processed prompts:  32%|███▏      | 164/512 [00:26<00:12, 28.59it/s, est. speed input: 97.58 toks/s, output: 1561.33 toks/s]
Processed prompts:  36%|███▌      | 185/512 [00:27<00:08, 37.59it/s, est. speed input: 109.59 toks/s, output: 1753.39 toks/s]
Processed prompts:  40%|████      | 205/512 [00:27<00:06, 48.30it/s, est. speed input: 120.94 toks/s, output: 1934.98 toks/s]
Processed prompts:  44%|████▍     | 224/512 [00:27<00:04, 60.46it/s, est. speed input: 131.63 toks/s, output: 2106.06 toks/s]
Processed prompts:  50%|█████     | 258/512 [00:27<00:03, 82.76it/s, est. speed input: 150.53 toks/s, output: 2408.41 toks/s]
Processed prompts:  56%|█████▋    | 288/512 [00:27<00:02, 101.70it/s, est. speed input: 167.02 toks/s, output: 2672.34 toks/s]
Processed prompts:  62%|██████▏   | 315/512 [00:27<00:01, 113.80it/s, est. speed input: 181.54 toks/s, output: 2904.64 toks/s]
Processed prompts:  66%|██████▌   | 338/512 [00:27<00:01, 120.42it/s, est. speed input: 193.67 toks/s, output: 3098.74 toks/s]
Processed prompts:  70%|██████▉   | 358/512 [00:28<00:01, 124.92it/s, est. speed input: 204.09 toks/s, output: 3265.45 toks/s]
Processed prompts:  73%|███████▎  | 376/512 [00:28<00:01, 129.31it/s, est. speed input: 213.41 toks/s, output: 3414.54 toks/s]
Processed prompts:  77%|███████▋  | 392/512 [00:28<00:00, 130.27it/s, est. speed input: 221.55 toks/s, output: 3544.76 toks/s]
Processed prompts:  79%|███████▉  | 407/512 [00:28<00:00, 131.43it/s, est. speed input: 229.13 toks/s, output: 3666.04 toks/s]
Processed prompts:  82%|████████▏ | 422/512 [00:28<00:00, 133.78it/s, est. speed input: 236.69 toks/s, output: 3786.98 toks/s]
Processed prompts:  85%|████████▌ | 437/512 [00:28<00:00, 125.17it/s, est. speed input: 243.89 toks/s, output: 3902.32 toks/s]
Processed prompts:  88%|████████▊ | 451/512 [00:28<00:00, 110.50it/s, est. speed input: 250.24 toks/s, output: 4003.83 toks/s]
Processed prompts:  90%|█████████ | 463/512 [00:28<00:00, 103.98it/s, est. speed input: 255.69 toks/s, output: 4091.00 toks/s]
Processed prompts:  93%|█████████▎| 474/512 [00:29<00:00, 88.66it/s, est. speed input: 260.13 toks/s, output: 4162.13 toks/s] 
Processed prompts:  95%|█████████▍| 484/512 [00:29<00:00, 75.37it/s, est. speed input: 263.85 toks/s, output: 4221.56 toks/s]
Processed prompts:  96%|█████████▋| 493/512 [00:29<00:00, 60.65it/s, est. speed input: 266.55 toks/s, output: 4264.80 toks/s]
Processed prompts:  98%|█████████▊| 500/512 [00:34<00:01,  6.21it/s, est. speed input: 231.27 toks/s, output: 3700.27 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:34<00:00,  6.21it/s, est. speed input: 236.32 toks/s, output: 3781.05 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:34<00:00, 14.77it/s, est. speed input: 236.32 toks/s, output: 3781.05 toks/s]
[rank0]:[W127 17:51:47.800520766 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 76.6s

测试结果:
  Requests/s:   14.73
  Tokens/s:     4007.27
  Total Reqs:   512
  Elapsed:      34.75s

  [Decode 分析]
  Total Decode Tokens:  131072
  Decode Tokens/s:      3771.55


------------------------------------------------------------
  生成 CSV: Llama3.2-3B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_10/Llama3.2-3B-FP8_decode.csv

预览:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
64,16,64,64,256,256,7.2012,1958.7237,8.8874
128,16,128,128,256,256,11.2287,3054.2042,11.3994
256,16,256,256,256,256,14.3198,3894.9858,17.8773
512,16,512,512,256,256,14.7326,4007.2747,34.7528

------------------------------------------------------------

[INFO] 完成: 4 成功, 0 失败


============================================================
  Benchmark 完成!
============================================================


总计: 20 成功, 0 失败
============================================================
