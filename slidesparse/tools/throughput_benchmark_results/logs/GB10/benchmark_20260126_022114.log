======================================================================
SlideSparse vLLM Throughput Benchmark Log
Created: 2026-01-26 02:21:14
======================================================================

原始命令:
  /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model llama3.2-1b-int8 --backend all --stage all --sparsity 2_4,2_6,2_10 --M quick

命令行参数:
  --model: llama3.2-1b-int8
  --backend: all
  --sparsity: 2_4,2_6,2_10
  --stage: all
  --M: quick
  --N: None
  --inner-32: False
  --eager: False
  --gpu-id: 0
  --gpu-mem: 0.8
  --dry-run: False
  --list-models: False

硬件信息:
  GPU: GB10
  Compute Capability: cc121
  VRAM: 119.7 GB
  CUDA: 12.9
  Python: py312

Backend 环境变量 (初始状态):
  DISABLE_SLIDESPARSE: 未设置
  USE_CUBLASLT: 未设置
  USE_CUSPARSELT: 未设置
  SPARSITY: 未设置
  INNER_DTYPE_32: 未设置

======================================================================

[WARNING] Backend 不支持，跳过: Llama3.2-1B-INT8 + cutlass
[WARNING]   原因: vLLM CUTLASS INT8 不支持: sm_121 >= sm_100

============================================================
  Llama3.2-1B-INT8 | cuBLASLt | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-INT8
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/GB10_cc121_INT8_py312_cu129_aarch64/cublaslt

============================================================
[1/3] 测试 M=16
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16
│   M_prefill     = 16 (= 1 x 16)
│   M_decode      = 1
│   batched_tokens = 17 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 17
│   --max-num-batched-tokens = 17
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:21:18 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 02:21:18 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=734027) WARNING 01-26 02:21:36 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 78.46 requests/s, 1333.78 total tokens/s, 78.46 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  128


─── STDERR ───
[2026-01-26 02:21:18] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:21:18] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 02:21:18] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 02:21:18] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:21:18] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:21:18] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:21:18] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:21:18] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:21:18] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 02:21:18] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:21:18] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:21:18] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:21:18] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:21:18] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:21:21] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:21:21] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 02:21:21] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 02:21:21] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:21:21] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:21:21] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:21:21] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:21:21] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:21:21] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 02:21:21] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:21:21] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:21:21] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:21:21] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:21:21] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=734027) [2026-01-26 02:21:22] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=734027) [2026-01-26 02:21:22] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=734027) [2026-01-26 02:21:22] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=734027) [2026-01-26 02:21:22] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=734027) [2026-01-26 02:21:22] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=734027) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=734027) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.03s/it]
(EngineCore_DP0 pid=734027) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.03s/it]
(EngineCore_DP0 pid=734027) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=734027) 2026-01-26 02:21:36,000 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=734027) 2026-01-26 02:21:36,007 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 12992.37it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:29,  4.30it/s, est. speed input: 68.87 toks/s, output: 4.30 toks/s]
Processed prompts:   9%|▊         | 11/128 [00:00<00:02, 39.13it/s, est. speed input: 512.90 toks/s, output: 32.05 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:01, 58.14it/s, est. speed input: 742.46 toks/s, output: 46.40 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:00<00:01, 68.99it/s, est. speed input: 878.83 toks/s, output: 54.92 toks/s]
Processed prompts:  31%|███▏      | 40/128 [00:00<00:01, 75.37it/s, est. speed input: 962.87 toks/s, output: 60.18 toks/s]
Processed prompts:  39%|███▉      | 50/128 [00:00<00:00, 80.64it/s, est. speed input: 1033.89 toks/s, output: 64.62 toks/s]
Processed prompts:  47%|████▋     | 60/128 [00:00<00:00, 84.33it/s, est. speed input: 1088.41 toks/s, output: 68.02 toks/s]
Processed prompts:  55%|█████▍    | 70/128 [00:00<00:00, 86.74it/s, est. speed input: 1130.62 toks/s, output: 70.66 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:01<00:00, 88.61it/s, est. speed input: 1165.40 toks/s, output: 72.84 toks/s]
Processed prompts:  70%|███████   | 90/128 [00:01<00:00, 89.91it/s, est. speed input: 1194.07 toks/s, output: 74.63 toks/s]
Processed prompts:  78%|███████▊  | 100/128 [00:01<00:00, 90.65it/s, est. speed input: 1217.44 toks/s, output: 76.09 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:01<00:00, 90.91it/s, est. speed input: 1236.35 toks/s, output: 77.27 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:01<00:00, 91.25it/s, est. speed input: 1253.12 toks/s, output: 78.32 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 91.25it/s, est. speed input: 1263.55 toks/s, output: 78.97 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 78.97it/s, est. speed input: 1263.55 toks/s, output: 78.97 toks/s]
[rank0]:[W126 02:21:38.691379266 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 25.9s

测试结果:
  Requests/s:   78.46
  Tokens/s:     1333.78
  Total Reqs:   128
  Elapsed:      1.63s

  [Prefill 分析]
  Total Prefill Tokens: 2048
  Prefill Tokens/s:     1255.32

============================================================
[2/3] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 128 (= 1 x 128)
│   M_decode      = 1
│   batched_tokens = 129 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 128
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 129
│   --max-num-batched-tokens = 129
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:21:44 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 02:21:44 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=734587) WARNING 01-26 02:22:02 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 67.00 requests/s, 8642.44 total tokens/s, 67.00 output tokens/s
Total num prompt tokens:  16384
Total num output tokens:  128


─── STDERR ───
[2026-01-26 02:21:44] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:21:44] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 02:21:44] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 02:21:44] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:21:44] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:21:44] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:21:44] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:21:44] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:21:44] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 02:21:44] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:21:44] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:21:44] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:21:44] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:21:44] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:21:47] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:21:47] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 02:21:47] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 02:21:47] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:21:47] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:21:47] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:21:47] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:21:47] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:21:47] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 02:21:47] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:21:47] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:21:47] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:21:47] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:21:47] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=734587) [2026-01-26 02:21:48] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=734587) [2026-01-26 02:21:48] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=734587) [2026-01-26 02:21:48] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=734587) [2026-01-26 02:21:48] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=734587) [2026-01-26 02:21:48] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=734587) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=734587) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.08s/it]
(EngineCore_DP0 pid=734587) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.08s/it]
(EngineCore_DP0 pid=734587) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=734587) 2026-01-26 02:22:02,076 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=734587) 2026-01-26 02:22:02,086 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 3795.40it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:29,  4.33it/s, est. speed input: 554.01 toks/s, output: 4.33 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:03, 32.59it/s, est. speed input: 3425.71 toks/s, output: 26.76 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:02, 48.56it/s, est. speed input: 4950.72 toks/s, output: 38.68 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:00<00:01, 58.20it/s, est. speed input: 5889.46 toks/s, output: 46.01 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:00<00:01, 64.41it/s, est. speed input: 6530.34 toks/s, output: 51.02 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:00<00:01, 68.74it/s, est. speed input: 7004.72 toks/s, output: 54.72 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:00<00:01, 71.31it/s, est. speed input: 7351.67 toks/s, output: 57.43 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:00<00:00, 73.37it/s, est. speed input: 7634.05 toks/s, output: 59.64 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:01<00:00, 74.46it/s, est. speed input: 7851.19 toks/s, output: 61.34 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:01<00:00, 74.25it/s, est. speed input: 7999.25 toks/s, output: 62.49 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:01<00:00, 75.27it/s, est. speed input: 8156.58 toks/s, output: 63.72 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:01<00:00, 75.88it/s, est. speed input: 8287.31 toks/s, output: 64.74 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:01<00:00, 76.61it/s, est. speed input: 8407.68 toks/s, output: 65.68 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:01<00:00, 76.94it/s, est. speed input: 8508.09 toks/s, output: 66.47 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:01<00:00, 77.04it/s, est. speed input: 8593.08 toks/s, output: 67.13 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:01<00:00, 77.42it/s, est. speed input: 8675.02 toks/s, output: 67.77 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 77.42it/s, est. speed input: 8734.94 toks/s, output: 68.24 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 68.24it/s, est. speed input: 8734.94 toks/s, output: 68.24 toks/s]
[rank0]:[W126 02:22:04.874130465 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 26.2s

测试结果:
  Requests/s:   67.00
  Tokens/s:     8642.44
  Total Reqs:   128
  Elapsed:      1.91s

  [Prefill 分析]
  Total Prefill Tokens: 16384
  Prefill Tokens/s:     8575.45

============================================================
[3/3] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 256 (= 1 x 256)
│   M_decode      = 1
│   batched_tokens = 257 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 256
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 257
│   --max-num-batched-tokens = 257
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:22:10 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 02:22:10 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=735143) WARNING 01-26 02:22:28 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 59.84 requests/s, 15378.34 total tokens/s, 59.84 output tokens/s
Total num prompt tokens:  32768
Total num output tokens:  128


─── STDERR ───
[2026-01-26 02:22:10] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:22:10] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 02:22:10] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 02:22:10] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:22:10] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:22:10] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:22:10] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:22:10] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:22:10] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 02:22:10] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:22:10] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:22:10] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:22:10] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:22:10] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:22:13] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:22:13] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 02:22:13] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 02:22:13] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:22:13] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:22:13] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:22:13] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:22:13] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:22:13] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 02:22:13] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:22:13] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:22:13] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:22:13] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:22:13] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=735143) [2026-01-26 02:22:14] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=735143) [2026-01-26 02:22:14] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=735143) [2026-01-26 02:22:14] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=735143) [2026-01-26 02:22:14] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=735143) [2026-01-26 02:22:14] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=735143) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=735143) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.26s/it]
(EngineCore_DP0 pid=735143) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.26s/it]
(EngineCore_DP0 pid=735143) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=735143) 2026-01-26 02:22:28,400 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=735143) 2026-01-26 02:22:28,413 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 2316.67it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:12,  9.92it/s, est. speed input: 2541.65 toks/s, output: 9.93 toks/s]
Processed prompts:   6%|▋         | 8/128 [00:00<00:02, 42.19it/s, est. speed input: 9629.57 toks/s, output: 37.61 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:00<00:02, 52.06it/s, est. speed input: 11891.19 toks/s, output: 46.45 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:00<00:01, 57.01it/s, est. speed input: 13058.55 toks/s, output: 51.01 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:00<00:01, 59.67it/s, est. speed input: 13748.61 toks/s, output: 53.70 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:00<00:01, 61.15it/s, est. speed input: 14192.79 toks/s, output: 55.44 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:00<00:01, 61.39it/s, est. speed input: 14436.23 toks/s, output: 56.39 toks/s]
Processed prompts:  39%|███▉      | 50/128 [00:00<00:01, 62.45it/s, est. speed input: 14702.73 toks/s, output: 57.43 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:00<00:01, 63.08it/s, est. speed input: 14902.89 toks/s, output: 58.21 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:01<00:01, 63.51it/s, est. speed input: 15063.15 toks/s, output: 58.84 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:01<00:00, 63.80it/s, est. speed input: 15192.98 toks/s, output: 59.35 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:01<00:00, 64.02it/s, est. speed input: 15303.26 toks/s, output: 59.78 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:01<00:00, 64.36it/s, est. speed input: 15408.01 toks/s, output: 60.19 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:01<00:00, 64.60it/s, est. speed input: 15498.06 toks/s, output: 60.54 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:01<00:00, 64.69it/s, est. speed input: 15572.32 toks/s, output: 60.83 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:01<00:00, 64.25it/s, est. speed input: 15611.92 toks/s, output: 60.98 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:01<00:00, 63.49it/s, est. speed input: 15624.33 toks/s, output: 61.03 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:01<00:00, 63.84it/s, est. speed input: 15675.65 toks/s, output: 61.23 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:02<00:00, 64.25it/s, est. speed input: 15728.61 toks/s, output: 61.44 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 64.25it/s, est. speed input: 15731.49 toks/s, output: 61.45 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 61.45it/s, est. speed input: 15731.49 toks/s, output: 61.45 toks/s]
[rank0]:[W126 02:22:31.452808591 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 26.6s

测试结果:
  Requests/s:   59.84
  Tokens/s:     15378.34
  Total Reqs:   128
  Elapsed:      2.14s

  [Prefill 分析]
  Total Prefill Tokens: 32768
  Prefill Tokens/s:     15318.50


------------------------------------------------------------
  生成 CSV: Llama3.2-1B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/GB10_cc121_INT8_py312_cu129_aarch64/cublaslt/Llama3.2-1B-INT8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
16,16,1,128,128,78.4578,1333.7825,1.6315
128,128,1,128,128,66.9957,8642.4424,1.9106
256,256,1,128,128,59.8379,15378.3360,2.1391

------------------------------------------------------------

[INFO] 完成: 3 成功, 0 失败

============================================================
  Llama3.2-1B-INT8 | cuBLASLt | decode
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-INT8
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cublaslt

============================================================
[1/3] 测试 M=16
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16
│   M_prefill     = 256 (= 16 x 16)
│   M_decode      = 16
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 16
│   --max-num-seqs           = 16
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:22:36 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 02:22:36 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=735693) WARNING 01-26 02:22:54 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 5.29 requests/s, 1438.88 total tokens/s, 1354.24 output tokens/s
Total num prompt tokens:  256
Total num output tokens:  4096


─── STDERR ───
[2026-01-26 02:22:36] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:22:36] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 02:22:36] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 02:22:36] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:22:36] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:22:36] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:22:36] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:22:36] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:22:36] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 02:22:36] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:22:36] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:22:36] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:22:36] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:22:36] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:22:40] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:22:40] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 02:22:40] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 02:22:40] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:22:40] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:22:40] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:22:40] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:22:40] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:22:40] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 02:22:40] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:22:40] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:22:40] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:22:40] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:22:40] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=735693) [2026-01-26 02:22:41] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=735693) [2026-01-26 02:22:41] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=735693) [2026-01-26 02:22:41] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=735693) [2026-01-26 02:22:41] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=735693) [2026-01-26 02:22:41] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=735693) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=735693) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:07<00:00,  7.89s/it]
(EngineCore_DP0 pid=735693) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:07<00:00,  7.89s/it]
(EngineCore_DP0 pid=735693) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=735693) 2026-01-26 02:22:54,273 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=735693) 2026-01-26 02:22:54,283 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/16 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 16/16 [00:00<00:00, 8557.62it/s]

Processed prompts:   0%|          | 0/16 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   6%|▋         | 1/16 [00:03<00:45,  3.01s/it, est. speed input: 5.32 toks/s, output: 85.10 toks/s]
Processed prompts: 100%|██████████| 16/16 [00:03<00:00,  3.01s/it, est. speed input: 84.71 toks/s, output: 1355.42 toks/s]
Processed prompts: 100%|██████████| 16/16 [00:03<00:00,  5.29it/s, est. speed input: 84.71 toks/s, output: 1355.42 toks/s]
[rank0]:[W126 02:22:58.211402805 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 26.7s

测试结果:
  Requests/s:   5.29
  Tokens/s:     1438.88
  Total Reqs:   16
  Elapsed:      3.02s

  [Decode 分析]
  Total Decode Tokens:  4096
  Decode Tokens/s:      1354.24

============================================================
[2/3] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 2048 (= 128 x 16)
│   M_decode      = 128
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 128
│   --max-num-seqs           = 128
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:23:03 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 02:23:03 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=736276) WARNING 01-26 02:23:21 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 23.28 requests/s, 6332.62 total tokens/s, 5960.11 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768


─── STDERR ───
[2026-01-26 02:23:03] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:23:03] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 02:23:03] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 02:23:03] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:23:03] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:23:03] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:23:03] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:23:03] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:23:03] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 02:23:03] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:23:03] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:23:03] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:23:03] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:23:03] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:23:06] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:23:07] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 02:23:07] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 02:23:07] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:23:07] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:23:07] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:23:07] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:23:07] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:23:07] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 02:23:07] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:23:07] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:23:07] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:23:07] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:23:07] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=736276) [2026-01-26 02:23:07] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=736276) [2026-01-26 02:23:07] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=736276) [2026-01-26 02:23:07] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=736276) [2026-01-26 02:23:07] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=736276) [2026-01-26 02:23:07] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=736276) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=736276) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.12s/it]
(EngineCore_DP0 pid=736276) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.12s/it]
(EngineCore_DP0 pid=736276) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=736276) 2026-01-26 02:23:21,363 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=736276) 2026-01-26 02:23:21,376 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 13084.20it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:05<11:06,  5.25s/it, est. speed input: 3.05 toks/s, output: 48.79 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:05<00:02, 20.00it/s, est. speed input: 226.87 toks/s, output: 3629.91 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:05<00:00, 37.80it/s, est. speed input: 373.22 toks/s, output: 5971.58 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:05<00:00, 37.80it/s, est. speed input: 373.22 toks/s, output: 5971.58 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:05<00:00, 23.33it/s, est. speed input: 373.22 toks/s, output: 5971.58 toks/s]
[rank0]:[W126 02:23:27.810189298 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 29.6s

测试结果:
  Requests/s:   23.28
  Tokens/s:     6332.62
  Total Reqs:   128
  Elapsed:      5.50s

  [Decode 分析]
  Total Decode Tokens:  32768
  Decode Tokens/s:      5960.11

============================================================
[3/3] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 4096 (= 256 x 16)
│   M_decode      = 256
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 256
│   --max-num-seqs           = 256
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:23:33 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 02:23:33 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=736868) WARNING 01-26 02:23:51 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 28.71 requests/s, 7807.80 total tokens/s, 7348.52 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536


─── STDERR ───
[2026-01-26 02:23:33] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:23:33] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 02:23:33] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 02:23:33] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:23:33] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:23:33] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:23:33] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:23:33] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:23:33] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 02:23:33] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:23:33] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:23:33] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:23:33] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:23:33] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:23:36] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:23:36] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 02:23:36] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 02:23:36] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:23:36] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:23:36] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:23:36] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:23:36] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:23:36] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 02:23:36] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:23:36] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:23:36] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:23:36] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:23:36] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=736868) [2026-01-26 02:23:37] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=736868) [2026-01-26 02:23:37] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=736868) [2026-01-26 02:23:37] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=736868) [2026-01-26 02:23:37] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=736868) [2026-01-26 02:23:37] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=736868) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=736868) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.12s/it]
(EngineCore_DP0 pid=736868) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.12s/it]
(EngineCore_DP0 pid=736868) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=736868) 2026-01-26 02:23:50,831 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=736868) 2026-01-26 02:23:50,844 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 13585.99it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:07<33:49,  7.96s/it, est. speed input: 2.01 toks/s, output: 32.17 toks/s]
Processed prompts:  14%|█▎        | 35/256 [00:08<00:36,  6.12it/s, est. speed input: 69.48 toks/s, output: 1111.74 toks/s]
Processed prompts:  34%|███▍      | 88/256 [00:08<00:08, 18.93it/s, est. speed input: 172.29 toks/s, output: 2756.67 toks/s]
Processed prompts:  54%|█████▎    | 137/256 [00:08<00:03, 34.62it/s, est. speed input: 264.36 toks/s, output: 4229.80 toks/s]
Processed prompts:  70%|██████▉   | 179/256 [00:08<00:01, 51.87it/s, est. speed input: 340.61 toks/s, output: 5449.79 toks/s]
Processed prompts:  84%|████████▍ | 215/256 [00:08<00:00, 69.67it/s, est. speed input: 403.21 toks/s, output: 6451.40 toks/s]
Processed prompts:  97%|█████████▋| 249/256 [00:08<00:00, 82.83it/s, est. speed input: 454.96 toks/s, output: 7279.34 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:08<00:00, 82.83it/s, est. speed input: 460.30 toks/s, output: 7364.86 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:08<00:00, 28.77it/s, est. speed input: 460.30 toks/s, output: 7364.86 toks/s]
[rank0]:[W126 02:24:00.741122194 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 32.9s

测试结果:
  Requests/s:   28.71
  Tokens/s:     7807.80
  Total Reqs:   256
  Elapsed:      8.92s

  [Decode 分析]
  Total Decode Tokens:  65536
  Decode Tokens/s:      7348.52


------------------------------------------------------------
  生成 CSV: Llama3.2-1B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cublaslt/Llama3.2-1B-INT8_decode.csv

预览:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
16,16,16,16,256,256,5.2900,1438.8774,3.0246
128,16,128,128,256,256,23.2817,6332.6184,5.4979
256,16,256,256,256,256,28.7052,7807.8026,8.9183

------------------------------------------------------------

[INFO] 完成: 3 成功, 0 失败

============================================================
  Llama3.2-1B-INT8 | cuSPARSELt (2_4) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_4

============================================================
[1/3] 测试 M=16
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16
│   M_prefill     = 16 (= 1 x 16)
│   M_decode      = 1
│   batched_tokens = 17 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 17
│   --max-num-batched-tokens = 17
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:24:06 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 02:24:06 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=737508) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=737508) WARNING 01-26 02:24:25 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 84.12 requests/s, 1430.02 total tokens/s, 84.12 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  128


─── STDERR ───
[2026-01-26 02:24:06] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:24:06] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 02:24:06] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 02:24:06] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:24:06] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:24:06] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:24:06] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:24:06] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:24:06] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 02:24:06] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:24:06] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:24:06] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:24:06] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:24:06] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:24:09] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:24:09] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 02:24:09] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 02:24:09] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:24:09] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:24:09] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:24:09] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:24:09] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:24:09] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 02:24:09] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:24:09] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:24:09] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:24:09] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:24:09] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=737508) [2026-01-26 02:24:10] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=737508) [2026-01-26 02:24:10] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=737508) [2026-01-26 02:24:10] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=737508) [2026-01-26 02:24:10] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=737508) [2026-01-26 02:24:10] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=737508) [2026-01-26 02:24:10] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=737508) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=737508) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.07s/it]
(EngineCore_DP0 pid=737508) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.07s/it]
(EngineCore_DP0 pid=737508) 
(EngineCore_DP0 pid=737508) [2026-01-26 02:24:19] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2048] -> 1D uint8
(EngineCore_DP0 pid=737508) [2026-01-26 02:24:19] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4718592 bytes
(EngineCore_DP0 pid=737508) [2026-01-26 02:24:19] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2048] -> 1D uint8
(EngineCore_DP0 pid=737508) [2026-01-26 02:24:19] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 3145728 bytes
(EngineCore_DP0 pid=737508) [2026-01-26 02:24:19] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2048] -> 1D uint8
(EngineCore_DP0 pid=737508) [2026-01-26 02:24:19] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 25165824 bytes
(EngineCore_DP0 pid=737508) [2026-01-26 02:24:19] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 8192] -> 1D uint8
(EngineCore_DP0 pid=737508) [2026-01-26 02:24:19] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12582912 bytes
(EngineCore_DP0 pid=737508) 2026-01-26 02:24:24,621 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=737508) 2026-01-26 02:24:24,628 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 13361.65it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:55,  2.30it/s, est. speed input: 36.77 toks/s, output: 2.30 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:03, 30.99it/s, est. speed input: 384.94 toks/s, output: 24.06 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:00<00:01, 53.86it/s, est. speed input: 623.73 toks/s, output: 38.98 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:00<00:01, 71.65it/s, est. speed input: 798.52 toks/s, output: 49.91 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:00<00:00, 84.30it/s, est. speed input: 928.15 toks/s, output: 58.01 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:00<00:00, 94.38it/s, est. speed input: 1033.14 toks/s, output: 64.57 toks/s]
Processed prompts:  58%|█████▊    | 74/128 [00:01<00:00, 102.25it/s, est. speed input: 1124.72 toks/s, output: 70.29 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:01<00:00, 105.90it/s, est. speed input: 1188.83 toks/s, output: 74.30 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:01<00:00, 109.79it/s, est. speed input: 1246.45 toks/s, output: 77.90 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:01<00:00, 112.50it/s, est. speed input: 1295.29 toks/s, output: 80.95 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:01<00:00, 113.80it/s, est. speed input: 1335.66 toks/s, output: 83.48 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 113.80it/s, est. speed input: 1355.18 toks/s, output: 84.70 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 84.69it/s, est. speed input: 1355.18 toks/s, output: 84.70 toks/s] 
[rank0]:[W126 02:24:27.503894319 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 26.8s

测试结果:
  Requests/s:   84.12
  Tokens/s:     1430.02
  Total Reqs:   128
  Elapsed:      1.52s

  [Prefill 分析]
  Total Prefill Tokens: 2048
  Prefill Tokens/s:     1345.90

============================================================
[2/3] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 128 (= 1 x 128)
│   M_decode      = 1
│   batched_tokens = 129 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 128
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 129
│   --max-num-batched-tokens = 129
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:24:33 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 02:24:33 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=738103) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=738103) WARNING 01-26 02:24:51 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 78.12 requests/s, 10076.99 total tokens/s, 78.12 output tokens/s
Total num prompt tokens:  16384
Total num output tokens:  128


─── STDERR ───
[2026-01-26 02:24:32] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:24:33] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 02:24:33] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 02:24:33] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:24:33] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:24:33] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:24:33] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:24:33] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:24:33] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 02:24:33] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:24:33] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:24:33] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:24:33] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:24:33] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:24:36] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:24:36] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 02:24:36] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 02:24:36] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:24:36] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:24:36] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:24:36] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:24:36] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:24:36] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 02:24:36] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:24:36] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:24:36] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:24:36] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:24:36] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=738103) [2026-01-26 02:24:37] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=738103) [2026-01-26 02:24:37] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=738103) [2026-01-26 02:24:37] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=738103) [2026-01-26 02:24:37] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=738103) [2026-01-26 02:24:37] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=738103) [2026-01-26 02:24:37] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=738103) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=738103) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.03s/it]
(EngineCore_DP0 pid=738103) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.03s/it]
(EngineCore_DP0 pid=738103) 
(EngineCore_DP0 pid=738103) [2026-01-26 02:24:45] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2048] -> 1D uint8
(EngineCore_DP0 pid=738103) [2026-01-26 02:24:45] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4718592 bytes
(EngineCore_DP0 pid=738103) [2026-01-26 02:24:45] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2048] -> 1D uint8
(EngineCore_DP0 pid=738103) [2026-01-26 02:24:45] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 3145728 bytes
(EngineCore_DP0 pid=738103) [2026-01-26 02:24:45] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2048] -> 1D uint8
(EngineCore_DP0 pid=738103) [2026-01-26 02:24:45] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 25165824 bytes
(EngineCore_DP0 pid=738103) [2026-01-26 02:24:45] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 8192] -> 1D uint8
(EngineCore_DP0 pid=738103) [2026-01-26 02:24:45] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12582912 bytes
(EngineCore_DP0 pid=738103) 2026-01-26 02:24:51,259 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=738103) 2026-01-26 02:24:51,267 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 4092.41it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:49,  2.57it/s, est. speed input: 328.80 toks/s, output: 2.57 toks/s]
Processed prompts:   9%|▉         | 12/128 [00:00<00:03, 30.64it/s, est. speed input: 3080.67 toks/s, output: 24.07 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:00<00:02, 51.67it/s, est. speed input: 4884.57 toks/s, output: 38.16 toks/s]
Processed prompts:  27%|██▋       | 34/128 [00:00<00:01, 66.77it/s, est. speed input: 6141.01 toks/s, output: 47.98 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:00<00:01, 77.86it/s, est. speed input: 7081.26 toks/s, output: 55.32 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:00<00:00, 85.75it/s, est. speed input: 7804.00 toks/s, output: 60.97 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:01<00:00, 90.35it/s, est. speed input: 8346.65 toks/s, output: 65.21 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:01<00:00, 94.73it/s, est. speed input: 8819.81 toks/s, output: 68.90 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:01<00:00, 97.42it/s, est. speed input: 9201.86 toks/s, output: 71.89 toks/s]
Processed prompts:  78%|███████▊  | 100/128 [00:01<00:00, 100.18it/s, est. speed input: 9545.45 toks/s, output: 74.57 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:01<00:00, 101.47it/s, est. speed input: 9824.12 toks/s, output: 76.75 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:01<00:00, 102.92it/s, est. speed input: 10077.05 toks/s, output: 78.73 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 102.92it/s, est. speed input: 10199.05 toks/s, output: 79.68 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 79.67it/s, est. speed input: 10199.05 toks/s, output: 79.68 toks/s] 
[rank0]:[W126 02:24:53.794998477 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 26.2s

测试结果:
  Requests/s:   78.12
  Tokens/s:     10076.99
  Total Reqs:   128
  Elapsed:      1.64s

  [Prefill 分析]
  Total Prefill Tokens: 16384
  Prefill Tokens/s:     9998.88

============================================================
[3/3] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 256 (= 1 x 256)
│   M_decode      = 1
│   batched_tokens = 257 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 256
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 257
│   --max-num-batched-tokens = 257
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:24:59 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 02:24:59 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=738666) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=738666) WARNING 01-26 02:25:17 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 71.61 requests/s, 18403.31 total tokens/s, 71.61 output tokens/s
Total num prompt tokens:  32768
Total num output tokens:  128


─── STDERR ───
[2026-01-26 02:24:59] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:24:59] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 02:24:59] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 02:24:59] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:24:59] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:24:59] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:24:59] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:24:59] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:24:59] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 02:24:59] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:24:59] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:24:59] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:24:59] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:24:59] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:25:02] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:25:02] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 02:25:02] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 02:25:02] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:25:02] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:25:02] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:25:02] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:25:02] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:25:02] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 02:25:02] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:25:02] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:25:02] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:25:02] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:25:02] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=738666) [2026-01-26 02:25:03] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=738666) [2026-01-26 02:25:03] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=738666) [2026-01-26 02:25:03] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=738666) [2026-01-26 02:25:03] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=738666) [2026-01-26 02:25:03] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=738666) [2026-01-26 02:25:03] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=738666) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=738666) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.05s/it]
(EngineCore_DP0 pid=738666) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.05s/it]
(EngineCore_DP0 pid=738666) 
(EngineCore_DP0 pid=738666) [2026-01-26 02:25:12] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2048] -> 1D uint8
(EngineCore_DP0 pid=738666) [2026-01-26 02:25:12] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4718592 bytes
(EngineCore_DP0 pid=738666) [2026-01-26 02:25:12] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2048] -> 1D uint8
(EngineCore_DP0 pid=738666) [2026-01-26 02:25:12] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 3145728 bytes
(EngineCore_DP0 pid=738666) [2026-01-26 02:25:12] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2048] -> 1D uint8
(EngineCore_DP0 pid=738666) [2026-01-26 02:25:12] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 25165824 bytes
(EngineCore_DP0 pid=738666) [2026-01-26 02:25:12] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 8192] -> 1D uint8
(EngineCore_DP0 pid=738666) [2026-01-26 02:25:12] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12582912 bytes
(EngineCore_DP0 pid=738666) 2026-01-26 02:25:17,481 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=738666) 2026-01-26 02:25:17,488 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 2311.80it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:34,  3.73it/s, est. speed input: 956.15 toks/s, output: 3.73 toks/s]
Processed prompts:   8%|▊         | 10/128 [00:00<00:03, 33.02it/s, est. speed input: 6843.15 toks/s, output: 26.73 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:00<00:02, 51.09it/s, est. speed input: 10185.81 toks/s, output: 39.79 toks/s]
Processed prompts:  22%|██▏       | 28/128 [00:00<00:01, 62.82it/s, est. speed input: 12350.82 toks/s, output: 48.24 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:00<00:01, 70.84it/s, est. speed input: 13885.91 toks/s, output: 54.24 toks/s]
Processed prompts:  36%|███▌      | 46/128 [00:00<00:01, 75.96it/s, est. speed input: 14995.72 toks/s, output: 58.58 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:00<00:00, 79.42it/s, est. speed input: 15846.78 toks/s, output: 61.90 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:00<00:00, 80.56it/s, est. speed input: 16434.40 toks/s, output: 64.20 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:01<00:00, 83.04it/s, est. speed input: 17017.66 toks/s, output: 66.47 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:01<00:00, 84.08it/s, est. speed input: 17461.40 toks/s, output: 68.21 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:01<00:00, 85.14it/s, est. speed input: 17851.72 toks/s, output: 69.73 toks/s]
Processed prompts:  78%|███████▊  | 100/128 [00:01<00:00, 85.25it/s, est. speed input: 18152.95 toks/s, output: 70.91 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:01<00:00, 86.04it/s, est. speed input: 18447.57 toks/s, output: 72.06 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:01<00:00, 85.72it/s, est. speed input: 18664.15 toks/s, output: 72.91 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:01<00:00, 86.58it/s, est. speed input: 18901.76 toks/s, output: 73.83 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 86.58it/s, est. speed input: 18927.54 toks/s, output: 73.94 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 73.93it/s, est. speed input: 18927.54 toks/s, output: 73.94 toks/s]
[rank0]:[W126 02:25:19.169966086 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 26.4s

测试结果:
  Requests/s:   71.61
  Tokens/s:     18403.31
  Total Reqs:   128
  Elapsed:      1.79s

  [Prefill 分析]
  Total Prefill Tokens: 32768
  Prefill Tokens/s:     18331.70


------------------------------------------------------------
  生成 CSV: Llama3.2-1B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_4/Llama3.2-1B-INT8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
16,16,1,128,128,84.1190,1430.0238,1.5217
128,128,1,128,128,78.1162,10076.9925,1.6386
256,256,1,128,128,71.6082,18403.3087,1.7875

------------------------------------------------------------

[INFO] 完成: 3 成功, 0 失败

============================================================
  Llama3.2-1B-INT8 | cuSPARSELt (2_4) | decode
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_4

============================================================
[1/3] 测试 M=16
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16
│   M_prefill     = 256 (= 16 x 16)
│   M_decode      = 16
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 16
│   --max-num-seqs           = 16
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:25:25 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 02:25:25 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=739223) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=739223) WARNING 01-26 02:25:43 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 6.63 requests/s, 1803.14 total tokens/s, 1697.07 output tokens/s
Total num prompt tokens:  256
Total num output tokens:  4096


─── STDERR ───
[2026-01-26 02:25:25] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:25:25] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 02:25:25] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 02:25:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:25:25] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:25:25] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:25:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:25:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:25:25] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 02:25:25] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:25:25] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:25:25] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:25:25] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:25:25] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:25:28] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:25:28] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 02:25:28] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 02:25:28] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:25:28] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:25:28] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:25:28] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:25:28] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:25:28] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 02:25:28] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:25:28] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:25:28] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:25:28] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:25:28] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=739223) [2026-01-26 02:25:29] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=739223) [2026-01-26 02:25:29] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=739223) [2026-01-26 02:25:29] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=739223) [2026-01-26 02:25:29] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=739223) [2026-01-26 02:25:29] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=739223) [2026-01-26 02:25:29] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=739223) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=739223) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.03s/it]
(EngineCore_DP0 pid=739223) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.03s/it]
(EngineCore_DP0 pid=739223) 
(EngineCore_DP0 pid=739223) [2026-01-26 02:25:38] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2048] -> 1D uint8
(EngineCore_DP0 pid=739223) [2026-01-26 02:25:38] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4718592 bytes
(EngineCore_DP0 pid=739223) [2026-01-26 02:25:38] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2048] -> 1D uint8
(EngineCore_DP0 pid=739223) [2026-01-26 02:25:38] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 3145728 bytes
(EngineCore_DP0 pid=739223) [2026-01-26 02:25:38] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2048] -> 1D uint8
(EngineCore_DP0 pid=739223) [2026-01-26 02:25:38] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 25165824 bytes
(EngineCore_DP0 pid=739223) [2026-01-26 02:25:38] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 8192] -> 1D uint8
(EngineCore_DP0 pid=739223) [2026-01-26 02:25:38] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12582912 bytes
(EngineCore_DP0 pid=739223) 2026-01-26 02:25:43,431 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=739223) 2026-01-26 02:25:43,439 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/16 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 16/16 [00:00<00:00, 8488.35it/s]

Processed prompts:   0%|          | 0/16 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   6%|▋         | 1/16 [00:02<00:35,  2.40s/it, est. speed input: 6.67 toks/s, output: 106.73 toks/s]
Processed prompts: 100%|██████████| 16/16 [00:02<00:00,  2.40s/it, est. speed input: 106.19 toks/s, output: 1699.04 toks/s]
Processed prompts: 100%|██████████| 16/16 [00:02<00:00,  6.64it/s, est. speed input: 106.19 toks/s, output: 1699.04 toks/s]
[rank0]:[W126 02:25:46.748792624 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 26.6s

测试结果:
  Requests/s:   6.63
  Tokens/s:     1803.14
  Total Reqs:   16
  Elapsed:      2.41s

  [Decode 分析]
  Total Decode Tokens:  4096
  Decode Tokens/s:      1697.07

============================================================
[2/3] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 2048 (= 128 x 16)
│   M_decode      = 128
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 128
│   --max-num-seqs           = 128
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:25:52 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 02:25:52 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=739770) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=739770) WARNING 01-26 02:26:10 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 26.31 requests/s, 7156.64 total tokens/s, 6735.66 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768


─── STDERR ───
[2026-01-26 02:25:52] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:25:52] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 02:25:52] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 02:25:52] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:25:52] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:25:52] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:25:52] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:25:52] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:25:52] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 02:25:52] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:25:52] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:25:52] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:25:52] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:25:52] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:25:55] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:25:55] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 02:25:55] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 02:25:55] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:25:55] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:25:55] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:25:55] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:25:55] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:25:55] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 02:25:55] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:25:55] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:25:55] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:25:55] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:25:55] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=739770) [2026-01-26 02:25:56] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=739770) [2026-01-26 02:25:56] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=739770) [2026-01-26 02:25:56] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=739770) [2026-01-26 02:25:56] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=739770) [2026-01-26 02:25:56] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=739770) [2026-01-26 02:25:56] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=739770) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=739770) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.22s/it]
(EngineCore_DP0 pid=739770) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.22s/it]
(EngineCore_DP0 pid=739770) 
(EngineCore_DP0 pid=739770) [2026-01-26 02:26:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2048] -> 1D uint8
(EngineCore_DP0 pid=739770) [2026-01-26 02:26:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4718592 bytes
(EngineCore_DP0 pid=739770) [2026-01-26 02:26:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2048] -> 1D uint8
(EngineCore_DP0 pid=739770) [2026-01-26 02:26:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 3145728 bytes
(EngineCore_DP0 pid=739770) [2026-01-26 02:26:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2048] -> 1D uint8
(EngineCore_DP0 pid=739770) [2026-01-26 02:26:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 25165824 bytes
(EngineCore_DP0 pid=739770) [2026-01-26 02:26:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 8192] -> 1D uint8
(EngineCore_DP0 pid=739770) [2026-01-26 02:26:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12582912 bytes
(EngineCore_DP0 pid=739770) 2026-01-26 02:26:10,249 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=739770) 2026-01-26 02:26:10,257 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 13294.15it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:04<09:33,  4.52s/it, est. speed input: 3.54 toks/s, output: 56.65 toks/s]
Processed prompts:   2%|▏         | 2/128 [00:04<04:03,  1.93s/it, est. speed input: 6.90 toks/s, output: 110.33 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:04<00:02, 25.41it/s, est. speed input: 208.86 toks/s, output: 3341.72 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:04<00:00, 60.22it/s, est. speed input: 418.59 toks/s, output: 6697.45 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:04<00:00, 60.22it/s, est. speed input: 421.88 toks/s, output: 6750.06 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:04<00:00, 26.37it/s, est. speed input: 421.88 toks/s, output: 6750.06 toks/s]
[rank0]:[W126 02:26:15.046349540 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 29.3s

测试结果:
  Requests/s:   26.31
  Tokens/s:     7156.64
  Total Reqs:   128
  Elapsed:      4.86s

  [Decode 分析]
  Total Decode Tokens:  32768
  Decode Tokens/s:      6735.66

============================================================
[3/3] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 4096 (= 256 x 16)
│   M_decode      = 256
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 256
│   --max-num-seqs           = 256
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:26:21 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 02:26:21 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=740379) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=740379) WARNING 01-26 02:26:40 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 32.08 requests/s, 8724.69 total tokens/s, 8211.47 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536


─── STDERR ───
[2026-01-26 02:26:21] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:26:21] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 02:26:21] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 02:26:21] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:26:21] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:26:21] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:26:21] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:26:21] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:26:21] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 02:26:21] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:26:21] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:26:21] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:26:21] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:26:21] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:26:24] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:26:24] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 02:26:24] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 02:26:24] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:26:24] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:26:24] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:26:24] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:26:24] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:26:24] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 02:26:24] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:26:24] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:26:24] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:26:24] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:26:24] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=740379) [2026-01-26 02:26:25] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=740379) [2026-01-26 02:26:25] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=740379) [2026-01-26 02:26:25] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=740379) [2026-01-26 02:26:25] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=740379) [2026-01-26 02:26:25] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=740379) [2026-01-26 02:26:25] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=740379) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=740379) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.08s/it]
(EngineCore_DP0 pid=740379) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:08<00:00,  8.08s/it]
(EngineCore_DP0 pid=740379) 
(EngineCore_DP0 pid=740379) [2026-01-26 02:26:34] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2048] -> 1D uint8
(EngineCore_DP0 pid=740379) [2026-01-26 02:26:34] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4718592 bytes
(EngineCore_DP0 pid=740379) [2026-01-26 02:26:34] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2048] -> 1D uint8
(EngineCore_DP0 pid=740379) [2026-01-26 02:26:34] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 3145728 bytes
(EngineCore_DP0 pid=740379) [2026-01-26 02:26:34] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2048] -> 1D uint8
(EngineCore_DP0 pid=740379) [2026-01-26 02:26:34] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 25165824 bytes
(EngineCore_DP0 pid=740379) [2026-01-26 02:26:34] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 8192] -> 1D uint8
(EngineCore_DP0 pid=740379) [2026-01-26 02:26:34] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 12582912 bytes
(EngineCore_DP0 pid=740379) 2026-01-26 02:26:39,453 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=740379) 2026-01-26 02:26:39,461 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 13656.84it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:06<29:36,  6.97s/it, est. speed input: 2.30 toks/s, output: 36.74 toks/s]
Processed prompts:  13%|█▎        | 34/256 [00:07<00:32,  6.74it/s, est. speed input: 76.73 toks/s, output: 1227.60 toks/s]
Processed prompts:  29%|██▉       | 75/256 [00:07<00:10, 17.94it/s, est. speed input: 166.87 toks/s, output: 2669.97 toks/s]
Processed prompts:  46%|████▌     | 118/256 [00:07<00:04, 33.27it/s, est. speed input: 257.87 toks/s, output: 4125.90 toks/s]
Processed prompts:  59%|█████▉    | 152/256 [00:07<00:02, 48.88it/s, est. speed input: 327.55 toks/s, output: 5240.78 toks/s]
Processed prompts:  71%|███████▏  | 183/256 [00:07<00:01, 66.44it/s, est. speed input: 388.90 toks/s, output: 6222.41 toks/s]
Processed prompts:  84%|████████▎ | 214/256 [00:07<00:00, 86.60it/s, est. speed input: 447.79 toks/s, output: 7164.66 toks/s]
Processed prompts:  95%|█████████▌| 244/256 [00:07<00:00, 104.55it/s, est. speed input: 500.83 toks/s, output: 8013.22 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:07<00:00, 104.55it/s, est. speed input: 514.48 toks/s, output: 8231.66 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:07<00:00, 32.15it/s, est. speed input: 514.48 toks/s, output: 8231.66 toks/s] 
[rank0]:[W126 02:26:48.391072798 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 32.4s

测试结果:
  Requests/s:   32.08
  Tokens/s:     8724.69
  Total Reqs:   256
  Elapsed:      7.98s

  [Decode 分析]
  Total Decode Tokens:  65536
  Decode Tokens/s:      8211.47


------------------------------------------------------------
  生成 CSV: Llama3.2-1B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_4/Llama3.2-1B-INT8_decode.csv

预览:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
16,16,16,16,256,256,6.6292,1803.1399,2.4136
128,16,128,128,256,256,26.3112,7156.6368,4.8649
256,16,256,256,256,256,32.0761,8724.6886,7.9810

------------------------------------------------------------

[INFO] 完成: 3 成功, 0 失败

============================================================
  Llama3.2-1B-INT8 | cuSPARSELt (2_6) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_6

============================================================
[1/3] 测试 M=16
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16
│   M_prefill     = 16 (= 1 x 16)
│   M_decode      = 1
│   batched_tokens = 17 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 17
│   --max-num-batched-tokens = 17
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:26:53 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 02:26:53 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=741039) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=741039) WARNING 01-26 02:27:14 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 81.04 requests/s, 1377.72 total tokens/s, 81.04 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  128


─── STDERR ───
[2026-01-26 02:26:53] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:26:53] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 02:26:53] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 02:26:53] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:26:53] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:26:53] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:26:53] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:26:53] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:26:53] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 02:26:53] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:26:53] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:26:53] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:26:53] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:26:53] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:26:57] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:26:57] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 02:26:57] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 02:26:57] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:26:57] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:26:57] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:26:57] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:26:57] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:26:57] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 02:26:57] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:26:57] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:26:57] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:26:57] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:26:57] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=741039) [2026-01-26 02:26:58] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=741039) [2026-01-26 02:26:58] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=741039) [2026-01-26 02:26:58] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=741039) [2026-01-26 02:26:58] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=741039) [2026-01-26 02:26:58] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=741039) [2026-01-26 02:26:58] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=741039) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=741039) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:09<00:00,  9.30s/it]
(EngineCore_DP0 pid=741039) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:09<00:00,  9.30s/it]
(EngineCore_DP0 pid=741039) 
(EngineCore_DP0 pid=741039) [2026-01-26 02:27:08] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=741039) [2026-01-26 02:27:08] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6389760 bytes
(EngineCore_DP0 pid=741039) [2026-01-26 02:27:08] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=741039) [2026-01-26 02:27:08] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4259840 bytes
(EngineCore_DP0 pid=741039) [2026-01-26 02:27:08] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=741039) [2026-01-26 02:27:08] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 34078720 bytes
(EngineCore_DP0 pid=741039) [2026-01-26 02:27:08] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=741039) [2026-01-26 02:27:08] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16842752 bytes
(EngineCore_DP0 pid=741039) 2026-01-26 02:27:14,158 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=741039) 2026-01-26 02:27:14,167 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 13262.95it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:30,  4.15it/s, est. speed input: 66.35 toks/s, output: 4.15 toks/s]
Processed prompts:   9%|▊         | 11/128 [00:00<00:03, 38.86it/s, est. speed input: 506.25 toks/s, output: 31.64 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:01, 58.68it/s, est. speed input: 741.79 toks/s, output: 46.36 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:00<00:01, 71.29it/s, est. speed input: 891.53 toks/s, output: 55.72 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:00<00:01, 79.31it/s, est. speed input: 993.43 toks/s, output: 62.09 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:00<00:00, 84.57it/s, est. speed input: 1067.27 toks/s, output: 66.70 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:00<00:00, 88.26it/s, est. speed input: 1124.16 toks/s, output: 70.26 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:00<00:00, 90.84it/s, est. speed input: 1169.20 toks/s, output: 73.07 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:01<00:00, 92.45it/s, est. speed input: 1204.80 toks/s, output: 75.30 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:01<00:00, 93.80it/s, est. speed input: 1235.10 toks/s, output: 77.19 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:01<00:00, 93.47it/s, est. speed input: 1255.92 toks/s, output: 78.49 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:01<00:00, 93.85it/s, est. speed input: 1275.63 toks/s, output: 79.73 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:01<00:00, 94.90it/s, est. speed input: 1295.05 toks/s, output: 80.94 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 94.90it/s, est. speed input: 1305.91 toks/s, output: 81.62 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 81.61it/s, est. speed input: 1305.91 toks/s, output: 81.62 toks/s]
[rank0]:[W126 02:27:16.844544931 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 28.5s

测试结果:
  Requests/s:   81.04
  Tokens/s:     1377.72
  Total Reqs:   128
  Elapsed:      1.58s

  [Prefill 分析]
  Total Prefill Tokens: 2048
  Prefill Tokens/s:     1296.68

============================================================
[2/3] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 128 (= 1 x 128)
│   M_decode      = 1
│   batched_tokens = 129 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 128
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 129
│   --max-num-batched-tokens = 129
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:27:22 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 02:27:22 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=741630) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=741630) WARNING 01-26 02:27:42 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 71.99 requests/s, 9286.85 total tokens/s, 71.99 output tokens/s
Total num prompt tokens:  16384
Total num output tokens:  128


─── STDERR ───
[2026-01-26 02:27:22] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:27:22] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 02:27:22] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 02:27:22] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:27:22] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:27:22] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:27:22] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:27:22] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:27:22] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 02:27:22] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:27:22] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:27:22] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:27:22] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:27:22] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:27:25] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:27:25] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 02:27:25] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 02:27:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:27:25] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:27:25] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:27:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:27:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:27:25] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 02:27:25] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:27:25] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:27:25] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:27:25] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:27:25] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=741630) [2026-01-26 02:27:26] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=741630) [2026-01-26 02:27:26] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=741630) [2026-01-26 02:27:26] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=741630) [2026-01-26 02:27:26] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=741630) [2026-01-26 02:27:26] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=741630) [2026-01-26 02:27:26] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=741630) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=741630) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:09<00:00,  9.16s/it]
(EngineCore_DP0 pid=741630) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:09<00:00,  9.16s/it]
(EngineCore_DP0 pid=741630) 
(EngineCore_DP0 pid=741630) [2026-01-26 02:27:36] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=741630) [2026-01-26 02:27:36] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6389760 bytes
(EngineCore_DP0 pid=741630) [2026-01-26 02:27:36] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=741630) [2026-01-26 02:27:36] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4259840 bytes
(EngineCore_DP0 pid=741630) [2026-01-26 02:27:36] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=741630) [2026-01-26 02:27:36] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 34078720 bytes
(EngineCore_DP0 pid=741630) [2026-01-26 02:27:36] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=741630) [2026-01-26 02:27:36] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16842752 bytes
(EngineCore_DP0 pid=741630) 2026-01-26 02:27:42,125 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=741630) 2026-01-26 02:27:42,133 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 3985.44it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:28,  4.40it/s, est. speed input: 563.52 toks/s, output: 4.40 toks/s]
Processed prompts:   8%|▊         | 10/128 [00:00<00:03, 36.33it/s, est. speed input: 3820.06 toks/s, output: 29.84 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:00<00:02, 53.45it/s, est. speed input: 5481.73 toks/s, output: 42.82 toks/s]
Processed prompts:  22%|██▏       | 28/128 [00:00<00:01, 64.00it/s, est. speed input: 6514.08 toks/s, output: 50.89 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:00<00:01, 69.63it/s, est. speed input: 7157.36 toks/s, output: 55.92 toks/s]
Processed prompts:  36%|███▌      | 46/128 [00:00<00:01, 74.22it/s, est. speed input: 7660.22 toks/s, output: 59.84 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:00<00:00, 77.53it/s, est. speed input: 8049.22 toks/s, output: 62.88 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:00<00:00, 79.53it/s, est. speed input: 8344.37 toks/s, output: 65.19 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:01<00:00, 80.96it/s, est. speed input: 8583.46 toks/s, output: 67.06 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:01<00:00, 82.04it/s, est. speed input: 8782.68 toks/s, output: 68.61 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:01<00:00, 82.96it/s, est. speed input: 8954.21 toks/s, output: 69.95 toks/s]
Processed prompts:  78%|███████▊  | 100/128 [00:01<00:00, 83.59it/s, est. speed input: 9099.49 toks/s, output: 71.09 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:01<00:00, 83.78it/s, est. speed input: 9218.13 toks/s, output: 72.02 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:01<00:00, 83.33it/s, est. speed input: 9307.07 toks/s, output: 72.71 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:01<00:00, 82.80it/s, est. speed input: 9379.39 toks/s, output: 73.28 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 82.80it/s, est. speed input: 9389.02 toks/s, output: 73.35 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 73.34it/s, est. speed input: 9389.02 toks/s, output: 73.35 toks/s]
[rank0]:[W126 02:27:44.833565807 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 27.9s

测试结果:
  Requests/s:   71.99
  Tokens/s:     9286.85
  Total Reqs:   128
  Elapsed:      1.78s

  [Prefill 分析]
  Total Prefill Tokens: 16384
  Prefill Tokens/s:     9214.85

============================================================
[3/3] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 256 (= 1 x 256)
│   M_decode      = 1
│   batched_tokens = 257 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 256
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 257
│   --max-num-batched-tokens = 257
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:27:50 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 02:27:50 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=742211) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=742211) WARNING 01-26 02:28:10 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 62.65 requests/s, 16101.86 total tokens/s, 62.65 output tokens/s
Total num prompt tokens:  32768
Total num output tokens:  128


─── STDERR ───
[2026-01-26 02:27:50] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:27:50] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 02:27:50] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 02:27:50] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:27:50] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:27:50] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:27:50] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:27:50] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:27:50] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 02:27:50] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:27:50] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:27:50] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:27:50] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:27:50] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:27:53] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:27:54] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 02:27:54] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 02:27:54] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:27:54] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:27:54] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:27:54] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:27:54] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:27:54] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 02:27:54] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:27:54] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:27:54] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:27:54] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:27:54] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=742211) [2026-01-26 02:27:54] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=742211) [2026-01-26 02:27:54] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=742211) [2026-01-26 02:27:54] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=742211) [2026-01-26 02:27:54] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=742211) [2026-01-26 02:27:54] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=742211) [2026-01-26 02:27:54] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=742211) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=742211) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:09<00:00,  9.04s/it]
(EngineCore_DP0 pid=742211) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:09<00:00,  9.04s/it]
(EngineCore_DP0 pid=742211) 
(EngineCore_DP0 pid=742211) [2026-01-26 02:28:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=742211) [2026-01-26 02:28:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6389760 bytes
(EngineCore_DP0 pid=742211) [2026-01-26 02:28:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=742211) [2026-01-26 02:28:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4259840 bytes
(EngineCore_DP0 pid=742211) [2026-01-26 02:28:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=742211) [2026-01-26 02:28:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 34078720 bytes
(EngineCore_DP0 pid=742211) [2026-01-26 02:28:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=742211) [2026-01-26 02:28:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16842752 bytes
(EngineCore_DP0 pid=742211) 2026-01-26 02:28:09,924 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=742211) 2026-01-26 02:28:09,931 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 2440.87it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:29,  4.29it/s, est. speed input: 1098.08 toks/s, output: 4.29 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:03, 31.57it/s, est. speed input: 6668.47 toks/s, output: 26.05 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:02, 46.40it/s, est. speed input: 9538.22 toks/s, output: 37.26 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:00<00:01, 55.36it/s, est. speed input: 11300.35 toks/s, output: 44.14 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:00<00:01, 61.34it/s, est. speed input: 12519.22 toks/s, output: 48.90 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:00<00:01, 64.83it/s, est. speed input: 13360.47 toks/s, output: 52.19 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:00<00:01, 67.30it/s, est. speed input: 14006.28 toks/s, output: 54.71 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:01<00:01, 69.11it/s, est. speed input: 14520.23 toks/s, output: 56.72 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:01<00:00, 69.37it/s, est. speed input: 14866.58 toks/s, output: 58.07 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:01<00:00, 69.84it/s, est. speed input: 15166.85 toks/s, output: 59.24 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:01<00:00, 70.79it/s, est. speed input: 15454.93 toks/s, output: 60.37 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:01<00:00, 71.36it/s, est. speed input: 15694.02 toks/s, output: 61.30 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:01<00:00, 71.67it/s, est. speed input: 15894.46 toks/s, output: 62.09 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:01<00:00, 72.13it/s, est. speed input: 16080.80 toks/s, output: 62.82 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:01<00:00, 72.26it/s, est. speed input: 16235.01 toks/s, output: 63.42 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:01<00:00, 72.34it/s, est. speed input: 16371.12 toks/s, output: 63.95 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 72.34it/s, est. speed input: 16473.26 toks/s, output: 64.35 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 64.34it/s, est. speed input: 16473.26 toks/s, output: 64.35 toks/s]
[rank0]:[W126 02:28:12.871679132 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 28.0s

测试结果:
  Requests/s:   62.65
  Tokens/s:     16101.86
  Total Reqs:   128
  Elapsed:      2.04s

  [Prefill 分析]
  Total Prefill Tokens: 32768
  Prefill Tokens/s:     16039.21


------------------------------------------------------------
  生成 CSV: Llama3.2-1B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_6/Llama3.2-1B-INT8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
16,16,1,128,128,81.0424,1377.7201,1.5794
128,128,1,128,128,71.9910,9286.8452,1.7780
256,256,1,128,128,62.6531,16101.8592,2.0430

------------------------------------------------------------

[INFO] 完成: 3 成功, 0 失败

============================================================
  Llama3.2-1B-INT8 | cuSPARSELt (2_6) | decode
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_6

============================================================
[1/3] 测试 M=16
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16
│   M_prefill     = 256 (= 16 x 16)
│   M_decode      = 16
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 16
│   --max-num-seqs           = 16
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:28:18 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 02:28:18 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=742802) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=742802) WARNING 01-26 02:28:37 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 5.48 requests/s, 1489.41 total tokens/s, 1401.80 output tokens/s
Total num prompt tokens:  256
Total num output tokens:  4096


─── STDERR ───
[2026-01-26 02:28:18] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:28:18] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 02:28:18] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 02:28:18] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:28:18] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:28:18] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:28:18] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:28:18] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:28:18] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 02:28:18] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:28:18] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:28:18] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:28:18] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:28:18] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:28:21] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:28:21] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 02:28:21] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 02:28:21] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:28:21] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:28:21] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:28:21] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:28:21] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:28:21] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 02:28:21] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:28:21] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:28:21] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:28:21] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:28:21] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=742802) [2026-01-26 02:28:22] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=742802) [2026-01-26 02:28:22] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=742802) [2026-01-26 02:28:22] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=742802) [2026-01-26 02:28:22] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=742802) [2026-01-26 02:28:22] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=742802) [2026-01-26 02:28:22] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=742802) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=742802) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:09<00:00,  9.08s/it]
(EngineCore_DP0 pid=742802) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:09<00:00,  9.08s/it]
(EngineCore_DP0 pid=742802) 
(EngineCore_DP0 pid=742802) [2026-01-26 02:28:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=742802) [2026-01-26 02:28:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6389760 bytes
(EngineCore_DP0 pid=742802) [2026-01-26 02:28:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=742802) [2026-01-26 02:28:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4259840 bytes
(EngineCore_DP0 pid=742802) [2026-01-26 02:28:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=742802) [2026-01-26 02:28:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 34078720 bytes
(EngineCore_DP0 pid=742802) [2026-01-26 02:28:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=742802) [2026-01-26 02:28:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16842752 bytes
(EngineCore_DP0 pid=742802) 2026-01-26 02:28:37,433 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=742802) 2026-01-26 02:28:37,440 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/16 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 16/16 [00:00<00:00, 8644.71it/s]

Processed prompts:   0%|          | 0/16 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   6%|▋         | 1/16 [00:02<00:43,  2.89s/it, est. speed input: 5.53 toks/s, output: 88.51 toks/s]
Processed prompts: 100%|██████████| 16/16 [00:02<00:00,  2.89s/it, est. speed input: 87.69 toks/s, output: 1403.03 toks/s]
Processed prompts: 100%|██████████| 16/16 [00:02<00:00,  5.48it/s, est. speed input: 87.69 toks/s, output: 1403.03 toks/s]
[rank0]:[W126 02:28:41.265358179 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 28.4s

测试结果:
  Requests/s:   5.48
  Tokens/s:     1489.41
  Total Reqs:   16
  Elapsed:      2.92s

  [Decode 分析]
  Total Decode Tokens:  4096
  Decode Tokens/s:      1401.80

============================================================
[2/3] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 2048 (= 128 x 16)
│   M_decode      = 128
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 128
│   --max-num-seqs           = 128
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:28:46 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 02:28:46 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=743364) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=743364) WARNING 01-26 02:29:06 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 23.55 requests/s, 6405.89 total tokens/s, 6029.07 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768


─── STDERR ───
[2026-01-26 02:28:46] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:28:46] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 02:28:46] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 02:28:46] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:28:46] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:28:46] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:28:46] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:28:46] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:28:46] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 02:28:46] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:28:46] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:28:46] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:28:46] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:28:46] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:28:50] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:28:50] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 02:28:50] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 02:28:50] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:28:50] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:28:50] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:28:50] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:28:50] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:28:50] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 02:28:50] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:28:50] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:28:50] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:28:50] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:28:50] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=743364) [2026-01-26 02:28:51] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=743364) [2026-01-26 02:28:51] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=743364) [2026-01-26 02:28:51] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=743364) [2026-01-26 02:28:51] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=743364) [2026-01-26 02:28:51] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=743364) [2026-01-26 02:28:51] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=743364) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=743364) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:09<00:00,  9.18s/it]
(EngineCore_DP0 pid=743364) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:09<00:00,  9.18s/it]
(EngineCore_DP0 pid=743364) 
(EngineCore_DP0 pid=743364) [2026-01-26 02:29:00] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=743364) [2026-01-26 02:29:00] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6389760 bytes
(EngineCore_DP0 pid=743364) [2026-01-26 02:29:00] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=743364) [2026-01-26 02:29:00] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4259840 bytes
(EngineCore_DP0 pid=743364) [2026-01-26 02:29:00] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=743364) [2026-01-26 02:29:00] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 34078720 bytes
(EngineCore_DP0 pid=743364) [2026-01-26 02:29:00] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=743364) [2026-01-26 02:29:00] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16842752 bytes
(EngineCore_DP0 pid=743364) 2026-01-26 02:29:05,785 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=743364) 2026-01-26 02:29:05,792 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 13201.64it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:05<10:43,  5.07s/it, est. speed input: 3.16 toks/s, output: 50.51 toks/s]
Processed prompts:   2%|▏         | 3/128 [00:05<02:50,  1.36s/it, est. speed input: 9.24 toks/s, output: 147.90 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:05<00:02, 22.66it/s, est. speed input: 186.84 toks/s, output: 2989.46 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:05<00:00, 49.65it/s, est. speed input: 348.68 toks/s, output: 5578.78 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:05<00:00, 49.65it/s, est. speed input: 377.55 toks/s, output: 6040.80 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:05<00:00, 23.60it/s, est. speed input: 377.55 toks/s, output: 6040.80 toks/s]
[rank0]:[W126 02:29:12.182585763 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 30.8s

测试结果:
  Requests/s:   23.55
  Tokens/s:     6405.89
  Total Reqs:   128
  Elapsed:      5.43s

  [Decode 分析]
  Total Decode Tokens:  32768
  Decode Tokens/s:      6029.07

============================================================
[3/3] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 4096 (= 256 x 16)
│   M_decode      = 256
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 256
│   --max-num-seqs           = 256
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:29:17 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 02:29:17 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=744003) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=744003) WARNING 01-26 02:29:34 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 29.38 requests/s, 7990.95 total tokens/s, 7520.90 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536


─── STDERR ───
[2026-01-26 02:29:17] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:29:17] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 02:29:17] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 02:29:17] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:29:17] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:29:17] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:29:17] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:29:17] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:29:17] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 02:29:17] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:29:17] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:29:17] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:29:17] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:29:17] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:29:20] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:29:20] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 02:29:20] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 02:29:20] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:29:20] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:29:20] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:29:20] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:29:20] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:29:20] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 02:29:20] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:29:20] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:29:20] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:29:20] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:29:20] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=744003) [2026-01-26 02:29:21] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=744003) [2026-01-26 02:29:21] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=744003) [2026-01-26 02:29:21] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=744003) [2026-01-26 02:29:21] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=744003) [2026-01-26 02:29:21] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=744003) [2026-01-26 02:29:21] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=744003) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=744003) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:06<00:00,  6.60s/it]
(EngineCore_DP0 pid=744003) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:06<00:00,  6.60s/it]
(EngineCore_DP0 pid=744003) 
(EngineCore_DP0 pid=744003) [2026-01-26 02:29:28] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=744003) [2026-01-26 02:29:29] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6389760 bytes
(EngineCore_DP0 pid=744003) [2026-01-26 02:29:29] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=744003) [2026-01-26 02:29:29] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4259840 bytes
(EngineCore_DP0 pid=744003) [2026-01-26 02:29:29] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=744003) [2026-01-26 02:29:29] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 34078720 bytes
(EngineCore_DP0 pid=744003) [2026-01-26 02:29:29] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=744003) [2026-01-26 02:29:29] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16842752 bytes
(EngineCore_DP0 pid=744003) 2026-01-26 02:29:34,124 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=744003) 2026-01-26 02:29:34,131 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 13552.73it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:07<32:29,  7.65s/it, est. speed input: 2.09 toks/s, output: 33.48 toks/s]
Processed prompts:  13%|█▎        | 33/256 [00:07<00:37,  5.96it/s, est. speed input: 67.90 toks/s, output: 1086.38 toks/s]
Processed prompts:  29%|██▉       | 75/256 [00:07<00:11, 16.43it/s, est. speed input: 152.12 toks/s, output: 2433.91 toks/s]
Processed prompts:  42%|████▏     | 108/256 [00:07<00:05, 27.36it/s, est. speed input: 216.26 toks/s, output: 3460.17 toks/s]
Processed prompts:  56%|█████▋    | 144/256 [00:08<00:02, 42.87it/s, est. speed input: 284.31 toks/s, output: 4548.96 toks/s]
Processed prompts:  70%|██████▉   | 178/256 [00:08<00:01, 60.55it/s, est. speed input: 345.97 toks/s, output: 5535.55 toks/s]
Processed prompts:  81%|████████  | 207/256 [00:08<00:00, 78.72it/s, est. speed input: 397.13 toks/s, output: 6354.09 toks/s]
Processed prompts:  92%|█████████▏| 235/256 [00:08<00:00, 93.81it/s, est. speed input: 442.40 toks/s, output: 7078.36 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:08<00:00, 93.81it/s, est. speed input: 471.12 toks/s, output: 7537.98 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:08<00:00, 29.44it/s, est. speed input: 471.12 toks/s, output: 7537.98 toks/s]
[rank0]:[W126 02:29:43.824635377 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 31.7s

测试结果:
  Requests/s:   29.38
  Tokens/s:     7990.95
  Total Reqs:   256
  Elapsed:      8.71s

  [Decode 分析]
  Total Decode Tokens:  65536
  Decode Tokens/s:      7520.90


------------------------------------------------------------
  生成 CSV: Llama3.2-1B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_6/Llama3.2-1B-INT8_decode.csv

预览:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
16,16,16,16,256,256,5.4758,1489.4132,2.9220
128,16,128,128,256,256,23.5511,6405.8899,5.4350
256,16,256,256,256,256,29.3785,7990.9524,8.7139

------------------------------------------------------------

[INFO] 完成: 3 成功, 0 失败

============================================================
  Llama3.2-1B-INT8 | cuSPARSELt (2_10) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_10

============================================================
[1/3] 测试 M=16
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16
│   M_prefill     = 16 (= 1 x 16)
│   M_decode      = 1
│   batched_tokens = 17 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 17
│   --max-num-batched-tokens = 17
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:29:49 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 02:29:49 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=744634) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=744634) WARNING 01-26 02:30:11 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 73.28 requests/s, 1245.75 total tokens/s, 73.28 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  128


─── STDERR ───
[2026-01-26 02:29:49] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:29:49] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 02:29:49] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 02:29:49] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:29:49] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:29:49] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:29:49] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:29:49] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:29:49] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 02:29:49] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:29:49] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:29:49] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:29:49] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:29:49] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:29:52] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:29:52] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 02:29:52] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 02:29:52] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:29:52] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:29:52] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:29:52] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:29:52] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:29:52] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 02:29:52] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:29:52] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:29:52] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:29:52] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:29:52] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=744634) [2026-01-26 02:29:53] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=744634) [2026-01-26 02:29:53] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=744634) [2026-01-26 02:29:53] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=744634) [2026-01-26 02:29:53] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=744634) [2026-01-26 02:29:53] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=744634) [2026-01-26 02:29:53] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=744634) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=744634) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:10<00:00, 10.70s/it]
(EngineCore_DP0 pid=744634) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:10<00:00, 10.70s/it]
(EngineCore_DP0 pid=744634) 
(EngineCore_DP0 pid=744634) [2026-01-26 02:30:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=744634) [2026-01-26 02:30:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7618560 bytes
(EngineCore_DP0 pid=744634) [2026-01-26 02:30:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=744634) [2026-01-26 02:30:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5079040 bytes
(EngineCore_DP0 pid=744634) [2026-01-26 02:30:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=744634) [2026-01-26 02:30:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 40632320 bytes
(EngineCore_DP0 pid=744634) [2026-01-26 02:30:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=744634) [2026-01-26 02:30:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 20185088 bytes
(EngineCore_DP0 pid=744634) 2026-01-26 02:30:10,712 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=744634) 2026-01-26 02:30:10,721 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 13232.55it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:31,  4.07it/s, est. speed input: 65.13 toks/s, output: 4.07 toks/s]
Processed prompts:   8%|▊         | 10/128 [00:00<00:03, 35.05it/s, est. speed input: 456.62 toks/s, output: 28.54 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:00<00:02, 53.28it/s, est. speed input: 670.98 toks/s, output: 41.93 toks/s]
Processed prompts:  21%|██        | 27/128 [00:00<00:01, 61.81it/s, est. speed input: 780.02 toks/s, output: 48.75 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:00<00:01, 65.37it/s, est. speed input: 842.76 toks/s, output: 52.67 toks/s]
Processed prompts:  34%|███▍      | 44/128 [00:00<00:01, 71.86it/s, est. speed input: 915.34 toks/s, output: 57.21 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:00<00:00, 76.75it/s, est. speed input: 973.03 toks/s, output: 60.81 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:00<00:00, 79.90it/s, est. speed input: 1017.54 toks/s, output: 63.60 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:01<00:00, 81.88it/s, est. speed input: 1052.74 toks/s, output: 65.79 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:01<00:00, 83.58it/s, est. speed input: 1082.98 toks/s, output: 67.68 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:01<00:00, 83.63it/s, est. speed input: 1104.38 toks/s, output: 69.02 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:01<00:00, 84.31it/s, est. speed input: 1124.68 toks/s, output: 70.29 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:01<00:00, 85.25it/s, est. speed input: 1143.56 toks/s, output: 71.47 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:01<00:00, 86.19it/s, est. speed input: 1160.82 toks/s, output: 72.55 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:01<00:00, 86.30it/s, est. speed input: 1174.51 toks/s, output: 73.41 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 86.30it/s, est. speed input: 1179.69 toks/s, output: 73.73 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 73.72it/s, est. speed input: 1179.69 toks/s, output: 73.73 toks/s]
[rank0]:[W126 02:30:13.538338572 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 29.8s

测试结果:
  Requests/s:   73.28
  Tokens/s:     1245.75
  Total Reqs:   128
  Elapsed:      1.75s

  [Prefill 分析]
  Total Prefill Tokens: 2048
  Prefill Tokens/s:     1172.47

============================================================
[2/3] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 128 (= 1 x 128)
│   M_decode      = 1
│   batched_tokens = 129 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 128
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 129
│   --max-num-batched-tokens = 129
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:30:19 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 02:30:19 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=745246) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=745246) WARNING 01-26 02:30:41 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 64.66 requests/s, 8341.72 total tokens/s, 64.66 output tokens/s
Total num prompt tokens:  16384
Total num output tokens:  128


─── STDERR ───
[2026-01-26 02:30:19] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:30:19] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 02:30:19] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 02:30:19] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:30:19] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:30:19] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:30:19] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:30:19] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:30:19] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 02:30:19] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:30:19] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:30:19] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:30:19] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:30:19] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:30:22] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:30:22] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 02:30:22] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 02:30:22] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:30:22] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:30:22] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:30:22] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:30:22] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:30:22] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 02:30:22] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:30:22] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:30:22] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:30:22] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:30:22] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=745246) [2026-01-26 02:30:23] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=745246) [2026-01-26 02:30:23] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=745246) [2026-01-26 02:30:23] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=745246) [2026-01-26 02:30:23] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=745246) [2026-01-26 02:30:23] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=745246) [2026-01-26 02:30:23] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=745246) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=745246) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:10<00:00, 10.66s/it]
(EngineCore_DP0 pid=745246) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:10<00:00, 10.66s/it]
(EngineCore_DP0 pid=745246) 
(EngineCore_DP0 pid=745246) [2026-01-26 02:30:34] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=745246) [2026-01-26 02:30:34] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7618560 bytes
(EngineCore_DP0 pid=745246) [2026-01-26 02:30:34] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=745246) [2026-01-26 02:30:34] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5079040 bytes
(EngineCore_DP0 pid=745246) [2026-01-26 02:30:34] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=745246) [2026-01-26 02:30:34] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 40632320 bytes
(EngineCore_DP0 pid=745246) [2026-01-26 02:30:34] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=745246) [2026-01-26 02:30:34] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 20185088 bytes
(EngineCore_DP0 pid=745246) 2026-01-26 02:30:41,050 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=745246) 2026-01-26 02:30:41,057 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 3598.43it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:28,  4.48it/s, est. speed input: 573.14 toks/s, output: 4.48 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:03, 32.58it/s, est. speed input: 3448.89 toks/s, output: 26.94 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:02, 48.04it/s, est. speed input: 4940.96 toks/s, output: 38.60 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:00<00:01, 57.03it/s, est. speed input: 5836.21 toks/s, output: 45.59 toks/s]
Processed prompts:  25%|██▌       | 32/128 [00:00<00:01, 59.65it/s, est. speed input: 6243.48 toks/s, output: 48.78 toks/s]
Processed prompts:  30%|███       | 39/128 [00:00<00:01, 60.03it/s, est. speed input: 6472.91 toks/s, output: 50.57 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:00<00:01, 64.80it/s, est. speed input: 6860.64 toks/s, output: 53.60 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:00<00:01, 68.42it/s, est. speed input: 7176.45 toks/s, output: 56.06 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:01<00:00, 70.95it/s, est. speed input: 7431.55 toks/s, output: 58.06 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:01<00:00, 72.53it/s, est. speed input: 7636.10 toks/s, output: 59.66 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:01<00:00, 73.81it/s, est. speed input: 7812.61 toks/s, output: 61.04 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:01<00:00, 73.34it/s, est. speed input: 7926.28 toks/s, output: 61.92 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:01<00:00, 73.73it/s, est. speed input: 8041.70 toks/s, output: 62.83 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:01<00:00, 74.67it/s, est. speed input: 8157.74 toks/s, output: 63.73 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:01<00:00, 75.28it/s, est. speed input: 8258.56 toks/s, output: 64.52 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:01<00:00, 75.52it/s, est. speed input: 8343.87 toks/s, output: 65.19 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:01<00:00, 75.81it/s, est. speed input: 8422.13 toks/s, output: 65.80 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 75.81it/s, est. speed input: 8432.38 toks/s, output: 65.88 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:01<00:00, 65.87it/s, est. speed input: 8432.38 toks/s, output: 65.88 toks/s]
[rank0]:[W126 02:30:43.943537859 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 30.3s

测试结果:
  Requests/s:   64.66
  Tokens/s:     8341.72
  Total Reqs:   128
  Elapsed:      1.98s

  [Prefill 分析]
  Total Prefill Tokens: 16384
  Prefill Tokens/s:     8277.06

============================================================
[3/3] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 256 (= 1 x 256)
│   M_decode      = 1
│   batched_tokens = 257 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 256
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 257
│   --max-num-batched-tokens = 257
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:30:49 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 02:30:49 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=745854) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=745854) WARNING 01-26 02:31:11 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 57.14 requests/s, 14685.97 total tokens/s, 57.14 output tokens/s
Total num prompt tokens:  32768
Total num output tokens:  128


─── STDERR ───
[2026-01-26 02:30:49] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:30:49] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 02:30:49] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 02:30:49] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:30:49] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:30:49] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:30:49] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:30:49] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:30:49] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 02:30:49] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:30:49] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:30:49] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:30:49] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:30:49] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:30:52] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:30:52] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 02:30:52] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 02:30:52] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:30:52] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:30:52] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:30:52] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:30:52] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:30:52] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 02:30:52] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:30:52] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:30:52] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:30:52] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:30:52] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=745854) [2026-01-26 02:30:53] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=745854) [2026-01-26 02:30:53] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=745854) [2026-01-26 02:30:53] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=745854) [2026-01-26 02:30:53] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=745854) [2026-01-26 02:30:53] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=745854) [2026-01-26 02:30:53] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=745854) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=745854) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:10<00:00, 10.91s/it]
(EngineCore_DP0 pid=745854) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:10<00:00, 10.91s/it]
(EngineCore_DP0 pid=745854) 
(EngineCore_DP0 pid=745854) [2026-01-26 02:31:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=745854) [2026-01-26 02:31:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7618560 bytes
(EngineCore_DP0 pid=745854) [2026-01-26 02:31:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=745854) [2026-01-26 02:31:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5079040 bytes
(EngineCore_DP0 pid=745854) [2026-01-26 02:31:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=745854) [2026-01-26 02:31:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 40632320 bytes
(EngineCore_DP0 pid=745854) [2026-01-26 02:31:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=745854) [2026-01-26 02:31:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 20185088 bytes
(EngineCore_DP0 pid=745854) 2026-01-26 02:31:10,662 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=745854) 2026-01-26 02:31:10,669 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 2380.00it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:25,  4.92it/s, est. speed input: 1259.22 toks/s, output: 4.92 toks/s]
Processed prompts:   6%|▋         | 8/128 [00:00<00:03, 30.65it/s, est. speed input: 6559.97 toks/s, output: 25.62 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:00<00:02, 43.80it/s, est. speed input: 9159.58 toks/s, output: 35.78 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:02, 48.93it/s, est. speed input: 10317.11 toks/s, output: 40.30 toks/s]
Processed prompts:  21%|██        | 27/128 [00:00<00:01, 52.30it/s, est. speed input: 11107.52 toks/s, output: 43.39 toks/s]
Processed prompts:  27%|██▋       | 34/128 [00:00<00:01, 56.43it/s, est. speed input: 11916.64 toks/s, output: 46.55 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:00<00:01, 59.02it/s, est. speed input: 12510.44 toks/s, output: 48.87 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:00<00:01, 61.11it/s, est. speed input: 12996.68 toks/s, output: 50.77 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:01<00:01, 61.84it/s, est. speed input: 13335.27 toks/s, output: 52.09 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:01<00:01, 62.73it/s, est. speed input: 13636.65 toks/s, output: 53.27 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:01<00:00, 63.12it/s, est. speed input: 13872.54 toks/s, output: 54.19 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:01<00:00, 63.79it/s, est. speed input: 14094.40 toks/s, output: 55.06 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:01<00:00, 63.91it/s, est. speed input: 14265.27 toks/s, output: 55.72 toks/s]
Processed prompts:  70%|███████   | 90/128 [00:01<00:00, 64.62it/s, est. speed input: 14444.54 toks/s, output: 56.42 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:01<00:00, 65.31it/s, est. speed input: 14610.48 toks/s, output: 57.07 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:01<00:00, 65.28it/s, est. speed input: 14734.08 toks/s, output: 57.55 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:01<00:00, 65.12it/s, est. speed input: 14838.18 toks/s, output: 57.96 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:02<00:00, 64.53it/s, est. speed input: 14911.45 toks/s, output: 58.25 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:02<00:00, 63.81it/s, est. speed input: 14964.62 toks/s, output: 58.46 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 63.81it/s, est. speed input: 14995.12 toks/s, output: 58.57 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 58.57it/s, est. speed input: 14995.12 toks/s, output: 58.57 toks/s]
[rank0]:[W126 02:31:13.834414162 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 29.8s

测试结果:
  Requests/s:   57.14
  Tokens/s:     14685.97
  Total Reqs:   128
  Elapsed:      2.24s

  [Prefill 分析]
  Total Prefill Tokens: 32768
  Prefill Tokens/s:     14628.83


------------------------------------------------------------
  生成 CSV: Llama3.2-1B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_10/Llama3.2-1B-INT8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
16,16,1,128,128,73.2796,1245.7537,1.7467
128,128,1,128,128,64.6645,8341.7231,1.9794
256,256,1,128,128,57.1439,14685.9713,2.2400

------------------------------------------------------------

[INFO] 完成: 3 成功, 0 失败

============================================================
  Llama3.2-1B-INT8 | cuSPARSELt (2_10) | decode
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_10

============================================================
[1/3] 测试 M=16
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16
│   M_prefill     = 256 (= 16 x 16)
│   M_decode      = 16
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 16
│   --max-num-seqs           = 16
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:31:19 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 02:31:19 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=746470) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=746470) WARNING 01-26 02:31:40 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 5.06 requests/s, 1376.49 total tokens/s, 1295.52 output tokens/s
Total num prompt tokens:  256
Total num output tokens:  4096


─── STDERR ───
[2026-01-26 02:31:19] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:31:19] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 02:31:19] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 02:31:19] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:31:19] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:31:19] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:31:19] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:31:19] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:31:19] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 02:31:19] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:31:19] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:31:19] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:31:19] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:31:19] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:31:22] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:31:22] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 02:31:22] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 02:31:22] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:31:22] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:31:22] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:31:22] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:31:22] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:31:22] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 02:31:22] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:31:22] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:31:22] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:31:22] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:31:22] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=746470) [2026-01-26 02:31:23] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=746470) [2026-01-26 02:31:23] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=746470) [2026-01-26 02:31:23] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=746470) [2026-01-26 02:31:23] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=746470) [2026-01-26 02:31:23] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=746470) [2026-01-26 02:31:23] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=746470) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=746470) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:10<00:00, 10.86s/it]
(EngineCore_DP0 pid=746470) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:10<00:00, 10.86s/it]
(EngineCore_DP0 pid=746470) 
(EngineCore_DP0 pid=746470) [2026-01-26 02:31:34] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=746470) [2026-01-26 02:31:34] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7618560 bytes
(EngineCore_DP0 pid=746470) [2026-01-26 02:31:34] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=746470) [2026-01-26 02:31:34] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5079040 bytes
(EngineCore_DP0 pid=746470) [2026-01-26 02:31:34] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=746470) [2026-01-26 02:31:34] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 40632320 bytes
(EngineCore_DP0 pid=746470) [2026-01-26 02:31:34] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=746470) [2026-01-26 02:31:34] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 20185088 bytes
(EngineCore_DP0 pid=746470) 2026-01-26 02:31:40,236 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=746470) 2026-01-26 02:31:40,243 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/16 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 16/16 [00:00<00:00, 8315.84it/s]

Processed prompts:   0%|          | 0/16 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   6%|▋         | 1/16 [00:03<00:47,  3.14s/it, est. speed input: 5.09 toks/s, output: 81.47 toks/s]
Processed prompts: 100%|██████████| 16/16 [00:03<00:00,  3.14s/it, est. speed input: 81.04 toks/s, output: 1296.63 toks/s]
Processed prompts: 100%|██████████| 16/16 [00:03<00:00,  5.06it/s, est. speed input: 81.04 toks/s, output: 1296.63 toks/s]
[rank0]:[W126 02:31:44.315266429 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 30.5s

测试结果:
  Requests/s:   5.06
  Tokens/s:     1376.49
  Total Reqs:   16
  Elapsed:      3.16s

  [Decode 分析]
  Total Decode Tokens:  4096
  Decode Tokens/s:      1295.52

============================================================
[2/3] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 2048 (= 128 x 16)
│   M_decode      = 128
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 128
│   --max-num-seqs           = 128
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:31:49 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 02:31:49 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=747091) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=747091) WARNING 01-26 02:32:10 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 22.36 requests/s, 6081.47 total tokens/s, 5723.74 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768


─── STDERR ───
[2026-01-26 02:31:49] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:31:49] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 02:31:49] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 02:31:49] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:31:49] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:31:49] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:31:49] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:31:49] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:31:49] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 02:31:49] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:31:49] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:31:49] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:31:49] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:31:49] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:31:53] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:31:53] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 02:31:53] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 02:31:53] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:31:53] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:31:53] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:31:53] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:31:53] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:31:53] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 02:31:53] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:31:53] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:31:53] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:31:53] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:31:53] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=747091) [2026-01-26 02:31:54] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=747091) [2026-01-26 02:31:54] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=747091) [2026-01-26 02:31:54] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=747091) [2026-01-26 02:31:54] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=747091) [2026-01-26 02:31:54] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=747091) [2026-01-26 02:31:54] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=747091) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=747091) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:10<00:00, 10.67s/it]
(EngineCore_DP0 pid=747091) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:10<00:00, 10.67s/it]
(EngineCore_DP0 pid=747091) 
(EngineCore_DP0 pid=747091) [2026-01-26 02:32:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=747091) [2026-01-26 02:32:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7618560 bytes
(EngineCore_DP0 pid=747091) [2026-01-26 02:32:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=747091) [2026-01-26 02:32:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5079040 bytes
(EngineCore_DP0 pid=747091) [2026-01-26 02:32:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=747091) [2026-01-26 02:32:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 40632320 bytes
(EngineCore_DP0 pid=747091) [2026-01-26 02:32:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=747091) [2026-01-26 02:32:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 20185088 bytes
(EngineCore_DP0 pid=747091) 2026-01-26 02:32:10,419 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=747091) 2026-01-26 02:32:10,427 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 13277.05it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:05<11:20,  5.36s/it, est. speed input: 2.98 toks/s, output: 47.76 toks/s]
Processed prompts:   2%|▏         | 3/128 [00:05<02:59,  1.43s/it, est. speed input: 8.76 toks/s, output: 140.17 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:05<00:03, 21.56it/s, est. speed input: 177.34 toks/s, output: 2837.46 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:05<00:00, 47.25it/s, est. speed input: 330.99 toks/s, output: 5295.77 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:05<00:00, 47.25it/s, est. speed input: 358.39 toks/s, output: 5734.26 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:05<00:00, 22.40it/s, est. speed input: 358.39 toks/s, output: 5734.26 toks/s]
[rank0]:[W126 02:32:16.075848243 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 32.7s

测试结果:
  Requests/s:   22.36
  Tokens/s:     6081.47
  Total Reqs:   128
  Elapsed:      5.72s

  [Decode 分析]
  Total Decode Tokens:  32768
  Decode Tokens/s:      5723.74

============================================================
[3/3] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 4096 (= 256 x 16)
│   M_decode      = 256
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 256
│   --max-num-seqs           = 256
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 02:32:22 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-26 02:32:22 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=747742) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=747742) WARNING 01-26 02:32:43 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 28.18 requests/s, 7665.91 total tokens/s, 7214.98 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536


─── STDERR ───
[2026-01-26 02:32:22] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:32:22] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 02:32:22] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 02:32:22] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:32:22] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:32:22] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:32:22] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:32:22] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:32:22] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 02:32:22] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:32:22] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:32:22] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:32:22] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:32:22] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 02:32:25] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-26 02:32:25] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 02:32:25] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 02:32:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:32:25] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:32:25] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:32:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:32:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 02:32:25] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 02:32:25] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 02:32:25] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 02:32:25] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 02:32:25] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 02:32:25] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=747742) [2026-01-26 02:32:26] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=747742) [2026-01-26 02:32:26] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=747742) [2026-01-26 02:32:26] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=747742) [2026-01-26 02:32:26] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=747742) [2026-01-26 02:32:26] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=747742) [2026-01-26 02:32:26] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=747742) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=747742) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:10<00:00, 10.69s/it]
(EngineCore_DP0 pid=747742) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:10<00:00, 10.69s/it]
(EngineCore_DP0 pid=747742) 
(EngineCore_DP0 pid=747742) [2026-01-26 02:32:37] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=747742) [2026-01-26 02:32:38] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 7618560 bytes
(EngineCore_DP0 pid=747742) [2026-01-26 02:32:38] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=747742) [2026-01-26 02:32:38] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5079040 bytes
(EngineCore_DP0 pid=747742) [2026-01-26 02:32:38] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=747742) [2026-01-26 02:32:38] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 40632320 bytes
(EngineCore_DP0 pid=747742) [2026-01-26 02:32:38] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=747742) [2026-01-26 02:32:38] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 20185088 bytes
(EngineCore_DP0 pid=747742) 2026-01-26 02:32:43,132 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=747742) 2026-01-26 02:32:43,140 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 13455.92it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:07<33:36,  7.91s/it, est. speed input: 2.02 toks/s, output: 32.37 toks/s]
Processed prompts:  14%|█▍        | 36/256 [00:08<00:34,  6.31it/s, est. speed input: 71.69 toks/s, output: 1147.10 toks/s]
Processed prompts:  30%|██▉       | 76/256 [00:08<00:11, 15.95it/s, est. speed input: 149.27 toks/s, output: 2388.29 toks/s]
Processed prompts:  43%|████▎     | 109/256 [00:08<00:05, 26.57it/s, est. speed input: 211.46 toks/s, output: 3383.40 toks/s]
Processed prompts:  54%|█████▍    | 138/256 [00:08<00:03, 38.71it/s, est. speed input: 264.49 toks/s, output: 4231.78 toks/s]
Processed prompts:  65%|██████▌   | 167/256 [00:08<00:01, 54.06it/s, est. speed input: 316.08 toks/s, output: 5057.29 toks/s]
Processed prompts:  77%|███████▋  | 196/256 [00:08<00:00, 72.81it/s, est. speed input: 366.39 toks/s, output: 5862.19 toks/s]
Processed prompts:  88%|████████▊ | 224/256 [00:08<00:00, 91.12it/s, est. speed input: 412.34 toks/s, output: 6597.42 toks/s]
Processed prompts:  98%|█████████▊| 250/256 [00:08<00:00, 95.96it/s, est. speed input: 448.06 toks/s, output: 7168.95 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:09<00:00, 95.96it/s, est. speed input: 451.93 toks/s, output: 7230.84 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:09<00:00, 28.24it/s, est. speed input: 451.93 toks/s, output: 7230.84 toks/s]
[rank0]:[W126 02:32:53.189851079 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 36.2s

测试结果:
  Requests/s:   28.18
  Tokens/s:     7665.91
  Total Reqs:   256
  Elapsed:      9.08s

  [Decode 分析]
  Total Decode Tokens:  65536
  Decode Tokens/s:      7214.98


------------------------------------------------------------
  生成 CSV: Llama3.2-1B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_10/Llama3.2-1B-INT8_decode.csv

预览:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
16,16,16,16,256,256,5.0606,1376.4907,3.1617
128,16,128,128,256,256,22.3584,6081.4716,5.7249
256,16,256,256,256,256,28.1835,7665.9140,9.0833

------------------------------------------------------------

[INFO] 完成: 3 成功, 0 失败


============================================================
  Benchmark 完成!
============================================================


总计: 24 成功, 0 失败
============================================================
