======================================================================
SlideSparse vLLM Throughput Benchmark Log
Created: 2026-01-27 18:25:57
======================================================================

原始命令:
  /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-7b-fp8 --backend cublaslt,cusparselt --stage decode --sparsity 2_4,2_6,2_8,2_10 --M 64,128,256,512

命令行参数:
  --model: qwen2.5-7b-fp8
  --backend: cublaslt,cusparselt
  --sparsity: 2_4,2_6,2_8,2_10
  --stage: decode
  --M: 64,128,256,512
  --N: None
  --inner-32: False
  --eager: False
  --gpu-id: 0
  --gpu-mem: 0.8
  --dry-run: False
  --list-models: False

硬件信息:
  GPU: GB10
  Compute Capability: cc121
  VRAM: 119.7 GB
  CUDA: 12.9
  Python: py312

Backend 环境变量 (初始状态):
  DISABLE_SLIDESPARSE: 未设置
  USE_CUBLASLT: 未设置
  USE_CUSPARSELT: 未设置
  SPARSITY: 未设置
  INNER_DTYPE_32: 未设置

======================================================================


============================================================
  Qwen2.5-7B-FP8 | cuBLASLt | decode
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints/Qwen2.5-7B-FP8
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cublaslt

============================================================
[1/4] 测试 M=64
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuBLASLt                                        │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 64
│   M_prefill     = 1024 (= 64 x 16)
│   M_decode      = 64
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 64
│   --max-num-seqs           = 64
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 18:26:00 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 18:26:00 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2927366) WARNING 01-27 18:26:53 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 4.63 requests/s, 1260.26 total tokens/s, 1186.12 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384


─── STDERR ───
[2026-01-27 18:26:00] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:26:00] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:26:00] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-27 18:26:00] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:26:00] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:26:00] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:26:00] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:26:00] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:26:00] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:26:00] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:26:00] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:26:00] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:26:00] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:26:00] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 18:26:04] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:26:04] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:26:04] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-27 18:26:04] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:26:04] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:26:04] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:26:04] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:26:04] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:26:04] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:26:04] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:26:04] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:26:04] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:26:04] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:26:04] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2927366) [2026-01-27 18:26:05] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=2927366) [2026-01-27 18:26:05] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2927366) [2026-01-27 18:26:05] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=2927366) [2026-01-27 18:26:05] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=2927366) [2026-01-27 18:26:05] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=2927366) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2927366) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:20<00:20, 20.71s/it]
(EngineCore_DP0 pid=2927366) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:41<00:00, 20.81s/it]
(EngineCore_DP0 pid=2927366) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:41<00:00, 20.79s/it]
(EngineCore_DP0 pid=2927366) 
(EngineCore_DP0 pid=2927366) 2026-01-27 18:26:53,283 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2927366) 2026-01-27 18:26:53,312 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 64/64 [00:00<00:00, 10150.32it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 1/64 [00:13<14:14, 13.57s/it, est. speed input: 1.18 toks/s, output: 18.87 toks/s]
Processed prompts:  28%|██▊       | 18/64 [00:13<00:24,  1.84it/s, est. speed input: 21.07 toks/s, output: 337.10 toks/s]
Processed prompts:  97%|█████████▋| 62/64 [00:13<00:00,  8.22it/s, est. speed input: 71.85 toks/s, output: 1149.67 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:13<00:00,  8.22it/s, est. speed input: 74.17 toks/s, output: 1186.75 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:13<00:00,  4.64it/s, est. speed input: 74.17 toks/s, output: 1186.75 toks/s]
[rank0]:[W127 18:27:07.984744198 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 73.6s

测试结果:
  Requests/s:   4.63
  Tokens/s:     1260.26
  Total Reqs:   64
  Elapsed:      13.81s

  [Decode 分析]
  Total Decode Tokens:  16384
  Decode Tokens/s:      1186.12

============================================================
[2/4] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuBLASLt                                        │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 2048 (= 128 x 16)
│   M_decode      = 128
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 128
│   --max-num-seqs           = 128
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 18:27:15 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 18:27:16 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2928610) WARNING 01-27 18:28:06 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 7.75 requests/s, 2107.52 total tokens/s, 1983.54 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768


─── STDERR ───
[2026-01-27 18:27:15] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:27:15] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:27:15] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-27 18:27:15] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:27:15] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:27:15] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:27:15] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:27:15] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:27:15] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:27:15] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:27:15] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:27:15] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:27:15] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:27:15] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 18:27:19] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:27:19] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:27:19] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-27 18:27:19] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:27:19] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:27:19] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:27:19] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:27:19] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:27:19] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:27:19] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:27:19] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:27:19] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:27:19] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:27:19] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2928610) [2026-01-27 18:27:20] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=2928610) [2026-01-27 18:27:20] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2928610) [2026-01-27 18:27:20] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=2928610) [2026-01-27 18:27:20] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=2928610) [2026-01-27 18:27:20] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=2928610) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2928610) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:19<00:19, 19.87s/it]
(EngineCore_DP0 pid=2928610) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:39<00:00, 20.00s/it]
(EngineCore_DP0 pid=2928610) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:39<00:00, 19.98s/it]
(EngineCore_DP0 pid=2928610) 
(EngineCore_DP0 pid=2928610) 2026-01-27 18:28:06,219 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2928610) 2026-01-27 18:28:06,252 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 4325.03it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:15<33:37, 15.89s/it, est. speed input: 1.01 toks/s, output: 16.11 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:16<01:10,  1.57it/s, est. speed input: 17.97 toks/s, output: 287.45 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:16<00:15,  5.30it/s, est. speed input: 47.57 toks/s, output: 761.08 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:16<00:05,  9.93it/s, est. speed input: 73.84 toks/s, output: 1181.36 toks/s]
Processed prompts:  84%|████████▍ | 108/128 [00:16<00:01, 17.53it/s, est. speed input: 105.35 toks/s, output: 1685.64 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:16<00:00, 17.53it/s, est. speed input: 124.21 toks/s, output: 1987.28 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:16<00:00,  7.76it/s, est. speed input: 124.21 toks/s, output: 1987.28 toks/s]
[rank0]:[W127 18:28:23.681694240 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 75.2s

测试结果:
  Requests/s:   7.75
  Tokens/s:     2107.52
  Total Reqs:   128
  Elapsed:      16.52s

  [Decode 分析]
  Total Decode Tokens:  32768
  Decode Tokens/s:      1983.54

============================================================
[3/4] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuBLASLt                                        │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 4096 (= 256 x 16)
│   M_decode      = 256
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 256
│   --max-num-seqs           = 256
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 18:28:31 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 18:28:31 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2929842) WARNING 01-27 18:29:21 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 10.69 requests/s, 2908.43 total tokens/s, 2737.34 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536


─── STDERR ───
[2026-01-27 18:28:31] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:28:31] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:28:31] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-27 18:28:31] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:28:31] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:28:31] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:28:31] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:28:31] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:28:31] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:28:31] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:28:31] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:28:31] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:28:31] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:28:31] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 18:28:34] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:28:34] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:28:34] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-27 18:28:34] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:28:34] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:28:34] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:28:34] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:28:34] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:28:34] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:28:34] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:28:34] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:28:34] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:28:34] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:28:34] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2929842) [2026-01-27 18:28:35] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=2929842) [2026-01-27 18:28:35] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2929842) [2026-01-27 18:28:35] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=2929842) [2026-01-27 18:28:35] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=2929842) [2026-01-27 18:28:35] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=2929842) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2929842) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:19<00:19, 19.67s/it]
(EngineCore_DP0 pid=2929842) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:39<00:00, 19.70s/it]
(EngineCore_DP0 pid=2929842) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:39<00:00, 19.70s/it]
(EngineCore_DP0 pid=2929842) 
(EngineCore_DP0 pid=2929842) 2026-01-27 18:29:20,834 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2929842) 2026-01-27 18:29:20,863 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 11346.86it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:20<1:29:12, 20.99s/it, est. speed input: 0.76 toks/s, output: 12.20 toks/s]
Processed prompts:   8%|▊         | 20/256 [00:21<02:58,  1.32it/s, est. speed input: 15.12 toks/s, output: 241.88 toks/s]
Processed prompts:  19%|█▉        | 49/256 [00:21<00:51,  4.05it/s, est. speed input: 36.76 toks/s, output: 588.09 toks/s]
Processed prompts:  30%|██▉       | 76/256 [00:21<00:23,  7.54it/s, est. speed input: 56.60 toks/s, output: 905.59 toks/s]
Processed prompts:  39%|███▊      | 99/256 [00:21<00:13, 11.58it/s, est. speed input: 73.25 toks/s, output: 1171.97 toks/s]
Processed prompts:  46%|████▋     | 119/256 [00:21<00:08, 16.27it/s, est. speed input: 87.50 toks/s, output: 1400.01 toks/s]
Processed prompts:  54%|█████▎    | 137/256 [00:21<00:05, 21.77it/s, est. speed input: 100.13 toks/s, output: 1602.14 toks/s]
Processed prompts:  60%|█████▉    | 153/256 [00:22<00:03, 27.99it/s, est. speed input: 111.21 toks/s, output: 1779.32 toks/s]
Processed prompts:  66%|██████▌   | 168/256 [00:22<00:02, 35.28it/s, est. speed input: 121.47 toks/s, output: 1943.58 toks/s]
Processed prompts:  71%|███████   | 182/256 [00:22<00:01, 43.35it/s, est. speed input: 130.93 toks/s, output: 2094.88 toks/s]
Processed prompts:  77%|███████▋  | 196/256 [00:22<00:01, 49.82it/s, est. speed input: 139.94 toks/s, output: 2239.10 toks/s]
Processed prompts:  81%|████████▏ | 208/256 [00:22<00:00, 54.45it/s, est. speed input: 147.47 toks/s, output: 2359.49 toks/s]
Processed prompts:  86%|████████▌ | 219/256 [00:22<00:00, 54.56it/s, est. speed input: 153.90 toks/s, output: 2462.41 toks/s]
Processed prompts:  89%|████████▉ | 229/256 [00:22<00:00, 57.55it/s, est. speed input: 159.92 toks/s, output: 2558.67 toks/s]
Processed prompts:  93%|█████████▎| 238/256 [00:23<00:00, 51.54it/s, est. speed input: 164.53 toks/s, output: 2632.44 toks/s]
Processed prompts:  96%|█████████▌| 246/256 [00:23<00:00, 44.26it/s, est. speed input: 168.13 toks/s, output: 2690.08 toks/s]
Processed prompts:  98%|█████████▊| 252/256 [00:23<00:00, 37.16it/s, est. speed input: 170.30 toks/s, output: 2724.74 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:23<00:00, 37.16it/s, est. speed input: 171.25 toks/s, output: 2740.03 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:23<00:00, 10.70it/s, est. speed input: 171.25 toks/s, output: 2740.03 toks/s]
[rank0]:[W127 18:29:45.680684690 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 81.9s

测试结果:
  Requests/s:   10.69
  Tokens/s:     2908.43
  Total Reqs:   256
  Elapsed:      23.94s

  [Decode 分析]
  Total Decode Tokens:  65536
  Decode Tokens/s:      2737.34

============================================================
[4/4] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuBLASLt                                        │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 8192 (= 512 x 16)
│   M_decode      = 512
│   batched_tokens = 512 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 512
│   --max-num-seqs           = 512
│   --max-model-len          = 272
│   --max-num-batched-tokens = 512
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 18:29:51 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 18:29:51 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2931135) WARNING 01-27 18:30:42 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 10.83 requests/s, 2944.70 total tokens/s, 2771.49 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072


─── STDERR ───
[2026-01-27 18:29:51] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:29:51] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:29:51] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-27 18:29:51] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:29:51] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:29:51] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:29:51] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:29:51] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:29:51] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:29:51] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:29:51] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:29:51] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:29:51] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:29:51] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 18:29:54] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:29:54] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:29:54] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-27 18:29:54] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:29:54] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:29:54] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:29:54] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:29:54] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:29:54] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:29:54] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:29:54] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:29:54] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:29:54] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:29:54] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2931135) [2026-01-27 18:29:55] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=2931135) [2026-01-27 18:29:55] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2931135) [2026-01-27 18:29:55] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=2931135) [2026-01-27 18:29:55] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=2931135) [2026-01-27 18:29:55] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=2931135) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2931135) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:19<00:19, 19.94s/it]
(EngineCore_DP0 pid=2931135) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:39<00:00, 19.97s/it]
(EngineCore_DP0 pid=2931135) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:39<00:00, 19.97s/it]
(EngineCore_DP0 pid=2931135) 
(EngineCore_DP0 pid=2931135) 2026-01-27 18:30:41,612 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2931135) 2026-01-27 18:30:41,633 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 12075.23it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:33<4:45:33, 33.53s/it, est. speed input: 0.48 toks/s, output: 7.63 toks/s]
Processed prompts:   1%|          | 5/512 [00:33<42:48,  5.07s/it, est. speed input: 2.36 toks/s, output: 37.80 toks/s] 
Processed prompts:   7%|▋         | 36/512 [00:34<03:55,  2.02it/s, est. speed input: 16.85 toks/s, output: 269.63 toks/s]
Processed prompts:  13%|█▎        | 65/512 [00:34<01:42,  4.35it/s, est. speed input: 30.16 toks/s, output: 482.63 toks/s]
Processed prompts:  18%|█▊        | 93/512 [00:34<00:56,  7.38it/s, est. speed input: 42.80 toks/s, output: 684.79 toks/s]
Processed prompts:  23%|██▎       | 119/512 [00:35<00:35, 11.11it/s, est. speed input: 54.34 toks/s, output: 869.46 toks/s]
Processed prompts:  28%|██▊       | 143/512 [00:35<00:23, 16.04it/s, est. speed input: 65.10 toks/s, output: 1041.54 toks/s]
Processed prompts:  32%|███▏      | 166/512 [00:35<00:15, 22.42it/s, est. speed input: 75.34 toks/s, output: 1205.42 toks/s]
Processed prompts:  37%|███▋      | 187/512 [00:35<00:10, 30.09it/s, est. speed input: 84.63 toks/s, output: 1354.00 toks/s]
Processed prompts:  44%|████▍     | 226/512 [00:35<00:06, 47.55it/s, est. speed input: 101.72 toks/s, output: 1627.51 toks/s]
Processed prompts:  51%|█████     | 259/512 [00:35<00:03, 63.59it/s, est. speed input: 115.97 toks/s, output: 1855.58 toks/s]
Processed prompts:  56%|█████▋    | 288/512 [00:35<00:02, 78.61it/s, est. speed input: 128.36 toks/s, output: 2053.79 toks/s]
Processed prompts:  62%|██████▏   | 315/512 [00:36<00:02, 92.61it/s, est. speed input: 139.77 toks/s, output: 2236.25 toks/s]
Processed prompts:  66%|██████▌   | 338/512 [00:36<00:01, 103.43it/s, est. speed input: 149.36 toks/s, output: 2389.75 toks/s]
Processed prompts:  70%|██████▉   | 358/512 [00:36<00:01, 110.79it/s, est. speed input: 157.59 toks/s, output: 2521.37 toks/s]
Processed prompts:  73%|███████▎  | 376/512 [00:36<00:01, 115.35it/s, est. speed input: 164.89 toks/s, output: 2638.31 toks/s]
Processed prompts:  77%|███████▋  | 392/512 [00:36<00:01, 117.37it/s, est. speed input: 171.31 toks/s, output: 2740.93 toks/s]
Processed prompts:  79%|███████▉  | 407/512 [00:36<00:00, 118.77it/s, est. speed input: 177.28 toks/s, output: 2836.40 toks/s]
Processed prompts:  82%|████████▏ | 421/512 [00:36<00:00, 119.48it/s, est. speed input: 182.80 toks/s, output: 2924.82 toks/s]
Processed prompts:  85%|████████▍ | 435/512 [00:37<00:00, 106.36it/s, est. speed input: 187.99 toks/s, output: 3007.90 toks/s]
Processed prompts:  87%|████████▋ | 447/512 [00:37<00:00, 96.42it/s, est. speed input: 192.34 toks/s, output: 3077.44 toks/s] 
Processed prompts:  89%|████████▉ | 458/512 [00:37<00:00, 88.73it/s, est. speed input: 196.25 toks/s, output: 3140.03 toks/s]
Processed prompts:  91%|█████████▏| 468/512 [00:37<00:00, 75.13it/s, est. speed input: 199.47 toks/s, output: 3191.56 toks/s]
Processed prompts:  93%|█████████▎| 477/512 [00:37<00:00, 65.35it/s, est. speed input: 202.25 toks/s, output: 3235.97 toks/s]
Processed prompts:  95%|█████████▍| 485/512 [00:38<00:00, 50.46it/s, est. speed input: 204.12 toks/s, output: 3265.95 toks/s]
Processed prompts:  96%|█████████▌| 491/512 [00:38<00:00, 39.72it/s, est. speed input: 205.12 toks/s, output: 3281.96 toks/s]
Processed prompts:  97%|█████████▋| 496/512 [00:38<00:00, 34.58it/s, est. speed input: 206.00 toks/s, output: 3296.05 toks/s]
Processed prompts:  98%|█████████▊| 500/512 [00:47<00:04,  2.47it/s, est. speed input: 169.97 toks/s, output: 2719.49 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:47<00:00,  4.24it/s, est. speed input: 173.38 toks/s, output: 2774.05 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:47<00:00,  4.24it/s, est. speed input: 173.38 toks/s, output: 2774.05 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:47<00:00, 10.84it/s, est. speed input: 173.38 toks/s, output: 2774.05 toks/s]
[rank0]:[W127 18:31:29.892280924 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 104.3s

测试结果:
  Requests/s:   10.83
  Tokens/s:     2944.70
  Total Reqs:   512
  Elapsed:      47.29s

  [Decode 分析]
  Total Decode Tokens:  131072
  Decode Tokens/s:      2771.49


------------------------------------------------------------
  生成 CSV: Qwen2.5-7B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cublaslt/Qwen2.5-7B-FP8_decode.csv

预览:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
64,16,64,64,256,256,4.6333,1260.2551,13.8131
128,16,128,128,256,256,7.7482,2107.5155,16.5199
256,16,256,256,256,256,10.6927,2908.4267,23.9415
512,16,512,512,256,256,10.8261,2944.7040,47.2930

------------------------------------------------------------

[INFO] 完成: 4 成功, 0 失败

============================================================
  Qwen2.5-7B-FP8 | cuSPARSELt (2_4) | decode
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_4
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_4

============================================================
[1/4] 测试 M=64
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 64
│   M_prefill     = 1024 (= 64 x 16)
│   M_decode      = 64
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 64
│   --max-num-seqs           = 64
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 18:31:35 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 18:31:35 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2932753) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2932753) WARNING 01-27 18:32:29 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 6.16 requests/s, 1676.21 total tokens/s, 1577.61 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384


─── STDERR ───
[2026-01-27 18:31:35] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:31:35] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:31:35] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-27 18:31:35] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:31:35] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:31:35] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:31:35] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:31:35] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:31:35] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:31:35] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:31:35] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:31:35] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:31:35] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:31:35] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 18:31:39] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:31:39] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:31:39] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-27 18:31:39] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:31:39] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:31:39] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:31:39] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:31:39] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:31:39] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:31:39] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:31:39] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:31:39] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:31:39] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:31:39] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2932753) [2026-01-27 18:31:39] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2932753) [2026-01-27 18:31:40] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2932753) [2026-01-27 18:31:40] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2932753) [2026-01-27 18:31:40] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=2932753) [2026-01-27 18:31:40] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=2932753) [2026-01-27 18:31:40] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=2932753) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2932753) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:20<00:20, 20.29s/it]
(EngineCore_DP0 pid=2932753) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:40<00:00, 20.47s/it]
(EngineCore_DP0 pid=2932753) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:40<00:00, 20.45s/it]
(EngineCore_DP0 pid=2932753) 
(EngineCore_DP0 pid=2932753) [2026-01-27 18:32:21] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 3584] -> 1D uint8
(EngineCore_DP0 pid=2932753) [2026-01-27 18:32:21] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10321920 bytes
(EngineCore_DP0 pid=2932753) [2026-01-27 18:32:21] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 3584] -> 1D uint8
(EngineCore_DP0 pid=2932753) [2026-01-27 18:32:21] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8028160 bytes
(EngineCore_DP0 pid=2932753) [2026-01-27 18:32:21] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 3584] -> 1D uint8
(EngineCore_DP0 pid=2932753) [2026-01-27 18:32:21] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 84869120 bytes
(EngineCore_DP0 pid=2932753) [2026-01-27 18:32:21] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 18944] -> 1D uint8
(EngineCore_DP0 pid=2932753) [2026-01-27 18:32:21] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 42434560 bytes
(EngineCore_DP0 pid=2932753) 2026-01-27 18:32:28,371 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2932753) 2026-01-27 18:32:28,388 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 64/64 [00:00<00:00, 9986.07it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 1/64 [00:10<10:37, 10.12s/it, est. speed input: 1.58 toks/s, output: 25.30 toks/s]
Processed prompts:  33%|███▎      | 21/64 [00:10<00:14,  2.88it/s, est. speed input: 32.83 toks/s, output: 525.28 toks/s]
Processed prompts:  78%|███████▊  | 50/64 [00:10<00:01,  8.45it/s, est. speed input: 77.35 toks/s, output: 1237.54 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:10<00:00,  8.45it/s, est. speed input: 98.72 toks/s, output: 1579.58 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:10<00:00,  6.17it/s, est. speed input: 98.72 toks/s, output: 1579.58 toks/s]
[rank0]:[W127 18:32:39.757796097 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 70.0s

测试结果:
  Requests/s:   6.16
  Tokens/s:     1676.21
  Total Reqs:   64
  Elapsed:      10.39s

  [Decode 分析]
  Total Decode Tokens:  16384
  Decode Tokens/s:      1577.61

============================================================
[2/4] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 2048 (= 128 x 16)
│   M_decode      = 128
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 128
│   --max-num-seqs           = 128
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 18:32:45 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 18:32:45 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2933925) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2933925) WARNING 01-27 18:33:37 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 10.01 requests/s, 2721.81 total tokens/s, 2561.70 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768


─── STDERR ───
[2026-01-27 18:32:45] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:32:45] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:32:45] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-27 18:32:45] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:32:45] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:32:45] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:32:45] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:32:45] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:32:45] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:32:45] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:32:45] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:32:45] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:32:45] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:32:45] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 18:32:49] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:32:49] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:32:49] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-27 18:32:49] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:32:49] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:32:49] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:32:49] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:32:49] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:32:49] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:32:49] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:32:49] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:32:49] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:32:49] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:32:49] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2933925) [2026-01-27 18:32:50] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2933925) [2026-01-27 18:32:50] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2933925) [2026-01-27 18:32:50] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2933925) [2026-01-27 18:32:50] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=2933925) [2026-01-27 18:32:50] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=2933925) [2026-01-27 18:32:50] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=2933925) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2933925) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:19<00:19, 19.74s/it]
(EngineCore_DP0 pid=2933925) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:40<00:00, 20.17s/it]
(EngineCore_DP0 pid=2933925) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:40<00:00, 20.11s/it]
(EngineCore_DP0 pid=2933925) 
(EngineCore_DP0 pid=2933925) [2026-01-27 18:33:31] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 3584] -> 1D uint8
(EngineCore_DP0 pid=2933925) [2026-01-27 18:33:31] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10321920 bytes
(EngineCore_DP0 pid=2933925) [2026-01-27 18:33:31] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 3584] -> 1D uint8
(EngineCore_DP0 pid=2933925) [2026-01-27 18:33:31] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8028160 bytes
(EngineCore_DP0 pid=2933925) [2026-01-27 18:33:31] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 3584] -> 1D uint8
(EngineCore_DP0 pid=2933925) [2026-01-27 18:33:31] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 84869120 bytes
(EngineCore_DP0 pid=2933925) [2026-01-27 18:33:31] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 18944] -> 1D uint8
(EngineCore_DP0 pid=2933925) [2026-01-27 18:33:31] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 42434560 bytes
(EngineCore_DP0 pid=2933925) 2026-01-27 18:33:37,049 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2933925) 2026-01-27 18:33:37,076 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 10719.62it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:12<25:46, 12.17s/it, est. speed input: 1.31 toks/s, output: 21.03 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:12<00:53,  2.05it/s, est. speed input: 23.41 toks/s, output: 374.59 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:12<00:11,  6.86it/s, est. speed input: 61.78 toks/s, output: 988.53 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:12<00:04, 12.69it/s, est. speed input: 95.49 toks/s, output: 1527.76 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:12<00:01, 19.34it/s, est. speed input: 123.64 toks/s, output: 1978.25 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:12<00:00, 19.34it/s, est. speed input: 160.27 toks/s, output: 2564.28 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:12<00:00, 10.02it/s, est. speed input: 160.27 toks/s, output: 2564.28 toks/s]
[rank0]:[W127 18:33:50.861647498 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 71.0s

测试结果:
  Requests/s:   10.01
  Tokens/s:     2721.81
  Total Reqs:   128
  Elapsed:      12.79s

  [Decode 分析]
  Total Decode Tokens:  32768
  Decode Tokens/s:      2561.70

============================================================
[3/4] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 4096 (= 256 x 16)
│   M_decode      = 256
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 256
│   --max-num-seqs           = 256
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 18:33:56 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 18:33:56 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2935102) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2935102) WARNING 01-27 18:34:48 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 12.85 requests/s, 3495.64 total tokens/s, 3290.02 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536


─── STDERR ───
[2026-01-27 18:33:56] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:33:56] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:33:56] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-27 18:33:56] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:33:56] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:33:56] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:33:56] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:33:56] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:33:56] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:33:56] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:33:56] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:33:56] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:33:56] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:33:56] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 18:34:00] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:34:00] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:34:00] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-27 18:34:00] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:34:00] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:34:00] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:34:00] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:34:00] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:34:00] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:34:00] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:34:00] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:34:00] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:34:00] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:34:00] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2935102) [2026-01-27 18:34:01] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2935102) [2026-01-27 18:34:01] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2935102) [2026-01-27 18:34:01] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2935102) [2026-01-27 18:34:01] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=2935102) [2026-01-27 18:34:01] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=2935102) [2026-01-27 18:34:01] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=2935102) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2935102) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:19<00:19, 19.94s/it]
(EngineCore_DP0 pid=2935102) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:39<00:00, 19.92s/it]
(EngineCore_DP0 pid=2935102) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:39<00:00, 19.93s/it]
(EngineCore_DP0 pid=2935102) 
(EngineCore_DP0 pid=2935102) [2026-01-27 18:34:42] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 3584] -> 1D uint8
(EngineCore_DP0 pid=2935102) [2026-01-27 18:34:42] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10321920 bytes
(EngineCore_DP0 pid=2935102) [2026-01-27 18:34:42] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 3584] -> 1D uint8
(EngineCore_DP0 pid=2935102) [2026-01-27 18:34:42] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8028160 bytes
(EngineCore_DP0 pid=2935102) [2026-01-27 18:34:42] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 3584] -> 1D uint8
(EngineCore_DP0 pid=2935102) [2026-01-27 18:34:42] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 84869120 bytes
(EngineCore_DP0 pid=2935102) [2026-01-27 18:34:42] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 18944] -> 1D uint8
(EngineCore_DP0 pid=2935102) [2026-01-27 18:34:42] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 42434560 bytes
(EngineCore_DP0 pid=2935102) 2026-01-27 18:34:47,509 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2935102) 2026-01-27 18:34:47,522 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 11788.09it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:17<1:13:28, 17.29s/it, est. speed input: 0.93 toks/s, output: 14.81 toks/s]
Processed prompts:   7%|▋         | 19/256 [00:17<02:35,  1.52it/s, est. speed input: 17.39 toks/s, output: 278.22 toks/s]
Processed prompts:  19%|█▉        | 48/256 [00:17<00:43,  4.80it/s, est. speed input: 43.55 toks/s, output: 696.78 toks/s]
Processed prompts:  29%|██▉       | 75/256 [00:17<00:20,  8.96it/s, est. speed input: 67.39 toks/s, output: 1078.21 toks/s]
Processed prompts:  38%|███▊      | 98/256 [00:17<00:11, 13.67it/s, est. speed input: 87.26 toks/s, output: 1396.14 toks/s]
Processed prompts:  46%|████▌     | 118/256 [00:18<00:07, 19.11it/s, est. speed input: 104.28 toks/s, output: 1668.56 toks/s]
Processed prompts:  53%|█████▎    | 136/256 [00:18<00:04, 25.41it/s, est. speed input: 119.35 toks/s, output: 1909.52 toks/s]
Processed prompts:  59%|█████▉    | 152/256 [00:18<00:03, 32.48it/s, est. speed input: 132.54 toks/s, output: 2120.56 toks/s]
Processed prompts:  65%|██████▌   | 167/256 [00:18<00:02, 40.64it/s, est. speed input: 144.74 toks/s, output: 2315.89 toks/s]
Processed prompts:  71%|███████   | 182/256 [00:18<00:01, 49.79it/s, est. speed input: 156.72 toks/s, output: 2507.47 toks/s]
Processed prompts:  77%|███████▋  | 196/256 [00:18<00:01, 57.06it/s, est. speed input: 167.44 toks/s, output: 2679.11 toks/s]
Processed prompts:  82%|████████▏ | 209/256 [00:18<00:00, 64.96it/s, est. speed input: 177.40 toks/s, output: 2838.34 toks/s]
Processed prompts:  86%|████████▋ | 221/256 [00:19<00:00, 66.60it/s, est. speed input: 185.93 toks/s, output: 2974.94 toks/s]
Processed prompts:  91%|█████████ | 232/256 [00:19<00:00, 66.20it/s, est. speed input: 193.47 toks/s, output: 3095.49 toks/s]
Processed prompts:  95%|█████████▍| 242/256 [00:19<00:00, 62.77it/s, est. speed input: 199.88 toks/s, output: 3198.15 toks/s]
Processed prompts:  98%|█████████▊| 251/256 [00:19<00:00, 51.18it/s, est. speed input: 204.38 toks/s, output: 3270.09 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:19<00:00, 51.18it/s, est. speed input: 205.86 toks/s, output: 3293.76 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:19<00:00, 12.87it/s, est. speed input: 205.86 toks/s, output: 3293.76 toks/s]
[rank0]:[W127 18:35:08.427917804 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 77.5s

测试结果:
  Requests/s:   12.85
  Tokens/s:     3495.64
  Total Reqs:   256
  Elapsed:      19.92s

  [Decode 分析]
  Total Decode Tokens:  65536
  Decode Tokens/s:      3290.02

============================================================
[4/4] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 8192 (= 512 x 16)
│   M_decode      = 512
│   batched_tokens = 512 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 512
│   --max-num-seqs           = 512
│   --max-model-len          = 272
│   --max-num-batched-tokens = 512
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 18:35:14 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 18:35:14 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2936377) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2936377) WARNING 01-27 18:36:04 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 12.90 requests/s, 3510.11 total tokens/s, 3303.64 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072


─── STDERR ───
[2026-01-27 18:35:14] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:35:14] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:35:14] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-27 18:35:14] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:35:14] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:35:14] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:35:14] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:35:14] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:35:14] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:35:14] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:35:14] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:35:14] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:35:14] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:35:14] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 18:35:17] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:35:17] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:35:17] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-27 18:35:17] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:35:17] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:35:17] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:35:17] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:35:17] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:35:17] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:35:17] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:35:17] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:35:17] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:35:17] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:35:17] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2936377) [2026-01-27 18:35:18] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2936377) [2026-01-27 18:35:18] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2936377) [2026-01-27 18:35:18] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2936377) [2026-01-27 18:35:18] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=2936377) [2026-01-27 18:35:18] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=2936377) [2026-01-27 18:35:18] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=2936377) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2936377) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:19<00:19, 19.76s/it]
(EngineCore_DP0 pid=2936377) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:38<00:00, 19.29s/it]
(EngineCore_DP0 pid=2936377) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:38<00:00, 19.36s/it]
(EngineCore_DP0 pid=2936377) 
(EngineCore_DP0 pid=2936377) [2026-01-27 18:35:58] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 3584] -> 1D uint8
(EngineCore_DP0 pid=2936377) [2026-01-27 18:35:58] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10321920 bytes
(EngineCore_DP0 pid=2936377) [2026-01-27 18:35:58] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 3584] -> 1D uint8
(EngineCore_DP0 pid=2936377) [2026-01-27 18:35:58] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8028160 bytes
(EngineCore_DP0 pid=2936377) [2026-01-27 18:35:58] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 3584] -> 1D uint8
(EngineCore_DP0 pid=2936377) [2026-01-27 18:35:58] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 84869120 bytes
(EngineCore_DP0 pid=2936377) [2026-01-27 18:35:58] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 18944] -> 1D uint8
(EngineCore_DP0 pid=2936377) [2026-01-27 18:35:58] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 42434560 bytes
(EngineCore_DP0 pid=2936377) 2026-01-27 18:36:03,976 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2936377) 2026-01-27 18:36:03,997 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 12019.88it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:28<4:06:32, 28.95s/it, est. speed input: 0.55 toks/s, output: 8.84 toks/s]
Processed prompts:   1%|          | 3/512 [00:29<1:04:39,  7.62s/it, est. speed input: 1.64 toks/s, output: 26.24 toks/s]
Processed prompts:   7%|▋         | 34/512 [00:29<03:33,  2.24it/s, est. speed input: 18.40 toks/s, output: 294.40 toks/s]
Processed prompts:  12%|█▏        | 63/512 [00:29<01:31,  4.92it/s, est. speed input: 33.77 toks/s, output: 540.36 toks/s]
Processed prompts:  18%|█▊        | 91/512 [00:30<00:50,  8.40it/s, est. speed input: 48.35 toks/s, output: 773.59 toks/s]
Processed prompts:  23%|██▎       | 117/512 [00:30<00:31, 12.59it/s, est. speed input: 61.59 toks/s, output: 985.48 toks/s]
Processed prompts:  28%|██▊       | 141/512 [00:30<00:20, 18.10it/s, est. speed input: 73.93 toks/s, output: 1182.95 toks/s]
Processed prompts:  32%|███▏      | 164/512 [00:30<00:13, 25.16it/s, est. speed input: 85.68 toks/s, output: 1370.80 toks/s]
Processed prompts:  36%|███▌      | 185/512 [00:30<00:09, 33.55it/s, est. speed input: 96.31 toks/s, output: 1540.94 toks/s]
Processed prompts:  40%|████      | 205/512 [00:30<00:07, 43.58it/s, est. speed input: 106.35 toks/s, output: 1701.59 toks/s]
Processed prompts:  44%|████▍     | 224/512 [00:30<00:05, 55.11it/s, est. speed input: 115.81 toks/s, output: 1852.97 toks/s]
Processed prompts:  50%|█████     | 258/512 [00:31<00:03, 75.38it/s, est. speed input: 132.47 toks/s, output: 2119.45 toks/s]
Processed prompts:  54%|█████▎    | 275/512 [00:31<00:02, 86.16it/s, est. speed input: 140.73 toks/s, output: 2251.76 toks/s]
Processed prompts:  59%|█████▉    | 302/512 [00:31<00:02, 104.37it/s, est. speed input: 153.80 toks/s, output: 2460.80 toks/s]
Processed prompts:  62%|██████▎   | 320/512 [00:31<00:01, 116.00it/s, est. speed input: 162.44 toks/s, output: 2599.03 toks/s]
Processed prompts:  66%|██████▌   | 338/512 [00:31<00:01, 113.64it/s, est. speed input: 170.67 toks/s, output: 2730.71 toks/s]
Processed prompts:  70%|██████▉   | 358/512 [00:31<00:01, 122.68it/s, est. speed input: 180.02 toks/s, output: 2880.33 toks/s]
Processed prompts:  73%|███████▎  | 376/512 [00:31<00:01, 126.70it/s, est. speed input: 188.31 toks/s, output: 3012.88 toks/s]
Processed prompts:  77%|███████▋  | 392/512 [00:32<00:00, 123.17it/s, est. speed input: 195.46 toks/s, output: 3127.35 toks/s]
Processed prompts:  79%|███████▉  | 406/512 [00:32<00:00, 122.73it/s, est. speed input: 201.72 toks/s, output: 3227.44 toks/s]
Processed prompts:  82%|████████▏ | 420/512 [00:32<00:00, 123.43it/s, est. speed input: 207.95 toks/s, output: 3327.20 toks/s]
Processed prompts:  85%|████████▍ | 434/512 [00:32<00:00, 112.33it/s, est. speed input: 213.86 toks/s, output: 3421.69 toks/s]
Processed prompts:  87%|████████▋ | 446/512 [00:32<00:00, 107.73it/s, est. speed input: 218.92 toks/s, output: 3502.79 toks/s]
Processed prompts:  89%|████████▉ | 458/512 [00:32<00:00, 98.04it/s, est. speed input: 223.76 toks/s, output: 3580.18 toks/s] 
Processed prompts:  92%|█████████▏| 469/512 [00:32<00:00, 86.20it/s, est. speed input: 227.93 toks/s, output: 3646.94 toks/s]
Processed prompts:  94%|█████████▎| 479/512 [00:33<00:00, 76.62it/s, est. speed input: 231.57 toks/s, output: 3705.09 toks/s]
Processed prompts:  95%|█████████▌| 488/512 [00:33<00:00, 58.75it/s, est. speed input: 234.05 toks/s, output: 3744.74 toks/s]
Processed prompts:  97%|█████████▋| 495/512 [00:33<00:00, 49.36it/s, est. speed input: 235.81 toks/s, output: 3772.90 toks/s]
Processed prompts:  98%|█████████▊| 501/512 [00:39<00:02,  4.51it/s, est. speed input: 202.56 toks/s, output: 3241.02 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:39<00:00,  4.51it/s, est. speed input: 206.71 toks/s, output: 3307.28 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:39<00:00, 12.92it/s, est. speed input: 206.71 toks/s, output: 3307.28 toks/s]
[rank0]:[W127 18:36:44.696266674 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 96.4s

测试结果:
  Requests/s:   12.90
  Tokens/s:     3510.11
  Total Reqs:   512
  Elapsed:      39.68s

  [Decode 分析]
  Total Decode Tokens:  131072
  Decode Tokens/s:      3303.64


------------------------------------------------------------
  生成 CSV: Qwen2.5-7B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_4/Qwen2.5-7B-FP8_decode.csv

预览:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
64,16,64,64,256,256,6.1626,1676.2142,10.3853
128,16,128,128,256,256,10.0067,2721.8112,12.7915
256,16,256,256,256,256,12.8516,3495.6411,19.9197
512,16,512,512,256,256,12.9048,3510.1143,39.6751

------------------------------------------------------------

[INFO] 完成: 4 成功, 0 失败

============================================================
  Qwen2.5-7B-FP8 | cuSPARSELt (2_6) | decode
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_6
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_6

============================================================
[1/4] 测试 M=64
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 64
│   M_prefill     = 1024 (= 64 x 16)
│   M_decode      = 64
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 64
│   --max-num-seqs           = 64
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 18:36:50 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 18:36:50 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2939319) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2939319) WARNING 01-27 18:38:01 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 4.94 requests/s, 1343.24 total tokens/s, 1264.23 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384


─── STDERR ───
[2026-01-27 18:36:50] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:36:50] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:36:50] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-27 18:36:50] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:36:50] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:36:50] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:36:50] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:36:50] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:36:50] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:36:50] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:36:50] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:36:50] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:36:50] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:36:50] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 18:36:54] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:36:54] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:36:54] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-27 18:36:54] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:36:54] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:36:54] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:36:54] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:36:54] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:36:54] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:36:54] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:36:54] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:36:54] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:36:54] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:36:54] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2939319) [2026-01-27 18:36:55] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2939319) [2026-01-27 18:36:55] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2939319) [2026-01-27 18:36:55] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2939319) [2026-01-27 18:36:55] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=2939319) [2026-01-27 18:36:55] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=2939319) [2026-01-27 18:36:55] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=2939319) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2939319) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:23<00:23, 23.75s/it]
(EngineCore_DP0 pid=2939319) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:57<00:00, 29.39s/it]
(EngineCore_DP0 pid=2939319) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:57<00:00, 28.55s/it]
(EngineCore_DP0 pid=2939319) 
(EngineCore_DP0 pid=2939319) [2026-01-27 18:37:53] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 4800] -> 1D uint8
(EngineCore_DP0 pid=2939319) [2026-01-27 18:37:53] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 13860864 bytes
(EngineCore_DP0 pid=2939319) [2026-01-27 18:37:53] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 4800] -> 1D uint8
(EngineCore_DP0 pid=2939319) [2026-01-27 18:37:53] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10780672 bytes
(EngineCore_DP0 pid=2939319) [2026-01-27 18:37:53] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 4800] -> 1D uint8
(EngineCore_DP0 pid=2939319) [2026-01-27 18:37:53] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 113967104 bytes
(EngineCore_DP0 pid=2939319) [2026-01-27 18:37:53] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 25280] -> 1D uint8
(EngineCore_DP0 pid=2939319) [2026-01-27 18:37:53] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 56655872 bytes
(EngineCore_DP0 pid=2939319) 2026-01-27 18:38:00,429 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2939319) 2026-01-27 18:38:00,447 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 64/64 [00:00<00:00, 9981.61it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 1/64 [00:12<13:18, 12.67s/it, est. speed input: 1.26 toks/s, output: 20.21 toks/s]
Processed prompts:  31%|███▏      | 20/64 [00:12<00:20,  2.20it/s, est. speed input: 25.06 toks/s, output: 400.93 toks/s]
Processed prompts:  77%|███████▋  | 49/64 [00:12<00:02,  6.66it/s, est. speed input: 60.71 toks/s, output: 971.44 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:12<00:00,  6.66it/s, est. speed input: 79.06 toks/s, output: 1264.97 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:12<00:00,  4.94it/s, est. speed input: 79.06 toks/s, output: 1264.97 toks/s]
[rank0]:[W127 18:38:14.295555224 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 89.7s

测试结果:
  Requests/s:   4.94
  Tokens/s:     1343.24
  Total Reqs:   64
  Elapsed:      12.96s

  [Decode 分析]
  Total Decode Tokens:  16384
  Decode Tokens/s:      1264.23

============================================================
[2/4] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 2048 (= 128 x 16)
│   M_decode      = 128
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 128
│   --max-num-seqs           = 128
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 18:38:20 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 18:38:20 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2941003) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2941003) WARNING 01-27 18:39:29 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 8.15 requests/s, 2216.89 total tokens/s, 2086.48 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768


─── STDERR ───
[2026-01-27 18:38:20] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:38:20] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:38:20] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-27 18:38:20] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:38:20] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:38:20] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:38:20] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:38:20] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:38:20] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:38:20] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:38:20] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:38:20] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:38:20] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:38:20] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 18:38:23] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:38:23] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:38:23] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-27 18:38:23] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:38:23] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:38:23] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:38:23] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:38:23] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:38:23] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:38:23] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:38:23] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:38:23] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:38:23] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:38:23] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2941003) [2026-01-27 18:38:24] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2941003) [2026-01-27 18:38:25] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2941003) [2026-01-27 18:38:25] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2941003) [2026-01-27 18:38:25] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=2941003) [2026-01-27 18:38:25] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=2941003) [2026-01-27 18:38:25] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=2941003) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2941003) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:23<00:23, 23.59s/it]
(EngineCore_DP0 pid=2941003) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:56<00:00, 29.19s/it]
(EngineCore_DP0 pid=2941003) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:56<00:00, 28.35s/it]
(EngineCore_DP0 pid=2941003) 
(EngineCore_DP0 pid=2941003) [2026-01-27 18:39:22] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 4800] -> 1D uint8
(EngineCore_DP0 pid=2941003) [2026-01-27 18:39:22] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 13860864 bytes
(EngineCore_DP0 pid=2941003) [2026-01-27 18:39:22] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 4800] -> 1D uint8
(EngineCore_DP0 pid=2941003) [2026-01-27 18:39:22] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10780672 bytes
(EngineCore_DP0 pid=2941003) [2026-01-27 18:39:22] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 4800] -> 1D uint8
(EngineCore_DP0 pid=2941003) [2026-01-27 18:39:22] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 113967104 bytes
(EngineCore_DP0 pid=2941003) [2026-01-27 18:39:22] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 25280] -> 1D uint8
(EngineCore_DP0 pid=2941003) [2026-01-27 18:39:22] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 56655872 bytes
(EngineCore_DP0 pid=2941003) 2026-01-27 18:39:28,626 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2941003) 2026-01-27 18:39:28,641 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 10960.35it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:15<31:46, 15.01s/it, est. speed input: 1.07 toks/s, output: 17.06 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:15<01:06,  1.66it/s, est. speed input: 19.01 toks/s, output: 304.16 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:15<00:14,  5.59it/s, est. speed input: 50.24 toks/s, output: 803.85 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:15<00:05, 10.38it/s, est. speed input: 77.76 toks/s, output: 1244.19 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:15<00:01, 15.84it/s, est. speed input: 100.72 toks/s, output: 1611.47 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:15<00:00, 25.18it/s, est. speed input: 129.49 toks/s, output: 2071.86 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:15<00:00, 25.18it/s, est. speed input: 130.51 toks/s, output: 2088.16 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:15<00:00,  8.16it/s, est. speed input: 130.51 toks/s, output: 2088.16 toks/s]
[rank0]:[W127 18:39:45.281962066 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 90.9s

测试结果:
  Requests/s:   8.15
  Tokens/s:     2216.89
  Total Reqs:   128
  Elapsed:      15.70s

  [Decode 分析]
  Total Decode Tokens:  32768
  Decode Tokens/s:      2086.48

============================================================
[3/4] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 4096 (= 256 x 16)
│   M_decode      = 256
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 256
│   --max-num-seqs           = 256
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 18:39:51 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 18:39:51 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2942767) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2942767) WARNING 01-27 18:40:59 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 10.85 requests/s, 2950.26 total tokens/s, 2776.72 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536


─── STDERR ───
[2026-01-27 18:39:50] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:39:51] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:39:51] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-27 18:39:51] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:39:51] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:39:51] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:39:51] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:39:51] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:39:51] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:39:51] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:39:51] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:39:51] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:39:51] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:39:51] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 18:39:54] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:39:54] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:39:54] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-27 18:39:54] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:39:54] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:39:54] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:39:54] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:39:54] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:39:54] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:39:54] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:39:54] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:39:54] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:39:54] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:39:54] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2942767) [2026-01-27 18:39:55] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2942767) [2026-01-27 18:39:55] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2942767) [2026-01-27 18:39:55] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2942767) [2026-01-27 18:39:55] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=2942767) [2026-01-27 18:39:55] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=2942767) [2026-01-27 18:39:55] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=2942767) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2942767) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:23<00:23, 23.42s/it]
(EngineCore_DP0 pid=2942767) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:56<00:00, 29.19s/it]
(EngineCore_DP0 pid=2942767) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:56<00:00, 28.33s/it]
(EngineCore_DP0 pid=2942767) 
(EngineCore_DP0 pid=2942767) [2026-01-27 18:40:53] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 4800] -> 1D uint8
(EngineCore_DP0 pid=2942767) [2026-01-27 18:40:53] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 13860864 bytes
(EngineCore_DP0 pid=2942767) [2026-01-27 18:40:53] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 4800] -> 1D uint8
(EngineCore_DP0 pid=2942767) [2026-01-27 18:40:53] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10780672 bytes
(EngineCore_DP0 pid=2942767) [2026-01-27 18:40:53] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 4800] -> 1D uint8
(EngineCore_DP0 pid=2942767) [2026-01-27 18:40:53] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 113967104 bytes
(EngineCore_DP0 pid=2942767) [2026-01-27 18:40:53] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 25280] -> 1D uint8
(EngineCore_DP0 pid=2942767) [2026-01-27 18:40:53] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 56655872 bytes
(EngineCore_DP0 pid=2942767) 2026-01-27 18:40:58,966 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2942767) 2026-01-27 18:40:58,981 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 11114.76it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:20<1:26:37, 20.38s/it, est. speed input: 0.78 toks/s, output: 12.56 toks/s]
Processed prompts:   2%|▏         | 4/256 [00:20<16:18,  3.88s/it, est. speed input: 3.12 toks/s, output: 49.98 toks/s]  
Processed prompts:   8%|▊         | 20/256 [00:20<02:09,  1.83it/s, est. speed input: 15.54 toks/s, output: 248.62 toks/s]
Processed prompts:  14%|█▎        | 35/256 [00:20<00:57,  3.87it/s, est. speed input: 27.04 toks/s, output: 432.67 toks/s]
Processed prompts:  19%|█▉        | 49/256 [00:20<00:31,  6.49it/s, est. speed input: 37.65 toks/s, output: 602.36 toks/s]
Processed prompts:  30%|██▉       | 76/256 [00:20<00:13, 13.32it/s, est. speed input: 57.92 toks/s, output: 926.76 toks/s]
Processed prompts:  39%|███▊      | 99/256 [00:21<00:07, 20.54it/s, est. speed input: 74.79 toks/s, output: 1196.62 toks/s]
Processed prompts:  46%|████▋     | 119/256 [00:21<00:04, 28.35it/s, est. speed input: 89.23 toks/s, output: 1427.73 toks/s]
Processed prompts:  54%|█████▎    | 137/256 [00:21<00:03, 36.47it/s, est. speed input: 101.97 toks/s, output: 1631.54 toks/s]
Processed prompts:  60%|█████▉    | 153/256 [00:21<00:02, 44.69it/s, est. speed input: 113.12 toks/s, output: 1809.97 toks/s]
Processed prompts:  65%|██████▌   | 167/256 [00:21<00:01, 51.59it/s, est. speed input: 122.63 toks/s, output: 1962.07 toks/s]
Processed prompts:  70%|██████▉   | 179/256 [00:21<00:01, 57.90it/s, est. speed input: 130.68 toks/s, output: 2090.96 toks/s]
Processed prompts:  74%|███████▍  | 190/256 [00:22<00:01, 65.09it/s, est. speed input: 138.08 toks/s, output: 2209.20 toks/s]
Processed prompts:  79%|███████▊  | 201/256 [00:22<00:00, 68.47it/s, est. speed input: 145.17 toks/s, output: 2322.75 toks/s]
Processed prompts:  82%|████████▏ | 211/256 [00:22<00:00, 68.62it/s, est. speed input: 151.41 toks/s, output: 2422.48 toks/s]
Processed prompts:  86%|████████▋ | 221/256 [00:22<00:00, 69.62it/s, est. speed input: 157.61 toks/s, output: 2521.72 toks/s]
Processed prompts:  90%|████████▉ | 230/256 [00:22<00:00, 61.27it/s, est. speed input: 162.58 toks/s, output: 2601.30 toks/s]
Processed prompts:  93%|█████████▎| 238/256 [00:22<00:00, 56.26it/s, est. speed input: 166.92 toks/s, output: 2670.70 toks/s]
Processed prompts:  96%|█████████▌| 245/256 [00:23<00:00, 46.97it/s, est. speed input: 170.11 toks/s, output: 2721.77 toks/s]
Processed prompts:  98%|█████████▊| 251/256 [00:23<00:00, 39.26it/s, est. speed input: 172.47 toks/s, output: 2759.55 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:23<00:00, 30.88it/s, est. speed input: 173.72 toks/s, output: 2779.55 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:23<00:00, 30.88it/s, est. speed input: 173.72 toks/s, output: 2779.55 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:23<00:00, 10.86it/s, est. speed input: 173.72 toks/s, output: 2779.55 toks/s]
[rank0]:[W127 18:41:23.812289090 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 98.5s

测试结果:
  Requests/s:   10.85
  Tokens/s:     2950.26
  Total Reqs:   256
  Elapsed:      23.60s

  [Decode 分析]
  Total Decode Tokens:  65536
  Decode Tokens/s:      2776.72

============================================================
[4/4] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 8192 (= 512 x 16)
│   M_decode      = 512
│   batched_tokens = 512 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 512
│   --max-num-seqs           = 512
│   --max-model-len          = 272
│   --max-num-batched-tokens = 512
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 18:41:29 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 18:41:29 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2944729) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2944729) WARNING 01-27 18:42:39 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 11.02 requests/s, 2997.23 total tokens/s, 2820.92 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072


─── STDERR ───
[2026-01-27 18:41:29] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:41:29] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:41:29] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-27 18:41:29] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:41:29] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:41:29] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:41:29] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:41:29] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:41:29] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:41:29] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:41:29] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:41:29] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:41:29] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:41:29] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 18:41:33] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:41:33] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:41:33] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-27 18:41:33] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:41:33] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:41:33] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:41:33] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:41:33] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:41:33] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:41:33] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:41:33] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:41:33] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:41:33] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:41:33] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2944729) [2026-01-27 18:41:34] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2944729) [2026-01-27 18:41:34] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2944729) [2026-01-27 18:41:34] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2944729) [2026-01-27 18:41:34] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=2944729) [2026-01-27 18:41:34] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=2944729) [2026-01-27 18:41:34] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=2944729) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2944729) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:24<00:24, 24.13s/it]
(EngineCore_DP0 pid=2944729) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:57<00:00, 29.83s/it]
(EngineCore_DP0 pid=2944729) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:57<00:00, 28.98s/it]
(EngineCore_DP0 pid=2944729) 
(EngineCore_DP0 pid=2944729) [2026-01-27 18:42:33] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 4800] -> 1D uint8
(EngineCore_DP0 pid=2944729) [2026-01-27 18:42:33] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 13860864 bytes
(EngineCore_DP0 pid=2944729) [2026-01-27 18:42:33] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 4800] -> 1D uint8
(EngineCore_DP0 pid=2944729) [2026-01-27 18:42:33] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 10780672 bytes
(EngineCore_DP0 pid=2944729) [2026-01-27 18:42:33] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 4800] -> 1D uint8
(EngineCore_DP0 pid=2944729) [2026-01-27 18:42:33] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 113967104 bytes
(EngineCore_DP0 pid=2944729) [2026-01-27 18:42:33] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 25280] -> 1D uint8
(EngineCore_DP0 pid=2944729) [2026-01-27 18:42:33] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 56655872 bytes
(EngineCore_DP0 pid=2944729) 2026-01-27 18:42:39,014 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2944729) 2026-01-27 18:42:39,039 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 12618.90it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:32<4:39:52, 32.86s/it, est. speed input: 0.49 toks/s, output: 7.79 toks/s]
Processed prompts:   1%|          | 6/512 [00:33<34:31,  4.09s/it, est. speed input: 2.89 toks/s, output: 46.28 toks/s] 
Processed prompts:   7%|▋         | 37/512 [00:33<03:45,  2.10it/s, est. speed input: 17.67 toks/s, output: 282.72 toks/s]
Processed prompts:  13%|█▎        | 66/512 [00:33<01:39,  4.48it/s, est. speed input: 31.25 toks/s, output: 499.93 toks/s]
Processed prompts:  18%|█▊        | 93/512 [00:34<00:56,  7.45it/s, est. speed input: 43.65 toks/s, output: 698.43 toks/s]
Processed prompts:  23%|██▎       | 119/512 [00:34<00:35, 11.21it/s, est. speed input: 55.39 toks/s, output: 886.29 toks/s]
Processed prompts:  28%|██▊       | 143/512 [00:34<00:22, 16.14it/s, est. speed input: 66.31 toks/s, output: 1060.93 toks/s]
Processed prompts:  32%|███▏      | 166/512 [00:34<00:15, 22.44it/s, est. speed input: 76.69 toks/s, output: 1227.11 toks/s]
Processed prompts:  37%|███▋      | 187/512 [00:34<00:10, 29.88it/s, est. speed input: 86.09 toks/s, output: 1377.43 toks/s]
Processed prompts:  40%|████      | 207/512 [00:34<00:07, 38.85it/s, est. speed input: 94.97 toks/s, output: 1519.56 toks/s]
Processed prompts:  44%|████▍     | 226/512 [00:34<00:05, 49.17it/s, est. speed input: 103.34 toks/s, output: 1653.51 toks/s]
Processed prompts:  47%|████▋     | 243/512 [00:35<00:04, 59.37it/s, est. speed input: 110.74 toks/s, output: 1771.91 toks/s]
Processed prompts:  51%|█████     | 259/512 [00:35<00:03, 68.73it/s, est. speed input: 117.61 toks/s, output: 1881.72 toks/s]
Processed prompts:  54%|█████▎    | 274/512 [00:35<00:03, 79.22it/s, est. speed input: 124.04 toks/s, output: 1984.72 toks/s]
Processed prompts:  59%|█████▉    | 302/512 [00:35<00:02, 99.15it/s, est. speed input: 136.03 toks/s, output: 2176.50 toks/s]
Processed prompts:  64%|██████▍   | 327/512 [00:35<00:01, 107.52it/s, est. speed input: 146.49 toks/s, output: 2343.83 toks/s]
Processed prompts:  68%|██████▊   | 348/512 [00:35<00:01, 113.26it/s, est. speed input: 155.20 toks/s, output: 2483.14 toks/s]
Processed prompts:  72%|███████▏  | 367/512 [00:36<00:01, 116.30it/s, est. speed input: 162.98 toks/s, output: 2607.68 toks/s]
Processed prompts:  75%|███████▌  | 384/512 [00:36<00:01, 114.86it/s, est. speed input: 169.81 toks/s, output: 2716.92 toks/s]
Processed prompts:  78%|███████▊  | 397/512 [00:36<00:00, 117.27it/s, est. speed input: 175.06 toks/s, output: 2800.95 toks/s]
Processed prompts:  80%|████████  | 410/512 [00:36<00:00, 111.30it/s, est. speed input: 180.12 toks/s, output: 2881.89 toks/s]
Processed prompts:  82%|████████▏ | 422/512 [00:36<00:00, 105.37it/s, est. speed input: 184.72 toks/s, output: 2955.44 toks/s]
Processed prompts:  85%|████████▍ | 433/512 [00:36<00:00, 94.42it/s, est. speed input: 188.73 toks/s, output: 3019.67 toks/s] 
Processed prompts:  87%|████████▋ | 443/512 [00:36<00:00, 95.67it/s, est. speed input: 192.56 toks/s, output: 3081.01 toks/s]
Processed prompts:  88%|████████▊ | 453/512 [00:36<00:00, 79.47it/s, est. speed input: 195.91 toks/s, output: 3134.48 toks/s]
Processed prompts:  90%|█████████ | 462/512 [00:37<00:00, 74.63it/s, est. speed input: 199.03 toks/s, output: 3184.43 toks/s]
Processed prompts:  92%|█████████▏| 470/512 [00:37<00:00, 64.35it/s, est. speed input: 201.50 toks/s, output: 3224.01 toks/s]
Processed prompts:  93%|█████████▎| 477/512 [00:37<00:00, 60.94it/s, est. speed input: 203.76 toks/s, output: 3260.21 toks/s]
Processed prompts:  95%|█████████▍| 484/512 [00:37<00:00, 47.88it/s, est. speed input: 205.43 toks/s, output: 3286.82 toks/s]
Processed prompts:  96%|█████████▌| 490/512 [00:37<00:00, 38.56it/s, est. speed input: 206.57 toks/s, output: 3305.13 toks/s]
Processed prompts:  97%|█████████▋| 495/512 [00:38<00:00, 33.97it/s, est. speed input: 207.53 toks/s, output: 3320.43 toks/s]
Processed prompts:  97%|█████████▋| 499/512 [00:46<00:05,  2.37it/s, est. speed input: 172.75 toks/s, output: 2763.99 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:46<00:00,  4.39it/s, est. speed input: 176.47 toks/s, output: 2823.50 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:46<00:00,  4.39it/s, est. speed input: 176.47 toks/s, output: 2823.50 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:46<00:00, 11.03it/s, est. speed input: 176.47 toks/s, output: 2823.50 toks/s]
[rank0]:[W127 18:43:26.639234502 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 123.0s

测试结果:
  Requests/s:   11.02
  Tokens/s:     2997.23
  Total Reqs:   512
  Elapsed:      46.46s

  [Decode 分析]
  Total Decode Tokens:  131072
  Decode Tokens/s:      2820.92


------------------------------------------------------------
  生成 CSV: Qwen2.5-7B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_6/Qwen2.5-7B-FP8_decode.csv

预览:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
64,16,64,64,256,256,4.9384,1343.2415,12.9597
128,16,128,128,256,256,8.1503,2216.8892,15.7049
256,16,256,256,256,256,10.8465,2950.2616,23.6020
512,16,512,512,256,256,11.0192,2997.2294,46.4642

------------------------------------------------------------

[INFO] 完成: 4 成功, 0 失败

============================================================
  Qwen2.5-7B-FP8 | cuSPARSELt (2_8) | decode
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_8
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_8

============================================================
[1/4] 测试 M=64
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 64
│   M_prefill     = 1024 (= 64 x 16)
│   M_decode      = 64
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 64
│   --max-num-seqs           = 64
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 18:43:32 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 18:43:32 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2947027) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2947027) WARNING 01-27 18:44:46 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 4.88 requests/s, 1327.10 total tokens/s, 1249.03 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384


─── STDERR ───
[2026-01-27 18:43:32] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:43:32] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:43:32] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-27 18:43:32] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:43:32] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:43:32] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:43:32] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:43:32] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:43:32] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:43:32] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:43:32] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:43:32] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:43:32] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:43:32] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 18:43:36] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:43:36] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:43:36] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-27 18:43:36] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:43:36] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:43:36] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:43:36] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:43:36] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:43:36] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:43:36] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:43:36] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:43:36] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:43:36] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:43:36] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2947027) [2026-01-27 18:43:37] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2947027) [2026-01-27 18:43:37] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2947027) [2026-01-27 18:43:37] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2947027) [2026-01-27 18:43:37] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=2947027) [2026-01-27 18:43:37] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=2947027) [2026-01-27 18:43:37] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=2947027) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2947027) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:27<00:27, 27.55s/it]
(EngineCore_DP0 pid=2947027) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [01:00<00:00, 30.83s/it]
(EngineCore_DP0 pid=2947027) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [01:00<00:00, 30.34s/it]
(EngineCore_DP0 pid=2947027) 
(EngineCore_DP0 pid=2947027) [2026-01-27 18:44:39] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5376] -> 1D uint8
(EngineCore_DP0 pid=2947027) [2026-01-27 18:44:39] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15482880 bytes
(EngineCore_DP0 pid=2947027) [2026-01-27 18:44:39] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5376] -> 1D uint8
(EngineCore_DP0 pid=2947027) [2026-01-27 18:44:39] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 12042240 bytes
(EngineCore_DP0 pid=2947027) [2026-01-27 18:44:39] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5376] -> 1D uint8
(EngineCore_DP0 pid=2947027) [2026-01-27 18:44:39] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 127303680 bytes
(EngineCore_DP0 pid=2947027) [2026-01-27 18:44:39] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 28416] -> 1D uint8
(EngineCore_DP0 pid=2947027) [2026-01-27 18:44:39] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 63651840 bytes
(EngineCore_DP0 pid=2947027) 2026-01-27 18:44:46,182 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2947027) 2026-01-27 18:44:46,196 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 64/64 [00:00<00:00, 2674.81it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 1/64 [00:12<13:26, 12.80s/it, est. speed input: 1.25 toks/s, output: 20.01 toks/s]
Processed prompts:  28%|██▊       | 18/64 [00:12<00:23,  1.95it/s, est. speed input: 22.29 toks/s, output: 356.71 toks/s]
Processed prompts:  75%|███████▌  | 48/64 [00:13<00:02,  6.55it/s, est. speed input: 58.94 toks/s, output: 943.04 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:13<00:00,  6.55it/s, est. speed input: 78.35 toks/s, output: 1253.66 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:13<00:00,  4.90it/s, est. speed input: 78.35 toks/s, output: 1253.66 toks/s]
[rank0]:[W127 18:45:00.244919407 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 94.5s

测试结果:
  Requests/s:   4.88
  Tokens/s:     1327.10
  Total Reqs:   64
  Elapsed:      13.12s

  [Decode 分析]
  Total Decode Tokens:  16384
  Decode Tokens/s:      1249.03

============================================================
[2/4] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 2048 (= 128 x 16)
│   M_decode      = 128
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 128
│   --max-num-seqs           = 128
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 18:45:08 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 18:45:09 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2948829) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2948829) WARNING 01-27 18:46:22 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 8.04 requests/s, 2186.81 total tokens/s, 2058.17 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768


─── STDERR ───
[2026-01-27 18:45:08] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:45:08] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:45:08] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-27 18:45:08] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:45:08] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:45:08] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:45:08] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:45:08] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:45:08] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:45:08] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:45:08] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:45:08] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:45:08] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:45:08] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 18:45:12] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:45:12] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:45:12] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-27 18:45:12] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:45:12] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:45:12] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:45:12] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:45:12] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:45:12] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:45:12] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:45:12] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:45:12] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:45:12] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:45:12] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2948829) [2026-01-27 18:45:13] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2948829) [2026-01-27 18:45:13] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2948829) [2026-01-27 18:45:13] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2948829) [2026-01-27 18:45:13] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=2948829) [2026-01-27 18:45:13] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=2948829) [2026-01-27 18:45:13] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=2948829) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2948829) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:27<00:27, 27.96s/it]
(EngineCore_DP0 pid=2948829) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [01:00<00:00, 30.78s/it]
(EngineCore_DP0 pid=2948829) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [01:00<00:00, 30.36s/it]
(EngineCore_DP0 pid=2948829) 
(EngineCore_DP0 pid=2948829) [2026-01-27 18:46:15] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5376] -> 1D uint8
(EngineCore_DP0 pid=2948829) [2026-01-27 18:46:15] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15482880 bytes
(EngineCore_DP0 pid=2948829) [2026-01-27 18:46:15] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5376] -> 1D uint8
(EngineCore_DP0 pid=2948829) [2026-01-27 18:46:15] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 12042240 bytes
(EngineCore_DP0 pid=2948829) [2026-01-27 18:46:15] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5376] -> 1D uint8
(EngineCore_DP0 pid=2948829) [2026-01-27 18:46:15] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 127303680 bytes
(EngineCore_DP0 pid=2948829) [2026-01-27 18:46:15] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 28416] -> 1D uint8
(EngineCore_DP0 pid=2948829) [2026-01-27 18:46:15] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 63651840 bytes
(EngineCore_DP0 pid=2948829) 2026-01-27 18:46:21,435 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2948829) 2026-01-27 18:46:21,449 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 10539.28it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:15<32:18, 15.26s/it, est. speed input: 1.05 toks/s, output: 16.77 toks/s]
Processed prompts:  19%|█▉        | 24/128 [00:15<00:47,  2.19it/s, est. speed input: 24.96 toks/s, output: 399.29 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:15<00:12,  5.90it/s, est. speed input: 54.56 toks/s, output: 872.98 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:15<00:04, 10.29it/s, est. speed input: 79.66 toks/s, output: 1274.56 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:15<00:01, 15.69it/s, est. speed input: 102.26 toks/s, output: 1636.21 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:15<00:00, 21.99it/s, est. speed input: 121.70 toks/s, output: 1947.23 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:15<00:00, 21.99it/s, est. speed input: 128.74 toks/s, output: 2059.86 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:15<00:00,  8.05it/s, est. speed input: 128.74 toks/s, output: 2059.86 toks/s]
[rank0]:[W127 18:46:38.472964834 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 97.2s

测试结果:
  Requests/s:   8.04
  Tokens/s:     2186.81
  Total Reqs:   128
  Elapsed:      15.92s

  [Decode 分析]
  Total Decode Tokens:  32768
  Decode Tokens/s:      2058.17

============================================================
[3/4] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 4096 (= 256 x 16)
│   M_decode      = 256
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 256
│   --max-num-seqs           = 256
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 18:46:44 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 18:46:44 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2950611) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2950611) WARNING 01-27 18:47:57 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 10.94 requests/s, 2975.69 total tokens/s, 2800.64 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536


─── STDERR ───
[2026-01-27 18:46:44] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:46:44] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:46:44] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-27 18:46:44] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:46:44] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:46:44] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:46:44] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:46:44] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:46:44] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:46:44] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:46:44] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:46:44] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:46:44] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:46:44] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 18:46:47] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:46:47] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:46:47] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-27 18:46:47] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:46:47] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:46:47] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:46:47] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:46:47] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:46:47] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:46:47] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:46:47] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:46:47] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:46:47] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:46:47] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2950611) [2026-01-27 18:46:48] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2950611) [2026-01-27 18:46:48] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2950611) [2026-01-27 18:46:48] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2950611) [2026-01-27 18:46:48] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=2950611) [2026-01-27 18:46:48] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=2950611) [2026-01-27 18:46:48] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=2950611) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2950611) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:27<00:27, 27.62s/it]
(EngineCore_DP0 pid=2950611) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [01:00<00:00, 30.73s/it]
(EngineCore_DP0 pid=2950611) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [01:00<00:00, 30.26s/it]
(EngineCore_DP0 pid=2950611) 
(EngineCore_DP0 pid=2950611) [2026-01-27 18:47:50] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5376] -> 1D uint8
(EngineCore_DP0 pid=2950611) [2026-01-27 18:47:50] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15482880 bytes
(EngineCore_DP0 pid=2950611) [2026-01-27 18:47:50] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5376] -> 1D uint8
(EngineCore_DP0 pid=2950611) [2026-01-27 18:47:50] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 12042240 bytes
(EngineCore_DP0 pid=2950611) [2026-01-27 18:47:50] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5376] -> 1D uint8
(EngineCore_DP0 pid=2950611) [2026-01-27 18:47:50] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 127303680 bytes
(EngineCore_DP0 pid=2950611) [2026-01-27 18:47:50] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 28416] -> 1D uint8
(EngineCore_DP0 pid=2950611) [2026-01-27 18:47:50] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 63651840 bytes
(EngineCore_DP0 pid=2950611) 2026-01-27 18:47:56,004 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2950611) 2026-01-27 18:47:56,022 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 4289.80it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:20<1:26:09, 20.27s/it, est. speed input: 0.79 toks/s, output: 12.63 toks/s]
Processed prompts:   7%|▋         | 18/256 [00:20<03:13,  1.23it/s, est. speed input: 14.07 toks/s, output: 225.06 toks/s]
Processed prompts:  13%|█▎        | 33/256 [00:20<01:22,  2.69it/s, est. speed input: 25.66 toks/s, output: 410.48 toks/s]
Processed prompts:  24%|██▍       | 62/256 [00:20<00:29,  6.60it/s, est. speed input: 47.81 toks/s, output: 764.88 toks/s]
Processed prompts:  34%|███▍      | 87/256 [00:20<00:15, 11.08it/s, est. speed input: 66.47 toks/s, output: 1063.52 toks/s]
Processed prompts:  42%|████▏     | 108/256 [00:21<00:09, 16.04it/s, est. speed input: 81.87 toks/s, output: 1309.98 toks/s]
Processed prompts:  50%|████▉     | 127/256 [00:21<00:05, 21.80it/s, est. speed input: 95.58 toks/s, output: 1529.28 toks/s]
Processed prompts:  56%|█████▋    | 144/256 [00:21<00:03, 28.16it/s, est. speed input: 107.61 toks/s, output: 1721.75 toks/s]
Processed prompts:  62%|██████▏   | 159/256 [00:21<00:02, 34.92it/s, est. speed input: 118.06 toks/s, output: 1888.97 toks/s]
Processed prompts:  67%|██████▋   | 172/256 [00:21<00:02, 41.49it/s, est. speed input: 126.94 toks/s, output: 2031.11 toks/s]
Processed prompts:  72%|███████▏  | 184/256 [00:21<00:01, 48.10it/s, est. speed input: 135.02 toks/s, output: 2160.25 toks/s]
Processed prompts:  76%|███████▌  | 195/256 [00:21<00:01, 54.13it/s, est. speed input: 142.28 toks/s, output: 2276.53 toks/s]
Processed prompts:  80%|████████  | 206/256 [00:22<00:00, 56.72it/s, est. speed input: 149.17 toks/s, output: 2386.75 toks/s]
Processed prompts:  84%|████████▍ | 216/256 [00:22<00:00, 57.77it/s, est. speed input: 155.27 toks/s, output: 2484.24 toks/s]
Processed prompts:  88%|████████▊ | 225/256 [00:22<00:00, 57.77it/s, est. speed input: 160.61 toks/s, output: 2569.77 toks/s]
Processed prompts:  91%|█████████ | 233/256 [00:22<00:00, 58.41it/s, est. speed input: 165.35 toks/s, output: 2645.55 toks/s]
Processed prompts:  94%|█████████▍| 241/256 [00:22<00:00, 55.24it/s, est. speed input: 169.76 toks/s, output: 2716.13 toks/s]
Processed prompts:  97%|█████████▋| 248/256 [00:23<00:00, 42.28it/s, est. speed input: 172.50 toks/s, output: 2759.93 toks/s]
Processed prompts:  99%|█████████▉| 254/256 [00:23<00:00, 36.41it/s, est. speed input: 174.82 toks/s, output: 2797.20 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:23<00:00, 36.41it/s, est. speed input: 175.50 toks/s, output: 2807.99 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:23<00:00, 10.97it/s, est. speed input: 175.50 toks/s, output: 2807.99 toks/s]
[rank0]:[W127 18:48:20.712594917 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 102.8s

测试结果:
  Requests/s:   10.94
  Tokens/s:     2975.69
  Total Reqs:   256
  Elapsed:      23.40s

  [Decode 分析]
  Total Decode Tokens:  65536
  Decode Tokens/s:      2800.64

============================================================
[4/4] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 8192 (= 512 x 16)
│   M_decode      = 512
│   batched_tokens = 512 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 512
│   --max-num-seqs           = 512
│   --max-model-len          = 272
│   --max-num-batched-tokens = 512
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 18:48:28 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 18:48:28 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2952546) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2952546) WARNING 01-27 18:49:41 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 10.84 requests/s, 2949.31 total tokens/s, 2775.82 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072


─── STDERR ───
[2026-01-27 18:48:28] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:48:28] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:48:28] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-27 18:48:28] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:48:28] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:48:28] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:48:28] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:48:28] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:48:28] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:48:28] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:48:28] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:48:28] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:48:28] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:48:28] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 18:48:31] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:48:31] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:48:31] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-27 18:48:31] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:48:31] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:48:31] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:48:31] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:48:31] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:48:31] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:48:31] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:48:31] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:48:31] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:48:31] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:48:31] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2952546) [2026-01-27 18:48:33] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2952546) [2026-01-27 18:48:33] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2952546) [2026-01-27 18:48:33] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2952546) [2026-01-27 18:48:33] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=2952546) [2026-01-27 18:48:33] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=2952546) [2026-01-27 18:48:33] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=2952546) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2952546) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:27<00:27, 27.99s/it]
(EngineCore_DP0 pid=2952546) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [01:01<00:00, 30.96s/it]
(EngineCore_DP0 pid=2952546) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [01:01<00:00, 30.51s/it]
(EngineCore_DP0 pid=2952546) 
(EngineCore_DP0 pid=2952546) [2026-01-27 18:49:35] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5376] -> 1D uint8
(EngineCore_DP0 pid=2952546) [2026-01-27 18:49:35] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 15482880 bytes
(EngineCore_DP0 pid=2952546) [2026-01-27 18:49:35] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5376] -> 1D uint8
(EngineCore_DP0 pid=2952546) [2026-01-27 18:49:35] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 12042240 bytes
(EngineCore_DP0 pid=2952546) [2026-01-27 18:49:35] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5376] -> 1D uint8
(EngineCore_DP0 pid=2952546) [2026-01-27 18:49:35] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 127303680 bytes
(EngineCore_DP0 pid=2952546) [2026-01-27 18:49:35] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 28416] -> 1D uint8
(EngineCore_DP0 pid=2952546) [2026-01-27 18:49:35] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 63651840 bytes
(EngineCore_DP0 pid=2952546) 2026-01-27 18:49:40,822 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2952546) 2026-01-27 18:49:40,840 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 11854.14it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:33<4:47:34, 33.77s/it, est. speed input: 0.47 toks/s, output: 7.58 toks/s]
Processed prompts:   1%|          | 3/512 [00:34<1:15:18,  8.88s/it, est. speed input: 1.41 toks/s, output: 22.52 toks/s]
Processed prompts:   7%|▋         | 34/512 [00:34<04:08,  1.92it/s, est. speed input: 15.81 toks/s, output: 252.94 toks/s]
Processed prompts:  12%|█▏        | 63/512 [00:34<01:45,  4.24it/s, est. speed input: 29.03 toks/s, output: 464.49 toks/s]
Processed prompts:  18%|█▊        | 91/512 [00:35<00:58,  7.25it/s, est. speed input: 41.59 toks/s, output: 665.42 toks/s]
Processed prompts:  23%|██▎       | 117/512 [00:35<00:36, 10.92it/s, est. speed input: 53.03 toks/s, output: 848.42 toks/s]
Processed prompts:  28%|██▊       | 141/512 [00:35<00:23, 15.73it/s, est. speed input: 63.66 toks/s, output: 1018.60 toks/s]
Processed prompts:  32%|███▏      | 164/512 [00:35<00:15, 21.85it/s, est. speed input: 73.77 toks/s, output: 1180.35 toks/s]
Processed prompts:  36%|███▌      | 185/512 [00:35<00:11, 29.11it/s, est. speed input: 82.93 toks/s, output: 1326.82 toks/s]
Processed prompts:  40%|████      | 205/512 [00:35<00:08, 37.56it/s, est. speed input: 91.54 toks/s, output: 1464.67 toks/s]
Processed prompts:  44%|████▍     | 224/512 [00:35<00:06, 47.75it/s, est. speed input: 99.71 toks/s, output: 1595.33 toks/s]
Processed prompts:  47%|████▋     | 242/512 [00:36<00:04, 59.02it/s, est. speed input: 107.39 toks/s, output: 1718.16 toks/s]
Processed prompts:  50%|█████     | 258/512 [00:36<00:03, 68.37it/s, est. speed input: 114.08 toks/s, output: 1825.35 toks/s]
Processed prompts:  53%|█████▎    | 273/512 [00:36<00:03, 78.44it/s, est. speed input: 120.35 toks/s, output: 1925.66 toks/s]
Processed prompts:  59%|█████▉    | 302/512 [00:36<00:02, 99.65it/s, est. speed input: 132.48 toks/s, output: 2119.72 toks/s]
Processed prompts:  64%|██████▍   | 327/512 [00:36<00:01, 107.70it/s, est. speed input: 142.69 toks/s, output: 2282.97 toks/s]
Processed prompts:  68%|██████▊   | 348/512 [00:36<00:01, 112.48it/s, est. speed input: 151.16 toks/s, output: 2418.62 toks/s]
Processed prompts:  72%|███████▏  | 367/512 [00:36<00:01, 114.88it/s, est. speed input: 158.74 toks/s, output: 2539.92 toks/s]
Processed prompts:  75%|███████▌  | 384/512 [00:37<00:01, 115.35it/s, est. speed input: 165.45 toks/s, output: 2647.15 toks/s]
Processed prompts:  78%|███████▊  | 399/512 [00:37<00:01, 110.12it/s, est. speed input: 171.19 toks/s, output: 2739.10 toks/s]
Processed prompts:  80%|████████  | 412/512 [00:37<00:00, 106.25it/s, est. speed input: 176.13 toks/s, output: 2818.03 toks/s]
Processed prompts:  83%|████████▎ | 424/512 [00:37<00:00, 102.20it/s, est. speed input: 180.62 toks/s, output: 2889.92 toks/s]
Processed prompts:  85%|████████▍ | 435/512 [00:37<00:00, 94.46it/s, est. speed input: 184.59 toks/s, output: 2953.48 toks/s] 
Processed prompts:  87%|████████▋ | 445/512 [00:37<00:00, 95.02it/s, est. speed input: 188.32 toks/s, output: 3013.13 toks/s]
Processed prompts:  89%|████████▉ | 455/512 [00:37<00:00, 79.82it/s, est. speed input: 191.61 toks/s, output: 3065.82 toks/s]
Processed prompts:  91%|█████████ | 464/512 [00:38<00:00, 75.06it/s, est. speed input: 194.68 toks/s, output: 3114.81 toks/s]
Processed prompts:  92%|█████████▏| 472/512 [00:38<00:00, 61.29it/s, est. speed input: 196.96 toks/s, output: 3151.33 toks/s]
Processed prompts:  94%|█████████▎| 479/512 [00:38<00:00, 59.04it/s, est. speed input: 199.19 toks/s, output: 3186.99 toks/s]
Processed prompts:  95%|█████████▍| 486/512 [00:38<00:00, 45.60it/s, est. speed input: 200.74 toks/s, output: 3211.80 toks/s]
Processed prompts:  96%|█████████▌| 492/512 [00:38<00:00, 37.30it/s, est. speed input: 201.87 toks/s, output: 3229.92 toks/s]
Processed prompts:  97%|█████████▋| 497/512 [00:39<00:00, 32.24it/s, est. speed input: 202.72 toks/s, output: 3243.49 toks/s]
Processed prompts:  98%|█████████▊| 501/512 [00:47<00:04,  2.41it/s, est. speed input: 170.21 toks/s, output: 2723.35 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:47<00:00,  2.41it/s, est. speed input: 173.65 toks/s, output: 2778.47 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:47<00:00, 10.85it/s, est. speed input: 173.65 toks/s, output: 2778.47 toks/s]
[rank0]:[W127 18:50:29.244544942 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 128.0s

测试结果:
  Requests/s:   10.84
  Tokens/s:     2949.31
  Total Reqs:   512
  Elapsed:      47.22s

  [Decode 分析]
  Total Decode Tokens:  131072
  Decode Tokens/s:      2775.82


------------------------------------------------------------
  生成 CSV: Qwen2.5-7B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_8/Qwen2.5-7B-FP8_decode.csv

预览:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
64,16,64,64,256,256,4.8790,1327.0984,13.1173
128,16,128,128,256,256,8.0397,2186.8069,15.9209
256,16,256,256,256,256,10.9400,2975.6850,23.4003
512,16,512,512,256,256,10.8430,2949.3086,47.2192

------------------------------------------------------------

[INFO] 完成: 4 成功, 0 失败

============================================================
  Qwen2.5-7B-FP8 | cuSPARSELt (2_10) | decode
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-7B-FP8-SlideSparse-2_10
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_10

============================================================
[1/4] 测试 M=64
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 64
│   M_prefill     = 1024 (= 64 x 16)
│   M_decode      = 64
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 64
│   --max-num-seqs           = 64
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 18:50:36 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 18:50:36 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2954881) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2954881) WARNING 01-27 18:52:03 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 4.47 requests/s, 1216.15 total tokens/s, 1144.61 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384


─── STDERR ───
[2026-01-27 18:50:36] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:50:36] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:50:36] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-27 18:50:36] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:50:36] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:50:36] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:50:36] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:50:36] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:50:36] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:50:36] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:50:36] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:50:36] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:50:36] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:50:36] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 18:50:40] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:50:40] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:50:40] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-27 18:50:40] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:50:40] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:50:40] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:50:40] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:50:40] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:50:40] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:50:40] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:50:40] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:50:40] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:50:40] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:50:40] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2954881) [2026-01-27 18:50:41] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2954881) [2026-01-27 18:50:41] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2954881) [2026-01-27 18:50:41] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2954881) [2026-01-27 18:50:41] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=2954881) [2026-01-27 18:50:41] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=2954881) [2026-01-27 18:50:41] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=2954881) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2954881) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:27<00:27, 27.79s/it]
(EngineCore_DP0 pid=2954881) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [01:05<00:00, 33.35s/it]
(EngineCore_DP0 pid=2954881) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [01:05<00:00, 32.51s/it]
(EngineCore_DP0 pid=2954881) 
(EngineCore_DP0 pid=2954881) [2026-01-27 18:51:47] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=2954881) [2026-01-27 18:51:47] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16588800 bytes
(EngineCore_DP0 pid=2954881) [2026-01-27 18:51:47] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=2954881) [2026-01-27 18:51:47] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 12902400 bytes
(EngineCore_DP0 pid=2954881) [2026-01-27 18:51:47] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=2954881) [2026-01-27 18:51:47] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 136396800 bytes
(EngineCore_DP0 pid=2954881) [2026-01-27 18:51:47] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=2954881) [2026-01-27 18:51:47] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 67952640 bytes
(EngineCore_DP0 pid=2954881) 2026-01-27 18:51:55,239 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2954881) 2026-01-27 18:51:55,569 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 1/64 [00:00<00:13,  4.62it/s]
Adding requests:   5%|▍         | 3/64 [00:00<00:06,  9.12it/s]
Adding requests:   9%|▉         | 6/64 [00:00<00:03, 14.81it/s]
Adding requests:  14%|█▍        | 9/64 [00:00<00:02, 18.41it/s]
Adding requests:  20%|██        | 13/64 [00:00<00:02, 23.57it/s]
Adding requests:  28%|██▊       | 18/64 [00:00<00:01, 30.79it/s]
Adding requests:  42%|████▏     | 27/64 [00:00<00:00, 46.64it/s]
Adding requests:  61%|██████    | 39/64 [00:01<00:00, 66.54it/s]
Adding requests:  83%|████████▎ | 53/64 [00:01<00:00, 85.82it/s]
Adding requests: 100%|██████████| 64/64 [00:01<00:00, 53.42it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 1/64 [00:12<12:56, 12.32s/it, est. speed input: 1.30 toks/s, output: 20.78 toks/s]
Processed prompts:   9%|▉         | 6/64 [00:12<01:29,  1.54s/it, est. speed input: 7.70 toks/s, output: 123.21 toks/s]
Processed prompts:  16%|█▌        | 10/64 [00:12<00:42,  1.28it/s, est. speed input: 12.73 toks/s, output: 203.67 toks/s]
Processed prompts:  22%|██▏       | 14/64 [00:12<00:23,  2.12it/s, est. speed input: 17.60 toks/s, output: 281.66 toks/s]
Processed prompts:  28%|██▊       | 18/64 [00:12<00:14,  3.28it/s, est. speed input: 22.46 toks/s, output: 359.30 toks/s]
Processed prompts:  62%|██████▎   | 40/64 [00:12<00:01, 12.15it/s, est. speed input: 49.36 toks/s, output: 789.77 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:13<00:00, 12.15it/s, est. speed input: 78.71 toks/s, output: 1259.44 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:13<00:00,  4.92it/s, est. speed input: 78.71 toks/s, output: 1259.44 toks/s]
[rank0]:[W127 18:52:18.906054925 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 124.3s

测试结果:
  Requests/s:   4.47
  Tokens/s:     1216.15
  Total Reqs:   64
  Elapsed:      14.31s

  [Decode 分析]
  Total Decode Tokens:  16384
  Decode Tokens/s:      1144.61

============================================================
[2/4] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 2048 (= 128 x 16)
│   M_decode      = 128
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 128
│   --max-num-seqs           = 128
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 18:52:40 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 18:52:40 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2957131) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2957131) WARNING 01-27 18:53:58 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 7.77 requests/s, 2112.84 total tokens/s, 1988.55 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768


─── STDERR ───
[2026-01-27 18:52:40] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:52:40] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:52:40] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-27 18:52:40] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:52:40] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:52:40] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:52:40] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:52:40] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:52:40] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:52:40] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:52:40] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:52:40] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:52:40] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:52:40] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 18:52:44] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:52:44] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:52:44] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-27 18:52:44] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:52:44] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:52:44] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:52:44] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:52:44] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:52:44] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:52:44] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:52:44] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:52:44] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:52:44] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:52:44] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2957131) [2026-01-27 18:52:45] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2957131) [2026-01-27 18:52:45] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2957131) [2026-01-27 18:52:45] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2957131) [2026-01-27 18:52:45] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=2957131) [2026-01-27 18:52:45] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=2957131) [2026-01-27 18:52:45] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=2957131) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2957131) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:28<00:28, 28.29s/it]
(EngineCore_DP0 pid=2957131) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [01:05<00:00, 33.28s/it]
(EngineCore_DP0 pid=2957131) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [01:05<00:00, 32.53s/it]
(EngineCore_DP0 pid=2957131) 
(EngineCore_DP0 pid=2957131) [2026-01-27 18:53:51] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=2957131) [2026-01-27 18:53:51] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16588800 bytes
(EngineCore_DP0 pid=2957131) [2026-01-27 18:53:51] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=2957131) [2026-01-27 18:53:51] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 12902400 bytes
(EngineCore_DP0 pid=2957131) [2026-01-27 18:53:51] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=2957131) [2026-01-27 18:53:51] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 136396800 bytes
(EngineCore_DP0 pid=2957131) [2026-01-27 18:53:51] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=2957131) [2026-01-27 18:53:51] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 67952640 bytes
(EngineCore_DP0 pid=2957131) 2026-01-27 18:53:57,747 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2957131) 2026-01-27 18:53:57,763 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 10988.17it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:15<33:07, 15.65s/it, est. speed input: 1.02 toks/s, output: 16.36 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:15<00:57,  1.86it/s, est. speed input: 21.24 toks/s, output: 339.78 toks/s]
Processed prompts:  39%|███▉      | 50/128 [00:15<00:14,  5.47it/s, est. speed input: 50.06 toks/s, output: 800.95 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:16<00:05,  9.93it/s, est. speed input: 75.52 toks/s, output: 1208.30 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:16<00:01, 15.18it/s, est. speed input: 97.53 toks/s, output: 1560.54 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:16<00:00, 21.29it/s, est. speed input: 116.43 toks/s, output: 1862.96 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:16<00:00, 21.29it/s, est. speed input: 124.38 toks/s, output: 1990.11 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:16<00:00,  7.77it/s, est. speed input: 124.38 toks/s, output: 1990.11 toks/s]
[rank0]:[W127 18:54:15.201839868 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 101.7s

测试结果:
  Requests/s:   7.77
  Tokens/s:     2112.84
  Total Reqs:   128
  Elapsed:      16.48s

  [Decode 分析]
  Total Decode Tokens:  32768
  Decode Tokens/s:      1988.55

============================================================
[3/4] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 4096 (= 256 x 16)
│   M_decode      = 256
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 256
│   --max-num-seqs           = 256
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 18:54:22 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 18:54:22 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2959027) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2959027) WARNING 01-27 18:55:39 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 10.44 requests/s, 2838.36 total tokens/s, 2671.40 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536


─── STDERR ───
[2026-01-27 18:54:22] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:54:22] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:54:22] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-27 18:54:22] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:54:22] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:54:22] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:54:22] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:54:22] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:54:22] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:54:22] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:54:22] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:54:22] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:54:22] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:54:22] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 18:54:25] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:54:25] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:54:25] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-27 18:54:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:54:25] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:54:25] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:54:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:54:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:54:25] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:54:25] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:54:25] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:54:25] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:54:25] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:54:25] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2959027) [2026-01-27 18:54:27] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2959027) [2026-01-27 18:54:27] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2959027) [2026-01-27 18:54:27] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2959027) [2026-01-27 18:54:27] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=2959027) [2026-01-27 18:54:27] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=2959027) [2026-01-27 18:54:27] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=2959027) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2959027) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:28<00:28, 28.36s/it]
(EngineCore_DP0 pid=2959027) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [01:04<00:00, 32.99s/it]
(EngineCore_DP0 pid=2959027) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [01:04<00:00, 32.30s/it]
(EngineCore_DP0 pid=2959027) 
(EngineCore_DP0 pid=2959027) [2026-01-27 18:55:32] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=2959027) [2026-01-27 18:55:32] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16588800 bytes
(EngineCore_DP0 pid=2959027) [2026-01-27 18:55:32] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=2959027) [2026-01-27 18:55:32] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 12902400 bytes
(EngineCore_DP0 pid=2959027) [2026-01-27 18:55:32] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=2959027) [2026-01-27 18:55:32] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 136396800 bytes
(EngineCore_DP0 pid=2959027) [2026-01-27 18:55:32] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=2959027) [2026-01-27 18:55:32] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 67952640 bytes
(EngineCore_DP0 pid=2959027) 2026-01-27 18:55:38,489 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2959027) 2026-01-27 18:55:38,507 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 11411.37it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:21<1:30:07, 21.21s/it, est. speed input: 0.75 toks/s, output: 12.07 toks/s]
Processed prompts:   2%|▏         | 4/256 [00:21<16:58,  4.04s/it, est. speed input: 3.00 toks/s, output: 48.04 toks/s]  
Processed prompts:   8%|▊         | 20/256 [00:21<02:14,  1.76it/s, est. speed input: 14.94 toks/s, output: 238.99 toks/s]
Processed prompts:  14%|█▎        | 35/256 [00:21<00:59,  3.73it/s, est. speed input: 26.01 toks/s, output: 416.16 toks/s]
Processed prompts:  19%|█▉        | 49/256 [00:21<00:33,  6.25it/s, est. speed input: 36.21 toks/s, output: 579.32 toks/s]
Processed prompts:  30%|██▉       | 76/256 [00:21<00:14, 12.79it/s, est. speed input: 55.69 toks/s, output: 890.97 toks/s]
Processed prompts:  39%|███▊      | 99/256 [00:22<00:07, 19.73it/s, est. speed input: 71.90 toks/s, output: 1150.43 toks/s]
Processed prompts:  46%|████▋     | 119/256 [00:22<00:05, 27.33it/s, est. speed input: 85.82 toks/s, output: 1373.10 toks/s]
Processed prompts:  54%|█████▎    | 137/256 [00:22<00:03, 35.11it/s, est. speed input: 98.06 toks/s, output: 1568.96 toks/s]
Processed prompts:  60%|█████▉    | 153/256 [00:22<00:02, 43.08it/s, est. speed input: 108.79 toks/s, output: 1740.68 toks/s]
Processed prompts:  65%|██████▌   | 167/256 [00:22<00:01, 50.50it/s, est. speed input: 118.03 toks/s, output: 1888.40 toks/s]
Processed prompts:  70%|██████▉   | 179/256 [00:22<00:01, 56.39it/s, est. speed input: 125.77 toks/s, output: 2012.24 toks/s]
Processed prompts:  74%|███████▍  | 190/256 [00:22<00:01, 62.86it/s, est. speed input: 132.85 toks/s, output: 2125.58 toks/s]
Processed prompts:  79%|███████▊  | 201/256 [00:23<00:00, 65.89it/s, est. speed input: 139.67 toks/s, output: 2234.65 toks/s]
Processed prompts:  82%|████████▏ | 211/256 [00:23<00:00, 66.21it/s, est. speed input: 145.67 toks/s, output: 2330.77 toks/s]
Processed prompts:  86%|████████▌ | 220/256 [00:23<00:00, 65.14it/s, est. speed input: 150.94 toks/s, output: 2415.06 toks/s]
Processed prompts:  89%|████████▉ | 228/256 [00:23<00:00, 60.82it/s, est. speed input: 155.36 toks/s, output: 2485.82 toks/s]
Processed prompts:  92%|█████████▏| 236/256 [00:23<00:00, 55.82it/s, est. speed input: 159.60 toks/s, output: 2553.64 toks/s]
Processed prompts:  95%|█████████▍| 243/256 [00:23<00:00, 49.16it/s, est. speed input: 162.98 toks/s, output: 2607.72 toks/s]
Processed prompts:  97%|█████████▋| 249/256 [00:24<00:00, 38.90it/s, est. speed input: 165.18 toks/s, output: 2642.83 toks/s]
Processed prompts:  99%|█████████▉| 254/256 [00:24<00:00, 34.29it/s, est. speed input: 167.04 toks/s, output: 2672.64 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:24<00:00, 34.29it/s, est. speed input: 167.12 toks/s, output: 2673.97 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:24<00:00, 10.45it/s, est. speed input: 167.12 toks/s, output: 2673.97 toks/s]
[rank0]:[W127 18:56:04.209391283 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 109.5s

测试结果:
  Requests/s:   10.44
  Tokens/s:     2838.36
  Total Reqs:   256
  Elapsed:      24.53s

  [Decode 分析]
  Total Decode Tokens:  65536
  Decode Tokens/s:      2671.40

============================================================
[4/4] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-7B-FP8                                  │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 8192 (= 512 x 16)
│   M_decode      = 512
│   batched_tokens = 512 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 512
│   --max-num-seqs           = 512
│   --max-model-len          = 272
│   --max-num-batched-tokens = 512
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 18:56:13 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 18:56:13 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2961071) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2961071) WARNING 01-27 18:57:30 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 10.42 requests/s, 2833.17 total tokens/s, 2666.51 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072


─── STDERR ───
[2026-01-27 18:56:13] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:56:13] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:56:13] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-27 18:56:13] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:56:13] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:56:13] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:56:13] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:56:13] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:56:13] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:56:13] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:56:13] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:56:13] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:56:13] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:56:13] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 18:56:16] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:56:16] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:56:16] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-7B-FP8
[2026-01-27 18:56:16] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:56:16] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:56:16] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:56:16] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:56:16] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-7B-FP8
[2026-01-27 18:56:16] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-7B-FP8'
[2026-01-27 18:56:16] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:56:16] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:56:16] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:56:16] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:56:16] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2961071) [2026-01-27 18:56:17] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2961071) [2026-01-27 18:56:17] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2961071) [2026-01-27 18:56:17] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2961071) [2026-01-27 18:56:17] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=2961071) [2026-01-27 18:56:17] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-7B-FP8
(EngineCore_DP0 pid=2961071) [2026-01-27 18:56:17] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=2961071) 
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2961071) 
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:27<00:27, 27.27s/it]
(EngineCore_DP0 pid=2961071) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [01:04<00:00, 33.14s/it]
(EngineCore_DP0 pid=2961071) 
Loading safetensors checkpoint shards: 100% Completed | 2/2 [01:04<00:00, 32.26s/it]
(EngineCore_DP0 pid=2961071) 
(EngineCore_DP0 pid=2961071) [2026-01-27 18:57:23] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [4608, 5760] -> 1D uint8
(EngineCore_DP0 pid=2961071) [2026-01-27 18:57:23] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16588800 bytes
(EngineCore_DP0 pid=2961071) [2026-01-27 18:57:23] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 5760] -> 1D uint8
(EngineCore_DP0 pid=2961071) [2026-01-27 18:57:23] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 12902400 bytes
(EngineCore_DP0 pid=2961071) [2026-01-27 18:57:23] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [37888, 5760] -> 1D uint8
(EngineCore_DP0 pid=2961071) [2026-01-27 18:57:23] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 136396800 bytes
(EngineCore_DP0 pid=2961071) [2026-01-27 18:57:23] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3584, 30336] -> 1D uint8
(EngineCore_DP0 pid=2961071) [2026-01-27 18:57:23] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 67952640 bytes
(EngineCore_DP0 pid=2961071) 2026-01-27 18:57:29,101 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2961071) 2026-01-27 18:57:29,118 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 12130.96it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:34<4:58:04, 35.00s/it, est. speed input: 0.46 toks/s, output: 7.31 toks/s]
Processed prompts:   0%|          | 2/512 [00:35<2:04:11, 14.61s/it, est. speed input: 0.91 toks/s, output: 14.49 toks/s]
Processed prompts:   6%|▋         | 33/512 [00:35<04:23,  1.82it/s, est. speed input: 14.81 toks/s, output: 236.88 toks/s]
Processed prompts:  12%|█▏        | 63/512 [00:35<01:48,  4.13it/s, est. speed input: 28.02 toks/s, output: 448.35 toks/s]
Processed prompts:  18%|█▊        | 91/512 [00:36<00:59,  7.05it/s, est. speed input: 40.15 toks/s, output: 642.41 toks/s]
Processed prompts:  23%|██▎       | 117/512 [00:36<00:37, 10.60it/s, est. speed input: 51.20 toks/s, output: 819.22 toks/s]
Processed prompts:  28%|██▊       | 141/512 [00:36<00:24, 15.23it/s, est. speed input: 61.46 toks/s, output: 983.35 toks/s]
Processed prompts:  32%|███▏      | 164/512 [00:36<00:16, 21.18it/s, est. speed input: 71.23 toks/s, output: 1139.68 toks/s]
Processed prompts:  36%|███▌      | 185/512 [00:36<00:11, 28.19it/s, est. speed input: 80.07 toks/s, output: 1281.09 toks/s]
Processed prompts:  40%|████      | 205/512 [00:37<00:08, 36.68it/s, est. speed input: 88.43 toks/s, output: 1414.86 toks/s]
Processed prompts:  44%|████▍     | 224/512 [00:37<00:06, 46.51it/s, est. speed input: 96.31 toks/s, output: 1540.96 toks/s]
Processed prompts:  47%|████▋     | 242/512 [00:37<00:04, 57.50it/s, est. speed input: 103.73 toks/s, output: 1659.67 toks/s]
Processed prompts:  50%|█████     | 258/512 [00:37<00:03, 67.30it/s, est. speed input: 110.23 toks/s, output: 1763.72 toks/s]
Processed prompts:  53%|█████▎    | 273/512 [00:37<00:03, 76.68it/s, est. speed input: 116.28 toks/s, output: 1860.48 toks/s]
Processed prompts:  59%|█████▉    | 302/512 [00:37<00:02, 96.64it/s, est. speed input: 127.99 toks/s, output: 2047.79 toks/s]
Processed prompts:  62%|██████▏   | 317/512 [00:37<00:01, 104.58it/s, est. speed input: 133.97 toks/s, output: 2143.54 toks/s]
Processed prompts:  66%|██████▌   | 338/512 [00:38<00:01, 105.20it/s, est. speed input: 142.11 toks/s, output: 2273.71 toks/s]
Processed prompts:  70%|██████▉   | 358/512 [00:38<00:01, 109.44it/s, est. speed input: 149.86 toks/s, output: 2397.80 toks/s]
Processed prompts:  73%|███████▎  | 376/512 [00:38<00:01, 110.77it/s, est. speed input: 156.75 toks/s, output: 2508.03 toks/s]
Processed prompts:  77%|███████▋  | 392/512 [00:38<00:01, 108.95it/s, est. speed input: 162.77 toks/s, output: 2604.33 toks/s]
Processed prompts:  79%|███████▉  | 406/512 [00:38<00:01, 105.69it/s, est. speed input: 167.95 toks/s, output: 2687.26 toks/s]
Processed prompts:  82%|████████▏ | 418/512 [00:38<00:00, 99.67it/s, est. speed input: 172.28 toks/s, output: 2756.49 toks/s] 
Processed prompts:  84%|████████▍ | 429/512 [00:38<00:00, 99.54it/s, est. speed input: 176.31 toks/s, output: 2820.96 toks/s]
Processed prompts:  86%|████████▌ | 440/512 [00:39<00:00, 92.34it/s, est. speed input: 180.16 toks/s, output: 2882.56 toks/s]
Processed prompts:  88%|████████▊ | 450/512 [00:39<00:00, 78.04it/s, est. speed input: 183.37 toks/s, output: 2933.87 toks/s]
Processed prompts:  90%|████████▉ | 459/512 [00:39<00:00, 72.58it/s, est. speed input: 186.31 toks/s, output: 2981.04 toks/s]
Processed prompts:  91%|█████████ | 467/512 [00:39<00:00, 71.37it/s, est. speed input: 189.00 toks/s, output: 3023.92 toks/s]
Processed prompts:  93%|█████████▎| 475/512 [00:39<00:00, 60.62it/s, est. speed input: 191.31 toks/s, output: 3060.88 toks/s]
Processed prompts:  94%|█████████▍| 482/512 [00:39<00:00, 49.48it/s, est. speed input: 193.03 toks/s, output: 3088.53 toks/s]
Processed prompts:  95%|█████████▌| 488/512 [00:40<00:00, 38.62it/s, est. speed input: 194.11 toks/s, output: 3105.81 toks/s]
Processed prompts:  96%|█████████▋| 493/512 [00:40<00:00, 33.65it/s, est. speed input: 195.04 toks/s, output: 3120.58 toks/s]
Processed prompts:  97%|█████████▋| 497/512 [00:40<00:00, 30.87it/s, est. speed input: 195.78 toks/s, output: 3132.43 toks/s]
Processed prompts:  98%|█████████▊| 501/512 [00:49<00:05,  2.07it/s, est. speed input: 163.50 toks/s, output: 2615.93 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:49<00:00,  2.07it/s, est. speed input: 166.80 toks/s, output: 2668.87 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:49<00:00, 10.43it/s, est. speed input: 166.80 toks/s, output: 2668.87 toks/s]
[rank0]:[W127 18:58:19.752029078 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 135.1s

测试结果:
  Requests/s:   10.42
  Tokens/s:     2833.17
  Total Reqs:   512
  Elapsed:      49.15s

  [Decode 分析]
  Total Decode Tokens:  131072
  Decode Tokens/s:      2666.51


------------------------------------------------------------
  生成 CSV: Qwen2.5-7B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_10/Qwen2.5-7B-FP8_decode.csv

预览:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
64,16,64,64,256,256,4.4711,1216.1467,14.3141
128,16,128,128,256,256,7.7678,2112.8365,16.4783
256,16,256,256,256,256,10.4351,2838.3606,24.5325
512,16,512,512,256,256,10.4161,2833.1698,49.1548

------------------------------------------------------------

[INFO] 完成: 4 成功, 0 失败


============================================================
  Benchmark 完成!
============================================================


总计: 20 成功, 0 失败
============================================================
