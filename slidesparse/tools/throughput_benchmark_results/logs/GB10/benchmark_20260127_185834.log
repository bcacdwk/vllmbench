======================================================================
SlideSparse vLLM Throughput Benchmark Log
Created: 2026-01-27 18:58:34
======================================================================

原始命令:
  /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-14b-int8 --backend cublaslt,cusparselt --stage decode --sparsity 2_4,2_6,2_8,2_10 --M 64,128,256,512

命令行参数:
  --model: qwen2.5-14b-int8
  --backend: cublaslt,cusparselt
  --sparsity: 2_4,2_6,2_8,2_10
  --stage: decode
  --M: 64,128,256,512
  --N: None
  --inner-32: False
  --eager: False
  --gpu-id: 0
  --gpu-mem: 0.8
  --dry-run: False
  --list-models: False

硬件信息:
  GPU: GB10
  Compute Capability: cc121
  VRAM: 119.7 GB
  CUDA: 12.9
  Python: py312

Backend 环境变量 (初始状态):
  DISABLE_SLIDESPARSE: 未设置
  USE_CUBLASLT: 未设置
  USE_CUSPARSELT: 未设置
  SPARSITY: 未设置
  INNER_DTYPE_32: 未设置

======================================================================


============================================================
  Qwen2.5-14B-INT8 | cuBLASLt | decode
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints/Qwen2.5-14B-INT8
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cublaslt

============================================================
[1/4] 测试 M=64
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-14B-INT8                                │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 64
│   M_prefill     = 1024 (= 64 x 16)
│   M_decode      = 64
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 64
│   --max-num-seqs           = 64
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 18:58:38 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 18:58:38 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2963737) WARNING 01-27 19:00:14 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 2.27 requests/s, 617.16 total tokens/s, 580.86 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384


─── STDERR ───
[2026-01-27 18:58:38] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:58:38] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-27 18:58:38] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-27 18:58:38] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 18:58:38] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 18:58:38] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 18:58:38] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 18:58:38] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 18:58:38] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-27 18:58:38] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:58:38] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:58:38] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:58:38] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:58:38] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 18:58:41] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 18:58:41] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-27 18:58:41] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-27 18:58:41] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 18:58:41] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 18:58:41] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 18:58:41] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 18:58:41] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 18:58:41] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-27 18:58:41] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 18:58:41] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 18:58:41] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 18:58:41] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 18:58:41] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2963737) [2026-01-27 18:58:42] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=2963737) [2026-01-27 18:58:43] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2963737) [2026-01-27 18:58:43] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=2963737) [2026-01-27 18:58:43] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=2963737) [2026-01-27 18:58:43] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=2963737) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2963737) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:08<00:25,  8.41s/it]
(EngineCore_DP0 pid=2963737) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:34<00:37, 18.59s/it]
(EngineCore_DP0 pid=2963737) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:55<00:19, 19.81s/it]
(EngineCore_DP0 pid=2963737) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:22<00:00, 22.71s/it]
(EngineCore_DP0 pid=2963737) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:22<00:00, 20.64s/it]
(EngineCore_DP0 pid=2963737) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=2963737) 2026-01-27 19:00:13,251 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2963737) 2026-01-27 19:00:13,299 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 64/64 [00:00<00:00, 4736.65it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 1/64 [00:27<29:03, 27.68s/it, est. speed input: 0.58 toks/s, output: 9.25 toks/s]
Processed prompts:   6%|▋         | 4/64 [00:27<05:15,  5.27s/it, est. speed input: 2.30 toks/s, output: 36.85 toks/s]
Processed prompts:  31%|███▏      | 20/64 [00:27<00:32,  1.35it/s, est. speed input: 11.47 toks/s, output: 183.55 toks/s]
Processed prompts:  77%|███████▋  | 49/64 [00:28<00:03,  4.28it/s, est. speed input: 27.92 toks/s, output: 446.65 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:28<00:00,  4.28it/s, est. speed input: 36.35 toks/s, output: 581.61 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:28<00:00,  2.27it/s, est. speed input: 36.35 toks/s, output: 581.61 toks/s]
[rank0]:[W127 19:00:42.476042597 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 132.1s

测试结果:
  Requests/s:   2.27
  Tokens/s:     617.16
  Total Reqs:   64
  Elapsed:      28.21s

  [Decode 分析]
  Total Decode Tokens:  16384
  Decode Tokens/s:      580.86

============================================================
[2/4] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-14B-INT8                                │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 2048 (= 128 x 16)
│   M_decode      = 128
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 128
│   --max-num-seqs           = 128
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 19:00:52 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 19:00:52 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2966183) WARNING 01-27 19:02:28 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 3.76 requests/s, 1022.55 total tokens/s, 962.40 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768


─── STDERR ───
[2026-01-27 19:00:52] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 19:00:52] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:00:52] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-27 19:00:52] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:00:52] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:00:52] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:00:52] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:00:52] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:00:52] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:00:52] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 19:00:52] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 19:00:52] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 19:00:52] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 19:00:52] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 19:00:55] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 19:00:55] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:00:55] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-27 19:00:55] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:00:55] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:00:55] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:00:55] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:00:55] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:00:55] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:00:55] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 19:00:55] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 19:00:55] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 19:00:55] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 19:00:55] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2966183) [2026-01-27 19:00:56] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=2966183) [2026-01-27 19:00:56] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2966183) [2026-01-27 19:00:56] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=2966183) [2026-01-27 19:00:56] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=2966183) [2026-01-27 19:00:56] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=2966183) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2966183) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:08<00:25,  8.45s/it]
(EngineCore_DP0 pid=2966183) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:35<00:38, 19.44s/it]
(EngineCore_DP0 pid=2966183) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:57<00:20, 20.45s/it]
(EngineCore_DP0 pid=2966183) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:23<00:00, 22.88s/it]
(EngineCore_DP0 pid=2966183) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:23<00:00, 20.96s/it]
(EngineCore_DP0 pid=2966183) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=2966183) 2026-01-27 19:02:27,340 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2966183) 2026-01-27 19:02:27,396 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 3093.52it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:32<1:09:20, 32.76s/it, est. speed input: 0.49 toks/s, output: 7.82 toks/s]
Processed prompts:   2%|▏         | 2/128 [00:32<28:29, 13.57s/it, est. speed input: 0.97 toks/s, output: 15.57 toks/s] 
Processed prompts:  14%|█▍        | 18/128 [00:33<01:44,  1.06it/s, est. speed input: 8.72 toks/s, output: 139.53 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:33<00:40,  2.35it/s, est. speed input: 15.93 toks/s, output: 254.81 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:33<00:19,  4.14it/s, est. speed input: 23.08 toks/s, output: 369.30 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:33<00:10,  6.46it/s, est. speed input: 29.71 toks/s, output: 475.35 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:33<00:05,  9.38it/s, est. speed input: 35.82 toks/s, output: 573.16 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:33<00:03, 13.01it/s, est. speed input: 41.43 toks/s, output: 662.82 toks/s]
Processed prompts:  84%|████████▍ | 108/128 [00:33<00:00, 20.96it/s, est. speed input: 51.13 toks/s, output: 818.12 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:33<00:00, 29.36it/s, est. speed input: 59.81 toks/s, output: 956.89 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:33<00:00, 29.36it/s, est. speed input: 60.28 toks/s, output: 964.42 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:33<00:00,  3.77it/s, est. speed input: 60.28 toks/s, output: 964.42 toks/s]
[rank0]:[W127 19:03:02.485216076 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 138.7s

测试结果:
  Requests/s:   3.76
  Tokens/s:     1022.55
  Total Reqs:   128
  Elapsed:      34.05s

  [Decode 分析]
  Total Decode Tokens:  32768
  Decode Tokens/s:      962.40

============================================================
[3/4] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-14B-INT8                                │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 4096 (= 256 x 16)
│   M_decode      = 256
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 256
│   --max-num-seqs           = 256
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 19:03:08 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 19:03:08 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2968633) WARNING 01-27 19:04:44 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 5.08 requests/s, 1381.97 total tokens/s, 1300.68 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536


─── STDERR ───
[2026-01-27 19:03:08] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 19:03:08] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:03:08] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-27 19:03:08] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:03:08] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:03:08] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:03:08] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:03:08] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:03:08] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:03:08] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 19:03:08] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 19:03:08] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 19:03:08] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 19:03:08] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 19:03:11] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 19:03:12] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:03:12] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-27 19:03:12] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:03:12] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:03:12] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:03:12] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:03:12] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:03:12] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:03:12] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 19:03:12] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 19:03:12] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 19:03:12] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 19:03:12] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2968633) [2026-01-27 19:03:12] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=2968633) [2026-01-27 19:03:12] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2968633) [2026-01-27 19:03:12] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=2968633) [2026-01-27 19:03:12] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=2968633) [2026-01-27 19:03:12] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=2968633) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2968633) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:08<00:25,  8.52s/it]
(EngineCore_DP0 pid=2968633) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:34<00:38, 19.08s/it]
(EngineCore_DP0 pid=2968633) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:56<00:20, 20.04s/it]
(EngineCore_DP0 pid=2968633) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:23<00:00, 22.90s/it]
(EngineCore_DP0 pid=2968633) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:23<00:00, 20.87s/it]
(EngineCore_DP0 pid=2968633) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=2968633) 2026-01-27 19:04:42,826 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2968633) 2026-01-27 19:04:42,875 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   0%|          | 1/256 [00:00<00:52,  4.90it/s]
Adding requests:   1%|          | 3/256 [00:00<00:25,  9.94it/s]
Adding requests:   2%|▏         | 6/256 [00:00<00:15, 15.90it/s]
Adding requests:   4%|▍         | 10/256 [00:00<00:11, 22.36it/s]
Adding requests:   5%|▌         | 14/256 [00:00<00:09, 26.88it/s]
Adding requests:   8%|▊         | 21/256 [00:00<00:06, 38.67it/s]
Adding requests:  12%|█▏        | 31/256 [00:00<00:04, 55.97it/s]
Adding requests:  17%|█▋        | 44/256 [00:00<00:02, 77.50it/s]
Adding requests:  25%|██▍       | 63/256 [00:01<00:01, 109.40it/s]
Adding requests:  32%|███▏      | 83/256 [00:01<00:01, 134.54it/s]
Adding requests:  41%|████▏     | 106/256 [00:01<00:00, 161.35it/s]
Adding requests:  51%|█████     | 130/256 [00:01<00:00, 182.57it/s]
Adding requests:  61%|██████    | 156/256 [00:01<00:00, 205.23it/s]
Adding requests:  69%|██████▉   | 177/256 [00:01<00:00, 143.11it/s]
Adding requests:  84%|████████▎ | 214/256 [00:01<00:00, 193.29it/s]
Adding requests: 100%|██████████| 256/256 [00:01<00:00, 246.94it/s]
Adding requests: 100%|██████████| 256/256 [00:01<00:00, 130.76it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:41<2:56:51, 41.61s/it, est. speed input: 0.38 toks/s, output: 6.15 toks/s]
Processed prompts:   1%|          | 2/256 [00:41<1:13:00, 17.25s/it, est. speed input: 0.77 toks/s, output: 12.25 toks/s]
Processed prompts:   2%|▏         | 6/256 [00:41<16:37,  3.99s/it, est. speed input: 2.29 toks/s, output: 36.58 toks/s]  
Processed prompts:   4%|▎         | 9/256 [00:42<09:07,  2.21s/it, est. speed input: 3.41 toks/s, output: 54.62 toks/s]
Processed prompts:   5%|▌         | 13/256 [00:42<04:54,  1.21s/it, est. speed input: 4.91 toks/s, output: 78.54 toks/s]
Processed prompts:   7%|▋         | 18/256 [00:42<02:42,  1.47it/s, est. speed input: 6.77 toks/s, output: 108.25 toks/s]
Processed prompts:  10%|█         | 26/256 [00:42<01:19,  2.89it/s, est. speed input: 9.73 toks/s, output: 155.68 toks/s]
Processed prompts:  15%|█▌        | 39/256 [00:42<00:36,  6.02it/s, est. speed input: 14.54 toks/s, output: 232.59 toks/s]
Processed prompts:  21%|██        | 53/256 [00:43<00:19, 10.40it/s, est. speed input: 19.68 toks/s, output: 314.85 toks/s]
Processed prompts:  26%|██▌       | 66/256 [00:43<00:12, 15.45it/s, est. speed input: 24.41 toks/s, output: 390.58 toks/s]
Processed prompts:  30%|███       | 78/256 [00:43<00:08, 20.95it/s, est. speed input: 28.74 toks/s, output: 459.88 toks/s]
Processed prompts:  35%|███▌      | 90/256 [00:43<00:06, 27.35it/s, est. speed input: 33.05 toks/s, output: 528.73 toks/s]
Processed prompts:  39%|███▉      | 101/256 [00:43<00:04, 33.49it/s, est. speed input: 36.95 toks/s, output: 591.28 toks/s]
Processed prompts:  43%|████▎     | 111/256 [00:43<00:03, 38.79it/s, est. speed input: 40.47 toks/s, output: 647.57 toks/s]
Processed prompts:  47%|████▋     | 121/256 [00:44<00:03, 44.33it/s, est. speed input: 43.97 toks/s, output: 703.60 toks/s]
Processed prompts:  51%|█████     | 130/256 [00:44<00:02, 48.42it/s, est. speed input: 47.10 toks/s, output: 753.55 toks/s]
Processed prompts:  54%|█████▍    | 138/256 [00:44<00:02, 50.71it/s, est. speed input: 49.84 toks/s, output: 797.47 toks/s]
Processed prompts:  57%|█████▋    | 146/256 [00:44<00:02, 53.03it/s, est. speed input: 52.57 toks/s, output: 841.20 toks/s]
Processed prompts:  60%|█████▉    | 153/256 [00:44<00:01, 53.41it/s, est. speed input: 54.94 toks/s, output: 878.99 toks/s]
Processed prompts:  62%|██████▎   | 160/256 [00:44<00:01, 53.90it/s, est. speed input: 57.29 toks/s, output: 916.61 toks/s]
Processed prompts:  65%|██████▌   | 167/256 [00:44<00:01, 53.59it/s, est. speed input: 59.62 toks/s, output: 953.88 toks/s]
Processed prompts:  68%|██████▊   | 173/256 [00:44<00:01, 53.27it/s, est. speed input: 61.60 toks/s, output: 985.63 toks/s]
Processed prompts:  70%|██████▉   | 179/256 [00:45<00:01, 52.63it/s, est. speed input: 63.57 toks/s, output: 1017.14 toks/s]
Processed prompts:  72%|███████▏  | 185/256 [00:45<00:01, 52.53it/s, est. speed input: 65.54 toks/s, output: 1048.56 toks/s]
Processed prompts:  75%|███████▍  | 191/256 [00:45<00:01, 52.57it/s, est. speed input: 67.49 toks/s, output: 1079.85 toks/s]
Processed prompts:  77%|███████▋  | 197/256 [00:45<00:01, 52.69it/s, est. speed input: 69.44 toks/s, output: 1110.99 toks/s]
Processed prompts:  79%|███████▉  | 203/256 [00:45<00:01, 41.63it/s, est. speed input: 71.21 toks/s, output: 1139.36 toks/s]
Processed prompts:  81%|████████▏ | 208/256 [00:45<00:01, 42.80it/s, est. speed input: 72.79 toks/s, output: 1164.68 toks/s]
Processed prompts:  83%|████████▎ | 213/256 [00:45<00:01, 35.47it/s, est. speed input: 74.21 toks/s, output: 1187.31 toks/s]
Processed prompts:  85%|████████▍ | 217/256 [00:46<00:01, 35.90it/s, est. speed input: 75.42 toks/s, output: 1206.79 toks/s]
Processed prompts:  86%|████████▋ | 221/256 [00:46<00:00, 36.70it/s, est. speed input: 76.65 toks/s, output: 1226.32 toks/s]
Processed prompts:  88%|████████▊ | 225/256 [00:46<00:01, 30.23it/s, est. speed input: 77.70 toks/s, output: 1243.20 toks/s]
Processed prompts:  90%|████████▉ | 230/256 [00:46<00:00, 28.60it/s, est. speed input: 79.09 toks/s, output: 1265.51 toks/s]
Processed prompts:  91%|█████████▏| 234/256 [00:46<00:00, 26.08it/s, est. speed input: 80.14 toks/s, output: 1282.27 toks/s]
Processed prompts:  93%|█████████▎| 238/256 [00:46<00:00, 24.65it/s, est. speed input: 81.19 toks/s, output: 1299.03 toks/s]
Processed prompts:  95%|█████████▍| 242/256 [00:47<00:00, 23.66it/s, est. speed input: 82.23 toks/s, output: 1315.65 toks/s]
Processed prompts:  96%|█████████▌| 245/256 [00:47<00:00, 18.88it/s, est. speed input: 82.78 toks/s, output: 1324.51 toks/s]
Processed prompts:  97%|█████████▋| 248/256 [00:47<00:00, 16.26it/s, est. speed input: 83.34 toks/s, output: 1333.37 toks/s]
Processed prompts:  98%|█████████▊| 250/256 [00:47<00:00, 15.06it/s, est. speed input: 83.70 toks/s, output: 1339.23 toks/s]
Processed prompts:  98%|█████████▊| 252/256 [00:47<00:00, 14.19it/s, est. speed input: 84.07 toks/s, output: 1345.14 toks/s]
Processed prompts:  99%|█████████▉| 254/256 [00:48<00:00, 13.42it/s, est. speed input: 84.43 toks/s, output: 1350.87 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:48<00:00, 12.52it/s, est. speed input: 84.76 toks/s, output: 1356.11 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:48<00:00, 12.52it/s, est. speed input: 84.76 toks/s, output: 1356.11 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:48<00:00,  5.30it/s, est. speed input: 84.76 toks/s, output: 1356.11 toks/s]
[rank0]:[W127 19:05:35.035372462 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 164.4s

测试结果:
  Requests/s:   5.08
  Tokens/s:     1381.97
  Total Reqs:   256
  Elapsed:      50.39s

  [Decode 分析]
  Total Decode Tokens:  65536
  Decode Tokens/s:      1300.68

============================================================
[4/4] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-14B-INT8                                │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 8192 (= 512 x 16)
│   M_decode      = 512
│   batched_tokens = 512 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 512
│   --max-num-seqs           = 512
│   --max-model-len          = 272
│   --max-num-batched-tokens = 512
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 19:05:54 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 19:05:54 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2971578) WARNING 01-27 19:07:31 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 5.31 requests/s, 1443.27 total tokens/s, 1358.37 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072


─── STDERR ───
[2026-01-27 19:05:54] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 19:05:54] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:05:54] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-27 19:05:54] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:05:54] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:05:54] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:05:54] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:05:54] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:05:54] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:05:54] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 19:05:54] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 19:05:54] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 19:05:54] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 19:05:54] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 19:05:58] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 19:05:58] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:05:58] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-27 19:05:58] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:05:58] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:05:58] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:05:58] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:05:58] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:05:58] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:05:58] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 19:05:58] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 19:05:58] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 19:05:58] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 19:05:58] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2971578) [2026-01-27 19:05:59] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=2971578) [2026-01-27 19:05:59] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2971578) [2026-01-27 19:05:59] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=2971578) [2026-01-27 19:05:59] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=2971578) [2026-01-27 19:05:59] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=2971578) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2971578) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:08<00:25,  8.53s/it]
(EngineCore_DP0 pid=2971578) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:35<00:38, 19.38s/it]
(EngineCore_DP0 pid=2971578) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:56<00:20, 20.25s/it]
(EngineCore_DP0 pid=2971578) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:23<00:00, 22.85s/it]
(EngineCore_DP0 pid=2971578) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:23<00:00, 20.91s/it]
(EngineCore_DP0 pid=2971578) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=2971578) 2026-01-27 19:07:29,833 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2971578) 2026-01-27 19:07:29,939 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 6024.30it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [01:08<9:46:31, 68.87s/it, est. speed input: 0.23 toks/s, output: 3.72 toks/s]
Processed prompts:   3%|▎         | 17/512 [01:09<24:10,  2.93s/it, est. speed input: 3.91 toks/s, output: 62.53 toks/s]
Processed prompts:   9%|▉         | 48/512 [01:10<06:19,  1.22it/s, est. speed input: 10.93 toks/s, output: 174.81 toks/s]
Processed prompts:  15%|█▌        | 77/512 [01:10<03:05,  2.34it/s, est. speed input: 17.36 toks/s, output: 277.79 toks/s]
Processed prompts:  20%|██        | 104/512 [01:11<01:48,  3.74it/s, est. speed input: 23.24 toks/s, output: 371.91 toks/s]
Processed prompts:  25%|██▌       | 129/512 [01:11<01:08,  5.59it/s, est. speed input: 28.73 toks/s, output: 459.70 toks/s]
Processed prompts:  30%|██▉       | 153/512 [01:12<00:44,  8.03it/s, est. speed input: 33.97 toks/s, output: 543.47 toks/s]
Processed prompts:  34%|███▍      | 175/512 [01:12<00:30, 11.01it/s, est. speed input: 38.73 toks/s, output: 619.65 toks/s]
Processed prompts:  38%|███▊      | 196/512 [01:12<00:21, 14.77it/s, est. speed input: 43.25 toks/s, output: 691.95 toks/s]
Processed prompts:  42%|████▏     | 215/512 [01:12<00:15, 19.08it/s, est. speed input: 47.30 toks/s, output: 756.80 toks/s]
Processed prompts:  46%|████▌     | 233/512 [01:12<00:11, 24.18it/s, est. speed input: 51.12 toks/s, output: 817.89 toks/s]
Processed prompts:  49%|████▉     | 250/512 [01:13<00:08, 29.98it/s, est. speed input: 54.70 toks/s, output: 875.27 toks/s]
Processed prompts:  52%|█████▏    | 266/512 [01:13<00:06, 35.52it/s, est. speed input: 58.03 toks/s, output: 928.53 toks/s]
Processed prompts:  55%|█████▍    | 281/512 [01:13<00:05, 41.69it/s, est. speed input: 61.15 toks/s, output: 978.46 toks/s]
Processed prompts:  58%|█████▊    | 295/512 [01:13<00:04, 47.49it/s, est. speed input: 64.05 toks/s, output: 1024.74 toks/s]
Processed prompts:  60%|██████    | 308/512 [01:13<00:03, 52.24it/s, est. speed input: 66.71 toks/s, output: 1067.35 toks/s]
Processed prompts:  62%|██████▎   | 320/512 [01:14<00:03, 56.24it/s, est. speed input: 69.15 toks/s, output: 1106.47 toks/s]
Processed prompts:  65%|██████▍   | 332/512 [01:14<00:02, 60.53it/s, est. speed input: 71.60 toks/s, output: 1145.54 toks/s]
Processed prompts:  67%|██████▋   | 343/512 [01:14<00:02, 62.87it/s, est. speed input: 73.81 toks/s, output: 1181.02 toks/s]
Processed prompts:  69%|██████▉   | 353/512 [01:14<00:02, 63.02it/s, est. speed input: 75.81 toks/s, output: 1212.88 toks/s]
Processed prompts:  71%|███████   | 363/512 [01:14<00:02, 64.17it/s, est. speed input: 77.80 toks/s, output: 1244.77 toks/s]
Processed prompts:  73%|███████▎  | 372/512 [01:14<00:02, 63.94it/s, est. speed input: 79.58 toks/s, output: 1273.21 toks/s]
Processed prompts:  74%|███████▍  | 380/512 [01:14<00:02, 61.57it/s, est. speed input: 81.13 toks/s, output: 1298.07 toks/s]
Processed prompts:  76%|███████▌  | 388/512 [01:15<00:02, 60.22it/s, est. speed input: 82.68 toks/s, output: 1322.91 toks/s]
Processed prompts:  77%|███████▋  | 395/512 [01:15<00:02, 58.38it/s, est. speed input: 84.03 toks/s, output: 1344.43 toks/s]
Processed prompts:  79%|███████▊  | 402/512 [01:15<00:01, 57.32it/s, est. speed input: 85.37 toks/s, output: 1365.92 toks/s]
Processed prompts:  80%|███████▉  | 408/512 [01:15<00:01, 54.81it/s, est. speed input: 86.50 toks/s, output: 1384.01 toks/s]
Processed prompts:  81%|████████  | 414/512 [01:15<00:01, 52.90it/s, est. speed input: 87.63 toks/s, output: 1402.05 toks/s]
Processed prompts:  82%|████████▏ | 420/512 [01:15<00:01, 51.98it/s, est. speed input: 88.76 toks/s, output: 1420.09 toks/s]
Processed prompts:  83%|████████▎ | 426/512 [01:15<00:01, 51.48it/s, est. speed input: 89.88 toks/s, output: 1438.11 toks/s]
Processed prompts:  84%|████████▍ | 432/512 [01:15<00:01, 50.55it/s, est. speed input: 91.00 toks/s, output: 1455.98 toks/s]
Processed prompts:  86%|████████▌ | 438/512 [01:16<00:01, 51.09it/s, est. speed input: 92.12 toks/s, output: 1473.99 toks/s]
Processed prompts:  87%|████████▋ | 444/512 [01:16<00:01, 40.33it/s, est. speed input: 93.11 toks/s, output: 1489.77 toks/s]
Processed prompts:  88%|████████▊ | 449/512 [01:16<00:01, 41.36it/s, est. speed input: 94.02 toks/s, output: 1504.34 toks/s]
Processed prompts:  89%|████████▊ | 454/512 [01:16<00:01, 34.13it/s, est. speed input: 94.80 toks/s, output: 1516.81 toks/s]
Processed prompts:  89%|████████▉ | 458/512 [01:16<00:01, 34.90it/s, est. speed input: 95.50 toks/s, output: 1528.05 toks/s]
Processed prompts:  90%|█████████ | 462/512 [01:16<00:01, 35.54it/s, est. speed input: 96.20 toks/s, output: 1539.26 toks/s]
Processed prompts:  91%|█████████ | 466/512 [01:17<00:01, 28.80it/s, est. speed input: 96.77 toks/s, output: 1548.31 toks/s]
Processed prompts:  92%|█████████▏| 470/512 [01:17<00:01, 25.54it/s, est. speed input: 97.34 toks/s, output: 1557.49 toks/s]
Processed prompts:  92%|█████████▏| 473/512 [01:17<00:01, 26.43it/s, est. speed input: 97.84 toks/s, output: 1565.40 toks/s]
Processed prompts:  93%|█████████▎| 476/512 [01:17<00:01, 22.42it/s, est. speed input: 98.21 toks/s, output: 1571.34 toks/s]
Processed prompts:  94%|█████████▍| 480/512 [01:17<00:01, 21.81it/s, est. speed input: 98.79 toks/s, output: 1580.59 toks/s]
Processed prompts:  94%|█████████▍| 483/512 [01:17<00:01, 19.93it/s, est. speed input: 99.16 toks/s, output: 1586.62 toks/s]
Processed prompts:  95%|█████████▍| 486/512 [01:18<00:01, 16.10it/s, est. speed input: 99.42 toks/s, output: 1590.66 toks/s]
Processed prompts:  95%|█████████▌| 488/512 [01:18<00:01, 14.65it/s, est. speed input: 99.59 toks/s, output: 1593.43 toks/s]
Processed prompts:  96%|█████████▌| 490/512 [01:18<00:01, 13.58it/s, est. speed input: 99.76 toks/s, output: 1596.21 toks/s]
Processed prompts:  96%|█████████▌| 492/512 [01:18<00:01, 12.82it/s, est. speed input: 99.94 toks/s, output: 1598.99 toks/s]
Processed prompts:  96%|█████████▋| 494/512 [01:18<00:01, 12.30it/s, est. speed input: 100.11 toks/s, output: 1601.80 toks/s]
Processed prompts:  97%|█████████▋| 496/512 [01:19<00:01, 11.93it/s, est. speed input: 100.29 toks/s, output: 1604.60 toks/s]
Processed prompts:  97%|█████████▋| 497/512 [01:29<00:01, 11.93it/s, est. speed input: 100.37 toks/s, output: 1605.91 toks/s]
Processed prompts:  97%|█████████▋| 498/512 [01:36<00:33,  2.42s/it, est. speed input: 82.90 toks/s, output: 1326.35 toks/s] 
Processed prompts:  98%|█████████▊| 502/512 [01:36<00:13,  1.37s/it, est. speed input: 83.41 toks/s, output: 1334.52 toks/s]
Processed prompts: 100%|██████████| 512/512 [01:36<00:00,  1.37s/it, est. speed input: 84.99 toks/s, output: 1359.88 toks/s]
Processed prompts: 100%|██████████| 512/512 [01:36<00:00,  5.31it/s, est. speed input: 84.99 toks/s, output: 1359.88 toks/s]
[rank0]:[W127 19:09:08.125132910 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 203.6s

测试结果:
  Requests/s:   5.31
  Tokens/s:     1443.27
  Total Reqs:   512
  Elapsed:      96.49s

  [Decode 分析]
  Total Decode Tokens:  131072
  Decode Tokens/s:      1358.37


------------------------------------------------------------
  生成 CSV: Qwen2.5-14B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cublaslt/Qwen2.5-14B-INT8_decode.csv

预览:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
64,16,64,64,256,256,2.2690,617.1633,28.2065
128,16,128,128,256,256,3.7594,1022.5541,34.0481
256,16,256,256,256,256,5.0808,1381.9726,50.3859
512,16,512,512,256,256,5.3061,1443.2663,96.4922

------------------------------------------------------------

[INFO] 完成: 4 成功, 0 失败

============================================================
  Qwen2.5-14B-INT8 | cuSPARSELt (2_4) | decode
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_4
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_4

============================================================
[1/4] 测试 M=64
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-14B-INT8                                │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 64
│   M_prefill     = 1024 (= 64 x 16)
│   M_decode      = 64
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 64
│   --max-num-seqs           = 64
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 19:09:20 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 19:09:20 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2975201) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2975201) WARNING 01-27 19:10:59 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 3.21 requests/s, 873.24 total tokens/s, 821.88 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384


─── STDERR ───
[2026-01-27 19:09:20] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 19:09:20] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:09:20] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-27 19:09:20] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:09:20] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:09:20] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:09:20] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:09:20] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:09:20] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:09:20] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 19:09:20] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 19:09:20] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 19:09:20] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 19:09:20] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 19:09:23] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 19:09:23] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:09:23] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-27 19:09:23] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:09:23] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:09:23] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:09:23] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:09:23] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:09:23] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:09:23] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 19:09:23] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 19:09:23] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 19:09:23] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 19:09:23] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2975201) [2026-01-27 19:09:24] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2975201) [2026-01-27 19:09:24] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2975201) [2026-01-27 19:09:24] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2975201) [2026-01-27 19:09:24] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=2975201) [2026-01-27 19:09:24] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=2975201) [2026-01-27 19:09:24] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2975201) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2975201) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:08<00:25,  8.41s/it]
(EngineCore_DP0 pid=2975201) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:34<00:37, 18.62s/it]
(EngineCore_DP0 pid=2975201) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:55<00:19, 19.86s/it]
(EngineCore_DP0 pid=2975201) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:23<00:00, 22.90s/it]
(EngineCore_DP0 pid=2975201) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:23<00:00, 20.77s/it]
(EngineCore_DP0 pid=2975201) 
(EngineCore_DP0 pid=2975201) [2026-01-27 19:10:49] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [7168, 5120] -> 1D uint8
(EngineCore_DP0 pid=2975201) [2026-01-27 19:10:49] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 27525120 bytes
(EngineCore_DP0 pid=2975201) [2026-01-27 19:10:49] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 5120] -> 1D uint8
(EngineCore_DP0 pid=2975201) [2026-01-27 19:10:49] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19660800 bytes
(EngineCore_DP0 pid=2975201) [2026-01-27 19:10:49] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [27648, 5120] -> 1D uint8
(EngineCore_DP0 pid=2975201) [2026-01-27 19:10:49] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 106168320 bytes
(EngineCore_DP0 pid=2975201) [2026-01-27 19:10:49] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 13824] -> 1D uint8
(EngineCore_DP0 pid=2975201) [2026-01-27 19:10:49] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 53084160 bytes
(EngineCore_DP0 pid=2975201) 2026-01-27 19:10:58,687 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2975201) 2026-01-27 19:10:58,716 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 1/64 [00:00<00:08,  7.66it/s]
Adding requests:   5%|▍         | 3/64 [00:00<00:04, 13.89it/s]
Adding requests:  11%|█         | 7/64 [00:00<00:02, 23.92it/s]
Adding requests:  19%|█▉        | 12/64 [00:00<00:01, 32.18it/s]
Adding requests:  31%|███▏      | 20/64 [00:00<00:00, 46.79it/s]
Adding requests:  52%|█████▏    | 33/64 [00:00<00:00, 72.70it/s]
Adding requests:  78%|███████▊  | 50/64 [00:00<00:00, 101.91it/s]
Adding requests: 100%|██████████| 64/64 [00:00<00:00, 76.87it/s] 

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 1/64 [00:18<19:34, 18.65s/it, est. speed input: 0.86 toks/s, output: 13.73 toks/s]
Processed prompts:  11%|█         | 7/64 [00:18<01:52,  1.97s/it, est. speed input: 5.95 toks/s, output: 95.28 toks/s]
Processed prompts:  59%|█████▉    | 38/64 [00:18<00:06,  3.79it/s, est. speed input: 32.05 toks/s, output: 512.78 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:19<00:00,  3.79it/s, est. speed input: 53.81 toks/s, output: 860.93 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:19<00:00,  3.36it/s, est. speed input: 53.81 toks/s, output: 860.93 toks/s]
[rank0]:[W127 19:11:19.872897545 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 140.0s

测试结果:
  Requests/s:   3.21
  Tokens/s:     873.24
  Total Reqs:   64
  Elapsed:      19.93s

  [Decode 分析]
  Total Decode Tokens:  16384
  Decode Tokens/s:      821.88

============================================================
[2/4] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-14B-INT8                                │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 2048 (= 128 x 16)
│   M_decode      = 128
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 128
│   --max-num-seqs           = 128
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 19:11:39 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 19:11:39 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2977766) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2977766) WARNING 01-27 19:13:17 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 5.26 requests/s, 1430.16 total tokens/s, 1346.03 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768


─── STDERR ───
[2026-01-27 19:11:39] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 19:11:39] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:11:39] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-27 19:11:39] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:11:39] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:11:39] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:11:39] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:11:39] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:11:39] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:11:39] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 19:11:39] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 19:11:39] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 19:11:39] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 19:11:39] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 19:11:42] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 19:11:42] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:11:42] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-27 19:11:42] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:11:42] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:11:42] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:11:42] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:11:42] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:11:42] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:11:42] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 19:11:42] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 19:11:42] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 19:11:42] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 19:11:42] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2977766) [2026-01-27 19:11:43] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2977766) [2026-01-27 19:11:43] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2977766) [2026-01-27 19:11:43] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2977766) [2026-01-27 19:11:43] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=2977766) [2026-01-27 19:11:43] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=2977766) [2026-01-27 19:11:43] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2977766) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2977766) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:08<00:25,  8.41s/it]
(EngineCore_DP0 pid=2977766) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:35<00:38, 19.11s/it]
(EngineCore_DP0 pid=2977766) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:56<00:20, 20.10s/it]
(EngineCore_DP0 pid=2977766) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:23<00:00, 22.84s/it]
(EngineCore_DP0 pid=2977766) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:23<00:00, 20.83s/it]
(EngineCore_DP0 pid=2977766) 
(EngineCore_DP0 pid=2977766) [2026-01-27 19:13:08] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [7168, 5120] -> 1D uint8
(EngineCore_DP0 pid=2977766) [2026-01-27 19:13:08] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 27525120 bytes
(EngineCore_DP0 pid=2977766) [2026-01-27 19:13:08] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 5120] -> 1D uint8
(EngineCore_DP0 pid=2977766) [2026-01-27 19:13:08] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19660800 bytes
(EngineCore_DP0 pid=2977766) [2026-01-27 19:13:08] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [27648, 5120] -> 1D uint8
(EngineCore_DP0 pid=2977766) [2026-01-27 19:13:08] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 106168320 bytes
(EngineCore_DP0 pid=2977766) [2026-01-27 19:13:08] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 13824] -> 1D uint8
(EngineCore_DP0 pid=2977766) [2026-01-27 19:13:08] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 53084160 bytes
(EngineCore_DP0 pid=2977766) 2026-01-27 19:13:16,624 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2977766) 2026-01-27 19:13:16,654 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 3620.14it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:23<49:22, 23.33s/it, est. speed input: 0.69 toks/s, output: 10.97 toks/s]
Processed prompts:   2%|▏         | 2/128 [00:23<20:18,  9.67s/it, est. speed input: 1.37 toks/s, output: 21.84 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:23<01:14,  1.48it/s, est. speed input: 12.23 toks/s, output: 195.65 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:23<00:28,  3.28it/s, est. speed input: 22.32 toks/s, output: 357.06 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:23<00:08,  8.06it/s, est. speed input: 41.62 toks/s, output: 665.85 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:24<00:03, 13.47it/s, est. speed input: 57.95 toks/s, output: 927.25 toks/s]
Processed prompts:  84%|████████▍ | 108/128 [00:24<00:01, 19.43it/s, est. speed input: 71.50 toks/s, output: 1143.99 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:24<00:00, 26.56it/s, est. speed input: 83.68 toks/s, output: 1338.82 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:24<00:00, 26.56it/s, est. speed input: 84.33 toks/s, output: 1349.36 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:24<00:00,  5.27it/s, est. speed input: 84.33 toks/s, output: 1349.36 toks/s]
[rank0]:[W127 19:13:42.257676139 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 132.1s

测试结果:
  Requests/s:   5.26
  Tokens/s:     1430.16
  Total Reqs:   128
  Elapsed:      24.34s

  [Decode 分析]
  Total Decode Tokens:  32768
  Decode Tokens/s:      1346.03

============================================================
[3/4] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-14B-INT8                                │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 4096 (= 256 x 16)
│   M_decode      = 256
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 256
│   --max-num-seqs           = 256
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 19:13:51 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 19:13:51 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2980166) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2980166) WARNING 01-27 19:15:31 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 6.69 requests/s, 1820.81 total tokens/s, 1713.71 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536


─── STDERR ───
[2026-01-27 19:13:51] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 19:13:51] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:13:51] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-27 19:13:51] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:13:51] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:13:51] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:13:51] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:13:51] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:13:51] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:13:51] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 19:13:51] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 19:13:51] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 19:13:51] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 19:13:51] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 19:13:54] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 19:13:54] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:13:54] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-27 19:13:54] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:13:54] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:13:54] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:13:54] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:13:54] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:13:54] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:13:54] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 19:13:54] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 19:13:54] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 19:13:54] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 19:13:54] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2980166) [2026-01-27 19:13:55] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2980166) [2026-01-27 19:13:55] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2980166) [2026-01-27 19:13:55] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2980166) [2026-01-27 19:13:55] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=2980166) [2026-01-27 19:13:55] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=2980166) [2026-01-27 19:13:55] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2980166) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2980166) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:08<00:25,  8.61s/it]
(EngineCore_DP0 pid=2980166) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:35<00:38, 19.43s/it]
(EngineCore_DP0 pid=2980166) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:57<00:20, 20.37s/it]
(EngineCore_DP0 pid=2980166) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:24<00:00, 23.15s/it]
(EngineCore_DP0 pid=2980166) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:24<00:00, 21.13s/it]
(EngineCore_DP0 pid=2980166) 
(EngineCore_DP0 pid=2980166) [2026-01-27 19:15:21] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [7168, 5120] -> 1D uint8
(EngineCore_DP0 pid=2980166) [2026-01-27 19:15:21] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 27525120 bytes
(EngineCore_DP0 pid=2980166) [2026-01-27 19:15:21] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 5120] -> 1D uint8
(EngineCore_DP0 pid=2980166) [2026-01-27 19:15:21] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19660800 bytes
(EngineCore_DP0 pid=2980166) [2026-01-27 19:15:21] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [27648, 5120] -> 1D uint8
(EngineCore_DP0 pid=2980166) [2026-01-27 19:15:21] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 106168320 bytes
(EngineCore_DP0 pid=2980166) [2026-01-27 19:15:21] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 13824] -> 1D uint8
(EngineCore_DP0 pid=2980166) [2026-01-27 19:15:21] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 53084160 bytes
(EngineCore_DP0 pid=2980166) 2026-01-27 19:15:29,887 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2980166) 2026-01-27 19:15:29,917 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   0%|          | 1/256 [00:00<00:58,  4.39it/s]
Adding requests:   1%|          | 3/256 [00:00<00:26,  9.56it/s]
Adding requests:   2%|▏         | 6/256 [00:00<00:15, 16.42it/s]
Adding requests:   4%|▍         | 11/256 [00:00<00:09, 25.17it/s]
Adding requests:   7%|▋         | 17/256 [00:00<00:06, 35.32it/s]
Adding requests:  11%|█         | 28/256 [00:00<00:04, 56.31it/s]
Adding requests:  17%|█▋        | 44/256 [00:00<00:02, 86.12it/s]
Adding requests:  26%|██▌       | 67/256 [00:00<00:01, 127.76it/s]
Adding requests:  37%|███▋      | 94/256 [00:01<00:00, 169.08it/s]
Adding requests:  52%|█████▏    | 132/256 [00:01<00:00, 230.85it/s]
Adding requests:  63%|██████▎   | 161/256 [00:01<00:00, 159.58it/s]
Adding requests:  77%|███████▋  | 198/256 [00:01<00:00, 203.58it/s]
Adding requests:  99%|█████████▉| 253/256 [00:01<00:00, 284.08it/s]
Adding requests: 100%|██████████| 256/256 [00:01<00:00, 151.42it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:31<2:12:58, 31.29s/it, est. speed input: 0.51 toks/s, output: 8.18 toks/s]
Processed prompts:   1%|          | 2/256 [00:31<54:55, 12.97s/it, est. speed input: 1.02 toks/s, output: 16.28 toks/s] 
Processed prompts:   2%|▏         | 5/256 [00:31<15:33,  3.72s/it, est. speed input: 2.53 toks/s, output: 40.52 toks/s]
Processed prompts:   3%|▎         | 7/256 [00:31<09:17,  2.24s/it, est. speed input: 3.53 toks/s, output: 56.47 toks/s]
Processed prompts:   4%|▍         | 11/256 [00:31<04:19,  1.06s/it, est. speed input: 5.52 toks/s, output: 88.33 toks/s]
Processed prompts:   5%|▌         | 14/256 [00:32<02:46,  1.45it/s, est. speed input: 6.99 toks/s, output: 111.90 toks/s]
Processed prompts:   7%|▋         | 18/256 [00:32<01:40,  2.36it/s, est. speed input: 8.95 toks/s, output: 143.13 toks/s]
Processed prompts:  10%|▉         | 25/256 [00:32<00:50,  4.56it/s, est. speed input: 12.37 toks/s, output: 197.92 toks/s]
Processed prompts:  14%|█▍        | 36/256 [00:32<00:24,  9.06it/s, est. speed input: 17.73 toks/s, output: 283.62 toks/s]
Processed prompts:  20%|█▉        | 50/256 [00:32<00:12, 16.48it/s, est. speed input: 24.52 toks/s, output: 392.35 toks/s]
Processed prompts:  25%|██▍       | 63/256 [00:32<00:07, 24.58it/s, est. speed input: 30.77 toks/s, output: 492.39 toks/s]
Processed prompts:  30%|██▉       | 76/256 [00:32<00:05, 33.41it/s, est. speed input: 36.97 toks/s, output: 591.44 toks/s]
Processed prompts:  34%|███▍      | 88/256 [00:33<00:04, 41.42it/s, est. speed input: 42.62 toks/s, output: 681.85 toks/s]
Processed prompts:  39%|███▊      | 99/256 [00:33<00:03, 48.83it/s, est. speed input: 47.75 toks/s, output: 764.08 toks/s]
Processed prompts:  43%|████▎     | 109/256 [00:33<00:02, 56.19it/s, est. speed input: 52.41 toks/s, output: 838.52 toks/s]
Processed prompts:  46%|████▋     | 119/256 [00:33<00:02, 63.40it/s, est. speed input: 57.03 toks/s, output: 912.52 toks/s]
Processed prompts:  50%|█████     | 128/256 [00:33<00:01, 68.13it/s, est. speed input: 61.15 toks/s, output: 978.45 toks/s]
Processed prompts:  54%|█████▎    | 137/256 [00:33<00:01, 70.82it/s, est. speed input: 65.23 toks/s, output: 1043.70 toks/s]
Processed prompts:  57%|█████▋    | 146/256 [00:33<00:01, 73.01it/s, est. speed input: 69.28 toks/s, output: 1108.52 toks/s]
Processed prompts:  61%|██████    | 155/256 [00:33<00:01, 76.12it/s, est. speed input: 73.32 toks/s, output: 1173.17 toks/s]
Processed prompts:  65%|██████▌   | 167/256 [00:34<00:01, 71.99it/s, est. speed input: 78.57 toks/s, output: 1257.18 toks/s]
Processed prompts:  70%|██████▉   | 179/256 [00:34<00:01, 70.97it/s, est. speed input: 83.79 toks/s, output: 1340.67 toks/s]
Processed prompts:  74%|███████▍  | 189/256 [00:34<00:00, 67.28it/s, est. speed input: 88.04 toks/s, output: 1408.64 toks/s]
Processed prompts:  77%|███████▋  | 198/256 [00:34<00:00, 64.42it/s, est. speed input: 91.82 toks/s, output: 1469.05 toks/s]
Processed prompts:  80%|████████  | 206/256 [00:34<00:00, 61.01it/s, est. speed input: 95.11 toks/s, output: 1521.73 toks/s]
Processed prompts:  83%|████████▎ | 213/256 [00:34<00:00, 57.59it/s, est. speed input: 97.94 toks/s, output: 1566.97 toks/s]
Processed prompts:  86%|████████▌ | 219/256 [00:34<00:00, 53.33it/s, est. speed input: 100.29 toks/s, output: 1604.63 toks/s]
Processed prompts:  88%|████████▊ | 225/256 [00:35<00:00, 49.38it/s, est. speed input: 102.60 toks/s, output: 1641.59 toks/s]
Processed prompts:  90%|████████▉ | 230/256 [00:35<00:00, 46.21it/s, est. speed input: 104.49 toks/s, output: 1671.78 toks/s]
Processed prompts:  92%|█████████▏| 235/256 [00:35<00:00, 43.75it/s, est. speed input: 106.36 toks/s, output: 1701.69 toks/s]
Processed prompts:  94%|█████████▍| 240/256 [00:35<00:00, 37.14it/s, est. speed input: 108.03 toks/s, output: 1728.46 toks/s]
Processed prompts:  95%|█████████▌| 244/256 [00:35<00:00, 32.31it/s, est. speed input: 109.28 toks/s, output: 1748.50 toks/s]
Processed prompts:  97%|█████████▋| 248/256 [00:35<00:00, 26.62it/s, est. speed input: 110.36 toks/s, output: 1765.73 toks/s]
Processed prompts:  98%|█████████▊| 251/256 [00:36<00:00, 24.08it/s, est. speed input: 111.17 toks/s, output: 1778.70 toks/s]
Processed prompts:  99%|█████████▉| 254/256 [00:36<00:00, 22.32it/s, est. speed input: 111.98 toks/s, output: 1791.66 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:36<00:00, 22.32it/s, est. speed input: 112.41 toks/s, output: 1798.60 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:36<00:00,  7.03it/s, est. speed input: 112.41 toks/s, output: 1798.60 toks/s]
[rank0]:[W127 19:16:10.395655239 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 159.4s

测试结果:
  Requests/s:   6.69
  Tokens/s:     1820.81
  Total Reqs:   256
  Elapsed:      38.24s

  [Decode 分析]
  Total Decode Tokens:  65536
  Decode Tokens/s:      1713.71

============================================================
[4/4] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-14B-INT8                                │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 8192 (= 512 x 16)
│   M_decode      = 512
│   batched_tokens = 512 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 512
│   --max-num-seqs           = 512
│   --max-model-len          = 272
│   --max-num-batched-tokens = 512
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 19:16:28 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 19:16:28 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2982989) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2982989) WARNING 01-27 19:18:06 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 6.81 requests/s, 1853.35 total tokens/s, 1744.33 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072


─── STDERR ───
[2026-01-27 19:16:28] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 19:16:28] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:16:28] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-27 19:16:28] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:16:28] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:16:28] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:16:28] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:16:28] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:16:28] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:16:28] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 19:16:28] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 19:16:28] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 19:16:28] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 19:16:28] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 19:16:31] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 19:16:31] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:16:31] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-27 19:16:31] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:16:31] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:16:31] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:16:31] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:16:31] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:16:31] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:16:31] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 19:16:31] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 19:16:31] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 19:16:31] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 19:16:31] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2982989) [2026-01-27 19:16:32] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2982989) [2026-01-27 19:16:32] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2982989) [2026-01-27 19:16:32] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2982989) [2026-01-27 19:16:32] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=2982989) [2026-01-27 19:16:32] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=2982989) [2026-01-27 19:16:32] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2982989) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2982989) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:08<00:25,  8.60s/it]
(EngineCore_DP0 pid=2982989) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:35<00:38, 19.11s/it]
(EngineCore_DP0 pid=2982989) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:55<00:19, 19.93s/it]
(EngineCore_DP0 pid=2982989) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:23<00:00, 22.95s/it]
(EngineCore_DP0 pid=2982989) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:23<00:00, 20.89s/it]
(EngineCore_DP0 pid=2982989) 
(EngineCore_DP0 pid=2982989) [2026-01-27 19:17:57] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [7168, 5120] -> 1D uint8
(EngineCore_DP0 pid=2982989) [2026-01-27 19:17:57] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 27525120 bytes
(EngineCore_DP0 pid=2982989) [2026-01-27 19:17:57] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 5120] -> 1D uint8
(EngineCore_DP0 pid=2982989) [2026-01-27 19:17:57] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 19660800 bytes
(EngineCore_DP0 pid=2982989) [2026-01-27 19:17:57] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [27648, 5120] -> 1D uint8
(EngineCore_DP0 pid=2982989) [2026-01-27 19:17:57] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 106168320 bytes
(EngineCore_DP0 pid=2982989) [2026-01-27 19:17:57] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 13824] -> 1D uint8
(EngineCore_DP0 pid=2982989) [2026-01-27 19:17:57] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 53084160 bytes
(EngineCore_DP0 pid=2982989) 2026-01-27 19:18:05,374 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2982989) 2026-01-27 19:18:05,411 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 11496.78it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:54<7:46:39, 54.79s/it, est. speed input: 0.29 toks/s, output: 4.67 toks/s]
Processed prompts:   0%|          | 2/512 [00:55<3:15:15, 22.97s/it, est. speed input: 0.58 toks/s, output: 9.23 toks/s]
Processed prompts:   6%|▋         | 33/512 [00:56<06:56,  1.15it/s, est. speed input: 9.40 toks/s, output: 150.47 toks/s]
Processed prompts:  12%|█▏        | 63/512 [00:56<02:52,  2.60it/s, est. speed input: 17.76 toks/s, output: 284.09 toks/s]
Processed prompts:  18%|█▊        | 91/512 [00:57<01:35,  4.39it/s, est. speed input: 25.39 toks/s, output: 406.16 toks/s]
Processed prompts:  23%|██▎       | 117/512 [00:57<01:00,  6.56it/s, est. speed input: 32.32 toks/s, output: 517.13 toks/s]
Processed prompts:  28%|██▊       | 141/512 [00:58<00:39,  9.44it/s, est. speed input: 38.81 toks/s, output: 620.91 toks/s]
Processed prompts:  32%|███▏      | 164/512 [00:58<00:26, 13.15it/s, est. speed input: 44.98 toks/s, output: 719.69 toks/s]
Processed prompts:  36%|███▌      | 185/512 [00:58<00:18, 17.57it/s, est. speed input: 50.57 toks/s, output: 809.15 toks/s]
Processed prompts:  40%|████      | 205/512 [00:58<00:13, 22.90it/s, est. speed input: 55.85 toks/s, output: 893.66 toks/s]
Processed prompts:  44%|████▍     | 224/512 [00:58<00:09, 29.23it/s, est. speed input: 60.85 toks/s, output: 973.54 toks/s]
Processed prompts:  47%|████▋     | 242/512 [00:59<00:07, 36.33it/s, est. speed input: 65.54 toks/s, output: 1048.70 toks/s]
Processed prompts:  50%|█████     | 258/512 [00:59<00:05, 42.49it/s, est. speed input: 69.65 toks/s, output: 1114.42 toks/s]
Processed prompts:  53%|█████▎    | 273/512 [00:59<00:04, 48.69it/s, est. speed input: 73.48 toks/s, output: 1175.70 toks/s]
Processed prompts:  56%|█████▋    | 288/512 [00:59<00:03, 57.25it/s, est. speed input: 77.34 toks/s, output: 1237.47 toks/s]
Processed prompts:  59%|█████▉    | 302/512 [00:59<00:03, 63.24it/s, est. speed input: 80.89 toks/s, output: 1294.22 toks/s]
Processed prompts:  62%|██████▏   | 315/512 [00:59<00:02, 68.75it/s, est. speed input: 84.17 toks/s, output: 1346.74 toks/s]
Processed prompts:  64%|██████▍   | 327/512 [01:00<00:02, 72.86it/s, est. speed input: 87.18 toks/s, output: 1394.87 toks/s]
Processed prompts:  66%|██████▌   | 338/512 [01:00<00:02, 74.73it/s, est. speed input: 89.91 toks/s, output: 1438.54 toks/s]
Processed prompts:  68%|██████▊   | 348/512 [01:00<00:02, 77.30it/s, est. speed input: 92.39 toks/s, output: 1478.26 toks/s]
Processed prompts:  70%|██████▉   | 358/512 [01:00<00:01, 77.53it/s, est. speed input: 94.84 toks/s, output: 1517.52 toks/s]
Processed prompts:  72%|███████▏  | 367/512 [01:00<00:01, 78.79it/s, est. speed input: 97.05 toks/s, output: 1552.87 toks/s]
Processed prompts:  73%|███████▎  | 376/512 [01:00<00:01, 77.53it/s, est. speed input: 99.24 toks/s, output: 1587.77 toks/s]
Processed prompts:  75%|███████▌  | 385/512 [01:00<00:01, 79.18it/s, est. speed input: 101.43 toks/s, output: 1622.91 toks/s]
Processed prompts:  77%|███████▋  | 394/512 [01:00<00:01, 76.30it/s, est. speed input: 103.58 toks/s, output: 1657.33 toks/s]
Processed prompts:  79%|███████▉  | 406/512 [01:01<00:01, 69.53it/s, est. speed input: 106.39 toks/s, output: 1702.18 toks/s]
Processed prompts:  82%|████████▏ | 418/512 [01:01<00:01, 66.81it/s, est. speed input: 109.18 toks/s, output: 1746.94 toks/s]
Processed prompts:  84%|████████▎ | 428/512 [01:01<00:01, 64.38it/s, est. speed input: 111.49 toks/s, output: 1783.80 toks/s]
Processed prompts:  85%|████████▍ | 435/512 [01:01<00:01, 65.50it/s, est. speed input: 113.13 toks/s, output: 1810.02 toks/s]
Processed prompts:  86%|████████▋ | 442/512 [01:01<00:01, 58.51it/s, est. speed input: 114.65 toks/s, output: 1834.37 toks/s]
Processed prompts:  88%|████████▊ | 450/512 [01:01<00:01, 52.69it/s, est. speed input: 116.36 toks/s, output: 1861.82 toks/s]
Processed prompts:  89%|████████▉ | 456/512 [01:02<00:01, 49.20it/s, est. speed input: 117.63 toks/s, output: 1882.12 toks/s]
Processed prompts:  90%|█████████ | 462/512 [01:02<00:01, 46.97it/s, est. speed input: 118.90 toks/s, output: 1902.43 toks/s]
Processed prompts:  91%|█████████▏| 468/512 [01:02<00:00, 44.22it/s, est. speed input: 120.14 toks/s, output: 1922.25 toks/s]
Processed prompts:  92%|█████████▏| 473/512 [01:02<00:00, 42.04it/s, est. speed input: 121.16 toks/s, output: 1938.51 toks/s]
Processed prompts:  93%|█████████▎| 478/512 [01:02<00:00, 35.86it/s, est. speed input: 122.05 toks/s, output: 1952.75 toks/s]
Processed prompts:  94%|█████████▍| 482/512 [01:02<00:00, 33.36it/s, est. speed input: 122.78 toks/s, output: 1964.43 toks/s]
Processed prompts:  95%|█████████▍| 486/512 [01:03<00:00, 26.28it/s, est. speed input: 123.30 toks/s, output: 1972.80 toks/s]
Processed prompts:  96%|█████████▌| 489/512 [01:03<00:00, 23.22it/s, est. speed input: 123.69 toks/s, output: 1979.10 toks/s]
Processed prompts:  96%|█████████▌| 492/512 [01:03<00:00, 21.25it/s, est. speed input: 124.10 toks/s, output: 1985.54 toks/s]
Processed prompts:  97%|█████████▋| 495/512 [01:03<00:00, 19.75it/s, est. speed input: 124.49 toks/s, output: 1991.84 toks/s]
Processed prompts:  97%|█████████▋| 498/512 [01:14<00:14,  1.01s/it, est. speed input: 106.43 toks/s, output: 1702.83 toks/s]
Processed prompts:  98%|█████████▊| 501/512 [01:14<00:08,  1.34it/s, est. speed input: 106.90 toks/s, output: 1710.44 toks/s]
Processed prompts: 100%|█████████▉| 511/512 [01:15<00:00,  3.00it/s, est. speed input: 108.87 toks/s, output: 1741.99 toks/s]
Processed prompts: 100%|██████████| 512/512 [01:15<00:00,  3.00it/s, est. speed input: 109.09 toks/s, output: 1745.40 toks/s]
Processed prompts: 100%|██████████| 512/512 [01:15<00:00,  6.82it/s, est. speed input: 109.09 toks/s, output: 1745.40 toks/s]
[rank0]:[W127 19:19:22.127372634 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 180.4s

测试结果:
  Requests/s:   6.81
  Tokens/s:     1853.35
  Total Reqs:   512
  Elapsed:      75.14s

  [Decode 分析]
  Total Decode Tokens:  131072
  Decode Tokens/s:      1744.33


------------------------------------------------------------
  生成 CSV: Qwen2.5-14B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_4/Qwen2.5-14B-INT8_decode.csv

预览:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
64,16,64,64,256,256,3.2105,873.2425,19.9349
128,16,128,128,256,256,5.2579,1430.1580,24.3442
256,16,256,256,256,256,6.6942,1820.8142,38.2422
512,16,512,512,256,256,6.8138,1853.3517,75.1417

------------------------------------------------------------

[INFO] 完成: 4 成功, 0 失败

============================================================
  Qwen2.5-14B-INT8 | cuSPARSELt (2_6) | decode
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_6
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_6

============================================================
[1/4] 测试 M=64
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-14B-INT8                                │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 64
│   M_prefill     = 1024 (= 64 x 16)
│   M_decode      = 64
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 64
│   --max-num-seqs           = 64
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 19:19:32 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 19:19:32 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2986255) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2986255) WARNING 01-27 19:21:41 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 2.37 requests/s, 645.11 total tokens/s, 607.16 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384


─── STDERR ───
[2026-01-27 19:19:32] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 19:19:32] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:19:32] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-27 19:19:32] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:19:32] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:19:32] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:19:32] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:19:32] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:19:32] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:19:32] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 19:19:32] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 19:19:32] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 19:19:32] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 19:19:32] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 19:19:36] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 19:19:36] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:19:36] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-27 19:19:36] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:19:36] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:19:36] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:19:36] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:19:36] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:19:36] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:19:36] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 19:19:36] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 19:19:36] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 19:19:36] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 19:19:36] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2986255) [2026-01-27 19:19:37] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2986255) [2026-01-27 19:19:37] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2986255) [2026-01-27 19:19:37] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2986255) [2026-01-27 19:19:37] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=2986255) [2026-01-27 19:19:37] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=2986255) [2026-01-27 19:19:37] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2986255) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2986255) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:08<00:25,  8.64s/it]
(EngineCore_DP0 pid=2986255) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:42<00:46, 23.46s/it]
(EngineCore_DP0 pid=2986255) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [01:14<00:27, 27.22s/it]
(EngineCore_DP0 pid=2986255) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:47<00:00, 29.66s/it]
(EngineCore_DP0 pid=2986255) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:47<00:00, 26.89s/it]
(EngineCore_DP0 pid=2986255) 
(EngineCore_DP0 pid=2986255) [2026-01-27 19:21:26] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [7168, 6848] -> 1D uint8
(EngineCore_DP0 pid=2986255) [2026-01-27 19:21:26] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 36929536 bytes
(EngineCore_DP0 pid=2986255) [2026-01-27 19:21:26] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 6848] -> 1D uint8
(EngineCore_DP0 pid=2986255) [2026-01-27 19:21:26] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 26378240 bytes
(EngineCore_DP0 pid=2986255) [2026-01-27 19:21:26] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [27648, 6848] -> 1D uint8
(EngineCore_DP0 pid=2986255) [2026-01-27 19:21:26] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 142442496 bytes
(EngineCore_DP0 pid=2986255) [2026-01-27 19:21:26] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 18432] -> 1D uint8
(EngineCore_DP0 pid=2986255) [2026-01-27 19:21:26] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 70778880 bytes
(EngineCore_DP0 pid=2986255) 2026-01-27 19:21:36,401 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2986255) 2026-01-27 19:21:36,633 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 1/64 [00:00<00:10,  6.21it/s]
Adding requests:   8%|▊         | 5/64 [00:00<00:02, 20.15it/s]
Adding requests:  17%|█▋        | 11/64 [00:00<00:01, 34.31it/s]
Adding requests:  33%|███▎      | 21/64 [00:00<00:00, 56.94it/s]
Adding requests:  62%|██████▎   | 40/64 [00:00<00:00, 99.86it/s]
Adding requests: 100%|██████████| 64/64 [00:00<00:00, 94.69it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 1/64 [00:25<26:57, 25.68s/it, est. speed input: 0.62 toks/s, output: 9.97 toks/s]
Processed prompts:   3%|▎         | 2/64 [00:25<10:59, 10.64s/it, est. speed input: 1.24 toks/s, output: 19.85 toks/s]
Processed prompts:  19%|█▉        | 12/64 [00:25<00:59,  1.14s/it, est. speed input: 7.41 toks/s, output: 118.61 toks/s]
Processed prompts:  44%|████▍     | 28/64 [00:26<00:13,  2.63it/s, est. speed input: 17.22 toks/s, output: 275.59 toks/s]
Processed prompts:  67%|██████▋   | 43/64 [00:26<00:04,  4.92it/s, est. speed input: 26.35 toks/s, output: 421.54 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:26<00:00,  4.92it/s, est. speed input: 39.09 toks/s, output: 625.41 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:26<00:00,  2.44it/s, est. speed input: 39.09 toks/s, output: 625.41 toks/s]
[rank0]:[W127 19:22:09.349039612 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 179.6s

测试结果:
  Requests/s:   2.37
  Tokens/s:     645.11
  Total Reqs:   64
  Elapsed:      26.98s

  [Decode 分析]
  Total Decode Tokens:  16384
  Decode Tokens/s:      607.16

============================================================
[2/4] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-14B-INT8                                │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 2048 (= 128 x 16)
│   M_decode      = 128
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 128
│   --max-num-seqs           = 128
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 19:22:32 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 19:22:32 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2989442) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2989442) WARNING 01-27 19:24:39 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 3.98 requests/s, 1083.53 total tokens/s, 1019.79 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768


─── STDERR ───
[2026-01-27 19:22:32] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 19:22:32] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:22:32] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-27 19:22:32] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:22:32] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:22:32] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:22:32] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:22:32] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:22:32] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:22:32] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 19:22:32] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 19:22:32] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 19:22:32] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 19:22:32] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 19:22:35] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 19:22:35] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:22:35] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-27 19:22:35] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:22:35] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:22:35] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:22:35] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:22:35] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:22:35] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:22:35] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 19:22:35] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 19:22:35] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 19:22:35] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 19:22:35] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2989442) [2026-01-27 19:22:37] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2989442) [2026-01-27 19:22:37] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2989442) [2026-01-27 19:22:37] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2989442) [2026-01-27 19:22:37] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=2989442) [2026-01-27 19:22:37] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=2989442) [2026-01-27 19:22:37] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2989442) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2989442) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:08<00:25,  8.50s/it]
(EngineCore_DP0 pid=2989442) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:43<00:48, 24.26s/it]
(EngineCore_DP0 pid=2989442) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [01:15<00:27, 27.76s/it]
(EngineCore_DP0 pid=2989442) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:49<00:00, 30.06s/it]
(EngineCore_DP0 pid=2989442) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:49<00:00, 27.32s/it]
(EngineCore_DP0 pid=2989442) 
(EngineCore_DP0 pid=2989442) [2026-01-27 19:24:27] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [7168, 6848] -> 1D uint8
(EngineCore_DP0 pid=2989442) [2026-01-27 19:24:27] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 36929536 bytes
(EngineCore_DP0 pid=2989442) [2026-01-27 19:24:27] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 6848] -> 1D uint8
(EngineCore_DP0 pid=2989442) [2026-01-27 19:24:27] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 26378240 bytes
(EngineCore_DP0 pid=2989442) [2026-01-27 19:24:27] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [27648, 6848] -> 1D uint8
(EngineCore_DP0 pid=2989442) [2026-01-27 19:24:28] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 142442496 bytes
(EngineCore_DP0 pid=2989442) [2026-01-27 19:24:28] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 18432] -> 1D uint8
(EngineCore_DP0 pid=2989442) [2026-01-27 19:24:28] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 70778880 bytes
(EngineCore_DP0 pid=2989442) 2026-01-27 19:24:37,832 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2989442) 2026-01-27 19:24:37,906 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:   1%|          | 1/128 [00:00<00:20,  6.18it/s]
Adding requests:   2%|▏         | 3/128 [00:00<00:10, 11.82it/s]
Adding requests:   5%|▍         | 6/128 [00:00<00:06, 18.52it/s]
Adding requests:   8%|▊         | 10/128 [00:00<00:04, 26.03it/s]
Adding requests:  12%|█▏        | 15/128 [00:00<00:03, 32.97it/s]
Adding requests:  20%|█▉        | 25/128 [00:00<00:01, 52.14it/s]
Adding requests:  29%|██▉       | 37/128 [00:00<00:01, 70.69it/s]
Adding requests:  41%|████      | 52/128 [00:00<00:00, 93.79it/s]
Adding requests:  56%|█████▋    | 72/128 [00:01<00:00, 123.17it/s]
Adding requests:  77%|███████▋  | 99/128 [00:01<00:00, 166.23it/s]
Adding requests: 100%|██████████| 128/128 [00:01<00:00, 105.95it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:29<1:02:23, 29.48s/it, est. speed input: 0.54 toks/s, output: 8.68 toks/s]
Processed prompts:   2%|▏         | 2/128 [00:29<25:37, 12.20s/it, est. speed input: 1.08 toks/s, output: 17.30 toks/s] 
Processed prompts:   5%|▌         | 7/128 [00:29<04:46,  2.37s/it, est. speed input: 3.77 toks/s, output: 60.28 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:29<01:14,  1.48it/s, est. speed input: 9.64 toks/s, output: 154.32 toks/s]
Processed prompts:  23%|██▎       | 30/128 [00:29<00:31,  3.07it/s, est. speed input: 16.01 toks/s, output: 256.17 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:30<00:14,  5.84it/s, est. speed input: 23.93 toks/s, output: 382.81 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:30<00:07,  9.31it/s, est. speed input: 31.24 toks/s, output: 499.85 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:30<00:04, 13.47it/s, est. speed input: 37.95 toks/s, output: 607.21 toks/s]
Processed prompts:  66%|██████▌   | 84/128 [00:30<00:02, 18.42it/s, est. speed input: 44.10 toks/s, output: 705.59 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:30<00:00, 29.58it/s, est. speed input: 55.30 toks/s, output: 884.84 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:30<00:00, 39.88it/s, est. speed input: 64.84 toks/s, output: 1037.45 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:30<00:00, 39.88it/s, est. speed input: 66.40 toks/s, output: 1062.35 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:30<00:00,  4.15it/s, est. speed input: 66.40 toks/s, output: 1062.35 toks/s]
[rank0]:[W127 19:25:11.781940006 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 180.6s

测试结果:
  Requests/s:   3.98
  Tokens/s:     1083.53
  Total Reqs:   128
  Elapsed:      32.13s

  [Decode 分析]
  Total Decode Tokens:  32768
  Decode Tokens/s:      1019.79

============================================================
[3/4] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-14B-INT8                                │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 4096 (= 256 x 16)
│   M_decode      = 256
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 256
│   --max-num-seqs           = 256
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 19:25:32 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 19:25:32 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2992633) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2992633) WARNING 01-27 19:27:39 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 5.38 requests/s, 1464.03 total tokens/s, 1377.91 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536


─── STDERR ───
[2026-01-27 19:25:32] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 19:25:32] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:25:32] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-27 19:25:32] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:25:32] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:25:32] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:25:32] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:25:32] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:25:32] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:25:32] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 19:25:32] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 19:25:32] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 19:25:32] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 19:25:32] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 19:25:36] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 19:25:36] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:25:36] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-27 19:25:36] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:25:36] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:25:36] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:25:36] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:25:36] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:25:36] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:25:36] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 19:25:36] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 19:25:36] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 19:25:36] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 19:25:36] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2992633) [2026-01-27 19:25:37] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2992633) [2026-01-27 19:25:37] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2992633) [2026-01-27 19:25:37] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2992633) [2026-01-27 19:25:37] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=2992633) [2026-01-27 19:25:37] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=2992633) [2026-01-27 19:25:37] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2992633) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2992633) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:08<00:25,  8.61s/it]
(EngineCore_DP0 pid=2992633) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:43<00:48, 24.05s/it]
(EngineCore_DP0 pid=2992633) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [01:15<00:27, 27.80s/it]
(EngineCore_DP0 pid=2992633) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:49<00:00, 30.04s/it]
(EngineCore_DP0 pid=2992633) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:49<00:00, 27.30s/it]
(EngineCore_DP0 pid=2992633) 
(EngineCore_DP0 pid=2992633) [2026-01-27 19:27:27] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [7168, 6848] -> 1D uint8
(EngineCore_DP0 pid=2992633) [2026-01-27 19:27:28] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 36929536 bytes
(EngineCore_DP0 pid=2992633) [2026-01-27 19:27:28] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 6848] -> 1D uint8
(EngineCore_DP0 pid=2992633) [2026-01-27 19:27:28] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 26378240 bytes
(EngineCore_DP0 pid=2992633) [2026-01-27 19:27:28] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [27648, 6848] -> 1D uint8
(EngineCore_DP0 pid=2992633) [2026-01-27 19:27:28] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 142442496 bytes
(EngineCore_DP0 pid=2992633) [2026-01-27 19:27:28] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 18432] -> 1D uint8
(EngineCore_DP0 pid=2992633) [2026-01-27 19:27:28] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 70778880 bytes
(EngineCore_DP0 pid=2992633) 2026-01-27 19:27:37,246 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2992633) 2026-01-27 19:27:37,279 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   0%|          | 1/256 [00:00<00:43,  5.80it/s]
Adding requests:   1%|          | 3/256 [00:00<00:22, 11.46it/s]
Adding requests:   2%|▏         | 6/256 [00:00<00:13, 18.30it/s]
Adding requests:   4%|▍         | 10/256 [00:00<00:09, 24.72it/s]
Adding requests:   5%|▌         | 14/256 [00:00<00:08, 28.58it/s]
Adding requests:   9%|▊         | 22/256 [00:00<00:05, 43.94it/s]
Adding requests:  13%|█▎        | 33/256 [00:00<00:03, 63.20it/s]
Adding requests:  18%|█▊        | 46/256 [00:00<00:02, 82.88it/s]
Adding requests:  25%|██▌       | 65/256 [00:01<00:01, 113.21it/s]
Adding requests:  32%|███▏      | 83/256 [00:01<00:01, 86.97it/s] 
Adding requests:  43%|████▎     | 109/256 [00:01<00:01, 123.96it/s]
Adding requests:  53%|█████▎    | 136/256 [00:01<00:00, 157.03it/s]
Adding requests:  64%|██████▍   | 164/256 [00:01<00:00, 186.35it/s]
Adding requests:  80%|███████▉  | 204/256 [00:01<00:00, 240.61it/s]
Adding requests:  98%|█████████▊| 252/256 [00:01<00:00, 305.27it/s]
Adding requests: 100%|██████████| 256/256 [00:01<00:00, 139.86it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:39<2:46:53, 39.27s/it, est. speed input: 0.41 toks/s, output: 6.52 toks/s]
Processed prompts:   1%|          | 2/256 [00:39<1:08:51, 16.26s/it, est. speed input: 0.81 toks/s, output: 12.98 toks/s]
Processed prompts:   3%|▎         | 8/256 [00:39<11:13,  2.72s/it, est. speed input: 3.23 toks/s, output: 51.70 toks/s]  
Processed prompts:   5%|▍         | 12/256 [00:39<06:14,  1.53s/it, est. speed input: 4.83 toks/s, output: 77.22 toks/s]
Processed prompts:   7%|▋         | 18/256 [00:39<03:10,  1.25it/s, est. speed input: 7.20 toks/s, output: 115.26 toks/s]
Processed prompts:  11%|█         | 27/256 [00:40<01:32,  2.49it/s, est. speed input: 10.76 toks/s, output: 172.16 toks/s]
Processed prompts:  16%|█▌        | 41/256 [00:40<00:42,  5.12it/s, est. speed input: 16.27 toks/s, output: 260.24 toks/s]
Processed prompts:  21%|██▏       | 55/256 [00:40<00:23,  8.59it/s, est. speed input: 21.72 toks/s, output: 347.60 toks/s]
Processed prompts:  27%|██▋       | 68/256 [00:40<00:14, 12.70it/s, est. speed input: 26.75 toks/s, output: 427.99 toks/s]
Processed prompts:  31%|███▏      | 80/256 [00:40<00:10, 17.46it/s, est. speed input: 31.35 toks/s, output: 501.66 toks/s]
Processed prompts:  36%|███▌      | 92/256 [00:40<00:07, 23.19it/s, est. speed input: 35.92 toks/s, output: 574.79 toks/s]
Processed prompts:  40%|████      | 103/256 [00:41<00:05, 28.86it/s, est. speed input: 40.07 toks/s, output: 641.07 toks/s]
Processed prompts:  44%|████▍     | 113/256 [00:41<00:04, 33.86it/s, est. speed input: 43.79 toks/s, output: 700.56 toks/s]
Processed prompts:  48%|████▊     | 123/256 [00:41<00:03, 40.13it/s, est. speed input: 47.51 toks/s, output: 760.13 toks/s]
Processed prompts:  52%|█████▏    | 132/256 [00:41<00:02, 43.77it/s, est. speed input: 50.80 toks/s, output: 812.72 toks/s]
Processed prompts:  55%|█████▍    | 140/256 [00:41<00:02, 48.17it/s, est. speed input: 53.72 toks/s, output: 859.56 toks/s]
Processed prompts:  58%|█████▊    | 148/256 [00:41<00:02, 51.83it/s, est. speed input: 56.63 toks/s, output: 906.03 toks/s]
Processed prompts:  61%|██████    | 155/256 [00:41<00:01, 53.00it/s, est. speed input: 59.13 toks/s, output: 946.10 toks/s]
Processed prompts:  63%|██████▎   | 162/256 [00:42<00:01, 52.46it/s, est. speed input: 61.60 toks/s, output: 985.60 toks/s]
Processed prompts:  66%|██████▌   | 169/256 [00:42<00:01, 54.45it/s, est. speed input: 64.09 toks/s, output: 1025.37 toks/s]
Processed prompts:  69%|██████▉   | 176/256 [00:42<00:01, 56.35it/s, est. speed input: 66.56 toks/s, output: 1064.99 toks/s]
Processed prompts:  71%|███████▏  | 183/256 [00:42<00:01, 58.10it/s, est. speed input: 69.03 toks/s, output: 1104.45 toks/s]
Processed prompts:  74%|███████▍  | 190/256 [00:42<00:01, 47.14it/s, est. speed input: 71.30 toks/s, output: 1140.86 toks/s]
Processed prompts:  77%|███████▋  | 196/256 [00:42<00:01, 46.12it/s, est. speed input: 73.32 toks/s, output: 1173.08 toks/s]
Processed prompts:  79%|███████▊  | 201/256 [00:42<00:01, 46.41it/s, est. speed input: 75.00 toks/s, output: 1200.05 toks/s]
Processed prompts:  80%|████████  | 206/256 [00:42<00:01, 46.81it/s, est. speed input: 76.68 toks/s, output: 1226.92 toks/s]
Processed prompts:  82%|████████▏ | 211/256 [00:43<00:01, 36.49it/s, est. speed input: 78.14 toks/s, output: 1250.29 toks/s]
Processed prompts:  84%|████████▍ | 216/256 [00:43<00:01, 39.38it/s, est. speed input: 79.81 toks/s, output: 1276.95 toks/s]
Processed prompts:  86%|████████▋ | 221/256 [00:43<00:01, 34.37it/s, est. speed input: 81.30 toks/s, output: 1300.73 toks/s]
Processed prompts:  88%|████████▊ | 226/256 [00:43<00:00, 30.83it/s, est. speed input: 82.75 toks/s, output: 1323.96 toks/s]
Processed prompts:  90%|████████▉ | 230/256 [00:43<00:00, 27.91it/s, est. speed input: 83.86 toks/s, output: 1341.72 toks/s]
Processed prompts:  91%|█████████▏| 234/256 [00:44<00:00, 25.96it/s, est. speed input: 84.96 toks/s, output: 1359.33 toks/s]
Processed prompts:  93%|█████████▎| 238/256 [00:44<00:00, 24.73it/s, est. speed input: 86.05 toks/s, output: 1376.88 toks/s]
Processed prompts:  95%|█████████▍| 242/256 [00:44<00:00, 24.20it/s, est. speed input: 87.16 toks/s, output: 1394.52 toks/s]
Processed prompts:  96%|█████████▌| 245/256 [00:44<00:00, 19.34it/s, est. speed input: 87.73 toks/s, output: 1403.62 toks/s]
Processed prompts:  97%|█████████▋| 248/256 [00:44<00:00, 16.71it/s, est. speed input: 88.30 toks/s, output: 1412.79 toks/s]
Processed prompts:  98%|█████████▊| 250/256 [00:45<00:00, 15.50it/s, est. speed input: 88.68 toks/s, output: 1418.85 toks/s]
Processed prompts:  98%|█████████▊| 252/256 [00:45<00:00, 14.46it/s, est. speed input: 89.05 toks/s, output: 1424.77 toks/s]
Processed prompts:  99%|█████████▉| 254/256 [00:45<00:00, 13.85it/s, est. speed input: 89.43 toks/s, output: 1430.87 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:45<00:00, 12.60it/s, est. speed input: 89.74 toks/s, output: 1435.78 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:45<00:00, 12.60it/s, est. speed input: 89.74 toks/s, output: 1435.78 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:45<00:00,  5.61it/s, est. speed input: 89.74 toks/s, output: 1435.78 toks/s]
[rank0]:[W127 19:28:27.844541297 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 197.1s

测试结果:
  Requests/s:   5.38
  Tokens/s:     1464.03
  Total Reqs:   256
  Elapsed:      47.56s

  [Decode 分析]
  Total Decode Tokens:  65536
  Decode Tokens/s:      1377.91

============================================================
[4/4] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-14B-INT8                                │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 8192 (= 512 x 16)
│   M_decode      = 512
│   batched_tokens = 512 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 512
│   --max-num-seqs           = 512
│   --max-model-len          = 272
│   --max-num-batched-tokens = 512
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 19:28:49 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 19:28:49 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=2996099) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2996099) WARNING 01-27 19:30:55 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 5.48 requests/s, 1491.54 total tokens/s, 1403.81 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072


─── STDERR ───
[2026-01-27 19:28:48] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 19:28:49] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:28:49] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-27 19:28:49] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:28:49] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:28:49] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:28:49] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:28:49] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:28:49] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:28:49] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 19:28:49] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 19:28:49] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 19:28:49] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 19:28:49] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 19:28:52] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 19:28:52] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:28:52] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-27 19:28:52] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:28:52] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:28:52] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:28:52] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:28:52] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:28:52] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:28:52] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 19:28:52] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 19:28:52] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 19:28:52] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 19:28:52] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=2996099) [2026-01-27 19:28:53] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=2996099) [2026-01-27 19:28:53] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=2996099) [2026-01-27 19:28:53] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=2996099) [2026-01-27 19:28:53] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=2996099) [2026-01-27 19:28:53] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=2996099) [2026-01-27 19:28:53] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=2996099) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=2996099) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:08<00:25,  8.48s/it]
(EngineCore_DP0 pid=2996099) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:43<00:48, 24.15s/it]
(EngineCore_DP0 pid=2996099) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [01:15<00:27, 27.75s/it]
(EngineCore_DP0 pid=2996099) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:49<00:00, 30.21s/it]
(EngineCore_DP0 pid=2996099) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:49<00:00, 27.40s/it]
(EngineCore_DP0 pid=2996099) 
(EngineCore_DP0 pid=2996099) [2026-01-27 19:30:44] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [7168, 6848] -> 1D uint8
(EngineCore_DP0 pid=2996099) [2026-01-27 19:30:44] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 36929536 bytes
(EngineCore_DP0 pid=2996099) [2026-01-27 19:30:44] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 6848] -> 1D uint8
(EngineCore_DP0 pid=2996099) [2026-01-27 19:30:44] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 26378240 bytes
(EngineCore_DP0 pid=2996099) [2026-01-27 19:30:44] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [27648, 6848] -> 1D uint8
(EngineCore_DP0 pid=2996099) [2026-01-27 19:30:44] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 142442496 bytes
(EngineCore_DP0 pid=2996099) [2026-01-27 19:30:44] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 18432] -> 1D uint8
(EngineCore_DP0 pid=2996099) [2026-01-27 19:30:44] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 70778880 bytes
(EngineCore_DP0 pid=2996099) 2026-01-27 19:30:54,192 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=2996099) 2026-01-27 19:30:54,272 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   0%|          | 1/512 [00:00<01:24,  6.07it/s]
Adding requests:   1%|          | 3/512 [00:00<00:45, 11.31it/s]
Adding requests:   1%|          | 5/512 [00:00<00:35, 14.21it/s]
Adding requests:   2%|▏         | 9/512 [00:00<00:23, 21.02it/s]
Adding requests:   3%|▎         | 14/512 [00:00<00:18, 27.48it/s]
Adding requests:   4%|▍         | 20/512 [00:00<00:13, 36.84it/s]
Adding requests:   6%|▌         | 31/512 [00:00<00:08, 56.86it/s]
Adding requests:   9%|▉         | 45/512 [00:00<00:05, 79.92it/s]
Adding requests:  12%|█▏        | 63/512 [00:01<00:04, 108.58it/s]
Adding requests:  17%|█▋        | 86/512 [00:01<00:02, 142.35it/s]
Adding requests:  21%|██        | 108/512 [00:01<00:02, 162.82it/s]
Adding requests:  26%|██▌       | 131/512 [00:01<00:02, 182.15it/s]
Adding requests:  31%|███       | 159/512 [00:01<00:01, 209.07it/s]
Adding requests:  38%|███▊      | 193/512 [00:01<00:01, 247.02it/s]
Adding requests:  45%|████▍     | 230/512 [00:01<00:00, 282.81it/s]
Adding requests:  54%|█████▍    | 276/512 [00:01<00:00, 335.13it/s]
Adding requests:  64%|██████▍   | 329/512 [00:01<00:00, 392.25it/s]
Adding requests:  84%|████████▍ | 430/512 [00:01<00:00, 574.43it/s]
Adding requests: 100%|██████████| 512/512 [00:02<00:00, 249.49it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [01:01<8:41:22, 61.22s/it, est. speed input: 0.26 toks/s, output: 4.18 toks/s]
Processed prompts:   0%|          | 2/512 [01:01<3:37:49, 25.63s/it, est. speed input: 0.52 toks/s, output: 8.27 toks/s]
Processed prompts:   1%|▏         | 7/512 [01:02<42:26,  5.04s/it, est. speed input: 1.79 toks/s, output: 28.60 toks/s] 
Processed prompts:   2%|▏         | 11/512 [01:03<22:36,  2.71s/it, est. speed input: 2.78 toks/s, output: 44.45 toks/s]
Processed prompts:   3%|▎         | 16/512 [01:04<12:35,  1.52s/it, est. speed input: 4.00 toks/s, output: 63.94 toks/s]
Processed prompts:   4%|▍         | 22/512 [01:04<07:21,  1.11it/s, est. speed input: 5.44 toks/s, output: 86.96 toks/s]
Processed prompts:   7%|▋         | 34/512 [01:05<03:25,  2.32it/s, est. speed input: 8.31 toks/s, output: 132.98 toks/s]
Processed prompts:  11%|█         | 54/512 [01:06<01:33,  4.90it/s, est. speed input: 13.07 toks/s, output: 209.08 toks/s]
Processed prompts:  16%|█▌        | 82/512 [01:06<00:46,  9.29it/s, est. speed input: 19.65 toks/s, output: 314.40 toks/s]
Processed prompts:  21%|██        | 108/512 [01:07<00:27, 14.85it/s, est. speed input: 25.78 toks/s, output: 412.41 toks/s]
Processed prompts:  26%|██▌       | 133/512 [01:07<00:17, 21.38it/s, est. speed input: 31.62 toks/s, output: 505.88 toks/s]
Processed prompts:  30%|███       | 156/512 [01:07<00:12, 28.65it/s, est. speed input: 36.96 toks/s, output: 591.34 toks/s]
Processed prompts:  35%|███▍      | 178/512 [01:07<00:09, 36.36it/s, est. speed input: 42.02 toks/s, output: 672.39 toks/s]
Processed prompts:  39%|███▊      | 198/512 [01:07<00:07, 43.64it/s, est. speed input: 46.59 toks/s, output: 745.44 toks/s]
Processed prompts:  42%|████▏     | 217/512 [01:08<00:05, 50.45it/s, est. speed input: 50.89 toks/s, output: 814.28 toks/s]
Processed prompts:  46%|████▌     | 235/512 [01:08<00:04, 56.63it/s, est. speed input: 54.94 toks/s, output: 879.06 toks/s]
Processed prompts:  49%|████▉     | 252/512 [01:08<00:04, 61.81it/s, est. speed input: 58.74 toks/s, output: 939.82 toks/s]
Processed prompts:  52%|█████▏    | 268/512 [01:08<00:03, 66.56it/s, est. speed input: 62.30 toks/s, output: 996.73 toks/s]
Processed prompts:  55%|█████▌    | 283/512 [01:09<00:03, 69.79it/s, est. speed input: 65.61 toks/s, output: 1049.69 toks/s]
Processed prompts:  58%|█████▊    | 297/512 [01:09<00:02, 73.47it/s, est. speed input: 68.69 toks/s, output: 1099.04 toks/s]
Processed prompts:  61%|██████    | 310/512 [01:09<00:02, 73.19it/s, est. speed input: 71.51 toks/s, output: 1144.18 toks/s]
Processed prompts:  63%|██████▎   | 322/512 [01:09<00:02, 72.85it/s, est. speed input: 74.10 toks/s, output: 1185.61 toks/s]
Processed prompts:  65%|██████▌   | 333/512 [01:09<00:02, 73.16it/s, est. speed input: 76.47 toks/s, output: 1223.50 toks/s]
Processed prompts:  67%|██████▋   | 344/512 [01:09<00:02, 72.13it/s, est. speed input: 78.82 toks/s, output: 1261.05 toks/s]
Processed prompts:  69%|██████▉   | 354/512 [01:09<00:02, 70.04it/s, est. speed input: 80.93 toks/s, output: 1294.83 toks/s]
Processed prompts:  71%|███████   | 363/512 [01:10<00:02, 68.90it/s, est. speed input: 82.82 toks/s, output: 1325.16 toks/s]
Processed prompts:  73%|███████▎  | 372/512 [01:10<00:02, 66.63it/s, est. speed input: 84.70 toks/s, output: 1355.16 toks/s]
Processed prompts:  74%|███████▍  | 380/512 [01:10<00:02, 65.20it/s, est. speed input: 86.36 toks/s, output: 1381.73 toks/s]
Processed prompts:  76%|███████▌  | 388/512 [01:10<00:02, 61.80it/s, est. speed input: 87.99 toks/s, output: 1407.85 toks/s]
Processed prompts:  77%|███████▋  | 395/512 [01:10<00:01, 60.38it/s, est. speed input: 89.42 toks/s, output: 1430.74 toks/s]
Processed prompts:  79%|███████▊  | 402/512 [01:10<00:01, 59.64it/s, est. speed input: 90.85 toks/s, output: 1453.59 toks/s]
Processed prompts:  80%|███████▉  | 408/512 [01:10<00:01, 57.10it/s, est. speed input: 92.05 toks/s, output: 1472.81 toks/s]
Processed prompts:  81%|████████  | 414/512 [01:11<00:01, 55.44it/s, est. speed input: 93.25 toks/s, output: 1492.00 toks/s]
Processed prompts:  82%|████████▏ | 420/512 [01:11<00:01, 53.36it/s, est. speed input: 94.44 toks/s, output: 1510.99 toks/s]
Processed prompts:  83%|████████▎ | 426/512 [01:11<00:01, 54.81it/s, est. speed input: 95.65 toks/s, output: 1530.39 toks/s]
Processed prompts:  84%|████████▍ | 432/512 [01:11<00:01, 54.73it/s, est. speed input: 96.85 toks/s, output: 1549.55 toks/s]
Processed prompts:  86%|████████▌ | 438/512 [01:11<00:01, 55.14it/s, est. speed input: 98.05 toks/s, output: 1568.72 toks/s]
Processed prompts:  87%|████████▋ | 444/512 [01:11<00:01, 42.44it/s, est. speed input: 99.08 toks/s, output: 1585.32 toks/s]
Processed prompts:  88%|████████▊ | 449/512 [01:11<00:01, 43.79it/s, est. speed input: 100.05 toks/s, output: 1600.86 toks/s]
Processed prompts:  89%|████████▊ | 454/512 [01:12<00:01, 34.05it/s, est. speed input: 100.84 toks/s, output: 1613.39 toks/s]
Processed prompts:  90%|████████▉ | 460/512 [01:12<00:01, 33.14it/s, est. speed input: 101.90 toks/s, output: 1630.38 toks/s]
Processed prompts:  91%|█████████ | 466/512 [01:12<00:01, 31.56it/s, est. speed input: 102.93 toks/s, output: 1646.85 toks/s]
Processed prompts:  92%|█████████▏| 470/512 [01:12<00:01, 27.91it/s, est. speed input: 103.53 toks/s, output: 1656.43 toks/s]
Processed prompts:  93%|█████████▎| 474/512 [01:12<00:01, 25.87it/s, est. speed input: 104.14 toks/s, output: 1666.19 toks/s]
Processed prompts:  93%|█████████▎| 478/512 [01:13<00:01, 24.58it/s, est. speed input: 104.75 toks/s, output: 1675.97 toks/s]
Processed prompts:  94%|█████████▍| 482/512 [01:13<00:01, 23.22it/s, est. speed input: 105.34 toks/s, output: 1685.43 toks/s]
Processed prompts:  95%|█████████▍| 485/512 [01:13<00:01, 18.62it/s, est. speed input: 105.60 toks/s, output: 1689.66 toks/s]
Processed prompts:  95%|█████████▌| 488/512 [01:13<00:01, 15.94it/s, est. speed input: 105.86 toks/s, output: 1693.83 toks/s]
Processed prompts:  96%|█████████▌| 490/512 [01:13<00:01, 14.81it/s, est. speed input: 106.04 toks/s, output: 1696.72 toks/s]
Processed prompts:  96%|█████████▌| 492/512 [01:14<00:01, 13.95it/s, est. speed input: 106.23 toks/s, output: 1699.63 toks/s]
Processed prompts:  96%|█████████▋| 494/512 [01:14<00:01, 13.29it/s, est. speed input: 106.41 toks/s, output: 1702.54 toks/s]
Processed prompts:  97%|█████████▋| 496/512 [01:14<00:01, 12.84it/s, est. speed input: 106.59 toks/s, output: 1705.50 toks/s]
Processed prompts:  97%|█████████▋| 497/512 [01:27<00:01, 12.84it/s, est. speed input: 106.69 toks/s, output: 1707.02 toks/s]
Processed prompts:  97%|█████████▋| 498/512 [01:30<00:30,  2.16s/it, est. speed input: 88.11 toks/s, output: 1409.73 toks/s] 
Processed prompts:  98%|█████████▊| 500/512 [01:30<00:19,  1.59s/it, est. speed input: 88.30 toks/s, output: 1412.72 toks/s]
Processed prompts:  98%|█████████▊| 502/512 [01:30<00:11,  1.17s/it, est. speed input: 88.48 toks/s, output: 1415.66 toks/s]
Processed prompts:  99%|█████████▉| 506/512 [01:30<00:04,  1.50it/s, est. speed input: 89.02 toks/s, output: 1424.34 toks/s]
Processed prompts: 100%|██████████| 512/512 [01:31<00:00,  1.50it/s, est. speed input: 89.99 toks/s, output: 1439.89 toks/s]
Processed prompts: 100%|██████████| 512/512 [01:31<00:00,  5.62it/s, est. speed input: 89.99 toks/s, output: 1439.89 toks/s]
[rank0]:[W127 19:32:29.497098329 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 240.6s

测试结果:
  Requests/s:   5.48
  Tokens/s:     1491.54
  Total Reqs:   512
  Elapsed:      93.37s

  [Decode 分析]
  Total Decode Tokens:  131072
  Decode Tokens/s:      1403.81


------------------------------------------------------------
  生成 CSV: Qwen2.5-14B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_6/Qwen2.5-14B-INT8_decode.csv

预览:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
64,16,64,64,256,256,2.3717,645.1052,26.9847
128,16,128,128,256,256,3.9836,1083.5283,32.1321
256,16,256,256,256,256,5.3825,1464.0266,47.5620
512,16,512,512,256,256,5.4836,1491.5435,93.3690

------------------------------------------------------------

[INFO] 完成: 4 成功, 0 失败

============================================================
  Qwen2.5-14B-INT8 | cuSPARSELt (2_8) | decode
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_8
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_8

============================================================
[1/4] 测试 M=64
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-14B-INT8                                │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 64
│   M_prefill     = 1024 (= 64 x 16)
│   M_decode      = 64
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 64
│   --max-num-seqs           = 64
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 19:32:50 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 19:32:50 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3000363) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3000363) WARNING 01-27 19:35:15 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 2.42 requests/s, 658.85 total tokens/s, 620.09 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384


─── STDERR ───
[2026-01-27 19:32:50] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 19:32:50] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:32:50] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-27 19:32:50] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:32:50] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:32:50] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:32:50] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:32:50] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:32:50] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:32:50] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 19:32:50] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 19:32:50] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 19:32:50] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 19:32:50] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 19:32:53] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 19:32:54] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:32:54] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-27 19:32:54] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:32:54] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:32:54] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:32:54] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:32:54] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:32:54] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:32:54] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 19:32:54] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 19:32:54] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 19:32:54] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 19:32:54] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3000363) [2026-01-27 19:32:55] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3000363) [2026-01-27 19:32:55] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3000363) [2026-01-27 19:32:55] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3000363) [2026-01-27 19:32:55] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3000363) [2026-01-27 19:32:55] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=3000363) [2026-01-27 19:32:55] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3000363) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3000363) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:08<00:25,  8.67s/it]
(EngineCore_DP0 pid=3000363) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:47<00:52, 26.13s/it]
(EngineCore_DP0 pid=3000363) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [01:18<00:28, 28.43s/it]
(EngineCore_DP0 pid=3000363) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:58<00:00, 33.26s/it]
(EngineCore_DP0 pid=3000363) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:58<00:00, 29.71s/it]
(EngineCore_DP0 pid=3000363) 
(EngineCore_DP0 pid=3000363) [2026-01-27 19:34:55] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [7168, 7680] -> 1D uint8
(EngineCore_DP0 pid=3000363) [2026-01-27 19:34:55] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 41287680 bytes
(EngineCore_DP0 pid=3000363) [2026-01-27 19:34:55] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 7680] -> 1D uint8
(EngineCore_DP0 pid=3000363) [2026-01-27 19:34:55] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 29491200 bytes
(EngineCore_DP0 pid=3000363) [2026-01-27 19:34:55] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [27648, 7680] -> 1D uint8
(EngineCore_DP0 pid=3000363) [2026-01-27 19:34:55] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 159252480 bytes
(EngineCore_DP0 pid=3000363) [2026-01-27 19:34:55] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 20736] -> 1D uint8
(EngineCore_DP0 pid=3000363) [2026-01-27 19:34:55] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 79626240 bytes
(EngineCore_DP0 pid=3000363) 2026-01-27 19:35:07,044 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3000363) 2026-01-27 19:35:07,385 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 1/64 [00:00<00:13,  4.51it/s]
Adding requests:   5%|▍         | 3/64 [00:00<00:06,  9.18it/s]
Adding requests:   9%|▉         | 6/64 [00:00<00:03, 15.58it/s]
Adding requests:  16%|█▌        | 10/64 [00:00<00:02, 22.11it/s]
Adding requests:  22%|██▏       | 14/64 [00:00<00:01, 25.20it/s]
Adding requests:  33%|███▎      | 21/64 [00:00<00:01, 36.89it/s]
Adding requests:  52%|█████▏    | 33/64 [00:00<00:00, 60.29it/s]
Adding requests:  73%|███████▎  | 47/64 [00:01<00:00, 82.30it/s]
Adding requests: 100%|██████████| 64/64 [00:01<00:00, 57.10it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 1/64 [00:24<25:46, 24.56s/it, est. speed input: 0.65 toks/s, output: 10.43 toks/s]
Processed prompts:   3%|▎         | 2/64 [00:24<10:30, 10.17s/it, est. speed input: 1.30 toks/s, output: 20.76 toks/s]
Processed prompts:   9%|▉         | 6/64 [00:24<02:16,  2.35s/it, est. speed input: 3.88 toks/s, output: 62.03 toks/s]
Processed prompts:  34%|███▍      | 22/64 [00:24<00:18,  2.27it/s, est. speed input: 14.15 toks/s, output: 226.45 toks/s]
Processed prompts:  58%|█████▊    | 37/64 [00:24<00:05,  4.66it/s, est. speed input: 23.70 toks/s, output: 379.17 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:25<00:00, 10.47it/s, est. speed input: 40.69 toks/s, output: 651.07 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:25<00:00, 10.47it/s, est. speed input: 40.69 toks/s, output: 651.07 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:25<00:00,  2.54it/s, est. speed input: 40.69 toks/s, output: 651.07 toks/s]
[rank0]:[W127 19:35:42.802230570 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 195.8s

测试结果:
  Requests/s:   2.42
  Tokens/s:     658.85
  Total Reqs:   64
  Elapsed:      26.42s

  [Decode 分析]
  Total Decode Tokens:  16384
  Decode Tokens/s:      620.09

============================================================
[2/4] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-14B-INT8                                │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 2048 (= 128 x 16)
│   M_decode      = 128
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 128
│   --max-num-seqs           = 128
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 19:36:05 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 19:36:05 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3003332) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3003332) WARNING 01-27 19:38:28 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 4.08 requests/s, 1109.78 total tokens/s, 1044.50 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768


─── STDERR ───
[2026-01-27 19:36:05] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 19:36:05] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:36:05] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-27 19:36:05] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:36:05] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:36:05] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:36:05] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:36:05] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:36:05] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:36:05] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 19:36:05] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 19:36:05] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 19:36:05] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 19:36:05] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 19:36:08] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 19:36:08] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:36:08] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-27 19:36:08] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:36:08] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:36:08] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:36:08] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:36:08] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:36:08] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:36:08] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 19:36:08] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 19:36:08] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 19:36:08] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 19:36:08] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3003332) [2026-01-27 19:36:09] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3003332) [2026-01-27 19:36:09] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3003332) [2026-01-27 19:36:09] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3003332) [2026-01-27 19:36:09] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3003332) [2026-01-27 19:36:09] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=3003332) [2026-01-27 19:36:09] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3003332) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3003332) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:08<00:25,  8.45s/it]
(EngineCore_DP0 pid=3003332) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:48<00:53, 26.96s/it]
(EngineCore_DP0 pid=3003332) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [01:20<00:29, 29.16s/it]
(EngineCore_DP0 pid=3003332) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [02:01<00:00, 33.97s/it]
(EngineCore_DP0 pid=3003332) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [02:01<00:00, 30.37s/it]
(EngineCore_DP0 pid=3003332) 
(EngineCore_DP0 pid=3003332) [2026-01-27 19:38:12] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [7168, 7680] -> 1D uint8
(EngineCore_DP0 pid=3003332) [2026-01-27 19:38:12] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 41287680 bytes
(EngineCore_DP0 pid=3003332) [2026-01-27 19:38:12] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 7680] -> 1D uint8
(EngineCore_DP0 pid=3003332) [2026-01-27 19:38:12] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 29491200 bytes
(EngineCore_DP0 pid=3003332) [2026-01-27 19:38:12] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [27648, 7680] -> 1D uint8
(EngineCore_DP0 pid=3003332) [2026-01-27 19:38:12] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 159252480 bytes
(EngineCore_DP0 pid=3003332) [2026-01-27 19:38:12] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 20736] -> 1D uint8
(EngineCore_DP0 pid=3003332) [2026-01-27 19:38:12] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 79626240 bytes
(EngineCore_DP0 pid=3003332) 2026-01-27 19:38:23,160 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3003332) 2026-01-27 19:38:23,377 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:   1%|          | 1/128 [00:00<00:28,  4.45it/s]
Adding requests:   2%|▏         | 3/128 [00:00<00:13,  9.42it/s]
Adding requests:   5%|▍         | 6/128 [00:00<00:08, 14.68it/s]
Adding requests:   8%|▊         | 10/128 [00:00<00:05, 21.29it/s]
Adding requests:  11%|█         | 14/128 [00:00<00:04, 25.86it/s]
Adding requests:  16%|█▋        | 21/128 [00:00<00:02, 37.91it/s]
Adding requests:  25%|██▌       | 32/128 [00:00<00:01, 56.93it/s]
Adding requests:  34%|███▍      | 44/128 [00:01<00:01, 73.91it/s]
Adding requests:  48%|████▊     | 61/128 [00:01<00:00, 100.35it/s]
Adding requests:  62%|██████▎   | 80/128 [00:01<00:00, 124.82it/s]
Adding requests:  80%|███████▉  | 102/128 [00:01<00:00, 151.73it/s]
Adding requests: 100%|██████████| 128/128 [00:01<00:00, 89.36it/s] 

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:28<59:52, 28.29s/it, est. speed input: 0.57 toks/s, output: 9.05 toks/s]
Processed prompts:   2%|▏         | 2/128 [00:28<24:36, 11.72s/it, est. speed input: 1.13 toks/s, output: 18.02 toks/s]
Processed prompts:   5%|▌         | 7/128 [00:28<04:35,  2.27s/it, est. speed input: 3.92 toks/s, output: 62.79 toks/s]
Processed prompts:   9%|▊         | 11/128 [00:28<02:20,  1.20s/it, est. speed input: 6.14 toks/s, output: 98.26 toks/s]
Processed prompts:  11%|█         | 14/128 [00:28<01:31,  1.24it/s, est. speed input: 7.78 toks/s, output: 124.48 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:28<00:41,  2.58it/s, est. speed input: 11.62 toks/s, output: 185.92 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:29<00:18,  5.20it/s, est. speed input: 17.08 toks/s, output: 273.28 toks/s]
Processed prompts:  36%|███▌      | 46/128 [00:29<00:07, 10.59it/s, est. speed input: 25.26 toks/s, output: 404.11 toks/s]
Processed prompts:  47%|████▋     | 60/128 [00:29<00:03, 17.07it/s, est. speed input: 32.82 toks/s, output: 525.20 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:29<00:01, 30.47it/s, est. speed input: 46.17 toks/s, output: 738.77 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:29<00:00, 42.73it/s, est. speed input: 57.74 toks/s, output: 923.91 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:29<00:00, 53.39it/s, est. speed input: 67.60 toks/s, output: 1081.68 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:29<00:00, 53.39it/s, est. speed input: 68.68 toks/s, output: 1098.84 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:29<00:00,  4.29it/s, est. speed input: 68.68 toks/s, output: 1098.84 toks/s]
[rank0]:[W127 19:39:01.024307964 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 197.2s

测试结果:
  Requests/s:   4.08
  Tokens/s:     1109.78
  Total Reqs:   128
  Elapsed:      31.37s

  [Decode 分析]
  Total Decode Tokens:  32768
  Decode Tokens/s:      1044.50

============================================================
[3/4] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-14B-INT8                                │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 4096 (= 256 x 16)
│   M_decode      = 256
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 256
│   --max-num-seqs           = 256
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 19:39:22 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 19:39:22 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3006356) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3006356) WARNING 01-27 19:41:46 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 5.46 requests/s, 1484.75 total tokens/s, 1397.41 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536


─── STDERR ───
[2026-01-27 19:39:22] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 19:39:22] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:39:22] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-27 19:39:22] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:39:22] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:39:22] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:39:22] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:39:22] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:39:22] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:39:22] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 19:39:22] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 19:39:22] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 19:39:22] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 19:39:22] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 19:39:26] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 19:39:26] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:39:26] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-27 19:39:26] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:39:26] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:39:26] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:39:26] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:39:26] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:39:26] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:39:26] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 19:39:26] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 19:39:26] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 19:39:26] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 19:39:26] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3006356) [2026-01-27 19:39:27] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3006356) [2026-01-27 19:39:27] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3006356) [2026-01-27 19:39:27] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3006356) [2026-01-27 19:39:27] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3006356) [2026-01-27 19:39:27] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=3006356) [2026-01-27 19:39:27] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3006356) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3006356) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:08<00:25,  8.39s/it]
(EngineCore_DP0 pid=3006356) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:47<00:53, 26.70s/it]
(EngineCore_DP0 pid=3006356) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [01:18<00:28, 28.52s/it]
(EngineCore_DP0 pid=3006356) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [02:00<00:00, 33.78s/it]
(EngineCore_DP0 pid=3006356) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [02:00<00:00, 30.11s/it]
(EngineCore_DP0 pid=3006356) 
(EngineCore_DP0 pid=3006356) [2026-01-27 19:41:29] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [7168, 7680] -> 1D uint8
(EngineCore_DP0 pid=3006356) [2026-01-27 19:41:29] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 41287680 bytes
(EngineCore_DP0 pid=3006356) [2026-01-27 19:41:29] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 7680] -> 1D uint8
(EngineCore_DP0 pid=3006356) [2026-01-27 19:41:29] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 29491200 bytes
(EngineCore_DP0 pid=3006356) [2026-01-27 19:41:29] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [27648, 7680] -> 1D uint8
(EngineCore_DP0 pid=3006356) [2026-01-27 19:41:29] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 159252480 bytes
(EngineCore_DP0 pid=3006356) [2026-01-27 19:41:29] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 20736] -> 1D uint8
(EngineCore_DP0 pid=3006356) [2026-01-27 19:41:29] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 79626240 bytes
(EngineCore_DP0 pid=3006356) 2026-01-27 19:41:39,428 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3006356) 2026-01-27 19:41:39,510 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   0%|          | 1/256 [00:00<00:37,  6.78it/s]
Adding requests:   2%|▏         | 4/256 [00:00<00:16, 15.48it/s]
Adding requests:   3%|▎         | 8/256 [00:00<00:10, 24.18it/s]
Adding requests:   5%|▌         | 13/256 [00:00<00:07, 32.43it/s]
Adding requests:   8%|▊         | 20/256 [00:00<00:05, 43.56it/s]
Adding requests:  12%|█▎        | 32/256 [00:00<00:03, 66.18it/s]
Adding requests:  19%|█▉        | 49/256 [00:00<00:02, 96.64it/s]
Adding requests:  28%|██▊       | 72/256 [00:00<00:01, 136.24it/s]
Adding requests:  39%|███▊      | 99/256 [00:01<00:00, 174.95it/s]
Adding requests:  52%|█████▏    | 134/256 [00:01<00:00, 226.40it/s]
Adding requests:  66%|██████▋   | 170/256 [00:01<00:00, 266.06it/s]
Adding requests:  77%|███████▋  | 197/256 [00:01<00:00, 198.14it/s]
Adding requests:  98%|█████████▊| 250/256 [00:01<00:00, 276.82it/s]
Adding requests: 100%|██████████| 256/256 [00:01<00:00, 165.27it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:39<2:46:46, 39.24s/it, est. speed input: 0.41 toks/s, output: 6.52 toks/s]
Processed prompts:   1%|          | 2/256 [00:39<1:08:50, 16.26s/it, est. speed input: 0.81 toks/s, output: 12.99 toks/s]
Processed prompts:   4%|▎         | 9/256 [00:39<09:49,  2.39s/it, est. speed input: 3.64 toks/s, output: 58.18 toks/s]  
Processed prompts:   6%|▌         | 15/256 [00:39<04:44,  1.18s/it, est. speed input: 6.04 toks/s, output: 96.57 toks/s]
Processed prompts:   9%|▊         | 22/256 [00:39<02:31,  1.54it/s, est. speed input: 8.81 toks/s, output: 140.97 toks/s]
Processed prompts:  13%|█▎        | 34/256 [00:40<01:09,  3.19it/s, est. speed input: 13.56 toks/s, output: 216.90 toks/s]
Processed prompts:  19%|█▉        | 48/256 [00:40<00:35,  5.84it/s, est. speed input: 19.07 toks/s, output: 305.09 toks/s]
Processed prompts:  24%|██▍       | 62/256 [00:40<00:20,  9.29it/s, est. speed input: 24.52 toks/s, output: 392.34 toks/s]
Processed prompts:  29%|██▉       | 75/256 [00:40<00:13, 13.44it/s, est. speed input: 29.55 toks/s, output: 472.77 toks/s]
Processed prompts:  34%|███▍      | 87/256 [00:40<00:09, 18.06it/s, est. speed input: 34.14 toks/s, output: 546.19 toks/s]
Processed prompts:  38%|███▊      | 98/256 [00:40<00:06, 23.07it/s, est. speed input: 38.31 toks/s, output: 612.92 toks/s]
Processed prompts:  42%|████▏     | 108/256 [00:41<00:05, 28.41it/s, est. speed input: 42.08 toks/s, output: 673.21 toks/s]
Processed prompts:  46%|████▌     | 118/256 [00:41<00:04, 33.86it/s, est. speed input: 45.80 toks/s, output: 732.85 toks/s]
Processed prompts:  50%|████▉     | 127/256 [00:41<00:03, 39.24it/s, est. speed input: 49.14 toks/s, output: 786.29 toks/s]
Processed prompts:  53%|█████▎    | 136/256 [00:41<00:02, 45.07it/s, est. speed input: 52.47 toks/s, output: 839.54 toks/s]
Processed prompts:  56%|█████▋    | 144/256 [00:41<00:02, 49.12it/s, est. speed input: 55.40 toks/s, output: 886.34 toks/s]
Processed prompts:  59%|█████▉    | 152/256 [00:41<00:01, 53.24it/s, est. speed input: 58.31 toks/s, output: 932.98 toks/s]
Processed prompts:  62%|██████▏   | 159/256 [00:41<00:01, 54.33it/s, est. speed input: 60.82 toks/s, output: 973.12 toks/s]
Processed prompts:  65%|██████▍   | 166/256 [00:41<00:01, 54.46it/s, est. speed input: 63.30 toks/s, output: 1012.87 toks/s]
Processed prompts:  68%|██████▊   | 173/256 [00:42<00:01, 56.78it/s, est. speed input: 65.80 toks/s, output: 1052.83 toks/s]
Processed prompts:  70%|███████   | 180/256 [00:42<00:01, 56.68it/s, est. speed input: 68.26 toks/s, output: 1092.21 toks/s]
Processed prompts:  73%|███████▎  | 187/256 [00:42<00:01, 59.43it/s, est. speed input: 70.74 toks/s, output: 1131.91 toks/s]
Processed prompts:  76%|███████▌  | 194/256 [00:42<00:01, 47.42it/s, est. speed input: 73.01 toks/s, output: 1168.19 toks/s]
Processed prompts:  78%|███████▊  | 200/256 [00:42<00:01, 48.23it/s, est. speed input: 75.06 toks/s, output: 1200.98 toks/s]
Processed prompts:  80%|████████  | 206/256 [00:42<00:01, 41.16it/s, est. speed input: 76.95 toks/s, output: 1231.15 toks/s]
Processed prompts:  82%|████████▏ | 211/256 [00:42<00:01, 42.12it/s, est. speed input: 78.61 toks/s, output: 1257.80 toks/s]
Processed prompts:  84%|████████▍ | 216/256 [00:43<00:01, 36.59it/s, est. speed input: 80.12 toks/s, output: 1282.00 toks/s]
Processed prompts:  87%|████████▋ | 222/256 [00:43<00:00, 35.21it/s, est. speed input: 82.00 toks/s, output: 1311.98 toks/s]
Processed prompts:  88%|████████▊ | 226/256 [00:43<00:00, 36.01it/s, est. speed input: 83.28 toks/s, output: 1332.47 toks/s]
Processed prompts:  90%|████████▉ | 230/256 [00:43<00:00, 30.86it/s, est. speed input: 84.39 toks/s, output: 1350.24 toks/s]
Processed prompts:  91%|█████████▏| 234/256 [00:43<00:00, 28.84it/s, est. speed input: 85.53 toks/s, output: 1368.56 toks/s]
Processed prompts:  93%|█████████▎| 238/256 [00:43<00:00, 26.92it/s, est. speed input: 86.65 toks/s, output: 1386.40 toks/s]
Processed prompts:  95%|█████████▍| 242/256 [00:44<00:00, 25.99it/s, est. speed input: 87.77 toks/s, output: 1404.34 toks/s]
Processed prompts:  96%|█████████▌| 245/256 [00:44<00:00, 20.70it/s, est. speed input: 88.37 toks/s, output: 1413.94 toks/s]
Processed prompts:  97%|█████████▋| 248/256 [00:44<00:00, 17.83it/s, est. speed input: 88.97 toks/s, output: 1423.56 toks/s]
Processed prompts:  98%|█████████▊| 250/256 [00:44<00:00, 16.17it/s, est. speed input: 89.35 toks/s, output: 1429.52 toks/s]
Processed prompts:  98%|█████████▊| 252/256 [00:44<00:00, 15.20it/s, est. speed input: 89.74 toks/s, output: 1435.80 toks/s]
Processed prompts:  99%|█████████▉| 254/256 [00:45<00:00, 14.47it/s, est. speed input: 90.13 toks/s, output: 1442.07 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:45<00:00, 13.52it/s, est. speed input: 90.48 toks/s, output: 1447.73 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:45<00:00, 13.52it/s, est. speed input: 90.48 toks/s, output: 1447.73 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:45<00:00,  5.66it/s, est. speed input: 90.48 toks/s, output: 1447.73 toks/s]
[rank0]:[W127 19:42:34.315219071 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 213.4s

测试结果:
  Requests/s:   5.46
  Tokens/s:     1484.75
  Total Reqs:   256
  Elapsed:      46.90s

  [Decode 分析]
  Total Decode Tokens:  65536
  Decode Tokens/s:      1397.41

============================================================
[4/4] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-14B-INT8                                │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 8192 (= 512 x 16)
│   M_decode      = 512
│   batched_tokens = 512 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 512
│   --max-num-seqs           = 512
│   --max-model-len          = 272
│   --max-num-batched-tokens = 512
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 19:42:56 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 19:42:56 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3009587) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3009587) WARNING 01-27 19:45:23 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 5.55 requests/s, 1510.53 total tokens/s, 1421.68 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072


─── STDERR ───
[2026-01-27 19:42:56] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 19:42:56] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:42:56] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-27 19:42:56] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:42:56] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:42:56] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:42:56] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:42:56] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:42:56] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:42:56] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 19:42:56] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 19:42:56] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 19:42:56] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 19:42:56] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 19:42:59] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 19:43:00] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:43:00] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-27 19:43:00] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:43:00] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:43:00] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:43:00] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:43:00] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:43:00] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:43:00] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 19:43:00] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 19:43:00] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 19:43:00] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 19:43:00] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3009587) [2026-01-27 19:43:01] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3009587) [2026-01-27 19:43:01] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3009587) [2026-01-27 19:43:01] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3009587) [2026-01-27 19:43:01] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3009587) [2026-01-27 19:43:01] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=3009587) [2026-01-27 19:43:01] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3009587) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3009587) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:08<00:25,  8.42s/it]
(EngineCore_DP0 pid=3009587) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:48<00:53, 26.94s/it]
(EngineCore_DP0 pid=3009587) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [01:19<00:29, 29.05s/it]
(EngineCore_DP0 pid=3009587) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [02:01<00:00, 34.17s/it]
(EngineCore_DP0 pid=3009587) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [02:01<00:00, 30.47s/it]
(EngineCore_DP0 pid=3009587) 
(EngineCore_DP0 pid=3009587) [2026-01-27 19:45:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [7168, 7680] -> 1D uint8
(EngineCore_DP0 pid=3009587) [2026-01-27 19:45:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 41287680 bytes
(EngineCore_DP0 pid=3009587) [2026-01-27 19:45:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 7680] -> 1D uint8
(EngineCore_DP0 pid=3009587) [2026-01-27 19:45:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 29491200 bytes
(EngineCore_DP0 pid=3009587) [2026-01-27 19:45:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [27648, 7680] -> 1D uint8
(EngineCore_DP0 pid=3009587) [2026-01-27 19:45:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 159252480 bytes
(EngineCore_DP0 pid=3009587) [2026-01-27 19:45:04] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 20736] -> 1D uint8
(EngineCore_DP0 pid=3009587) [2026-01-27 19:45:04] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 79626240 bytes
(EngineCore_DP0 pid=3009587) 2026-01-27 19:45:14,499 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3009587) 2026-01-27 19:45:14,602 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   0%|          | 1/512 [00:00<01:09,  7.40it/s]
Adding requests:   1%|          | 4/512 [00:00<00:27, 18.73it/s]
Adding requests:   2%|▏         | 9/512 [00:00<00:15, 31.65it/s]
Adding requests:   3%|▎         | 14/512 [00:00<00:13, 36.68it/s]
Adding requests:   5%|▍         | 24/512 [00:00<00:08, 57.42it/s]
Adding requests:   8%|▊         | 42/512 [00:00<00:04, 96.44it/s]
Adding requests:  13%|█▎        | 67/512 [00:00<00:03, 144.64it/s]
Adding requests:  20%|█▉        | 100/512 [00:00<00:02, 200.29it/s]
Adding requests:  26%|██▌       | 131/512 [00:00<00:01, 232.06it/s]
Adding requests:  32%|███▏      | 165/512 [00:01<00:01, 263.38it/s]
Adding requests:  40%|████      | 207/512 [00:01<00:00, 307.48it/s]
Adding requests:  51%|█████     | 261/512 [00:01<00:00, 376.63it/s]
Adding requests:  66%|██████▌   | 339/512 [00:01<00:00, 496.21it/s]
Adding requests:  88%|████████▊ | 449/512 [00:01<00:00, 675.76it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 338.32it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [01:03<8:58:49, 63.27s/it, est. speed input: 0.25 toks/s, output: 4.05 toks/s]
Processed prompts:   0%|          | 2/512 [01:03<3:45:03, 26.48s/it, est. speed input: 0.50 toks/s, output: 8.00 toks/s]
Processed prompts:   3%|▎         | 14/512 [01:04<20:16,  2.44s/it, est. speed input: 3.46 toks/s, output: 55.38 toks/s]
Processed prompts:   5%|▌         | 27/512 [01:05<08:27,  1.05s/it, est. speed input: 6.60 toks/s, output: 105.67 toks/s]
Processed prompts:  10%|█         | 52/512 [01:06<03:12,  2.39it/s, est. speed input: 12.59 toks/s, output: 201.44 toks/s]
Processed prompts:  16%|█▌        | 80/512 [01:06<01:35,  4.51it/s, est. speed input: 19.18 toks/s, output: 306.93 toks/s]
Processed prompts:  21%|██        | 107/512 [01:07<00:57,  7.09it/s, est. speed input: 25.42 toks/s, output: 406.73 toks/s]
Processed prompts:  26%|██▌       | 132/512 [01:07<00:36, 10.50it/s, est. speed input: 31.24 toks/s, output: 499.90 toks/s]
Processed prompts:  30%|███       | 155/512 [01:07<00:24, 14.54it/s, est. speed input: 36.55 toks/s, output: 584.80 toks/s]
Processed prompts:  35%|███▍      | 177/512 [01:08<00:17, 19.42it/s, est. speed input: 41.58 toks/s, output: 665.36 toks/s]
Processed prompts:  39%|███▊      | 198/512 [01:08<00:12, 25.25it/s, est. speed input: 46.37 toks/s, output: 741.88 toks/s]
Processed prompts:  42%|████▏     | 217/512 [01:08<00:09, 31.24it/s, est. speed input: 50.65 toks/s, output: 810.41 toks/s]
Processed prompts:  46%|████▌     | 235/512 [01:08<00:07, 37.48it/s, est. speed input: 54.68 toks/s, output: 874.81 toks/s]
Processed prompts:  49%|████▉     | 252/512 [01:08<00:05, 43.54it/s, est. speed input: 58.45 toks/s, output: 935.14 toks/s]
Processed prompts:  52%|█████▏    | 268/512 [01:09<00:04, 50.09it/s, est. speed input: 61.99 toks/s, output: 991.85 toks/s]
Processed prompts:  55%|█████▌    | 283/512 [01:09<00:04, 55.91it/s, est. speed input: 65.29 toks/s, output: 1044.64 toks/s]
Processed prompts:  58%|█████▊    | 297/512 [01:09<00:03, 61.60it/s, est. speed input: 68.36 toks/s, output: 1093.78 toks/s]
Processed prompts:  61%|██████    | 310/512 [01:09<00:03, 64.92it/s, est. speed input: 71.18 toks/s, output: 1138.89 toks/s]
Processed prompts:  63%|██████▎   | 322/512 [01:09<00:02, 67.49it/s, est. speed input: 73.77 toks/s, output: 1180.32 toks/s]
Processed prompts:  65%|██████▌   | 333/512 [01:09<00:02, 68.76it/s, est. speed input: 76.13 toks/s, output: 1218.01 toks/s]
Processed prompts:  67%|██████▋   | 344/512 [01:10<00:02, 68.78it/s, est. speed input: 78.46 toks/s, output: 1255.38 toks/s]
Processed prompts:  69%|██████▉   | 354/512 [01:10<00:02, 67.68it/s, est. speed input: 80.56 toks/s, output: 1289.03 toks/s]
Processed prompts:  71%|███████   | 363/512 [01:10<00:02, 67.19it/s, est. speed input: 82.45 toks/s, output: 1319.23 toks/s]
Processed prompts:  73%|███████▎  | 372/512 [01:10<00:02, 65.67it/s, est. speed input: 84.32 toks/s, output: 1349.15 toks/s]
Processed prompts:  74%|███████▍  | 380/512 [01:10<00:02, 64.76it/s, est. speed input: 85.98 toks/s, output: 1375.65 toks/s]
Processed prompts:  76%|███████▌  | 388/512 [01:10<00:02, 60.72it/s, est. speed input: 87.60 toks/s, output: 1401.54 toks/s]
Processed prompts:  77%|███████▋  | 395/512 [01:10<00:01, 59.59it/s, est. speed input: 89.02 toks/s, output: 1424.33 toks/s]
Processed prompts:  79%|███████▊  | 402/512 [01:11<00:01, 57.54it/s, est. speed input: 90.43 toks/s, output: 1446.85 toks/s]
Processed prompts:  80%|███████▉  | 408/512 [01:11<00:01, 56.27it/s, est. speed input: 91.63 toks/s, output: 1466.10 toks/s]
Processed prompts:  81%|████████  | 414/512 [01:11<00:01, 55.39it/s, est. speed input: 92.83 toks/s, output: 1485.30 toks/s]
Processed prompts:  82%|████████▏ | 420/512 [01:11<00:01, 52.68it/s, est. speed input: 94.01 toks/s, output: 1504.10 toks/s]
Processed prompts:  83%|████████▎ | 426/512 [01:11<00:01, 53.45it/s, est. speed input: 95.21 toks/s, output: 1523.29 toks/s]
Processed prompts:  84%|████████▍ | 432/512 [01:11<00:01, 54.38it/s, est. speed input: 96.40 toks/s, output: 1542.47 toks/s]
Processed prompts:  86%|████████▌ | 438/512 [01:11<00:01, 53.18it/s, est. speed input: 97.58 toks/s, output: 1561.30 toks/s]
Processed prompts:  87%|████████▋ | 444/512 [01:12<00:01, 42.49it/s, est. speed input: 98.63 toks/s, output: 1578.07 toks/s]
Processed prompts:  88%|████████▊ | 449/512 [01:12<00:01, 44.08it/s, est. speed input: 99.60 toks/s, output: 1593.60 toks/s]
Processed prompts:  89%|████████▊ | 454/512 [01:12<00:01, 34.83it/s, est. speed input: 100.40 toks/s, output: 1606.33 toks/s]
Processed prompts:  90%|████████▉ | 460/512 [01:12<00:01, 33.45it/s, est. speed input: 101.45 toks/s, output: 1623.19 toks/s]
Processed prompts:  91%|█████████ | 466/512 [01:12<00:01, 32.22it/s, est. speed input: 102.49 toks/s, output: 1639.81 toks/s]
Processed prompts:  92%|█████████▏| 470/512 [01:12<00:01, 28.99it/s, est. speed input: 103.11 toks/s, output: 1649.70 toks/s]
Processed prompts:  93%|█████████▎| 474/512 [01:13<00:01, 26.66it/s, est. speed input: 103.72 toks/s, output: 1659.49 toks/s]
Processed prompts:  93%|█████████▎| 478/512 [01:13<00:01, 25.42it/s, est. speed input: 104.34 toks/s, output: 1669.42 toks/s]
Processed prompts:  94%|█████████▍| 482/512 [01:13<00:01, 24.01it/s, est. speed input: 104.94 toks/s, output: 1679.01 toks/s]
Processed prompts:  95%|█████████▍| 485/512 [01:13<00:01, 19.29it/s, est. speed input: 105.22 toks/s, output: 1683.48 toks/s]
Processed prompts:  95%|█████████▌| 488/512 [01:14<00:01, 16.65it/s, est. speed input: 105.50 toks/s, output: 1688.01 toks/s]
Processed prompts:  96%|█████████▌| 490/512 [01:14<00:01, 15.31it/s, est. speed input: 105.68 toks/s, output: 1690.91 toks/s]
Processed prompts:  96%|█████████▌| 492/512 [01:14<00:01, 14.46it/s, est. speed input: 105.87 toks/s, output: 1694.00 toks/s]
Processed prompts:  96%|█████████▋| 494/512 [01:14<00:01, 13.84it/s, est. speed input: 106.07 toks/s, output: 1697.11 toks/s]
Processed prompts:  97%|█████████▋| 496/512 [01:14<00:01, 13.39it/s, est. speed input: 106.27 toks/s, output: 1700.25 toks/s]
Processed prompts:  97%|█████████▋| 497/512 [01:28<00:01, 13.39it/s, est. speed input: 106.36 toks/s, output: 1701.81 toks/s]
Processed prompts:  97%|█████████▋| 498/512 [01:30<00:29,  2.09s/it, est. speed input: 88.35 toks/s, output: 1413.62 toks/s] 
Processed prompts:  98%|█████████▊| 500/512 [01:30<00:18,  1.54s/it, est. speed input: 88.55 toks/s, output: 1416.77 toks/s]
Processed prompts:  99%|█████████▊| 505/512 [01:30<00:05,  1.25it/s, est. speed input: 89.27 toks/s, output: 1428.40 toks/s]
Processed prompts: 100%|██████████| 512/512 [01:30<00:00,  1.25it/s, est. speed input: 90.42 toks/s, output: 1446.79 toks/s]
Processed prompts: 100%|██████████| 512/512 [01:30<00:00,  5.65it/s, est. speed input: 90.42 toks/s, output: 1446.79 toks/s]
[rank0]:[W127 19:46:56.222558008 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 260.8s

测试结果:
  Requests/s:   5.55
  Tokens/s:     1510.53
  Total Reqs:   512
  Elapsed:      92.20s

  [Decode 分析]
  Total Decode Tokens:  131072
  Decode Tokens/s:      1421.68


------------------------------------------------------------
  生成 CSV: Qwen2.5-14B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_8/Qwen2.5-14B-INT8_decode.csv

预览:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
64,16,64,64,256,256,2.4222,658.8457,26.4220
128,16,128,128,256,256,4.0801,1109.7790,31.3720
256,16,256,256,256,256,5.4587,1484.7529,46.8980
512,16,512,512,256,256,5.5534,1510.5324,92.1953

------------------------------------------------------------

[INFO] 完成: 4 成功, 0 失败

============================================================
  Qwen2.5-14B-INT8 | cuSPARSELt (2_10) | decode
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-INT8-SlideSparse-2_10
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_10

============================================================
[1/4] 测试 M=64
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-14B-INT8                                │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 64
│   M_prefill     = 1024 (= 64 x 16)
│   M_decode      = 64
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 64
│   --max-num-seqs           = 64
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 19:47:17 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 19:47:17 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3013490) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3013490) WARNING 01-27 19:49:40 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 2.32 requests/s, 630.48 total tokens/s, 593.39 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384


─── STDERR ───
[2026-01-27 19:47:17] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 19:47:17] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:47:17] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-27 19:47:17] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:47:17] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:47:17] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:47:17] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:47:17] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:47:17] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:47:17] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 19:47:17] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 19:47:17] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 19:47:17] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 19:47:17] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 19:47:21] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 19:47:21] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:47:21] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-27 19:47:21] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:47:21] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:47:21] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:47:21] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:47:21] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:47:21] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:47:21] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 19:47:21] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 19:47:21] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 19:47:21] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 19:47:21] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3013490) [2026-01-27 19:47:22] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3013490) [2026-01-27 19:47:22] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3013490) [2026-01-27 19:47:22] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3013490) [2026-01-27 19:47:22] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3013490) [2026-01-27 19:47:22] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=3013490) [2026-01-27 19:47:22] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3013490) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3013490) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:08<00:25,  8.52s/it]
(EngineCore_DP0 pid=3013490) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:40<00:44, 22.07s/it]
(EngineCore_DP0 pid=3013490) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [01:15<00:28, 28.31s/it]
(EngineCore_DP0 pid=3013490) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:57<00:00, 33.50s/it]
(EngineCore_DP0 pid=3013490) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:57<00:00, 29.32s/it]
(EngineCore_DP0 pid=3013490) 
(EngineCore_DP0 pid=3013490) [2026-01-27 19:49:21] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [7168, 8192] -> 1D uint8
(EngineCore_DP0 pid=3013490) [2026-01-27 19:49:21] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 44040192 bytes
(EngineCore_DP0 pid=3013490) [2026-01-27 19:49:21] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 8192] -> 1D uint8
(EngineCore_DP0 pid=3013490) [2026-01-27 19:49:21] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 31457280 bytes
(EngineCore_DP0 pid=3013490) [2026-01-27 19:49:21] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [27648, 8192] -> 1D uint8
(EngineCore_DP0 pid=3013490) [2026-01-27 19:49:21] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 169869312 bytes
(EngineCore_DP0 pid=3013490) [2026-01-27 19:49:21] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 22144] -> 1D uint8
(EngineCore_DP0 pid=3013490) [2026-01-27 19:49:21] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 85032960 bytes
(EngineCore_DP0 pid=3013490) 2026-01-27 19:49:32,667 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3013490) 2026-01-27 19:49:33,039 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 1/64 [00:00<00:14,  4.28it/s]
Adding requests:   5%|▍         | 3/64 [00:00<00:06,  9.42it/s]
Adding requests:   9%|▉         | 6/64 [00:00<00:03, 16.07it/s]
Adding requests:  16%|█▌        | 10/64 [00:00<00:02, 23.20it/s]
Adding requests:  22%|██▏       | 14/64 [00:00<00:01, 28.32it/s]
Adding requests:  34%|███▍      | 22/64 [00:00<00:00, 43.50it/s]
Adding requests:  55%|█████▍    | 35/64 [00:00<00:00, 68.10it/s]
Adding requests:  80%|███████▉  | 51/64 [00:00<00:00, 95.09it/s]
Adding requests: 100%|██████████| 64/64 [00:01<00:00, 61.11it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 1/64 [00:25<27:13, 25.93s/it, est. speed input: 0.62 toks/s, output: 9.87 toks/s]
Processed prompts:   3%|▎         | 2/64 [00:26<11:05, 10.74s/it, est. speed input: 1.23 toks/s, output: 19.66 toks/s]
Processed prompts:  11%|█         | 7/64 [00:26<01:58,  2.08s/it, est. speed input: 4.28 toks/s, output: 68.54 toks/s]
Processed prompts:  36%|███▌      | 23/64 [00:26<00:18,  2.23it/s, est. speed input: 14.02 toks/s, output: 224.27 toks/s]
Processed prompts:  59%|█████▉    | 38/64 [00:26<00:05,  4.50it/s, est. speed input: 23.06 toks/s, output: 369.01 toks/s]
Processed prompts:  81%|████████▏ | 52/64 [00:26<00:01,  7.41it/s, est. speed input: 31.44 toks/s, output: 503.05 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:26<00:00,  7.41it/s, est. speed input: 38.70 toks/s, output: 619.13 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:26<00:00,  2.42it/s, est. speed input: 38.70 toks/s, output: 619.13 toks/s]
[rank0]:[W127 19:50:09.095252422 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 194.2s

测试结果:
  Requests/s:   2.32
  Tokens/s:     630.48
  Total Reqs:   64
  Elapsed:      27.61s

  [Decode 分析]
  Total Decode Tokens:  16384
  Decode Tokens/s:      593.39

============================================================
[2/4] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-14B-INT8                                │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 2048 (= 128 x 16)
│   M_decode      = 128
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 128
│   --max-num-seqs           = 128
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 19:50:30 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 19:50:31 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3016363) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3016363) WARNING 01-27 19:52:47 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 3.90 requests/s, 1059.48 total tokens/s, 997.16 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768


─── STDERR ───
[2026-01-27 19:50:30] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 19:50:30] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:50:30] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-27 19:50:30] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:50:30] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:50:30] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:50:30] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:50:30] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:50:30] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:50:30] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 19:50:30] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 19:50:30] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 19:50:30] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 19:50:30] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 19:50:34] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 19:50:34] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:50:34] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-27 19:50:34] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:50:34] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:50:34] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:50:34] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:50:34] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:50:34] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:50:34] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 19:50:34] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 19:50:34] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 19:50:34] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 19:50:34] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3016363) [2026-01-27 19:50:35] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3016363) [2026-01-27 19:50:35] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3016363) [2026-01-27 19:50:35] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3016363) [2026-01-27 19:50:35] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3016363) [2026-01-27 19:50:35] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=3016363) [2026-01-27 19:50:35] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3016363) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3016363) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:08<00:25,  8.66s/it]
(EngineCore_DP0 pid=3016363) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:41<00:45, 22.84s/it]
(EngineCore_DP0 pid=3016363) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [01:17<00:28, 28.78s/it]
(EngineCore_DP0 pid=3016363) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:58<00:00, 33.64s/it]
(EngineCore_DP0 pid=3016363) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:58<00:00, 29.59s/it]
(EngineCore_DP0 pid=3016363) 
(EngineCore_DP0 pid=3016363) [2026-01-27 19:52:35] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [7168, 8192] -> 1D uint8
(EngineCore_DP0 pid=3016363) [2026-01-27 19:52:35] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 44040192 bytes
(EngineCore_DP0 pid=3016363) [2026-01-27 19:52:35] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 8192] -> 1D uint8
(EngineCore_DP0 pid=3016363) [2026-01-27 19:52:35] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 31457280 bytes
(EngineCore_DP0 pid=3016363) [2026-01-27 19:52:35] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [27648, 8192] -> 1D uint8
(EngineCore_DP0 pid=3016363) [2026-01-27 19:52:35] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 169869312 bytes
(EngineCore_DP0 pid=3016363) [2026-01-27 19:52:35] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 22144] -> 1D uint8
(EngineCore_DP0 pid=3016363) [2026-01-27 19:52:35] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 85032960 bytes
(EngineCore_DP0 pid=3016363) 2026-01-27 19:52:45,865 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3016363) 2026-01-27 19:52:45,941 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:   1%|          | 1/128 [00:00<00:29,  4.33it/s]
Adding requests:   2%|▏         | 3/128 [00:00<00:14,  8.71it/s]
Adding requests:   5%|▍         | 6/128 [00:00<00:08, 14.97it/s]
Adding requests:   8%|▊         | 10/128 [00:00<00:05, 21.87it/s]
Adding requests:  11%|█         | 14/128 [00:00<00:04, 26.37it/s]
Adding requests:  17%|█▋        | 22/128 [00:00<00:02, 41.48it/s]
Adding requests:  27%|██▋       | 35/128 [00:00<00:01, 65.79it/s]
Adding requests:  39%|███▉      | 50/128 [00:01<00:00, 88.28it/s]
Adding requests:  54%|█████▍    | 69/128 [00:01<00:00, 117.13it/s]
Adding requests:  71%|███████   | 91/128 [00:01<00:00, 145.55it/s]
Adding requests:  92%|█████████▏| 118/128 [00:01<00:00, 180.77it/s]
Adding requests: 100%|██████████| 128/128 [00:01<00:00, 92.90it/s] 

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:29<1:03:15, 29.88s/it, est. speed input: 0.54 toks/s, output: 8.57 toks/s]
Processed prompts:   2%|▏         | 2/128 [00:30<25:59, 12.38s/it, est. speed input: 1.07 toks/s, output: 17.06 toks/s] 
Processed prompts:   5%|▍         | 6/128 [00:30<05:49,  2.86s/it, est. speed input: 3.19 toks/s, output: 50.96 toks/s]
Processed prompts:   9%|▊         | 11/128 [00:30<02:22,  1.22s/it, est. speed input: 5.81 toks/s, output: 93.03 toks/s]
Processed prompts:  11%|█         | 14/128 [00:30<01:34,  1.21it/s, est. speed input: 7.37 toks/s, output: 117.90 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:30<00:36,  2.84it/s, est. speed input: 12.05 toks/s, output: 192.77 toks/s]
Processed prompts:  30%|██▉       | 38/128 [00:30<00:13,  6.60it/s, est. speed input: 19.83 toks/s, output: 317.22 toks/s]
Processed prompts:  41%|████      | 52/128 [00:30<00:06, 11.26it/s, est. speed input: 27.03 toks/s, output: 432.48 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:30<00:03, 16.87it/s, est. speed input: 33.67 toks/s, output: 538.71 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:30<00:02, 24.03it/s, est. speed input: 40.27 toks/s, output: 644.30 toks/s]
Processed prompts:  70%|███████   | 90/128 [00:31<00:01, 31.71it/s, est. speed input: 46.30 toks/s, output: 740.75 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:31<00:00, 39.61it/s, est. speed input: 51.77 toks/s, output: 828.39 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:31<00:00, 54.33it/s, est. speed input: 61.66 toks/s, output: 986.54 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:31<00:00, 54.33it/s, est. speed input: 65.23 toks/s, output: 1043.61 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:31<00:00,  4.08it/s, est. speed input: 65.23 toks/s, output: 1043.61 toks/s]
[rank0]:[W127 19:53:20.864606679 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 191.8s

测试结果:
  Requests/s:   3.90
  Tokens/s:     1059.48
  Total Reqs:   128
  Elapsed:      32.86s

  [Decode 分析]
  Total Decode Tokens:  32768
  Decode Tokens/s:      997.16

============================================================
[3/4] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-14B-INT8                                │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 4096 (= 256 x 16)
│   M_decode      = 256
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 256
│   --max-num-seqs           = 256
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 19:53:43 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 19:53:44 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3019222) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3019222) WARNING 01-27 19:56:11 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 5.11 requests/s, 1391.10 total tokens/s, 1309.27 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536


─── STDERR ───
[2026-01-27 19:53:43] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 19:53:43] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:53:43] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-27 19:53:43] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:53:43] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:53:43] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:53:43] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:53:43] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:53:43] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:53:43] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 19:53:43] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 19:53:43] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 19:53:43] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 19:53:43] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 19:53:47] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 19:53:47] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:53:47] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-27 19:53:47] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:53:47] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:53:47] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:53:47] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:53:47] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:53:47] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:53:47] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 19:53:47] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 19:53:47] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 19:53:47] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 19:53:47] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3019222) [2026-01-27 19:53:48] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3019222) [2026-01-27 19:53:48] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3019222) [2026-01-27 19:53:48] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3019222) [2026-01-27 19:53:48] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3019222) [2026-01-27 19:53:48] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=3019222) [2026-01-27 19:53:48] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3019222) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3019222) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:08<00:25,  8.54s/it]
(EngineCore_DP0 pid=3019222) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:41<00:45, 22.66s/it]
(EngineCore_DP0 pid=3019222) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [01:17<00:28, 28.88s/it]
(EngineCore_DP0 pid=3019222) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:58<00:00, 33.81s/it]
(EngineCore_DP0 pid=3019222) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:58<00:00, 29.68s/it]
(EngineCore_DP0 pid=3019222) 
(EngineCore_DP0 pid=3019222) [2026-01-27 19:55:49] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [7168, 8192] -> 1D uint8
(EngineCore_DP0 pid=3019222) [2026-01-27 19:55:49] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 44040192 bytes
(EngineCore_DP0 pid=3019222) [2026-01-27 19:55:49] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 8192] -> 1D uint8
(EngineCore_DP0 pid=3019222) [2026-01-27 19:55:49] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 31457280 bytes
(EngineCore_DP0 pid=3019222) [2026-01-27 19:55:49] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [27648, 8192] -> 1D uint8
(EngineCore_DP0 pid=3019222) [2026-01-27 19:55:49] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 169869312 bytes
(EngineCore_DP0 pid=3019222) [2026-01-27 19:55:49] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 22144] -> 1D uint8
(EngineCore_DP0 pid=3019222) [2026-01-27 19:55:49] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 85032960 bytes
(EngineCore_DP0 pid=3019222) 2026-01-27 19:56:01,104 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3019222) 2026-01-27 19:56:01,539 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   0%|          | 1/256 [00:00<00:53,  4.78it/s]
Adding requests:   1%|          | 3/256 [00:00<00:26,  9.48it/s]
Adding requests:   2%|▏         | 6/256 [00:00<00:16, 15.25it/s]
Adding requests:   4%|▎         | 9/256 [00:00<00:13, 18.77it/s]
Adding requests:   5%|▌         | 13/256 [00:00<00:10, 23.79it/s]
Adding requests:   7%|▋         | 18/256 [00:00<00:07, 31.15it/s]
Adding requests:  10%|█         | 26/256 [00:00<00:05, 44.36it/s]
Adding requests:  14%|█▍        | 36/256 [00:01<00:03, 58.75it/s]
Adding requests:  19%|█▉        | 49/256 [00:01<00:02, 78.21it/s]
Adding requests:  26%|██▌       | 66/256 [00:01<00:01, 103.28it/s]
Adding requests:  33%|███▎      | 84/256 [00:01<00:01, 122.20it/s]
Adding requests:  40%|████      | 103/256 [00:01<00:01, 141.22it/s]
Adding requests:  49%|████▉     | 126/256 [00:01<00:00, 166.60it/s]
Adding requests:  59%|█████▊    | 150/256 [00:01<00:00, 186.90it/s]
Adding requests:  66%|██████▌   | 169/256 [00:01<00:00, 125.81it/s]
Adding requests:  78%|███████▊  | 199/256 [00:02<00:00, 163.64it/s]
Adding requests:  92%|█████████▏| 236/256 [00:02<00:00, 211.85it/s]
Adding requests: 100%|██████████| 256/256 [00:02<00:00, 118.13it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:41<2:54:28, 41.05s/it, est. speed input: 0.39 toks/s, output: 6.24 toks/s]
Processed prompts:   1%|          | 2/256 [00:41<1:12:02, 17.02s/it, est. speed input: 0.78 toks/s, output: 12.41 toks/s]
Processed prompts:   3%|▎         | 7/256 [00:41<13:42,  3.30s/it, est. speed input: 2.70 toks/s, output: 43.25 toks/s]  
Processed prompts:   4%|▍         | 11/256 [00:41<07:05,  1.74s/it, est. speed input: 4.23 toks/s, output: 67.68 toks/s]
Processed prompts:   6%|▌         | 15/256 [00:41<04:12,  1.05s/it, est. speed input: 5.74 toks/s, output: 91.89 toks/s]
Processed prompts:   8%|▊         | 20/256 [00:41<02:27,  1.60it/s, est. speed input: 7.62 toks/s, output: 121.95 toks/s]
Processed prompts:  11%|█         | 28/256 [00:42<01:15,  3.03it/s, est. speed input: 10.62 toks/s, output: 169.97 toks/s]
Processed prompts:  16%|█▌        | 40/256 [00:42<00:36,  5.96it/s, est. speed input: 15.12 toks/s, output: 241.88 toks/s]
Processed prompts:  21%|██        | 54/256 [00:42<00:19, 10.34it/s, est. speed input: 20.32 toks/s, output: 325.07 toks/s]
Processed prompts:  26%|██▌       | 67/256 [00:42<00:12, 15.33it/s, est. speed input: 25.10 toks/s, output: 401.65 toks/s]
Processed prompts:  31%|███       | 79/256 [00:42<00:08, 20.92it/s, est. speed input: 29.49 toks/s, output: 471.88 toks/s]
Processed prompts:  36%|███▌      | 91/256 [00:43<00:06, 27.14it/s, est. speed input: 33.84 toks/s, output: 541.45 toks/s]
Processed prompts:  40%|███▉      | 102/256 [00:43<00:04, 33.49it/s, est. speed input: 37.80 toks/s, output: 604.84 toks/s]
Processed prompts:  44%|████▍     | 112/256 [00:43<00:03, 39.13it/s, est. speed input: 41.37 toks/s, output: 661.93 toks/s]
Processed prompts:  48%|████▊     | 122/256 [00:43<00:03, 43.74it/s, est. speed input: 44.90 toks/s, output: 718.37 toks/s]
Processed prompts:  51%|█████     | 131/256 [00:43<00:02, 47.35it/s, est. speed input: 48.05 toks/s, output: 768.76 toks/s]
Processed prompts:  54%|█████▍    | 139/256 [00:43<00:02, 50.42it/s, est. speed input: 50.83 toks/s, output: 813.31 toks/s]
Processed prompts:  57%|█████▋    | 147/256 [00:43<00:02, 51.29it/s, est. speed input: 53.58 toks/s, output: 857.21 toks/s]
Processed prompts:  60%|██████    | 154/256 [00:44<00:01, 53.38it/s, est. speed input: 55.98 toks/s, output: 895.68 toks/s]
Processed prompts:  63%|██████▎   | 161/256 [00:44<00:01, 53.94it/s, est. speed input: 58.36 toks/s, output: 933.72 toks/s]
Processed prompts:  66%|██████▌   | 168/256 [00:44<00:01, 55.23it/s, est. speed input: 60.73 toks/s, output: 971.71 toks/s]
Processed prompts:  68%|██████▊   | 174/256 [00:44<00:01, 54.14it/s, est. speed input: 62.73 toks/s, output: 1003.75 toks/s]
Processed prompts:  70%|███████   | 180/256 [00:44<00:01, 53.25it/s, est. speed input: 64.73 toks/s, output: 1035.61 toks/s]
Processed prompts:  73%|███████▎  | 186/256 [00:44<00:01, 53.34it/s, est. speed input: 66.72 toks/s, output: 1067.45 toks/s]
Processed prompts:  75%|███████▌  | 192/256 [00:44<00:01, 53.78it/s, est. speed input: 68.70 toks/s, output: 1099.19 toks/s]
Processed prompts:  77%|███████▋  | 198/256 [00:44<00:01, 49.95it/s, est. speed input: 70.62 toks/s, output: 1129.96 toks/s]
Processed prompts:  80%|███████▉  | 204/256 [00:45<00:01, 41.01it/s, est. speed input: 72.42 toks/s, output: 1158.77 toks/s]
Processed prompts:  82%|████████▏ | 209/256 [00:45<00:01, 42.76it/s, est. speed input: 74.03 toks/s, output: 1184.48 toks/s]
Processed prompts:  84%|████████▎ | 214/256 [00:45<00:01, 34.93it/s, est. speed input: 75.44 toks/s, output: 1207.08 toks/s]
Processed prompts:  86%|████████▌ | 220/256 [00:45<00:01, 33.06it/s, est. speed input: 77.21 toks/s, output: 1235.41 toks/s]
Processed prompts:  88%|████████▊ | 226/256 [00:45<00:00, 31.76it/s, est. speed input: 78.96 toks/s, output: 1263.41 toks/s]
Processed prompts:  90%|████████▉ | 230/256 [00:45<00:00, 28.57it/s, est. speed input: 80.03 toks/s, output: 1280.52 toks/s]
Processed prompts:  91%|█████████▏| 234/256 [00:46<00:00, 26.42it/s, est. speed input: 81.10 toks/s, output: 1297.54 toks/s]
Processed prompts:  93%|█████████▎| 238/256 [00:46<00:00, 25.04it/s, est. speed input: 82.16 toks/s, output: 1314.49 toks/s]
Processed prompts:  95%|█████████▍| 242/256 [00:46<00:00, 24.21it/s, est. speed input: 83.21 toks/s, output: 1331.42 toks/s]
Processed prompts:  96%|█████████▌| 245/256 [00:46<00:00, 19.25it/s, est. speed input: 83.77 toks/s, output: 1340.26 toks/s]
Processed prompts:  97%|█████████▋| 248/256 [00:47<00:00, 16.62it/s, est. speed input: 84.33 toks/s, output: 1349.25 toks/s]
Processed prompts:  98%|█████████▊| 250/256 [00:47<00:00, 15.40it/s, est. speed input: 84.70 toks/s, output: 1355.21 toks/s]
Processed prompts:  98%|█████████▊| 252/256 [00:47<00:00, 14.22it/s, est. speed input: 85.05 toks/s, output: 1360.84 toks/s]
Processed prompts:  99%|█████████▉| 254/256 [00:47<00:00, 13.61it/s, est. speed input: 85.43 toks/s, output: 1366.81 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:47<00:00, 12.35it/s, est. speed input: 85.72 toks/s, output: 1371.60 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:47<00:00, 12.35it/s, est. speed input: 85.72 toks/s, output: 1371.60 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:47<00:00,  5.36it/s, est. speed input: 85.72 toks/s, output: 1371.60 toks/s]
[rank0]:[W127 19:57:02.719461407 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 223.9s

测试结果:
  Requests/s:   5.11
  Tokens/s:     1391.10
  Total Reqs:   256
  Elapsed:      50.06s

  [Decode 分析]
  Total Decode Tokens:  65536
  Decode Tokens/s:      1309.27

============================================================
[4/4] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-14B-INT8                                │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 8192 (= 512 x 16)
│   M_decode      = 512
│   batched_tokens = 512 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 512
│   --max-num-seqs           = 512
│   --max-model-len          = 272
│   --max-num-batched-tokens = 512
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 19:57:26 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 19:57:26 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3022506) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3022506) WARNING 01-27 19:59:43 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 5.39 requests/s, 1465.18 total tokens/s, 1378.99 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072


─── STDERR ───
[2026-01-27 19:57:26] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 19:57:26] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:57:26] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-27 19:57:26] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:57:26] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:57:26] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:57:26] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:57:26] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:57:26] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:57:26] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 19:57:26] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 19:57:26] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 19:57:26] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 19:57:26] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 19:57:30] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 19:57:30] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:57:30] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-INT8
[2026-01-27 19:57:30] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:57:30] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:57:30] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:57:30] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:57:30] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-INT8
[2026-01-27 19:57:30] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-INT8'
[2026-01-27 19:57:30] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 19:57:30] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 19:57:30] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 19:57:30] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 19:57:30] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3022506) [2026-01-27 19:57:31] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3022506) [2026-01-27 19:57:31] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3022506) [2026-01-27 19:57:31] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3022506) [2026-01-27 19:57:31] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3022506) [2026-01-27 19:57:31] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Qwen2.5-14B-INT8
(EngineCore_DP0 pid=3022506) [2026-01-27 19:57:31] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=3022506) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3022506) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:08<00:25,  8.59s/it]
(EngineCore_DP0 pid=3022506) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:41<00:45, 22.99s/it]
(EngineCore_DP0 pid=3022506) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [01:17<00:28, 28.90s/it]
(EngineCore_DP0 pid=3022506) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:59<00:00, 33.99s/it]
(EngineCore_DP0 pid=3022506) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:59<00:00, 29.85s/it]
(EngineCore_DP0 pid=3022506) 
(EngineCore_DP0 pid=3022506) [2026-01-27 19:59:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [7168, 8192] -> 1D uint8
(EngineCore_DP0 pid=3022506) [2026-01-27 19:59:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 44040192 bytes
(EngineCore_DP0 pid=3022506) [2026-01-27 19:59:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 8192] -> 1D uint8
(EngineCore_DP0 pid=3022506) [2026-01-27 19:59:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 31457280 bytes
(EngineCore_DP0 pid=3022506) [2026-01-27 19:59:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [27648, 8192] -> 1D uint8
(EngineCore_DP0 pid=3022506) [2026-01-27 19:59:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 169869312 bytes
(EngineCore_DP0 pid=3022506) [2026-01-27 19:59:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [5120, 22144] -> 1D uint8
(EngineCore_DP0 pid=3022506) [2026-01-27 19:59:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 85032960 bytes
(EngineCore_DP0 pid=3022506) 2026-01-27 19:59:41,889 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3022506) 2026-01-27 19:59:41,960 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:  33%|███▎      | 168/512 [00:00<00:00, 1679.82it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 3356.82it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [01:06<9:28:56, 66.80s/it, est. speed input: 0.24 toks/s, output: 3.83 toks/s]
Processed prompts:   0%|          | 2/512 [01:07<3:57:30, 27.94s/it, est. speed input: 0.47 toks/s, output: 7.58 toks/s]
Processed prompts:   6%|▋         | 33/512 [01:08<08:25,  1.05s/it, est. speed input: 7.74 toks/s, output: 123.79 toks/s]
Processed prompts:  12%|█▏        | 63/512 [01:08<03:28,  2.15it/s, est. speed input: 14.63 toks/s, output: 234.04 toks/s]
Processed prompts:  18%|█▊        | 91/512 [01:09<01:55,  3.65it/s, est. speed input: 20.94 toks/s, output: 334.98 toks/s]
Processed prompts:  23%|██▎       | 117/512 [01:10<01:12,  5.47it/s, est. speed input: 26.68 toks/s, output: 426.82 toks/s]
Processed prompts:  28%|██▊       | 141/512 [01:10<00:47,  7.87it/s, est. speed input: 32.03 toks/s, output: 512.42 toks/s]
Processed prompts:  32%|███▏      | 164/512 [01:10<00:31, 10.93it/s, est. speed input: 37.11 toks/s, output: 593.83 toks/s]
Processed prompts:  36%|███▌      | 185/512 [01:10<00:22, 14.57it/s, est. speed input: 41.72 toks/s, output: 667.55 toks/s]
Processed prompts:  40%|████      | 205/512 [01:11<00:16, 18.97it/s, est. speed input: 46.08 toks/s, output: 737.25 toks/s]
Processed prompts:  44%|████▍     | 224/512 [01:11<00:11, 24.10it/s, est. speed input: 50.19 toks/s, output: 803.01 toks/s]
Processed prompts:  47%|████▋     | 242/512 [01:11<00:09, 29.80it/s, est. speed input: 54.05 toks/s, output: 864.86 toks/s]
Processed prompts:  50%|█████     | 258/512 [01:11<00:07, 35.26it/s, est. speed input: 57.46 toks/s, output: 919.30 toks/s]
Processed prompts:  53%|█████▎    | 273/512 [01:12<00:05, 40.60it/s, est. speed input: 60.62 toks/s, output: 969.97 toks/s]
Processed prompts:  56%|█████▋    | 288/512 [01:12<00:04, 47.38it/s, est. speed input: 63.80 toks/s, output: 1020.82 toks/s]
Processed prompts:  59%|█████▉    | 302/512 [01:12<00:03, 52.57it/s, est. speed input: 66.73 toks/s, output: 1067.72 toks/s]
Processed prompts:  62%|██████▏   | 315/512 [01:12<00:03, 56.71it/s, est. speed input: 69.44 toks/s, output: 1110.96 toks/s]
Processed prompts:  64%|██████▍   | 327/512 [01:12<00:03, 59.68it/s, est. speed input: 71.91 toks/s, output: 1150.58 toks/s]
Processed prompts:  66%|██████▌   | 338/512 [01:12<00:02, 60.90it/s, est. speed input: 74.16 toks/s, output: 1186.52 toks/s]
Processed prompts:  68%|██████▊   | 348/512 [01:13<00:02, 62.68it/s, est. speed input: 76.20 toks/s, output: 1219.20 toks/s]
Processed prompts:  70%|██████▉   | 358/512 [01:13<00:02, 62.75it/s, est. speed input: 78.22 toks/s, output: 1251.51 toks/s]
Processed prompts:  72%|███████▏  | 367/512 [01:13<00:02, 63.04it/s, est. speed input: 80.03 toks/s, output: 1280.52 toks/s]
Processed prompts:  73%|███████▎  | 376/512 [01:13<00:02, 62.03it/s, est. speed input: 81.83 toks/s, output: 1309.21 toks/s]
Processed prompts:  75%|███████▌  | 384/512 [01:13<00:02, 61.25it/s, est. speed input: 83.41 toks/s, output: 1334.61 toks/s]
Processed prompts:  77%|███████▋  | 392/512 [01:13<00:02, 57.19it/s, est. speed input: 84.96 toks/s, output: 1359.35 toks/s]
Processed prompts:  78%|███████▊  | 399/512 [01:13<00:01, 56.70it/s, est. speed input: 86.33 toks/s, output: 1381.26 toks/s]
Processed prompts:  79%|███████▉  | 406/512 [01:14<00:01, 55.12it/s, est. speed input: 87.68 toks/s, output: 1402.89 toks/s]
Processed prompts:  80%|████████  | 412/512 [01:14<00:01, 53.74it/s, est. speed input: 88.83 toks/s, output: 1421.31 toks/s]
Processed prompts:  82%|████████▏ | 418/512 [01:14<00:01, 51.32it/s, est. speed input: 89.97 toks/s, output: 1439.45 toks/s]
Processed prompts:  83%|████████▎ | 424/512 [01:14<00:01, 51.32it/s, est. speed input: 91.11 toks/s, output: 1457.82 toks/s]
Processed prompts:  84%|████████▍ | 430/512 [01:14<00:01, 51.87it/s, est. speed input: 92.26 toks/s, output: 1476.22 toks/s]
Processed prompts:  85%|████████▌ | 436/512 [01:14<00:01, 48.69it/s, est. speed input: 93.37 toks/s, output: 1493.97 toks/s]
Processed prompts:  86%|████████▌ | 441/512 [01:14<00:01, 47.61it/s, est. speed input: 94.30 toks/s, output: 1508.84 toks/s]
Processed prompts:  87%|████████▋ | 446/512 [01:15<00:01, 36.63it/s, est. speed input: 95.09 toks/s, output: 1521.40 toks/s]
Processed prompts:  88%|████████▊ | 451/512 [01:15<00:01, 36.67it/s, est. speed input: 95.98 toks/s, output: 1535.67 toks/s]
Processed prompts:  89%|████████▉ | 455/512 [01:15<00:01, 37.06it/s, est. speed input: 96.70 toks/s, output: 1547.14 toks/s]
Processed prompts:  90%|████████▉ | 459/512 [01:15<00:01, 30.12it/s, est. speed input: 97.28 toks/s, output: 1556.49 toks/s]
Processed prompts:  91%|█████████ | 465/512 [01:15<00:01, 29.42it/s, est. speed input: 98.28 toks/s, output: 1572.41 toks/s]
Processed prompts:  92%|█████████▏| 470/512 [01:15<00:01, 28.15it/s, est. speed input: 99.08 toks/s, output: 1585.22 toks/s]
Processed prompts:  92%|█████████▏| 473/512 [01:16<00:01, 28.38it/s, est. speed input: 99.57 toks/s, output: 1593.19 toks/s]
Processed prompts:  93%|█████████▎| 476/512 [01:16<00:01, 23.95it/s, est. speed input: 99.95 toks/s, output: 1599.23 toks/s]
Processed prompts:  94%|█████████▍| 480/512 [01:16<00:01, 23.13it/s, est. speed input: 100.55 toks/s, output: 1608.73 toks/s]
Processed prompts:  94%|█████████▍| 483/512 [01:16<00:01, 20.45it/s, est. speed input: 100.91 toks/s, output: 1614.56 toks/s]
Processed prompts:  95%|█████████▍| 486/512 [01:16<00:01, 16.67it/s, est. speed input: 101.17 toks/s, output: 1618.76 toks/s]
Processed prompts:  95%|█████████▌| 488/512 [01:17<00:01, 15.04it/s, est. speed input: 101.34 toks/s, output: 1621.50 toks/s]
Processed prompts:  96%|█████████▌| 490/512 [01:17<00:01, 13.85it/s, est. speed input: 101.52 toks/s, output: 1624.25 toks/s]
Processed prompts:  96%|█████████▌| 492/512 [01:17<00:01, 13.14it/s, est. speed input: 101.70 toks/s, output: 1627.14 toks/s]
Processed prompts:  96%|█████████▋| 494/512 [01:17<00:01, 12.62it/s, est. speed input: 101.88 toks/s, output: 1630.03 toks/s]
Processed prompts:  97%|█████████▋| 496/512 [01:17<00:01, 12.26it/s, est. speed input: 102.06 toks/s, output: 1632.92 toks/s]
Processed prompts:  97%|█████████▋| 497/512 [01:29<00:01, 12.26it/s, est. speed input: 102.15 toks/s, output: 1634.43 toks/s]
Processed prompts:  97%|█████████▋| 498/512 [01:34<00:33,  2.37s/it, est. speed input: 84.33 toks/s, output: 1349.21 toks/s] 
Processed prompts:  98%|█████████▊| 501/512 [01:34<00:16,  1.51s/it, est. speed input: 84.68 toks/s, output: 1354.84 toks/s]
Processed prompts: 100%|█████████▉| 511/512 [01:34<00:00,  1.80it/s, est. speed input: 86.21 toks/s, output: 1379.42 toks/s]
Processed prompts: 100%|██████████| 512/512 [01:34<00:00,  1.80it/s, est. speed input: 86.38 toks/s, output: 1382.12 toks/s]
Processed prompts: 100%|██████████| 512/512 [01:34<00:00,  5.40it/s, est. speed input: 86.38 toks/s, output: 1382.12 toks/s]
[rank0]:[W127 20:01:18.888339858 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 246.6s

测试结果:
  Requests/s:   5.39
  Tokens/s:     1465.18
  Total Reqs:   512
  Elapsed:      95.05s

  [Decode 分析]
  Total Decode Tokens:  131072
  Decode Tokens/s:      1378.99


------------------------------------------------------------
  生成 CSV: Qwen2.5-14B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_INT8_py312_cu129_aarch64/cusparselt/2_10/Qwen2.5-14B-INT8_decode.csv

预览:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
64,16,64,64,256,256,2.3179,630.4753,27.6109
128,16,128,128,256,256,3.8951,1059.4772,32.8615
256,16,256,256,256,256,5.1144,1391.1043,50.0552
512,16,512,512,256,256,5.3867,1465.1763,95.0493

------------------------------------------------------------

[INFO] 完成: 4 成功, 0 失败


============================================================
  Benchmark 完成!
============================================================


总计: 20 成功, 0 失败
============================================================
