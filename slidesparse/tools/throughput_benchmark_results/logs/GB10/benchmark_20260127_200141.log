======================================================================
SlideSparse vLLM Throughput Benchmark Log
Created: 2026-01-27 20:01:41
======================================================================

原始命令:
  /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model qwen2.5-14b-fp8 --backend cublaslt,cusparselt --stage decode --sparsity 2_4,2_6,2_8,2_10 --M 64,128,256,512

命令行参数:
  --model: qwen2.5-14b-fp8
  --backend: cublaslt,cusparselt
  --sparsity: 2_4,2_6,2_8,2_10
  --stage: decode
  --M: 64,128,256,512
  --N: None
  --inner-32: False
  --eager: False
  --gpu-id: 0
  --gpu-mem: 0.8
  --dry-run: False
  --list-models: False

硬件信息:
  GPU: GB10
  Compute Capability: cc121
  VRAM: 119.7 GB
  CUDA: 12.9
  Python: py312

Backend 环境变量 (初始状态):
  DISABLE_SLIDESPARSE: 未设置
  USE_CUBLASLT: 未设置
  USE_CUSPARSELT: 未设置
  SPARSITY: 未设置
  INNER_DTYPE_32: 未设置

======================================================================


============================================================
  Qwen2.5-14B-FP8 | cuBLASLt | decode
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints/Qwen2.5-14B-FP8
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cublaslt

============================================================
[1/4] 测试 M=64
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-14B-FP8                                 │
│ Backend:  cuBLASLt                                        │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 64
│   M_prefill     = 1024 (= 64 x 16)
│   M_decode      = 64
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 64
│   --max-num-seqs           = 64
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 20:01:45 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 20:01:45 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3026336) WARNING 01-27 20:03:21 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 2.39 requests/s, 648.84 total tokens/s, 610.67 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384


─── STDERR ───
[2026-01-27 20:01:45] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 20:01:45] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:01:45] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-27 20:01:45] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:01:45] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:01:45] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:01:45] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:01:45] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:01:45] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:01:45] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 20:01:45] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 20:01:45] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 20:01:45] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 20:01:45] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 20:01:48] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 20:01:48] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:01:48] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-27 20:01:48] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:01:48] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:01:48] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:01:48] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:01:48] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:01:48] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:01:48] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 20:01:48] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 20:01:48] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 20:01:48] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 20:01:48] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3026336) [2026-01-27 20:01:50] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=3026336) [2026-01-27 20:01:50] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3026336) [2026-01-27 20:01:50] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=3026336) [2026-01-27 20:01:50] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=3026336) [2026-01-27 20:01:50] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=3026336) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3026336) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:08<00:25,  8.40s/it]
(EngineCore_DP0 pid=3026336) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:33<00:36, 18.42s/it]
(EngineCore_DP0 pid=3026336) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:55<00:19, 19.74s/it]
(EngineCore_DP0 pid=3026336) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:22<00:00, 22.61s/it]
(EngineCore_DP0 pid=3026336) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:22<00:00, 20.54s/it]
(EngineCore_DP0 pid=3026336) 
(EngineCore_DP0 pid=3026336) 2026-01-27 20:03:20,355 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3026336) 2026-01-27 20:03:20,405 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 1/64 [00:00<00:08,  7.02it/s]
Adding requests:  11%|█         | 7/64 [00:00<00:01, 31.67it/s]
Adding requests:  38%|███▊      | 24/64 [00:00<00:00, 87.29it/s]
Adding requests: 100%|██████████| 64/64 [00:00<00:00, 157.55it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 1/64 [00:25<27:09, 25.87s/it, est. speed input: 0.62 toks/s, output: 9.89 toks/s]
Processed prompts:  28%|██▊       | 18/64 [00:26<00:47,  1.03s/it, est. speed input: 11.05 toks/s, output: 176.80 toks/s]
Processed prompts:  75%|███████▌  | 48/64 [00:26<00:04,  3.26it/s, est. speed input: 29.26 toks/s, output: 468.14 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:26<00:00,  3.26it/s, est. speed input: 38.89 toks/s, output: 622.21 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:26<00:00,  2.43it/s, est. speed input: 38.89 toks/s, output: 622.21 toks/s]
[rank0]:[W127 20:03:48.392420629 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 140.2s

测试结果:
  Requests/s:   2.39
  Tokens/s:     648.84
  Total Reqs:   64
  Elapsed:      26.83s

  [Decode 分析]
  Total Decode Tokens:  16384
  Decode Tokens/s:      610.67

============================================================
[2/4] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-14B-FP8                                 │
│ Backend:  cuBLASLt                                        │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 2048 (= 128 x 16)
│   M_decode      = 128
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 128
│   --max-num-seqs           = 128
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 20:04:07 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 20:04:07 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3028489) WARNING 01-27 20:05:42 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 4.05 requests/s, 1101.25 total tokens/s, 1036.47 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768


─── STDERR ───
[2026-01-27 20:04:07] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 20:04:07] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:04:07] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-27 20:04:07] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:04:07] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:04:07] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:04:07] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:04:07] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:04:07] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:04:07] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 20:04:07] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 20:04:07] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 20:04:07] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 20:04:07] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 20:04:10] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 20:04:10] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:04:10] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-27 20:04:10] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:04:10] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:04:10] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:04:10] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:04:10] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:04:10] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:04:10] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 20:04:10] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 20:04:10] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 20:04:10] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 20:04:10] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3028489) [2026-01-27 20:04:11] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=3028489) [2026-01-27 20:04:11] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3028489) [2026-01-27 20:04:11] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=3028489) [2026-01-27 20:04:11] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=3028489) [2026-01-27 20:04:11] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=3028489) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3028489) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:08<00:24,  8.31s/it]
(EngineCore_DP0 pid=3028489) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:35<00:38, 19.20s/it]
(EngineCore_DP0 pid=3028489) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:56<00:20, 20.26s/it]
(EngineCore_DP0 pid=3028489) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:23<00:00, 22.83s/it]
(EngineCore_DP0 pid=3028489) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:23<00:00, 20.86s/it]
(EngineCore_DP0 pid=3028489) 
(EngineCore_DP0 pid=3028489) 2026-01-27 20:05:41,830 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3028489) 2026-01-27 20:05:41,867 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 3972.41it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:30<1:04:22, 30.41s/it, est. speed input: 0.53 toks/s, output: 8.42 toks/s]
Processed prompts:   2%|▏         | 2/128 [00:30<26:27, 12.60s/it, est. speed input: 1.05 toks/s, output: 16.76 toks/s] 
Processed prompts:  14%|█▍        | 18/128 [00:30<01:36,  1.14it/s, est. speed input: 9.39 toks/s, output: 150.22 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:30<00:37,  2.53it/s, est. speed input: 17.15 toks/s, output: 274.35 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:30<00:17,  4.46it/s, est. speed input: 24.85 toks/s, output: 397.61 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:31<00:09,  6.95it/s, est. speed input: 31.99 toks/s, output: 511.78 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:31<00:05, 10.09it/s, est. speed input: 38.56 toks/s, output: 617.00 toks/s]
Processed prompts:  69%|██████▉   | 88/128 [00:31<00:02, 14.31it/s, est. speed input: 45.10 toks/s, output: 721.60 toks/s]
Processed prompts:  84%|████████▍ | 108/128 [00:31<00:00, 22.24it/s, est. speed input: 55.01 toks/s, output: 880.20 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:31<00:00, 31.22it/s, est. speed input: 64.34 toks/s, output: 1029.48 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:31<00:00, 31.22it/s, est. speed input: 64.85 toks/s, output: 1037.58 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:31<00:00,  4.05it/s, est. speed input: 64.85 toks/s, output: 1037.58 toks/s]
[rank0]:[W127 20:06:14.500307975 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 136.0s

测试结果:
  Requests/s:   4.05
  Tokens/s:     1101.25
  Total Reqs:   128
  Elapsed:      31.61s

  [Decode 分析]
  Total Decode Tokens:  32768
  Decode Tokens/s:      1036.47

============================================================
[3/4] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-14B-FP8                                 │
│ Backend:  cuBLASLt                                        │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 4096 (= 256 x 16)
│   M_decode      = 256
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 256
│   --max-num-seqs           = 256
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 20:06:23 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 20:06:23 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3030558) WARNING 01-27 20:08:05 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 5.45 requests/s, 1481.11 total tokens/s, 1393.99 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536


─── STDERR ───
[2026-01-27 20:06:23] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 20:06:23] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:06:23] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-27 20:06:23] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:06:23] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:06:23] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:06:23] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:06:23] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:06:23] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:06:23] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 20:06:23] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 20:06:23] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 20:06:23] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 20:06:23] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 20:06:26] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 20:06:26] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:06:26] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-27 20:06:26] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:06:26] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:06:26] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:06:26] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:06:26] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:06:26] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:06:26] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 20:06:26] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 20:06:26] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 20:06:26] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 20:06:26] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3030558) [2026-01-27 20:06:27] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=3030558) [2026-01-27 20:06:27] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3030558) [2026-01-27 20:06:27] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=3030558) [2026-01-27 20:06:27] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=3030558) [2026-01-27 20:06:27] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=3030558) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3030558) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:08<00:25,  8.45s/it]
(EngineCore_DP0 pid=3030558) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:35<00:38, 19.16s/it]
(EngineCore_DP0 pid=3030558) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:56<00:20, 20.13s/it]
(EngineCore_DP0 pid=3030558) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:23<00:00, 22.82s/it]
(EngineCore_DP0 pid=3030558) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:23<00:00, 20.83s/it]
(EngineCore_DP0 pid=3030558) 
(EngineCore_DP0 pid=3030558) 2026-01-27 20:07:58,370 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3030558) 2026-01-27 20:07:58,602 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   0%|          | 1/256 [00:00<00:53,  4.78it/s]
Adding requests:   1%|          | 3/256 [00:00<00:26,  9.42it/s]
Adding requests:   2%|▏         | 5/256 [00:00<00:19, 12.64it/s]
Adding requests:   3%|▎         | 8/256 [00:00<00:14, 17.00it/s]
Adding requests:   5%|▍         | 12/256 [00:00<00:10, 22.29it/s]
Adding requests:   7%|▋         | 18/256 [00:00<00:07, 31.80it/s]
Adding requests:  10%|█         | 26/256 [00:00<00:05, 45.11it/s]
Adding requests:  14%|█▍        | 36/256 [00:01<00:03, 60.35it/s]
Adding requests:  20%|█▉        | 50/256 [00:01<00:02, 81.25it/s]
Adding requests:  25%|██▌       | 65/256 [00:01<00:01, 100.07it/s]
Adding requests:  31%|███▏      | 80/256 [00:01<00:01, 114.05it/s]
Adding requests:  38%|███▊      | 98/256 [00:01<00:01, 132.41it/s]
Adding requests:  48%|████▊     | 122/256 [00:01<00:00, 163.46it/s]
Adding requests:  57%|█████▋    | 145/256 [00:01<00:00, 182.95it/s]
Adding requests:  64%|██████▍   | 164/256 [00:01<00:00, 125.93it/s]
Adding requests:  77%|███████▋  | 196/256 [00:01<00:00, 168.87it/s]
Adding requests:  92%|█████████▏| 236/256 [00:02<00:00, 224.18it/s]
Adding requests: 100%|██████████| 256/256 [00:02<00:00, 119.14it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:38<2:41:35, 38.02s/it, est. speed input: 0.42 toks/s, output: 6.73 toks/s]
Processed prompts:   1%|          | 2/256 [00:38<1:06:42, 15.76s/it, est. speed input: 0.84 toks/s, output: 13.41 toks/s]
Processed prompts:   2%|▏         | 4/256 [00:38<24:55,  5.93s/it, est. speed input: 1.67 toks/s, output: 26.69 toks/s]  
Processed prompts:   2%|▏         | 6/256 [00:38<13:13,  3.18s/it, est. speed input: 2.49 toks/s, output: 39.83 toks/s]
Processed prompts:   3%|▎         | 8/256 [00:38<07:58,  1.93s/it, est. speed input: 3.30 toks/s, output: 52.88 toks/s]
Processed prompts:   4%|▍         | 11/256 [00:38<04:20,  1.06s/it, est. speed input: 4.52 toks/s, output: 72.38 toks/s]
Processed prompts:   5%|▌         | 14/256 [00:39<02:40,  1.51it/s, est. speed input: 5.73 toks/s, output: 91.70 toks/s]
Processed prompts:   8%|▊         | 20/256 [00:39<01:16,  3.07it/s, est. speed input: 8.15 toks/s, output: 130.40 toks/s]
Processed prompts:  11%|█         | 28/256 [00:39<00:39,  5.81it/s, est. speed input: 11.36 toks/s, output: 181.82 toks/s]
Processed prompts:  15%|█▍        | 38/256 [00:39<00:21, 10.14it/s, est. speed input: 15.36 toks/s, output: 245.69 toks/s]
Processed prompts:  20%|██        | 52/256 [00:39<00:11, 17.70it/s, est. speed input: 20.92 toks/s, output: 334.78 toks/s]
Processed prompts:  25%|██▌       | 65/256 [00:39<00:07, 25.79it/s, est. speed input: 26.06 toks/s, output: 416.94 toks/s]
Processed prompts:  30%|███       | 78/256 [00:40<00:05, 34.37it/s, est. speed input: 31.15 toks/s, output: 498.41 toks/s]
Processed prompts:  35%|███▌      | 90/256 [00:40<00:03, 42.36it/s, est. speed input: 35.81 toks/s, output: 573.03 toks/s]
Processed prompts:  39%|███▉      | 101/256 [00:40<00:03, 48.35it/s, est. speed input: 40.04 toks/s, output: 640.66 toks/s]
Processed prompts:  43%|████▎     | 111/256 [00:40<00:02, 54.11it/s, est. speed input: 43.87 toks/s, output: 701.87 toks/s]
Processed prompts:  47%|████▋     | 121/256 [00:40<00:02, 58.12it/s, est. speed input: 47.65 toks/s, output: 762.46 toks/s]
Processed prompts:  51%|█████     | 130/256 [00:40<00:02, 60.92it/s, est. speed input: 51.04 toks/s, output: 816.60 toks/s]
Processed prompts:  54%|█████▍    | 138/256 [00:40<00:01, 61.67it/s, est. speed input: 54.01 toks/s, output: 864.20 toks/s]
Processed prompts:  57%|█████▋    | 146/256 [00:41<00:01, 62.38it/s, est. speed input: 56.97 toks/s, output: 911.53 toks/s]
Processed prompts:  60%|█████▉    | 153/256 [00:41<00:01, 60.00it/s, est. speed input: 59.51 toks/s, output: 952.21 toks/s]
Processed prompts:  62%|██████▎   | 160/256 [00:41<00:01, 61.17it/s, est. speed input: 62.07 toks/s, output: 993.17 toks/s]
Processed prompts:  65%|██████▌   | 167/256 [00:41<00:01, 60.77it/s, est. speed input: 64.60 toks/s, output: 1033.68 toks/s]
Processed prompts:  68%|██████▊   | 174/256 [00:41<00:01, 61.14it/s, est. speed input: 67.13 toks/s, output: 1074.08 toks/s]
Processed prompts:  71%|███████   | 181/256 [00:41<00:01, 61.09it/s, est. speed input: 69.64 toks/s, output: 1114.21 toks/s]
Processed prompts:  73%|███████▎  | 188/256 [00:41<00:01, 61.39it/s, est. speed input: 72.14 toks/s, output: 1154.17 toks/s]
Processed prompts:  76%|███████▌  | 195/256 [00:41<00:01, 48.83it/s, est. speed input: 74.44 toks/s, output: 1191.03 toks/s]
Processed prompts:  79%|███████▊  | 201/256 [00:42<00:01, 50.65it/s, est. speed input: 76.54 toks/s, output: 1224.57 toks/s]
Processed prompts:  81%|████████  | 207/256 [00:42<00:01, 42.04it/s, est. speed input: 78.43 toks/s, output: 1254.95 toks/s]
Processed prompts:  83%|████████▎ | 212/256 [00:42<00:01, 43.75it/s, est. speed input: 80.14 toks/s, output: 1282.22 toks/s]
Processed prompts:  85%|████████▍ | 217/256 [00:42<00:01, 36.63it/s, est. speed input: 81.64 toks/s, output: 1306.30 toks/s]
Processed prompts:  87%|████████▋ | 222/256 [00:42<00:01, 32.53it/s, est. speed input: 83.13 toks/s, output: 1330.14 toks/s]
Processed prompts:  89%|████████▉ | 228/256 [00:42<00:00, 32.41it/s, est. speed input: 85.01 toks/s, output: 1360.15 toks/s]
Processed prompts:  91%|█████████ | 232/256 [00:43<00:00, 28.99it/s, est. speed input: 86.13 toks/s, output: 1378.04 toks/s]
Processed prompts:  92%|█████████▏| 236/256 [00:43<00:00, 26.84it/s, est. speed input: 87.24 toks/s, output: 1395.89 toks/s]
Processed prompts:  94%|█████████▍| 240/256 [00:43<00:00, 25.52it/s, est. speed input: 88.36 toks/s, output: 1413.72 toks/s]
Processed prompts:  95%|█████████▍| 243/256 [00:43<00:00, 23.12it/s, est. speed input: 89.11 toks/s, output: 1425.71 toks/s]
Processed prompts:  96%|█████████▌| 246/256 [00:43<00:00, 18.56it/s, est. speed input: 89.67 toks/s, output: 1434.65 toks/s]
Processed prompts:  97%|█████████▋| 249/256 [00:44<00:00, 16.12it/s, est. speed input: 90.23 toks/s, output: 1443.67 toks/s]
Processed prompts:  98%|█████████▊| 251/256 [00:44<00:00, 15.05it/s, est. speed input: 90.61 toks/s, output: 1449.70 toks/s]
Processed prompts:  99%|█████████▉| 253/256 [00:44<00:00, 14.27it/s, est. speed input: 90.99 toks/s, output: 1455.79 toks/s]
Processed prompts: 100%|█████████▉| 255/256 [00:44<00:00, 13.69it/s, est. speed input: 91.37 toks/s, output: 1461.87 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:44<00:00, 13.69it/s, est. speed input: 91.51 toks/s, output: 1464.20 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:44<00:00,  5.72it/s, est. speed input: 91.51 toks/s, output: 1464.20 toks/s]
[rank0]:[W127 20:08:54.069755230 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 172.4s

测试结果:
  Requests/s:   5.45
  Tokens/s:     1481.11
  Total Reqs:   256
  Elapsed:      47.01s

  [Decode 分析]
  Total Decode Tokens:  65536
  Decode Tokens/s:      1393.99

============================================================
[4/4] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-14B-FP8                                 │
│ Backend:  cuBLASLt                                        │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 8192 (= 512 x 16)
│   M_decode      = 512
│   batched_tokens = 512 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 512
│   --max-num-seqs           = 512
│   --max-model-len          = 272
│   --max-num-batched-tokens = 512
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 20:09:13 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 20:09:13 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3033132) WARNING 01-27 20:10:49 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 5.63 requests/s, 1532.21 total tokens/s, 1442.08 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072


─── STDERR ───
[2026-01-27 20:09:13] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 20:09:13] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:09:13] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-27 20:09:13] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:09:13] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:09:13] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:09:13] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:09:13] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:09:13] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:09:13] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 20:09:13] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 20:09:13] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 20:09:13] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 20:09:13] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 20:09:17] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 20:09:17] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:09:17] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-27 20:09:17] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:09:17] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:09:17] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:09:17] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:09:17] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:09:17] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:09:17] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 20:09:17] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 20:09:17] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 20:09:17] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 20:09:17] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3033132) [2026-01-27 20:09:17] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=3033132) [2026-01-27 20:09:18] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3033132) [2026-01-27 20:09:18] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=3033132) [2026-01-27 20:09:18] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=3033132) [2026-01-27 20:09:18] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=3033132) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3033132) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:08<00:25,  8.43s/it]
(EngineCore_DP0 pid=3033132) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:34<00:38, 19.05s/it]
(EngineCore_DP0 pid=3033132) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:56<00:20, 20.08s/it]
(EngineCore_DP0 pid=3033132) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:23<00:00, 22.81s/it]
(EngineCore_DP0 pid=3033132) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:23<00:00, 20.80s/it]
(EngineCore_DP0 pid=3033132) 
(EngineCore_DP0 pid=3033132) 2026-01-27 20:10:48,168 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3033132) 2026-01-27 20:10:48,309 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   1%|▏         | 7/512 [00:00<00:07, 66.51it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 2778.16it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [01:03<8:59:45, 63.38s/it, est. speed input: 0.25 toks/s, output: 4.04 toks/s]
Processed prompts:   0%|          | 2/512 [01:04<3:45:31, 26.53s/it, est. speed input: 0.50 toks/s, output: 7.99 toks/s]
Processed prompts:   6%|▋         | 33/512 [01:04<08:00,  1.00s/it, est. speed input: 8.15 toks/s, output: 130.34 toks/s]
Processed prompts:  12%|█▏        | 63/512 [01:05<03:18,  2.26it/s, est. speed input: 15.39 toks/s, output: 246.28 toks/s]
Processed prompts:  18%|█▊        | 91/512 [01:06<01:49,  3.83it/s, est. speed input: 22.02 toks/s, output: 352.33 toks/s]
Processed prompts:  23%|██▎       | 117/512 [01:06<01:08,  5.74it/s, est. speed input: 28.06 toks/s, output: 448.96 toks/s]
Processed prompts:  28%|██▊       | 141/512 [01:06<00:44,  8.27it/s, est. speed input: 33.70 toks/s, output: 539.19 toks/s]
Processed prompts:  32%|███▏      | 164/512 [01:07<00:30, 11.54it/s, est. speed input: 39.07 toks/s, output: 625.07 toks/s]
Processed prompts:  36%|███▌      | 185/512 [01:07<00:21, 15.44it/s, est. speed input: 43.93 toks/s, output: 702.86 toks/s]
Processed prompts:  40%|████      | 205/512 [01:07<00:15, 20.20it/s, est. speed input: 48.53 toks/s, output: 776.45 toks/s]
Processed prompts:  44%|████▍     | 224/512 [01:07<00:11, 25.76it/s, est. speed input: 52.87 toks/s, output: 845.86 toks/s]
Processed prompts:  47%|████▋     | 242/512 [01:07<00:08, 32.13it/s, est. speed input: 56.96 toks/s, output: 911.30 toks/s]
Processed prompts:  50%|█████     | 258/512 [01:08<00:06, 38.05it/s, est. speed input: 60.55 toks/s, output: 968.76 toks/s]
Processed prompts:  53%|█████▎    | 273/512 [01:08<00:05, 43.77it/s, est. speed input: 63.89 toks/s, output: 1022.19 toks/s]
Processed prompts:  56%|█████▋    | 288/512 [01:08<00:04, 50.79it/s, est. speed input: 67.23 toks/s, output: 1075.75 toks/s]
Processed prompts:  59%|█████▉    | 302/512 [01:08<00:03, 57.00it/s, est. speed input: 70.34 toks/s, output: 1125.39 toks/s]
Processed prompts:  62%|██████▏   | 315/512 [01:08<00:03, 61.67it/s, est. speed input: 73.19 toks/s, output: 1171.09 toks/s]
Processed prompts:  64%|██████▍   | 327/512 [01:09<00:02, 65.66it/s, est. speed input: 75.82 toks/s, output: 1213.09 toks/s]
Processed prompts:  66%|██████▌   | 338/512 [01:09<00:02, 67.81it/s, est. speed input: 78.20 toks/s, output: 1251.24 toks/s]
Processed prompts:  68%|██████▊   | 348/512 [01:09<00:02, 68.65it/s, est. speed input: 80.35 toks/s, output: 1285.66 toks/s]
Processed prompts:  70%|██████▉   | 358/512 [01:09<00:02, 69.27it/s, est. speed input: 82.50 toks/s, output: 1319.92 toks/s]
Processed prompts:  72%|███████▏  | 367/512 [01:09<00:02, 68.35it/s, est. speed input: 84.40 toks/s, output: 1350.44 toks/s]
Processed prompts:  73%|███████▎  | 376/512 [01:09<00:01, 68.47it/s, est. speed input: 86.31 toks/s, output: 1380.96 toks/s]
Processed prompts:  75%|███████▌  | 384/512 [01:09<00:01, 66.06it/s, est. speed input: 87.98 toks/s, output: 1407.64 toks/s]
Processed prompts:  77%|███████▋  | 392/512 [01:09<00:01, 63.11it/s, est. speed input: 89.63 toks/s, output: 1434.04 toks/s]
Processed prompts:  78%|███████▊  | 399/512 [01:10<00:01, 61.20it/s, est. speed input: 91.07 toks/s, output: 1457.05 toks/s]
Processed prompts:  79%|███████▉  | 406/512 [01:10<00:01, 60.26it/s, est. speed input: 92.50 toks/s, output: 1480.05 toks/s]
Processed prompts:  81%|████████  | 413/512 [01:10<00:01, 59.73it/s, est. speed input: 93.94 toks/s, output: 1503.00 toks/s]
Processed prompts:  82%|████████▏ | 420/512 [01:10<00:01, 59.97it/s, est. speed input: 95.37 toks/s, output: 1525.97 toks/s]
Processed prompts:  83%|████████▎ | 427/512 [01:10<00:01, 60.60it/s, est. speed input: 96.81 toks/s, output: 1548.93 toks/s]
Processed prompts:  85%|████████▍ | 434/512 [01:10<00:01, 47.74it/s, est. speed input: 98.09 toks/s, output: 1569.37 toks/s]
Processed prompts:  86%|████████▌ | 440/512 [01:10<00:01, 49.10it/s, est. speed input: 99.28 toks/s, output: 1588.54 toks/s]
Processed prompts:  87%|████████▋ | 446/512 [01:11<00:01, 40.88it/s, est. speed input: 100.34 toks/s, output: 1605.39 toks/s]
Processed prompts:  88%|████████▊ | 451/512 [01:11<00:01, 42.25it/s, est. speed input: 101.31 toks/s, output: 1620.97 toks/s]
Processed prompts:  89%|████████▉ | 456/512 [01:11<00:01, 35.33it/s, est. speed input: 102.14 toks/s, output: 1634.17 toks/s]
Processed prompts:  90%|████████▉ | 460/512 [01:11<00:01, 36.24it/s, est. speed input: 102.89 toks/s, output: 1646.19 toks/s]
Processed prompts:  91%|█████████ | 464/512 [01:11<00:01, 36.91it/s, est. speed input: 103.63 toks/s, output: 1658.13 toks/s]
Processed prompts:  91%|█████████▏| 468/512 [01:11<00:01, 30.50it/s, est. speed input: 104.24 toks/s, output: 1667.86 toks/s]
Processed prompts:  92%|█████████▏| 472/512 [01:12<00:01, 26.87it/s, est. speed input: 104.85 toks/s, output: 1677.53 toks/s]
Processed prompts:  93%|█████████▎| 476/512 [01:12<00:01, 24.97it/s, est. speed input: 105.46 toks/s, output: 1687.31 toks/s]
Processed prompts:  94%|█████████▍| 480/512 [01:12<00:01, 23.83it/s, est. speed input: 106.07 toks/s, output: 1697.09 toks/s]
Processed prompts:  94%|█████████▍| 483/512 [01:12<00:01, 21.53it/s, est. speed input: 106.46 toks/s, output: 1703.39 toks/s]
Processed prompts:  95%|█████████▍| 486/512 [01:12<00:01, 17.40it/s, est. speed input: 106.72 toks/s, output: 1707.58 toks/s]
Processed prompts:  95%|█████████▌| 488/512 [01:13<00:01, 15.61it/s, est. speed input: 106.89 toks/s, output: 1710.30 toks/s]
Processed prompts:  96%|█████████▌| 490/512 [01:13<00:01, 14.41it/s, est. speed input: 107.07 toks/s, output: 1713.15 toks/s]
Processed prompts:  96%|█████████▌| 492/512 [01:13<00:01, 13.55it/s, est. speed input: 107.25 toks/s, output: 1716.01 toks/s]
Processed prompts:  96%|█████████▋| 494/512 [01:13<00:01, 12.95it/s, est. speed input: 107.43 toks/s, output: 1718.90 toks/s]
Processed prompts:  97%|█████████▋| 496/512 [01:13<00:01, 12.52it/s, est. speed input: 107.61 toks/s, output: 1721.77 toks/s]
Processed prompts:  97%|█████████▋| 497/512 [01:29<00:01, 12.52it/s, est. speed input: 107.70 toks/s, output: 1723.25 toks/s]
Processed prompts:  97%|█████████▋| 498/512 [01:30<00:32,  2.33s/it, est. speed input: 88.26 toks/s, output: 1412.15 toks/s] 
Processed prompts:  98%|█████████▊| 501/512 [01:30<00:16,  1.49s/it, est. speed input: 88.62 toks/s, output: 1417.98 toks/s]
Processed prompts: 100%|█████████▉| 511/512 [01:30<00:00,  1.82it/s, est. speed input: 90.22 toks/s, output: 1443.46 toks/s]
Processed prompts: 100%|██████████| 512/512 [01:30<00:00,  1.82it/s, est. speed input: 90.39 toks/s, output: 1446.28 toks/s]
Processed prompts: 100%|██████████| 512/512 [01:30<00:00,  5.65it/s, est. speed input: 90.39 toks/s, output: 1446.28 toks/s]
[rank0]:[W127 20:12:21.034587998 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 202.2s

测试结果:
  Requests/s:   5.63
  Tokens/s:     1532.21
  Total Reqs:   512
  Elapsed:      90.89s

  [Decode 分析]
  Total Decode Tokens:  131072
  Decode Tokens/s:      1442.08


------------------------------------------------------------
  生成 CSV: Qwen2.5-14B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cublaslt/Qwen2.5-14B-FP8_decode.csv

预览:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
64,16,64,64,256,256,2.3854,648.8417,26.8293
128,16,128,128,256,256,4.0487,1101.2543,31.6149
256,16,256,256,256,256,5.4453,1481.1125,47.0133
512,16,512,512,256,256,5.6331,1532.2120,90.8908

------------------------------------------------------------

[INFO] 完成: 4 成功, 0 失败

============================================================
  Qwen2.5-14B-FP8 | cuSPARSELt (2_4) | decode
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_4
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_4

============================================================
[1/4] 测试 M=64
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-14B-FP8                                 │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 64
│   M_prefill     = 1024 (= 64 x 16)
│   M_decode      = 64
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 64
│   --max-num-seqs           = 64
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 20:12:39 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 20:12:39 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3036314) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3036314) WARNING 01-27 20:14:18 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 3.21 requests/s, 872.17 total tokens/s, 820.86 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384


─── STDERR ───
[2026-01-27 20:12:39] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 20:12:39] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:12:39] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-27 20:12:39] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:12:39] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:12:39] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:12:39] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:12:39] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:12:39] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:12:39] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 20:12:39] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 20:12:39] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 20:12:39] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 20:12:39] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 20:12:42] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 20:12:42] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:12:42] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-27 20:12:42] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:12:42] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:12:42] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:12:42] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:12:42] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:12:42] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:12:42] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 20:12:42] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 20:12:42] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 20:12:42] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 20:12:42] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3036314) [2026-01-27 20:12:44] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3036314) [2026-01-27 20:12:44] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3036314) [2026-01-27 20:12:44] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3036314) [2026-01-27 20:12:44] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3036314) [2026-01-27 20:12:44] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=3036314) [2026-01-27 20:12:44] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3036314) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3036314) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:08<00:25,  8.43s/it]
(EngineCore_DP0 pid=3036314) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:33<00:36, 18.38s/it]
(EngineCore_DP0 pid=3036314) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:54<00:19, 19.63s/it]
(EngineCore_DP0 pid=3036314) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:22<00:00, 22.67s/it]
(EngineCore_DP0 pid=3036314) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:22<00:00, 20.56s/it]
(EngineCore_DP0 pid=3036314) 
(EngineCore_DP0 pid=3036314) [2026-01-27 20:14:07] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 5120] -> 1D uint8
(EngineCore_DP0 pid=3036314) [2026-01-27 20:14:07] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 22937600 bytes
(EngineCore_DP0 pid=3036314) [2026-01-27 20:14:07] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 5120] -> 1D uint8
(EngineCore_DP0 pid=3036314) [2026-01-27 20:14:07] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16384000 bytes
(EngineCore_DP0 pid=3036314) [2026-01-27 20:14:07] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 5120] -> 1D uint8
(EngineCore_DP0 pid=3036314) [2026-01-27 20:14:07] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 88473600 bytes
(EngineCore_DP0 pid=3036314) [2026-01-27 20:14:07] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 13824] -> 1D uint8
(EngineCore_DP0 pid=3036314) [2026-01-27 20:14:07] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 44236800 bytes
(EngineCore_DP0 pid=3036314) 2026-01-27 20:14:15,686 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3036314) 2026-01-27 20:14:15,740 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 1/64 [00:00<00:10,  6.06it/s]
Adding requests:   5%|▍         | 3/64 [00:00<00:05, 11.09it/s]
Adding requests:   9%|▉         | 6/64 [00:00<00:03, 17.16it/s]
Adding requests:  16%|█▌        | 10/64 [00:00<00:02, 23.85it/s]
Adding requests:  23%|██▎       | 15/64 [00:00<00:01, 31.37it/s]
Adding requests:  36%|███▌      | 23/64 [00:00<00:00, 46.20it/s]
Adding requests:  56%|█████▋    | 36/64 [00:00<00:00, 70.34it/s]
Adding requests:  81%|████████▏ | 52/64 [00:00<00:00, 96.43it/s]
Adding requests: 100%|██████████| 64/64 [00:00<00:00, 64.67it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 1/64 [00:18<19:08, 18.23s/it, est. speed input: 0.88 toks/s, output: 14.04 toks/s]
Processed prompts:   8%|▊         | 5/64 [00:18<02:42,  2.75s/it, est. speed input: 4.35 toks/s, output: 69.64 toks/s]
Processed prompts:  16%|█▌        | 10/64 [00:18<00:59,  1.11s/it, est. speed input: 8.63 toks/s, output: 138.15 toks/s]
Processed prompts:  27%|██▋       | 17/64 [00:18<00:24,  1.95it/s, est. speed input: 14.56 toks/s, output: 233.02 toks/s]
Processed prompts:  75%|███████▌  | 48/64 [00:18<00:01,  8.41it/s, est. speed input: 40.79 toks/s, output: 652.70 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:18<00:00,  8.41it/s, est. speed input: 54.23 toks/s, output: 867.67 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:18<00:00,  3.39it/s, est. speed input: 54.23 toks/s, output: 867.67 toks/s]
[rank0]:[W127 20:14:39.414628487 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 141.7s

测试结果:
  Requests/s:   3.21
  Tokens/s:     872.17
  Total Reqs:   64
  Elapsed:      19.96s

  [Decode 分析]
  Total Decode Tokens:  16384
  Decode Tokens/s:      820.86

============================================================
[2/4] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-14B-FP8                                 │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 2048 (= 128 x 16)
│   M_decode      = 128
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 128
│   --max-num-seqs           = 128
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 20:14:59 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 20:14:59 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3038601) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3038601) WARNING 01-27 20:16:35 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 5.43 requests/s, 1477.59 total tokens/s, 1390.68 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768


─── STDERR ───
[2026-01-27 20:14:59] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 20:14:59] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:14:59] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-27 20:14:59] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:14:59] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:14:59] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:14:59] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:14:59] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:14:59] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:14:59] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 20:14:59] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 20:14:59] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 20:14:59] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 20:14:59] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 20:15:02] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 20:15:02] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:15:02] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-27 20:15:02] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:15:02] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:15:02] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:15:02] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:15:02] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:15:02] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:15:02] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 20:15:02] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 20:15:02] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 20:15:02] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 20:15:02] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3038601) [2026-01-27 20:15:03] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3038601) [2026-01-27 20:15:03] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3038601) [2026-01-27 20:15:03] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3038601) [2026-01-27 20:15:03] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3038601) [2026-01-27 20:15:03] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=3038601) [2026-01-27 20:15:03] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3038601) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3038601) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:08<00:25,  8.56s/it]
(EngineCore_DP0 pid=3038601) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:35<00:38, 19.26s/it]
(EngineCore_DP0 pid=3038601) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:56<00:20, 20.21s/it]
(EngineCore_DP0 pid=3038601) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:23<00:00, 22.67s/it]
(EngineCore_DP0 pid=3038601) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:23<00:00, 20.77s/it]
(EngineCore_DP0 pid=3038601) 
(EngineCore_DP0 pid=3038601) [2026-01-27 20:16:28] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 5120] -> 1D uint8
(EngineCore_DP0 pid=3038601) [2026-01-27 20:16:28] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 22937600 bytes
(EngineCore_DP0 pid=3038601) [2026-01-27 20:16:28] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 5120] -> 1D uint8
(EngineCore_DP0 pid=3038601) [2026-01-27 20:16:28] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16384000 bytes
(EngineCore_DP0 pid=3038601) [2026-01-27 20:16:28] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 5120] -> 1D uint8
(EngineCore_DP0 pid=3038601) [2026-01-27 20:16:28] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 88473600 bytes
(EngineCore_DP0 pid=3038601) [2026-01-27 20:16:28] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 13824] -> 1D uint8
(EngineCore_DP0 pid=3038601) [2026-01-27 20:16:28] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 44236800 bytes
(EngineCore_DP0 pid=3038601) 2026-01-27 20:16:34,544 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3038601) 2026-01-27 20:16:34,572 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 9971.60it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:22<47:49, 22.60s/it, est. speed input: 0.71 toks/s, output: 11.33 toks/s]
Processed prompts:   5%|▍         | 6/128 [00:22<05:40,  2.79s/it, est. speed input: 4.23 toks/s, output: 67.65 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:22<00:59,  1.77it/s, est. speed input: 15.42 toks/s, output: 246.76 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:22<00:25,  3.63it/s, est. speed input: 25.82 toks/s, output: 413.06 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:23<00:07,  8.21it/s, est. speed input: 44.34 toks/s, output: 709.48 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:23<00:04, 11.36it/s, est. speed input: 53.77 toks/s, output: 860.39 toks/s]
Processed prompts:  78%|███████▊  | 100/128 [00:23<00:01, 17.83it/s, est. speed input: 68.41 toks/s, output: 1094.49 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:23<00:00, 25.24it/s, est. speed input: 81.53 toks/s, output: 1304.55 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:23<00:00, 25.24it/s, est. speed input: 86.97 toks/s, output: 1391.51 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:23<00:00,  5.44it/s, est. speed input: 86.97 toks/s, output: 1391.51 toks/s]
[rank0]:[W127 20:16:59.511859181 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 128.8s

测试结果:
  Requests/s:   5.43
  Tokens/s:     1477.59
  Total Reqs:   128
  Elapsed:      23.56s

  [Decode 分析]
  Total Decode Tokens:  32768
  Decode Tokens/s:      1390.68

============================================================
[3/4] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-14B-FP8                                 │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 4096 (= 256 x 16)
│   M_decode      = 256
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 256
│   --max-num-seqs           = 256
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 20:17:09 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 20:17:09 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3040609) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3040609) WARNING 01-27 20:18:53 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 6.81 requests/s, 1852.71 total tokens/s, 1743.73 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536


─── STDERR ───
[2026-01-27 20:17:08] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 20:17:09] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:17:09] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-27 20:17:09] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:17:09] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:17:09] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:17:09] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:17:09] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:17:09] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:17:09] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 20:17:09] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 20:17:09] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 20:17:09] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 20:17:09] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 20:17:12] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 20:17:12] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:17:12] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-27 20:17:12] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:17:12] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:17:12] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:17:12] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:17:12] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:17:12] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:17:12] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 20:17:12] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 20:17:12] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 20:17:12] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 20:17:12] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3040609) [2026-01-27 20:17:13] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3040609) [2026-01-27 20:17:13] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3040609) [2026-01-27 20:17:13] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3040609) [2026-01-27 20:17:13] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3040609) [2026-01-27 20:17:13] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=3040609) [2026-01-27 20:17:13] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3040609) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3040609) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:08<00:25,  8.39s/it]
(EngineCore_DP0 pid=3040609) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:35<00:38, 19.11s/it]
(EngineCore_DP0 pid=3040609) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:56<00:20, 20.13s/it]
(EngineCore_DP0 pid=3040609) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:23<00:00, 22.92s/it]
(EngineCore_DP0 pid=3040609) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:23<00:00, 20.89s/it]
(EngineCore_DP0 pid=3040609) 
(EngineCore_DP0 pid=3040609) [2026-01-27 20:18:38] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 5120] -> 1D uint8
(EngineCore_DP0 pid=3040609) [2026-01-27 20:18:38] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 22937600 bytes
(EngineCore_DP0 pid=3040609) [2026-01-27 20:18:38] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 5120] -> 1D uint8
(EngineCore_DP0 pid=3040609) [2026-01-27 20:18:38] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16384000 bytes
(EngineCore_DP0 pid=3040609) [2026-01-27 20:18:38] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 5120] -> 1D uint8
(EngineCore_DP0 pid=3040609) [2026-01-27 20:18:38] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 88473600 bytes
(EngineCore_DP0 pid=3040609) [2026-01-27 20:18:38] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 13824] -> 1D uint8
(EngineCore_DP0 pid=3040609) [2026-01-27 20:18:38] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 44236800 bytes
(EngineCore_DP0 pid=3040609) 2026-01-27 20:18:46,240 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3040609) 2026-01-27 20:18:46,566 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   0%|          | 1/256 [00:00<00:54,  4.70it/s]
Adding requests:   1%|          | 2/256 [00:00<00:38,  6.53it/s]
Adding requests:   2%|▏         | 5/256 [00:00<00:18, 13.63it/s]
Adding requests:   4%|▎         | 9/256 [00:00<00:12, 19.47it/s]
Adding requests:   5%|▌         | 13/256 [00:00<00:09, 24.76it/s]
Adding requests:   8%|▊         | 20/256 [00:00<00:06, 36.44it/s]
Adding requests:  12%|█▏        | 30/256 [00:00<00:04, 53.64it/s]
Adding requests:  17%|█▋        | 43/256 [00:01<00:02, 74.77it/s]
Adding requests:  23%|██▎       | 59/256 [00:01<00:01, 98.97it/s]
Adding requests:  31%|███       | 79/256 [00:01<00:01, 126.66it/s]
Adding requests:  41%|████      | 104/256 [00:01<00:00, 161.31it/s]
Adding requests:  51%|█████     | 130/256 [00:01<00:00, 187.58it/s]
Adding requests:  61%|██████    | 156/256 [00:01<00:00, 141.59it/s]
Adding requests:  75%|███████▌  | 192/256 [00:01<00:00, 187.99it/s]
Adding requests:  91%|█████████▏| 234/256 [00:01<00:00, 242.96it/s]
Adding requests: 100%|██████████| 256/256 [00:01<00:00, 131.33it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:30<2:09:02, 30.36s/it, est. speed input: 0.53 toks/s, output: 8.43 toks/s]
Processed prompts:   1%|          | 2/256 [00:30<53:16, 12.59s/it, est. speed input: 1.05 toks/s, output: 16.78 toks/s] 
Processed prompts:   2%|▏         | 5/256 [00:30<15:05,  3.61s/it, est. speed input: 2.61 toks/s, output: 41.77 toks/s]
Processed prompts:   4%|▎         | 9/256 [00:30<06:22,  1.55s/it, est. speed input: 4.68 toks/s, output: 74.83 toks/s]
Processed prompts:   5%|▍         | 12/256 [00:30<03:57,  1.03it/s, est. speed input: 6.21 toks/s, output: 99.31 toks/s]
Processed prompts:   6%|▌         | 15/256 [00:31<02:36,  1.54it/s, est. speed input: 7.72 toks/s, output: 123.56 toks/s]
Processed prompts:   7%|▋         | 19/256 [00:31<01:35,  2.47it/s, est. speed input: 9.73 toks/s, output: 155.65 toks/s]
Processed prompts:  10%|█         | 26/256 [00:31<00:48,  4.71it/s, est. speed input: 13.25 toks/s, output: 211.99 toks/s]
Processed prompts:  16%|█▌        | 41/256 [00:31<00:19, 11.09it/s, est. speed input: 20.79 toks/s, output: 332.60 toks/s]
Processed prompts:  21%|██▏       | 55/256 [00:31<00:10, 18.28it/s, est. speed input: 27.75 toks/s, output: 444.05 toks/s]
Processed prompts:  27%|██▋       | 68/256 [00:31<00:07, 25.79it/s, est. speed input: 34.15 toks/s, output: 546.35 toks/s]
Processed prompts:  31%|███▏      | 80/256 [00:31<00:05, 34.43it/s, est. speed input: 40.03 toks/s, output: 640.54 toks/s]
Processed prompts:  36%|███▌      | 92/256 [00:32<00:03, 42.36it/s, est. speed input: 45.83 toks/s, output: 733.30 toks/s]
Processed prompts:  40%|████      | 103/256 [00:32<00:03, 48.65it/s, est. speed input: 51.08 toks/s, output: 817.27 toks/s]
Processed prompts:  44%|████▍     | 113/256 [00:32<00:02, 55.19it/s, est. speed input: 55.84 toks/s, output: 893.36 toks/s]
Processed prompts:  48%|████▊     | 123/256 [00:32<00:02, 62.90it/s, est. speed input: 60.58 toks/s, output: 969.34 toks/s]
Processed prompts:  52%|█████▏    | 132/256 [00:32<00:01, 65.11it/s, est. speed input: 64.77 toks/s, output: 1036.28 toks/s]
Processed prompts:  58%|█████▊    | 148/256 [00:32<00:01, 66.75it/s, est. speed input: 72.11 toks/s, output: 1153.72 toks/s]
Processed prompts:  63%|██████▎   | 162/256 [00:33<00:01, 70.29it/s, est. speed input: 78.50 toks/s, output: 1256.04 toks/s]
Processed prompts:  68%|██████▊   | 174/256 [00:33<00:01, 70.09it/s, est. speed input: 83.88 toks/s, output: 1342.07 toks/s]
Processed prompts:  72%|███████▏  | 185/256 [00:33<00:01, 69.79it/s, est. speed input: 88.76 toks/s, output: 1420.10 toks/s]
Processed prompts:  76%|███████▌  | 195/256 [00:33<00:00, 68.04it/s, est. speed input: 93.11 toks/s, output: 1489.83 toks/s]
Processed prompts:  79%|███████▉  | 203/256 [00:33<00:00, 63.71it/s, est. speed input: 96.50 toks/s, output: 1543.95 toks/s]
Processed prompts:  82%|████████▏ | 211/256 [00:33<00:00, 60.90it/s, est. speed input: 99.86 toks/s, output: 1597.73 toks/s]
Processed prompts:  85%|████████▌ | 218/256 [00:33<00:00, 57.60it/s, est. speed input: 102.74 toks/s, output: 1643.79 toks/s]
Processed prompts:  88%|████████▊ | 224/256 [00:34<00:00, 53.84it/s, est. speed input: 105.14 toks/s, output: 1682.27 toks/s]
Processed prompts:  90%|████████▉ | 230/256 [00:34<00:00, 43.54it/s, est. speed input: 107.27 toks/s, output: 1716.27 toks/s]
Processed prompts:  92%|█████████▏| 235/256 [00:34<00:00, 42.33it/s, est. speed input: 109.19 toks/s, output: 1746.98 toks/s]
Processed prompts:  94%|█████████▍| 240/256 [00:34<00:00, 36.83it/s, est. speed input: 110.90 toks/s, output: 1774.32 toks/s]
Processed prompts:  95%|█████████▌| 244/256 [00:34<00:00, 32.38it/s, est. speed input: 112.17 toks/s, output: 1794.71 toks/s]
Processed prompts:  97%|█████████▋| 248/256 [00:35<00:00, 26.59it/s, est. speed input: 113.24 toks/s, output: 1811.79 toks/s]
Processed prompts:  98%|█████████▊| 251/256 [00:35<00:00, 23.56it/s, est. speed input: 114.01 toks/s, output: 1824.09 toks/s]
Processed prompts:  99%|█████████▉| 254/256 [00:35<00:00, 22.05it/s, est. speed input: 114.83 toks/s, output: 1837.23 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:35<00:00, 22.05it/s, est. speed input: 115.30 toks/s, output: 1844.87 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:35<00:00,  7.21it/s, est. speed input: 115.30 toks/s, output: 1844.87 toks/s]
[rank0]:[W127 20:19:31.838537969 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 165.1s

测试结果:
  Requests/s:   6.81
  Tokens/s:     1852.71
  Total Reqs:   256
  Elapsed:      37.58s

  [Decode 分析]
  Total Decode Tokens:  65536
  Decode Tokens/s:      1743.73

============================================================
[4/4] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-14B-FP8                                 │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 8192 (= 512 x 16)
│   M_decode      = 512
│   batched_tokens = 512 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 512
│   --max-num-seqs           = 512
│   --max-model-len          = 272
│   --max-num-batched-tokens = 512
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 20:19:51 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 20:19:51 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3043084) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3043084) WARNING 01-27 20:21:27 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 6.86 requests/s, 1865.01 total tokens/s, 1755.30 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072


─── STDERR ───
[2026-01-27 20:19:51] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 20:19:51] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:19:51] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-27 20:19:51] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:19:51] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:19:51] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:19:51] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:19:51] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:19:51] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:19:51] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 20:19:51] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 20:19:51] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 20:19:51] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 20:19:51] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 20:19:54] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 20:19:54] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:19:54] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-27 20:19:54] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:19:54] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:19:54] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:19:54] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:19:54] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:19:54] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:19:54] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 20:19:54] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 20:19:54] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 20:19:54] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 20:19:54] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3043084) [2026-01-27 20:19:55] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3043084) [2026-01-27 20:19:55] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3043084) [2026-01-27 20:19:55] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3043084) [2026-01-27 20:19:55] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3043084) [2026-01-27 20:19:55] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=3043084) [2026-01-27 20:19:55] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3043084) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3043084) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:08<00:25,  8.44s/it]
(EngineCore_DP0 pid=3043084) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:35<00:38, 19.19s/it]
(EngineCore_DP0 pid=3043084) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:56<00:20, 20.03s/it]
(EngineCore_DP0 pid=3043084) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:23<00:00, 22.75s/it]
(EngineCore_DP0 pid=3043084) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:23<00:00, 20.78s/it]
(EngineCore_DP0 pid=3043084) 
(EngineCore_DP0 pid=3043084) [2026-01-27 20:21:20] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 5120] -> 1D uint8
(EngineCore_DP0 pid=3043084) [2026-01-27 20:21:20] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 22937600 bytes
(EngineCore_DP0 pid=3043084) [2026-01-27 20:21:20] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 5120] -> 1D uint8
(EngineCore_DP0 pid=3043084) [2026-01-27 20:21:20] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16384000 bytes
(EngineCore_DP0 pid=3043084) [2026-01-27 20:21:20] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 5120] -> 1D uint8
(EngineCore_DP0 pid=3043084) [2026-01-27 20:21:20] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 88473600 bytes
(EngineCore_DP0 pid=3043084) [2026-01-27 20:21:20] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 13824] -> 1D uint8
(EngineCore_DP0 pid=3043084) [2026-01-27 20:21:20] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 44236800 bytes
(EngineCore_DP0 pid=3043084) 2026-01-27 20:21:26,203 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3043084) 2026-01-27 20:21:26,240 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 11886.48it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:54<7:42:03, 54.25s/it, est. speed input: 0.29 toks/s, output: 4.72 toks/s]
Processed prompts:   0%|          | 2/512 [00:54<3:13:22, 22.75s/it, est. speed input: 0.58 toks/s, output: 9.32 toks/s]
Processed prompts:   6%|▋         | 33/512 [00:55<06:52,  1.16it/s, est. speed input: 9.50 toks/s, output: 151.92 toks/s]
Processed prompts:  12%|█▏        | 63/512 [00:56<02:51,  2.62it/s, est. speed input: 17.93 toks/s, output: 286.81 toks/s]
Processed prompts:  18%|█▊        | 91/512 [00:56<01:35,  4.43it/s, est. speed input: 25.62 toks/s, output: 409.97 toks/s]
Processed prompts:  23%|██▎       | 117/512 [00:57<00:59,  6.60it/s, est. speed input: 32.61 toks/s, output: 521.84 toks/s]
Processed prompts:  28%|██▊       | 141/512 [00:57<00:39,  9.50it/s, est. speed input: 39.16 toks/s, output: 626.52 toks/s]
Processed prompts:  32%|███▏      | 164/512 [00:57<00:26, 13.22it/s, est. speed input: 45.38 toks/s, output: 726.07 toks/s]
Processed prompts:  36%|███▌      | 185/512 [00:58<00:18, 17.63it/s, est. speed input: 51.01 toks/s, output: 816.20 toks/s]
Processed prompts:  40%|████      | 205/512 [00:58<00:13, 22.98it/s, est. speed input: 56.34 toks/s, output: 901.46 toks/s]
Processed prompts:  44%|████▍     | 224/512 [00:58<00:09, 29.27it/s, est. speed input: 61.37 toks/s, output: 981.94 toks/s]
Processed prompts:  47%|████▋     | 242/512 [00:58<00:07, 36.28it/s, est. speed input: 66.10 toks/s, output: 1057.63 toks/s]
Processed prompts:  50%|█████     | 258/512 [00:58<00:05, 42.55it/s, est. speed input: 70.25 toks/s, output: 1123.96 toks/s]
Processed prompts:  53%|█████▎    | 273/512 [00:58<00:04, 49.02it/s, est. speed input: 74.12 toks/s, output: 1185.86 toks/s]
Processed prompts:  56%|█████▋    | 288/512 [00:59<00:03, 57.70it/s, est. speed input: 78.01 toks/s, output: 1248.18 toks/s]
Processed prompts:  59%|█████▉    | 302/512 [00:59<00:03, 63.87it/s, est. speed input: 81.59 toks/s, output: 1305.46 toks/s]
Processed prompts:  62%|██████▏   | 315/512 [00:59<00:02, 68.63it/s, est. speed input: 84.89 toks/s, output: 1358.24 toks/s]
Processed prompts:  64%|██████▍   | 327/512 [00:59<00:02, 72.10it/s, est. speed input: 87.91 toks/s, output: 1406.63 toks/s]
Processed prompts:  66%|██████▌   | 338/512 [00:59<00:02, 73.54it/s, est. speed input: 90.66 toks/s, output: 1450.52 toks/s]
Processed prompts:  68%|██████▊   | 348/512 [00:59<00:02, 76.08it/s, est. speed input: 93.16 toks/s, output: 1490.50 toks/s]
Processed prompts:  70%|██████▉   | 358/512 [00:59<00:02, 76.10it/s, est. speed input: 95.62 toks/s, output: 1529.97 toks/s]
Processed prompts:  72%|███████▏  | 367/512 [01:00<00:01, 78.24it/s, est. speed input: 97.85 toks/s, output: 1565.68 toks/s]
Processed prompts:  73%|███████▎  | 376/512 [01:00<00:01, 76.59it/s, est. speed input: 100.05 toks/s, output: 1600.75 toks/s]
Processed prompts:  77%|███████▋  | 392/512 [01:00<00:01, 73.10it/s, est. speed input: 103.90 toks/s, output: 1662.40 toks/s]
Processed prompts:  79%|███████▉  | 406/512 [01:00<00:01, 71.84it/s, est. speed input: 107.25 toks/s, output: 1716.04 toks/s]
Processed prompts:  82%|████████▏ | 418/512 [01:00<00:01, 67.87it/s, est. speed input: 110.06 toks/s, output: 1760.93 toks/s]
Processed prompts:  84%|████████▎ | 428/512 [01:00<00:01, 65.36it/s, est. speed input: 112.38 toks/s, output: 1798.05 toks/s]
Processed prompts:  85%|████████▍ | 435/512 [01:01<00:01, 64.02it/s, est. speed input: 113.99 toks/s, output: 1823.92 toks/s]
Processed prompts:  86%|████████▋ | 442/512 [01:01<00:01, 58.51it/s, est. speed input: 115.54 toks/s, output: 1848.58 toks/s]
Processed prompts:  88%|████████▊ | 450/512 [01:01<00:01, 52.95it/s, est. speed input: 117.26 toks/s, output: 1876.21 toks/s]
Processed prompts:  89%|████████▉ | 456/512 [01:01<00:01, 49.44it/s, est. speed input: 118.54 toks/s, output: 1896.62 toks/s]
Processed prompts:  90%|█████████ | 462/512 [01:01<00:01, 47.36it/s, est. speed input: 119.82 toks/s, output: 1917.11 toks/s]
Processed prompts:  91%|█████████▏| 468/512 [01:01<00:01, 43.86it/s, est. speed input: 121.05 toks/s, output: 1936.79 toks/s]
Processed prompts:  92%|█████████▏| 473/512 [01:01<00:00, 42.04it/s, est. speed input: 122.08 toks/s, output: 1953.22 toks/s]
Processed prompts:  93%|█████████▎| 478/512 [01:02<00:00, 35.88it/s, est. speed input: 122.97 toks/s, output: 1967.51 toks/s]
Processed prompts:  94%|█████████▍| 482/512 [01:02<00:00, 33.38it/s, est. speed input: 123.70 toks/s, output: 1979.24 toks/s]
Processed prompts:  95%|█████████▍| 486/512 [01:02<00:00, 26.42it/s, est. speed input: 124.23 toks/s, output: 1987.67 toks/s]
Processed prompts:  96%|█████████▌| 489/512 [01:02<00:00, 23.41it/s, est. speed input: 124.63 toks/s, output: 1994.06 toks/s]
Processed prompts:  96%|█████████▌| 492/512 [01:02<00:00, 21.27it/s, est. speed input: 125.03 toks/s, output: 2000.40 toks/s]
Processed prompts:  97%|█████████▋| 495/512 [01:03<00:00, 19.98it/s, est. speed input: 125.43 toks/s, output: 2006.92 toks/s]
Processed prompts:  97%|█████████▋| 498/512 [01:14<00:14,  1.01s/it, est. speed input: 107.10 toks/s, output: 1713.66 toks/s]
Processed prompts:  98%|█████████▊| 501/512 [01:14<00:08,  1.34it/s, est. speed input: 107.58 toks/s, output: 1721.32 toks/s]
Processed prompts: 100%|█████████▉| 511/512 [01:14<00:00,  3.00it/s, est. speed input: 109.56 toks/s, output: 1752.94 toks/s]
Processed prompts: 100%|██████████| 512/512 [01:14<00:00,  3.00it/s, est. speed input: 109.77 toks/s, output: 1756.36 toks/s]
Processed prompts: 100%|██████████| 512/512 [01:14<00:00,  6.86it/s, est. speed input: 109.77 toks/s, output: 1756.36 toks/s]
[rank0]:[W127 20:22:42.366219832 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 177.5s

测试结果:
  Requests/s:   6.86
  Tokens/s:     1865.01
  Total Reqs:   512
  Elapsed:      74.67s

  [Decode 分析]
  Total Decode Tokens:  131072
  Decode Tokens/s:      1755.30


------------------------------------------------------------
  生成 CSV: Qwen2.5-14B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_4/Qwen2.5-14B-FP8_decode.csv

预览:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
64,16,64,64,256,256,3.2065,872.1676,19.9595
128,16,128,128,256,256,5.4323,1477.5926,23.5627
256,16,256,256,256,256,6.8114,1852.7117,37.5838
512,16,512,512,256,256,6.8566,1865.0084,74.6720

------------------------------------------------------------

[INFO] 完成: 4 成功, 0 失败

============================================================
  Qwen2.5-14B-FP8 | cuSPARSELt (2_6) | decode
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_6
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_6

============================================================
[1/4] 测试 M=64
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-14B-FP8                                 │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 64
│   M_prefill     = 1024 (= 64 x 16)
│   M_decode      = 64
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 64
│   --max-num-seqs           = 64
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 20:22:52 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 20:22:52 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3045784) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3045784) WARNING 01-27 20:25:03 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 2.59 requests/s, 703.65 total tokens/s, 662.25 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384


─── STDERR ───
[2026-01-27 20:22:52] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 20:22:52] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:22:52] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-27 20:22:52] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:22:52] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:22:52] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:22:52] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:22:52] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:22:52] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:22:52] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 20:22:52] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 20:22:52] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 20:22:52] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 20:22:52] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 20:22:56] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 20:22:56] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:22:56] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-27 20:22:56] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:22:56] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:22:56] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:22:56] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:22:56] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:22:56] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:22:56] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 20:22:56] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 20:22:56] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 20:22:56] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 20:22:56] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3045784) [2026-01-27 20:22:57] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3045784) [2026-01-27 20:22:57] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3045784) [2026-01-27 20:22:57] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3045784) [2026-01-27 20:22:57] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3045784) [2026-01-27 20:22:57] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=3045784) [2026-01-27 20:22:57] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3045784) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3045784) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:08<00:25,  8.63s/it]
(EngineCore_DP0 pid=3045784) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:42<00:47, 23.54s/it]
(EngineCore_DP0 pid=3045784) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [01:14<00:27, 27.36s/it]
(EngineCore_DP0 pid=3045784) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:47<00:00, 29.77s/it]
(EngineCore_DP0 pid=3045784) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:47<00:00, 27.00s/it]
(EngineCore_DP0 pid=3045784) 
(EngineCore_DP0 pid=3045784) [2026-01-27 20:24:46] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 6848] -> 1D uint8
(EngineCore_DP0 pid=3045784) [2026-01-27 20:24:47] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 30736384 bytes
(EngineCore_DP0 pid=3045784) [2026-01-27 20:24:47] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 6848] -> 1D uint8
(EngineCore_DP0 pid=3045784) [2026-01-27 20:24:47] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 21954560 bytes
(EngineCore_DP0 pid=3045784) [2026-01-27 20:24:47] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 6848] -> 1D uint8
(EngineCore_DP0 pid=3045784) [2026-01-27 20:24:47] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 118554624 bytes
(EngineCore_DP0 pid=3045784) [2026-01-27 20:24:47] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 18432] -> 1D uint8
(EngineCore_DP0 pid=3045784) [2026-01-27 20:24:47] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 58982400 bytes
(EngineCore_DP0 pid=3045784) 2026-01-27 20:24:56,672 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3045784) 2026-01-27 20:24:56,966 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 1/64 [00:00<00:12,  4.98it/s]
Adding requests:   5%|▍         | 3/64 [00:00<00:06,  9.77it/s]
Adding requests:   9%|▉         | 6/64 [00:00<00:03, 15.71it/s]
Adding requests:  16%|█▌        | 10/64 [00:00<00:02, 22.64it/s]
Adding requests:  22%|██▏       | 14/64 [00:00<00:01, 27.14it/s]
Adding requests:  33%|███▎      | 21/64 [00:00<00:01, 38.40it/s]
Adding requests:  50%|█████     | 32/64 [00:00<00:00, 57.92it/s]
Adding requests:  67%|██████▋   | 43/64 [00:00<00:00, 72.60it/s]
Adding requests:  88%|████████▊ | 56/64 [00:01<00:00, 88.86it/s]
Adding requests: 100%|██████████| 64/64 [00:01<00:00, 55.64it/s]

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 1/64 [00:22<24:01, 22.88s/it, est. speed input: 0.70 toks/s, output: 11.19 toks/s]
Processed prompts:   9%|▉         | 6/64 [00:23<02:44,  2.84s/it, est. speed input: 4.16 toks/s, output: 66.58 toks/s]
Processed prompts:  34%|███▍      | 22/64 [00:23<00:24,  1.75it/s, est. speed input: 15.18 toks/s, output: 242.95 toks/s]
Processed prompts:  58%|█████▊    | 37/64 [00:23<00:07,  3.57it/s, est. speed input: 25.42 toks/s, output: 406.76 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:23<00:00,  8.05it/s, est. speed input: 43.62 toks/s, output: 697.99 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:23<00:00,  8.05it/s, est. speed input: 43.62 toks/s, output: 697.99 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:23<00:00,  2.73it/s, est. speed input: 43.62 toks/s, output: 697.99 toks/s]
[rank0]:[W127 20:25:29.569773967 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 180.9s

测试结果:
  Requests/s:   2.59
  Tokens/s:     703.65
  Total Reqs:   64
  Elapsed:      24.74s

  [Decode 分析]
  Total Decode Tokens:  16384
  Decode Tokens/s:      662.25

============================================================
[2/4] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-14B-FP8                                 │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 2048 (= 128 x 16)
│   M_decode      = 128
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 128
│   --max-num-seqs           = 128
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 20:25:53 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 20:25:53 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3048472) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3048472) WARNING 01-27 20:27:56 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 4.35 requests/s, 1183.05 total tokens/s, 1113.46 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768


─── STDERR ───
[2026-01-27 20:25:52] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 20:25:52] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:25:53] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-27 20:25:53] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:25:53] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:25:53] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:25:53] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:25:53] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:25:53] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:25:53] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 20:25:53] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 20:25:53] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 20:25:53] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 20:25:53] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 20:25:56] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 20:25:56] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:25:56] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-27 20:25:56] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:25:56] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:25:56] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:25:56] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:25:56] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:25:56] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:25:56] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 20:25:56] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 20:25:56] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 20:25:56] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 20:25:56] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3048472) [2026-01-27 20:25:57] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3048472) [2026-01-27 20:25:57] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3048472) [2026-01-27 20:25:57] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3048472) [2026-01-27 20:25:57] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3048472) [2026-01-27 20:25:57] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=3048472) [2026-01-27 20:25:57] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3048472) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3048472) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:08<00:24,  8.24s/it]
(EngineCore_DP0 pid=3048472) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:43<00:48, 24.04s/it]
(EngineCore_DP0 pid=3048472) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [01:15<00:27, 27.68s/it]
(EngineCore_DP0 pid=3048472) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:48<00:00, 30.03s/it]
(EngineCore_DP0 pid=3048472) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:48<00:00, 27.25s/it]
(EngineCore_DP0 pid=3048472) 
(EngineCore_DP0 pid=3048472) [2026-01-27 20:27:48] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 6848] -> 1D uint8
(EngineCore_DP0 pid=3048472) [2026-01-27 20:27:48] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 30736384 bytes
(EngineCore_DP0 pid=3048472) [2026-01-27 20:27:48] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 6848] -> 1D uint8
(EngineCore_DP0 pid=3048472) [2026-01-27 20:27:48] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 21954560 bytes
(EngineCore_DP0 pid=3048472) [2026-01-27 20:27:48] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 6848] -> 1D uint8
(EngineCore_DP0 pid=3048472) [2026-01-27 20:27:48] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 118554624 bytes
(EngineCore_DP0 pid=3048472) [2026-01-27 20:27:48] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 18432] -> 1D uint8
(EngineCore_DP0 pid=3048472) [2026-01-27 20:27:48] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 58982400 bytes
(EngineCore_DP0 pid=3048472) 2026-01-27 20:27:55,385 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3048472) 2026-01-27 20:27:55,424 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  11%|█         | 14/128 [00:00<00:00, 136.49it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 1100.84it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:28<59:20, 28.04s/it, est. speed input: 0.57 toks/s, output: 9.13 toks/s]
Processed prompts:   2%|▏         | 2/128 [00:28<24:24, 11.62s/it, est. speed input: 1.14 toks/s, output: 18.18 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:28<01:29,  1.23it/s, est. speed input: 10.18 toks/s, output: 162.81 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:28<00:34,  2.72it/s, est. speed input: 18.56 toks/s, output: 296.93 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:28<00:16,  4.81it/s, est. speed input: 26.89 toks/s, output: 430.32 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:28<00:08,  7.47it/s, est. speed input: 34.59 toks/s, output: 553.50 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:28<00:04, 10.76it/s, est. speed input: 41.66 toks/s, output: 666.63 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:28<00:02, 14.75it/s, est. speed input: 48.13 toks/s, output: 770.11 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:29<00:01, 19.49it/s, est. speed input: 54.02 toks/s, output: 864.27 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:29<00:00, 30.42it/s, est. speed input: 64.69 toks/s, output: 1035.03 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:29<00:00, 30.42it/s, est. speed input: 69.99 toks/s, output: 1119.85 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:29<00:00,  4.37it/s, est. speed input: 69.99 toks/s, output: 1119.85 toks/s]
[rank0]:[W127 20:28:26.208987319 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 168.7s

测试结果:
  Requests/s:   4.35
  Tokens/s:     1183.05
  Total Reqs:   128
  Elapsed:      29.43s

  [Decode 分析]
  Total Decode Tokens:  32768
  Decode Tokens/s:      1113.46

============================================================
[3/4] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-14B-FP8                                 │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 4096 (= 256 x 16)
│   M_decode      = 256
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 256
│   --max-num-seqs           = 256
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 20:28:40 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 20:28:40 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3050982) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3050982) WARNING 01-27 20:30:44 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 5.84 requests/s, 1587.50 total tokens/s, 1494.12 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536


─── STDERR ───
[2026-01-27 20:28:40] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 20:28:40] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:28:40] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-27 20:28:40] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:28:40] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:28:40] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:28:40] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:28:40] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:28:40] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:28:40] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 20:28:40] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 20:28:40] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 20:28:40] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 20:28:40] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 20:28:44] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 20:28:44] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:28:44] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-27 20:28:44] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:28:44] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:28:44] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:28:44] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:28:44] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:28:44] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:28:44] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 20:28:44] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 20:28:44] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 20:28:44] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 20:28:44] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3050982) [2026-01-27 20:28:45] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3050982) [2026-01-27 20:28:45] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3050982) [2026-01-27 20:28:45] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3050982) [2026-01-27 20:28:45] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3050982) [2026-01-27 20:28:45] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=3050982) [2026-01-27 20:28:45] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3050982) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3050982) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:08<00:25,  8.55s/it]
(EngineCore_DP0 pid=3050982) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:43<00:47, 23.99s/it]
(EngineCore_DP0 pid=3050982) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [01:15<00:27, 27.73s/it]
(EngineCore_DP0 pid=3050982) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:48<00:00, 29.88s/it]
(EngineCore_DP0 pid=3050982) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:48<00:00, 27.18s/it]
(EngineCore_DP0 pid=3050982) 
(EngineCore_DP0 pid=3050982) [2026-01-27 20:30:35] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 6848] -> 1D uint8
(EngineCore_DP0 pid=3050982) [2026-01-27 20:30:35] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 30736384 bytes
(EngineCore_DP0 pid=3050982) [2026-01-27 20:30:35] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 6848] -> 1D uint8
(EngineCore_DP0 pid=3050982) [2026-01-27 20:30:35] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 21954560 bytes
(EngineCore_DP0 pid=3050982) [2026-01-27 20:30:35] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 6848] -> 1D uint8
(EngineCore_DP0 pid=3050982) [2026-01-27 20:30:35] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 118554624 bytes
(EngineCore_DP0 pid=3050982) [2026-01-27 20:30:35] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 18432] -> 1D uint8
(EngineCore_DP0 pid=3050982) [2026-01-27 20:30:35] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 58982400 bytes
(EngineCore_DP0 pid=3050982) 2026-01-27 20:30:42,905 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3050982) 2026-01-27 20:30:42,978 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   0%|          | 1/256 [00:00<00:38,  6.66it/s]
Adding requests:   1%|          | 3/256 [00:00<00:22, 11.06it/s]
Adding requests:   2%|▏         | 6/256 [00:00<00:14, 17.34it/s]
Adding requests:   4%|▎         | 9/256 [00:00<00:11, 20.88it/s]
Adding requests:   5%|▍         | 12/256 [00:00<00:10, 23.69it/s]
Adding requests:   7%|▋         | 18/256 [00:00<00:07, 33.54it/s]
Adding requests:  11%|█         | 27/256 [00:00<00:04, 49.86it/s]
Adding requests:  14%|█▎        | 35/256 [00:00<00:03, 58.76it/s]
Adding requests:  19%|█▉        | 48/256 [00:01<00:02, 79.12it/s]
Adding requests:  27%|██▋       | 68/256 [00:01<00:01, 113.21it/s]
Adding requests:  36%|███▋      | 93/256 [00:01<00:01, 152.81it/s]
Adding requests:  49%|████▉     | 125/256 [00:01<00:00, 201.10it/s]
Adding requests:  62%|██████▏   | 158/256 [00:01<00:00, 237.79it/s]
Adding requests:  71%|███████   | 182/256 [00:01<00:00, 167.53it/s]
Adding requests:  90%|█████████ | 231/256 [00:01<00:00, 240.87it/s]
Adding requests: 100%|██████████| 256/256 [00:01<00:00, 139.30it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:35<2:32:15, 35.83s/it, est. speed input: 0.45 toks/s, output: 7.15 toks/s]
Processed prompts:   1%|          | 2/256 [00:35<1:02:50, 14.84s/it, est. speed input: 0.89 toks/s, output: 14.23 toks/s]
Processed prompts:   3%|▎         | 7/256 [00:36<11:57,  2.88s/it, est. speed input: 3.10 toks/s, output: 49.57 toks/s]  
Processed prompts:   4%|▍         | 10/256 [00:36<07:04,  1.73s/it, est. speed input: 4.41 toks/s, output: 70.49 toks/s]
Processed prompts:   5%|▌         | 13/256 [00:36<04:29,  1.11s/it, est. speed input: 5.70 toks/s, output: 91.23 toks/s]
Processed prompts:   7%|▋         | 17/256 [00:36<02:40,  1.49it/s, est. speed input: 7.42 toks/s, output: 118.67 toks/s]
Processed prompts:  10%|█         | 26/256 [00:36<01:08,  3.35it/s, est. speed input: 11.29 toks/s, output: 180.68 toks/s]
Processed prompts:  14%|█▍        | 36/256 [00:37<00:36,  6.04it/s, est. speed input: 15.56 toks/s, output: 248.96 toks/s]
Processed prompts:  20%|█▉        | 50/256 [00:37<00:18, 11.00it/s, est. speed input: 21.51 toks/s, output: 344.17 toks/s]
Processed prompts:  25%|██▍       | 63/256 [00:37<00:11, 16.77it/s, est. speed input: 27.00 toks/s, output: 431.96 toks/s]
Processed prompts:  30%|██▉       | 76/256 [00:37<00:07, 23.75it/s, est. speed input: 32.44 toks/s, output: 519.12 toks/s]
Processed prompts:  34%|███▍      | 88/256 [00:37<00:05, 30.40it/s, est. speed input: 37.41 toks/s, output: 598.51 toks/s]
Processed prompts:  39%|███▊      | 99/256 [00:37<00:04, 37.32it/s, est. speed input: 41.93 toks/s, output: 670.91 toks/s]
Processed prompts:  43%|████▎     | 109/256 [00:37<00:03, 43.59it/s, est. speed input: 46.01 toks/s, output: 736.16 toks/s]
Processed prompts:  46%|████▋     | 119/256 [00:38<00:02, 48.07it/s, est. speed input: 50.03 toks/s, output: 800.44 toks/s]
Processed prompts:  50%|█████     | 128/256 [00:38<00:02, 52.96it/s, est. speed input: 53.64 toks/s, output: 858.21 toks/s]
Processed prompts:  54%|█████▎    | 137/256 [00:38<00:02, 55.77it/s, est. speed input: 57.20 toks/s, output: 915.22 toks/s]
Processed prompts:  57%|█████▋    | 145/256 [00:38<00:01, 55.78it/s, est. speed input: 60.32 toks/s, output: 965.06 toks/s]
Processed prompts:  60%|█████▉    | 153/256 [00:38<00:01, 59.19it/s, est. speed input: 63.46 toks/s, output: 1015.32 toks/s]
Processed prompts:  62%|██████▎   | 160/256 [00:38<00:01, 59.99it/s, est. speed input: 66.17 toks/s, output: 1058.70 toks/s]
Processed prompts:  65%|██████▌   | 167/256 [00:38<00:01, 61.26it/s, est. speed input: 68.87 toks/s, output: 1101.95 toks/s]
Processed prompts:  68%|██████▊   | 174/256 [00:38<00:01, 62.09it/s, est. speed input: 71.56 toks/s, output: 1144.94 toks/s]
Processed prompts:  71%|███████   | 181/256 [00:39<00:01, 63.81it/s, est. speed input: 74.24 toks/s, output: 1187.88 toks/s]
Processed prompts:  73%|███████▎  | 188/256 [00:39<00:01, 65.06it/s, est. speed input: 76.91 toks/s, output: 1230.59 toks/s]
Processed prompts:  76%|███████▌  | 195/256 [00:39<00:01, 52.52it/s, est. speed input: 79.38 toks/s, output: 1270.05 toks/s]
Processed prompts:  79%|███████▉  | 202/256 [00:39<00:01, 46.71it/s, est. speed input: 81.83 toks/s, output: 1309.33 toks/s]
Processed prompts:  82%|████████▏ | 210/256 [00:39<00:01, 43.93it/s, est. speed input: 84.64 toks/s, output: 1354.16 toks/s]
Processed prompts:  84%|████████▍ | 216/256 [00:39<00:00, 40.55it/s, est. speed input: 86.66 toks/s, output: 1386.53 toks/s]
Processed prompts:  87%|████████▋ | 222/256 [00:40<00:00, 38.44it/s, est. speed input: 88.67 toks/s, output: 1418.72 toks/s]
Processed prompts:  89%|████████▊ | 227/256 [00:40<00:00, 40.10it/s, est. speed input: 90.42 toks/s, output: 1446.77 toks/s]
Processed prompts:  91%|█████████ | 232/256 [00:40<00:00, 31.85it/s, est. speed input: 91.84 toks/s, output: 1469.49 toks/s]
Processed prompts:  92%|█████████▏| 236/256 [00:40<00:00, 29.87it/s, est. speed input: 93.05 toks/s, output: 1488.84 toks/s]
Processed prompts:  94%|█████████▍| 240/256 [00:40<00:00, 28.29it/s, est. speed input: 94.25 toks/s, output: 1507.97 toks/s]
Processed prompts:  95%|█████████▍| 243/256 [00:40<00:00, 25.78it/s, est. speed input: 95.06 toks/s, output: 1521.01 toks/s]
Processed prompts:  96%|█████████▌| 246/256 [00:41<00:00, 20.88it/s, est. speed input: 95.69 toks/s, output: 1530.96 toks/s]
Processed prompts:  97%|█████████▋| 249/256 [00:41<00:00, 18.22it/s, est. speed input: 96.32 toks/s, output: 1541.07 toks/s]
Processed prompts:  98%|█████████▊| 251/256 [00:41<00:00, 16.98it/s, est. speed input: 96.73 toks/s, output: 1547.76 toks/s]
Processed prompts:  99%|█████████▉| 253/256 [00:41<00:00, 16.13it/s, est. speed input: 97.16 toks/s, output: 1554.59 toks/s]
Processed prompts: 100%|█████████▉| 255/256 [00:41<00:00, 15.43it/s, est. speed input: 97.58 toks/s, output: 1561.33 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:41<00:00, 15.43it/s, est. speed input: 97.69 toks/s, output: 1563.00 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:41<00:00,  6.11it/s, est. speed input: 97.69 toks/s, output: 1563.00 toks/s]
[rank0]:[W127 20:31:28.535085591 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 186.4s

测试结果:
  Requests/s:   5.84
  Tokens/s:     1587.50
  Total Reqs:   256
  Elapsed:      43.86s

  [Decode 分析]
  Total Decode Tokens:  65536
  Decode Tokens/s:      1494.12

============================================================
[4/4] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-14B-FP8                                 │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 8192 (= 512 x 16)
│   M_decode      = 512
│   batched_tokens = 512 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 512
│   --max-num-seqs           = 512
│   --max-model-len          = 272
│   --max-num-batched-tokens = 512
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 20:31:48 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 20:31:48 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3053779) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3053779) WARNING 01-27 20:33:53 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 5.96 requests/s, 1620.85 total tokens/s, 1525.51 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072


─── STDERR ───
[2026-01-27 20:31:48] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 20:31:48] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:31:48] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-27 20:31:48] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:31:48] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:31:48] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:31:48] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:31:48] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:31:48] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:31:48] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 20:31:48] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 20:31:48] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 20:31:48] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 20:31:48] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 20:31:51] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 20:31:52] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:31:52] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-27 20:31:52] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:31:52] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:31:52] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:31:52] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:31:52] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:31:52] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:31:52] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 20:31:52] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 20:31:52] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 20:31:52] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 20:31:52] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3053779) [2026-01-27 20:31:53] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3053779) [2026-01-27 20:31:53] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3053779) [2026-01-27 20:31:53] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3053779) [2026-01-27 20:31:53] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3053779) [2026-01-27 20:31:53] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=3053779) [2026-01-27 20:31:53] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3053779) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3053779) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:08<00:25,  8.50s/it]
(EngineCore_DP0 pid=3053779) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:43<00:48, 24.19s/it]
(EngineCore_DP0 pid=3053779) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [01:16<00:27, 27.94s/it]
(EngineCore_DP0 pid=3053779) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:50<00:00, 30.37s/it]
(EngineCore_DP0 pid=3053779) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:50<00:00, 27.55s/it]
(EngineCore_DP0 pid=3053779) 
(EngineCore_DP0 pid=3053779) [2026-01-27 20:33:44] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 6848] -> 1D uint8
(EngineCore_DP0 pid=3053779) [2026-01-27 20:33:45] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 30736384 bytes
(EngineCore_DP0 pid=3053779) [2026-01-27 20:33:45] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 6848] -> 1D uint8
(EngineCore_DP0 pid=3053779) [2026-01-27 20:33:45] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 21954560 bytes
(EngineCore_DP0 pid=3053779) [2026-01-27 20:33:45] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 6848] -> 1D uint8
(EngineCore_DP0 pid=3053779) [2026-01-27 20:33:45] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 118554624 bytes
(EngineCore_DP0 pid=3053779) [2026-01-27 20:33:45] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 18432] -> 1D uint8
(EngineCore_DP0 pid=3053779) [2026-01-27 20:33:45] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 58982400 bytes
(EngineCore_DP0 pid=3053779) 2026-01-27 20:33:52,058 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3053779) 2026-01-27 20:33:52,107 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   0%|          | 1/512 [00:00<00:54,  9.35it/s]
Adding requests:   2%|▏         | 9/512 [00:00<00:10, 48.40it/s]
Adding requests:  14%|█▍        | 71/512 [00:00<00:01, 300.62it/s]
Adding requests:  30%|███       | 156/512 [00:00<00:00, 512.65it/s]
Adding requests:  45%|████▌     | 231/512 [00:00<00:00, 595.63it/s]
Adding requests:  63%|██████▎   | 323/512 [00:00<00:00, 702.91it/s]
Adding requests:  86%|████████▌ | 441/512 [00:00<00:00, 857.32it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 674.10it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [00:59<8:28:52, 59.75s/it, est. speed input: 0.27 toks/s, output: 4.28 toks/s]
Processed prompts:   0%|          | 2/512 [01:00<3:32:37, 25.02s/it, est. speed input: 0.53 toks/s, output: 8.47 toks/s]
Processed prompts:   6%|▋         | 33/512 [01:01<07:33,  1.06it/s, est. speed input: 8.64 toks/s, output: 138.17 toks/s]
Processed prompts:  12%|█▏        | 63/512 [01:01<03:07,  2.39it/s, est. speed input: 16.31 toks/s, output: 261.00 toks/s]
Processed prompts:  18%|█▊        | 91/512 [01:02<01:43,  4.05it/s, est. speed input: 23.33 toks/s, output: 373.27 toks/s]
Processed prompts:  23%|██▎       | 117/512 [01:03<01:05,  6.05it/s, est. speed input: 29.71 toks/s, output: 475.38 toks/s]
Processed prompts:  28%|██▊       | 141/512 [01:03<00:42,  8.69it/s, est. speed input: 35.67 toks/s, output: 570.64 toks/s]
Processed prompts:  32%|███▏      | 164/512 [01:03<00:28, 12.09it/s, est. speed input: 41.33 toks/s, output: 661.31 toks/s]
Processed prompts:  36%|███▌      | 185/512 [01:03<00:20, 16.10it/s, est. speed input: 46.46 toks/s, output: 743.34 toks/s]
Processed prompts:  40%|████      | 205/512 [01:03<00:14, 20.97it/s, est. speed input: 51.31 toks/s, output: 820.96 toks/s]
Processed prompts:  44%|████▍     | 224/512 [01:04<00:10, 26.67it/s, est. speed input: 55.89 toks/s, output: 894.21 toks/s]
Processed prompts:  47%|████▋     | 242/512 [01:04<00:08, 32.86it/s, est. speed input: 60.18 toks/s, output: 962.95 toks/s]
Processed prompts:  50%|█████     | 258/512 [01:04<00:06, 38.63it/s, est. speed input: 63.96 toks/s, output: 1023.38 toks/s]
Processed prompts:  53%|█████▎    | 273/512 [01:04<00:05, 44.29it/s, est. speed input: 67.48 toks/s, output: 1079.64 toks/s]
Processed prompts:  56%|█████▋    | 288/512 [01:04<00:04, 51.82it/s, est. speed input: 71.01 toks/s, output: 1136.24 toks/s]
Processed prompts:  59%|█████▉    | 302/512 [01:05<00:03, 57.05it/s, est. speed input: 74.27 toks/s, output: 1188.25 toks/s]
Processed prompts:  62%|██████▏   | 315/512 [01:05<00:03, 61.29it/s, est. speed input: 77.26 toks/s, output: 1236.23 toks/s]
Processed prompts:  64%|██████▍   | 327/512 [01:05<00:02, 62.73it/s, est. speed input: 79.99 toks/s, output: 1279.83 toks/s]
Processed prompts:  66%|██████▌   | 338/512 [01:05<00:02, 63.93it/s, est. speed input: 82.48 toks/s, output: 1319.61 toks/s]
Processed prompts:  68%|██████▊   | 348/512 [01:05<00:02, 66.21it/s, est. speed input: 84.74 toks/s, output: 1355.87 toks/s]
Processed prompts:  70%|██████▉   | 358/512 [01:05<00:02, 66.08it/s, est. speed input: 86.98 toks/s, output: 1391.61 toks/s]
Processed prompts:  72%|███████▏  | 367/512 [01:05<00:02, 67.57it/s, est. speed input: 88.99 toks/s, output: 1423.91 toks/s]
Processed prompts:  73%|███████▎  | 376/512 [01:06<00:02, 65.55it/s, est. speed input: 90.97 toks/s, output: 1455.55 toks/s]
Processed prompts:  75%|███████▌  | 384/512 [01:06<00:01, 66.01it/s, est. speed input: 92.74 toks/s, output: 1483.85 toks/s]
Processed prompts:  77%|███████▋  | 392/512 [01:06<00:01, 62.04it/s, est. speed input: 94.46 toks/s, output: 1511.33 toks/s]
Processed prompts:  78%|███████▊  | 399/512 [01:06<00:01, 61.91it/s, est. speed input: 95.98 toks/s, output: 1535.69 toks/s]
Processed prompts:  79%|███████▉  | 406/512 [01:06<00:01, 59.24it/s, est. speed input: 97.47 toks/s, output: 1559.52 toks/s]
Processed prompts:  80%|████████  | 412/512 [01:06<00:01, 58.18it/s, est. speed input: 98.75 toks/s, output: 1579.99 toks/s]
Processed prompts:  82%|████████▏ | 418/512 [01:06<00:01, 54.78it/s, est. speed input: 100.00 toks/s, output: 1599.93 toks/s]
Processed prompts:  83%|████████▎ | 424/512 [01:06<00:01, 55.65it/s, est. speed input: 101.27 toks/s, output: 1620.39 toks/s]
Processed prompts:  84%|████████▍ | 430/512 [01:07<00:01, 56.54it/s, est. speed input: 102.55 toks/s, output: 1640.83 toks/s]
Processed prompts:  85%|████████▌ | 436/512 [01:07<00:01, 51.71it/s, est. speed input: 103.76 toks/s, output: 1660.24 toks/s]
Processed prompts:  86%|████████▋ | 442/512 [01:07<00:01, 42.00it/s, est. speed input: 104.87 toks/s, output: 1677.87 toks/s]
Processed prompts:  88%|████████▊ | 450/512 [01:07<00:01, 39.17it/s, est. speed input: 106.40 toks/s, output: 1702.45 toks/s]
Processed prompts:  89%|████████▉ | 456/512 [01:07<00:01, 36.87it/s, est. speed input: 107.52 toks/s, output: 1720.37 toks/s]
Processed prompts:  90%|█████████ | 462/512 [01:08<00:01, 35.55it/s, est. speed input: 108.64 toks/s, output: 1738.29 toks/s]
Processed prompts:  91%|█████████ | 466/512 [01:08<00:01, 35.66it/s, est. speed input: 109.41 toks/s, output: 1750.49 toks/s]
Processed prompts:  92%|█████████▏| 470/512 [01:08<00:01, 31.52it/s, est. speed input: 110.06 toks/s, output: 1760.98 toks/s]
Processed prompts:  93%|█████████▎| 474/512 [01:08<00:01, 28.71it/s, est. speed input: 110.71 toks/s, output: 1771.38 toks/s]
Processed prompts:  93%|█████████▎| 478/512 [01:08<00:01, 27.22it/s, est. speed input: 111.37 toks/s, output: 1781.95 toks/s]
Processed prompts:  94%|█████████▍| 482/512 [01:08<00:01, 25.35it/s, est. speed input: 112.00 toks/s, output: 1791.99 toks/s]
Processed prompts:  95%|█████████▍| 485/512 [01:09<00:01, 20.43it/s, est. speed input: 112.30 toks/s, output: 1796.77 toks/s]
Processed prompts:  95%|█████████▌| 488/512 [01:09<00:01, 17.68it/s, est. speed input: 112.60 toks/s, output: 1801.62 toks/s]
Processed prompts:  96%|█████████▌| 490/512 [01:09<00:01, 16.38it/s, est. speed input: 112.80 toks/s, output: 1804.82 toks/s]
Processed prompts:  96%|█████████▌| 492/512 [01:09<00:01, 15.28it/s, est. speed input: 113.00 toks/s, output: 1807.93 toks/s]
Processed prompts:  96%|█████████▋| 494/512 [01:09<00:01, 14.68it/s, est. speed input: 113.20 toks/s, output: 1811.27 toks/s]
Processed prompts:  97%|█████████▋| 496/512 [01:09<00:01, 14.17it/s, est. speed input: 113.41 toks/s, output: 1814.56 toks/s]
Processed prompts:  97%|█████████▋| 498/512 [01:24<00:27,  2.00s/it, est. speed input: 93.98 toks/s, output: 1503.75 toks/s] 
Processed prompts:  98%|█████████▊| 501/512 [01:24<00:14,  1.30s/it, est. speed input: 94.38 toks/s, output: 1510.07 toks/s]
Processed prompts: 100%|█████████▉| 511/512 [01:25<00:00,  2.05it/s, est. speed input: 96.09 toks/s, output: 1537.49 toks/s]
Processed prompts: 100%|██████████| 512/512 [01:25<00:00,  2.05it/s, est. speed input: 96.28 toks/s, output: 1540.49 toks/s]
Processed prompts: 100%|██████████| 512/512 [01:25<00:00,  6.02it/s, est. speed input: 96.28 toks/s, output: 1540.49 toks/s]
[rank0]:[W127 20:35:19.702036887 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 231.5s

测试结果:
  Requests/s:   5.96
  Tokens/s:     1620.85
  Total Reqs:   512
  Elapsed:      85.92s

  [Decode 分析]
  Total Decode Tokens:  131072
  Decode Tokens/s:      1525.51


------------------------------------------------------------
  生成 CSV: Qwen2.5-14B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_6/Qwen2.5-14B-FP8_decode.csv

预览:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
64,16,64,64,256,256,2.5869,703.6457,24.7397
128,16,128,128,256,256,4.3494,1183.0479,29.4291
256,16,256,256,256,256,5.8364,1587.5044,43.8626
512,16,512,512,256,256,5.9590,1620.8547,85.9201

------------------------------------------------------------

[INFO] 完成: 4 成功, 0 失败

============================================================
  Qwen2.5-14B-FP8 | cuSPARSELt (2_8) | decode
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_8
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_8

============================================================
[1/4] 测试 M=64
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-14B-FP8                                 │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 64
│   M_prefill     = 1024 (= 64 x 16)
│   M_decode      = 64
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 64
│   --max-num-seqs           = 64
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 20:35:39 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 20:35:39 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3057156) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3057156) WARNING 01-27 20:37:59 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 2.57 requests/s, 698.27 total tokens/s, 657.20 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384


─── STDERR ───
[2026-01-27 20:35:39] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 20:35:39] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:35:39] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-27 20:35:39] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:35:39] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:35:39] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:35:39] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:35:39] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:35:39] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:35:39] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 20:35:39] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 20:35:39] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 20:35:39] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 20:35:39] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 20:35:43] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 20:35:43] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:35:43] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-27 20:35:43] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:35:43] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:35:43] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:35:43] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:35:43] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:35:43] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:35:43] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 20:35:43] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 20:35:43] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 20:35:43] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 20:35:43] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3057156) [2026-01-27 20:35:44] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3057156) [2026-01-27 20:35:44] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3057156) [2026-01-27 20:35:44] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3057156) [2026-01-27 20:35:44] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3057156) [2026-01-27 20:35:44] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=3057156) [2026-01-27 20:35:44] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3057156) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3057156) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:08<00:25,  8.56s/it]
(EngineCore_DP0 pid=3057156) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:47<00:52, 26.24s/it]
(EngineCore_DP0 pid=3057156) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [01:18<00:28, 28.36s/it]
(EngineCore_DP0 pid=3057156) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:58<00:00, 33.30s/it]
(EngineCore_DP0 pid=3057156) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:58<00:00, 29.73s/it]
(EngineCore_DP0 pid=3057156) 
(EngineCore_DP0 pid=3057156) [2026-01-27 20:37:44] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 7680] -> 1D uint8
(EngineCore_DP0 pid=3057156) [2026-01-27 20:37:45] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 34406400 bytes
(EngineCore_DP0 pid=3057156) [2026-01-27 20:37:45] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 7680] -> 1D uint8
(EngineCore_DP0 pid=3057156) [2026-01-27 20:37:45] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 24576000 bytes
(EngineCore_DP0 pid=3057156) [2026-01-27 20:37:45] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 7680] -> 1D uint8
(EngineCore_DP0 pid=3057156) [2026-01-27 20:37:45] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 132710400 bytes
(EngineCore_DP0 pid=3057156) [2026-01-27 20:37:45] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 20736] -> 1D uint8
(EngineCore_DP0 pid=3057156) [2026-01-27 20:37:45] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 66355200 bytes
(EngineCore_DP0 pid=3057156) 2026-01-27 20:37:53,214 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3057156) 2026-01-27 20:37:53,455 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 1/64 [00:00<00:10,  5.79it/s]
Adding requests:   5%|▍         | 3/64 [00:00<00:05, 11.58it/s]
Adding requests:  11%|█         | 7/64 [00:00<00:02, 20.40it/s]
Adding requests:  19%|█▉        | 12/64 [00:00<00:01, 27.32it/s]
Adding requests:  30%|██▉       | 19/64 [00:00<00:01, 39.71it/s]
Adding requests:  47%|████▋     | 30/64 [00:00<00:00, 60.41it/s]
Adding requests:  69%|██████▉   | 44/64 [00:00<00:00, 82.64it/s]
Adding requests: 100%|██████████| 64/64 [00:00<00:00, 115.30it/s]
Adding requests: 100%|██████████| 64/64 [00:00<00:00, 66.71it/s] 

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 1/64 [00:23<24:32, 23.38s/it, est. speed input: 0.68 toks/s, output: 10.95 toks/s]
Processed prompts:  11%|█         | 7/64 [00:23<02:20,  2.47s/it, est. speed input: 4.75 toks/s, output: 75.99 toks/s]
Processed prompts:  36%|███▌      | 23/64 [00:23<00:23,  1.77it/s, est. speed input: 15.54 toks/s, output: 248.64 toks/s]
Processed prompts:  59%|█████▉    | 38/64 [00:23<00:07,  3.55it/s, est. speed input: 25.56 toks/s, output: 408.88 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:23<00:00,  3.55it/s, est. speed input: 42.88 toks/s, output: 686.11 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:23<00:00,  2.68it/s, est. speed input: 42.88 toks/s, output: 686.11 toks/s]
[rank0]:[W127 20:38:25.793077436 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 188.1s

测试结果:
  Requests/s:   2.57
  Tokens/s:     698.27
  Total Reqs:   64
  Elapsed:      24.93s

  [Decode 分析]
  Total Decode Tokens:  16384
  Decode Tokens/s:      657.20

============================================================
[2/4] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-14B-FP8                                 │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 2048 (= 128 x 16)
│   M_decode      = 128
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 128
│   --max-num-seqs           = 128
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 20:38:47 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 20:38:47 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3059956) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3059956) WARNING 01-27 20:41:02 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 4.28 requests/s, 1163.48 total tokens/s, 1095.04 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768


─── STDERR ───
[2026-01-27 20:38:47] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 20:38:47] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:38:47] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-27 20:38:47] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:38:47] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:38:47] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:38:47] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:38:47] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:38:47] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:38:47] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 20:38:47] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 20:38:47] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 20:38:47] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 20:38:47] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 20:38:50] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 20:38:50] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:38:50] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-27 20:38:50] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:38:50] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:38:50] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:38:50] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:38:50] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:38:50] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:38:50] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 20:38:50] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 20:38:50] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 20:38:50] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 20:38:50] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3059956) [2026-01-27 20:38:51] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3059956) [2026-01-27 20:38:51] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3059956) [2026-01-27 20:38:51] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3059956) [2026-01-27 20:38:51] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3059956) [2026-01-27 20:38:51] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=3059956) [2026-01-27 20:38:51] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3059956) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3059956) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:08<00:25,  8.54s/it]
(EngineCore_DP0 pid=3059956) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:48<00:54, 27.03s/it]
(EngineCore_DP0 pid=3059956) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [01:19<00:28, 28.81s/it]
(EngineCore_DP0 pid=3059956) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:59<00:00, 33.38s/it]
(EngineCore_DP0 pid=3059956) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:59<00:00, 29.96s/it]
(EngineCore_DP0 pid=3059956) 
(EngineCore_DP0 pid=3059956) [2026-01-27 20:40:52] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 7680] -> 1D uint8
(EngineCore_DP0 pid=3059956) [2026-01-27 20:40:53] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 34406400 bytes
(EngineCore_DP0 pid=3059956) [2026-01-27 20:40:53] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 7680] -> 1D uint8
(EngineCore_DP0 pid=3059956) [2026-01-27 20:40:53] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 24576000 bytes
(EngineCore_DP0 pid=3059956) [2026-01-27 20:40:53] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 7680] -> 1D uint8
(EngineCore_DP0 pid=3059956) [2026-01-27 20:40:53] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 132710400 bytes
(EngineCore_DP0 pid=3059956) [2026-01-27 20:40:53] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 20736] -> 1D uint8
(EngineCore_DP0 pid=3059956) [2026-01-27 20:40:53] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 66355200 bytes
(EngineCore_DP0 pid=3059956) 2026-01-27 20:41:00,837 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3059956) 2026-01-27 20:41:00,909 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:   1%|          | 1/128 [00:00<00:17,  7.20it/s]
Adding requests:   5%|▍         | 6/128 [00:00<00:04, 28.79it/s]
Adding requests:  11%|█         | 14/128 [00:00<00:02, 49.70it/s]
Adding requests:  35%|███▌      | 45/128 [00:00<00:00, 145.90it/s]
Adding requests:  73%|███████▎  | 94/128 [00:00<00:00, 264.25it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 218.30it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:28<59:16, 28.00s/it, est. speed input: 0.57 toks/s, output: 9.14 toks/s]
Processed prompts:   2%|▏         | 2/128 [00:28<24:21, 11.60s/it, est. speed input: 1.14 toks/s, output: 18.20 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:28<01:29,  1.23it/s, est. speed input: 10.19 toks/s, output: 163.03 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:28<00:34,  2.73it/s, est. speed input: 18.59 toks/s, output: 297.44 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:28<00:16,  4.82it/s, est. speed input: 26.94 toks/s, output: 431.02 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:28<00:08,  7.48it/s, est. speed input: 34.65 toks/s, output: 554.37 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:28<00:04, 10.79it/s, est. speed input: 41.74 toks/s, output: 667.76 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:28<00:02, 14.78it/s, est. speed input: 48.21 toks/s, output: 771.38 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:28<00:01, 19.49it/s, est. speed input: 54.10 toks/s, output: 865.56 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:29<00:00, 30.33it/s, est. speed input: 64.78 toks/s, output: 1036.41 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:29<00:00, 30.33it/s, est. speed input: 70.08 toks/s, output: 1121.25 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:29<00:00,  4.38it/s, est. speed input: 70.08 toks/s, output: 1121.25 toks/s]
[rank0]:[W127 20:41:32.880713627 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 187.2s

测试结果:
  Requests/s:   4.28
  Tokens/s:     1163.48
  Total Reqs:   128
  Elapsed:      29.92s

  [Decode 分析]
  Total Decode Tokens:  32768
  Decode Tokens/s:      1095.04

============================================================
[3/4] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-14B-FP8                                 │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 4096 (= 256 x 16)
│   M_decode      = 256
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 256
│   --max-num-seqs           = 256
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 20:41:55 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 20:41:55 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3062837) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3062837) WARNING 01-27 20:44:16 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 5.84 requests/s, 1587.47 total tokens/s, 1494.09 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536


─── STDERR ───
[2026-01-27 20:41:55] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 20:41:55] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:41:55] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-27 20:41:55] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:41:55] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:41:55] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:41:55] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:41:55] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:41:55] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:41:55] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 20:41:55] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 20:41:55] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 20:41:55] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 20:41:55] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 20:41:58] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 20:41:58] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:41:58] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-27 20:41:58] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:41:58] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:41:58] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:41:58] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:41:58] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:41:58] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:41:58] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 20:41:58] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 20:41:58] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 20:41:58] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 20:41:58] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3062837) [2026-01-27 20:41:59] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3062837) [2026-01-27 20:41:59] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3062837) [2026-01-27 20:41:59] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3062837) [2026-01-27 20:41:59] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3062837) [2026-01-27 20:41:59] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=3062837) [2026-01-27 20:41:59] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3062837) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3062837) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:08<00:25,  8.61s/it]
(EngineCore_DP0 pid=3062837) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:48<00:53, 26.73s/it]
(EngineCore_DP0 pid=3062837) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [01:18<00:28, 28.59s/it]
(EngineCore_DP0 pid=3062837) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:59<00:00, 33.44s/it]
(EngineCore_DP0 pid=3062837) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:59<00:00, 29.93s/it]
(EngineCore_DP0 pid=3062837) 
(EngineCore_DP0 pid=3062837) [2026-01-27 20:44:01] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 7680] -> 1D uint8
(EngineCore_DP0 pid=3062837) [2026-01-27 20:44:01] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 34406400 bytes
(EngineCore_DP0 pid=3062837) [2026-01-27 20:44:01] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 7680] -> 1D uint8
(EngineCore_DP0 pid=3062837) [2026-01-27 20:44:01] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 24576000 bytes
(EngineCore_DP0 pid=3062837) [2026-01-27 20:44:01] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 7680] -> 1D uint8
(EngineCore_DP0 pid=3062837) [2026-01-27 20:44:01] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 132710400 bytes
(EngineCore_DP0 pid=3062837) [2026-01-27 20:44:01] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 20736] -> 1D uint8
(EngineCore_DP0 pid=3062837) [2026-01-27 20:44:01] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 66355200 bytes
(EngineCore_DP0 pid=3062837) 2026-01-27 20:44:09,259 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3062837) 2026-01-27 20:44:09,318 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   0%|          | 1/256 [00:00<00:37,  6.89it/s]
Adding requests:   2%|▏         | 4/256 [00:00<00:13, 18.08it/s]
Adding requests:   4%|▎         | 9/256 [00:00<00:07, 31.11it/s]
Adding requests:   7%|▋         | 17/256 [00:00<00:04, 48.36it/s]
Adding requests:  13%|█▎        | 33/256 [00:00<00:02, 85.22it/s]
Adding requests:  23%|██▎       | 60/256 [00:00<00:01, 143.87it/s]
Adding requests:  38%|███▊      | 98/256 [00:00<00:00, 217.61it/s]
Adding requests:  52%|█████▏    | 134/256 [00:00<00:00, 261.03it/s]
Adding requests:  64%|██████▍   | 164/256 [00:00<00:00, 270.72it/s]
Adding requests:  75%|███████▌  | 192/256 [00:01<00:00, 211.33it/s]
Adding requests:  93%|█████████▎| 237/256 [00:01<00:00, 270.48it/s]
Adding requests: 100%|██████████| 256/256 [00:01<00:00, 196.68it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:36<2:36:36, 36.85s/it, est. speed input: 0.43 toks/s, output: 6.95 toks/s]
Processed prompts:   1%|          | 2/256 [00:37<1:04:38, 15.27s/it, est. speed input: 0.86 toks/s, output: 13.83 toks/s]
Processed prompts:   5%|▌         | 14/256 [00:37<05:36,  1.39s/it, est. speed input: 6.03 toks/s, output: 96.41 toks/s] 
Processed prompts:  12%|█▏        | 30/256 [00:37<01:56,  1.94it/s, est. speed input: 12.85 toks/s, output: 205.61 toks/s]
Processed prompts:  18%|█▊        | 45/256 [00:37<00:59,  3.52it/s, est. speed input: 19.18 toks/s, output: 306.94 toks/s]
Processed prompts:  23%|██▎       | 59/256 [00:37<00:35,  5.54it/s, est. speed input: 25.05 toks/s, output: 400.82 toks/s]
Processed prompts:  28%|██▊       | 72/256 [00:37<00:22,  8.02it/s, est. speed input: 30.43 toks/s, output: 486.80 toks/s]
Processed prompts:  33%|███▎      | 84/256 [00:38<00:15, 11.04it/s, est. speed input: 35.35 toks/s, output: 565.52 toks/s]
Processed prompts:  37%|███▋      | 95/256 [00:38<00:10, 14.67it/s, est. speed input: 39.83 toks/s, output: 637.30 toks/s]
Processed prompts:  41%|████▏     | 106/256 [00:38<00:07, 19.03it/s, est. speed input: 44.26 toks/s, output: 708.18 toks/s]
Processed prompts:  45%|████▌     | 116/256 [00:38<00:05, 23.68it/s, est. speed input: 48.25 toks/s, output: 771.98 toks/s]
Processed prompts:  49%|████▉     | 125/256 [00:38<00:04, 28.73it/s, est. speed input: 51.83 toks/s, output: 829.21 toks/s]
Processed prompts:  52%|█████▏    | 134/256 [00:38<00:03, 32.98it/s, est. speed input: 55.32 toks/s, output: 885.15 toks/s]
Processed prompts:  55%|█████▌    | 142/256 [00:38<00:03, 37.84it/s, est. speed input: 58.44 toks/s, output: 935.08 toks/s]
Processed prompts:  59%|█████▊    | 150/256 [00:39<00:02, 41.93it/s, est. speed input: 61.52 toks/s, output: 984.36 toks/s]
Processed prompts:  61%|██████▏   | 157/256 [00:39<00:02, 45.80it/s, est. speed input: 64.21 toks/s, output: 1027.37 toks/s]
Processed prompts:  64%|██████▍   | 164/256 [00:39<00:01, 47.61it/s, est. speed input: 66.85 toks/s, output: 1069.60 toks/s]
Processed prompts:  67%|██████▋   | 171/256 [00:39<00:01, 51.53it/s, est. speed input: 69.52 toks/s, output: 1112.25 toks/s]
Processed prompts:  70%|██████▉   | 178/256 [00:39<00:01, 55.04it/s, est. speed input: 72.17 toks/s, output: 1154.70 toks/s]
Processed prompts:  72%|███████▏  | 185/256 [00:39<00:01, 55.48it/s, est. speed input: 74.77 toks/s, output: 1196.36 toks/s]
Processed prompts:  75%|███████▌  | 192/256 [00:39<00:01, 47.27it/s, est. speed input: 77.21 toks/s, output: 1235.34 toks/s]
Processed prompts:  77%|███████▋  | 198/256 [00:39<00:01, 45.58it/s, est. speed input: 79.33 toks/s, output: 1269.32 toks/s]
Processed prompts:  80%|████████  | 205/256 [00:40<00:01, 42.55it/s, est. speed input: 81.75 toks/s, output: 1307.99 toks/s]
Processed prompts:  82%|████████▏ | 210/256 [00:40<00:01, 43.04it/s, est. speed input: 83.51 toks/s, output: 1336.16 toks/s]
Processed prompts:  84%|████████▍ | 216/256 [00:40<00:01, 39.67it/s, est. speed input: 85.51 toks/s, output: 1368.20 toks/s]
Processed prompts:  87%|████████▋ | 222/256 [00:40<00:00, 37.88it/s, est. speed input: 87.51 toks/s, output: 1400.12 toks/s]
Processed prompts:  88%|████████▊ | 226/256 [00:40<00:00, 37.67it/s, est. speed input: 88.85 toks/s, output: 1421.56 toks/s]
Processed prompts:  90%|████████▉ | 230/256 [00:40<00:00, 33.15it/s, est. speed input: 90.05 toks/s, output: 1440.80 toks/s]
Processed prompts:  91%|█████████▏| 234/256 [00:41<00:00, 30.07it/s, est. speed input: 91.24 toks/s, output: 1459.80 toks/s]
Processed prompts:  93%|█████████▎| 238/256 [00:41<00:00, 28.40it/s, est. speed input: 92.43 toks/s, output: 1478.90 toks/s]
Processed prompts:  95%|█████████▍| 242/256 [00:41<00:00, 27.31it/s, est. speed input: 93.62 toks/s, output: 1497.89 toks/s]
Processed prompts:  96%|█████████▌| 245/256 [00:41<00:00, 21.52it/s, est. speed input: 94.23 toks/s, output: 1507.72 toks/s]
Processed prompts:  97%|█████████▋| 248/256 [00:41<00:00, 18.40it/s, est. speed input: 94.85 toks/s, output: 1517.58 toks/s]
Processed prompts:  98%|█████████▊| 251/256 [00:42<00:00, 16.45it/s, est. speed input: 95.45 toks/s, output: 1527.26 toks/s]
Processed prompts:  99%|█████████▉| 253/256 [00:42<00:00, 15.66it/s, est. speed input: 95.87 toks/s, output: 1533.88 toks/s]
Processed prompts: 100%|█████████▉| 255/256 [00:42<00:00, 15.01it/s, est. speed input: 96.28 toks/s, output: 1540.42 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:42<00:00, 15.01it/s, est. speed input: 96.42 toks/s, output: 1542.68 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:42<00:00,  6.03it/s, est. speed input: 96.42 toks/s, output: 1542.68 toks/s]
[rank0]:[W127 20:45:01.588261787 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 207.3s

测试结果:
  Requests/s:   5.84
  Tokens/s:     1587.47
  Total Reqs:   256
  Elapsed:      43.86s

  [Decode 分析]
  Total Decode Tokens:  65536
  Decode Tokens/s:      1494.09

============================================================
[4/4] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-14B-FP8                                 │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 8192 (= 512 x 16)
│   M_decode      = 512
│   batched_tokens = 512 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 512
│   --max-num-seqs           = 512
│   --max-model-len          = 272
│   --max-num-batched-tokens = 512
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 20:45:22 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 20:45:22 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3065999) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3065999) WARNING 01-27 20:47:40 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 5.71 requests/s, 1553.37 total tokens/s, 1462.00 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072


─── STDERR ───
[2026-01-27 20:45:21] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 20:45:22] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:45:22] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-27 20:45:22] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:45:22] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:45:22] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:45:22] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:45:22] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:45:22] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:45:22] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 20:45:22] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 20:45:22] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 20:45:22] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 20:45:22] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 20:45:25] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 20:45:25] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:45:25] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-27 20:45:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:45:25] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:45:25] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:45:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:45:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:45:25] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:45:25] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 20:45:25] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 20:45:25] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 20:45:25] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 20:45:25] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3065999) [2026-01-27 20:45:26] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3065999) [2026-01-27 20:45:26] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3065999) [2026-01-27 20:45:26] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3065999) [2026-01-27 20:45:26] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3065999) [2026-01-27 20:45:26] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=3065999) [2026-01-27 20:45:26] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3065999) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3065999) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:08<00:25,  8.54s/it]
(EngineCore_DP0 pid=3065999) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:49<00:54, 27.32s/it]
(EngineCore_DP0 pid=3065999) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [01:20<00:29, 29.06s/it]
(EngineCore_DP0 pid=3065999) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [02:00<00:00, 33.44s/it]
(EngineCore_DP0 pid=3065999) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [02:00<00:00, 30.08s/it]
(EngineCore_DP0 pid=3065999) 
(EngineCore_DP0 pid=3065999) [2026-01-27 20:47:28] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 7680] -> 1D uint8
(EngineCore_DP0 pid=3065999) [2026-01-27 20:47:28] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 34406400 bytes
(EngineCore_DP0 pid=3065999) [2026-01-27 20:47:28] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 7680] -> 1D uint8
(EngineCore_DP0 pid=3065999) [2026-01-27 20:47:28] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 24576000 bytes
(EngineCore_DP0 pid=3065999) [2026-01-27 20:47:28] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 7680] -> 1D uint8
(EngineCore_DP0 pid=3065999) [2026-01-27 20:47:28] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 132710400 bytes
(EngineCore_DP0 pid=3065999) [2026-01-27 20:47:28] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 20736] -> 1D uint8
(EngineCore_DP0 pid=3065999) [2026-01-27 20:47:28] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 66355200 bytes
(EngineCore_DP0 pid=3065999) 2026-01-27 20:47:36,325 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3065999) 2026-01-27 20:47:36,413 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   0%|          | 1/512 [00:00<01:15,  6.78it/s]
Adding requests:   1%|          | 3/512 [00:00<00:38, 13.15it/s]
Adding requests:   1%|▏         | 7/512 [00:00<00:21, 23.47it/s]
Adding requests:   2%|▏         | 12/512 [00:00<00:15, 33.01it/s]
Adding requests:   4%|▍         | 20/512 [00:00<00:10, 47.71it/s]
Adding requests:   6%|▌         | 30/512 [00:00<00:07, 64.16it/s]
Adding requests:   9%|▊         | 44/512 [00:00<00:05, 86.85it/s]
Adding requests:  15%|█▍        | 76/512 [00:00<00:02, 157.50it/s]
Adding requests:  20%|██        | 103/512 [00:00<00:02, 188.01it/s]
Adding requests:  29%|██▊       | 146/512 [00:01<00:01, 257.86it/s]
Adding requests:  39%|███▊      | 198/512 [00:01<00:00, 333.83it/s]
Adding requests:  52%|█████▏    | 264/512 [00:01<00:00, 429.81it/s]
Adding requests:  72%|███████▏  | 370/512 [00:01<00:00, 617.14it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 350.38it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [01:00<8:37:48, 60.80s/it, est. speed input: 0.26 toks/s, output: 4.21 toks/s]
Processed prompts:   0%|          | 2/512 [01:01<3:36:18, 25.45s/it, est. speed input: 0.52 toks/s, output: 8.33 toks/s]
Processed prompts:   2%|▏         | 11/512 [01:02<25:24,  3.04s/it, est. speed input: 2.83 toks/s, output: 45.25 toks/s]
Processed prompts:   4%|▎         | 19/512 [01:02<12:09,  1.48s/it, est. speed input: 4.83 toks/s, output: 77.30 toks/s]
Processed prompts:   6%|▌         | 30/512 [01:03<06:05,  1.32it/s, est. speed input: 7.55 toks/s, output: 120.73 toks/s]
Processed prompts:  10%|▉         | 49/512 [01:04<02:44,  2.81it/s, est. speed input: 12.19 toks/s, output: 195.11 toks/s]
Processed prompts:  15%|█▌        | 78/512 [01:04<01:14,  5.79it/s, est. speed input: 19.22 toks/s, output: 307.52 toks/s]
Processed prompts:  21%|██        | 105/512 [01:05<00:44,  9.12it/s, est. speed input: 25.63 toks/s, output: 410.02 toks/s]
Processed prompts:  25%|██▌       | 130/512 [01:05<00:28, 13.53it/s, est. speed input: 31.61 toks/s, output: 505.75 toks/s]
Processed prompts:  30%|██▉       | 153/512 [01:06<00:19, 18.66it/s, est. speed input: 37.07 toks/s, output: 593.07 toks/s]
Processed prompts:  34%|███▍      | 175/512 [01:06<00:13, 24.63it/s, est. speed input: 42.24 toks/s, output: 675.89 toks/s]
Processed prompts:  38%|███▊      | 196/512 [01:06<00:10, 31.55it/s, est. speed input: 47.16 toks/s, output: 754.56 toks/s]
Processed prompts:  42%|████▏     | 215/512 [01:06<00:07, 38.17it/s, est. speed input: 51.56 toks/s, output: 824.94 toks/s]
Processed prompts:  46%|████▌     | 233/512 [01:06<00:06, 45.37it/s, est. speed input: 55.71 toks/s, output: 891.40 toks/s]
Processed prompts:  49%|████▉     | 250/512 [01:07<00:05, 51.94it/s, est. speed input: 59.60 toks/s, output: 953.62 toks/s]
Processed prompts:  52%|█████▏    | 266/512 [01:07<00:04, 57.91it/s, est. speed input: 63.24 toks/s, output: 1011.81 toks/s]
Processed prompts:  55%|█████▍    | 281/512 [01:07<00:03, 62.34it/s, est. speed input: 66.62 toks/s, output: 1065.88 toks/s]
Processed prompts:  58%|█████▊    | 295/512 [01:07<00:03, 65.57it/s, est. speed input: 69.75 toks/s, output: 1115.98 toks/s]
Processed prompts:  60%|██████    | 308/512 [01:07<00:02, 69.72it/s, est. speed input: 72.66 toks/s, output: 1162.55 toks/s]
Processed prompts:  62%|██████▎   | 320/512 [01:07<00:02, 72.67it/s, est. speed input: 75.33 toks/s, output: 1205.27 toks/s]
Processed prompts:  65%|██████▍   | 332/512 [01:08<00:02, 72.94it/s, est. speed input: 77.97 toks/s, output: 1247.48 toks/s]
Processed prompts:  67%|██████▋   | 343/512 [01:08<00:02, 71.95it/s, est. speed input: 80.36 toks/s, output: 1285.82 toks/s]
Processed prompts:  69%|██████▉   | 353/512 [01:08<00:02, 71.23it/s, est. speed input: 82.53 toks/s, output: 1320.51 toks/s]
Processed prompts:  71%|███████   | 363/512 [01:08<00:02, 71.99it/s, est. speed input: 84.70 toks/s, output: 1355.25 toks/s]
Processed prompts:  73%|███████▎  | 372/512 [01:08<00:01, 72.09it/s, est. speed input: 86.65 toks/s, output: 1386.33 toks/s]
Processed prompts:  74%|███████▍  | 380/512 [01:08<00:01, 70.15it/s, est. speed input: 88.35 toks/s, output: 1413.60 toks/s]
Processed prompts:  76%|███████▌  | 388/512 [01:08<00:01, 63.36it/s, est. speed input: 90.00 toks/s, output: 1439.99 toks/s]
Processed prompts:  77%|███████▋  | 395/512 [01:09<00:01, 62.78it/s, est. speed input: 91.47 toks/s, output: 1463.53 toks/s]
Processed prompts:  79%|███████▊  | 402/512 [01:09<00:01, 59.11it/s, est. speed input: 92.91 toks/s, output: 1486.49 toks/s]
Processed prompts:  80%|███████▉  | 408/512 [01:09<00:01, 57.79it/s, est. speed input: 94.14 toks/s, output: 1506.26 toks/s]
Processed prompts:  81%|████████  | 414/512 [01:09<00:01, 57.00it/s, est. speed input: 95.38 toks/s, output: 1526.00 toks/s]
Processed prompts:  82%|████████▏ | 420/512 [01:09<00:01, 53.69it/s, est. speed input: 96.58 toks/s, output: 1545.24 toks/s]
Processed prompts:  83%|████████▎ | 426/512 [01:09<00:01, 54.23it/s, est. speed input: 97.81 toks/s, output: 1564.89 toks/s]
Processed prompts:  84%|████████▍ | 432/512 [01:09<00:01, 55.17it/s, est. speed input: 99.04 toks/s, output: 1584.57 toks/s]
Processed prompts:  86%|████████▌ | 438/512 [01:09<00:01, 52.96it/s, est. speed input: 100.23 toks/s, output: 1603.71 toks/s]
Processed prompts:  87%|████████▋ | 444/512 [01:10<00:01, 43.42it/s, est. speed input: 101.32 toks/s, output: 1621.07 toks/s]
Processed prompts:  88%|████████▊ | 451/512 [01:10<00:01, 38.81it/s, est. speed input: 102.59 toks/s, output: 1641.48 toks/s]
Processed prompts:  89%|████████▉ | 457/512 [01:10<00:01, 36.56it/s, est. speed input: 103.68 toks/s, output: 1658.89 toks/s]
Processed prompts:  90%|█████████ | 463/512 [01:10<00:01, 35.24it/s, est. speed input: 104.77 toks/s, output: 1676.27 toks/s]
Processed prompts:  91%|█████████ | 467/512 [01:10<00:01, 35.46it/s, est. speed input: 105.51 toks/s, output: 1688.12 toks/s]
Processed prompts:  92%|█████████▏| 471/512 [01:10<00:01, 31.37it/s, est. speed input: 106.15 toks/s, output: 1698.38 toks/s]
Processed prompts:  93%|█████████▎| 475/512 [01:11<00:01, 28.86it/s, est. speed input: 106.79 toks/s, output: 1708.67 toks/s]
Processed prompts:  93%|█████████▎| 478/512 [01:11<00:01, 25.39it/s, est. speed input: 107.21 toks/s, output: 1715.35 toks/s]
Processed prompts:  94%|█████████▍| 482/512 [01:11<00:01, 24.00it/s, est. speed input: 107.82 toks/s, output: 1725.13 toks/s]
Processed prompts:  95%|█████████▍| 485/512 [01:11<00:01, 19.35it/s, est. speed input: 108.11 toks/s, output: 1729.77 toks/s]
Processed prompts:  95%|█████████▌| 488/512 [01:12<00:01, 16.89it/s, est. speed input: 108.41 toks/s, output: 1734.55 toks/s]
Processed prompts:  96%|█████████▌| 490/512 [01:12<00:01, 15.82it/s, est. speed input: 108.61 toks/s, output: 1737.81 toks/s]
Processed prompts:  96%|█████████▌| 492/512 [01:12<00:01, 14.96it/s, est. speed input: 108.82 toks/s, output: 1741.04 toks/s]
Processed prompts:  96%|█████████▋| 494/512 [01:12<00:01, 14.36it/s, est. speed input: 109.02 toks/s, output: 1744.33 toks/s]
Processed prompts:  97%|█████████▋| 496/512 [01:12<00:01, 13.98it/s, est. speed input: 109.23 toks/s, output: 1747.67 toks/s]
Processed prompts:  97%|█████████▋| 498/512 [01:27<00:28,  2.04s/it, est. speed input: 90.92 toks/s, output: 1454.64 toks/s] 
Processed prompts:  98%|█████████▊| 500/512 [01:27<00:18,  1.50s/it, est. speed input: 91.11 toks/s, output: 1457.75 toks/s]
Processed prompts:  98%|█████████▊| 503/512 [01:27<00:08,  1.04it/s, est. speed input: 91.49 toks/s, output: 1463.87 toks/s]
Processed prompts: 100%|█████████▉| 511/512 [01:28<00:00,  2.43it/s, est. speed input: 92.78 toks/s, output: 1484.55 toks/s]
Processed prompts: 100%|██████████| 512/512 [01:28<00:00,  2.43it/s, est. speed input: 92.97 toks/s, output: 1487.45 toks/s]
Processed prompts: 100%|██████████| 512/512 [01:28<00:00,  5.81it/s, est. speed input: 92.97 toks/s, output: 1487.45 toks/s]
[rank0]:[W127 20:49:11.711035338 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 250.5s

测试结果:
  Requests/s:   5.71
  Tokens/s:     1553.37
  Total Reqs:   512
  Elapsed:      89.65s

  [Decode 分析]
  Total Decode Tokens:  131072
  Decode Tokens/s:      1462.00


------------------------------------------------------------
  生成 CSV: Qwen2.5-14B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_8/Qwen2.5-14B-FP8_decode.csv

预览:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
64,16,64,64,256,256,2.5672,698.2723,24.9301
128,16,128,128,256,256,4.2775,1163.4820,29.9240
256,16,256,256,256,256,5.8363,1587.4720,43.8634
512,16,512,512,256,256,5.7109,1553.3726,89.6527

------------------------------------------------------------

[INFO] 完成: 4 成功, 0 失败

============================================================
  Qwen2.5-14B-FP8 | cuSPARSELt (2_10) | decode
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/Qwen2.5-14B-FP8-SlideSparse-2_10
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_10

============================================================
[1/4] 测试 M=64
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-14B-FP8                                 │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 64
│   M_prefill     = 1024 (= 64 x 16)
│   M_decode      = 64
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 64
│   --max-num-seqs           = 64
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 20:49:33 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 20:49:33 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3069721) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3069721) WARNING 01-27 20:51:53 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 2.47 requests/s, 670.60 total tokens/s, 631.15 output tokens/s
Total num prompt tokens:  1024
Total num output tokens:  16384


─── STDERR ───
[2026-01-27 20:49:33] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 20:49:33] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:49:33] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-27 20:49:33] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:49:33] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:49:33] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:49:33] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:49:33] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:49:33] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:49:33] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 20:49:33] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 20:49:33] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 20:49:33] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 20:49:33] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 20:49:36] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 20:49:36] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:49:36] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-27 20:49:36] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:49:36] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:49:36] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:49:36] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:49:36] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:49:36] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:49:36] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 20:49:36] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 20:49:36] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 20:49:36] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 20:49:36] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3069721) [2026-01-27 20:49:37] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3069721) [2026-01-27 20:49:38] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3069721) [2026-01-27 20:49:38] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3069721) [2026-01-27 20:49:38] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3069721) [2026-01-27 20:49:38] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=3069721) [2026-01-27 20:49:38] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3069721) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3069721) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:08<00:25,  8.55s/it]
(EngineCore_DP0 pid=3069721) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:40<00:44, 22.16s/it]
(EngineCore_DP0 pid=3069721) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [01:16<00:28, 28.47s/it]
(EngineCore_DP0 pid=3069721) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:57<00:00, 33.57s/it]
(EngineCore_DP0 pid=3069721) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:57<00:00, 29.40s/it]
(EngineCore_DP0 pid=3069721) 
(EngineCore_DP0 pid=3069721) [2026-01-27 20:51:37] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 8192] -> 1D uint8
(EngineCore_DP0 pid=3069721) [2026-01-27 20:51:37] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 36700160 bytes
(EngineCore_DP0 pid=3069721) [2026-01-27 20:51:37] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 8192] -> 1D uint8
(EngineCore_DP0 pid=3069721) [2026-01-27 20:51:37] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 26214400 bytes
(EngineCore_DP0 pid=3069721) [2026-01-27 20:51:37] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 8192] -> 1D uint8
(EngineCore_DP0 pid=3069721) [2026-01-27 20:51:37] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 141557760 bytes
(EngineCore_DP0 pid=3069721) [2026-01-27 20:51:37] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 22144] -> 1D uint8
(EngineCore_DP0 pid=3069721) [2026-01-27 20:51:37] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 70860800 bytes
(EngineCore_DP0 pid=3069721) 2026-01-27 20:51:45,989 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3069721) 2026-01-27 20:51:46,255 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 1/64 [00:00<00:13,  4.82it/s]
Adding requests:   5%|▍         | 3/64 [00:00<00:06, 10.10it/s]
Adding requests:   9%|▉         | 6/64 [00:00<00:03, 16.15it/s]
Adding requests:  16%|█▌        | 10/64 [00:00<00:02, 22.88it/s]
Adding requests:  20%|██        | 13/64 [00:00<00:02, 25.01it/s]
Adding requests:  31%|███▏      | 20/64 [00:00<00:01, 38.15it/s]
Adding requests:  48%|████▊     | 31/64 [00:00<00:00, 58.39it/s]
Adding requests:  69%|██████▉   | 44/64 [00:00<00:00, 77.87it/s]
Adding requests:  98%|█████████▊| 63/64 [00:01<00:00, 109.84it/s]
Adding requests: 100%|██████████| 64/64 [00:01<00:00, 58.85it/s] 

Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 1/64 [00:24<25:28, 24.26s/it, est. speed input: 0.66 toks/s, output: 10.55 toks/s]
Processed prompts:   3%|▎         | 2/64 [00:24<10:23, 10.05s/it, est. speed input: 1.31 toks/s, output: 21.02 toks/s]
Processed prompts:  11%|█         | 7/64 [00:24<01:51,  1.95s/it, est. speed input: 4.58 toks/s, output: 73.26 toks/s]
Processed prompts:  36%|███▌      | 23/64 [00:24<00:17,  2.38it/s, est. speed input: 14.97 toks/s, output: 239.57 toks/s]
Processed prompts:  59%|█████▉    | 38/64 [00:24<00:05,  4.79it/s, est. speed input: 24.63 toks/s, output: 394.01 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:24<00:00,  4.79it/s, est. speed input: 41.32 toks/s, output: 661.18 toks/s]
Processed prompts: 100%|██████████| 64/64 [00:24<00:00,  2.58it/s, est. speed input: 41.32 toks/s, output: 661.18 toks/s]
[rank0]:[W127 20:52:20.348357616 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 189.6s

测试结果:
  Requests/s:   2.47
  Tokens/s:     670.60
  Total Reqs:   64
  Elapsed:      25.96s

  [Decode 分析]
  Total Decode Tokens:  16384
  Decode Tokens/s:      631.15

============================================================
[2/4] 测试 M=128
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-14B-FP8                                 │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 128
│   M_prefill     = 2048 (= 128 x 16)
│   M_decode      = 128
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 128
│   --max-num-seqs           = 128
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 20:52:41 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 20:52:41 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3072525) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3072525) WARNING 01-27 20:54:55 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 4.07 requests/s, 1106.56 total tokens/s, 1041.47 output tokens/s
Total num prompt tokens:  2048
Total num output tokens:  32768


─── STDERR ───
[2026-01-27 20:52:41] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 20:52:41] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:52:41] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-27 20:52:41] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:52:41] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:52:41] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:52:41] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:52:41] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:52:41] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:52:41] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 20:52:41] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 20:52:41] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 20:52:41] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 20:52:41] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 20:52:45] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 20:52:45] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:52:45] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-27 20:52:45] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:52:45] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:52:45] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:52:45] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:52:45] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:52:45] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:52:45] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 20:52:45] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 20:52:45] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 20:52:45] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 20:52:45] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3072525) [2026-01-27 20:52:46] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3072525) [2026-01-27 20:52:46] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3072525) [2026-01-27 20:52:46] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3072525) [2026-01-27 20:52:46] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3072525) [2026-01-27 20:52:46] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=3072525) [2026-01-27 20:52:46] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3072525) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3072525) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:08<00:25,  8.34s/it]
(EngineCore_DP0 pid=3072525) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:41<00:45, 22.75s/it]
(EngineCore_DP0 pid=3072525) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [01:17<00:28, 28.87s/it]
(EngineCore_DP0 pid=3072525) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:59<00:00, 33.97s/it]
(EngineCore_DP0 pid=3072525) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:59<00:00, 29.78s/it]
(EngineCore_DP0 pid=3072525) 
(EngineCore_DP0 pid=3072525) [2026-01-27 20:54:47] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 8192] -> 1D uint8
(EngineCore_DP0 pid=3072525) [2026-01-27 20:54:47] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 36700160 bytes
(EngineCore_DP0 pid=3072525) [2026-01-27 20:54:47] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 8192] -> 1D uint8
(EngineCore_DP0 pid=3072525) [2026-01-27 20:54:47] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 26214400 bytes
(EngineCore_DP0 pid=3072525) [2026-01-27 20:54:47] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 8192] -> 1D uint8
(EngineCore_DP0 pid=3072525) [2026-01-27 20:54:47] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 141557760 bytes
(EngineCore_DP0 pid=3072525) [2026-01-27 20:54:47] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 22144] -> 1D uint8
(EngineCore_DP0 pid=3072525) [2026-01-27 20:54:47] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 70860800 bytes
(EngineCore_DP0 pid=3072525) 2026-01-27 20:54:54,334 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3072525) 2026-01-27 20:54:54,368 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:   1%|          | 1/128 [00:00<00:27,  4.69it/s]
Adding requests:   2%|▏         | 3/128 [00:00<00:13,  9.29it/s]
Adding requests:   5%|▍         | 6/128 [00:00<00:07, 15.55it/s]
Adding requests:   7%|▋         | 9/128 [00:00<00:05, 19.91it/s]
Adding requests:  10%|█         | 13/128 [00:00<00:04, 26.06it/s]
Adding requests:  15%|█▍        | 19/128 [00:00<00:03, 36.02it/s]
Adding requests:  23%|██▎       | 29/128 [00:00<00:01, 53.80it/s]
Adding requests:  31%|███▏      | 40/128 [00:00<00:01, 70.10it/s]
Adding requests:  43%|████▎     | 55/128 [00:01<00:00, 92.71it/s]
Adding requests:  53%|█████▎    | 68/128 [00:01<00:00, 103.31it/s]
Adding requests:  66%|██████▋   | 85/128 [00:01<00:00, 121.82it/s]
Adding requests:  84%|████████▎ | 107/128 [00:01<00:00, 149.92it/s]
Adding requests: 100%|██████████| 128/128 [00:01<00:00, 87.46it/s] 

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:28<59:52, 28.29s/it, est. speed input: 0.57 toks/s, output: 9.05 toks/s]
Processed prompts:   2%|▏         | 2/128 [00:28<24:36, 11.72s/it, est. speed input: 1.13 toks/s, output: 18.02 toks/s]
Processed prompts:   5%|▍         | 6/128 [00:28<05:30,  2.71s/it, est. speed input: 3.36 toks/s, output: 53.81 toks/s]
Processed prompts:   9%|▊         | 11/128 [00:28<02:15,  1.15s/it, est. speed input: 6.14 toks/s, output: 98.27 toks/s]
Processed prompts:  12%|█▏        | 15/128 [00:28<01:19,  1.41it/s, est. speed input: 8.34 toks/s, output: 133.45 toks/s]
Processed prompts:  16%|█▌        | 20/128 [00:28<00:45,  2.35it/s, est. speed input: 11.06 toks/s, output: 177.02 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:29<00:21,  4.71it/s, est. speed input: 15.98 toks/s, output: 255.64 toks/s]
Processed prompts:  34%|███▍      | 44/128 [00:29<00:08, 10.06it/s, est. speed input: 24.15 toks/s, output: 386.36 toks/s]
Processed prompts:  45%|████▌     | 58/128 [00:29<00:04, 16.50it/s, est. speed input: 31.71 toks/s, output: 507.38 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:29<00:02, 23.35it/s, est. speed input: 38.63 toks/s, output: 618.02 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:29<00:01, 31.06it/s, est. speed input: 44.98 toks/s, output: 719.60 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:29<00:00, 46.23it/s, est. speed input: 56.51 toks/s, output: 904.21 toks/s]
Processed prompts:  97%|█████████▋| 124/128 [00:29<00:00, 58.65it/s, est. speed input: 66.36 toks/s, output: 1061.77 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:29<00:00, 58.65it/s, est. speed input: 68.50 toks/s, output: 1096.01 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:29<00:00,  4.28it/s, est. speed input: 68.50 toks/s, output: 1096.01 toks/s]
[rank0]:[W127 20:55:27.499528907 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 186.2s

测试结果:
  Requests/s:   4.07
  Tokens/s:     1106.56
  Total Reqs:   128
  Elapsed:      31.46s

  [Decode 分析]
  Total Decode Tokens:  32768
  Decode Tokens/s:      1041.47

============================================================
[3/4] 测试 M=256
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-14B-FP8                                 │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 256
│   M_prefill     = 4096 (= 256 x 16)
│   M_decode      = 256
│   batched_tokens = 272 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 256
│   --max-num-seqs           = 256
│   --max-model-len          = 272
│   --max-num-batched-tokens = 272
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 20:55:48 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 20:55:48 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3075302) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3075302) WARNING 01-27 20:58:12 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 5.57 requests/s, 1514.28 total tokens/s, 1425.21 output tokens/s
Total num prompt tokens:  4096
Total num output tokens:  65536


─── STDERR ───
[2026-01-27 20:55:48] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 20:55:48] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:55:48] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-27 20:55:48] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:55:48] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:55:48] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:55:48] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:55:48] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:55:48] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:55:48] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 20:55:48] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 20:55:48] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 20:55:48] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 20:55:48] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 20:55:51] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 20:55:51] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:55:51] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-27 20:55:51] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:55:51] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:55:51] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:55:51] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:55:51] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:55:51] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:55:51] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 20:55:51] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 20:55:51] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 20:55:51] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 20:55:51] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3075302) [2026-01-27 20:55:53] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3075302) [2026-01-27 20:55:53] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3075302) [2026-01-27 20:55:53] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3075302) [2026-01-27 20:55:53] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3075302) [2026-01-27 20:55:53] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=3075302) [2026-01-27 20:55:53] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3075302) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3075302) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:08<00:25,  8.52s/it]
(EngineCore_DP0 pid=3075302) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:41<00:45, 22.77s/it]
(EngineCore_DP0 pid=3075302) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [01:17<00:28, 28.93s/it]
(EngineCore_DP0 pid=3075302) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:59<00:00, 34.09s/it]
(EngineCore_DP0 pid=3075302) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:59<00:00, 29.88s/it]
(EngineCore_DP0 pid=3075302) 
(EngineCore_DP0 pid=3075302) [2026-01-27 20:57:54] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 8192] -> 1D uint8
(EngineCore_DP0 pid=3075302) [2026-01-27 20:57:54] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 36700160 bytes
(EngineCore_DP0 pid=3075302) [2026-01-27 20:57:54] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 8192] -> 1D uint8
(EngineCore_DP0 pid=3075302) [2026-01-27 20:57:54] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 26214400 bytes
(EngineCore_DP0 pid=3075302) [2026-01-27 20:57:54] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 8192] -> 1D uint8
(EngineCore_DP0 pid=3075302) [2026-01-27 20:57:54] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 141557760 bytes
(EngineCore_DP0 pid=3075302) [2026-01-27 20:57:54] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 22144] -> 1D uint8
(EngineCore_DP0 pid=3075302) [2026-01-27 20:57:54] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 70860800 bytes
(EngineCore_DP0 pid=3075302) 2026-01-27 20:58:03,282 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3075302) 2026-01-27 20:58:03,681 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   0%|          | 1/256 [00:00<01:06,  3.84it/s]
Adding requests:   1%|          | 3/256 [00:00<00:30,  8.27it/s]
Adding requests:   2%|▏         | 6/256 [00:00<00:18, 13.83it/s]
Adding requests:   4%|▎         | 9/256 [00:00<00:13, 18.07it/s]
Adding requests:   5%|▌         | 13/256 [00:00<00:10, 23.89it/s]
Adding requests:   7%|▋         | 19/256 [00:00<00:07, 33.19it/s]
Adding requests:  11%|█         | 28/256 [00:00<00:04, 48.08it/s]
Adding requests:  15%|█▍        | 38/256 [00:01<00:03, 62.46it/s]
Adding requests:  20%|█▉        | 51/256 [00:01<00:02, 81.11it/s]
Adding requests:  26%|██▌       | 67/256 [00:01<00:01, 102.54it/s]
Adding requests:  33%|███▎      | 84/256 [00:01<00:01, 120.65it/s]
Adding requests:  42%|████▏     | 107/256 [00:01<00:00, 151.73it/s]
Adding requests:  52%|█████▏    | 133/256 [00:01<00:00, 180.63it/s]
Adding requests:  62%|██████▏   | 158/256 [00:01<00:00, 126.75it/s]
Adding requests:  73%|███████▎  | 186/256 [00:01<00:00, 157.46it/s]
Adding requests:  84%|████████▍ | 216/256 [00:02<00:00, 189.63it/s]
Adding requests: 100%|██████████| 256/256 [00:02<00:00, 239.27it/s]
Adding requests: 100%|██████████| 256/256 [00:02<00:00, 116.69it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/256 [00:37<2:39:08, 37.45s/it, est. speed input: 0.43 toks/s, output: 6.84 toks/s]
Processed prompts:   1%|          | 2/256 [00:37<1:05:41, 15.52s/it, est. speed input: 0.85 toks/s, output: 13.61 toks/s]
Processed prompts:   3%|▎         | 7/256 [00:37<12:29,  3.01s/it, est. speed input: 2.96 toks/s, output: 47.42 toks/s]  
Processed prompts:   4%|▍         | 11/256 [00:37<06:28,  1.59s/it, est. speed input: 4.64 toks/s, output: 74.18 toks/s]
Processed prompts:   7%|▋         | 17/256 [00:38<03:11,  1.25it/s, est. speed input: 7.13 toks/s, output: 114.02 toks/s]
Processed prompts:   9%|▉         | 24/256 [00:38<01:42,  2.25it/s, est. speed input: 10.02 toks/s, output: 160.31 toks/s]
Processed prompts:  14%|█▎        | 35/256 [00:38<00:50,  4.39it/s, est. speed input: 14.54 toks/s, output: 232.59 toks/s]
Processed prompts:  19%|█▉        | 49/256 [00:38<00:25,  8.07it/s, est. speed input: 20.27 toks/s, output: 324.30 toks/s]
Processed prompts:  25%|██▍       | 63/256 [00:38<00:15, 12.84it/s, est. speed input: 25.96 toks/s, output: 415.35 toks/s]
Processed prompts:  30%|██▉       | 76/256 [00:39<00:09, 18.10it/s, est. speed input: 31.18 toks/s, output: 498.85 toks/s]
Processed prompts:  34%|███▍      | 88/256 [00:39<00:07, 23.67it/s, est. speed input: 35.94 toks/s, output: 575.11 toks/s]
Processed prompts:  39%|███▊      | 99/256 [00:39<00:05, 29.12it/s, est. speed input: 40.27 toks/s, output: 644.25 toks/s]
Processed prompts:  43%|████▎     | 109/256 [00:39<00:04, 34.97it/s, est. speed input: 44.18 toks/s, output: 706.90 toks/s]
Processed prompts:  46%|████▋     | 119/256 [00:39<00:03, 39.63it/s, est. speed input: 48.03 toks/s, output: 768.51 toks/s]
Processed prompts:  50%|█████     | 128/256 [00:39<00:02, 44.74it/s, est. speed input: 51.50 toks/s, output: 823.93 toks/s]
Processed prompts:  54%|█████▎    | 137/256 [00:39<00:02, 49.80it/s, est. speed input: 54.94 toks/s, output: 879.06 toks/s]
Processed prompts:  57%|█████▋    | 145/256 [00:40<00:02, 53.20it/s, est. speed input: 57.97 toks/s, output: 927.58 toks/s]
Processed prompts:  60%|█████▉    | 153/256 [00:40<00:01, 56.18it/s, est. speed input: 60.99 toks/s, output: 975.81 toks/s]
Processed prompts:  62%|██████▎   | 160/256 [00:40<00:01, 57.22it/s, est. speed input: 63.60 toks/s, output: 1017.52 toks/s]
Processed prompts:  65%|██████▌   | 167/256 [00:40<00:01, 58.54it/s, est. speed input: 66.19 toks/s, output: 1059.09 toks/s]
Processed prompts:  68%|██████▊   | 174/256 [00:40<00:01, 58.36it/s, est. speed input: 68.76 toks/s, output: 1100.19 toks/s]
Processed prompts:  71%|███████   | 181/256 [00:40<00:01, 60.99it/s, est. speed input: 71.35 toks/s, output: 1141.58 toks/s]
Processed prompts:  73%|███████▎  | 188/256 [00:40<00:01, 61.85it/s, est. speed input: 73.91 toks/s, output: 1182.55 toks/s]
Processed prompts:  76%|███████▌  | 195/256 [00:40<00:01, 46.00it/s, est. speed input: 76.20 toks/s, output: 1219.18 toks/s]
Processed prompts:  79%|███████▉  | 202/256 [00:41<00:01, 42.30it/s, est. speed input: 78.56 toks/s, output: 1256.88 toks/s]
Processed prompts:  81%|████████  | 207/256 [00:41<00:01, 43.29it/s, est. speed input: 80.29 toks/s, output: 1284.66 toks/s]
Processed prompts:  83%|████████▎ | 212/256 [00:41<00:01, 42.84it/s, est. speed input: 81.99 toks/s, output: 1311.87 toks/s]
Processed prompts:  85%|████████▍ | 217/256 [00:41<00:01, 37.06it/s, est. speed input: 83.55 toks/s, output: 1336.78 toks/s]
Processed prompts:  87%|████████▋ | 222/256 [00:41<00:01, 33.64it/s, est. speed input: 85.09 toks/s, output: 1361.52 toks/s]
Processed prompts:  88%|████████▊ | 226/256 [00:41<00:00, 34.28it/s, est. speed input: 86.40 toks/s, output: 1382.42 toks/s]
Processed prompts:  90%|████████▉ | 230/256 [00:42<00:00, 30.48it/s, est. speed input: 87.57 toks/s, output: 1401.05 toks/s]
Processed prompts:  91%|█████████▏| 234/256 [00:42<00:00, 28.09it/s, est. speed input: 88.72 toks/s, output: 1419.57 toks/s]
Processed prompts:  93%|█████████▎| 238/256 [00:42<00:00, 26.78it/s, est. speed input: 89.88 toks/s, output: 1438.12 toks/s]
Processed prompts:  95%|█████████▍| 242/256 [00:42<00:00, 25.99it/s, est. speed input: 91.04 toks/s, output: 1456.60 toks/s]
Processed prompts:  96%|█████████▌| 245/256 [00:42<00:00, 20.62it/s, est. speed input: 91.64 toks/s, output: 1466.20 toks/s]
Processed prompts:  97%|█████████▋| 248/256 [00:43<00:00, 17.83it/s, est. speed input: 92.25 toks/s, output: 1475.99 toks/s]
Processed prompts:  98%|█████████▊| 250/256 [00:43<00:00, 16.50it/s, est. speed input: 92.65 toks/s, output: 1482.40 toks/s]
Processed prompts:  98%|█████████▊| 252/256 [00:43<00:00, 15.19it/s, est. speed input: 93.02 toks/s, output: 1488.40 toks/s]
Processed prompts:  99%|█████████▉| 254/256 [00:43<00:00, 14.57it/s, est. speed input: 93.43 toks/s, output: 1494.84 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:43<00:00, 13.42it/s, est. speed input: 93.77 toks/s, output: 1500.27 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:43<00:00, 13.42it/s, est. speed input: 93.77 toks/s, output: 1500.27 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:43<00:00,  5.86it/s, est. speed input: 93.77 toks/s, output: 1500.27 toks/s]
[rank0]:[W127 20:59:00.096393281 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 214.7s

测试结果:
  Requests/s:   5.57
  Tokens/s:     1514.28
  Total Reqs:   256
  Elapsed:      45.98s

  [Decode 分析]
  Total Decode Tokens:  65536
  Decode Tokens/s:      1425.21

============================================================
[4/4] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Qwen2.5-14B-FP8                                 │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     decode                                          │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 8192 (= 512 x 16)
│   M_decode      = 512
│   batched_tokens = 512 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 16
│   --output-len             = 256
│   --num-prompts            = 512
│   --max-num-seqs           = 512
│   --max-model-len          = 272
│   --max-num-batched-tokens = 512
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 1
│   N_decode  = 256
├─────────────────────────────────────────────────────────────┤
│ 编译模式: --enforce-eager
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-27 20:59:22 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
WARNING 01-27 20:59:22 [vllm.py:622] Enforce eager set, overriding optimization level to -O0
(EngineCore_DP0 pid=3078475) [INFO] Loading compress extension: cusparselt_compress_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3078475) WARNING 01-27 21:01:36 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
Throughput: 5.61 requests/s, 1525.73 total tokens/s, 1435.98 output tokens/s
Total num prompt tokens:  8192
Total num output tokens:  131072


─── STDERR ───
[2026-01-27 20:59:22] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 20:59:22] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:59:22] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-27 20:59:22] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:59:22] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:59:22] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:59:22] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:59:22] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:59:22] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:59:22] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 20:59:22] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 20:59:22] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 20:59:22] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 20:59:22] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-27 20:59:26] INFO kernels.py:578: Triton kernel custom ops registered
/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:283: UserWarning: 
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)
    
  warnings.warn(
[2026-01-27 20:59:26] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:59:26] INFO kernels.py:109: Loaded tuned kernel for model: Qwen2.5-14B-FP8
[2026-01-27 20:59:26] INFO kernels.py:155: Dequant+bias kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:59:26] INFO kernels.py:224: FP8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:59:26] INFO kernels.py:348: INT8 quant kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:59:26] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:59:26] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Qwen2.5-14B-FP8
[2026-01-27 20:59:26] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Qwen2.5-14B-FP8'
[2026-01-27 20:59:26] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-27 20:59:26] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-27 20:59:26] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-27 20:59:26] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-27 20:59:26] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3078475) [2026-01-27 20:59:27] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3078475) [2026-01-27 20:59:27] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_GB10_cc121_py312_cu129_aarch64.so
(EngineCore_DP0 pid=3078475) [2026-01-27 20:59:27] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3078475) [2026-01-27 20:59:27] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3078475) [2026-01-27 20:59:27] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: Qwen2.5-14B-FP8
(EngineCore_DP0 pid=3078475) [2026-01-27 20:59:27] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3078475) 
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3078475) 
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:08<00:25,  8.61s/it]
(EngineCore_DP0 pid=3078475) 
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:41<00:45, 22.63s/it]
(EngineCore_DP0 pid=3078475) 
Loading safetensors checkpoint shards:  75% Completed | 3/4 [01:17<00:28, 28.97s/it]
(EngineCore_DP0 pid=3078475) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:59<00:00, 33.98s/it]
(EngineCore_DP0 pid=3078475) 
Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:59<00:00, 29.80s/it]
(EngineCore_DP0 pid=3078475) 
(EngineCore_DP0 pid=3078475) [2026-01-27 21:01:28] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [7168, 8192] -> 1D uint8
(EngineCore_DP0 pid=3078475) [2026-01-27 21:01:28] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 36700160 bytes
(EngineCore_DP0 pid=3078475) [2026-01-27 21:01:28] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 8192] -> 1D uint8
(EngineCore_DP0 pid=3078475) [2026-01-27 21:01:28] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 26214400 bytes
(EngineCore_DP0 pid=3078475) [2026-01-27 21:01:28] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [27648, 8192] -> 1D uint8
(EngineCore_DP0 pid=3078475) [2026-01-27 21:01:28] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 141557760 bytes
(EngineCore_DP0 pid=3078475) [2026-01-27 21:01:28] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [5120, 22144] -> 1D uint8
(EngineCore_DP0 pid=3078475) [2026-01-27 21:01:28] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 70860800 bytes
(EngineCore_DP0 pid=3078475) 2026-01-27 21:01:35,140 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3078475) 2026-01-27 21:01:35,194 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests: 100%|██████████| 512/512 [00:00<00:00, 11721.05it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   0%|          | 1/512 [01:04<9:10:51, 64.68s/it, est. speed input: 0.25 toks/s, output: 3.96 toks/s]
Processed prompts:   1%|          | 3/512 [01:05<2:24:35, 17.04s/it, est. speed input: 0.73 toks/s, output: 11.74 toks/s]
Processed prompts:   7%|▋         | 34/512 [01:06<07:58,  1.00s/it, est. speed input: 8.23 toks/s, output: 131.62 toks/s]
Processed prompts:  12%|█▏        | 63/512 [01:06<03:24,  2.19it/s, est. speed input: 15.09 toks/s, output: 241.45 toks/s]
Processed prompts:  18%|█▊        | 91/512 [01:07<01:52,  3.74it/s, est. speed input: 21.59 toks/s, output: 345.45 toks/s]
Processed prompts:  23%|██▎       | 117/512 [01:08<01:10,  5.60it/s, est. speed input: 27.50 toks/s, output: 440.07 toks/s]
Processed prompts:  28%|██▊       | 141/512 [01:08<00:45,  8.07it/s, est. speed input: 33.02 toks/s, output: 528.36 toks/s]
Processed prompts:  32%|███▏      | 164/512 [01:08<00:30, 11.25it/s, est. speed input: 38.28 toks/s, output: 612.40 toks/s]
Processed prompts:  36%|███▌      | 185/512 [01:08<00:21, 15.02it/s, est. speed input: 43.03 toks/s, output: 688.51 toks/s]
Processed prompts:  40%|████      | 205/512 [01:09<00:15, 19.61it/s, est. speed input: 47.53 toks/s, output: 760.49 toks/s]
Processed prompts:  44%|████▍     | 224/512 [01:09<00:11, 24.92it/s, est. speed input: 51.77 toks/s, output: 828.33 toks/s]
Processed prompts:  47%|████▋     | 242/512 [01:09<00:08, 30.75it/s, est. speed input: 55.76 toks/s, output: 892.09 toks/s]
Processed prompts:  50%|█████     | 258/512 [01:09<00:07, 36.08it/s, est. speed input: 59.25 toks/s, output: 948.05 toks/s]
Processed prompts:  53%|█████▎    | 273/512 [01:09<00:05, 41.46it/s, est. speed input: 62.51 toks/s, output: 1000.23 toks/s]
Processed prompts:  56%|█████▋    | 288/512 [01:10<00:04, 48.69it/s, est. speed input: 65.80 toks/s, output: 1052.76 toks/s]
Processed prompts:  59%|█████▉    | 302/512 [01:10<00:03, 54.10it/s, est. speed input: 68.82 toks/s, output: 1101.13 toks/s]
Processed prompts:  62%|██████▏   | 315/512 [01:10<00:03, 57.78it/s, est. speed input: 71.60 toks/s, output: 1145.58 toks/s]
Processed prompts:  64%|██████▍   | 327/512 [01:10<00:03, 60.36it/s, est. speed input: 74.14 toks/s, output: 1186.31 toks/s]
Processed prompts:  66%|██████▌   | 338/512 [01:10<00:02, 61.64it/s, est. speed input: 76.46 toks/s, output: 1223.32 toks/s]
Processed prompts:  68%|██████▊   | 348/512 [01:10<00:02, 64.06it/s, est. speed input: 78.57 toks/s, output: 1257.07 toks/s]
Processed prompts:  70%|██████▉   | 358/512 [01:11<00:02, 63.87it/s, est. speed input: 80.65 toks/s, output: 1290.32 toks/s]
Processed prompts:  72%|███████▏  | 367/512 [01:11<00:02, 64.74it/s, est. speed input: 82.52 toks/s, output: 1320.28 toks/s]
Processed prompts:  73%|███████▎  | 376/512 [01:11<00:02, 62.99it/s, est. speed input: 84.36 toks/s, output: 1349.74 toks/s]
Processed prompts:  75%|███████▌  | 384/512 [01:11<00:02, 62.45it/s, est. speed input: 86.00 toks/s, output: 1375.92 toks/s]
Processed prompts:  77%|███████▋  | 392/512 [01:11<00:02, 59.09it/s, est. speed input: 87.60 toks/s, output: 1401.54 toks/s]
Processed prompts:  78%|███████▊  | 399/512 [01:11<00:01, 59.24it/s, est. speed input: 89.01 toks/s, output: 1424.23 toks/s]
Processed prompts:  79%|███████▉  | 406/512 [01:11<00:01, 56.64it/s, est. speed input: 90.40 toks/s, output: 1446.41 toks/s]
Processed prompts:  80%|████████  | 412/512 [01:11<00:01, 55.76it/s, est. speed input: 91.59 toks/s, output: 1465.49 toks/s]
Processed prompts:  82%|████████▏ | 418/512 [01:12<00:01, 52.21it/s, est. speed input: 92.75 toks/s, output: 1484.03 toks/s]
Processed prompts:  83%|████████▎ | 424/512 [01:12<00:01, 52.90it/s, est. speed input: 93.94 toks/s, output: 1503.05 toks/s]
Processed prompts:  84%|████████▍ | 430/512 [01:12<00:01, 53.70it/s, est. speed input: 95.13 toks/s, output: 1522.06 toks/s]
Processed prompts:  85%|████████▌ | 436/512 [01:12<00:01, 50.98it/s, est. speed input: 96.28 toks/s, output: 1540.47 toks/s]
Processed prompts:  86%|████████▋ | 442/512 [01:12<00:01, 41.22it/s, est. speed input: 97.32 toks/s, output: 1557.06 toks/s]
Processed prompts:  87%|████████▋ | 447/512 [01:12<00:01, 42.94it/s, est. speed input: 98.28 toks/s, output: 1572.46 toks/s]
Processed prompts:  88%|████████▊ | 452/512 [01:12<00:01, 40.66it/s, est. speed input: 99.19 toks/s, output: 1586.98 toks/s]
Processed prompts:  89%|████████▉ | 457/512 [01:13<00:01, 35.04it/s, est. speed input: 100.02 toks/s, output: 1600.27 toks/s]
Processed prompts:  90%|█████████ | 462/512 [01:13<00:01, 32.09it/s, est. speed input: 100.85 toks/s, output: 1613.61 toks/s]
Processed prompts:  91%|█████████ | 466/512 [01:13<00:01, 32.71it/s, est. speed input: 101.56 toks/s, output: 1625.03 toks/s]
Processed prompts:  92%|█████████▏| 470/512 [01:13<00:01, 28.68it/s, est. speed input: 102.17 toks/s, output: 1634.77 toks/s]
Processed prompts:  93%|█████████▎| 474/512 [01:13<00:01, 26.66it/s, est. speed input: 102.79 toks/s, output: 1644.69 toks/s]
Processed prompts:  93%|█████████▎| 478/512 [01:13<00:01, 25.40it/s, est. speed input: 103.41 toks/s, output: 1654.61 toks/s]
Processed prompts:  94%|█████████▍| 482/512 [01:14<00:01, 24.68it/s, est. speed input: 104.03 toks/s, output: 1664.55 toks/s]
Processed prompts:  95%|█████████▍| 485/512 [01:14<00:01, 19.59it/s, est. speed input: 104.32 toks/s, output: 1669.12 toks/s]
Processed prompts:  95%|█████████▌| 488/512 [01:14<00:01, 16.68it/s, est. speed input: 104.60 toks/s, output: 1673.60 toks/s]
Processed prompts:  96%|█████████▌| 490/512 [01:14<00:01, 15.50it/s, est. speed input: 104.79 toks/s, output: 1676.71 toks/s]
Processed prompts:  96%|█████████▌| 492/512 [01:14<00:01, 14.63it/s, est. speed input: 104.99 toks/s, output: 1679.85 toks/s]
Processed prompts:  96%|█████████▋| 494/512 [01:15<00:01, 13.98it/s, est. speed input: 105.19 toks/s, output: 1683.00 toks/s]
Processed prompts:  97%|█████████▋| 496/512 [01:15<00:01, 13.52it/s, est. speed input: 105.39 toks/s, output: 1686.17 toks/s]
Processed prompts:  97%|█████████▋| 497/512 [01:29<00:01, 13.52it/s, est. speed input: 105.45 toks/s, output: 1687.26 toks/s]
Processed prompts:  97%|█████████▋| 498/512 [01:30<00:29,  2.12s/it, est. speed input: 87.64 toks/s, output: 1402.31 toks/s] 
Processed prompts:  98%|█████████▊| 501/512 [01:31<00:15,  1.37s/it, est. speed input: 88.01 toks/s, output: 1408.22 toks/s]
Processed prompts: 100%|█████████▉| 511/512 [01:31<00:00,  1.95it/s, est. speed input: 89.62 toks/s, output: 1433.89 toks/s]
Processed prompts: 100%|██████████| 512/512 [01:31<00:00,  1.95it/s, est. speed input: 89.79 toks/s, output: 1436.70 toks/s]
Processed prompts: 100%|██████████| 512/512 [01:31<00:00,  5.61it/s, est. speed input: 89.79 toks/s, output: 1436.70 toks/s]
[rank0]:[W127 21:03:08.313375002 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 234.9s

测试结果:
  Requests/s:   5.61
  Tokens/s:     1525.73
  Total Reqs:   512
  Elapsed:      91.28s

  [Decode 分析]
  Total Decode Tokens:  131072
  Decode Tokens/s:      1435.98


------------------------------------------------------------
  生成 CSV: Qwen2.5-14B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/decode/GB10_cc121_FP8E4M3_py312_cu129_aarch64/cusparselt/2_10/Qwen2.5-14B-FP8_decode.csv

预览:
------------------------------------------------------------
M_decode,prompt_len,max_num_seqs,num_prompts,N_decode,output_len,requests_per_s,tokens_per_s,elapsed_time_s
64,16,64,64,256,256,2.4654,670.6002,25.9588
128,16,128,128,256,256,4.0682,1106.5621,31.4632
256,16,256,256,256,256,5.5672,1514.2846,45.9834
512,16,512,512,256,256,5.6093,1525.7269,91.2771

------------------------------------------------------------

[INFO] 完成: 4 成功, 0 失败


============================================================
  Benchmark 完成!
============================================================


总计: 20 成功, 0 失败
============================================================
