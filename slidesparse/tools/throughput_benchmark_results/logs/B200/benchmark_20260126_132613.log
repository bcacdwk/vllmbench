======================================================================
SlideSparse vLLM Throughput Benchmark Log
Created: 2026-01-26 13:26:13
======================================================================

原始命令:
  /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model llama3.2-1b-int8 --backend cublaslt,cusparselt --stage prefill --sparsity 2_4,2_6,2_8,2_10 --M 512,1024,2048,4096,8192,16384,32768,65536

命令行参数:
  --model: llama3.2-1b-int8
  --backend: cublaslt,cusparselt
  --sparsity: 2_4,2_6,2_8,2_10
  --stage: prefill
  --M: 512,1024,2048,4096,8192,16384,32768,65536
  --N: None
  --inner-32: False
  --eager: False
  --gpu-id: 0
  --gpu-mem: 0.8
  --dry-run: False
  --list-models: False

硬件信息:
  GPU: B200
  Compute Capability: cc100
  VRAM: 178.4 GB
  CUDA: 12.9
  Python: py312

Backend 环境变量 (初始状态):
  DISABLE_SLIDESPARSE: 未设置
  USE_CUBLASLT: 未设置
  USE_CUSPARSELT: 未设置
  SPARSITY: 未设置
  INNER_DTYPE_32: 未设置

======================================================================


============================================================
  Llama3.2-1B-INT8 | cuBLASLt | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints/Llama3.2-1B-INT8
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cublaslt

============================================================
[1/8] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:26:20 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=491950) WARNING 01-26 13:26:32 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=491950) WARNING 01-26 13:26:39 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 40.79 requests/s, 20925.80 total tokens/s, 40.79 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-26 13:26:20] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:26:20] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:26:20] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:26:20] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:26:20] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:26:20] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:26:20] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:26:20] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:26:20] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:26:20] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:26:20] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:26:20] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:26:20] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:26:20] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:26:26] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:26:27] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:26:27] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:26:27] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:26:27] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:26:27] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:26:27] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:26:27] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:26:27] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:26:27] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:26:27] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:26:27] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:26:27] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:26:27] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=491950) [2026-01-26 13:26:27] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=491950) [2026-01-26 13:26:27] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=491950) [2026-01-26 13:26:27] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=491950) [2026-01-26 13:26:27] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=491950) [2026-01-26 13:26:27] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=491950) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=491950) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.46it/s]
(EngineCore_DP0 pid=491950) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.46it/s]
(EngineCore_DP0 pid=491950) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=491950) 2026-01-26 13:26:39,505 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=491950) 2026-01-26 13:26:39,528 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=491950) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 13.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 13.60it/s]
(EngineCore_DP0 pid=491950) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 17.87it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  58%|█████▊    | 74/128 [00:00<00:00, 731.07it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 353.72it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   9%|▊         | 11/128 [00:00<00:01, 91.83it/s, est. speed input: 47017.92 toks/s, output: 91.83 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:01, 56.50it/s, est. speed input: 30790.94 toks/s, output: 60.14 toks/s]
Processed prompts:  22%|██▏       | 28/128 [00:00<00:01, 50.81it/s, est. speed input: 28027.87 toks/s, output: 54.74 toks/s]
Processed prompts:  27%|██▋       | 34/128 [00:00<00:01, 48.33it/s, est. speed input: 26806.21 toks/s, output: 52.36 toks/s]
Processed prompts:  30%|███       | 39/128 [00:00<00:01, 47.03it/s, est. speed input: 26153.03 toks/s, output: 51.08 toks/s]
Processed prompts:  34%|███▍      | 44/128 [00:00<00:01, 46.15it/s, est. speed input: 25681.24 toks/s, output: 50.16 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:00<00:01, 45.41it/s, est. speed input: 25295.99 toks/s, output: 49.41 toks/s]
Processed prompts:  42%|████▏     | 54/128 [00:01<00:01, 45.10it/s, est. speed input: 25032.82 toks/s, output: 48.89 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:01<00:01, 44.83it/s, est. speed input: 24808.40 toks/s, output: 48.45 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:01<00:01, 44.72it/s, est. speed input: 24634.69 toks/s, output: 48.11 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:01<00:01, 44.63it/s, est. speed input: 24487.13 toks/s, output: 47.83 toks/s]
Processed prompts:  58%|█████▊    | 74/128 [00:01<00:01, 44.63it/s, est. speed input: 24369.69 toks/s, output: 47.60 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:01<00:01, 44.50it/s, est. speed input: 24251.14 toks/s, output: 47.37 toks/s]
Processed prompts:  66%|██████▌   | 84/128 [00:01<00:00, 44.46it/s, est. speed input: 24154.29 toks/s, output: 47.18 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:01<00:00, 44.32it/s, est. speed input: 24055.90 toks/s, output: 46.98 toks/s]
Processed prompts:  73%|███████▎  | 94/128 [00:02<00:00, 44.30it/s, est. speed input: 23977.51 toks/s, output: 46.83 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:02<00:00, 44.31it/s, est. speed input: 23909.01 toks/s, output: 46.70 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:02<00:00, 44.28it/s, est. speed input: 23845.10 toks/s, output: 46.57 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:02<00:00, 44.35it/s, est. speed input: 23794.82 toks/s, output: 46.47 toks/s]
Processed prompts:  89%|████████▉ | 114/128 [00:02<00:00, 44.28it/s, est. speed input: 23738.89 toks/s, output: 46.36 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:02<00:00, 44.28it/s, est. speed input: 23692.40 toks/s, output: 46.27 toks/s]
Processed prompts:  97%|█████████▋| 124/128 [00:02<00:00, 44.24it/s, est. speed input: 23645.99 toks/s, output: 46.18 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 44.24it/s, est. speed input: 23616.78 toks/s, output: 46.13 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 46.12it/s, est. speed input: 23616.78 toks/s, output: 46.13 toks/s]
[rank0]:[W126 13:26:44.669950907 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 32.6s

测试结果:
  Requests/s:   40.79
  Tokens/s:     20925.80
  Total Reqs:   128
  Elapsed:      3.14s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     20885.01

============================================================
[2/8] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:26:53 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=492981) WARNING 01-26 13:27:05 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=492981) WARNING 01-26 13:27:12 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 40.39 requests/s, 41394.72 total tokens/s, 40.39 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-26 13:26:52] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:26:53] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:26:53] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:26:53] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:26:53] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:26:53] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:26:53] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:26:53] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:26:53] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:26:53] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:26:53] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:26:53] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:26:53] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:26:53] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:26:59] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:26:59] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:26:59] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:26:59] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:26:59] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:26:59] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:26:59] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:26:59] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:26:59] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:26:59] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:26:59] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:26:59] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:26:59] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:26:59] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=492981) [2026-01-26 13:27:00] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=492981) [2026-01-26 13:27:00] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=492981) [2026-01-26 13:27:00] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=492981) [2026-01-26 13:27:00] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=492981) [2026-01-26 13:27:00] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=492981) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=492981) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.46it/s]
(EngineCore_DP0 pid=492981) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.46it/s]
(EngineCore_DP0 pid=492981) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=492981) 2026-01-26 13:27:12,936 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=492981) 2026-01-26 13:27:12,961 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=492981) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 14.67it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 14.66it/s]
(EngineCore_DP0 pid=492981) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 17.08it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  31%|███▏      | 40/128 [00:00<00:00, 399.44it/s]
Adding requests:  66%|██████▋   | 85/128 [00:00<00:00, 425.79it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 426.68it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   5%|▌         | 7/128 [00:00<00:02, 57.81it/s, est. speed input: 59200.80 toks/s, output: 57.81 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:02, 49.26it/s, est. speed input: 51674.18 toks/s, output: 50.46 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:00<00:02, 47.07it/s, est. speed input: 49680.69 toks/s, output: 48.52 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:00<00:02, 45.90it/s, est. speed input: 48584.17 toks/s, output: 47.44 toks/s]
Processed prompts:  22%|██▏       | 28/128 [00:00<00:02, 44.82it/s, est. speed input: 47674.41 toks/s, output: 46.56 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:00<00:02, 44.65it/s, est. speed input: 47309.63 toks/s, output: 46.20 toks/s]
Processed prompts:  30%|██▉       | 38/128 [00:00<00:02, 44.54it/s, est. speed input: 47044.92 toks/s, output: 45.94 toks/s]
Processed prompts:  34%|███▎      | 43/128 [00:00<00:01, 44.40it/s, est. speed input: 46818.81 toks/s, output: 45.72 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:01<00:01, 44.25it/s, est. speed input: 46618.29 toks/s, output: 45.53 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:01<00:01, 44.21it/s, est. speed input: 46479.89 toks/s, output: 45.39 toks/s]
Processed prompts:  45%|████▌     | 58/128 [00:01<00:01, 44.19it/s, est. speed input: 46365.21 toks/s, output: 45.28 toks/s]
Processed prompts:  49%|████▉     | 63/128 [00:01<00:01, 44.20it/s, est. speed input: 46277.28 toks/s, output: 45.19 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:01<00:01, 44.15it/s, est. speed input: 46188.75 toks/s, output: 45.11 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:01<00:01, 44.18it/s, est. speed input: 46126.38 toks/s, output: 45.05 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:01<00:01, 44.17it/s, est. speed input: 46068.02 toks/s, output: 44.99 toks/s]
Processed prompts:  65%|██████▍   | 83/128 [00:01<00:01, 43.90it/s, est. speed input: 45957.63 toks/s, output: 44.88 toks/s]
Processed prompts:  69%|██████▉   | 88/128 [00:01<00:00, 43.98it/s, est. speed input: 45916.43 toks/s, output: 44.84 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:02<00:00, 44.12it/s, est. speed input: 45895.21 toks/s, output: 44.82 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:02<00:00, 44.18it/s, est. speed input: 45867.86 toks/s, output: 44.79 toks/s]
Processed prompts:  80%|████████  | 103/128 [00:02<00:00, 43.84it/s, est. speed input: 45778.65 toks/s, output: 44.71 toks/s]
Processed prompts:  84%|████████▍ | 108/128 [00:02<00:00, 44.07it/s, est. speed input: 45775.05 toks/s, output: 44.70 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:02<00:00, 44.21it/s, est. speed input: 45767.60 toks/s, output: 44.69 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:02<00:00, 44.22it/s, est. speed input: 45748.07 toks/s, output: 44.68 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:02<00:00, 43.94it/s, est. speed input: 45688.41 toks/s, output: 44.62 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 44.19it/s, est. speed input: 45695.10 toks/s, output: 44.62 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 44.19it/s, est. speed input: 45695.10 toks/s, output: 44.62 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 44.62it/s, est. speed input: 45695.10 toks/s, output: 44.62 toks/s]
[rank0]:[W126 13:27:17.183041959 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 33.8s

测试结果:
  Requests/s:   40.39
  Tokens/s:     41394.72
  Total Reqs:   128
  Elapsed:      3.17s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     41354.34

============================================================
[3/8] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:27:27 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=494052) WARNING 01-26 13:27:40 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=494052) WARNING 01-26 13:27:47 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 79.50 requests/s, 81491.68 total tokens/s, 79.50 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-26 13:27:27] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:27:27] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:27:27] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:27:27] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:27:27] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:27:27] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:27:27] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:27:27] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:27:27] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:27:27] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:27:27] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:27:27] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:27:27] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:27:27] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:27:34] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:27:34] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:27:34] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:27:34] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:27:34] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:27:34] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:27:34] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:27:34] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:27:34] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:27:34] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:27:34] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:27:34] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:27:34] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:27:34] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=494052) [2026-01-26 13:27:35] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=494052) [2026-01-26 13:27:35] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=494052) [2026-01-26 13:27:35] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=494052) [2026-01-26 13:27:35] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=494052) [2026-01-26 13:27:35] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=494052) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=494052) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.57it/s]
(EngineCore_DP0 pid=494052) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.57it/s]
(EngineCore_DP0 pid=494052) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=494052) 2026-01-26 13:27:47,342 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=494052) 2026-01-26 13:27:47,365 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=494052) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:00<00:00, 19.68it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 16.81it/s]
(EngineCore_DP0 pid=494052) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 19.84it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 19.81it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   0%|          | 1/256 [00:00<01:15,  3.36it/s]
Adding requests:  11%|█         | 28/256 [00:00<00:02, 89.71it/s]
Adding requests:  28%|██▊       | 72/256 [00:00<00:00, 200.02it/s]
Adding requests:  46%|████▌     | 117/256 [00:00<00:00, 276.84it/s]
Adding requests:  64%|██████▎   | 163/256 [00:00<00:00, 331.05it/s]
Adding requests:  81%|████████▏ | 208/256 [00:00<00:00, 365.72it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 400.12it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 283.40it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  21%|██        | 54/256 [00:00<00:00, 510.04it/s, est. speed input: 522309.13 toks/s, output: 510.04 toks/s]
Processed prompts:  41%|████▏     | 106/256 [00:00<00:01, 139.37it/s, est. speed input: 160546.39 toks/s, output: 156.78 toks/s]
Processed prompts:  52%|█████▏    | 133/256 [00:00<00:01, 122.31it/s, est. speed input: 141902.22 toks/s, output: 138.58 toks/s]
Processed prompts:  59%|█████▉    | 152/256 [00:01<00:00, 111.71it/s, est. speed input: 132058.66 toks/s, output: 128.96 toks/s]
Processed prompts:  65%|██████▌   | 167/256 [00:01<00:00, 108.54it/s, est. speed input: 128400.13 toks/s, output: 125.39 toks/s]
Processed prompts:  70%|███████   | 180/256 [00:01<00:00, 102.71it/s, est. speed input: 124132.06 toks/s, output: 121.22 toks/s]
Processed prompts:  75%|███████▌  | 192/256 [00:01<00:00, 100.13it/s, est. speed input: 121672.04 toks/s, output: 118.82 toks/s]
Processed prompts:  79%|███████▉  | 203/256 [00:01<00:00, 100.31it/s, est. speed input: 120516.69 toks/s, output: 117.69 toks/s]
Processed prompts:  84%|████████▎ | 214/256 [00:01<00:00, 95.89it/s, est. speed input: 118081.60 toks/s, output: 115.31 toks/s] 
Processed prompts:  88%|████████▊ | 224/256 [00:01<00:00, 94.81it/s, est. speed input: 116722.06 toks/s, output: 113.99 toks/s]
Processed prompts:  91%|█████████▏| 234/256 [00:02<00:00, 93.89it/s, est. speed input: 115492.57 toks/s, output: 112.79 toks/s]
Processed prompts:  95%|█████████▌| 244/256 [00:02<00:00, 93.24it/s, est. speed input: 114400.98 toks/s, output: 111.72 toks/s]
Processed prompts:  99%|█████████▉| 254/256 [00:02<00:00, 92.76it/s, est. speed input: 113413.72 toks/s, output: 110.76 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 92.76it/s, est. speed input: 113233.73 toks/s, output: 110.58 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 110.58it/s, est. speed input: 113233.73 toks/s, output: 110.58 toks/s]
[rank0]:[W126 13:27:52.873705303 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 34.5s

测试结果:
  Requests/s:   79.50
  Tokens/s:     81491.68
  Total Reqs:   256
  Elapsed:      3.22s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     81412.18

============================================================
[4/8] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:28:03 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=495124) WARNING 01-26 13:28:15 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=495124) WARNING 01-26 13:28:22 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 168.56 requests/s, 172769.33 total tokens/s, 168.56 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-26 13:28:03] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:28:03] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:28:03] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:28:03] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:28:03] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:28:03] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:28:03] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:28:03] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:28:03] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:28:03] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:28:03] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:28:03] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:28:03] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:28:03] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:28:09] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:28:10] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:28:10] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:28:10] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:28:10] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:28:10] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:28:10] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:28:10] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:28:10] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:28:10] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:28:10] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:28:10] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:28:10] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:28:10] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=495124) [2026-01-26 13:28:10] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=495124) [2026-01-26 13:28:11] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=495124) [2026-01-26 13:28:11] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=495124) [2026-01-26 13:28:11] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=495124) [2026-01-26 13:28:11] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=495124) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=495124) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.62it/s]
(EngineCore_DP0 pid=495124) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.62it/s]
(EngineCore_DP0 pid=495124) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=495124) 2026-01-26 13:28:22,333 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=495124) 2026-01-26 13:28:22,355 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=495124) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 3/4 [00:00<00:00, 20.60it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 18.10it/s]
(EngineCore_DP0 pid=495124) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 20.60it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 20.58it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   8%|▊         | 40/512 [00:00<00:01, 396.46it/s]
Adding requests:  16%|█▋        | 84/512 [00:00<00:01, 418.99it/s]
Adding requests:  25%|██▌       | 129/512 [00:00<00:00, 429.91it/s]
Adding requests:  34%|███▍      | 173/512 [00:00<00:00, 432.84it/s]
Adding requests:  43%|████▎     | 219/512 [00:00<00:00, 440.25it/s]
Adding requests:  52%|█████▏    | 264/512 [00:00<00:00, 440.19it/s]
Adding requests:  60%|██████    | 309/512 [00:00<00:00, 442.36it/s]
Adding requests:  70%|██████▉   | 356/512 [00:00<00:00, 447.90it/s]
Adding requests:  79%|███████▊  | 403/512 [00:00<00:00, 453.28it/s]
Adding requests:  88%|████████▊ | 450/512 [00:01<00:00, 455.46it/s]
Adding requests:  97%|█████████▋| 497/512 [00:01<00:00, 458.59it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 446.00it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  38%|███▊      | 194/512 [00:00<00:00, 1613.49it/s, est. speed input: 1652323.25 toks/s, output: 1613.52 toks/s]
Processed prompts:  70%|██████▉   | 356/512 [00:01<00:00, 303.48it/s, est. speed input: 358329.22 toks/s, output: 349.93 toks/s]   
Processed prompts:  85%|████████▍ | 434/512 [00:01<00:00, 252.27it/s, est. speed input: 303149.23 toks/s, output: 296.04 toks/s]
Processed prompts:  95%|█████████▍| 484/512 [00:01<00:00, 235.91it/s, est. speed input: 285977.03 toks/s, output: 279.27 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:01<00:00, 235.91it/s, est. speed input: 277636.78 toks/s, output: 271.13 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:01<00:00, 271.11it/s, est. speed input: 277636.78 toks/s, output: 271.13 toks/s]
[rank0]:[W126 13:28:27.975663697 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 35.0s

测试结果:
  Requests/s:   168.56
  Tokens/s:     172769.33
  Total Reqs:   512
  Elapsed:      3.04s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     172600.77

============================================================
[5/8] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:28:41 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=496207) WARNING 01-26 13:28:53 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=496207) WARNING 01-26 13:29:00 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 314.33 requests/s, 322186.72 total tokens/s, 314.33 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-26 13:28:40] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:28:41] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:28:41] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:28:41] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:28:41] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:28:41] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:28:41] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:28:41] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:28:41] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:28:41] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:28:41] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:28:41] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:28:41] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:28:41] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:28:47] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:28:47] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:28:47] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:28:47] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:28:47] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:28:47] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:28:47] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:28:47] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:28:47] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:28:47] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:28:47] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:28:47] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:28:47] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:28:47] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=496207) [2026-01-26 13:28:48] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=496207) [2026-01-26 13:28:48] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=496207) [2026-01-26 13:28:48] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=496207) [2026-01-26 13:28:48] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=496207) [2026-01-26 13:28:48] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=496207) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=496207) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.60it/s]
(EngineCore_DP0 pid=496207) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.59it/s]
(EngineCore_DP0 pid=496207) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=496207) 2026-01-26 13:29:00,235 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=496207) 2026-01-26 13:29:00,257 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=496207) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00, 19.97it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 18.42it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 18.58it/s]
(EngineCore_DP0 pid=496207) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  75%|███████▌  | 3/4 [00:00<00:00, 20.03it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 20.18it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   4%|▍         | 40/1024 [00:00<00:02, 396.07it/s]
Adding requests:   8%|▊         | 85/1024 [00:00<00:02, 422.83it/s]
Adding requests:  13%|█▎        | 129/1024 [00:00<00:02, 429.55it/s]
Adding requests:  17%|█▋        | 173/1024 [00:00<00:01, 431.29it/s]
Adding requests:  21%|██▏       | 218/1024 [00:00<00:01, 436.77it/s]
Adding requests:  26%|██▌       | 266/1024 [00:00<00:01, 448.81it/s]
Adding requests:  30%|███       | 311/1024 [00:00<00:01, 448.36it/s]
Adding requests:  35%|███▍      | 358/1024 [00:00<00:01, 452.18it/s]
Adding requests:  39%|███▉      | 404/1024 [00:00<00:01, 454.20it/s]
Adding requests:  44%|████▍     | 451/1024 [00:01<00:01, 456.86it/s]
Adding requests:  49%|████▊     | 498/1024 [00:01<00:01, 458.36it/s]
Adding requests:  53%|█████▎    | 544/1024 [00:01<00:01, 452.19it/s]
Adding requests:  58%|█████▊    | 593/1024 [00:01<00:00, 461.32it/s]
Adding requests:  63%|██████▎   | 641/1024 [00:01<00:00, 464.94it/s]
Adding requests:  67%|██████▋   | 690/1024 [00:01<00:00, 470.09it/s]
Adding requests:  72%|███████▏  | 739/1024 [00:01<00:00, 474.26it/s]
Adding requests:  77%|███████▋  | 787/1024 [00:01<00:00, 470.34it/s]
Adding requests:  82%|████████▏ | 835/1024 [00:01<00:00, 458.65it/s]
Adding requests:  86%|████████▋ | 884/1024 [00:01<00:00, 465.30it/s]
Adding requests:  91%|█████████ | 933/1024 [00:02<00:00, 471.07it/s]
Adding requests:  96%|█████████▌| 981/1024 [00:02<00:00, 471.88it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 458.79it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  68%|██████▊   | 698/1024 [00:00<00:00, 6767.35it/s, est. speed input: 6930249.14 toks/s, output: 6767.48 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:01<00:00, 6767.35it/s, est. speed input: 1023466.05 toks/s, output: 999.47 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:01<00:00, 999.37it/s, est. speed input: 1023466.05 toks/s, output: 999.47 toks/s] 
[rank0]:[W126 13:29:06.326491076 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 38.4s

测试结果:
  Requests/s:   314.33
  Tokens/s:     322186.72
  Total Reqs:   1024
  Elapsed:      3.26s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     321872.39

============================================================
[6/8] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:29:24 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=497378) WARNING 01-26 13:29:36 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=497378) WARNING 01-26 13:29:44 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 459.72 requests/s, 471210.28 total tokens/s, 459.72 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-26 13:29:23] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:29:24] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:29:24] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:29:24] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:29:24] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:29:24] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:29:24] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:29:24] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:29:24] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:29:24] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:29:24] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:29:24] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:29:24] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:29:24] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:29:30] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:29:31] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:29:31] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:29:31] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:29:31] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:29:31] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:29:31] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:29:31] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:29:31] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:29:31] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:29:31] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:29:31] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:29:31] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:29:31] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=497378) [2026-01-26 13:29:32] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=497378) [2026-01-26 13:29:32] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=497378) [2026-01-26 13:29:32] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=497378) [2026-01-26 13:29:32] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=497378) [2026-01-26 13:29:32] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=497378) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=497378) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.49it/s]
(EngineCore_DP0 pid=497378) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.49it/s]
(EngineCore_DP0 pid=497378) 
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=497378) 2026-01-26 13:29:44,038 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=497378) 2026-01-26 13:29:44,062 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=497378) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00, 19.02it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 5/7 [00:00<00:00, 20.45it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 18.79it/s]
(EngineCore_DP0 pid=497378) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00, 19.75it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 18.96it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 19.04it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 40/2048 [00:00<00:05, 394.92it/s]
Adding requests:   4%|▍         | 85/2048 [00:00<00:04, 422.11it/s]
Adding requests:   6%|▋         | 129/2048 [00:00<00:04, 429.33it/s]
Adding requests:   8%|▊         | 173/2048 [00:00<00:04, 429.95it/s]
Adding requests:  11%|█         | 219/2048 [00:00<00:04, 438.11it/s]
Adding requests:  13%|█▎        | 267/2048 [00:00<00:03, 449.07it/s]
Adding requests:  15%|█▌        | 312/2048 [00:00<00:03, 446.96it/s]
Adding requests:  17%|█▋        | 358/2048 [00:00<00:03, 450.44it/s]
Adding requests:  20%|█▉        | 405/2048 [00:00<00:03, 455.25it/s]
Adding requests:  22%|██▏       | 452/2048 [00:01<00:03, 458.47it/s]
Adding requests:  24%|██▍       | 498/2048 [00:01<00:03, 457.37it/s]
Adding requests:  27%|██▋       | 544/2048 [00:01<00:03, 451.24it/s]
Adding requests:  29%|██▉       | 591/2048 [00:01<00:03, 452.76it/s]
Adding requests:  31%|███       | 638/2048 [00:01<00:03, 455.54it/s]
Adding requests:  34%|███▎      | 687/2048 [00:01<00:02, 463.64it/s]
Adding requests:  36%|███▌      | 736/2048 [00:01<00:02, 470.94it/s]
Adding requests:  38%|███▊      | 784/2048 [00:01<00:02, 466.48it/s]
Adding requests:  41%|████      | 831/2048 [00:01<00:02, 454.70it/s]
Adding requests:  43%|████▎     | 879/2048 [00:01<00:02, 460.98it/s]
Adding requests:  45%|████▌     | 927/2048 [00:02<00:02, 466.08it/s]
Adding requests:  48%|████▊     | 975/2048 [00:02<00:02, 467.54it/s]
Adding requests:  50%|████▉     | 1023/2048 [00:02<00:02, 470.82it/s]
Adding requests:  52%|█████▏    | 1071/2048 [00:02<00:02, 466.56it/s]
Adding requests:  55%|█████▍    | 1118/2048 [00:02<00:02, 463.88it/s]
Adding requests:  57%|█████▋    | 1168/2048 [00:02<00:01, 472.15it/s]
Adding requests:  59%|█████▉    | 1218/2048 [00:02<00:01, 479.45it/s]
Adding requests:  62%|██████▏   | 1266/2048 [00:02<00:01, 463.74it/s]
Adding requests:  64%|██████▍   | 1313/2048 [00:02<00:01, 463.88it/s]
Adding requests:  67%|██████▋   | 1362/2048 [00:02<00:01, 468.72it/s]
Adding requests:  69%|██████▉   | 1412/2048 [00:03<00:01, 474.91it/s]
Adding requests:  71%|███████▏  | 1460/2048 [00:03<00:01, 473.75it/s]
Adding requests:  74%|███████▎  | 1509/2048 [00:03<00:01, 478.33it/s]
Adding requests:  76%|███████▌  | 1557/2048 [00:03<00:01, 475.33it/s]
Adding requests:  78%|███████▊  | 1605/2048 [00:03<00:00, 471.53it/s]
Adding requests:  81%|████████  | 1653/2048 [00:03<00:00, 472.66it/s]
Adding requests:  83%|████████▎ | 1701/2048 [00:03<00:00, 468.54it/s]
Adding requests:  85%|████████▌ | 1750/2048 [00:03<00:00, 472.77it/s]
Adding requests:  88%|████████▊ | 1798/2048 [00:03<00:00, 471.49it/s]
Adding requests:  90%|█████████ | 1847/2048 [00:03<00:00, 475.83it/s]
Adding requests:  93%|█████████▎| 1895/2048 [00:04<00:00, 473.37it/s]
Adding requests:  95%|█████████▍| 1943/2048 [00:04<00:00, 474.17it/s]
Adding requests:  97%|█████████▋| 1991/2048 [00:04<00:00, 474.07it/s]
Adding requests: 100%|█████████▉| 2040/2048 [00:04<00:00, 477.48it/s]
Adding requests: 100%|██████████| 2048/2048 [00:04<00:00, 464.16it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:00<00:00, 49938.58it/s, est. speed input: 51155243.84 toks/s, output: 49951.35 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:00<00:00, 49810.00it/s, est. speed input: 51155243.84 toks/s, output: 49951.35 toks/s]
[rank0]:[W126 13:29:50.202999212 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 45.0s

测试结果:
  Requests/s:   459.72
  Tokens/s:     471210.28
  Total Reqs:   2048
  Elapsed:      4.45s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     470750.56

============================================================
[7/8] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:30:18 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=498664) WARNING 01-26 13:30:30 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=498664) WARNING 01-26 13:30:39 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 471.49 requests/s, 483272.59 total tokens/s, 471.49 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-26 13:30:17] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:30:17] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:30:17] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:30:17] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:30:17] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:30:17] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:30:17] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:30:17] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:30:17] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:30:18] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:30:18] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:30:18] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:30:18] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:30:18] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:30:24] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:30:24] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:30:24] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:30:24] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:30:24] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:30:24] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:30:24] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:30:24] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:30:24] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:30:24] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:30:24] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:30:24] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:30:24] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:30:24] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=498664) [2026-01-26 13:30:25] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=498664) [2026-01-26 13:30:25] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=498664) [2026-01-26 13:30:25] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=498664) [2026-01-26 13:30:25] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=498664) [2026-01-26 13:30:25] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=498664) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=498664) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.46it/s]
(EngineCore_DP0 pid=498664) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.46it/s]
(EngineCore_DP0 pid=498664) 
(EngineCore_DP0 pid=498664) [rank0]:W0126 13:30:33.655000 498664 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=498664) [rank0]:W0126 13:30:34.358000 498664 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=498664) [rank0]:W0126 13:30:35.962000 498664 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=498664) [rank0]:W0126 13:30:36.060000 498664 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=498664) 2026-01-26 13:30:39,328 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=498664) 2026-01-26 13:30:39,352 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=498664) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:00, 18.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:00<00:00, 16.13it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 6/11 [00:00<00:00, 16.15it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 9/11 [00:00<00:00, 18.18it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 17.45it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 17.32it/s]
(EngineCore_DP0 pid=498664) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 3/7 [00:00<00:00, 20.02it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00, 20.05it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 20.04it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 41/4096 [00:00<00:10, 403.70it/s]
Adding requests:   2%|▏         | 86/4096 [00:00<00:09, 427.17it/s]
Adding requests:   3%|▎         | 132/4096 [00:00<00:09, 438.96it/s]
Adding requests:   4%|▍         | 176/4096 [00:00<00:08, 439.09it/s]
Adding requests:   5%|▌         | 222/4096 [00:00<00:08, 446.25it/s]
Adding requests:   7%|▋         | 270/4096 [00:00<00:08, 457.36it/s]
Adding requests:   8%|▊         | 316/4096 [00:00<00:08, 454.87it/s]
Adding requests:   9%|▉         | 362/4096 [00:00<00:08, 454.03it/s]
Adding requests:  10%|▉         | 409/4096 [00:00<00:08, 457.29it/s]
Adding requests:  11%|█         | 457/4096 [00:01<00:07, 461.87it/s]
Adding requests:  12%|█▏        | 504/4096 [00:01<00:07, 463.55it/s]
Adding requests:  13%|█▎        | 551/4096 [00:01<00:07, 459.13it/s]
Adding requests:  15%|█▍        | 600/4096 [00:01<00:07, 464.46it/s]
Adding requests:  16%|█▌        | 650/4096 [00:01<00:07, 472.38it/s]
Adding requests:  17%|█▋        | 701/4096 [00:01<00:07, 481.22it/s]
Adding requests:  18%|█▊        | 750/4096 [00:01<00:06, 478.07it/s]
Adding requests:  19%|█▉        | 798/4096 [00:01<00:07, 469.79it/s]
Adding requests:  21%|██        | 846/4096 [00:01<00:07, 462.86it/s]
Adding requests:  22%|██▏       | 896/4096 [00:01<00:06, 471.87it/s]
Adding requests:  23%|██▎       | 944/4096 [00:02<00:06, 473.94it/s]
Adding requests:  24%|██▍       | 993/4096 [00:02<00:06, 476.81it/s]
Adding requests:  25%|██▌       | 1042/4096 [00:02<00:06, 480.11it/s]
Adding requests:  27%|██▋       | 1091/4096 [00:02<00:06, 477.92it/s]
Adding requests:  28%|██▊       | 1139/4096 [00:02<00:06, 471.84it/s]
Adding requests:  29%|██▉       | 1192/4096 [00:02<00:05, 485.88it/s]
Adding requests:  30%|███       | 1241/4096 [00:02<00:05, 485.90it/s]
Adding requests:  31%|███▏      | 1290/4096 [00:02<00:05, 481.74it/s]
Adding requests:  33%|███▎      | 1340/4096 [00:02<00:05, 486.00it/s]
Adding requests:  34%|███▍      | 1389/4096 [00:02<00:05, 486.55it/s]
Adding requests:  35%|███▌      | 1438/4096 [00:03<00:05, 484.95it/s]
Adding requests:  36%|███▋      | 1488/4096 [00:03<00:05, 486.47it/s]
Adding requests:  38%|███▊      | 1538/4096 [00:03<00:05, 489.49it/s]
Adding requests:  39%|███▉      | 1588/4096 [00:03<00:05, 490.97it/s]
Adding requests:  40%|███▉      | 1638/4096 [00:03<00:05, 475.80it/s]
Adding requests:  41%|████      | 1686/4096 [00:03<00:05, 473.23it/s]
Adding requests:  42%|████▏     | 1736/4096 [00:03<00:04, 478.08it/s]
Adding requests:  44%|████▎     | 1784/4096 [00:03<00:04, 474.51it/s]
Adding requests:  45%|████▍     | 1832/4096 [00:03<00:04, 468.13it/s]
Adding requests:  46%|████▌     | 1881/4096 [00:03<00:04, 472.85it/s]
Adding requests:  47%|████▋     | 1929/4096 [00:04<00:04, 474.30it/s]
Adding requests:  48%|████▊     | 1978/4096 [00:04<00:04, 477.25it/s]
Adding requests:  50%|████▉     | 2028/4096 [00:04<00:04, 481.16it/s]
Adding requests:  51%|█████     | 2078/4096 [00:04<00:04, 485.84it/s]
Adding requests:  52%|█████▏    | 2127/4096 [00:04<00:04, 480.18it/s]
Adding requests:  53%|█████▎    | 2176/4096 [00:04<00:04, 476.71it/s]
Adding requests:  54%|█████▍    | 2224/4096 [00:04<00:03, 476.79it/s]
Adding requests:  55%|█████▌    | 2272/4096 [00:04<00:03, 477.40it/s]
Adding requests:  57%|█████▋    | 2320/4096 [00:04<00:03, 474.64it/s]
Adding requests:  58%|█████▊    | 2368/4096 [00:05<00:03, 474.73it/s]
Adding requests:  59%|█████▉    | 2417/4096 [00:05<00:03, 476.11it/s]
Adding requests:  60%|██████    | 2466/4096 [00:05<00:03, 479.36it/s]
Adding requests:  61%|██████▏   | 2514/4096 [00:05<00:03, 477.62it/s]
Adding requests:  63%|██████▎   | 2565/4096 [00:05<00:03, 484.31it/s]
Adding requests:  64%|██████▍   | 2614/4096 [00:05<00:03, 482.62it/s]
Adding requests:  65%|██████▌   | 2664/4096 [00:05<00:02, 486.59it/s]
Adding requests:  66%|██████▌   | 2713/4096 [00:05<00:02, 479.05it/s]
Adding requests:  67%|██████▋   | 2762/4096 [00:05<00:02, 480.67it/s]
Adding requests:  69%|██████▊   | 2811/4096 [00:05<00:02, 476.44it/s]
Adding requests:  70%|██████▉   | 2860/4096 [00:06<00:02, 478.53it/s]
Adding requests:  71%|███████   | 2909/4096 [00:06<00:02, 481.80it/s]
Adding requests:  72%|███████▏  | 2958/4096 [00:06<00:02, 476.43it/s]
Adding requests:  73%|███████▎  | 3006/4096 [00:06<00:02, 469.30it/s]
Adding requests:  75%|███████▍  | 3055/4096 [00:06<00:02, 473.38it/s]
Adding requests:  76%|███████▌  | 3103/4096 [00:06<00:02, 472.11it/s]
Adding requests:  77%|███████▋  | 3151/4096 [00:06<00:01, 473.00it/s]
Adding requests:  78%|███████▊  | 3200/4096 [00:06<00:01, 475.29it/s]
Adding requests:  79%|███████▉  | 3250/4096 [00:06<00:01, 480.62it/s]
Adding requests:  81%|████████  | 3299/4096 [00:06<00:01, 480.55it/s]
Adding requests:  82%|████████▏ | 3348/4096 [00:07<00:01, 482.47it/s]
Adding requests:  83%|████████▎ | 3397/4096 [00:07<00:01, 480.08it/s]
Adding requests:  84%|████████▍ | 3446/4096 [00:07<00:01, 480.02it/s]
Adding requests:  85%|████████▌ | 3495/4096 [00:07<00:01, 476.73it/s]
Adding requests:  86%|████████▋ | 3543/4096 [00:07<00:01, 475.86it/s]
Adding requests:  88%|████████▊ | 3591/4096 [00:07<00:01, 472.97it/s]
Adding requests:  89%|████████▉ | 3639/4096 [00:07<00:00, 469.91it/s]
Adding requests:  90%|█████████ | 3688/4096 [00:07<00:00, 474.39it/s]
Adding requests:  91%|█████████ | 3736/4096 [00:07<00:00, 474.67it/s]
Adding requests:  92%|█████████▏| 3787/4096 [00:07<00:00, 482.79it/s]
Adding requests:  94%|█████████▎| 3836/4096 [00:08<00:00, 484.06it/s]
Adding requests:  95%|█████████▍| 3886/4096 [00:08<00:00, 486.57it/s]
Adding requests:  96%|█████████▌| 3935/4096 [00:08<00:00, 485.05it/s]
Adding requests:  97%|█████████▋| 3984/4096 [00:08<00:00, 485.97it/s]
Adding requests:  98%|█████████▊| 4033/4096 [00:08<00:00, 483.27it/s]
Adding requests: 100%|█████████▉| 4082/4096 [00:08<00:00, 479.58it/s]
Adding requests: 100%|██████████| 4096/4096 [00:08<00:00, 475.35it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:00<00:00, 59635.76it/s, est. speed input: 61079101.48 toks/s, output: 59643.83 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:00<00:00, 59547.50it/s, est. speed input: 61079101.48 toks/s, output: 59643.83 toks/s]
[rank0]:[W126 13:30:50.072771370 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 59.8s

测试结果:
  Requests/s:   471.49
  Tokens/s:     483272.59
  Total Reqs:   4096
  Elapsed:      8.69s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     482801.11

============================================================
[8/8] 测试 M=65536
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuBLASLt [INT32 output]                         │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 65536
│   M_prefill     = 65536 (= 64 x 1024)
│   M_decode      = 64
│   batched_tokens = 65536 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 8192
│   --max-num-seqs           = 64
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 65536
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:31:36 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=500441) WARNING 01-26 13:31:48 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=500441) WARNING 01-26 13:31:56 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 469.75 requests/s, 481497.38 total tokens/s, 469.75 output tokens/s
Total num prompt tokens:  8388608
Total num output tokens:  8192


─── STDERR ───
[2026-01-26 13:31:35] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:31:36] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:31:36] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:31:36] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:31:36] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:31:36] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:31:36] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:31:36] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:31:36] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:31:36] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:31:36] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:31:36] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:31:36] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:31:36] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:31:42] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:31:43] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:31:43] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:31:43] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:31:43] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:31:43] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:31:43] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:31:43] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:31:43] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:31:43] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:31:43] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:31:43] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:31:43] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:31:43] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:44] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:44] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:44] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:44] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:44] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuBLASLt, symmetric=True
(EngineCore_DP0 pid=500441) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=500441) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.60it/s]
(EngineCore_DP0 pid=500441) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.60it/s]
(EngineCore_DP0 pid=500441) 
(EngineCore_DP0 pid=500441) [rank0]:W0126 13:31:52.231000 500441 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=500441) [rank0]:W0126 13:31:52.436000 500441 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:52] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=2048), falling back to default heuristic
[cuBLASLt] INFO: INT8 GEMM only supports INT32 output. inner_dtype parameter is ignored, always using int32.
(EngineCore_DP0 pid=500441) [rank0]:W0126 13:31:53.413000 500441 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=500441) [rank0]:W0126 13:31:53.511000 500441 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:53] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:53] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:53] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:53] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:54] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:54] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:54] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:54] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:54] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:54] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:54] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:54] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:54] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:54] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:54] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:54] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:54] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:54] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:54] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:54] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:54] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:54] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:54] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:54] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:54] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:54] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:54] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:54] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:54] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:54] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:54] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:54] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:54] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:54] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:54] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:54] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:54] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:54] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:54] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:54] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:54] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:54] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:54] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:54] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:54] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:54] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:54] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:54] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:54] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:54] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:54] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:54] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:54] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:54] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:54] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:54] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:54] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:54] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:54] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:54] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:55] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:55] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:55] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=500441) 2026-01-26 13:31:56,423 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=3072, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=16384, K=2048), falling back to default heuristic
(EngineCore_DP0 pid=500441) [2026-01-26 13:31:56] WARNING gemm_wrapper.py:352: cuBLASLt: M=65536 exceeds max searched M=32768 for (N=2048, K=8192), falling back to default heuristic
(EngineCore_DP0 pid=500441) 2026-01-26 13:31:56,455 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=500441) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 2/19 [00:00<00:00, 17.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  21%|██        | 4/19 [00:00<00:00, 17.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  32%|███▏      | 6/19 [00:00<00:00, 17.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 8/19 [00:00<00:00, 17.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 11/19 [00:00<00:00, 18.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▎  | 14/19 [00:00<00:00, 19.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 17/19 [00:00<00:00, 20.14it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:01<00:00, 17.84it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:01<00:00, 18.34it/s]
(EngineCore_DP0 pid=500441) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  18%|█▊        | 2/11 [00:00<00:00, 13.84it/s]
Capturing CUDA graphs (decode, FULL):  36%|███▋      | 4/11 [00:00<00:00, 12.43it/s]
Capturing CUDA graphs (decode, FULL):  64%|██████▎   | 7/11 [00:00<00:00, 16.04it/s]
Capturing CUDA graphs (decode, FULL):  91%|█████████ | 10/11 [00:00<00:00, 17.91it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:00<00:00, 16.87it/s]

Adding requests:   0%|          | 0/8192 [00:00<?, ?it/s]
Adding requests:   0%|          | 40/8192 [00:00<00:20, 394.81it/s]
Adding requests:   1%|          | 85/8192 [00:00<00:19, 423.25it/s]
Adding requests:   2%|▏         | 129/8192 [00:00<00:18, 429.51it/s]
Adding requests:   2%|▏         | 173/8192 [00:00<00:18, 433.62it/s]
Adding requests:   3%|▎         | 218/8192 [00:00<00:18, 439.44it/s]
Adding requests:   3%|▎         | 266/8192 [00:00<00:17, 452.52it/s]
Adding requests:   4%|▍         | 312/8192 [00:00<00:17, 450.61it/s]
Adding requests:   4%|▍         | 359/8192 [00:00<00:17, 453.47it/s]
Adding requests:   5%|▍         | 407/8192 [00:00<00:16, 459.62it/s]
Adding requests:   6%|▌         | 453/8192 [00:01<00:16, 457.93it/s]
Adding requests:   6%|▌         | 500/8192 [00:01<00:16, 458.57it/s]
Adding requests:   7%|▋         | 546/8192 [00:01<00:16, 453.62it/s]
Adding requests:   7%|▋         | 595/8192 [00:01<00:16, 463.61it/s]
Adding requests:   8%|▊         | 642/8192 [00:01<00:16, 464.73it/s]
Adding requests:   8%|▊         | 691/8192 [00:01<00:15, 471.34it/s]
Adding requests:   9%|▉         | 739/8192 [00:01<00:15, 473.50it/s]
Adding requests:  10%|▉         | 787/8192 [00:01<00:15, 469.02it/s]
Adding requests:  10%|█         | 834/8192 [00:01<00:16, 458.04it/s]
Adding requests:  11%|█         | 882/8192 [00:01<00:15, 463.64it/s]
Adding requests:  11%|█▏        | 931/8192 [00:02<00:15, 469.26it/s]
Adding requests:  12%|█▏        | 978/8192 [00:02<00:15, 469.45it/s]
Adding requests:  13%|█▎        | 1027/8192 [00:02<00:15, 473.46it/s]
Adding requests:  13%|█▎        | 1075/8192 [00:02<00:15, 468.28it/s]
Adding requests:  14%|█▎        | 1122/8192 [00:02<00:15, 467.27it/s]
Adding requests:  14%|█▍        | 1171/8192 [00:02<00:14, 473.34it/s]
Adding requests:  15%|█▍        | 1222/8192 [00:02<00:14, 479.61it/s]
Adding requests:  16%|█▌        | 1270/8192 [00:02<00:14, 475.25it/s]
Adding requests:  16%|█▌        | 1319/8192 [00:02<00:14, 476.52it/s]
Adding requests:  17%|█▋        | 1368/8192 [00:02<00:14, 478.42it/s]
Adding requests:  17%|█▋        | 1416/8192 [00:03<00:14, 478.29it/s]
Adding requests:  18%|█▊        | 1465/8192 [00:03<00:13, 480.56it/s]
Adding requests:  18%|█▊        | 1514/8192 [00:03<00:14, 468.74it/s]
Adding requests:  19%|█▉        | 1561/8192 [00:03<00:14, 469.05it/s]
Adding requests:  20%|█▉        | 1611/8192 [00:03<00:13, 473.87it/s]
Adding requests:  20%|██        | 1659/8192 [00:03<00:13, 473.22it/s]
Adding requests:  21%|██        | 1707/8192 [00:03<00:13, 472.43it/s]
Adding requests:  21%|██▏       | 1755/8192 [00:03<00:13, 472.37it/s]
Adding requests:  22%|██▏       | 1803/8192 [00:03<00:13, 472.80it/s]
Adding requests:  23%|██▎       | 1851/8192 [00:03<00:13, 472.80it/s]
Adding requests:  23%|██▎       | 1899/8192 [00:04<00:13, 473.30it/s]
Adding requests:  24%|██▍       | 1947/8192 [00:04<00:13, 473.70it/s]
Adding requests:  24%|██▍       | 1995/8192 [00:04<00:13, 473.04it/s]
Adding requests:  25%|██▍       | 2044/8192 [00:04<00:12, 476.82it/s]
Adding requests:  26%|██▌       | 2093/8192 [00:04<00:12, 480.17it/s]
Adding requests:  26%|██▌       | 2142/8192 [00:04<00:12, 473.73it/s]
Adding requests:  27%|██▋       | 2190/8192 [00:04<00:12, 470.03it/s]
Adding requests:  27%|██▋       | 2239/8192 [00:04<00:12, 475.16it/s]
Adding requests:  28%|██▊       | 2287/8192 [00:04<00:12, 473.13it/s]
Adding requests:  29%|██▊       | 2335/8192 [00:05<00:12, 465.34it/s]
Adding requests:  29%|██▉       | 2383/8192 [00:05<00:12, 469.09it/s]
Adding requests:  30%|██▉       | 2432/8192 [00:05<00:12, 472.76it/s]
Adding requests:  30%|███       | 2480/8192 [00:05<00:12, 471.91it/s]
Adding requests:  31%|███       | 2528/8192 [00:05<00:11, 472.52it/s]
Adding requests:  31%|███▏      | 2576/8192 [00:05<00:11, 468.72it/s]
Adding requests:  32%|███▏      | 2624/8192 [00:05<00:11, 469.97it/s]
Adding requests:  33%|███▎      | 2673/8192 [00:05<00:11, 472.32it/s]
Adding requests:  33%|███▎      | 2721/8192 [00:05<00:11, 471.88it/s]
Adding requests:  34%|███▍      | 2769/8192 [00:05<00:11, 472.74it/s]
Adding requests:  34%|███▍      | 2817/8192 [00:06<00:11, 468.93it/s]
Adding requests:  35%|███▍      | 2864/8192 [00:06<00:11, 468.93it/s]
Adding requests:  36%|███▌      | 2913/8192 [00:06<00:11, 472.92it/s]
Adding requests:  36%|███▌      | 2961/8192 [00:06<00:11, 469.55it/s]
Adding requests:  37%|███▋      | 3010/8192 [00:06<00:10, 474.63it/s]
Adding requests:  37%|███▋      | 3058/8192 [00:06<00:10, 472.45it/s]
Adding requests:  38%|███▊      | 3106/8192 [00:06<00:10, 470.73it/s]
Adding requests:  39%|███▊      | 3154/8192 [00:06<00:10, 471.03it/s]
Adding requests:  39%|███▉      | 3202/8192 [00:06<00:10, 472.13it/s]
Adding requests:  40%|███▉      | 3251/8192 [00:06<00:10, 476.51it/s]
Adding requests:  40%|████      | 3300/8192 [00:07<00:10, 477.78it/s]
Adding requests:  41%|████      | 3349/8192 [00:07<00:10, 479.57it/s]
Adding requests:  41%|████▏     | 3397/8192 [00:07<00:10, 477.93it/s]
Adding requests:  42%|████▏     | 3445/8192 [00:07<00:09, 476.03it/s]
Adding requests:  43%|████▎     | 3493/8192 [00:07<00:09, 472.45it/s]
Adding requests:  43%|████▎     | 3541/8192 [00:07<00:09, 474.60it/s]
Adding requests:  44%|████▍     | 3589/8192 [00:07<00:10, 455.72it/s]
Adding requests:  44%|████▍     | 3636/8192 [00:07<00:09, 458.40it/s]
Adding requests:  45%|████▍     | 3684/8192 [00:07<00:09, 461.92it/s]
Adding requests:  46%|████▌     | 3731/8192 [00:07<00:09, 463.50it/s]
Adding requests:  46%|████▌     | 3778/8192 [00:08<00:09, 458.65it/s]
Adding requests:  47%|████▋     | 3826/8192 [00:08<00:09, 462.59it/s]
Adding requests:  47%|████▋     | 3875/8192 [00:08<00:09, 470.50it/s]
Adding requests:  48%|████▊     | 3923/8192 [00:08<00:09, 472.23it/s]
Adding requests:  48%|████▊     | 3971/8192 [00:08<00:08, 472.52it/s]
Adding requests:  49%|████▉     | 4019/8192 [00:08<00:08, 474.73it/s]
Adding requests:  50%|████▉     | 4067/8192 [00:08<00:08, 467.49it/s]
Adding requests:  50%|█████     | 4115/8192 [00:08<00:08, 471.13it/s]
Adding requests:  51%|█████     | 4163/8192 [00:08<00:08, 473.39it/s]
Adding requests:  51%|█████▏    | 4212/8192 [00:08<00:08, 476.42it/s]
Adding requests:  52%|█████▏    | 4260/8192 [00:09<00:08, 476.10it/s]
Adding requests:  53%|█████▎    | 4309/8192 [00:09<00:08, 477.90it/s]
Adding requests:  53%|█████▎    | 4358/8192 [00:09<00:07, 481.04it/s]
Adding requests:  54%|█████▍    | 4408/8192 [00:09<00:07, 486.63it/s]
Adding requests:  54%|█████▍    | 4457/8192 [00:09<00:07, 483.75it/s]
Adding requests:  55%|█████▌    | 4506/8192 [00:09<00:07, 479.72it/s]
Adding requests:  56%|█████▌    | 4554/8192 [00:09<00:07, 476.69it/s]
Adding requests:  56%|█████▌    | 4604/8192 [00:09<00:07, 481.79it/s]
Adding requests:  57%|█████▋    | 4653/8192 [00:09<00:07, 480.67it/s]
Adding requests:  57%|█████▋    | 4702/8192 [00:10<00:07, 478.50it/s]
Adding requests:  58%|█████▊    | 4751/8192 [00:10<00:07, 480.64it/s]
Adding requests:  59%|█████▊    | 4800/8192 [00:10<00:07, 479.65it/s]
Adding requests:  59%|█████▉    | 4848/8192 [00:10<00:06, 478.86it/s]
Adding requests:  60%|█████▉    | 4896/8192 [00:10<00:06, 475.09it/s]
Adding requests:  60%|██████    | 4944/8192 [00:10<00:06, 469.00it/s]
Adding requests:  61%|██████    | 4992/8192 [00:10<00:06, 470.87it/s]
Adding requests:  62%|██████▏   | 5041/8192 [00:10<00:06, 475.53it/s]
Adding requests:  62%|██████▏   | 5092/8192 [00:10<00:06, 483.64it/s]
Adding requests:  63%|██████▎   | 5141/8192 [00:10<00:06, 480.87it/s]
Adding requests:  63%|██████▎   | 5190/8192 [00:11<00:06, 482.35it/s]
Adding requests:  64%|██████▍   | 5239/8192 [00:11<00:06, 478.87it/s]
Adding requests:  65%|██████▍   | 5287/8192 [00:11<00:06, 471.75it/s]
Adding requests:  65%|██████▌   | 5335/8192 [00:11<00:06, 472.26it/s]
Adding requests:  66%|██████▌   | 5383/8192 [00:11<00:05, 470.63it/s]
Adding requests:  66%|██████▋   | 5431/8192 [00:11<00:05, 471.25it/s]
Adding requests:  67%|██████▋   | 5479/8192 [00:11<00:05, 465.85it/s]
Adding requests:  67%|██████▋   | 5526/8192 [00:11<00:05, 463.37it/s]
Adding requests:  68%|██████▊   | 5573/8192 [00:11<00:05, 464.38it/s]
Adding requests:  69%|██████▊   | 5620/8192 [00:11<00:05, 465.69it/s]
Adding requests:  69%|██████▉   | 5667/8192 [00:12<00:05, 458.77it/s]
Adding requests:  70%|██████▉   | 5715/8192 [00:12<00:05, 464.54it/s]
Adding requests:  70%|███████   | 5764/8192 [00:12<00:05, 469.76it/s]
Adding requests:  71%|███████   | 5811/8192 [00:12<00:05, 466.01it/s]
Adding requests:  72%|███████▏  | 5858/8192 [00:12<00:05, 465.50it/s]
Adding requests:  72%|███████▏  | 5907/8192 [00:12<00:04, 470.86it/s]
Adding requests:  73%|███████▎  | 5955/8192 [00:12<00:04, 471.30it/s]
Adding requests:  73%|███████▎  | 6004/8192 [00:12<00:04, 475.29it/s]
Adding requests:  74%|███████▍  | 6052/8192 [00:12<00:04, 475.15it/s]
Adding requests:  74%|███████▍  | 6100/8192 [00:12<00:04, 461.62it/s]
Adding requests:  75%|███████▌  | 6149/8192 [00:13<00:04, 467.46it/s]
Adding requests:  76%|███████▌  | 6197/8192 [00:13<00:04, 470.75it/s]
Adding requests:  76%|███████▋  | 6247/8192 [00:13<00:04, 478.64it/s]
Adding requests:  77%|███████▋  | 6297/8192 [00:13<00:03, 482.78it/s]
Adding requests:  77%|███████▋  | 6346/8192 [00:13<00:03, 484.12it/s]
Adding requests:  78%|███████▊  | 6396/8192 [00:13<00:03, 485.84it/s]
Adding requests:  79%|███████▊  | 6447/8192 [00:13<00:03, 490.73it/s]
Adding requests:  79%|███████▉  | 6497/8192 [00:13<00:03, 490.88it/s]
Adding requests:  80%|███████▉  | 6547/8192 [00:13<00:03, 489.60it/s]
Adding requests:  81%|████████  | 6596/8192 [00:13<00:03, 487.73it/s]
Adding requests:  81%|████████  | 6645/8192 [00:14<00:03, 488.16it/s]
Adding requests:  82%|████████▏ | 6694/8192 [00:14<00:03, 485.52it/s]
Adding requests:  82%|████████▏ | 6743/8192 [00:14<00:02, 484.22it/s]
Adding requests:  83%|████████▎ | 6793/8192 [00:14<00:02, 488.58it/s]
Adding requests:  84%|████████▎ | 6842/8192 [00:14<00:02, 487.71it/s]
Adding requests:  84%|████████▍ | 6892/8192 [00:14<00:02, 490.97it/s]
Adding requests:  85%|████████▍ | 6943/8192 [00:14<00:02, 494.18it/s]
Adding requests:  85%|████████▌ | 6993/8192 [00:14<00:02, 490.21it/s]
Adding requests:  86%|████████▌ | 7043/8192 [00:14<00:02, 485.13it/s]
Adding requests:  87%|████████▋ | 7092/8192 [00:15<00:02, 484.34it/s]
Adding requests:  87%|████████▋ | 7141/8192 [00:15<00:02, 484.58it/s]
Adding requests:  88%|████████▊ | 7190/8192 [00:15<00:02, 482.55it/s]
Adding requests:  88%|████████▊ | 7239/8192 [00:15<00:01, 482.36it/s]
Adding requests:  89%|████████▉ | 7288/8192 [00:15<00:01, 474.60it/s]
Adding requests:  90%|████████▉ | 7337/8192 [00:15<00:01, 477.31it/s]
Adding requests:  90%|█████████ | 7385/8192 [00:15<00:01, 475.82it/s]
Adding requests:  91%|█████████ | 7437/8192 [00:15<00:01, 486.68it/s]
Adding requests:  91%|█████████▏| 7486/8192 [00:15<00:01, 486.90it/s]
Adding requests:  92%|█████████▏| 7535/8192 [00:15<00:01, 486.67it/s]
Adding requests:  93%|█████████▎| 7584/8192 [00:16<00:01, 483.80it/s]
Adding requests:  93%|█████████▎| 7633/8192 [00:16<00:01, 482.68it/s]
Adding requests:  94%|█████████▍| 7683/8192 [00:16<00:01, 487.17it/s]
Adding requests:  94%|█████████▍| 7732/8192 [00:16<00:00, 484.65it/s]
Adding requests:  95%|█████████▍| 7781/8192 [00:16<00:00, 484.30it/s]
Adding requests:  96%|█████████▌| 7830/8192 [00:16<00:00, 482.09it/s]
Adding requests:  96%|█████████▌| 7879/8192 [00:16<00:00, 479.47it/s]
Adding requests:  97%|█████████▋| 7927/8192 [00:16<00:00, 478.10it/s]
Adding requests:  97%|█████████▋| 7975/8192 [00:16<00:00, 475.48it/s]
Adding requests:  98%|█████████▊| 8023/8192 [00:16<00:00, 475.23it/s]
Adding requests:  99%|█████████▊| 8072/8192 [00:17<00:00, 478.29it/s]
Adding requests:  99%|█████████▉| 8122/8192 [00:17<00:00, 482.77it/s]
Adding requests: 100%|█████████▉| 8171/8192 [00:17<00:00, 480.59it/s]
Adding requests: 100%|██████████| 8192/8192 [00:17<00:00, 473.66it/s]

Processed prompts:   0%|          | 0/8192 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  69%|██████▉   | 5674/8192 [00:00<00:00, 56696.39it/s, est. speed input: 58062085.73 toks/s, output: 56697.47 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [00:00<00:00, 56696.39it/s, est. speed input: 59540676.71 toks/s, output: 58143.22 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [00:00<00:00, 58095.94it/s, est. speed input: 59540676.71 toks/s, output: 58143.22 toks/s]
[rank0]:[W126 13:32:17.826111147 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 86.9s

测试结果:
  Requests/s:   469.75
  Tokens/s:     481497.38
  Total Reqs:   8192
  Elapsed:      17.44s

  [Prefill 分析]
  Total Prefill Tokens: 8388608
  Prefill Tokens/s:     481027.63


------------------------------------------------------------
  生成 CSV: Llama3.2-1B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cublaslt/Llama3.2-1B-INT8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,40.7910,20925.7960,3.1379
1024,1024,1,128,128,40.3851,41394.7206,3.1695
2048,1024,2,256,128,79.5041,81491.6798,3.2200
4096,1024,4,512,128,168.5554,172769.3295,3.0376
8192,1024,8,1024,128,314.3285,322186.7189,3.2577
16384,1024,16,2048,128,459.7173,471210.2781,4.4549
32768,1024,32,4096,128,471.4855,483272.5922,8.6874
65536,1024,64,8192,128,469.7535,481497.3823,17.4389

------------------------------------------------------------

[INFO] 完成: 8 成功, 0 失败

============================================================
  Llama3.2-1B-INT8 | cuSPARSELt (2_4) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_4
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_4

============================================================
[1/8] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:32:26 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=501692) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=501692) WARNING 01-26 13:32:38 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=501692) WARNING 01-26 13:32:46 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 40.61 requests/s, 20832.83 total tokens/s, 40.61 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-26 13:32:26] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:32:26] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:32:26] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:32:26] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:32:26] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:32:26] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:32:26] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:32:26] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:32:26] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:32:26] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:32:26] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:32:26] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:32:26] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:32:26] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:32:32] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:32:33] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:32:33] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:32:33] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:32:33] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:32:33] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:32:33] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:32:33] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:32:33] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:32:33] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:32:33] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:32:33] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:32:33] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:32:33] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=501692) [2026-01-26 13:32:34] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=501692) [2026-01-26 13:32:34] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=501692) [2026-01-26 13:32:34] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=501692) [2026-01-26 13:32:34] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=501692) [2026-01-26 13:32:34] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=501692) [2026-01-26 13:32:34] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=501692) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=501692) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.31it/s]
(EngineCore_DP0 pid=501692) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.31it/s]
(EngineCore_DP0 pid=501692) 
(EngineCore_DP0 pid=501692) [2026-01-26 13:32:34] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2048] -> 1D uint8
(EngineCore_DP0 pid=501692) [2026-01-26 13:32:34] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 3932160 bytes
(EngineCore_DP0 pid=501692) [2026-01-26 13:32:34] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2048] -> 1D uint8
(EngineCore_DP0 pid=501692) [2026-01-26 13:32:34] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 2621440 bytes
(EngineCore_DP0 pid=501692) [2026-01-26 13:32:34] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2048] -> 1D uint8
(EngineCore_DP0 pid=501692) [2026-01-26 13:32:34] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 20971520 bytes
(EngineCore_DP0 pid=501692) [2026-01-26 13:32:34] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 8192] -> 1D uint8
(EngineCore_DP0 pid=501692) [2026-01-26 13:32:34] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 10485760 bytes
(EngineCore_DP0 pid=501692) 2026-01-26 13:32:46,001 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=501692) 2026-01-26 13:32:46,023 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=501692) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 20.36it/s]
(EngineCore_DP0 pid=501692) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 19.86it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  57%|█████▋    | 73/128 [00:00<00:00, 728.30it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 326.47it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   9%|▉         | 12/128 [00:00<00:01, 112.48it/s, est. speed input: 57595.55 toks/s, output: 112.49 toks/s]
Processed prompts:  19%|█▉        | 24/128 [00:00<00:01, 58.71it/s, est. speed input: 32379.74 toks/s, output: 63.24 toks/s]  
Processed prompts:  25%|██▌       | 32/128 [00:00<00:01, 52.42it/s, est. speed input: 29190.41 toks/s, output: 57.01 toks/s]
Processed prompts:  30%|██▉       | 38/128 [00:00<00:01, 49.64it/s, est. speed input: 27843.13 toks/s, output: 54.38 toks/s]
Processed prompts:  34%|███▍      | 44/128 [00:00<00:01, 47.89it/s, est. speed input: 26973.26 toks/s, output: 52.68 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:00<00:01, 46.84it/s, est. speed input: 26439.61 toks/s, output: 51.64 toks/s]
Processed prompts:  42%|████▏     | 54/128 [00:01<00:01, 46.06it/s, est. speed input: 26022.86 toks/s, output: 50.83 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:01<00:01, 45.42it/s, est. speed input: 25674.42 toks/s, output: 50.15 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:01<00:01, 45.00it/s, est. speed input: 25396.42 toks/s, output: 49.60 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:01<00:01, 44.62it/s, est. speed input: 25150.31 toks/s, output: 49.12 toks/s]
Processed prompts:  58%|█████▊    | 74/128 [00:01<00:01, 44.04it/s, est. speed input: 24896.11 toks/s, output: 48.63 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:01<00:01, 43.96it/s, est. speed input: 24723.13 toks/s, output: 48.29 toks/s]
Processed prompts:  66%|██████▌   | 84/128 [00:01<00:01, 43.90it/s, est. speed input: 24571.89 toks/s, output: 47.99 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:01<00:00, 43.53it/s, est. speed input: 24400.64 toks/s, output: 47.66 toks/s]
Processed prompts:  73%|███████▎  | 94/128 [00:01<00:00, 43.68it/s, est. speed input: 24294.19 toks/s, output: 47.45 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:02<00:00, 43.68it/s, est. speed input: 24188.90 toks/s, output: 47.24 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:02<00:00, 43.77it/s, est. speed input: 24102.86 toks/s, output: 47.08 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:02<00:00, 43.77it/s, est. speed input: 24019.54 toks/s, output: 46.91 toks/s]
Processed prompts:  89%|████████▉ | 114/128 [00:02<00:00, 43.71it/s, est. speed input: 23939.22 toks/s, output: 46.76 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:02<00:00, 43.69it/s, est. speed input: 23867.45 toks/s, output: 46.62 toks/s]
Processed prompts:  97%|█████████▋| 124/128 [00:02<00:00, 43.79it/s, est. speed input: 23810.76 toks/s, output: 46.51 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 43.79it/s, est. speed input: 23755.74 toks/s, output: 46.40 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 46.40it/s, est. speed input: 23755.74 toks/s, output: 46.40 toks/s]
[rank0]:[W126 13:32:51.236900172 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 33.4s

测试结果:
  Requests/s:   40.61
  Tokens/s:     20832.83
  Total Reqs:   128
  Elapsed:      3.15s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     20792.23

============================================================
[2/8] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:33:00 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=502763) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=502763) WARNING 01-26 13:33:12 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=502763) WARNING 01-26 13:33:19 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 35.83 requests/s, 36728.93 total tokens/s, 35.83 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-26 13:32:59] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:33:00] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:33:00] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:33:00] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:33:00] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:33:00] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:33:00] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:33:00] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:33:00] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:33:00] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:33:00] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:33:00] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:33:00] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:33:00] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:33:06] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:33:07] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:33:07] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:33:07] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:33:07] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:33:07] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:33:07] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:33:07] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:33:07] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:33:07] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:33:07] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:33:07] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:33:07] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:33:07] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=502763) [2026-01-26 13:33:07] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=502763) [2026-01-26 13:33:07] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=502763) [2026-01-26 13:33:07] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=502763) [2026-01-26 13:33:07] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=502763) [2026-01-26 13:33:07] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=502763) [2026-01-26 13:33:07] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=502763) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=502763) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.42it/s]
(EngineCore_DP0 pid=502763) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.42it/s]
(EngineCore_DP0 pid=502763) 
(EngineCore_DP0 pid=502763) [2026-01-26 13:33:08] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2048] -> 1D uint8
(EngineCore_DP0 pid=502763) [2026-01-26 13:33:08] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 3932160 bytes
(EngineCore_DP0 pid=502763) [2026-01-26 13:33:08] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2048] -> 1D uint8
(EngineCore_DP0 pid=502763) [2026-01-26 13:33:08] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 2621440 bytes
(EngineCore_DP0 pid=502763) [2026-01-26 13:33:08] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2048] -> 1D uint8
(EngineCore_DP0 pid=502763) [2026-01-26 13:33:08] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 20971520 bytes
(EngineCore_DP0 pid=502763) [2026-01-26 13:33:08] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 8192] -> 1D uint8
(EngineCore_DP0 pid=502763) [2026-01-26 13:33:08] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 10485760 bytes
(EngineCore_DP0 pid=502763) 2026-01-26 13:33:19,843 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=502763) 2026-01-26 13:33:19,864 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=502763) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 20.92it/s]
(EngineCore_DP0 pid=502763) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 20.53it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  30%|██▉       | 38/128 [00:00<00:00, 379.68it/s]
Adding requests:  63%|██████▎   | 81/128 [00:00<00:00, 405.71it/s]
Adding requests:  96%|█████████▌| 123/128 [00:00<00:00, 411.40it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 407.88it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   2%|▏         | 2/128 [00:00<00:07, 16.26it/s, est. speed input: 16651.55 toks/s, output: 16.26 toks/s]
Processed prompts:   5%|▌         | 7/128 [00:00<00:03, 30.43it/s, est. speed input: 28991.34 toks/s, output: 28.31 toks/s]
Processed prompts:   9%|▉         | 12/128 [00:00<00:03, 34.78it/s, est. speed input: 32984.12 toks/s, output: 32.21 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:03, 36.81it/s, est. speed input: 34963.41 toks/s, output: 34.14 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:00<00:02, 37.97it/s, est. speed input: 36170.97 toks/s, output: 35.32 toks/s]
Processed prompts:  21%|██        | 27/128 [00:00<00:02, 38.73it/s, est. speed input: 37003.20 toks/s, output: 36.14 toks/s]
Processed prompts:  25%|██▌       | 32/128 [00:00<00:02, 39.17it/s, est. speed input: 37582.22 toks/s, output: 36.70 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:00<00:02, 39.53it/s, est. speed input: 38039.99 toks/s, output: 37.15 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:01<00:02, 39.64it/s, est. speed input: 38301.14 toks/s, output: 37.40 toks/s]
Processed prompts:  36%|███▌      | 46/128 [00:01<00:02, 39.94it/s, est. speed input: 38628.90 toks/s, output: 37.72 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:01<00:01, 40.04it/s, est. speed input: 38869.51 toks/s, output: 37.96 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:01<00:01, 40.13it/s, est. speed input: 39075.68 toks/s, output: 38.16 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:01<00:01, 40.13it/s, est. speed input: 39232.78 toks/s, output: 38.31 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:01<00:01, 40.21it/s, est. speed input: 39387.07 toks/s, output: 38.46 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:01<00:01, 40.44it/s, est. speed input: 39558.52 toks/s, output: 38.63 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:01<00:01, 40.43it/s, est. speed input: 39671.98 toks/s, output: 38.74 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:02<00:01, 40.16it/s, est. speed input: 39722.20 toks/s, output: 38.79 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:02<00:01, 40.24it/s, est. speed input: 39815.41 toks/s, output: 38.88 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:02<00:00, 40.23it/s, est. speed input: 39888.56 toks/s, output: 38.95 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:02<00:00, 40.31it/s, est. speed input: 39968.31 toks/s, output: 39.03 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:02<00:00, 39.97it/s, est. speed input: 39976.81 toks/s, output: 39.04 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:02<00:00, 40.00it/s, est. speed input: 40025.50 toks/s, output: 39.09 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:02<00:00, 40.12it/s, est. speed input: 40083.22 toks/s, output: 39.14 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:02<00:00, 40.19it/s, est. speed input: 40135.55 toks/s, output: 39.19 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:03<00:00, 40.24it/s, est. speed input: 40183.06 toks/s, output: 39.24 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:03<00:00, 40.22it/s, est. speed input: 40219.99 toks/s, output: 39.28 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 40.22it/s, est. speed input: 40242.03 toks/s, output: 39.30 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 39.30it/s, est. speed input: 40242.03 toks/s, output: 39.30 toks/s]
[rank0]:[W126 13:33:25.514961944 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 34.2s

测试结果:
  Requests/s:   35.83
  Tokens/s:     36728.93
  Total Reqs:   128
  Elapsed:      3.57s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     36693.10

============================================================
[3/8] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:33:35 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=503825) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=503825) WARNING 01-26 13:33:47 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=503825) WARNING 01-26 13:33:54 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 80.25 requests/s, 82259.06 total tokens/s, 80.25 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-26 13:33:34] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:33:35] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:33:35] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:33:35] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:33:35] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:33:35] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:33:35] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:33:35] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:33:35] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:33:35] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:33:35] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:33:35] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:33:35] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:33:35] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:33:41] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:33:41] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:33:41] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:33:41] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:33:41] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:33:41] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:33:41] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:33:41] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:33:41] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:33:41] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:33:41] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:33:41] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:33:41] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:33:41] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=503825) [2026-01-26 13:33:42] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=503825) [2026-01-26 13:33:42] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=503825) [2026-01-26 13:33:42] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=503825) [2026-01-26 13:33:42] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=503825) [2026-01-26 13:33:42] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=503825) [2026-01-26 13:33:42] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=503825) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=503825) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.73it/s]
(EngineCore_DP0 pid=503825) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.73it/s]
(EngineCore_DP0 pid=503825) 
(EngineCore_DP0 pid=503825) [2026-01-26 13:33:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2048] -> 1D uint8
(EngineCore_DP0 pid=503825) [2026-01-26 13:33:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 3932160 bytes
(EngineCore_DP0 pid=503825) [2026-01-26 13:33:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2048] -> 1D uint8
(EngineCore_DP0 pid=503825) [2026-01-26 13:33:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 2621440 bytes
(EngineCore_DP0 pid=503825) [2026-01-26 13:33:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2048] -> 1D uint8
(EngineCore_DP0 pid=503825) [2026-01-26 13:33:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 20971520 bytes
(EngineCore_DP0 pid=503825) [2026-01-26 13:33:43] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 8192] -> 1D uint8
(EngineCore_DP0 pid=503825) [2026-01-26 13:33:43] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 10485760 bytes
(EngineCore_DP0 pid=503825) 2026-01-26 13:33:54,082 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=503825) 2026-01-26 13:33:54,104 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=503825) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 18.86it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 18.84it/s]
(EngineCore_DP0 pid=503825) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 22.78it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   0%|          | 1/256 [00:00<00:53,  4.76it/s]
Adding requests:  16%|█▋        | 42/256 [00:00<00:01, 167.70it/s]
Adding requests:  34%|███▎      | 86/256 [00:00<00:00, 266.88it/s]
Adding requests:  51%|█████     | 131/256 [00:00<00:00, 328.57it/s]
Adding requests:  68%|██████▊   | 175/256 [00:00<00:00, 364.34it/s]
Adding requests:  84%|████████▍ | 215/256 [00:00<00:00, 372.03it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 315.12it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  21%|██        | 54/256 [00:00<00:00, 480.46it/s, est. speed input: 492024.89 toks/s, output: 480.47 toks/s]
Processed prompts:  40%|████      | 103/256 [00:00<00:01, 140.49it/s, est. speed input: 161883.68 toks/s, output: 158.09 toks/s]
Processed prompts:  50%|█████     | 129/256 [00:00<00:01, 119.64it/s, est. speed input: 139895.67 toks/s, output: 136.62 toks/s]
Processed prompts:  57%|█████▋    | 147/256 [00:01<00:00, 110.91it/s, est. speed input: 131292.01 toks/s, output: 128.21 toks/s]
Processed prompts:  63%|██████▎   | 162/256 [00:01<00:00, 103.54it/s, est. speed input: 125036.43 toks/s, output: 122.11 toks/s]
Processed prompts:  68%|██████▊   | 175/256 [00:01<00:00, 102.10it/s, est. speed input: 122674.98 toks/s, output: 119.80 toks/s]
Processed prompts:  73%|███████▎  | 187/256 [00:01<00:00, 99.11it/s, est. speed input: 120081.32 toks/s, output: 117.27 toks/s] 
Processed prompts:  77%|███████▋  | 198/256 [00:01<00:00, 94.80it/s, est. speed input: 117297.56 toks/s, output: 114.55 toks/s]
Processed prompts:  81%|████████▏ | 208/256 [00:01<00:00, 93.57it/s, est. speed input: 115742.53 toks/s, output: 113.03 toks/s]
Processed prompts:  85%|████████▌ | 218/256 [00:01<00:00, 92.61it/s, est. speed input: 114378.88 toks/s, output: 111.70 toks/s]
Processed prompts:  89%|████████▉ | 228/256 [00:02<00:00, 91.86it/s, est. speed input: 113165.91 toks/s, output: 110.51 toks/s]
Processed prompts:  93%|█████████▎| 238/256 [00:02<00:00, 91.21it/s, est. speed input: 112059.05 toks/s, output: 109.43 toks/s]
Processed prompts:  97%|█████████▋| 248/256 [00:02<00:00, 90.74it/s, est. speed input: 111064.40 toks/s, output: 108.46 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 90.74it/s, est. speed input: 110326.77 toks/s, output: 107.74 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 107.73it/s, est. speed input: 110326.77 toks/s, output: 107.74 toks/s]
[rank0]:[W126 13:33:59.439597557 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 33.9s

测试结果:
  Requests/s:   80.25
  Tokens/s:     82259.06
  Total Reqs:   256
  Elapsed:      3.19s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     82178.81

============================================================
[4/8] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:34:09 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=504880) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=504880) WARNING 01-26 13:34:22 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=504880) WARNING 01-26 13:34:29 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 162.21 requests/s, 166263.00 total tokens/s, 162.21 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-26 13:34:09] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:34:09] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:34:09] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:34:09] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:34:09] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:34:09] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:34:09] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:34:09] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:34:09] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:34:09] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:34:09] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:34:09] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:34:09] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:34:09] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:34:16] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:34:16] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:34:16] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:34:16] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:34:16] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:34:16] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:34:16] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:34:16] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:34:16] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:34:16] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:34:16] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:34:16] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:34:16] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:34:16] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=504880) [2026-01-26 13:34:17] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=504880) [2026-01-26 13:34:17] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=504880) [2026-01-26 13:34:17] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=504880) [2026-01-26 13:34:17] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=504880) [2026-01-26 13:34:17] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=504880) [2026-01-26 13:34:17] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=504880) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=504880) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.08it/s]
(EngineCore_DP0 pid=504880) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.08it/s]
(EngineCore_DP0 pid=504880) 
(EngineCore_DP0 pid=504880) [2026-01-26 13:34:18] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2048] -> 1D uint8
(EngineCore_DP0 pid=504880) [2026-01-26 13:34:18] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 3932160 bytes
(EngineCore_DP0 pid=504880) [2026-01-26 13:34:18] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2048] -> 1D uint8
(EngineCore_DP0 pid=504880) [2026-01-26 13:34:18] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 2621440 bytes
(EngineCore_DP0 pid=504880) [2026-01-26 13:34:18] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2048] -> 1D uint8
(EngineCore_DP0 pid=504880) [2026-01-26 13:34:18] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 20971520 bytes
(EngineCore_DP0 pid=504880) [2026-01-26 13:34:18] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 8192] -> 1D uint8
(EngineCore_DP0 pid=504880) [2026-01-26 13:34:18] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 10485760 bytes
(EngineCore_DP0 pid=504880) 2026-01-26 13:34:29,127 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=504880) 2026-01-26 13:34:29,149 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=504880) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 3/4 [00:00<00:00, 22.96it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 20.01it/s]
(EngineCore_DP0 pid=504880) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 22.99it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 22.97it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   8%|▊         | 41/512 [00:00<00:01, 400.30it/s]
Adding requests:  17%|█▋        | 86/512 [00:00<00:01, 425.61it/s]
Adding requests:  26%|██▌       | 131/512 [00:00<00:00, 433.80it/s]
Adding requests:  34%|███▍      | 175/512 [00:00<00:00, 433.31it/s]
Adding requests:  43%|████▎     | 221/512 [00:00<00:00, 441.34it/s]
Adding requests:  53%|█████▎    | 269/512 [00:00<00:00, 451.95it/s]
Adding requests:  62%|██████▏   | 315/512 [00:00<00:00, 439.87it/s]
Adding requests:  71%|███████   | 361/512 [00:00<00:00, 445.89it/s]
Adding requests:  80%|███████▉  | 409/512 [00:00<00:00, 454.08it/s]
Adding requests:  89%|████████▉ | 456/512 [00:01<00:00, 458.27it/s]
Adding requests:  98%|█████████▊| 503/512 [00:01<00:00, 460.06it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 447.93it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  36%|███▋      | 186/512 [00:00<00:00, 1656.30it/s, est. speed input: 1696152.15 toks/s, output: 1656.32 toks/s]
Processed prompts:  69%|██████▉   | 352/512 [00:01<00:00, 286.20it/s, est. speed input: 337300.22 toks/s, output: 329.39 toks/s]   
Processed prompts:  84%|████████▍ | 431/512 [00:01<00:00, 242.09it/s, est. speed input: 289036.47 toks/s, output: 282.26 toks/s]
Processed prompts:  94%|█████████▍| 482/512 [00:01<00:00, 220.11it/s, est. speed input: 268049.82 toks/s, output: 261.77 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:02<00:00, 220.11it/s, est. speed input: 260550.59 toks/s, output: 254.44 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:02<00:00, 254.43it/s, est. speed input: 260550.59 toks/s, output: 254.44 toks/s]
[rank0]:[W126 13:34:34.960237838 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 35.4s

测试结果:
  Requests/s:   162.21
  Tokens/s:     166263.00
  Total Reqs:   512
  Elapsed:      3.16s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     166100.79

============================================================
[5/8] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:34:47 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=505960) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=505960) WARNING 01-26 13:35:00 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=505960) WARNING 01-26 13:35:06 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 291.81 requests/s, 299104.10 total tokens/s, 291.81 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-26 13:34:47] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:34:47] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:34:47] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:34:47] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:34:47] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:34:47] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:34:47] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:34:47] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:34:47] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:34:47] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:34:47] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:34:47] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:34:47] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:34:47] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:34:54] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:34:54] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:34:54] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:34:54] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:34:54] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:34:54] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:34:54] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:34:54] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:34:54] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:34:54] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:34:54] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:34:54] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:34:54] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:34:54] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=505960) [2026-01-26 13:34:55] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=505960) [2026-01-26 13:34:55] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=505960) [2026-01-26 13:34:55] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=505960) [2026-01-26 13:34:55] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=505960) [2026-01-26 13:34:55] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=505960) [2026-01-26 13:34:55] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=505960) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=505960) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.08it/s]
(EngineCore_DP0 pid=505960) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.08it/s]
(EngineCore_DP0 pid=505960) 
(EngineCore_DP0 pid=505960) [2026-01-26 13:34:55] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2048] -> 1D uint8
(EngineCore_DP0 pid=505960) [2026-01-26 13:34:55] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 3932160 bytes
(EngineCore_DP0 pid=505960) [2026-01-26 13:34:56] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2048] -> 1D uint8
(EngineCore_DP0 pid=505960) [2026-01-26 13:34:56] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 2621440 bytes
(EngineCore_DP0 pid=505960) [2026-01-26 13:34:56] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2048] -> 1D uint8
(EngineCore_DP0 pid=505960) [2026-01-26 13:34:56] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 20971520 bytes
(EngineCore_DP0 pid=505960) [2026-01-26 13:34:56] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 8192] -> 1D uint8
(EngineCore_DP0 pid=505960) [2026-01-26 13:34:56] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 10485760 bytes
(EngineCore_DP0 pid=505960) 2026-01-26 13:35:06,839 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=505960) 2026-01-26 13:35:06,862 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=505960) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 3/5 [00:00<00:00, 22.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 20.59it/s]
(EngineCore_DP0 pid=505960) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  75%|███████▌  | 3/4 [00:00<00:00, 23.15it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 23.06it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   4%|▍         | 40/1024 [00:00<00:02, 398.22it/s]
Adding requests:   8%|▊         | 85/1024 [00:00<00:02, 426.13it/s]
Adding requests:  13%|█▎        | 130/1024 [00:00<00:02, 435.74it/s]
Adding requests:  17%|█▋        | 174/1024 [00:00<00:01, 436.61it/s]
Adding requests:  21%|██▏       | 220/1024 [00:00<00:01, 443.64it/s]
Adding requests:  26%|██▌       | 268/1024 [00:00<00:01, 455.44it/s]
Adding requests:  31%|███       | 314/1024 [00:00<00:01, 453.92it/s]
Adding requests:  35%|███▌      | 361/1024 [00:00<00:01, 456.56it/s]
Adding requests:  40%|███▉      | 408/1024 [00:00<00:01, 458.08it/s]
Adding requests:  45%|████▍     | 456/1024 [00:01<00:01, 461.86it/s]
Adding requests:  49%|████▉     | 503/1024 [00:01<00:01, 462.50it/s]
Adding requests:  54%|█████▎    | 550/1024 [00:01<00:01, 458.39it/s]
Adding requests:  58%|█████▊    | 599/1024 [00:01<00:00, 465.67it/s]
Adding requests:  63%|██████▎   | 648/1024 [00:01<00:00, 471.39it/s]
Adding requests:  68%|██████▊   | 697/1024 [00:01<00:00, 476.53it/s]
Adding requests:  73%|███████▎  | 745/1024 [00:01<00:00, 475.19it/s]
Adding requests:  77%|███████▋  | 793/1024 [00:01<00:00, 474.42it/s]
Adding requests:  82%|████████▏ | 841/1024 [00:01<00:00, 463.59it/s]
Adding requests:  87%|████████▋ | 889/1024 [00:01<00:00, 468.15it/s]
Adding requests:  91%|█████████▏| 936/1024 [00:02<00:00, 467.28it/s]
Adding requests:  96%|█████████▌| 983/1024 [00:02<00:00, 467.57it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 461.21it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  61%|██████    | 626/1024 [00:00<00:00, 5652.96it/s, est. speed input: 5789001.25 toks/s, output: 5653.07 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:01<00:00, 5652.96it/s, est. speed input: 814336.94 toks/s, output: 795.25 toks/s] 
Processed prompts: 100%|██████████| 1024/1024 [00:01<00:00, 795.18it/s, est. speed input: 814336.94 toks/s, output: 795.25 toks/s] 
[rank0]:[W126 13:35:12.149124092 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 38.3s

测试结果:
  Requests/s:   291.81
  Tokens/s:     299104.10
  Total Reqs:   1024
  Elapsed:      3.51s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     298812.29

============================================================
[6/8] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:35:30 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=507108) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=507108) WARNING 01-26 13:35:43 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=507108) WARNING 01-26 13:35:50 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 460.39 requests/s, 471899.46 total tokens/s, 460.39 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-26 13:35:30] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:35:30] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:35:30] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:35:30] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:35:30] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:35:30] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:35:30] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:35:30] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:35:30] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:35:30] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:35:30] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:35:30] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:35:30] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:35:30] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:35:37] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:35:37] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:35:37] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:35:37] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:35:37] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:35:37] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:35:37] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:35:37] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:35:37] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:35:37] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:35:37] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:35:37] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:35:37] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:35:37] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=507108) [2026-01-26 13:35:38] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=507108) [2026-01-26 13:35:38] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=507108) [2026-01-26 13:35:38] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=507108) [2026-01-26 13:35:38] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=507108) [2026-01-26 13:35:38] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=507108) [2026-01-26 13:35:38] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=507108) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=507108) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.11it/s]
(EngineCore_DP0 pid=507108) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.10it/s]
(EngineCore_DP0 pid=507108) 
(EngineCore_DP0 pid=507108) [2026-01-26 13:35:39] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2048] -> 1D uint8
(EngineCore_DP0 pid=507108) [2026-01-26 13:35:39] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 3932160 bytes
(EngineCore_DP0 pid=507108) [2026-01-26 13:35:39] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2048] -> 1D uint8
(EngineCore_DP0 pid=507108) [2026-01-26 13:35:39] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 2621440 bytes
(EngineCore_DP0 pid=507108) [2026-01-26 13:35:39] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2048] -> 1D uint8
(EngineCore_DP0 pid=507108) [2026-01-26 13:35:39] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 20971520 bytes
(EngineCore_DP0 pid=507108) [2026-01-26 13:35:39] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 8192] -> 1D uint8
(EngineCore_DP0 pid=507108) [2026-01-26 13:35:39] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 10485760 bytes
(EngineCore_DP0 pid=507108) 2026-01-26 13:35:50,207 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=507108) 2026-01-26 13:35:50,231 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=507108) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00, 19.80it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00, 18.74it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:00<00:00, 18.68it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 18.01it/s]
(EngineCore_DP0 pid=507108) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00, 18.31it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00, 18.43it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 18.96it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 39/2048 [00:00<00:05, 380.80it/s]
Adding requests:   4%|▍         | 83/2048 [00:00<00:04, 415.26it/s]
Adding requests:   6%|▋         | 128/2048 [00:00<00:04, 426.85it/s]
Adding requests:   8%|▊         | 172/2048 [00:00<00:04, 431.08it/s]
Adding requests:  11%|█         | 218/2048 [00:00<00:04, 439.07it/s]
Adding requests:  13%|█▎        | 266/2048 [00:00<00:03, 451.42it/s]
Adding requests:  15%|█▌        | 312/2048 [00:00<00:03, 451.63it/s]
Adding requests:  18%|█▊        | 359/2048 [00:00<00:03, 453.95it/s]
Adding requests:  20%|█▉        | 405/2048 [00:00<00:03, 450.20it/s]
Adding requests:  22%|██▏       | 452/2048 [00:01<00:03, 454.65it/s]
Adding requests:  24%|██▍       | 499/2048 [00:01<00:03, 456.93it/s]
Adding requests:  27%|██▋       | 545/2048 [00:01<00:03, 451.91it/s]
Adding requests:  29%|██▉       | 593/2048 [00:01<00:03, 457.74it/s]
Adding requests:  31%|███▏      | 641/2048 [00:01<00:03, 462.61it/s]
Adding requests:  34%|███▎      | 690/2048 [00:01<00:02, 468.65it/s]
Adding requests:  36%|███▌      | 739/2048 [00:01<00:02, 473.82it/s]
Adding requests:  38%|███▊      | 787/2048 [00:01<00:02, 470.24it/s]
Adding requests:  41%|████      | 835/2048 [00:01<00:02, 458.73it/s]
Adding requests:  43%|████▎     | 884/2048 [00:01<00:02, 465.81it/s]
Adding requests:  46%|████▌     | 933/2048 [00:02<00:02, 471.32it/s]
Adding requests:  48%|████▊     | 981/2048 [00:02<00:02, 471.95it/s]
Adding requests:  50%|█████     | 1030/2048 [00:02<00:02, 476.09it/s]
Adding requests:  53%|█████▎    | 1078/2048 [00:02<00:02, 472.21it/s]
Adding requests:  55%|█████▍    | 1126/2048 [00:02<00:01, 470.64it/s]
Adding requests:  57%|█████▋    | 1176/2048 [00:02<00:01, 476.44it/s]
Adding requests:  60%|█████▉    | 1226/2048 [00:02<00:01, 482.98it/s]
Adding requests:  62%|██████▏   | 1275/2048 [00:02<00:01, 476.34it/s]
Adding requests:  65%|██████▍   | 1325/2048 [00:02<00:01, 480.76it/s]
Adding requests:  67%|██████▋   | 1374/2048 [00:02<00:01, 481.01it/s]
Adding requests:  69%|██████▉   | 1423/2048 [00:03<00:01, 482.08it/s]
Adding requests:  72%|███████▏  | 1472/2048 [00:03<00:01, 464.23it/s]
Adding requests:  74%|███████▍  | 1521/2048 [00:03<00:01, 469.69it/s]
Adding requests:  77%|███████▋  | 1570/2048 [00:03<00:01, 474.09it/s]
Adding requests:  79%|███████▉  | 1618/2048 [00:03<00:00, 467.84it/s]
Adding requests:  81%|████████▏ | 1666/2048 [00:03<00:00, 469.91it/s]
Adding requests:  84%|████████▎ | 1714/2048 [00:03<00:00, 472.72it/s]
Adding requests:  86%|████████▌ | 1762/2048 [00:03<00:00, 473.12it/s]
Adding requests:  88%|████████▊ | 1810/2048 [00:03<00:00, 472.05it/s]
Adding requests:  91%|█████████ | 1858/2048 [00:03<00:00, 474.24it/s]
Adding requests:  93%|█████████▎| 1906/2048 [00:04<00:00, 472.65it/s]
Adding requests:  95%|█████████▌| 1954/2048 [00:04<00:00, 472.56it/s]
Adding requests:  98%|█████████▊| 2003/2048 [00:04<00:00, 477.17it/s]
Adding requests: 100%|██████████| 2048/2048 [00:04<00:00, 466.06it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:00<00:00, 39058.12it/s, est. speed input: 40006426.71 toks/s, output: 39065.76 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:00<00:00, 38983.85it/s, est. speed input: 40006426.71 toks/s, output: 39065.76 toks/s]
[rank0]:[W126 13:35:57.393857278 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 44.3s

测试结果:
  Requests/s:   460.39
  Tokens/s:     471899.46
  Total Reqs:   2048
  Elapsed:      4.45s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     471439.07

============================================================
[7/8] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:36:24 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=508402) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=508402) WARNING 01-26 13:36:36 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=508402) WARNING 01-26 13:36:43 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 468.82 requests/s, 480542.79 total tokens/s, 468.82 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-26 13:36:23] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:36:24] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:36:24] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:36:24] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:36:24] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:36:24] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:36:24] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:36:24] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:36:24] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:36:24] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:36:24] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:36:24] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:36:24] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:36:24] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:36:30] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:36:31] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:36:31] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:36:31] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:36:31] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:36:31] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:36:31] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:36:31] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:36:31] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:36:31] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:36:31] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:36:31] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:36:31] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:36:31] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=508402) [2026-01-26 13:36:32] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=508402) [2026-01-26 13:36:32] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=508402) [2026-01-26 13:36:32] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=508402) [2026-01-26 13:36:32] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=508402) [2026-01-26 13:36:32] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=508402) [2026-01-26 13:36:32] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=508402) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=508402) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.07it/s]
(EngineCore_DP0 pid=508402) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.07it/s]
(EngineCore_DP0 pid=508402) 
(EngineCore_DP0 pid=508402) [2026-01-26 13:36:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2048] -> 1D uint8
(EngineCore_DP0 pid=508402) [2026-01-26 13:36:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 3932160 bytes
(EngineCore_DP0 pid=508402) [2026-01-26 13:36:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2048] -> 1D uint8
(EngineCore_DP0 pid=508402) [2026-01-26 13:36:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 2621440 bytes
(EngineCore_DP0 pid=508402) [2026-01-26 13:36:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2048] -> 1D uint8
(EngineCore_DP0 pid=508402) [2026-01-26 13:36:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 20971520 bytes
(EngineCore_DP0 pid=508402) [2026-01-26 13:36:32] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 8192] -> 1D uint8
(EngineCore_DP0 pid=508402) [2026-01-26 13:36:32] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 10485760 bytes
(EngineCore_DP0 pid=508402) [rank0]:W0126 13:36:40.296000 508402 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=508402) [rank0]:W0126 13:36:40.368000 508402 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=508402) [rank0]:W0126 13:36:41.173000 508402 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=508402) [rank0]:W0126 13:36:41.270000 508402 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=508402) 2026-01-26 13:36:43,854 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=508402) 2026-01-26 13:36:43,878 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=508402) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:00, 19.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 5/11 [00:00<00:00, 20.06it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:00<00:00, 22.02it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 20.79it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 20.79it/s]
(EngineCore_DP0 pid=508402) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 3/7 [00:00<00:00, 20.45it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00, 21.77it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 21.83it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 38/4096 [00:00<00:10, 372.15it/s]
Adding requests:   2%|▏         | 82/4096 [00:00<00:09, 407.50it/s]
Adding requests:   3%|▎         | 127/4096 [00:00<00:09, 424.78it/s]
Adding requests:   4%|▍         | 171/4096 [00:00<00:09, 430.33it/s]
Adding requests:   5%|▌         | 216/4096 [00:00<00:08, 436.71it/s]
Adding requests:   6%|▋         | 264/4096 [00:00<00:08, 450.17it/s]
Adding requests:   8%|▊         | 310/4096 [00:00<00:08, 450.59it/s]
Adding requests:   9%|▊         | 357/4096 [00:00<00:08, 454.80it/s]
Adding requests:  10%|▉         | 405/4096 [00:00<00:08, 459.73it/s]
Adding requests:  11%|█         | 452/4096 [00:01<00:07, 461.22it/s]
Adding requests:  12%|█▏        | 499/4096 [00:01<00:07, 463.22it/s]
Adding requests:  13%|█▎        | 546/4096 [00:01<00:07, 455.05it/s]
Adding requests:  15%|█▍        | 596/4096 [00:01<00:07, 463.61it/s]
Adding requests:  16%|█▌        | 644/4096 [00:01<00:07, 467.79it/s]
Adding requests:  17%|█▋        | 694/4096 [00:01<00:07, 476.20it/s]
Adding requests:  18%|█▊        | 742/4096 [00:01<00:07, 473.70it/s]
Adding requests:  19%|█▉        | 790/4096 [00:01<00:07, 466.32it/s]
Adding requests:  20%|██        | 837/4096 [00:01<00:07, 459.18it/s]
Adding requests:  22%|██▏       | 886/4096 [00:01<00:06, 467.16it/s]
Adding requests:  23%|██▎       | 935/4096 [00:02<00:06, 472.24it/s]
Adding requests:  24%|██▍       | 983/4096 [00:02<00:06, 474.41it/s]
Adding requests:  25%|██▌       | 1032/4096 [00:02<00:06, 478.87it/s]
Adding requests:  26%|██▋       | 1080/4096 [00:02<00:06, 475.15it/s]
Adding requests:  28%|██▊       | 1128/4096 [00:02<00:06, 454.05it/s]
Adding requests:  29%|██▊       | 1177/4096 [00:02<00:06, 464.36it/s]
Adding requests:  30%|██▉       | 1227/4096 [00:02<00:06, 472.77it/s]
Adding requests:  31%|███       | 1275/4096 [00:02<00:05, 470.62it/s]
Adding requests:  32%|███▏      | 1324/4096 [00:02<00:05, 476.01it/s]
Adding requests:  34%|███▎      | 1373/4096 [00:02<00:05, 478.44it/s]
Adding requests:  35%|███▍      | 1422/4096 [00:03<00:05, 479.50it/s]
Adding requests:  36%|███▌      | 1471/4096 [00:03<00:05, 482.28it/s]
Adding requests:  37%|███▋      | 1521/4096 [00:03<00:05, 484.76it/s]
Adding requests:  38%|███▊      | 1570/4096 [00:03<00:05, 485.10it/s]
Adding requests:  40%|███▉      | 1620/4096 [00:03<00:05, 489.04it/s]
Adding requests:  41%|████      | 1669/4096 [00:03<00:05, 485.06it/s]
Adding requests:  42%|████▏     | 1718/4096 [00:03<00:04, 481.61it/s]
Adding requests:  43%|████▎     | 1767/4096 [00:03<00:04, 478.94it/s]
Adding requests:  44%|████▍     | 1816/4096 [00:03<00:04, 473.13it/s]
Adding requests:  46%|████▌     | 1864/4096 [00:03<00:04, 472.16it/s]
Adding requests:  47%|████▋     | 1913/4096 [00:04<00:04, 475.60it/s]
Adding requests:  48%|████▊     | 1962/4096 [00:04<00:04, 478.52it/s]
Adding requests:  49%|████▉     | 2012/4096 [00:04<00:04, 482.56it/s]
Adding requests:  50%|█████     | 2061/4096 [00:04<00:04, 484.68it/s]
Adding requests:  52%|█████▏    | 2111/4096 [00:04<00:04, 484.72it/s]
Adding requests:  53%|█████▎    | 2160/4096 [00:04<00:04, 476.50it/s]
Adding requests:  54%|█████▍    | 2208/4096 [00:04<00:03, 473.90it/s]
Adding requests:  55%|█████▌    | 2258/4096 [00:04<00:03, 480.90it/s]
Adding requests:  56%|█████▋    | 2307/4096 [00:04<00:03, 479.62it/s]
Adding requests:  57%|█████▋    | 2355/4096 [00:05<00:03, 478.41it/s]
Adding requests:  59%|█████▊    | 2403/4096 [00:05<00:03, 477.94it/s]
Adding requests:  60%|█████▉    | 2452/4096 [00:05<00:03, 479.90it/s]
Adding requests:  61%|██████    | 2501/4096 [00:05<00:03, 481.69it/s]
Adding requests:  62%|██████▏   | 2550/4096 [00:05<00:03, 483.45it/s]
Adding requests:  63%|██████▎   | 2599/4096 [00:05<00:03, 483.03it/s]
Adding requests:  65%|██████▍   | 2649/4096 [00:05<00:02, 485.27it/s]
Adding requests:  66%|██████▌   | 2698/4096 [00:05<00:02, 480.87it/s]
Adding requests:  67%|██████▋   | 2747/4096 [00:05<00:02, 480.68it/s]
Adding requests:  68%|██████▊   | 2796/4096 [00:05<00:02, 477.57it/s]
Adding requests:  69%|██████▉   | 2844/4096 [00:06<00:02, 473.95it/s]
Adding requests:  71%|███████   | 2892/4096 [00:06<00:02, 474.84it/s]
Adding requests:  72%|███████▏  | 2940/4096 [00:06<00:02, 472.63it/s]
Adding requests:  73%|███████▎  | 2988/4096 [00:06<00:02, 466.02it/s]
Adding requests:  74%|███████▍  | 3036/4096 [00:06<00:02, 468.16it/s]
Adding requests:  75%|███████▌  | 3084/4096 [00:06<00:02, 468.70it/s]
Adding requests:  76%|███████▋  | 3133/4096 [00:06<00:02, 474.07it/s]
Adding requests:  78%|███████▊  | 3181/4096 [00:06<00:01, 475.56it/s]
Adding requests:  79%|███████▉  | 3229/4096 [00:06<00:01, 474.42it/s]
Adding requests:  80%|████████  | 3278/4096 [00:06<00:01, 477.68it/s]
Adding requests:  81%|████████  | 3327/4096 [00:07<00:01, 478.52it/s]
Adding requests:  82%|████████▏ | 3377/4096 [00:07<00:01, 482.58it/s]
Adding requests:  84%|████████▎ | 3427/4096 [00:07<00:01, 485.56it/s]
Adding requests:  85%|████████▍ | 3476/4096 [00:07<00:01, 473.51it/s]
Adding requests:  86%|████████▌ | 3524/4096 [00:07<00:01, 464.03it/s]
Adding requests:  87%|████████▋ | 3571/4096 [00:07<00:01, 464.58it/s]
Adding requests:  88%|████████▊ | 3619/4096 [00:07<00:01, 466.57it/s]
Adding requests:  90%|████████▉ | 3667/4096 [00:07<00:00, 470.23it/s]
Adding requests:  91%|█████████ | 3716/4096 [00:07<00:00, 474.10it/s]
Adding requests:  92%|█████████▏| 3765/4096 [00:07<00:00, 477.51it/s]
Adding requests:  93%|█████████▎| 3815/4096 [00:08<00:00, 483.44it/s]
Adding requests:  94%|█████████▍| 3865/4096 [00:08<00:00, 487.95it/s]
Adding requests:  96%|█████████▌| 3914/4096 [00:08<00:00, 486.80it/s]
Adding requests:  97%|█████████▋| 3963/4096 [00:08<00:00, 486.80it/s]
Adding requests:  98%|█████████▊| 4012/4096 [00:08<00:00, 485.71it/s]
Adding requests:  99%|█████████▉| 4061/4096 [00:08<00:00, 477.29it/s]
Adding requests: 100%|██████████| 4096/4096 [00:08<00:00, 473.02it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:00<00:00, 54181.16it/s, est. speed input: 55492528.98 toks/s, output: 54188.50 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:00<00:00, 54107.95it/s, est. speed input: 55492528.98 toks/s, output: 54188.50 toks/s]
[rank0]:[W126 13:36:55.534319533 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 58.2s

测试结果:
  Requests/s:   468.82
  Tokens/s:     480542.79
  Total Reqs:   4096
  Elapsed:      8.74s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     480073.97

============================================================
[8/8] 测试 M=65536
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 65536
│   M_prefill     = 65536 (= 64 x 1024)
│   M_decode      = 64
│   batched_tokens = 65536 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 8192
│   --max-num-seqs           = 64
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 65536
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:37:40 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=510000) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=510000) WARNING 01-26 13:37:53 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=510000) WARNING 01-26 13:38:00 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 469.19 requests/s, 480920.83 total tokens/s, 469.19 output tokens/s
Total num prompt tokens:  8388608
Total num output tokens:  8192


─── STDERR ───
[2026-01-26 13:37:40] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:37:40] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:37:40] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:37:40] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:37:40] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:37:40] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:37:40] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:37:40] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:37:40] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:37:40] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:37:40] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:37:40] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:37:40] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:37:40] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:37:47] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:37:47] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:37:47] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:37:47] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:37:47] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:37:47] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:37:47] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:37:47] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:37:47] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:37:47] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:37:47] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:37:47] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:37:47] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:37:47] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=510000) [2026-01-26 13:37:48] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=510000) [2026-01-26 13:37:48] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=510000) [2026-01-26 13:37:48] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=510000) [2026-01-26 13:37:48] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=510000) [2026-01-26 13:37:48] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=510000) [2026-01-26 13:37:48] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=510000) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=510000) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.11it/s]
(EngineCore_DP0 pid=510000) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.11it/s]
(EngineCore_DP0 pid=510000) 
(EngineCore_DP0 pid=510000) [2026-01-26 13:37:49] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2048] -> 1D uint8
(EngineCore_DP0 pid=510000) [2026-01-26 13:37:49] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 3932160 bytes
(EngineCore_DP0 pid=510000) [2026-01-26 13:37:49] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2048] -> 1D uint8
(EngineCore_DP0 pid=510000) [2026-01-26 13:37:49] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 2621440 bytes
(EngineCore_DP0 pid=510000) [2026-01-26 13:37:49] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2048] -> 1D uint8
(EngineCore_DP0 pid=510000) [2026-01-26 13:37:49] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 20971520 bytes
(EngineCore_DP0 pid=510000) [2026-01-26 13:37:49] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 8192] -> 1D uint8
(EngineCore_DP0 pid=510000) [2026-01-26 13:37:49] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 10485760 bytes
(EngineCore_DP0 pid=510000) [rank0]:W0126 13:37:56.457000 510000 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=510000) [rank0]:W0126 13:37:56.527000 510000 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=510000) [rank0]:W0126 13:37:57.331000 510000 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=510000) [rank0]:W0126 13:37:57.429000 510000 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=510000) 2026-01-26 13:38:00,164 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=510000) 2026-01-26 13:38:00,188 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=510000) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 2/19 [00:00<00:00, 17.59it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▋       | 5/19 [00:00<00:00, 19.68it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 8/19 [00:00<00:00, 20.84it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 11/19 [00:00<00:00, 20.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▎  | 14/19 [00:00<00:00, 19.64it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 17/19 [00:00<00:00, 19.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:00<00:00, 19.15it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:00<00:00, 19.55it/s]
(EngineCore_DP0 pid=510000) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  27%|██▋       | 3/11 [00:00<00:00, 23.09it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 6/11 [00:00<00:00, 23.23it/s]
Capturing CUDA graphs (decode, FULL):  82%|████████▏ | 9/11 [00:00<00:00, 23.21it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:00<00:00, 23.25it/s]

Adding requests:   0%|          | 0/8192 [00:00<?, ?it/s]
Adding requests:   0%|          | 40/8192 [00:00<00:20, 391.73it/s]
Adding requests:   1%|          | 85/8192 [00:00<00:19, 420.63it/s]
Adding requests:   2%|▏         | 129/8192 [00:00<00:18, 426.71it/s]
Adding requests:   2%|▏         | 173/8192 [00:00<00:18, 430.56it/s]
Adding requests:   3%|▎         | 218/8192 [00:00<00:18, 436.62it/s]
Adding requests:   3%|▎         | 266/8192 [00:00<00:17, 449.60it/s]
Adding requests:   4%|▍         | 311/8192 [00:00<00:17, 447.41it/s]
Adding requests:   4%|▍         | 358/8192 [00:00<00:17, 451.40it/s]
Adding requests:   5%|▍         | 405/8192 [00:00<00:17, 456.11it/s]
Adding requests:   6%|▌         | 451/8192 [00:01<00:17, 454.54it/s]
Adding requests:   6%|▌         | 498/8192 [00:01<00:16, 457.30it/s]
Adding requests:   7%|▋         | 544/8192 [00:01<00:17, 449.78it/s]
Adding requests:   7%|▋         | 593/8192 [00:01<00:16, 459.88it/s]
Adding requests:   8%|▊         | 640/8192 [00:01<00:16, 462.35it/s]
Adding requests:   8%|▊         | 689/8192 [00:01<00:16, 468.16it/s]
Adding requests:   9%|▉         | 738/8192 [00:01<00:15, 472.80it/s]
Adding requests:  10%|▉         | 786/8192 [00:01<00:15, 467.44it/s]
Adding requests:  10%|█         | 833/8192 [00:01<00:16, 458.72it/s]
Adding requests:  11%|█         | 881/8192 [00:01<00:15, 462.52it/s]
Adding requests:  11%|█▏        | 930/8192 [00:02<00:15, 468.27it/s]
Adding requests:  12%|█▏        | 978/8192 [00:02<00:15, 468.69it/s]
Adding requests:  13%|█▎        | 1027/8192 [00:02<00:15, 472.77it/s]
Adding requests:  13%|█▎        | 1075/8192 [00:02<00:15, 467.35it/s]
Adding requests:  14%|█▎        | 1122/8192 [00:02<00:15, 466.30it/s]
Adding requests:  14%|█▍        | 1171/8192 [00:02<00:14, 471.54it/s]
Adding requests:  15%|█▍        | 1221/8192 [00:02<00:14, 479.05it/s]
Adding requests:  15%|█▌        | 1269/8192 [00:02<00:14, 472.28it/s]
Adding requests:  16%|█▌        | 1317/8192 [00:02<00:14, 474.26it/s]
Adding requests:  17%|█▋        | 1366/8192 [00:02<00:14, 477.96it/s]
Adding requests:  17%|█▋        | 1415/8192 [00:03<00:14, 480.30it/s]
Adding requests:  18%|█▊        | 1464/8192 [00:03<00:14, 479.74it/s]
Adding requests:  18%|█▊        | 1512/8192 [00:03<00:14, 469.85it/s]
Adding requests:  19%|█▉        | 1560/8192 [00:03<00:14, 470.50it/s]
Adding requests:  20%|█▉        | 1610/8192 [00:03<00:13, 477.58it/s]
Adding requests:  20%|██        | 1658/8192 [00:03<00:13, 474.68it/s]
Adding requests:  21%|██        | 1706/8192 [00:03<00:13, 473.49it/s]
Adding requests:  21%|██▏       | 1754/8192 [00:03<00:13, 473.47it/s]
Adding requests:  22%|██▏       | 1802/8192 [00:03<00:13, 474.07it/s]
Adding requests:  23%|██▎       | 1850/8192 [00:03<00:13, 474.34it/s]
Adding requests:  23%|██▎       | 1898/8192 [00:04<00:13, 474.14it/s]
Adding requests:  24%|██▍       | 1946/8192 [00:04<00:13, 474.58it/s]
Adding requests:  24%|██▍       | 1994/8192 [00:04<00:13, 473.48it/s]
Adding requests:  25%|██▍       | 2043/8192 [00:04<00:12, 477.31it/s]
Adding requests:  26%|██▌       | 2092/8192 [00:04<00:12, 480.13it/s]
Adding requests:  26%|██▌       | 2141/8192 [00:04<00:12, 473.38it/s]
Adding requests:  27%|██▋       | 2189/8192 [00:04<00:12, 468.72it/s]
Adding requests:  27%|██▋       | 2238/8192 [00:04<00:12, 474.23it/s]
Adding requests:  28%|██▊       | 2286/8192 [00:04<00:12, 470.75it/s]
Adding requests:  29%|██▊       | 2335/8192 [00:05<00:12, 473.96it/s]
Adding requests:  29%|██▉       | 2383/8192 [00:05<00:12, 475.49it/s]
Adding requests:  30%|██▉       | 2432/8192 [00:05<00:12, 477.52it/s]
Adding requests:  30%|███       | 2480/8192 [00:05<00:12, 475.14it/s]
Adding requests:  31%|███       | 2528/8192 [00:05<00:11, 474.60it/s]
Adding requests:  31%|███▏      | 2576/8192 [00:05<00:11, 470.57it/s]
Adding requests:  32%|███▏      | 2624/8192 [00:05<00:11, 471.19it/s]
Adding requests:  33%|███▎      | 2673/8192 [00:05<00:11, 473.13it/s]
Adding requests:  33%|███▎      | 2721/8192 [00:05<00:11, 463.02it/s]
Adding requests:  34%|███▍      | 2768/8192 [00:05<00:11, 464.73it/s]
Adding requests:  34%|███▍      | 2815/8192 [00:06<00:11, 463.69it/s]
Adding requests:  35%|███▍      | 2862/8192 [00:06<00:11, 465.15it/s]
Adding requests:  36%|███▌      | 2911/8192 [00:06<00:11, 471.82it/s]
Adding requests:  36%|███▌      | 2959/8192 [00:06<00:11, 466.25it/s]
Adding requests:  37%|███▋      | 3007/8192 [00:06<00:11, 469.49it/s]
Adding requests:  37%|███▋      | 3055/8192 [00:06<00:10, 471.12it/s]
Adding requests:  38%|███▊      | 3103/8192 [00:06<00:10, 468.01it/s]
Adding requests:  38%|███▊      | 3150/8192 [00:06<00:10, 467.98it/s]
Adding requests:  39%|███▉      | 3198/8192 [00:06<00:10, 469.79it/s]
Adding requests:  40%|███▉      | 3247/8192 [00:06<00:10, 474.06it/s]
Adding requests:  40%|████      | 3295/8192 [00:07<00:10, 474.68it/s]
Adding requests:  41%|████      | 3344/8192 [00:07<00:10, 476.64it/s]
Adding requests:  41%|████▏     | 3392/8192 [00:07<00:10, 475.94it/s]
Adding requests:  42%|████▏     | 3442/8192 [00:07<00:09, 480.58it/s]
Adding requests:  43%|████▎     | 3491/8192 [00:07<00:09, 470.59it/s]
Adding requests:  43%|████▎     | 3539/8192 [00:07<00:09, 472.80it/s]
Adding requests:  44%|████▍     | 3587/8192 [00:07<00:09, 470.92it/s]
Adding requests:  44%|████▍     | 3635/8192 [00:07<00:09, 462.06it/s]
Adding requests:  45%|████▍     | 3682/8192 [00:07<00:09, 458.05it/s]
Adding requests:  46%|████▌     | 3728/8192 [00:07<00:09, 455.45it/s]
Adding requests:  46%|████▌     | 3774/8192 [00:08<00:09, 446.07it/s]
Adding requests:  47%|████▋     | 3821/8192 [00:08<00:09, 450.02it/s]
Adding requests:  47%|████▋     | 3869/8192 [00:08<00:09, 456.49it/s]
Adding requests:  48%|████▊     | 3916/8192 [00:08<00:09, 457.61it/s]
Adding requests:  48%|████▊     | 3963/8192 [00:08<00:09, 460.79it/s]
Adding requests:  49%|████▉     | 4010/8192 [00:08<00:09, 462.04it/s]
Adding requests:  50%|████▉     | 4057/8192 [00:08<00:09, 458.35it/s]
Adding requests:  50%|█████     | 4105/8192 [00:08<00:08, 463.34it/s]
Adding requests:  51%|█████     | 4152/8192 [00:08<00:08, 465.15it/s]
Adding requests:  51%|█████▏    | 4202/8192 [00:09<00:08, 472.67it/s]
Adding requests:  52%|█████▏    | 4250/8192 [00:09<00:08, 472.87it/s]
Adding requests:  52%|█████▏    | 4298/8192 [00:09<00:08, 471.68it/s]
Adding requests:  53%|█████▎    | 4348/8192 [00:09<00:08, 477.16it/s]
Adding requests:  54%|█████▎    | 4397/8192 [00:09<00:07, 480.42it/s]
Adding requests:  54%|█████▍    | 4446/8192 [00:09<00:07, 479.82it/s]
Adding requests:  55%|█████▍    | 4494/8192 [00:09<00:07, 475.08it/s]
Adding requests:  55%|█████▌    | 4542/8192 [00:09<00:07, 473.41it/s]
Adding requests:  56%|█████▌    | 4591/8192 [00:09<00:07, 475.22it/s]
Adding requests:  57%|█████▋    | 4640/8192 [00:09<00:07, 478.62it/s]
Adding requests:  57%|█████▋    | 4688/8192 [00:10<00:07, 473.28it/s]
Adding requests:  58%|█████▊    | 4738/8192 [00:10<00:07, 478.99it/s]
Adding requests:  58%|█████▊    | 4786/8192 [00:10<00:07, 477.98it/s]
Adding requests:  59%|█████▉    | 4834/8192 [00:10<00:07, 475.79it/s]
Adding requests:  60%|█████▉    | 4882/8192 [00:10<00:06, 473.70it/s]
Adding requests:  60%|██████    | 4930/8192 [00:10<00:07, 463.92it/s]
Adding requests:  61%|██████    | 4978/8192 [00:10<00:06, 468.03it/s]
Adding requests:  61%|██████▏   | 5026/8192 [00:10<00:06, 471.37it/s]
Adding requests:  62%|██████▏   | 5075/8192 [00:10<00:06, 475.73it/s]
Adding requests:  63%|██████▎   | 5124/8192 [00:10<00:06, 479.67it/s]
Adding requests:  63%|██████▎   | 5172/8192 [00:11<00:06, 479.53it/s]
Adding requests:  64%|██████▎   | 5220/8192 [00:11<00:06, 477.53it/s]
Adding requests:  64%|██████▍   | 5268/8192 [00:11<00:06, 475.89it/s]
Adding requests:  65%|██████▍   | 5317/8192 [00:11<00:06, 479.03it/s]
Adding requests:  66%|██████▌   | 5366/8192 [00:11<00:05, 481.54it/s]
Adding requests:  66%|██████▌   | 5415/8192 [00:11<00:05, 480.48it/s]
Adding requests:  67%|██████▋   | 5464/8192 [00:11<00:05, 475.90it/s]
Adding requests:  67%|██████▋   | 5512/8192 [00:11<00:05, 471.63it/s]
Adding requests:  68%|██████▊   | 5560/8192 [00:11<00:05, 472.77it/s]
Adding requests:  68%|██████▊   | 5608/8192 [00:11<00:05, 471.56it/s]
Adding requests:  69%|██████▉   | 5656/8192 [00:12<00:05, 471.55it/s]
Adding requests:  70%|██████▉   | 5704/8192 [00:12<00:05, 473.24it/s]
Adding requests:  70%|███████   | 5753/8192 [00:12<00:05, 475.87it/s]
Adding requests:  71%|███████   | 5801/8192 [00:12<00:05, 474.50it/s]
Adding requests:  71%|███████▏  | 5849/8192 [00:12<00:04, 472.24it/s]
Adding requests:  72%|███████▏  | 5898/8192 [00:12<00:04, 477.28it/s]
Adding requests:  73%|███████▎  | 5947/8192 [00:12<00:04, 479.52it/s]
Adding requests:  73%|███████▎  | 5996/8192 [00:12<00:04, 481.35it/s]
Adding requests:  74%|███████▍  | 6046/8192 [00:12<00:04, 485.32it/s]
Adding requests:  74%|███████▍  | 6095/8192 [00:12<00:04, 470.43it/s]
Adding requests:  75%|███████▌  | 6144/8192 [00:13<00:04, 473.99it/s]
Adding requests:  76%|███████▌  | 6193/8192 [00:13<00:04, 477.06it/s]
Adding requests:  76%|███████▌  | 6243/8192 [00:13<00:04, 483.14it/s]
Adding requests:  77%|███████▋  | 6294/8192 [00:13<00:03, 488.40it/s]
Adding requests:  77%|███████▋  | 6344/8192 [00:13<00:03, 489.47it/s]
Adding requests:  78%|███████▊  | 6394/8192 [00:13<00:03, 491.34it/s]
Adding requests:  79%|███████▊  | 6445/8192 [00:13<00:03, 495.97it/s]
Adding requests:  79%|███████▉  | 6495/8192 [00:13<00:03, 496.49it/s]
Adding requests:  80%|███████▉  | 6545/8192 [00:13<00:03, 497.07it/s]
Adding requests:  81%|████████  | 6595/8192 [00:13<00:03, 492.34it/s]
Adding requests:  81%|████████  | 6645/8192 [00:14<00:03, 492.27it/s]
Adding requests:  82%|████████▏ | 6695/8192 [00:14<00:03, 489.10it/s]
Adding requests:  82%|████████▏ | 6744/8192 [00:14<00:02, 487.59it/s]
Adding requests:  83%|████████▎ | 6795/8192 [00:14<00:02, 492.53it/s]
Adding requests:  84%|████████▎ | 6845/8192 [00:14<00:02, 490.96it/s]
Adding requests:  84%|████████▍ | 6895/8192 [00:14<00:02, 493.61it/s]
Adding requests:  85%|████████▍ | 6946/8192 [00:14<00:02, 496.84it/s]
Adding requests:  85%|████████▌ | 6996/8192 [00:14<00:02, 490.64it/s]
Adding requests:  86%|████████▌ | 7046/8192 [00:14<00:02, 487.41it/s]
Adding requests:  87%|████████▋ | 7095/8192 [00:15<00:02, 485.55it/s]
Adding requests:  87%|████████▋ | 7144/8192 [00:15<00:02, 486.33it/s]
Adding requests:  88%|████████▊ | 7193/8192 [00:15<00:02, 483.48it/s]
Adding requests:  88%|████████▊ | 7242/8192 [00:15<00:01, 484.61it/s]
Adding requests:  89%|████████▉ | 7291/8192 [00:15<00:01, 477.73it/s]
Adding requests:  90%|████████▉ | 7340/8192 [00:15<00:01, 479.23it/s]
Adding requests:  90%|█████████ | 7388/8192 [00:15<00:01, 479.28it/s]
Adding requests:  91%|█████████ | 7439/8192 [00:15<00:01, 488.35it/s]
Adding requests:  91%|█████████▏| 7488/8192 [00:15<00:01, 487.44it/s]
Adding requests:  92%|█████████▏| 7537/8192 [00:15<00:01, 486.64it/s]
Adding requests:  93%|█████████▎| 7586/8192 [00:16<00:01, 484.68it/s]
Adding requests:  93%|█████████▎| 7635/8192 [00:16<00:01, 482.47it/s]
Adding requests:  94%|█████████▍| 7685/8192 [00:16<00:01, 486.85it/s]
Adding requests:  94%|█████████▍| 7734/8192 [00:16<00:00, 485.16it/s]
Adding requests:  95%|█████████▌| 7783/8192 [00:16<00:00, 483.19it/s]
Adding requests:  96%|█████████▌| 7832/8192 [00:16<00:00, 482.53it/s]
Adding requests:  96%|█████████▌| 7881/8192 [00:16<00:00, 480.55it/s]
Adding requests:  97%|█████████▋| 7930/8192 [00:16<00:00, 459.11it/s]
Adding requests:  97%|█████████▋| 7977/8192 [00:16<00:00, 460.66it/s]
Adding requests:  98%|█████████▊| 8025/8192 [00:16<00:00, 461.91it/s]
Adding requests:  99%|█████████▊| 8074/8192 [00:17<00:00, 469.47it/s]
Adding requests:  99%|█████████▉| 8124/8192 [00:17<00:00, 475.97it/s]
Adding requests: 100%|█████████▉| 8172/8192 [00:17<00:00, 475.73it/s]
Adding requests: 100%|██████████| 8192/8192 [00:17<00:00, 473.18it/s]

Processed prompts:   0%|          | 0/8192 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  69%|██████▉   | 5632/8192 [00:00<00:00, 56318.31it/s, est. speed input: 57673660.07 toks/s, output: 56319.38 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [00:00<00:00, 56318.31it/s, est. speed input: 58178777.62 toks/s, output: 56813.33 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [00:00<00:00, 56768.75it/s, est. speed input: 58178777.62 toks/s, output: 56813.33 toks/s]
[rank0]:[W126 13:38:21.350841771 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 85.8s

测试结果:
  Requests/s:   469.19
  Tokens/s:     480920.83
  Total Reqs:   8192
  Elapsed:      17.46s

  [Prefill 分析]
  Total Prefill Tokens: 8388608
  Prefill Tokens/s:     480451.64


------------------------------------------------------------
  生成 CSV: Llama3.2-1B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_4/Llama3.2-1B-INT8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,40.6098,20832.8349,3.1519
1024,1024,1,128,128,35.8331,36728.9307,3.5721
2048,1024,2,256,128,80.2527,82259.0625,3.1899
4096,1024,4,512,128,162.2078,166262.9963,3.1564
8192,1024,8,1024,128,291.8089,299104.1036,3.5091
16384,1024,16,2048,128,460.3897,471899.4644,4.4484
32768,1024,32,4096,128,468.8222,480542.7881,8.7368
65536,1024,64,8192,128,469.1911,480920.8268,17.4598

------------------------------------------------------------

[INFO] 完成: 8 成功, 0 失败

============================================================
  Llama3.2-1B-INT8 | cuSPARSELt (2_6) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_6
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_6

============================================================
[1/8] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:38:30 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=511227) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=511227) WARNING 01-26 13:38:42 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=511227) WARNING 01-26 13:38:49 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 43.40 requests/s, 22262.90 total tokens/s, 43.40 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-26 13:38:29] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:38:29] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:38:29] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:38:29] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:38:29] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:38:29] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:38:29] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:38:29] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:38:29] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:38:29] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:38:29] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:38:29] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:38:29] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:38:29] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:38:36] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:38:36] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:38:36] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:38:36] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:38:36] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:38:36] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:38:36] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:38:36] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:38:36] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:38:36] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:38:36] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:38:36] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:38:36] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:38:36] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=511227) [2026-01-26 13:38:37] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=511227) [2026-01-26 13:38:37] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=511227) [2026-01-26 13:38:37] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=511227) [2026-01-26 13:38:37] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=511227) [2026-01-26 13:38:37] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=511227) [2026-01-26 13:38:37] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=511227) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=511227) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.70it/s]
(EngineCore_DP0 pid=511227) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.70it/s]
(EngineCore_DP0 pid=511227) 
(EngineCore_DP0 pid=511227) [2026-01-26 13:38:38] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=511227) [2026-01-26 13:38:38] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5308416 bytes
(EngineCore_DP0 pid=511227) [2026-01-26 13:38:38] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=511227) [2026-01-26 13:38:38] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 3538944 bytes
(EngineCore_DP0 pid=511227) [2026-01-26 13:38:38] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=511227) [2026-01-26 13:38:38] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 28311552 bytes
(EngineCore_DP0 pid=511227) [2026-01-26 13:38:38] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=511227) [2026-01-26 13:38:38] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 14024704 bytes
(EngineCore_DP0 pid=511227) 2026-01-26 13:38:49,451 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=511227) 2026-01-26 13:38:49,473 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=511227) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 20.48it/s]
(EngineCore_DP0 pid=511227) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 20.41it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  58%|█████▊    | 74/128 [00:00<00:00, 731.99it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 748.89it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   8%|▊         | 10/128 [00:00<00:01, 84.47it/s, est. speed input: 43254.44 toks/s, output: 84.48 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:00<00:01, 55.81it/s, est. speed input: 30193.58 toks/s, output: 58.97 toks/s]
Processed prompts:  20%|██        | 26/128 [00:00<00:02, 50.72it/s, est. speed input: 27713.72 toks/s, output: 54.13 toks/s]
Processed prompts:  25%|██▌       | 32/128 [00:00<00:01, 48.48it/s, est. speed input: 26608.68 toks/s, output: 51.97 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:00<00:01, 47.23it/s, est. speed input: 25994.63 toks/s, output: 50.77 toks/s]
Processed prompts:  33%|███▎      | 42/128 [00:00<00:01, 46.34it/s, est. speed input: 25544.68 toks/s, output: 49.89 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:00<00:01, 45.72it/s, est. speed input: 25202.49 toks/s, output: 49.22 toks/s]
Processed prompts:  41%|████      | 52/128 [00:01<00:01, 45.21it/s, est. speed input: 24919.49 toks/s, output: 48.67 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:01<00:01, 44.85it/s, est. speed input: 24689.15 toks/s, output: 48.22 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:01<00:01, 44.78it/s, est. speed input: 24529.16 toks/s, output: 47.91 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:01<00:01, 44.62it/s, est. speed input: 24378.51 toks/s, output: 47.61 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:01<00:01, 44.56it/s, est. speed input: 24258.21 toks/s, output: 47.38 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:01<00:01, 44.59it/s, est. speed input: 24162.48 toks/s, output: 47.19 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:01<00:01, 44.55it/s, est. speed input: 24071.85 toks/s, output: 47.01 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:01<00:00, 44.59it/s, est. speed input: 24000.08 toks/s, output: 46.88 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:01<00:00, 44.52it/s, est. speed input: 23926.49 toks/s, output: 46.73 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:02<00:00, 44.48it/s, est. speed input: 23860.94 toks/s, output: 46.60 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:02<00:00, 44.43it/s, est. speed input: 23801.03 toks/s, output: 46.49 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:02<00:00, 44.48it/s, est. speed input: 23753.67 toks/s, output: 46.39 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:02<00:00, 44.58it/s, est. speed input: 23716.86 toks/s, output: 46.32 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:02<00:00, 44.54it/s, est. speed input: 23674.11 toks/s, output: 46.24 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:02<00:00, 44.56it/s, est. speed input: 23638.53 toks/s, output: 46.17 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:02<00:00, 44.56it/s, est. speed input: 23605.27 toks/s, output: 46.10 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 44.56it/s, est. speed input: 23597.13 toks/s, output: 46.09 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 46.09it/s, est. speed input: 23597.13 toks/s, output: 46.09 toks/s]
[rank0]:[W126 13:38:54.572888589 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 33.2s

测试结果:
  Requests/s:   43.40
  Tokens/s:     22262.90
  Total Reqs:   128
  Elapsed:      2.95s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     22219.50

============================================================
[2/8] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:39:03 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=512249) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=512249) WARNING 01-26 13:39:16 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=512249) WARNING 01-26 13:39:23 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 40.77 requests/s, 41787.13 total tokens/s, 40.77 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-26 13:39:03] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:39:03] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:39:03] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:39:03] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:39:03] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:39:03] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:39:03] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:39:03] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:39:03] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:39:03] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:39:03] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:39:03] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:39:03] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:39:03] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:39:10] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:39:10] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:39:10] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:39:10] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:39:10] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:39:10] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:39:10] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:39:10] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:39:10] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:39:10] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:39:10] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:39:10] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:39:10] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:39:10] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=512249) [2026-01-26 13:39:11] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=512249) [2026-01-26 13:39:11] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=512249) [2026-01-26 13:39:11] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=512249) [2026-01-26 13:39:11] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=512249) [2026-01-26 13:39:11] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=512249) [2026-01-26 13:39:11] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=512249) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=512249) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.79it/s]
(EngineCore_DP0 pid=512249) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.79it/s]
(EngineCore_DP0 pid=512249) 
(EngineCore_DP0 pid=512249) [2026-01-26 13:39:12] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=512249) [2026-01-26 13:39:12] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5308416 bytes
(EngineCore_DP0 pid=512249) [2026-01-26 13:39:12] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=512249) [2026-01-26 13:39:12] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 3538944 bytes
(EngineCore_DP0 pid=512249) [2026-01-26 13:39:12] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=512249) [2026-01-26 13:39:12] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 28311552 bytes
(EngineCore_DP0 pid=512249) [2026-01-26 13:39:12] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=512249) [2026-01-26 13:39:12] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 14024704 bytes
(EngineCore_DP0 pid=512249) 2026-01-26 13:39:23,177 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=512249) 2026-01-26 13:39:23,200 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=512249) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 20.46it/s]
(EngineCore_DP0 pid=512249) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 20.11it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  28%|██▊       | 36/128 [00:00<00:00, 353.86it/s]
Adding requests:  59%|█████▉    | 76/128 [00:00<00:00, 376.40it/s]
Adding requests:  92%|█████████▏| 118/128 [00:00<00:00, 391.79it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 389.33it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   9%|▊         | 11/128 [00:00<00:01, 92.73it/s, est. speed input: 94961.24 toks/s, output: 92.73 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:01, 56.48it/s, est. speed input: 61624.97 toks/s, output: 60.18 toks/s]
Processed prompts:  22%|██▏       | 28/128 [00:00<00:01, 51.12it/s, est. speed input: 56338.60 toks/s, output: 55.02 toks/s]
Processed prompts:  27%|██▋       | 34/128 [00:00<00:01, 48.74it/s, est. speed input: 53982.47 toks/s, output: 52.72 toks/s]
Processed prompts:  31%|███▏      | 40/128 [00:00<00:01, 47.13it/s, est. speed input: 52387.17 toks/s, output: 51.16 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:00<00:01, 46.08it/s, est. speed input: 51369.87 toks/s, output: 50.17 toks/s]
Processed prompts:  39%|███▉      | 50/128 [00:01<00:01, 45.33it/s, est. speed input: 50589.02 toks/s, output: 49.40 toks/s]
Processed prompts:  43%|████▎     | 55/128 [00:01<00:01, 44.79it/s, est. speed input: 49968.02 toks/s, output: 48.80 toks/s]
Processed prompts:  47%|████▋     | 60/128 [00:01<00:01, 44.36it/s, est. speed input: 49450.27 toks/s, output: 48.29 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:01<00:01, 44.13it/s, est. speed input: 49042.81 toks/s, output: 47.89 toks/s]
Processed prompts:  55%|█████▍    | 70/128 [00:01<00:01, 43.93it/s, est. speed input: 48686.08 toks/s, output: 47.54 toks/s]
Processed prompts:  59%|█████▊    | 75/128 [00:01<00:01, 43.73it/s, est. speed input: 48366.07 toks/s, output: 47.23 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:01<00:01, 43.71it/s, est. speed input: 48119.88 toks/s, output: 46.99 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:01<00:00, 43.64it/s, est. speed input: 47891.83 toks/s, output: 46.77 toks/s]
Processed prompts:  70%|███████   | 90/128 [00:01<00:00, 43.48it/s, est. speed input: 47667.09 toks/s, output: 46.55 toks/s]
Processed prompts:  74%|███████▍  | 95/128 [00:02<00:00, 43.37it/s, est. speed input: 47467.35 toks/s, output: 46.35 toks/s]
Processed prompts:  78%|███████▊  | 100/128 [00:02<00:00, 43.39it/s, est. speed input: 47308.36 toks/s, output: 46.20 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:02<00:00, 43.42it/s, est. speed input: 47168.09 toks/s, output: 46.06 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:02<00:00, 43.39it/s, est. speed input: 47032.72 toks/s, output: 45.93 toks/s]
Processed prompts:  90%|████████▉ | 115/128 [00:02<00:00, 43.35it/s, est. speed input: 46906.93 toks/s, output: 45.81 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:02<00:00, 43.33it/s, est. speed input: 46793.37 toks/s, output: 45.70 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:02<00:00, 43.37it/s, est. speed input: 46696.81 toks/s, output: 45.60 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 43.37it/s, est. speed input: 46647.69 toks/s, output: 45.55 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 45.55it/s, est. speed input: 46647.69 toks/s, output: 45.55 toks/s]
[rank0]:[W126 13:39:28.439558000 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 34.0s

测试结果:
  Requests/s:   40.77
  Tokens/s:     41787.13
  Total Reqs:   128
  Elapsed:      3.14s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     41746.36

============================================================
[3/8] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:39:38 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=513290) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=513290) WARNING 01-26 13:39:50 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=513290) WARNING 01-26 13:39:57 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 82.95 requests/s, 85026.45 total tokens/s, 82.95 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-26 13:39:37] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:39:38] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:39:38] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:39:38] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:39:38] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:39:38] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:39:38] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:39:38] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:39:38] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:39:38] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:39:38] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:39:38] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:39:38] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:39:38] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:39:44] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:39:44] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:39:44] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:39:44] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:39:44] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:39:44] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:39:44] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:39:44] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:39:44] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:39:44] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:39:44] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:39:44] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:39:44] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:39:44] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=513290) [2026-01-26 13:39:45] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=513290) [2026-01-26 13:39:45] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=513290) [2026-01-26 13:39:45] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=513290) [2026-01-26 13:39:45] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=513290) [2026-01-26 13:39:45] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=513290) [2026-01-26 13:39:45] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=513290) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=513290) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.78it/s]
(EngineCore_DP0 pid=513290) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.78it/s]
(EngineCore_DP0 pid=513290) 
(EngineCore_DP0 pid=513290) [2026-01-26 13:39:46] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=513290) [2026-01-26 13:39:46] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5308416 bytes
(EngineCore_DP0 pid=513290) [2026-01-26 13:39:46] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=513290) [2026-01-26 13:39:46] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 3538944 bytes
(EngineCore_DP0 pid=513290) [2026-01-26 13:39:46] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=513290) [2026-01-26 13:39:46] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 28311552 bytes
(EngineCore_DP0 pid=513290) [2026-01-26 13:39:46] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=513290) [2026-01-26 13:39:46] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 14024704 bytes
(EngineCore_DP0 pid=513290) 2026-01-26 13:39:57,496 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=513290) 2026-01-26 13:39:57,517 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=513290) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 18.25it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 18.24it/s]
(EngineCore_DP0 pid=513290) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 15.47it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 15.46it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  16%|█▌        | 40/256 [00:00<00:00, 390.47it/s]
Adding requests:  33%|███▎      | 84/256 [00:00<00:00, 415.55it/s]
Adding requests:  50%|█████     | 128/256 [00:00<00:00, 426.53it/s]
Adding requests:  67%|██████▋   | 172/256 [00:00<00:00, 429.50it/s]
Adding requests:  84%|████████▍ | 216/256 [00:00<00:00, 432.95it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 433.96it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  20%|█▉        | 50/256 [00:00<00:00, 472.35it/s, est. speed input: 483719.78 toks/s, output: 472.36 toks/s]
Processed prompts:  38%|███▊      | 98/256 [00:00<00:01, 131.79it/s, est. speed input: 151695.07 toks/s, output: 148.14 toks/s]
Processed prompts:  48%|████▊     | 123/256 [00:00<00:01, 115.67it/s, est. speed input: 134117.48 toks/s, output: 130.97 toks/s]
Processed prompts:  55%|█████▌    | 141/256 [00:01<00:01, 107.18it/s, est. speed input: 125894.41 toks/s, output: 122.94 toks/s]
Processed prompts:  61%|██████    | 155/256 [00:01<00:00, 101.44it/s, est. speed input: 120879.59 toks/s, output: 118.05 toks/s]
Processed prompts:  65%|██████▌   | 167/256 [00:01<00:00, 98.03it/s, est. speed input: 117805.93 toks/s, output: 115.04 toks/s] 
Processed prompts:  70%|██████▉   | 178/256 [00:01<00:00, 93.39it/s, est. speed input: 114610.38 toks/s, output: 111.92 toks/s]
Processed prompts:  73%|███████▎  | 188/256 [00:01<00:00, 91.86it/s, est. speed input: 112860.76 toks/s, output: 110.22 toks/s]
Processed prompts:  77%|███████▋  | 198/256 [00:01<00:00, 90.02it/s, est. speed input: 111151.41 toks/s, output: 108.55 toks/s]
Processed prompts:  81%|████████▏ | 208/256 [00:01<00:00, 89.11it/s, est. speed input: 109803.88 toks/s, output: 107.23 toks/s]
Processed prompts:  85%|████████▌ | 218/256 [00:02<00:00, 88.36it/s, est. speed input: 108600.07 toks/s, output: 106.05 toks/s]
Processed prompts:  89%|████████▉ | 228/256 [00:02<00:00, 87.91it/s, est. speed input: 107547.67 toks/s, output: 105.03 toks/s]
Processed prompts:  93%|█████████▎| 238/256 [00:02<00:00, 87.50it/s, est. speed input: 106587.44 toks/s, output: 104.09 toks/s]
Processed prompts:  97%|█████████▋| 248/256 [00:02<00:00, 86.99it/s, est. speed input: 105675.46 toks/s, output: 103.20 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 86.99it/s, est. speed input: 105070.99 toks/s, output: 102.61 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 102.60it/s, est. speed input: 105070.99 toks/s, output: 102.61 toks/s]
[rank0]:[W126 13:40:02.959055421 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 34.2s

测试结果:
  Requests/s:   82.95
  Tokens/s:     85026.45
  Total Reqs:   256
  Elapsed:      3.09s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     84943.49

============================================================
[4/8] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:40:13 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=514341) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=514341) WARNING 01-26 13:40:25 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=514341) WARNING 01-26 13:40:32 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 160.80 requests/s, 164817.36 total tokens/s, 160.80 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-26 13:40:13] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:40:13] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:40:13] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:40:13] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:40:13] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:40:13] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:40:13] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:40:13] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:40:13] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:40:13] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:40:13] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:40:13] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:40:13] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:40:13] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:40:19] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:40:20] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:40:20] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:40:20] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:40:20] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:40:20] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:40:20] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:40:20] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:40:20] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:40:20] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:40:20] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:40:20] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:40:20] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:40:20] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=514341) [2026-01-26 13:40:21] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=514341) [2026-01-26 13:40:21] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=514341) [2026-01-26 13:40:21] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=514341) [2026-01-26 13:40:21] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=514341) [2026-01-26 13:40:21] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=514341) [2026-01-26 13:40:21] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=514341) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=514341) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.07it/s]
(EngineCore_DP0 pid=514341) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.07it/s]
(EngineCore_DP0 pid=514341) 
(EngineCore_DP0 pid=514341) [2026-01-26 13:40:21] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=514341) [2026-01-26 13:40:21] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5308416 bytes
(EngineCore_DP0 pid=514341) [2026-01-26 13:40:21] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=514341) [2026-01-26 13:40:21] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 3538944 bytes
(EngineCore_DP0 pid=514341) [2026-01-26 13:40:21] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=514341) [2026-01-26 13:40:21] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 28311552 bytes
(EngineCore_DP0 pid=514341) [2026-01-26 13:40:21] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=514341) [2026-01-26 13:40:21] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 14024704 bytes
(EngineCore_DP0 pid=514341) 2026-01-26 13:40:32,703 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=514341) 2026-01-26 13:40:32,725 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=514341) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 3/4 [00:00<00:00, 23.00it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 20.00it/s]
(EngineCore_DP0 pid=514341) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 23.10it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 23.08it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   8%|▊         | 40/512 [00:00<00:01, 399.40it/s]
Adding requests:  17%|█▋        | 85/512 [00:00<00:01, 424.67it/s]
Adding requests:  25%|██▌       | 128/512 [00:00<00:00, 423.11it/s]
Adding requests:  34%|███▎      | 172/512 [00:00<00:00, 427.29it/s]
Adding requests:  43%|████▎     | 218/512 [00:00<00:00, 436.44it/s]
Adding requests:  52%|█████▏    | 266/512 [00:00<00:00, 449.42it/s]
Adding requests:  61%|██████    | 312/512 [00:00<00:00, 449.86it/s]
Adding requests:  70%|██████▉   | 358/512 [00:00<00:00, 452.98it/s]
Adding requests:  79%|███████▉  | 406/512 [00:00<00:00, 459.26it/s]
Adding requests:  88%|████████▊ | 453/512 [00:01<00:00, 460.71it/s]
Adding requests:  98%|█████████▊| 500/512 [00:01<00:00, 461.75it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 449.43it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  36%|███▌      | 182/512 [00:00<00:00, 1585.81it/s, est. speed input: 1623968.09 toks/s, output: 1585.83 toks/s]
Processed prompts:  67%|██████▋   | 341/512 [00:01<00:00, 291.55it/s, est. speed input: 343432.13 toks/s, output: 335.38 toks/s]   
Processed prompts:  81%|████████▏ | 417/512 [00:01<00:00, 242.55it/s, est. speed input: 290676.38 toks/s, output: 283.86 toks/s]
Processed prompts:  91%|█████████ | 466/512 [00:01<00:00, 218.71it/s, est. speed input: 268075.37 toks/s, output: 261.79 toks/s]
Processed prompts:  98%|█████████▊| 502/512 [00:01<00:00, 208.88it/s, est. speed input: 258456.10 toks/s, output: 252.40 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:02<00:00, 208.88it/s, est. speed input: 256534.44 toks/s, output: 250.52 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:02<00:00, 250.51it/s, est. speed input: 256534.44 toks/s, output: 250.52 toks/s]
[rank0]:[W126 13:40:38.464750881 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 35.5s

测试结果:
  Requests/s:   160.80
  Tokens/s:     164817.36
  Total Reqs:   512
  Elapsed:      3.18s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     164656.56

============================================================
[5/8] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:40:51 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=515427) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=515427) WARNING 01-26 13:41:03 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=515427) WARNING 01-26 13:41:10 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 300.17 requests/s, 307675.96 total tokens/s, 300.17 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-26 13:40:50] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:40:51] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:40:51] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:40:51] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:40:51] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:40:51] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:40:51] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:40:51] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:40:51] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:40:51] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:40:51] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:40:51] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:40:51] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:40:51] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:40:57] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:40:58] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:40:58] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:40:58] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:40:58] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:40:58] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:40:58] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:40:58] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:40:58] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:40:58] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:40:58] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:40:58] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:40:58] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:40:58] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=515427) [2026-01-26 13:40:59] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=515427) [2026-01-26 13:40:59] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=515427) [2026-01-26 13:40:59] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=515427) [2026-01-26 13:40:59] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=515427) [2026-01-26 13:40:59] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=515427) [2026-01-26 13:40:59] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=515427) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=515427) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.39it/s]
(EngineCore_DP0 pid=515427) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.39it/s]
(EngineCore_DP0 pid=515427) 
(EngineCore_DP0 pid=515427) [2026-01-26 13:40:59] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=515427) [2026-01-26 13:40:59] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5308416 bytes
(EngineCore_DP0 pid=515427) [2026-01-26 13:40:59] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=515427) [2026-01-26 13:40:59] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 3538944 bytes
(EngineCore_DP0 pid=515427) [2026-01-26 13:40:59] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=515427) [2026-01-26 13:40:59] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 28311552 bytes
(EngineCore_DP0 pid=515427) [2026-01-26 13:40:59] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=515427) [2026-01-26 13:40:59] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 14024704 bytes
(EngineCore_DP0 pid=515427) 2026-01-26 13:41:10,494 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=515427) 2026-01-26 13:41:10,516 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=515427) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  20%|██        | 1/5 [00:00<00:00,  8.34it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00, 17.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 15.58it/s]
(EngineCore_DP0 pid=515427) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  75%|███████▌  | 3/4 [00:00<00:00, 23.24it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 23.13it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   4%|▍         | 41/1024 [00:00<00:02, 402.90it/s]
Adding requests:   8%|▊         | 86/1024 [00:00<00:02, 430.17it/s]
Adding requests:  13%|█▎        | 131/1024 [00:00<00:02, 438.97it/s]
Adding requests:  17%|█▋        | 176/1024 [00:00<00:01, 440.46it/s]
Adding requests:  22%|██▏       | 223/1024 [00:00<00:01, 449.47it/s]
Adding requests:  26%|██▋       | 271/1024 [00:00<00:01, 459.59it/s]
Adding requests:  31%|███       | 317/1024 [00:00<00:01, 452.77it/s]
Adding requests:  36%|███▌      | 366/1024 [00:00<00:01, 461.52it/s]
Adding requests:  40%|████      | 413/1024 [00:00<00:01, 463.34it/s]
Adding requests:  45%|████▌     | 461/1024 [00:01<00:01, 467.31it/s]
Adding requests:  50%|████▉     | 508/1024 [00:01<00:01, 465.06it/s]
Adding requests:  54%|█████▍    | 555/1024 [00:01<00:01, 464.36it/s]
Adding requests:  59%|█████▉    | 602/1024 [00:01<00:00, 465.98it/s]
Adding requests:  64%|██████▎   | 652/1024 [00:01<00:00, 474.80it/s]
Adding requests:  69%|██████▊   | 702/1024 [00:01<00:00, 481.65it/s]
Adding requests:  73%|███████▎  | 751/1024 [00:01<00:00, 479.32it/s]
Adding requests:  78%|███████▊  | 799/1024 [00:01<00:00, 476.04it/s]
Adding requests:  83%|████████▎ | 847/1024 [00:01<00:00, 466.62it/s]
Adding requests:  88%|████████▊ | 897/1024 [00:01<00:00, 475.22it/s]
Adding requests:  92%|█████████▏| 945/1024 [00:02<00:00, 475.53it/s]
Adding requests:  97%|█████████▋| 994/1024 [00:02<00:00, 478.13it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 466.40it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  63%|██████▎   | 650/1024 [00:00<00:00, 5497.04it/s, est. speed input: 5629302.52 toks/s, output: 5497.13 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:01<00:00, 5497.04it/s, est. speed input: 863290.52 toks/s, output: 843.05 toks/s] 
Processed prompts: 100%|██████████| 1024/1024 [00:01<00:00, 842.98it/s, est. speed input: 863290.52 toks/s, output: 843.05 toks/s] 
[rank0]:[W126 13:41:16.789073693 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 38.3s

测试结果:
  Requests/s:   300.17
  Tokens/s:     307675.96
  Total Reqs:   1024
  Elapsed:      3.41s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     307375.79

============================================================
[6/8] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:41:34 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=516585) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=516585) WARNING 01-26 13:41:46 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=516585) WARNING 01-26 13:41:53 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 461.30 requests/s, 472834.42 total tokens/s, 461.30 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-26 13:41:33] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:41:34] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:41:34] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:41:34] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:41:34] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:41:34] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:41:34] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:41:34] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:41:34] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:41:34] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:41:34] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:41:34] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:41:34] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:41:34] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:41:40] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:41:41] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:41:41] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:41:41] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:41:41] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:41:41] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:41:41] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:41:41] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:41:41] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:41:41] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:41:41] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:41:41] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:41:41] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:41:41] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=516585) [2026-01-26 13:41:42] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=516585) [2026-01-26 13:41:42] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=516585) [2026-01-26 13:41:42] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=516585) [2026-01-26 13:41:42] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=516585) [2026-01-26 13:41:42] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=516585) [2026-01-26 13:41:42] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=516585) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=516585) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.43it/s]
(EngineCore_DP0 pid=516585) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.42it/s]
(EngineCore_DP0 pid=516585) 
(EngineCore_DP0 pid=516585) [2026-01-26 13:41:42] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=516585) [2026-01-26 13:41:42] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5308416 bytes
(EngineCore_DP0 pid=516585) [2026-01-26 13:41:42] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=516585) [2026-01-26 13:41:42] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 3538944 bytes
(EngineCore_DP0 pid=516585) [2026-01-26 13:41:42] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=516585) [2026-01-26 13:41:42] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 28311552 bytes
(EngineCore_DP0 pid=516585) [2026-01-26 13:41:42] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=516585) [2026-01-26 13:41:42] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 14024704 bytes
(EngineCore_DP0 pid=516585) 2026-01-26 13:41:53,430 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=516585) 2026-01-26 13:41:53,466 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=516585) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 3/7 [00:00<00:00, 21.92it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:00<00:00, 23.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 21.62it/s]
(EngineCore_DP0 pid=516585) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 3/5 [00:00<00:00, 23.49it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 23.68it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 40/2048 [00:00<00:05, 399.46it/s]
Adding requests:   4%|▍         | 85/2048 [00:00<00:04, 424.90it/s]
Adding requests:   6%|▋         | 130/2048 [00:00<00:04, 433.49it/s]
Adding requests:   8%|▊         | 174/2048 [00:00<00:04, 434.54it/s]
Adding requests:  11%|█         | 220/2048 [00:00<00:04, 442.23it/s]
Adding requests:  13%|█▎        | 268/2048 [00:00<00:03, 452.43it/s]
Adding requests:  15%|█▌        | 314/2048 [00:00<00:03, 451.87it/s]
Adding requests:  18%|█▊        | 360/2048 [00:00<00:03, 453.62it/s]
Adding requests:  20%|█▉        | 408/2048 [00:00<00:03, 459.39it/s]
Adding requests:  22%|██▏       | 455/2048 [00:01<00:03, 462.60it/s]
Adding requests:  25%|██▍       | 502/2048 [00:01<00:03, 461.22it/s]
Adding requests:  27%|██▋       | 549/2048 [00:01<00:03, 456.58it/s]
Adding requests:  29%|██▉       | 596/2048 [00:01<00:03, 458.66it/s]
Adding requests:  31%|███▏      | 644/2048 [00:01<00:03, 464.88it/s]
Adding requests:  34%|███▍      | 694/2048 [00:01<00:02, 472.63it/s]
Adding requests:  36%|███▌      | 742/2048 [00:01<00:02, 471.75it/s]
Adding requests:  39%|███▊      | 790/2048 [00:01<00:02, 469.83it/s]
Adding requests:  41%|████      | 837/2048 [00:01<00:02, 460.01it/s]
Adding requests:  43%|████▎     | 886/2048 [00:01<00:02, 467.51it/s]
Adding requests:  46%|████▌     | 935/2048 [00:02<00:02, 470.71it/s]
Adding requests:  48%|████▊     | 983/2048 [00:02<00:02, 473.38it/s]
Adding requests:  50%|█████     | 1032/2048 [00:02<00:02, 475.95it/s]
Adding requests:  53%|█████▎    | 1080/2048 [00:02<00:02, 473.42it/s]
Adding requests:  55%|█████▌    | 1128/2048 [00:02<00:01, 469.59it/s]
Adding requests:  58%|█████▊    | 1178/2048 [00:02<00:01, 475.83it/s]
Adding requests:  60%|█████▉    | 1228/2048 [00:02<00:01, 481.06it/s]
Adding requests:  62%|██████▏   | 1277/2048 [00:02<00:01, 472.85it/s]
Adding requests:  65%|██████▍   | 1326/2048 [00:02<00:01, 476.93it/s]
Adding requests:  67%|██████▋   | 1375/2048 [00:02<00:01, 479.58it/s]
Adding requests:  70%|██████▉   | 1424/2048 [00:03<00:01, 480.24it/s]
Adding requests:  72%|███████▏  | 1473/2048 [00:03<00:01, 480.28it/s]
Adding requests:  74%|███████▍  | 1522/2048 [00:03<00:01, 482.08it/s]
Adding requests:  77%|███████▋  | 1571/2048 [00:03<00:00, 482.71it/s]
Adding requests:  79%|███████▉  | 1620/2048 [00:03<00:00, 475.78it/s]
Adding requests:  81%|████████▏ | 1668/2048 [00:03<00:00, 474.25it/s]
Adding requests:  84%|████████▍ | 1717/2048 [00:03<00:00, 477.03it/s]
Adding requests:  86%|████████▌ | 1765/2048 [00:03<00:00, 474.25it/s]
Adding requests:  89%|████████▊ | 1813/2048 [00:03<00:00, 474.15it/s]
Adding requests:  91%|█████████ | 1861/2048 [00:03<00:00, 472.51it/s]
Adding requests:  93%|█████████▎| 1909/2048 [00:04<00:00, 471.09it/s]
Adding requests:  96%|█████████▌| 1957/2048 [00:04<00:00, 473.61it/s]
Adding requests:  98%|█████████▊| 2006/2048 [00:04<00:00, 477.71it/s]
Adding requests: 100%|██████████| 2048/2048 [00:04<00:00, 468.30it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:00<00:00, 31567.41it/s, est. speed input: 32334535.47 toks/s, output: 31574.61 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:00<00:00, 31518.76it/s, est. speed input: 32334535.47 toks/s, output: 31574.61 toks/s]
[rank0]:[W126 13:42:00.383973370 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 43.6s

测试结果:
  Requests/s:   461.30
  Tokens/s:     472834.42
  Total Reqs:   2048
  Elapsed:      4.44s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     472373.12

============================================================
[7/8] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:42:26 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=517848) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=517848) WARNING 01-26 13:42:39 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=517848) WARNING 01-26 13:42:46 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 466.73 requests/s, 478402.83 total tokens/s, 466.73 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-26 13:42:26] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:42:26] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:42:26] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:42:26] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:42:26] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:42:26] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:42:26] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:42:26] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:42:26] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:42:26] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:42:26] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:42:26] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:42:26] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:42:26] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:42:33] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:42:33] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:42:33] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:42:33] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:42:33] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:42:33] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:42:33] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:42:33] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:42:33] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:42:33] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:42:33] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:42:33] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:42:33] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:42:33] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=517848) [2026-01-26 13:42:34] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=517848) [2026-01-26 13:42:34] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=517848) [2026-01-26 13:42:34] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=517848) [2026-01-26 13:42:34] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=517848) [2026-01-26 13:42:34] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=517848) [2026-01-26 13:42:34] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=517848) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=517848) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.42it/s]
(EngineCore_DP0 pid=517848) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.42it/s]
(EngineCore_DP0 pid=517848) 
(EngineCore_DP0 pid=517848) [2026-01-26 13:42:35] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=517848) [2026-01-26 13:42:35] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5308416 bytes
(EngineCore_DP0 pid=517848) [2026-01-26 13:42:35] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=517848) [2026-01-26 13:42:35] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 3538944 bytes
(EngineCore_DP0 pid=517848) [2026-01-26 13:42:35] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=517848) [2026-01-26 13:42:35] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 28311552 bytes
(EngineCore_DP0 pid=517848) [2026-01-26 13:42:35] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=517848) [2026-01-26 13:42:35] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 14024704 bytes
(EngineCore_DP0 pid=517848) [rank0]:W0126 13:42:43.017000 517848 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=517848) [rank0]:W0126 13:42:43.087000 517848 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=517848) [rank0]:W0126 13:42:43.887000 517848 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=517848) [rank0]:W0126 13:42:43.985000 517848 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=517848) 2026-01-26 13:42:46,323 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=517848) 2026-01-26 13:42:46,348 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=517848) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:00, 19.66it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 5/11 [00:00<00:00, 20.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:00<00:00, 22.25it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 19.26it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 19.88it/s]
(EngineCore_DP0 pid=517848) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 3/7 [00:00<00:00, 21.87it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00, 22.97it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 22.95it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 40/4096 [00:00<00:10, 397.79it/s]
Adding requests:   2%|▏         | 84/4096 [00:00<00:09, 420.52it/s]
Adding requests:   3%|▎         | 129/4096 [00:00<00:09, 431.23it/s]
Adding requests:   4%|▍         | 173/4096 [00:00<00:09, 434.42it/s]
Adding requests:   5%|▌         | 219/4096 [00:00<00:08, 440.92it/s]
Adding requests:   7%|▋         | 267/4096 [00:00<00:08, 452.80it/s]
Adding requests:   8%|▊         | 313/4096 [00:00<00:08, 451.05it/s]
Adding requests:   9%|▉         | 359/4096 [00:00<00:08, 453.34it/s]
Adding requests:  10%|▉         | 406/4096 [00:00<00:08, 457.92it/s]
Adding requests:  11%|█         | 453/4096 [00:01<00:07, 459.18it/s]
Adding requests:  12%|█▏        | 500/4096 [00:01<00:07, 459.81it/s]
Adding requests:  13%|█▎        | 546/4096 [00:01<00:07, 455.09it/s]
Adding requests:  15%|█▍        | 596/4096 [00:01<00:07, 463.94it/s]
Adding requests:  16%|█▌        | 644/4096 [00:01<00:07, 468.11it/s]
Adding requests:  17%|█▋        | 694/4096 [00:01<00:07, 475.90it/s]
Adding requests:  18%|█▊        | 742/4096 [00:01<00:07, 473.23it/s]
Adding requests:  19%|█▉        | 790/4096 [00:01<00:06, 472.55it/s]
Adding requests:  20%|██        | 838/4096 [00:01<00:07, 463.97it/s]
Adding requests:  22%|██▏       | 885/4096 [00:01<00:06, 462.46it/s]
Adding requests:  23%|██▎       | 934/4096 [00:02<00:06, 469.19it/s]
Adding requests:  24%|██▍       | 981/4096 [00:02<00:06, 459.54it/s]
Adding requests:  25%|██▌       | 1030/4096 [00:02<00:06, 466.09it/s]
Adding requests:  26%|██▋       | 1077/4096 [00:02<00:06, 462.86it/s]
Adding requests:  27%|██▋       | 1124/4096 [00:02<00:06, 462.80it/s]
Adding requests:  29%|██▊       | 1173/4096 [00:02<00:06, 470.01it/s]
Adding requests:  30%|██▉       | 1223/4096 [00:02<00:06, 476.65it/s]
Adding requests:  31%|███       | 1271/4096 [00:02<00:05, 472.81it/s]
Adding requests:  32%|███▏      | 1320/4096 [00:02<00:05, 475.39it/s]
Adding requests:  33%|███▎      | 1369/4096 [00:02<00:05, 478.58it/s]
Adding requests:  35%|███▍      | 1417/4096 [00:03<00:05, 478.68it/s]
Adding requests:  36%|███▌      | 1466/4096 [00:03<00:05, 481.21it/s]
Adding requests:  37%|███▋      | 1515/4096 [00:03<00:05, 482.85it/s]
Adding requests:  38%|███▊      | 1564/4096 [00:03<00:05, 482.25it/s]
Adding requests:  39%|███▉      | 1614/4096 [00:03<00:05, 485.51it/s]
Adding requests:  41%|████      | 1663/4096 [00:03<00:05, 481.70it/s]
Adding requests:  42%|████▏     | 1712/4096 [00:03<00:04, 481.08it/s]
Adding requests:  43%|████▎     | 1761/4096 [00:03<00:04, 479.01it/s]
Adding requests:  44%|████▍     | 1810/4096 [00:03<00:04, 479.85it/s]
Adding requests:  45%|████▌     | 1859/4096 [00:03<00:04, 480.54it/s]
Adding requests:  47%|████▋     | 1908/4096 [00:04<00:04, 467.96it/s]
Adding requests:  48%|████▊     | 1956/4096 [00:04<00:04, 470.69it/s]
Adding requests:  49%|████▉     | 2005/4096 [00:04<00:04, 474.91it/s]
Adding requests:  50%|█████     | 2054/4096 [00:04<00:04, 476.91it/s]
Adding requests:  51%|█████▏    | 2102/4096 [00:04<00:04, 459.94it/s]
Adding requests:  52%|█████▏    | 2149/4096 [00:04<00:04, 459.35it/s]
Adding requests:  54%|█████▎    | 2196/4096 [00:04<00:04, 459.17it/s]
Adding requests:  55%|█████▍    | 2245/4096 [00:04<00:03, 465.89it/s]
Adding requests:  56%|█████▌    | 2292/4096 [00:04<00:03, 466.84it/s]
Adding requests:  57%|█████▋    | 2340/4096 [00:05<00:03, 469.18it/s]
Adding requests:  58%|█████▊    | 2388/4096 [00:05<00:03, 471.92it/s]
Adding requests:  59%|█████▉    | 2437/4096 [00:05<00:03, 474.26it/s]
Adding requests:  61%|██████    | 2485/4096 [00:05<00:03, 475.07it/s]
Adding requests:  62%|██████▏   | 2533/4096 [00:05<00:03, 476.01it/s]
Adding requests:  63%|██████▎   | 2582/4096 [00:05<00:03, 479.05it/s]
Adding requests:  64%|██████▍   | 2630/4096 [00:05<00:03, 478.13it/s]
Adding requests:  65%|██████▌   | 2678/4096 [00:05<00:03, 472.05it/s]
Adding requests:  67%|██████▋   | 2726/4096 [00:05<00:02, 473.44it/s]
Adding requests:  68%|██████▊   | 2774/4096 [00:05<00:02, 474.20it/s]
Adding requests:  69%|██████▉   | 2822/4096 [00:06<00:02, 469.23it/s]
Adding requests:  70%|███████   | 2871/4096 [00:06<00:02, 473.10it/s]
Adding requests:  71%|███████▏  | 2919/4096 [00:06<00:02, 474.09it/s]
Adding requests:  72%|███████▏  | 2967/4096 [00:06<00:02, 472.29it/s]
Adding requests:  74%|███████▎  | 3015/4096 [00:06<00:02, 473.87it/s]
Adding requests:  75%|███████▍  | 3063/4096 [00:06<00:02, 473.17it/s]
Adding requests:  76%|███████▌  | 3111/4096 [00:06<00:02, 462.24it/s]
Adding requests:  77%|███████▋  | 3159/4096 [00:06<00:02, 466.04it/s]
Adding requests:  78%|███████▊  | 3206/4096 [00:06<00:01, 466.86it/s]
Adding requests:  79%|███████▉  | 3255/4096 [00:06<00:01, 473.14it/s]
Adding requests:  81%|████████  | 3303/4096 [00:07<00:01, 473.72it/s]
Adding requests:  82%|████████▏ | 3352/4096 [00:07<00:01, 476.86it/s]
Adding requests:  83%|████████▎ | 3400/4096 [00:07<00:01, 475.33it/s]
Adding requests:  84%|████████▍ | 3448/4096 [00:07<00:01, 476.04it/s]
Adding requests:  85%|████████▌ | 3496/4096 [00:07<00:01, 472.11it/s]
Adding requests:  87%|████████▋ | 3544/4096 [00:07<00:01, 474.17it/s]
Adding requests:  88%|████████▊ | 3592/4096 [00:07<00:01, 473.41it/s]
Adding requests:  89%|████████▉ | 3640/4096 [00:07<00:00, 470.03it/s]
Adding requests:  90%|█████████ | 3689/4096 [00:07<00:00, 473.51it/s]
Adding requests:  91%|█████████ | 3737/4096 [00:07<00:00, 472.95it/s]
Adding requests:  92%|█████████▏| 3787/4096 [00:08<00:00, 480.28it/s]
Adding requests:  94%|█████████▎| 3836/4096 [00:08<00:00, 480.97it/s]
Adding requests:  95%|█████████▍| 3885/4096 [00:08<00:00, 483.30it/s]
Adding requests:  96%|█████████▌| 3934/4096 [00:08<00:00, 482.93it/s]
Adding requests:  97%|█████████▋| 3983/4096 [00:08<00:00, 481.62it/s]
Adding requests:  98%|█████████▊| 4032/4096 [00:08<00:00, 479.58it/s]
Adding requests: 100%|█████████▉| 4080/4096 [00:08<00:00, 475.66it/s]
Adding requests: 100%|██████████| 4096/4096 [00:08<00:00, 470.40it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:00<00:00, 61662.12it/s, est. speed input: 63156068.53 toks/s, output: 61671.20 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:00<00:00, 61564.24it/s, est. speed input: 63156068.53 toks/s, output: 61671.20 toks/s]
[rank0]:[W126 13:42:57.065522874 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 57.9s

测试结果:
  Requests/s:   466.73
  Tokens/s:     478402.83
  Total Reqs:   4096
  Elapsed:      8.78s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     477936.09

============================================================
[8/8] 测试 M=65536
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 65536
│   M_prefill     = 65536 (= 64 x 1024)
│   M_decode      = 64
│   batched_tokens = 65536 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 8192
│   --max-num-seqs           = 64
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 65536
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:43:43 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=519450) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=519450) WARNING 01-26 13:43:55 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=519450) WARNING 01-26 13:44:02 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 470.45 requests/s, 482206.74 total tokens/s, 470.45 output tokens/s
Total num prompt tokens:  8388608
Total num output tokens:  8192


─── STDERR ───
[2026-01-26 13:43:42] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:43:43] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:43:43] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:43:43] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:43:43] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:43:43] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:43:43] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:43:43] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:43:43] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:43:43] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:43:43] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:43:43] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:43:43] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:43:43] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:43:49] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:43:50] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:43:50] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:43:50] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:43:50] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:43:50] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:43:50] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:43:50] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:43:50] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:43:50] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:43:50] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:43:50] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:43:50] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:43:50] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=519450) [2026-01-26 13:43:51] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=519450) [2026-01-26 13:43:51] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=519450) [2026-01-26 13:43:51] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=519450) [2026-01-26 13:43:51] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=519450) [2026-01-26 13:43:51] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=519450) [2026-01-26 13:43:51] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=519450) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=519450) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.88it/s]
(EngineCore_DP0 pid=519450) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.88it/s]
(EngineCore_DP0 pid=519450) 
(EngineCore_DP0 pid=519450) [2026-01-26 13:43:51] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 2752] -> 1D uint8
(EngineCore_DP0 pid=519450) [2026-01-26 13:43:51] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5308416 bytes
(EngineCore_DP0 pid=519450) [2026-01-26 13:43:51] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 2752] -> 1D uint8
(EngineCore_DP0 pid=519450) [2026-01-26 13:43:51] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 3538944 bytes
(EngineCore_DP0 pid=519450) [2026-01-26 13:43:51] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 2752] -> 1D uint8
(EngineCore_DP0 pid=519450) [2026-01-26 13:43:51] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 28311552 bytes
(EngineCore_DP0 pid=519450) [2026-01-26 13:43:51] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 10944] -> 1D uint8
(EngineCore_DP0 pid=519450) [2026-01-26 13:43:51] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 14024704 bytes
(EngineCore_DP0 pid=519450) [rank0]:W0126 13:43:59.218000 519450 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=519450) [rank0]:W0126 13:43:59.288000 519450 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=519450) [rank0]:W0126 13:44:00.096000 519450 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=519450) [rank0]:W0126 13:44:00.195000 519450 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=519450) 2026-01-26 13:44:02,518 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=519450) 2026-01-26 13:44:02,586 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=519450) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 2/19 [00:00<00:00, 17.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▋       | 5/19 [00:00<00:00, 19.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 8/19 [00:00<00:00, 20.86it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 11/19 [00:00<00:00, 20.85it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▎  | 14/19 [00:00<00:00, 21.28it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 17/19 [00:00<00:00, 22.26it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:00<00:00, 20.00it/s]
(EngineCore_DP0 pid=519450) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  27%|██▋       | 3/11 [00:00<00:00, 21.48it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 6/11 [00:00<00:00, 22.58it/s]
Capturing CUDA graphs (decode, FULL):  82%|████████▏ | 9/11 [00:00<00:00, 22.95it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:00<00:00, 22.87it/s]

Adding requests:   0%|          | 0/8192 [00:00<?, ?it/s]
Adding requests:   0%|          | 36/8192 [00:00<00:22, 355.57it/s]
Adding requests:   1%|          | 80/8192 [00:00<00:20, 404.02it/s]
Adding requests:   2%|▏         | 124/8192 [00:00<00:19, 418.93it/s]
Adding requests:   2%|▏         | 166/8192 [00:00<00:19, 418.63it/s]
Adding requests:   3%|▎         | 211/8192 [00:00<00:18, 427.81it/s]
Adding requests:   3%|▎         | 259/8192 [00:00<00:17, 444.74it/s]
Adding requests:   4%|▎         | 304/8192 [00:00<00:17, 442.83it/s]
Adding requests:   4%|▍         | 351/8192 [00:00<00:17, 450.80it/s]
Adding requests:   5%|▍         | 399/8192 [00:00<00:17, 457.30it/s]
Adding requests:   5%|▌         | 446/8192 [00:01<00:16, 459.33it/s]
Adding requests:   6%|▌         | 492/8192 [00:01<00:17, 451.55it/s]
Adding requests:   7%|▋         | 538/8192 [00:01<00:17, 440.00it/s]
Adding requests:   7%|▋         | 588/8192 [00:01<00:16, 454.92it/s]
Adding requests:   8%|▊         | 635/8192 [00:01<00:16, 458.54it/s]
Adding requests:   8%|▊         | 684/8192 [00:01<00:16, 466.65it/s]
Adding requests:   9%|▉         | 733/8192 [00:01<00:15, 472.07it/s]
Adding requests:  10%|▉         | 781/8192 [00:01<00:15, 467.19it/s]
Adding requests:  10%|█         | 828/8192 [00:01<00:16, 458.42it/s]
Adding requests:  11%|█         | 876/8192 [00:01<00:15, 462.18it/s]
Adding requests:  11%|█▏        | 925/8192 [00:02<00:15, 467.77it/s]
Adding requests:  12%|█▏        | 973/8192 [00:02<00:15, 469.55it/s]
Adding requests:  12%|█▏        | 1021/8192 [00:02<00:15, 472.46it/s]
Adding requests:  13%|█▎        | 1069/8192 [00:02<00:15, 469.90it/s]
Adding requests:  14%|█▎        | 1117/8192 [00:02<00:15, 465.12it/s]
Adding requests:  14%|█▍        | 1167/8192 [00:02<00:14, 473.20it/s]
Adding requests:  15%|█▍        | 1217/8192 [00:02<00:14, 480.28it/s]
Adding requests:  15%|█▌        | 1266/8192 [00:02<00:14, 474.88it/s]
Adding requests:  16%|█▌        | 1314/8192 [00:02<00:14, 474.41it/s]
Adding requests:  17%|█▋        | 1363/8192 [00:02<00:14, 478.32it/s]
Adding requests:  17%|█▋        | 1413/8192 [00:03<00:14, 483.19it/s]
Adding requests:  18%|█▊        | 1462/8192 [00:03<00:14, 468.16it/s]
Adding requests:  18%|█▊        | 1510/8192 [00:03<00:14, 470.91it/s]
Adding requests:  19%|█▉        | 1558/8192 [00:03<00:14, 472.18it/s]
Adding requests:  20%|█▉        | 1606/8192 [00:03<00:14, 465.27it/s]
Adding requests:  20%|██        | 1653/8192 [00:03<00:14, 466.51it/s]
Adding requests:  21%|██        | 1700/8192 [00:03<00:13, 466.87it/s]
Adding requests:  21%|██▏       | 1748/8192 [00:03<00:13, 470.54it/s]
Adding requests:  22%|██▏       | 1797/8192 [00:03<00:13, 474.78it/s]
Adding requests:  23%|██▎       | 1846/8192 [00:03<00:13, 476.77it/s]
Adding requests:  23%|██▎       | 1894/8192 [00:04<00:13, 475.37it/s]
Adding requests:  24%|██▎       | 1942/8192 [00:04<00:13, 476.20it/s]
Adding requests:  24%|██▍       | 1990/8192 [00:04<00:13, 475.97it/s]
Adding requests:  25%|██▍       | 2039/8192 [00:04<00:12, 479.66it/s]
Adding requests:  26%|██▌       | 2089/8192 [00:04<00:12, 482.70it/s]
Adding requests:  26%|██▌       | 2138/8192 [00:04<00:12, 478.14it/s]
Adding requests:  27%|██▋       | 2186/8192 [00:04<00:12, 472.31it/s]
Adding requests:  27%|██▋       | 2236/8192 [00:04<00:12, 478.35it/s]
Adding requests:  28%|██▊       | 2284/8192 [00:04<00:12, 477.01it/s]
Adding requests:  28%|██▊       | 2333/8192 [00:05<00:12, 478.05it/s]
Adding requests:  29%|██▉       | 2381/8192 [00:05<00:12, 477.73it/s]
Adding requests:  30%|██▉       | 2430/8192 [00:05<00:11, 480.80it/s]
Adding requests:  30%|███       | 2479/8192 [00:05<00:11, 477.65it/s]
Adding requests:  31%|███       | 2527/8192 [00:05<00:12, 471.01it/s]
Adding requests:  31%|███▏      | 2575/8192 [00:05<00:11, 471.46it/s]
Adding requests:  32%|███▏      | 2623/8192 [00:05<00:11, 469.72it/s]
Adding requests:  33%|███▎      | 2670/8192 [00:05<00:11, 460.62it/s]
Adding requests:  33%|███▎      | 2717/8192 [00:05<00:11, 457.98it/s]
Adding requests:  34%|███▍      | 2765/8192 [00:05<00:11, 462.28it/s]
Adding requests:  34%|███▍      | 2812/8192 [00:06<00:11, 458.33it/s]
Adding requests:  35%|███▍      | 2860/8192 [00:06<00:11, 461.84it/s]
Adding requests:  35%|███▌      | 2907/8192 [00:06<00:11, 461.47it/s]
Adding requests:  36%|███▌      | 2954/8192 [00:06<00:11, 459.37it/s]
Adding requests:  37%|███▋      | 3001/8192 [00:06<00:11, 461.69it/s]
Adding requests:  37%|███▋      | 3049/8192 [00:06<00:11, 465.15it/s]
Adding requests:  38%|███▊      | 3096/8192 [00:06<00:11, 450.37it/s]
Adding requests:  38%|███▊      | 3143/8192 [00:06<00:11, 455.41it/s]
Adding requests:  39%|███▉      | 3191/8192 [00:06<00:10, 462.24it/s]
Adding requests:  40%|███▉      | 3240/8192 [00:06<00:10, 468.34it/s]
Adding requests:  40%|████      | 3288/8192 [00:07<00:10, 470.12it/s]
Adding requests:  41%|████      | 3336/8192 [00:07<00:10, 472.69it/s]
Adding requests:  41%|████▏     | 3386/8192 [00:07<00:10, 477.87it/s]
Adding requests:  42%|████▏     | 3435/8192 [00:07<00:09, 481.33it/s]
Adding requests:  43%|████▎     | 3484/8192 [00:07<00:10, 470.05it/s]
Adding requests:  43%|████▎     | 3532/8192 [00:07<00:09, 472.35it/s]
Adding requests:  44%|████▎     | 3580/8192 [00:07<00:09, 473.40it/s]
Adding requests:  44%|████▍     | 3628/8192 [00:07<00:09, 472.71it/s]
Adding requests:  45%|████▍     | 3676/8192 [00:07<00:09, 471.99it/s]
Adding requests:  45%|████▌     | 3724/8192 [00:07<00:09, 468.70it/s]
Adding requests:  46%|████▌     | 3774/8192 [00:08<00:09, 476.03it/s]
Adding requests:  47%|████▋     | 3823/8192 [00:08<00:09, 479.14it/s]
Adding requests:  47%|████▋     | 3871/8192 [00:08<00:09, 471.58it/s]
Adding requests:  48%|████▊     | 3920/8192 [00:08<00:09, 474.46it/s]
Adding requests:  48%|████▊     | 3968/8192 [00:08<00:08, 475.31it/s]
Adding requests:  49%|████▉     | 4016/8192 [00:08<00:08, 476.19it/s]
Adding requests:  50%|████▉     | 4064/8192 [00:08<00:08, 470.88it/s]
Adding requests:  50%|█████     | 4113/8192 [00:08<00:08, 475.24it/s]
Adding requests:  51%|█████     | 4162/8192 [00:08<00:08, 477.70it/s]
Adding requests:  51%|█████▏    | 4211/8192 [00:09<00:08, 481.15it/s]
Adding requests:  52%|█████▏    | 4260/8192 [00:09<00:08, 480.57it/s]
Adding requests:  53%|█████▎    | 4309/8192 [00:09<00:08, 482.06it/s]
Adding requests:  53%|█████▎    | 4359/8192 [00:09<00:07, 485.73it/s]
Adding requests:  54%|█████▍    | 4410/8192 [00:09<00:07, 491.27it/s]
Adding requests:  54%|█████▍    | 4460/8192 [00:09<00:07, 472.12it/s]
Adding requests:  55%|█████▌    | 4508/8192 [00:09<00:07, 471.55it/s]
Adding requests:  56%|█████▌    | 4556/8192 [00:09<00:07, 471.38it/s]
Adding requests:  56%|█████▌    | 4606/8192 [00:09<00:07, 479.42it/s]
Adding requests:  57%|█████▋    | 4655/8192 [00:09<00:07, 479.04it/s]
Adding requests:  57%|█████▋    | 4703/8192 [00:10<00:07, 477.60it/s]
Adding requests:  58%|█████▊    | 4752/8192 [00:10<00:07, 479.80it/s]
Adding requests:  59%|█████▊    | 4801/8192 [00:10<00:07, 480.47it/s]
Adding requests:  59%|█████▉    | 4850/8192 [00:10<00:06, 479.49it/s]
Adding requests:  60%|█████▉    | 4898/8192 [00:10<00:06, 475.48it/s]
Adding requests:  60%|██████    | 4948/8192 [00:10<00:06, 480.34it/s]
Adding requests:  61%|██████    | 4997/8192 [00:10<00:06, 480.09it/s]
Adding requests:  62%|██████▏   | 5046/8192 [00:10<00:06, 465.92it/s]
Adding requests:  62%|██████▏   | 5096/8192 [00:10<00:06, 475.72it/s]
Adding requests:  63%|██████▎   | 5145/8192 [00:10<00:06, 477.32it/s]
Adding requests:  63%|██████▎   | 5195/8192 [00:11<00:06, 481.35it/s]
Adding requests:  64%|██████▍   | 5244/8192 [00:11<00:06, 479.71it/s]
Adding requests:  65%|██████▍   | 5293/8192 [00:11<00:06, 478.94it/s]
Adding requests:  65%|██████▌   | 5342/8192 [00:11<00:05, 482.13it/s]
Adding requests:  66%|██████▌   | 5391/8192 [00:11<00:05, 484.16it/s]
Adding requests:  66%|██████▋   | 5440/8192 [00:11<00:05, 481.28it/s]
Adding requests:  67%|██████▋   | 5489/8192 [00:11<00:05, 478.88it/s]
Adding requests:  68%|██████▊   | 5538/8192 [00:11<00:05, 479.78it/s]
Adding requests:  68%|██████▊   | 5587/8192 [00:11<00:05, 480.80it/s]
Adding requests:  69%|██████▉   | 5636/8192 [00:11<00:05, 478.50it/s]
Adding requests:  69%|██████▉   | 5684/8192 [00:12<00:05, 476.65it/s]
Adding requests:  70%|██████▉   | 5733/8192 [00:12<00:05, 478.88it/s]
Adding requests:  71%|███████   | 5782/8192 [00:12<00:05, 480.15it/s]
Adding requests:  71%|███████   | 5831/8192 [00:12<00:05, 470.92it/s]
Adding requests:  72%|███████▏  | 5880/8192 [00:12<00:04, 474.08it/s]
Adding requests:  72%|███████▏  | 5928/8192 [00:12<00:04, 475.31it/s]
Adding requests:  73%|███████▎  | 5976/8192 [00:12<00:04, 475.00it/s]
Adding requests:  74%|███████▎  | 6025/8192 [00:12<00:04, 477.67it/s]
Adding requests:  74%|███████▍  | 6074/8192 [00:12<00:04, 479.94it/s]
Adding requests:  75%|███████▍  | 6123/8192 [00:13<00:04, 479.73it/s]
Adding requests:  75%|███████▌  | 6171/8192 [00:13<00:04, 479.04it/s]
Adding requests:  76%|███████▌  | 6219/8192 [00:13<00:04, 474.17it/s]
Adding requests:  77%|███████▋  | 6269/8192 [00:13<00:03, 481.46it/s]
Adding requests:  77%|███████▋  | 6319/8192 [00:13<00:03, 485.30it/s]
Adding requests:  78%|███████▊  | 6369/8192 [00:13<00:03, 487.83it/s]
Adding requests:  78%|███████▊  | 6418/8192 [00:13<00:03, 487.45it/s]
Adding requests:  79%|███████▉  | 6469/8192 [00:13<00:03, 491.29it/s]
Adding requests:  80%|███████▉  | 6521/8192 [00:13<00:03, 496.82it/s]
Adding requests:  80%|████████  | 6571/8192 [00:13<00:03, 496.05it/s]
Adding requests:  81%|████████  | 6621/8192 [00:14<00:03, 489.98it/s]
Adding requests:  81%|████████▏ | 6671/8192 [00:14<00:03, 488.60it/s]
Adding requests:  82%|████████▏ | 6721/8192 [00:14<00:03, 489.11it/s]
Adding requests:  83%|████████▎ | 6770/8192 [00:14<00:02, 486.52it/s]
Adding requests:  83%|████████▎ | 6820/8192 [00:14<00:02, 488.09it/s]
Adding requests:  84%|████████▍ | 6871/8192 [00:14<00:02, 492.17it/s]
Adding requests:  84%|████████▍ | 6921/8192 [00:14<00:02, 491.15it/s]
Adding requests:  85%|████████▌ | 6971/8192 [00:14<00:02, 493.20it/s]
Adding requests:  86%|████████▌ | 7021/8192 [00:14<00:02, 486.75it/s]
Adding requests:  86%|████████▋ | 7070/8192 [00:14<00:02, 485.36it/s]
Adding requests:  87%|████████▋ | 7121/8192 [00:15<00:02, 490.49it/s]
Adding requests:  88%|████████▊ | 7171/8192 [00:15<00:02, 485.48it/s]
Adding requests:  88%|████████▊ | 7220/8192 [00:15<00:01, 486.49it/s]
Adding requests:  89%|████████▊ | 7270/8192 [00:15<00:01, 488.35it/s]
Adding requests:  89%|████████▉ | 7320/8192 [00:15<00:01, 488.94it/s]
Adding requests:  90%|████████▉ | 7369/8192 [00:15<00:01, 475.39it/s]
Adding requests:  91%|█████████ | 7420/8192 [00:15<00:01, 484.50it/s]
Adding requests:  91%|█████████ | 7470/8192 [00:15<00:01, 487.12it/s]
Adding requests:  92%|█████████▏| 7519/8192 [00:15<00:01, 487.46it/s]
Adding requests:  92%|█████████▏| 7568/8192 [00:15<00:01, 486.38it/s]
Adding requests:  93%|█████████▎| 7617/8192 [00:16<00:01, 482.33it/s]
Adding requests:  94%|█████████▎| 7668/8192 [00:16<00:01, 486.51it/s]
Adding requests:  94%|█████████▍| 7718/8192 [00:16<00:00, 489.11it/s]
Adding requests:  95%|█████████▍| 7767/8192 [00:16<00:00, 484.68it/s]
Adding requests:  95%|█████████▌| 7816/8192 [00:16<00:00, 482.89it/s]
Adding requests:  96%|█████████▌| 7865/8192 [00:16<00:00, 483.65it/s]
Adding requests:  97%|█████████▋| 7914/8192 [00:16<00:00, 478.87it/s]
Adding requests:  97%|█████████▋| 7962/8192 [00:16<00:00, 478.51it/s]
Adding requests:  98%|█████████▊| 8010/8192 [00:16<00:00, 477.76it/s]
Adding requests:  98%|█████████▊| 8058/8192 [00:16<00:00, 477.12it/s]
Adding requests:  99%|█████████▉| 8108/8192 [00:17<00:00, 482.83it/s]
Adding requests: 100%|█████████▉| 8157/8192 [00:17<00:00, 482.42it/s]
Adding requests: 100%|██████████| 8192/8192 [00:17<00:00, 474.55it/s]

Processed prompts:   0%|          | 0/8192 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  66%|██████▌   | 5407/8192 [00:00<00:00, 54069.54it/s, est. speed input: 55370901.15 toks/s, output: 54070.31 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [00:00<00:00, 54069.54it/s, est. speed input: 56885512.47 toks/s, output: 55550.46 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [00:00<00:00, 55509.00it/s, est. speed input: 56885512.47 toks/s, output: 55550.46 toks/s]
[rank0]:[W126 13:44:23.675239501 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 85.7s

测试结果:
  Requests/s:   470.45
  Tokens/s:     482206.74
  Total Reqs:   8192
  Elapsed:      17.41s

  [Prefill 分析]
  Total Prefill Tokens: 8388608
  Prefill Tokens/s:     481736.29


------------------------------------------------------------
  生成 CSV: Llama3.2-1B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_6/Llama3.2-1B-INT8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,43.3975,22262.8951,2.9495
1024,1024,1,128,128,40.7679,41787.1323,3.1397
2048,1024,2,256,128,82.9526,85026.4451,3.0861
4096,1024,4,512,128,160.7974,164817.3559,3.1841
8192,1024,8,1024,128,300.1717,307675.9619,3.4114
16384,1024,16,2048,128,461.3019,472834.4170,4.4396
32768,1024,32,4096,128,466.7345,478402.8285,8.7759
65536,1024,64,8192,128,470.4456,482206.7397,17.4133

------------------------------------------------------------

[INFO] 完成: 8 成功, 0 失败

============================================================
  Llama3.2-1B-INT8 | cuSPARSELt (2_8) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_8
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_8

============================================================
[1/8] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:44:32 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=520682) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=520682) WARNING 01-26 13:44:44 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=520682) WARNING 01-26 13:44:51 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 39.60 requests/s, 20312.70 total tokens/s, 39.60 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-26 13:44:32] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:44:32] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:44:32] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:44:32] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:44:32] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:44:32] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:44:32] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:44:32] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:44:32] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:44:32] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:44:32] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:44:32] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:44:32] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:44:32] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:44:38] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:44:39] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:44:39] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:44:39] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:44:39] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:44:39] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:44:39] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:44:39] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:44:39] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:44:39] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:44:39] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:44:39] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:44:39] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:44:39] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=520682) [2026-01-26 13:44:40] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=520682) [2026-01-26 13:44:40] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=520682) [2026-01-26 13:44:40] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=520682) [2026-01-26 13:44:40] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=520682) [2026-01-26 13:44:40] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=520682) [2026-01-26 13:44:40] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=520682) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=520682) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.71it/s]
(EngineCore_DP0 pid=520682) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.71it/s]
(EngineCore_DP0 pid=520682) 
(EngineCore_DP0 pid=520682) [2026-01-26 13:44:40] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=520682) [2026-01-26 13:44:40] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5898240 bytes
(EngineCore_DP0 pid=520682) [2026-01-26 13:44:40] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 3072] -> 1D uint8
(EngineCore_DP0 pid=520682) [2026-01-26 13:44:40] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 3932160 bytes
(EngineCore_DP0 pid=520682) [2026-01-26 13:44:40] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=520682) [2026-01-26 13:44:40] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 31457280 bytes
(EngineCore_DP0 pid=520682) [2026-01-26 13:44:40] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 12288] -> 1D uint8
(EngineCore_DP0 pid=520682) [2026-01-26 13:44:40] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15728640 bytes
(EngineCore_DP0 pid=520682) 2026-01-26 13:44:51,557 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=520682) 2026-01-26 13:44:51,578 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=520682) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  3.34it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.24it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.25it/s]
(EngineCore_DP0 pid=520682) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 21.81it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  59%|█████▊    | 75/128 [00:00<00:00, 749.15it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 760.62it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:22,  5.63it/s, est. speed input: 2884.83 toks/s, output: 5.63 toks/s]
Processed prompts:   5%|▍         | 6/128 [00:00<00:05, 23.79it/s, est. speed input: 10491.96 toks/s, output: 20.49 toks/s]
Processed prompts:   9%|▊         | 11/128 [00:00<00:03, 31.98it/s, est. speed input: 13863.59 toks/s, output: 27.08 toks/s]
Processed prompts:  12%|█▎        | 16/128 [00:00<00:03, 36.44it/s, est. speed input: 15772.66 toks/s, output: 30.81 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:02, 39.08it/s, est. speed input: 16995.64 toks/s, output: 33.19 toks/s]
Processed prompts:  20%|██        | 26/128 [00:00<00:02, 40.72it/s, est. speed input: 17840.58 toks/s, output: 34.84 toks/s]
Processed prompts:  24%|██▍       | 31/128 [00:00<00:02, 41.76it/s, est. speed input: 18458.27 toks/s, output: 36.05 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:00<00:02, 42.47it/s, est. speed input: 18934.15 toks/s, output: 36.98 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:01<00:02, 42.99it/s, est. speed input: 19316.52 toks/s, output: 37.73 toks/s]
Processed prompts:  36%|███▌      | 46/128 [00:01<00:01, 43.35it/s, est. speed input: 19628.60 toks/s, output: 38.34 toks/s]
Processed prompts:  40%|███▉      | 51/128 [00:01<00:01, 43.66it/s, est. speed input: 19893.97 toks/s, output: 38.86 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:01<00:01, 43.89it/s, est. speed input: 20118.25 toks/s, output: 39.29 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:01<00:01, 44.01it/s, est. speed input: 20306.41 toks/s, output: 39.66 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:01<00:01, 44.19it/s, est. speed input: 20478.27 toks/s, output: 40.00 toks/s]
Processed prompts:  55%|█████▌    | 71/128 [00:01<00:01, 43.53it/s, est. speed input: 20550.36 toks/s, output: 40.14 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:01<00:01, 43.74it/s, est. speed input: 20675.72 toks/s, output: 40.38 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:01<00:01, 43.91it/s, est. speed input: 20789.44 toks/s, output: 40.60 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:02<00:00, 43.99it/s, est. speed input: 20887.40 toks/s, output: 40.80 toks/s]
Processed prompts:  71%|███████   | 91/128 [00:02<00:00, 43.61it/s, est. speed input: 20940.60 toks/s, output: 40.90 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:02<00:00, 43.72it/s, est. speed input: 21017.37 toks/s, output: 41.05 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:02<00:00, 43.82it/s, est. speed input: 21088.69 toks/s, output: 41.19 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:02<00:00, 43.93it/s, est. speed input: 21156.30 toks/s, output: 41.32 toks/s]
Processed prompts:  87%|████████▋ | 111/128 [00:02<00:00, 44.01it/s, est. speed input: 21218.45 toks/s, output: 41.44 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:02<00:00, 44.05it/s, est. speed input: 21274.39 toks/s, output: 41.55 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:02<00:00, 44.14it/s, est. speed input: 21330.05 toks/s, output: 41.66 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:03<00:00, 44.08it/s, est. speed input: 21374.38 toks/s, output: 41.75 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 44.08it/s, est. speed input: 21394.34 toks/s, output: 41.79 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 41.78it/s, est. speed input: 21394.34 toks/s, output: 41.79 toks/s]
[rank0]:[W126 13:44:57.737629466 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 34.2s

测试结果:
  Requests/s:   39.60
  Tokens/s:     20312.70
  Total Reqs:   128
  Elapsed:      3.23s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     20273.10

============================================================
[2/8] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:45:06 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=521782) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=521782) WARNING 01-26 13:45:19 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=521782) WARNING 01-26 13:45:26 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 37.49 requests/s, 38426.89 total tokens/s, 37.49 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-26 13:45:06] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:45:06] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:45:06] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:45:06] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:45:06] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:45:06] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:45:06] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:45:06] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:45:06] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:45:06] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:45:06] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:45:06] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:45:06] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:45:06] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:45:13] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:45:13] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:45:13] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:45:13] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:45:13] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:45:13] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:45:13] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:45:13] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:45:13] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:45:13] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:45:13] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:45:13] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:45:13] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:45:13] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=521782) [2026-01-26 13:45:14] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=521782) [2026-01-26 13:45:14] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=521782) [2026-01-26 13:45:14] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=521782) [2026-01-26 13:45:14] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=521782) [2026-01-26 13:45:14] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=521782) [2026-01-26 13:45:14] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=521782) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=521782) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.47it/s]
(EngineCore_DP0 pid=521782) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.47it/s]
(EngineCore_DP0 pid=521782) 
(EngineCore_DP0 pid=521782) [2026-01-26 13:45:15] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=521782) [2026-01-26 13:45:15] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5898240 bytes
(EngineCore_DP0 pid=521782) [2026-01-26 13:45:15] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 3072] -> 1D uint8
(EngineCore_DP0 pid=521782) [2026-01-26 13:45:15] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 3932160 bytes
(EngineCore_DP0 pid=521782) [2026-01-26 13:45:15] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=521782) [2026-01-26 13:45:15] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 31457280 bytes
(EngineCore_DP0 pid=521782) [2026-01-26 13:45:15] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 12288] -> 1D uint8
(EngineCore_DP0 pid=521782) [2026-01-26 13:45:15] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15728640 bytes
(EngineCore_DP0 pid=521782) 2026-01-26 13:45:25,998 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=521782) 2026-01-26 13:45:26,019 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=521782) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 21.40it/s]
(EngineCore_DP0 pid=521782) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 21.00it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  32%|███▏      | 41/128 [00:00<00:00, 402.60it/s]
Adding requests:  68%|██████▊   | 87/128 [00:00<00:00, 432.14it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 406.79it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   8%|▊         | 10/128 [00:00<00:01, 79.99it/s, est. speed input: 81914.69 toks/s, output: 79.99 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:00<00:02, 52.12it/s, est. speed input: 56662.59 toks/s, output: 55.33 toks/s]
Processed prompts:  19%|█▉        | 24/128 [00:00<00:02, 46.83it/s, est. speed input: 51611.17 toks/s, output: 50.40 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:00<00:02, 44.54it/s, est. speed input: 49402.02 toks/s, output: 48.24 toks/s]
Processed prompts:  27%|██▋       | 34/128 [00:00<00:02, 42.94it/s, est. speed input: 47871.45 toks/s, output: 46.75 toks/s]
Processed prompts:  30%|███       | 39/128 [00:00<00:02, 41.99it/s, est. speed input: 46849.21 toks/s, output: 45.75 toks/s]
Processed prompts:  34%|███▍      | 44/128 [00:00<00:02, 41.28it/s, est. speed input: 46053.68 toks/s, output: 44.97 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:01<00:01, 40.79it/s, est. speed input: 45436.71 toks/s, output: 44.37 toks/s]
Processed prompts:  42%|████▏     | 54/128 [00:01<00:01, 40.54it/s, est. speed input: 44976.58 toks/s, output: 43.92 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:01<00:01, 40.20it/s, est. speed input: 44546.26 toks/s, output: 43.50 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:01<00:01, 39.96it/s, est. speed input: 44189.62 toks/s, output: 43.15 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:01<00:01, 39.76it/s, est. speed input: 43927.45 toks/s, output: 42.90 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:01<00:01, 39.68it/s, est. speed input: 43715.40 toks/s, output: 42.69 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:01<00:01, 39.39it/s, est. speed input: 43476.22 toks/s, output: 42.46 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:01<00:01, 39.38it/s, est. speed input: 43305.08 toks/s, output: 42.29 toks/s]
Processed prompts:  66%|██████▌   | 84/128 [00:01<00:01, 39.43it/s, est. speed input: 43162.98 toks/s, output: 42.15 toks/s]
Processed prompts:  69%|██████▉   | 88/128 [00:02<00:01, 39.22it/s, est. speed input: 42990.09 toks/s, output: 41.98 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:02<00:00, 39.43it/s, est. speed input: 42895.63 toks/s, output: 41.89 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:02<00:00, 39.64it/s, est. speed input: 42794.04 toks/s, output: 41.79 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:02<00:00, 39.71it/s, est. speed input: 42712.67 toks/s, output: 41.71 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:02<00:00, 39.83it/s, est. speed input: 42631.03 toks/s, output: 41.63 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:02<00:00, 39.82it/s, est. speed input: 42558.67 toks/s, output: 41.56 toks/s]
Processed prompts:  89%|████████▉ | 114/128 [00:02<00:00, 39.84it/s, est. speed input: 42497.16 toks/s, output: 41.50 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:02<00:00, 39.83it/s, est. speed input: 42435.59 toks/s, output: 41.44 toks/s]
Processed prompts:  96%|█████████▌| 123/128 [00:02<00:00, 39.91it/s, est. speed input: 42376.77 toks/s, output: 41.38 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:03<00:00, 39.87it/s, est. speed input: 42322.35 toks/s, output: 41.33 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 39.87it/s, est. speed input: 42307.59 toks/s, output: 41.32 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 41.31it/s, est. speed input: 42307.59 toks/s, output: 41.32 toks/s]
[rank0]:[W126 13:45:31.396389062 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 33.4s

测试结果:
  Requests/s:   37.49
  Tokens/s:     38426.89
  Total Reqs:   128
  Elapsed:      3.41s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     38389.40

============================================================
[3/8] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:45:41 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=522833) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=522833) WARNING 01-26 13:45:53 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=522833) WARNING 01-26 13:46:00 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 75.18 requests/s, 77057.03 total tokens/s, 75.18 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-26 13:45:40] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:45:40] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:45:40] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:45:40] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:45:40] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:45:40] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:45:40] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:45:40] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:45:40] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:45:41] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:45:41] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:45:41] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:45:41] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:45:41] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:45:47] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:45:47] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:45:47] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:45:47] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:45:47] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:45:47] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:45:47] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:45:47] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:45:47] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:45:47] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:45:47] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:45:47] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:45:47] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:45:47] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=522833) [2026-01-26 13:45:48] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=522833) [2026-01-26 13:45:48] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=522833) [2026-01-26 13:45:48] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=522833) [2026-01-26 13:45:48] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=522833) [2026-01-26 13:45:48] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=522833) [2026-01-26 13:45:48] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=522833) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=522833) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.75it/s]
(EngineCore_DP0 pid=522833) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.75it/s]
(EngineCore_DP0 pid=522833) 
(EngineCore_DP0 pid=522833) [2026-01-26 13:45:49] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=522833) [2026-01-26 13:45:49] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5898240 bytes
(EngineCore_DP0 pid=522833) [2026-01-26 13:45:49] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 3072] -> 1D uint8
(EngineCore_DP0 pid=522833) [2026-01-26 13:45:49] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 3932160 bytes
(EngineCore_DP0 pid=522833) [2026-01-26 13:45:49] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=522833) [2026-01-26 13:45:49] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 31457280 bytes
(EngineCore_DP0 pid=522833) [2026-01-26 13:45:49] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 12288] -> 1D uint8
(EngineCore_DP0 pid=522833) [2026-01-26 13:45:49] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15728640 bytes
(EngineCore_DP0 pid=522833) 2026-01-26 13:46:00,154 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=522833) 2026-01-26 13:46:00,176 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=522833) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:00<00:00, 13.04it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 12.79it/s]
(EngineCore_DP0 pid=522833) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 18.15it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 18.13it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   0%|          | 1/256 [00:00<00:56,  4.53it/s]
Adding requests:  16%|█▌        | 40/256 [00:00<00:01, 155.99it/s]
Adding requests:  32%|███▏      | 83/256 [00:00<00:00, 254.08it/s]
Adding requests:  50%|████▉     | 127/256 [00:00<00:00, 315.93it/s]
Adding requests:  66%|██████▋   | 170/256 [00:00<00:00, 351.21it/s]
Adding requests:  84%|████████▎ | 214/256 [00:00<00:00, 377.31it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 313.57it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  20%|█▉        | 50/256 [00:00<00:00, 461.38it/s, est. speed input: 472532.12 toks/s, output: 461.40 toks/s]
Processed prompts:  38%|███▊      | 97/256 [00:00<00:01, 129.92it/s, est. speed input: 149666.83 toks/s, output: 146.16 toks/s]
Processed prompts:  47%|████▋     | 121/256 [00:00<00:01, 111.33it/s, est. speed input: 130009.80 toks/s, output: 126.96 toks/s]
Processed prompts:  54%|█████▍    | 138/256 [00:01<00:01, 101.48it/s, est. speed input: 120753.86 toks/s, output: 117.92 toks/s]
Processed prompts:  59%|█████▉    | 152/256 [00:01<00:01, 96.94it/s, est. speed input: 116280.09 toks/s, output: 113.55 toks/s] 
Processed prompts:  64%|██████▍   | 164/256 [00:01<00:00, 93.85it/s, est. speed input: 113315.14 toks/s, output: 110.66 toks/s]
Processed prompts:  68%|██████▊   | 175/256 [00:01<00:00, 93.33it/s, est. speed input: 111838.75 toks/s, output: 109.22 toks/s]
Processed prompts:  73%|███████▎  | 186/256 [00:01<00:00, 88.70it/s, est. speed input: 109050.24 toks/s, output: 106.49 toks/s]
Processed prompts:  77%|███████▋  | 196/256 [00:01<00:00, 87.38it/s, est. speed input: 107524.32 toks/s, output: 105.00 toks/s]
Processed prompts:  80%|████████  | 206/256 [00:01<00:00, 86.25it/s, est. speed input: 106158.70 toks/s, output: 103.67 toks/s]
Processed prompts:  84%|████████▍ | 216/256 [00:02<00:00, 85.34it/s, est. speed input: 104941.27 toks/s, output: 102.48 toks/s]
Processed prompts:  88%|████████▊ | 226/256 [00:02<00:00, 84.73it/s, est. speed input: 103872.05 toks/s, output: 101.44 toks/s]
Processed prompts:  92%|█████████▏| 236/256 [00:02<00:00, 84.28it/s, est. speed input: 102913.15 toks/s, output: 100.50 toks/s]
Processed prompts:  96%|█████████▌| 246/256 [00:02<00:00, 84.06it/s, est. speed input: 102069.66 toks/s, output: 99.68 toks/s] 
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 83.90it/s, est. speed input: 101302.70 toks/s, output: 98.93 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 83.90it/s, est. speed input: 101302.70 toks/s, output: 98.93 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 98.92it/s, est. speed input: 101302.70 toks/s, output: 98.93 toks/s]
[rank0]:[W126 13:46:05.851895891 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 34.4s

测试结果:
  Requests/s:   75.18
  Tokens/s:     77057.03
  Total Reqs:   256
  Elapsed:      3.41s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     76981.85

============================================================
[4/8] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:46:16 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=523880) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=523880) WARNING 01-26 13:46:28 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=523880) WARNING 01-26 13:46:35 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 159.33 requests/s, 163312.06 total tokens/s, 159.33 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-26 13:46:16] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:46:16] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:46:16] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:46:16] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:46:16] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:46:16] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:46:16] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:46:16] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:46:16] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:46:16] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:46:16] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:46:16] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:46:16] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:46:16] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:46:22] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:46:23] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:46:23] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:46:23] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:46:23] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:46:23] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:46:23] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:46:23] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:46:23] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:46:23] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:46:23] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:46:23] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:46:23] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:46:23] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=523880) [2026-01-26 13:46:24] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=523880) [2026-01-26 13:46:24] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=523880) [2026-01-26 13:46:24] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=523880) [2026-01-26 13:46:24] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=523880) [2026-01-26 13:46:24] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=523880) [2026-01-26 13:46:24] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=523880) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=523880) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.15it/s]
(EngineCore_DP0 pid=523880) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.14it/s]
(EngineCore_DP0 pid=523880) 
(EngineCore_DP0 pid=523880) [2026-01-26 13:46:24] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=523880) [2026-01-26 13:46:24] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5898240 bytes
(EngineCore_DP0 pid=523880) [2026-01-26 13:46:24] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 3072] -> 1D uint8
(EngineCore_DP0 pid=523880) [2026-01-26 13:46:24] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 3932160 bytes
(EngineCore_DP0 pid=523880) [2026-01-26 13:46:24] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=523880) [2026-01-26 13:46:24] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 31457280 bytes
(EngineCore_DP0 pid=523880) [2026-01-26 13:46:24] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 12288] -> 1D uint8
(EngineCore_DP0 pid=523880) [2026-01-26 13:46:24] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15728640 bytes
(EngineCore_DP0 pid=523880) 2026-01-26 13:46:35,722 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=523880) 2026-01-26 13:46:35,743 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=523880) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00, 19.27it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 18.74it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 18.80it/s]
(EngineCore_DP0 pid=523880) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 23.42it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 23.40it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   7%|▋         | 34/512 [00:00<00:01, 334.42it/s]
Adding requests:  15%|█▌        | 78/512 [00:00<00:01, 395.01it/s]
Adding requests:  24%|██▍       | 123/512 [00:00<00:00, 418.48it/s]
Adding requests:  33%|███▎      | 167/512 [00:00<00:00, 425.51it/s]
Adding requests:  41%|████▏     | 212/512 [00:00<00:00, 433.67it/s]
Adding requests:  51%|█████     | 260/512 [00:00<00:00, 447.11it/s]
Adding requests:  60%|█████▉    | 305/512 [00:00<00:00, 445.64it/s]
Adding requests:  69%|██████▉   | 352/512 [00:00<00:00, 451.51it/s]
Adding requests:  78%|███████▊  | 400/512 [00:00<00:00, 457.65it/s]
Adding requests:  87%|████████▋ | 447/512 [00:01<00:00, 458.66it/s]
Adding requests:  96%|█████████▋| 494/512 [00:01<00:00, 460.57it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 443.93it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  36%|███▌      | 182/512 [00:00<00:00, 1755.94it/s, est. speed input: 1798214.97 toks/s, output: 1755.97 toks/s]
Processed prompts:  70%|██████▉   | 358/512 [00:01<00:00, 273.75it/s, est. speed input: 321745.24 toks/s, output: 314.20 toks/s]   
Processed prompts:  86%|████████▌ | 441/512 [00:01<00:00, 237.93it/s, est. speed input: 281350.42 toks/s, output: 274.76 toks/s]
Processed prompts:  96%|█████████▋| 494/512 [00:01<00:00, 212.97it/s, est. speed input: 258651.78 toks/s, output: 252.59 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:02<00:00, 212.97it/s, est. speed input: 254638.24 toks/s, output: 248.67 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:02<00:00, 248.66it/s, est. speed input: 254638.24 toks/s, output: 248.67 toks/s]
[rank0]:[W126 13:46:41.554749497 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 35.5s

测试结果:
  Requests/s:   159.33
  Tokens/s:     163312.06
  Total Reqs:   512
  Elapsed:      3.21s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     163152.74

============================================================
[5/8] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:46:54 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=524991) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=524991) WARNING 01-26 13:47:07 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=524991) WARNING 01-26 13:47:13 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 302.44 requests/s, 309996.49 total tokens/s, 302.44 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-26 13:46:54] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:46:54] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:46:54] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:46:54] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:46:54] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:46:54] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:46:54] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:46:54] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:46:54] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:46:54] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:46:54] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:46:54] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:46:54] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:46:54] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:47:00] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:47:01] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:47:01] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:47:01] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:47:01] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:47:01] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:47:01] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:47:01] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:47:01] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:47:01] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:47:01] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:47:01] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:47:01] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:47:01] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=524991) [2026-01-26 13:47:02] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=524991) [2026-01-26 13:47:02] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=524991) [2026-01-26 13:47:02] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=524991) [2026-01-26 13:47:02] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=524991) [2026-01-26 13:47:02] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=524991) [2026-01-26 13:47:02] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=524991) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=524991) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.59it/s]
(EngineCore_DP0 pid=524991) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.59it/s]
(EngineCore_DP0 pid=524991) 
(EngineCore_DP0 pid=524991) [2026-01-26 13:47:02] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=524991) [2026-01-26 13:47:03] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5898240 bytes
(EngineCore_DP0 pid=524991) [2026-01-26 13:47:03] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 3072] -> 1D uint8
(EngineCore_DP0 pid=524991) [2026-01-26 13:47:03] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 3932160 bytes
(EngineCore_DP0 pid=524991) [2026-01-26 13:47:03] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=524991) [2026-01-26 13:47:03] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 31457280 bytes
(EngineCore_DP0 pid=524991) [2026-01-26 13:47:03] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 12288] -> 1D uint8
(EngineCore_DP0 pid=524991) [2026-01-26 13:47:03] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15728640 bytes
(EngineCore_DP0 pid=524991) 2026-01-26 13:47:13,940 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=524991) 2026-01-26 13:47:13,962 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=524991) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  60%|██████    | 3/5 [00:00<00:00, 22.85it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 20.59it/s]
(EngineCore_DP0 pid=524991) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  75%|███████▌  | 3/4 [00:00<00:00, 23.20it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 23.13it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   4%|▍         | 40/1024 [00:00<00:02, 396.67it/s]
Adding requests:   8%|▊         | 85/1024 [00:00<00:02, 424.06it/s]
Adding requests:  13%|█▎        | 130/1024 [00:00<00:02, 432.72it/s]
Adding requests:  17%|█▋        | 174/1024 [00:00<00:01, 434.56it/s]
Adding requests:  21%|██▏       | 220/1024 [00:00<00:01, 440.77it/s]
Adding requests:  26%|██▌       | 268/1024 [00:00<00:01, 452.34it/s]
Adding requests:  31%|███       | 314/1024 [00:00<00:01, 449.90it/s]
Adding requests:  35%|███▌      | 360/1024 [00:00<00:01, 452.26it/s]
Adding requests:  40%|███▉      | 407/1024 [00:00<00:01, 454.79it/s]
Adding requests:  44%|████▍     | 454/1024 [00:01<00:01, 458.18it/s]
Adding requests:  49%|████▉     | 500/1024 [00:01<00:01, 457.17it/s]
Adding requests:  53%|█████▎    | 546/1024 [00:01<00:01, 453.16it/s]
Adding requests:  58%|█████▊    | 595/1024 [00:01<00:00, 462.18it/s]
Adding requests:  63%|██████▎   | 643/1024 [00:01<00:00, 465.02it/s]
Adding requests:  68%|██████▊   | 692/1024 [00:01<00:00, 471.20it/s]
Adding requests:  72%|███████▏  | 740/1024 [00:01<00:00, 471.40it/s]
Adding requests:  77%|███████▋  | 788/1024 [00:01<00:00, 469.31it/s]
Adding requests:  82%|████████▏ | 835/1024 [00:01<00:00, 457.98it/s]
Adding requests:  86%|████████▋ | 884/1024 [00:01<00:00, 465.02it/s]
Adding requests:  91%|█████████ | 933/1024 [00:02<00:00, 470.62it/s]
Adding requests:  96%|█████████▌| 981/1024 [00:02<00:00, 471.14it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 459.06it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  65%|██████▌   | 666/1024 [00:00<00:00, 6058.53it/s, est. speed input: 6204405.78 toks/s, output: 6058.65 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:01<00:00, 6058.53it/s, est. speed input: 908717.52 toks/s, output: 887.42 toks/s] 
Processed prompts: 100%|██████████| 1024/1024 [00:01<00:00, 887.33it/s, est. speed input: 908717.52 toks/s, output: 887.42 toks/s] 
[rank0]:[W126 13:47:19.120881552 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 38.6s

测试结果:
  Requests/s:   302.44
  Tokens/s:     309996.49
  Total Reqs:   1024
  Elapsed:      3.39s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     309694.05

============================================================
[6/8] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:47:37 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=526137) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=526137) WARNING 01-26 13:47:50 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=526137) WARNING 01-26 13:47:57 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 459.87 requests/s, 471368.34 total tokens/s, 459.87 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-26 13:47:37] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:47:37] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:47:37] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:47:37] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:47:37] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:47:37] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:47:37] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:47:37] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:47:37] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:47:37] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:47:37] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:47:37] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:47:37] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:47:37] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:47:44] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:47:44] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:47:44] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:47:44] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:47:44] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:47:44] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:47:44] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:47:44] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:47:44] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:47:44] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:47:44] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:47:44] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:47:44] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:47:44] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=526137) [2026-01-26 13:47:45] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=526137) [2026-01-26 13:47:45] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=526137) [2026-01-26 13:47:45] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=526137) [2026-01-26 13:47:45] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=526137) [2026-01-26 13:47:45] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=526137) [2026-01-26 13:47:45] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=526137) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=526137) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.71it/s]
(EngineCore_DP0 pid=526137) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.71it/s]
(EngineCore_DP0 pid=526137) 
(EngineCore_DP0 pid=526137) [2026-01-26 13:47:46] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=526137) [2026-01-26 13:47:46] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5898240 bytes
(EngineCore_DP0 pid=526137) [2026-01-26 13:47:46] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 3072] -> 1D uint8
(EngineCore_DP0 pid=526137) [2026-01-26 13:47:46] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 3932160 bytes
(EngineCore_DP0 pid=526137) [2026-01-26 13:47:46] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=526137) [2026-01-26 13:47:46] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 31457280 bytes
(EngineCore_DP0 pid=526137) [2026-01-26 13:47:46] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 12288] -> 1D uint8
(EngineCore_DP0 pid=526137) [2026-01-26 13:47:46] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15728640 bytes
(EngineCore_DP0 pid=526137) 2026-01-26 13:47:56,970 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=526137) 2026-01-26 13:47:56,993 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=526137) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00, 19.87it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 5/7 [00:00<00:00, 22.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 20.81it/s]
(EngineCore_DP0 pid=526137) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00, 19.64it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 19.47it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 19.48it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 40/2048 [00:00<00:05, 397.59it/s]
Adding requests:   4%|▍         | 85/2048 [00:00<00:04, 423.05it/s]
Adding requests:   6%|▋         | 129/2048 [00:00<00:04, 429.75it/s]
Adding requests:   8%|▊         | 173/2048 [00:00<00:04, 431.90it/s]
Adding requests:  11%|█         | 219/2048 [00:00<00:04, 439.12it/s]
Adding requests:  13%|█▎        | 267/2048 [00:00<00:03, 449.89it/s]
Adding requests:  15%|█▌        | 312/2048 [00:00<00:03, 448.89it/s]
Adding requests:  17%|█▋        | 358/2048 [00:00<00:03, 451.66it/s]
Adding requests:  20%|█▉        | 405/2048 [00:00<00:03, 457.04it/s]
Adding requests:  22%|██▏       | 452/2048 [00:01<00:03, 460.89it/s]
Adding requests:  24%|██▍       | 499/2048 [00:01<00:03, 461.50it/s]
Adding requests:  27%|██▋       | 546/2048 [00:01<00:03, 455.33it/s]
Adding requests:  29%|██▉       | 594/2048 [00:01<00:03, 460.77it/s]
Adding requests:  31%|███▏      | 642/2048 [00:01<00:03, 464.87it/s]
Adding requests:  34%|███▎      | 689/2048 [00:01<00:02, 462.80it/s]
Adding requests:  36%|███▌      | 736/2048 [00:01<00:02, 462.40it/s]
Adding requests:  38%|███▊      | 783/2048 [00:01<00:02, 452.82it/s]
Adding requests:  40%|████      | 829/2048 [00:01<00:02, 441.01it/s]
Adding requests:  43%|████▎     | 874/2048 [00:01<00:02, 441.64it/s]
Adding requests:  45%|████▍     | 920/2048 [00:02<00:02, 444.82it/s]
Adding requests:  47%|████▋     | 966/2048 [00:02<00:02, 449.19it/s]
Adding requests:  50%|████▉     | 1014/2048 [00:02<00:02, 457.04it/s]
Adding requests:  52%|█████▏    | 1061/2048 [00:02<00:02, 459.29it/s]
Adding requests:  54%|█████▍    | 1107/2048 [00:02<00:02, 459.01it/s]
Adding requests:  56%|█████▋    | 1155/2048 [00:02<00:01, 463.82it/s]
Adding requests:  59%|█████▉    | 1206/2048 [00:02<00:01, 475.11it/s]
Adding requests:  61%|██████    | 1254/2048 [00:02<00:01, 472.33it/s]
Adding requests:  64%|██████▎   | 1302/2048 [00:02<00:01, 467.53it/s]
Adding requests:  66%|██████▌   | 1350/2048 [00:02<00:01, 470.96it/s]
Adding requests:  68%|██████▊   | 1400/2048 [00:03<00:01, 476.88it/s]
Adding requests:  71%|███████   | 1448/2048 [00:03<00:01, 475.01it/s]
Adding requests:  73%|███████▎  | 1498/2048 [00:03<00:01, 481.04it/s]
Adding requests:  76%|███████▌  | 1547/2048 [00:03<00:01, 480.55it/s]
Adding requests:  78%|███████▊  | 1597/2048 [00:03<00:00, 486.06it/s]
Adding requests:  80%|████████  | 1646/2048 [00:03<00:00, 477.66it/s]
Adding requests:  83%|████████▎ | 1694/2048 [00:03<00:00, 473.87it/s]
Adding requests:  85%|████████▌ | 1744/2048 [00:03<00:00, 476.64it/s]
Adding requests:  88%|████████▊ | 1792/2048 [00:03<00:00, 474.92it/s]
Adding requests:  90%|████████▉ | 1841/2048 [00:03<00:00, 476.76it/s]
Adding requests:  92%|█████████▏| 1889/2048 [00:04<00:00, 476.82it/s]
Adding requests:  95%|█████████▍| 1937/2048 [00:04<00:00, 477.23it/s]
Adding requests:  97%|█████████▋| 1985/2048 [00:04<00:00, 477.02it/s]
Adding requests:  99%|█████████▉| 2035/2048 [00:04<00:00, 480.10it/s]
Adding requests: 100%|██████████| 2048/2048 [00:04<00:00, 463.65it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:00<00:00, 58989.92it/s, est. speed input: 60430988.91 toks/s, output: 59007.34 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:00<00:00, 58818.65it/s, est. speed input: 60430988.91 toks/s, output: 59007.34 toks/s]
[rank0]:[W126 13:48:03.129322754 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 44.1s

测试结果:
  Requests/s:   459.87
  Tokens/s:     471368.34
  Total Reqs:   2048
  Elapsed:      4.45s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     470908.47

============================================================
[7/8] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:48:30 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=527424) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=527424) WARNING 01-26 13:48:43 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=527424) WARNING 01-26 13:48:49 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 467.92 requests/s, 479616.98 total tokens/s, 467.92 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-26 13:48:30] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:48:30] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:48:30] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:48:30] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:48:30] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:48:30] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:48:30] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:48:30] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:48:30] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:48:30] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:48:30] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:48:30] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:48:30] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:48:30] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:48:37] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:48:37] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:48:37] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:48:37] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:48:37] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:48:37] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:48:37] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:48:37] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:48:37] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:48:37] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:48:37] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:48:37] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:48:37] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:48:37] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=527424) [2026-01-26 13:48:38] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=527424) [2026-01-26 13:48:38] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=527424) [2026-01-26 13:48:38] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=527424) [2026-01-26 13:48:38] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=527424) [2026-01-26 13:48:38] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=527424) [2026-01-26 13:48:38] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=527424) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=527424) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.14it/s]
(EngineCore_DP0 pid=527424) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.14it/s]
(EngineCore_DP0 pid=527424) 
(EngineCore_DP0 pid=527424) [2026-01-26 13:48:38] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=527424) [2026-01-26 13:48:38] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5898240 bytes
(EngineCore_DP0 pid=527424) [2026-01-26 13:48:38] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 3072] -> 1D uint8
(EngineCore_DP0 pid=527424) [2026-01-26 13:48:38] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 3932160 bytes
(EngineCore_DP0 pid=527424) [2026-01-26 13:48:38] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=527424) [2026-01-26 13:48:38] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 31457280 bytes
(EngineCore_DP0 pid=527424) [2026-01-26 13:48:38] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 12288] -> 1D uint8
(EngineCore_DP0 pid=527424) [2026-01-26 13:48:38] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15728640 bytes
(EngineCore_DP0 pid=527424) [rank0]:W0126 13:48:46.388000 527424 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=527424) [rank0]:W0126 13:48:46.457000 527424 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=527424) [rank0]:W0126 13:48:47.263000 527424 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=527424) [rank0]:W0126 13:48:47.360000 527424 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=527424) 2026-01-26 13:48:49,713 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=527424) 2026-01-26 13:48:49,737 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=527424) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:00, 13.86it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 5/11 [00:00<00:00, 17.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:00<00:00, 20.60it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 20.17it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 19.47it/s]
(EngineCore_DP0 pid=527424) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  43%|████▎     | 3/7 [00:00<00:00, 23.00it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00, 23.11it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 23.10it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 41/4096 [00:00<00:10, 402.00it/s]
Adding requests:   2%|▏         | 86/4096 [00:00<00:09, 425.07it/s]
Adding requests:   3%|▎         | 131/4096 [00:00<00:09, 434.47it/s]
Adding requests:   4%|▍         | 176/4096 [00:00<00:08, 437.73it/s]
Adding requests:   5%|▌         | 223/4096 [00:00<00:08, 447.43it/s]
Adding requests:   7%|▋         | 271/4096 [00:00<00:08, 457.10it/s]
Adding requests:   8%|▊         | 317/4096 [00:00<00:08, 452.99it/s]
Adding requests:   9%|▉         | 365/4096 [00:00<00:08, 459.43it/s]
Adding requests:  10%|█         | 412/4096 [00:00<00:07, 462.38it/s]
Adding requests:  11%|█         | 460/4096 [00:01<00:07, 465.02it/s]
Adding requests:  12%|█▏        | 507/4096 [00:01<00:07, 465.58it/s]
Adding requests:  14%|█▎        | 554/4096 [00:01<00:07, 460.36it/s]
Adding requests:  15%|█▍        | 602/4096 [00:01<00:07, 464.31it/s]
Adding requests:  16%|█▌        | 652/4096 [00:01<00:07, 471.95it/s]
Adding requests:  17%|█▋        | 703/4096 [00:01<00:07, 481.02it/s]
Adding requests:  18%|█▊        | 752/4096 [00:01<00:06, 478.23it/s]
Adding requests:  20%|█▉        | 800/4096 [00:01<00:07, 468.50it/s]
Adding requests:  21%|██        | 847/4096 [00:01<00:07, 461.74it/s]
Adding requests:  22%|██▏       | 897/4096 [00:01<00:06, 470.92it/s]
Adding requests:  23%|██▎       | 945/4096 [00:02<00:06, 473.21it/s]
Adding requests:  24%|██▍       | 994/4096 [00:02<00:06, 475.27it/s]
Adding requests:  25%|██▌       | 1043/4096 [00:02<00:06, 478.64it/s]
Adding requests:  27%|██▋       | 1091/4096 [00:02<00:06, 476.17it/s]
Adding requests:  28%|██▊       | 1139/4096 [00:02<00:06, 469.67it/s]
Adding requests:  29%|██▉       | 1191/4096 [00:02<00:06, 483.56it/s]
Adding requests:  30%|███       | 1240/4096 [00:02<00:05, 483.37it/s]
Adding requests:  31%|███▏      | 1289/4096 [00:02<00:05, 479.83it/s]
Adding requests:  33%|███▎      | 1339/4096 [00:02<00:05, 484.31it/s]
Adding requests:  34%|███▍      | 1388/4096 [00:02<00:05, 480.35it/s]
Adding requests:  35%|███▌      | 1437/4096 [00:03<00:05, 478.81it/s]
Adding requests:  36%|███▋      | 1487/4096 [00:03<00:05, 482.14it/s]
Adding requests:  38%|███▊      | 1537/4096 [00:03<00:05, 485.31it/s]
Adding requests:  39%|███▊      | 1586/4096 [00:03<00:05, 485.53it/s]
Adding requests:  40%|███▉      | 1637/4096 [00:03<00:05, 489.99it/s]
Adding requests:  41%|████      | 1687/4096 [00:03<00:04, 482.93it/s]
Adding requests:  42%|████▏     | 1736/4096 [00:03<00:04, 484.47it/s]
Adding requests:  44%|████▎     | 1785/4096 [00:03<00:04, 479.76it/s]
Adding requests:  45%|████▍     | 1833/4096 [00:03<00:04, 472.09it/s]
Adding requests:  46%|████▌     | 1881/4096 [00:03<00:04, 474.34it/s]
Adding requests:  47%|████▋     | 1929/4096 [00:04<00:04, 465.14it/s]
Adding requests:  48%|████▊     | 1977/4096 [00:04<00:04, 468.82it/s]
Adding requests:  49%|████▉     | 2026/4096 [00:04<00:04, 473.23it/s]
Adding requests:  51%|█████     | 2076/4096 [00:04<00:04, 479.42it/s]
Adding requests:  52%|█████▏    | 2124/4096 [00:04<00:04, 476.19it/s]
Adding requests:  53%|█████▎    | 2172/4096 [00:04<00:04, 472.96it/s]
Adding requests:  54%|█████▍    | 2220/4096 [00:04<00:03, 472.06it/s]
Adding requests:  55%|█████▌    | 2269/4096 [00:04<00:03, 474.53it/s]
Adding requests:  57%|█████▋    | 2319/4096 [00:04<00:03, 480.10it/s]
Adding requests:  58%|█████▊    | 2368/4096 [00:05<00:03, 477.01it/s]
Adding requests:  59%|█████▉    | 2416/4096 [00:05<00:03, 477.89it/s]
Adding requests:  60%|██████    | 2465/4096 [00:05<00:03, 480.02it/s]
Adding requests:  61%|██████▏   | 2514/4096 [00:05<00:03, 477.23it/s]
Adding requests:  63%|██████▎   | 2564/4096 [00:05<00:03, 483.75it/s]
Adding requests:  64%|██████▍   | 2613/4096 [00:05<00:03, 481.57it/s]
Adding requests:  65%|██████▌   | 2663/4096 [00:05<00:02, 485.81it/s]
Adding requests:  66%|██████▌   | 2712/4096 [00:05<00:02, 477.57it/s]
Adding requests:  67%|██████▋   | 2760/4096 [00:05<00:02, 478.08it/s]
Adding requests:  69%|██████▊   | 2808/4096 [00:05<00:02, 475.18it/s]
Adding requests:  70%|██████▉   | 2856/4096 [00:06<00:02, 475.33it/s]
Adding requests:  71%|███████   | 2905/4096 [00:06<00:02, 477.95it/s]
Adding requests:  72%|███████▏  | 2953/4096 [00:06<00:02, 474.93it/s]
Adding requests:  73%|███████▎  | 3001/4096 [00:06<00:02, 444.50it/s]
Adding requests:  74%|███████▍  | 3048/4096 [00:06<00:02, 450.93it/s]
Adding requests:  76%|███████▌  | 3095/4096 [00:06<00:02, 454.00it/s]
Adding requests:  77%|███████▋  | 3143/4096 [00:06<00:02, 457.01it/s]
Adding requests:  78%|███████▊  | 3191/4096 [00:06<00:01, 461.95it/s]
Adding requests:  79%|███████▉  | 3239/4096 [00:06<00:01, 467.15it/s]
Adding requests:  80%|████████  | 3287/4096 [00:06<00:01, 468.83it/s]
Adding requests:  81%|████████▏ | 3335/4096 [00:07<00:01, 470.16it/s]
Adding requests:  83%|████████▎ | 3383/4096 [00:07<00:01, 473.01it/s]
Adding requests:  84%|████████▍ | 3432/4096 [00:07<00:01, 475.90it/s]
Adding requests:  85%|████████▍ | 3480/4096 [00:07<00:01, 465.36it/s]
Adding requests:  86%|████████▌ | 3528/4096 [00:07<00:01, 468.21it/s]
Adding requests:  87%|████████▋ | 3576/4096 [00:07<00:01, 470.72it/s]
Adding requests:  88%|████████▊ | 3624/4096 [00:07<00:01, 468.62it/s]
Adding requests:  90%|████████▉ | 3672/4096 [00:07<00:00, 470.59it/s]
Adding requests:  91%|█████████ | 3720/4096 [00:07<00:00, 472.19it/s]
Adding requests:  92%|█████████▏| 3769/4096 [00:07<00:00, 476.87it/s]
Adding requests:  93%|█████████▎| 3819/4096 [00:08<00:00, 482.99it/s]
Adding requests:  94%|█████████▍| 3869/4096 [00:08<00:00, 486.35it/s]
Adding requests:  96%|█████████▌| 3918/4096 [00:08<00:00, 484.67it/s]
Adding requests:  97%|█████████▋| 3967/4096 [00:08<00:00, 483.85it/s]
Adding requests:  98%|█████████▊| 4016/4096 [00:08<00:00, 482.90it/s]
Adding requests:  99%|█████████▉| 4065/4096 [00:08<00:00, 476.72it/s]
Adding requests: 100%|██████████| 4096/4096 [00:08<00:00, 472.45it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:00<00:00, 49915.65it/s, est. speed input: 51122093.81 toks/s, output: 49921.31 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:00<00:00, 49853.80it/s, est. speed input: 51122093.81 toks/s, output: 49921.31 toks/s]
[rank0]:[W126 13:49:01.368439142 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 57.2s

测试结果:
  Requests/s:   467.92
  Tokens/s:     479616.98
  Total Reqs:   4096
  Elapsed:      8.75s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     479149.06

============================================================
[8/8] 测试 M=65536
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 65536
│   M_prefill     = 65536 (= 64 x 1024)
│   M_decode      = 64
│   batched_tokens = 65536 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 8192
│   --max-num-seqs           = 64
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 65536
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:49:46 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=529009) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=529009) WARNING 01-26 13:49:59 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=529009) WARNING 01-26 13:50:05 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 466.02 requests/s, 477670.73 total tokens/s, 466.02 output tokens/s
Total num prompt tokens:  8388608
Total num output tokens:  8192


─── STDERR ───
[2026-01-26 13:49:45] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:49:46] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:49:46] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:49:46] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:49:46] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:49:46] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:49:46] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:49:46] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:49:46] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:49:46] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:49:46] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:49:46] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:49:46] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:49:46] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:49:52] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:49:53] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:49:53] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:49:53] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:49:53] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:49:53] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:49:53] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:49:53] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:49:53] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:49:53] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:49:53] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:49:53] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:49:53] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:49:53] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=529009) [2026-01-26 13:49:54] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=529009) [2026-01-26 13:49:54] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=529009) [2026-01-26 13:49:54] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=529009) [2026-01-26 13:49:54] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=529009) [2026-01-26 13:49:54] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=529009) [2026-01-26 13:49:54] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=529009) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=529009) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.16it/s]
(EngineCore_DP0 pid=529009) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.16it/s]
(EngineCore_DP0 pid=529009) 
(EngineCore_DP0 pid=529009) [2026-01-26 13:49:54] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3072] -> 1D uint8
(EngineCore_DP0 pid=529009) [2026-01-26 13:49:55] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 5898240 bytes
(EngineCore_DP0 pid=529009) [2026-01-26 13:49:55] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 3072] -> 1D uint8
(EngineCore_DP0 pid=529009) [2026-01-26 13:49:55] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 3932160 bytes
(EngineCore_DP0 pid=529009) [2026-01-26 13:49:55] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3072] -> 1D uint8
(EngineCore_DP0 pid=529009) [2026-01-26 13:49:55] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 31457280 bytes
(EngineCore_DP0 pid=529009) [2026-01-26 13:49:55] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 12288] -> 1D uint8
(EngineCore_DP0 pid=529009) [2026-01-26 13:49:55] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 15728640 bytes
(EngineCore_DP0 pid=529009) [rank0]:W0126 13:50:02.311000 529009 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=529009) [rank0]:W0126 13:50:02.379000 529009 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=529009) [rank0]:W0126 13:50:03.179000 529009 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=529009) [rank0]:W0126 13:50:03.276000 529009 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=529009) 2026-01-26 13:50:05,602 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=529009) 2026-01-26 13:50:05,625 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=529009) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   5%|▌         | 1/19 [00:00<00:03,  4.71it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 2/19 [00:00<00:03,  5.32it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  21%|██        | 4/19 [00:00<00:01,  9.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  37%|███▋      | 7/19 [00:00<00:00, 14.12it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  47%|████▋     | 9/19 [00:00<00:00, 14.07it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 11/19 [00:00<00:00, 14.84it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▎  | 14/19 [00:01<00:00, 17.30it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 17/19 [00:01<00:00, 19.58it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:01<00:00, 15.14it/s]
(EngineCore_DP0 pid=529009) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  27%|██▋       | 3/11 [00:00<00:00, 23.02it/s]
Capturing CUDA graphs (decode, FULL):  55%|█████▍    | 6/11 [00:00<00:00, 23.55it/s]
Capturing CUDA graphs (decode, FULL):  82%|████████▏ | 9/11 [00:00<00:00, 23.73it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:00<00:00, 23.66it/s]

Adding requests:   0%|          | 0/8192 [00:00<?, ?it/s]
Adding requests:   0%|          | 40/8192 [00:00<00:20, 394.21it/s]
Adding requests:   1%|          | 85/8192 [00:00<00:19, 421.82it/s]
Adding requests:   2%|▏         | 129/8192 [00:00<00:18, 426.44it/s]
Adding requests:   2%|▏         | 173/8192 [00:00<00:18, 431.02it/s]
Adding requests:   3%|▎         | 218/8192 [00:00<00:18, 436.84it/s]
Adding requests:   3%|▎         | 266/8192 [00:00<00:17, 449.41it/s]
Adding requests:   4%|▍         | 311/8192 [00:00<00:17, 447.69it/s]
Adding requests:   4%|▍         | 357/8192 [00:00<00:17, 450.88it/s]
Adding requests:   5%|▍         | 404/8192 [00:00<00:17, 454.33it/s]
Adding requests:   5%|▌         | 450/8192 [00:01<00:17, 452.22it/s]
Adding requests:   6%|▌         | 497/8192 [00:01<00:16, 454.71it/s]
Adding requests:   7%|▋         | 543/8192 [00:01<00:17, 447.91it/s]
Adding requests:   7%|▋         | 592/8192 [00:01<00:16, 459.06it/s]
Adding requests:   8%|▊         | 639/8192 [00:01<00:16, 461.14it/s]
Adding requests:   8%|▊         | 688/8192 [00:01<00:16, 467.29it/s]
Adding requests:   9%|▉         | 737/8192 [00:01<00:15, 472.24it/s]
Adding requests:  10%|▉         | 785/8192 [00:01<00:15, 467.72it/s]
Adding requests:  10%|█         | 832/8192 [00:01<00:16, 457.19it/s]
Adding requests:  11%|█         | 880/8192 [00:01<00:15, 460.96it/s]
Adding requests:  11%|█▏        | 928/8192 [00:02<00:15, 466.19it/s]
Adding requests:  12%|█▏        | 975/8192 [00:02<00:15, 465.35it/s]
Adding requests:  12%|█▏        | 1023/8192 [00:02<00:15, 467.00it/s]
Adding requests:  13%|█▎        | 1070/8192 [00:02<00:15, 462.50it/s]
Adding requests:  14%|█▎        | 1117/8192 [00:02<00:15, 459.09it/s]
Adding requests:  14%|█▍        | 1166/8192 [00:02<00:15, 466.06it/s]
Adding requests:  15%|█▍        | 1215/8192 [00:02<00:14, 472.91it/s]
Adding requests:  15%|█▌        | 1263/8192 [00:02<00:14, 466.30it/s]
Adding requests:  16%|█▌        | 1310/8192 [00:02<00:14, 466.91it/s]
Adding requests:  17%|█▋        | 1358/8192 [00:02<00:14, 470.25it/s]
Adding requests:  17%|█▋        | 1407/8192 [00:03<00:14, 474.67it/s]
Adding requests:  18%|█▊        | 1455/8192 [00:03<00:14, 472.86it/s]
Adding requests:  18%|█▊        | 1503/8192 [00:03<00:14, 465.94it/s]
Adding requests:  19%|█▉        | 1551/8192 [00:03<00:14, 467.80it/s]
Adding requests:  20%|█▉        | 1601/8192 [00:03<00:13, 475.06it/s]
Adding requests:  20%|██        | 1649/8192 [00:03<00:13, 473.81it/s]
Adding requests:  21%|██        | 1697/8192 [00:03<00:13, 471.43it/s]
Adding requests:  21%|██▏       | 1745/8192 [00:03<00:13, 470.43it/s]
Adding requests:  22%|██▏       | 1794/8192 [00:03<00:13, 473.84it/s]
Adding requests:  22%|██▏       | 1842/8192 [00:03<00:13, 472.77it/s]
Adding requests:  23%|██▎       | 1890/8192 [00:04<00:13, 473.84it/s]
Adding requests:  24%|██▎       | 1938/8192 [00:04<00:13, 473.01it/s]
Adding requests:  24%|██▍       | 1986/8192 [00:04<00:13, 473.48it/s]
Adding requests:  25%|██▍       | 2035/8192 [00:04<00:12, 476.69it/s]
Adding requests:  25%|██▌       | 2084/8192 [00:04<00:12, 479.97it/s]
Adding requests:  26%|██▌       | 2133/8192 [00:04<00:12, 474.10it/s]
Adding requests:  27%|██▋       | 2181/8192 [00:04<00:12, 467.44it/s]
Adding requests:  27%|██▋       | 2230/8192 [00:04<00:12, 471.22it/s]
Adding requests:  28%|██▊       | 2278/8192 [00:04<00:12, 473.62it/s]
Adding requests:  28%|██▊       | 2326/8192 [00:05<00:12, 473.07it/s]
Adding requests:  29%|██▉       | 2374/8192 [00:05<00:12, 474.12it/s]
Adding requests:  30%|██▉       | 2422/8192 [00:05<00:12, 474.73it/s]
Adding requests:  30%|███       | 2470/8192 [00:05<00:12, 474.28it/s]
Adding requests:  31%|███       | 2518/8192 [00:05<00:12, 472.68it/s]
Adding requests:  31%|███▏      | 2568/8192 [00:05<00:11, 478.17it/s]
Adding requests:  32%|███▏      | 2616/8192 [00:05<00:11, 465.69it/s]
Adding requests:  33%|███▎      | 2665/8192 [00:05<00:11, 471.45it/s]
Adding requests:  33%|███▎      | 2713/8192 [00:05<00:11, 465.86it/s]
Adding requests:  34%|███▎      | 2761/8192 [00:05<00:11, 468.92it/s]
Adding requests:  34%|███▍      | 2808/8192 [00:06<00:11, 466.50it/s]
Adding requests:  35%|███▍      | 2856/8192 [00:06<00:11, 468.06it/s]
Adding requests:  35%|███▌      | 2905/8192 [00:06<00:11, 471.58it/s]
Adding requests:  36%|███▌      | 2953/8192 [00:06<00:11, 469.02it/s]
Adding requests:  37%|███▋      | 3000/8192 [00:06<00:11, 468.78it/s]
Adding requests:  37%|███▋      | 3048/8192 [00:06<00:10, 470.10it/s]
Adding requests:  38%|███▊      | 3096/8192 [00:06<00:10, 468.71it/s]
Adding requests:  38%|███▊      | 3143/8192 [00:06<00:10, 468.55it/s]
Adding requests:  39%|███▉      | 3191/8192 [00:06<00:10, 471.43it/s]
Adding requests:  40%|███▉      | 3240/8192 [00:06<00:10, 474.51it/s]
Adding requests:  40%|████      | 3288/8192 [00:07<00:10, 475.71it/s]
Adding requests:  41%|████      | 3336/8192 [00:07<00:10, 475.85it/s]
Adding requests:  41%|████▏     | 3385/8192 [00:07<00:10, 479.79it/s]
Adding requests:  42%|████▏     | 3434/8192 [00:07<00:09, 482.52it/s]
Adding requests:  43%|████▎     | 3483/8192 [00:07<00:10, 470.49it/s]
Adding requests:  43%|████▎     | 3531/8192 [00:07<00:09, 472.48it/s]
Adding requests:  44%|████▎     | 3579/8192 [00:07<00:09, 473.62it/s]
Adding requests:  44%|████▍     | 3627/8192 [00:07<00:09, 461.69it/s]
Adding requests:  45%|████▍     | 3674/8192 [00:07<00:09, 459.20it/s]
Adding requests:  45%|████▌     | 3720/8192 [00:07<00:09, 454.02it/s]
Adding requests:  46%|████▌     | 3766/8192 [00:08<00:09, 445.45it/s]
Adding requests:  47%|████▋     | 3813/8192 [00:08<00:09, 450.73it/s]
Adding requests:  47%|████▋     | 3860/8192 [00:08<00:09, 455.80it/s]
Adding requests:  48%|████▊     | 3906/8192 [00:08<00:09, 452.82it/s]
Adding requests:  48%|████▊     | 3952/8192 [00:08<00:09, 451.72it/s]
Adding requests:  49%|████▉     | 3998/8192 [00:08<00:09, 450.11it/s]
Adding requests:  49%|████▉     | 4044/8192 [00:08<00:09, 447.64it/s]
Adding requests:  50%|████▉     | 4090/8192 [00:08<00:09, 449.84it/s]
Adding requests:  50%|█████     | 4136/8192 [00:08<00:08, 452.70it/s]
Adding requests:  51%|█████     | 4182/8192 [00:09<00:08, 454.31it/s]
Adding requests:  52%|█████▏    | 4228/8192 [00:09<00:08, 453.53it/s]
Adding requests:  52%|█████▏    | 4274/8192 [00:09<00:08, 447.20it/s]
Adding requests:  53%|█████▎    | 4322/8192 [00:09<00:08, 456.45it/s]
Adding requests:  53%|█████▎    | 4372/8192 [00:09<00:08, 466.90it/s]
Adding requests:  54%|█████▍    | 4421/8192 [00:09<00:07, 471.86it/s]
Adding requests:  55%|█████▍    | 4471/8192 [00:09<00:07, 478.89it/s]
Adding requests:  55%|█████▌    | 4519/8192 [00:09<00:07, 469.51it/s]
Adding requests:  56%|█████▌    | 4568/8192 [00:09<00:07, 475.33it/s]
Adding requests:  56%|█████▋    | 4617/8192 [00:09<00:07, 478.63it/s]
Adding requests:  57%|█████▋    | 4666/8192 [00:10<00:07, 479.04it/s]
Adding requests:  58%|█████▊    | 4714/8192 [00:10<00:07, 477.37it/s]
Adding requests:  58%|█████▊    | 4763/8192 [00:10<00:07, 480.58it/s]
Adding requests:  59%|█████▊    | 4812/8192 [00:10<00:07, 479.42it/s]
Adding requests:  59%|█████▉    | 4860/8192 [00:10<00:06, 479.41it/s]
Adding requests:  60%|█████▉    | 4908/8192 [00:10<00:06, 476.00it/s]
Adding requests:  60%|██████    | 4956/8192 [00:10<00:06, 469.17it/s]
Adding requests:  61%|██████    | 5004/8192 [00:10<00:06, 471.11it/s]
Adding requests:  62%|██████▏   | 5053/8192 [00:10<00:06, 474.70it/s]
Adding requests:  62%|██████▏   | 5104/8192 [00:10<00:06, 482.13it/s]
Adding requests:  63%|██████▎   | 5153/8192 [00:11<00:06, 481.23it/s]
Adding requests:  64%|██████▎   | 5202/8192 [00:11<00:06, 481.13it/s]
Adding requests:  64%|██████▍   | 5251/8192 [00:11<00:06, 474.81it/s]
Adding requests:  65%|██████▍   | 5301/8192 [00:11<00:06, 481.00it/s]
Adding requests:  65%|██████▌   | 5350/8192 [00:11<00:05, 481.58it/s]
Adding requests:  66%|██████▌   | 5399/8192 [00:11<00:05, 482.48it/s]
Adding requests:  67%|██████▋   | 5448/8192 [00:11<00:05, 479.86it/s]
Adding requests:  67%|██████▋   | 5496/8192 [00:11<00:05, 475.98it/s]
Adding requests:  68%|██████▊   | 5544/8192 [00:11<00:05, 475.06it/s]
Adding requests:  68%|██████▊   | 5592/8192 [00:11<00:05, 474.69it/s]
Adding requests:  69%|██████▉   | 5640/8192 [00:12<00:05, 474.84it/s]
Adding requests:  69%|██████▉   | 5688/8192 [00:12<00:05, 472.50it/s]
Adding requests:  70%|███████   | 5736/8192 [00:12<00:05, 462.11it/s]
Adding requests:  71%|███████   | 5783/8192 [00:12<00:05, 462.21it/s]
Adding requests:  71%|███████   | 5830/8192 [00:12<00:05, 460.14it/s]
Adding requests:  72%|███████▏  | 5879/8192 [00:12<00:04, 467.29it/s]
Adding requests:  72%|███████▏  | 5927/8192 [00:12<00:04, 470.74it/s]
Adding requests:  73%|███████▎  | 5975/8192 [00:12<00:04, 467.32it/s]
Adding requests:  74%|███████▎  | 6024/8192 [00:12<00:04, 471.53it/s]
Adding requests:  74%|███████▍  | 6073/8192 [00:12<00:04, 475.27it/s]
Adding requests:  75%|███████▍  | 6121/8192 [00:13<00:04, 462.79it/s]
Adding requests:  75%|███████▌  | 6169/8192 [00:13<00:04, 465.87it/s]
Adding requests:  76%|███████▌  | 6220/8192 [00:13<00:04, 476.66it/s]
Adding requests:  77%|███████▋  | 6270/8192 [00:13<00:03, 482.12it/s]
Adding requests:  77%|███████▋  | 6320/8192 [00:13<00:03, 485.66it/s]
Adding requests:  78%|███████▊  | 6370/8192 [00:13<00:03, 487.74it/s]
Adding requests:  78%|███████▊  | 6419/8192 [00:13<00:03, 486.86it/s]
Adding requests:  79%|███████▉  | 6469/8192 [00:13<00:03, 490.26it/s]
Adding requests:  80%|███████▉  | 6520/8192 [00:13<00:03, 495.12it/s]
Adding requests:  80%|████████  | 6570/8192 [00:14<00:03, 493.04it/s]
Adding requests:  81%|████████  | 6620/8192 [00:14<00:03, 487.34it/s]
Adding requests:  81%|████████▏ | 6669/8192 [00:14<00:03, 485.56it/s]
Adding requests:  82%|████████▏ | 6718/8192 [00:14<00:03, 486.67it/s]
Adding requests:  83%|████████▎ | 6767/8192 [00:14<00:03, 474.21it/s]
Adding requests:  83%|████████▎ | 6817/8192 [00:14<00:02, 480.07it/s]
Adding requests:  84%|████████▍ | 6868/8192 [00:14<00:02, 486.93it/s]
Adding requests:  84%|████████▍ | 6918/8192 [00:14<00:02, 488.33it/s]
Adding requests:  85%|████████▌ | 6968/8192 [00:14<00:02, 491.13it/s]
Adding requests:  86%|████████▌ | 7018/8192 [00:14<00:02, 484.38it/s]
Adding requests:  86%|████████▋ | 7067/8192 [00:15<00:02, 484.63it/s]
Adding requests:  87%|████████▋ | 7117/8192 [00:15<00:02, 487.04it/s]
Adding requests:  87%|████████▋ | 7166/8192 [00:15<00:02, 481.80it/s]
Adding requests:  88%|████████▊ | 7216/8192 [00:15<00:02, 482.83it/s]
Adding requests:  89%|████████▊ | 7265/8192 [00:15<00:01, 476.61it/s]
Adding requests:  89%|████████▉ | 7313/8192 [00:15<00:01, 476.23it/s]
Adding requests:  90%|████████▉ | 7361/8192 [00:15<00:01, 474.42it/s]
Adding requests:  90%|█████████ | 7411/8192 [00:15<00:01, 481.20it/s]
Adding requests:  91%|█████████ | 7461/8192 [00:15<00:01, 484.53it/s]
Adding requests:  92%|█████████▏| 7510/8192 [00:15<00:01, 484.09it/s]
Adding requests:  92%|█████████▏| 7559/8192 [00:16<00:01, 483.47it/s]
Adding requests:  93%|█████████▎| 7608/8192 [00:16<00:01, 478.92it/s]
Adding requests:  93%|█████████▎| 7656/8192 [00:16<00:01, 468.59it/s]
Adding requests:  94%|█████████▍| 7705/8192 [00:16<00:01, 472.66it/s]
Adding requests:  95%|█████████▍| 7753/8192 [00:16<00:00, 472.79it/s]
Adding requests:  95%|█████████▌| 7801/8192 [00:16<00:00, 469.23it/s]
Adding requests:  96%|█████████▌| 7849/8192 [00:16<00:00, 470.45it/s]
Adding requests:  96%|█████████▋| 7898/8192 [00:16<00:00, 473.50it/s]
Adding requests:  97%|█████████▋| 7946/8192 [00:16<00:00, 468.19it/s]
Adding requests:  98%|█████████▊| 7994/8192 [00:16<00:00, 469.27it/s]
Adding requests:  98%|█████████▊| 8041/8192 [00:17<00:00, 467.64it/s]
Adding requests:  99%|█████████▉| 8091/8192 [00:17<00:00, 475.50it/s]
Adding requests:  99%|█████████▉| 8139/8192 [00:17<00:00, 476.21it/s]
Adding requests: 100%|█████████▉| 8189/8192 [00:17<00:00, 480.30it/s]
Adding requests: 100%|██████████| 8192/8192 [00:17<00:00, 470.55it/s]

Processed prompts:   0%|          | 0/8192 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  58%|█████▊    | 4766/8192 [00:00<00:00, 47608.85it/s, est. speed input: 48755645.85 toks/s, output: 47609.87 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [00:00<00:00, 47608.85it/s, est. speed input: 50529389.84 toks/s, output: 49343.55 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [00:00<00:00, 49309.13it/s, est. speed input: 50529389.84 toks/s, output: 49343.55 toks/s]
[rank0]:[W126 13:50:26.055403359 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 85.9s

测试结果:
  Requests/s:   466.02
  Tokens/s:     477670.73
  Total Reqs:   8192
  Elapsed:      17.58s

  [Prefill 分析]
  Total Prefill Tokens: 8388608
  Prefill Tokens/s:     477204.71


------------------------------------------------------------
  生成 CSV: Llama3.2-1B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_8/Llama3.2-1B-INT8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,39.5959,20312.6979,3.2327
1024,1024,1,128,128,37.4896,38426.8867,3.4143
2048,1024,2,256,128,75.1776,77057.0316,3.4053
4096,1024,4,512,128,159.3288,163312.0641,3.2135
8192,1024,8,1024,128,302.4356,309996.4869,3.3858
16384,1024,16,2048,128,459.8715,471368.3383,4.4534
32768,1024,32,4096,128,467.9190,479616.9757,8.7537
65536,1024,64,8192,128,466.0202,477670.7290,17.5786

------------------------------------------------------------

[INFO] 完成: 8 成功, 0 失败

============================================================
  Llama3.2-1B-INT8 | cuSPARSELt (2_10) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/Llama3.2-1B-INT8-SlideSparse-2_10
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_10

============================================================
[1/8] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:50:35 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=530253) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=530253) WARNING 01-26 13:50:48 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=530253) WARNING 01-26 13:50:55 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 42.84 requests/s, 21976.05 total tokens/s, 42.84 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-26 13:50:35] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:50:35] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:50:35] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:50:35] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:50:35] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:50:35] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:50:35] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:50:35] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:50:35] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:50:35] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:50:35] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:50:35] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:50:35] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:50:35] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:50:42] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:50:42] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:50:42] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:50:42] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:50:42] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:50:42] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:50:42] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:50:42] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:50:42] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:50:42] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:50:42] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:50:42] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:50:42] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:50:42] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=530253) [2026-01-26 13:50:43] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=530253) [2026-01-26 13:50:43] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=530253) [2026-01-26 13:50:43] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=530253) [2026-01-26 13:50:43] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=530253) [2026-01-26 13:50:43] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=530253) [2026-01-26 13:50:43] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=530253) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=530253) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.43it/s]
(EngineCore_DP0 pid=530253) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.43it/s]
(EngineCore_DP0 pid=530253) 
(EngineCore_DP0 pid=530253) [2026-01-26 13:50:44] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=530253) [2026-01-26 13:50:44] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6340608 bytes
(EngineCore_DP0 pid=530253) [2026-01-26 13:50:44] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=530253) [2026-01-26 13:50:44] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4227072 bytes
(EngineCore_DP0 pid=530253) [2026-01-26 13:50:44] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=530253) [2026-01-26 13:50:44] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 33816576 bytes
(EngineCore_DP0 pid=530253) [2026-01-26 13:50:44] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=530253) [2026-01-26 13:50:44] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16809984 bytes
(EngineCore_DP0 pid=530253) 2026-01-26 13:50:55,318 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=530253) 2026-01-26 13:50:55,339 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=530253) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 20.63it/s]
(EngineCore_DP0 pid=530253) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 20.52it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  58%|█████▊    | 74/128 [00:00<00:00, 735.37it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 737.84it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   8%|▊         | 10/128 [00:00<00:01, 95.87it/s, est. speed input: 49090.84 toks/s, output: 95.88 toks/s]
Processed prompts:  16%|█▌        | 20/128 [00:00<00:01, 56.17it/s, est. speed input: 30665.44 toks/s, output: 59.89 toks/s]
Processed prompts:  21%|██        | 27/128 [00:00<00:01, 50.77it/s, est. speed input: 27966.42 toks/s, output: 54.62 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:00<00:01, 48.21it/s, est. speed input: 26706.79 toks/s, output: 52.16 toks/s]
Processed prompts:  30%|███       | 39/128 [00:00<00:01, 46.90it/s, est. speed input: 25986.18 toks/s, output: 50.75 toks/s]
Processed prompts:  34%|███▍      | 44/128 [00:00<00:01, 45.65it/s, est. speed input: 25430.68 toks/s, output: 49.67 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:01<00:01, 45.06it/s, est. speed input: 25074.39 toks/s, output: 48.97 toks/s]
Processed prompts:  42%|████▏     | 54/128 [00:01<00:01, 44.66it/s, est. speed input: 24795.16 toks/s, output: 48.43 toks/s]
Processed prompts:  46%|████▌     | 59/128 [00:01<00:01, 44.37it/s, est. speed input: 24568.54 toks/s, output: 47.99 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:01<00:01, 44.17it/s, est. speed input: 24381.58 toks/s, output: 47.62 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:01<00:01, 44.02it/s, est. speed input: 24223.03 toks/s, output: 47.31 toks/s]
Processed prompts:  58%|█████▊    | 74/128 [00:01<00:01, 43.96it/s, est. speed input: 24093.53 toks/s, output: 47.06 toks/s]
Processed prompts:  62%|██████▏   | 79/128 [00:01<00:01, 43.91it/s, est. speed input: 23979.76 toks/s, output: 46.84 toks/s]
Processed prompts:  66%|██████▌   | 84/128 [00:01<00:01, 43.49it/s, est. speed input: 23836.03 toks/s, output: 46.55 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:01<00:00, 43.55it/s, est. speed input: 23749.22 toks/s, output: 46.39 toks/s]
Processed prompts:  73%|███████▎  | 94/128 [00:02<00:00, 43.61it/s, est. speed input: 23673.58 toks/s, output: 46.24 toks/s]
Processed prompts:  77%|███████▋  | 99/128 [00:02<00:00, 43.26it/s, est. speed input: 23567.57 toks/s, output: 46.03 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:02<00:00, 43.43it/s, est. speed input: 23510.66 toks/s, output: 45.92 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:02<00:00, 43.52it/s, est. speed input: 23457.15 toks/s, output: 45.81 toks/s]
Processed prompts:  89%|████████▉ | 114/128 [00:02<00:00, 43.53it/s, est. speed input: 23403.37 toks/s, output: 45.71 toks/s]
Processed prompts:  93%|█████████▎| 119/128 [00:02<00:00, 43.58it/s, est. speed input: 23358.38 toks/s, output: 45.62 toks/s]
Processed prompts:  97%|█████████▋| 124/128 [00:02<00:00, 43.63it/s, est. speed input: 23318.05 toks/s, output: 45.54 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 43.63it/s, est. speed input: 23294.65 toks/s, output: 45.50 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 45.50it/s, est. speed input: 23294.65 toks/s, output: 45.50 toks/s]
[rank0]:[W126 13:51:00.370429565 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 33.2s

测试结果:
  Requests/s:   42.84
  Tokens/s:     21976.05
  Total Reqs:   128
  Elapsed:      2.99s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     21933.21

============================================================
[2/8] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:51:09 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=531281) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=531281) WARNING 01-26 13:51:21 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=531281) WARNING 01-26 13:51:28 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 41.06 requests/s, 42085.19 total tokens/s, 41.06 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-26 13:51:08] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:51:09] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:51:09] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:51:09] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:51:09] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:51:09] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:51:09] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:51:09] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:51:09] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:51:09] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:51:09] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:51:09] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:51:09] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:51:09] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:51:15] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:51:16] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:51:16] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:51:16] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:51:16] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:51:16] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:51:16] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:51:16] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:51:16] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:51:16] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:51:16] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:51:16] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:51:16] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:51:16] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=531281) [2026-01-26 13:51:17] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=531281) [2026-01-26 13:51:17] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=531281) [2026-01-26 13:51:17] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=531281) [2026-01-26 13:51:17] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=531281) [2026-01-26 13:51:17] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=531281) [2026-01-26 13:51:17] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=531281) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=531281) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.62it/s]
(EngineCore_DP0 pid=531281) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.62it/s]
(EngineCore_DP0 pid=531281) 
(EngineCore_DP0 pid=531281) [2026-01-26 13:51:17] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=531281) [2026-01-26 13:51:17] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6340608 bytes
(EngineCore_DP0 pid=531281) [2026-01-26 13:51:17] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=531281) [2026-01-26 13:51:17] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4227072 bytes
(EngineCore_DP0 pid=531281) [2026-01-26 13:51:17] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=531281) [2026-01-26 13:51:17] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 33816576 bytes
(EngineCore_DP0 pid=531281) [2026-01-26 13:51:17] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=531281) [2026-01-26 13:51:17] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16809984 bytes
(EngineCore_DP0 pid=531281) 2026-01-26 13:51:28,720 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=531281) 2026-01-26 13:51:28,742 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=531281) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 20.48it/s]
(EngineCore_DP0 pid=531281) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 20.12it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  31%|███▏      | 40/128 [00:00<00:00, 399.01it/s]
Adding requests:  66%|██████▋   | 85/128 [00:00<00:00, 425.00it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 425.27it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 422.46it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   8%|▊         | 10/128 [00:00<00:01, 86.88it/s, est. speed input: 88972.45 toks/s, output: 86.88 toks/s]
Processed prompts:  15%|█▍        | 19/128 [00:00<00:01, 55.57it/s, est. speed input: 60342.20 toks/s, output: 58.93 toks/s]
Processed prompts:  20%|██        | 26/128 [00:00<00:02, 50.30it/s, est. speed input: 55154.87 toks/s, output: 53.86 toks/s]
Processed prompts:  25%|██▌       | 32/128 [00:00<00:02, 47.96it/s, est. speed input: 52838.27 toks/s, output: 51.60 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:00<00:01, 46.86it/s, est. speed input: 51667.07 toks/s, output: 50.46 toks/s]
Processed prompts:  33%|███▎      | 42/128 [00:00<00:01, 46.01it/s, est. speed input: 50777.67 toks/s, output: 49.59 toks/s]
Processed prompts:  37%|███▋      | 47/128 [00:00<00:01, 45.39it/s, est. speed input: 50090.98 toks/s, output: 48.92 toks/s]
Processed prompts:  41%|████      | 52/128 [00:01<00:01, 44.98it/s, est. speed input: 49560.39 toks/s, output: 48.40 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:01<00:01, 44.66it/s, est. speed input: 49121.30 toks/s, output: 47.97 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:01<00:01, 44.47it/s, est. speed input: 48766.23 toks/s, output: 47.62 toks/s]
Processed prompts:  52%|█████▏    | 67/128 [00:01<00:01, 44.21it/s, est. speed input: 48435.47 toks/s, output: 47.30 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:01<00:01, 44.13it/s, est. speed input: 48179.59 toks/s, output: 47.05 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:01<00:01, 43.72it/s, est. speed input: 47868.60 toks/s, output: 46.75 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:01<00:01, 43.78it/s, est. speed input: 47682.52 toks/s, output: 46.56 toks/s]
Processed prompts:  68%|██████▊   | 87/128 [00:01<00:00, 43.77it/s, est. speed input: 47505.97 toks/s, output: 46.39 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:01<00:00, 43.61it/s, est. speed input: 47318.10 toks/s, output: 46.21 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:02<00:00, 43.70it/s, est. speed input: 47190.20 toks/s, output: 46.08 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:02<00:00, 43.74it/s, est. speed input: 47072.06 toks/s, output: 45.97 toks/s]
Processed prompts:  84%|████████▎ | 107/128 [00:02<00:00, 43.70it/s, est. speed input: 46953.98 toks/s, output: 45.85 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:02<00:00, 43.72it/s, est. speed input: 46854.54 toks/s, output: 45.76 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:02<00:00, 43.78it/s, est. speed input: 46770.72 toks/s, output: 45.67 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:02<00:00, 43.78it/s, est. speed input: 46687.57 toks/s, output: 45.59 toks/s]
Processed prompts:  99%|█████████▉| 127/128 [00:02<00:00, 43.69it/s, est. speed input: 46598.59 toks/s, output: 45.51 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 43.69it/s, est. speed input: 46587.79 toks/s, output: 45.50 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:02<00:00, 45.49it/s, est. speed input: 46587.79 toks/s, output: 45.50 toks/s]
[rank0]:[W126 13:51:33.954347312 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 33.5s

测试结果:
  Requests/s:   41.06
  Tokens/s:     42085.19
  Total Reqs:   128
  Elapsed:      3.12s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     42044.13

============================================================
[3/8] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:51:43 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=532322) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=532322) WARNING 01-26 13:51:55 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=532322) WARNING 01-26 13:52:02 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 77.23 requests/s, 79164.19 total tokens/s, 77.23 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-26 13:51:43] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:51:43] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:51:43] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:51:43] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:51:43] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:51:43] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:51:43] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:51:43] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:51:43] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:51:43] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:51:43] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:51:43] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:51:43] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:51:43] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:51:49] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:51:50] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:51:50] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:51:50] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:51:50] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:51:50] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:51:50] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:51:50] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:51:50] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:51:50] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:51:50] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:51:50] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:51:50] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:51:50] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=532322) [2026-01-26 13:51:50] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=532322) [2026-01-26 13:51:50] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=532322) [2026-01-26 13:51:50] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=532322) [2026-01-26 13:51:50] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=532322) [2026-01-26 13:51:50] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=532322) [2026-01-26 13:51:50] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=532322) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=532322) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.99it/s]
(EngineCore_DP0 pid=532322) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.99it/s]
(EngineCore_DP0 pid=532322) 
(EngineCore_DP0 pid=532322) [2026-01-26 13:51:51] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=532322) [2026-01-26 13:51:51] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6340608 bytes
(EngineCore_DP0 pid=532322) [2026-01-26 13:51:51] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=532322) [2026-01-26 13:51:51] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4227072 bytes
(EngineCore_DP0 pid=532322) [2026-01-26 13:51:51] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=532322) [2026-01-26 13:51:51] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 33816576 bytes
(EngineCore_DP0 pid=532322) [2026-01-26 13:51:51] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=532322) [2026-01-26 13:51:51] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16809984 bytes
(EngineCore_DP0 pid=532322) 2026-01-26 13:52:02,698 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=532322) 2026-01-26 13:52:02,721 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=532322) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 18.70it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 18.68it/s]
(EngineCore_DP0 pid=532322) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 22.50it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   0%|          | 1/256 [00:00<00:51,  4.91it/s]
Adding requests:  16%|█▋        | 42/256 [00:00<00:01, 170.65it/s]
Adding requests:  34%|███▎      | 86/256 [00:00<00:00, 269.10it/s]
Adding requests:  51%|█████     | 130/256 [00:00<00:00, 327.35it/s]
Adding requests:  68%|██████▊   | 174/256 [00:00<00:00, 362.92it/s]
Adding requests:  84%|████████▎ | 214/256 [00:00<00:00, 370.67it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 316.48it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  20%|█▉        | 50/256 [00:00<00:00, 460.10it/s, est. speed input: 471177.05 toks/s, output: 460.11 toks/s]
Processed prompts:  38%|███▊      | 97/256 [00:00<00:01, 134.18it/s, est. speed input: 154301.25 toks/s, output: 150.68 toks/s]
Processed prompts:  48%|████▊     | 122/256 [00:00<00:01, 112.90it/s, est. speed input: 132091.87 toks/s, output: 128.99 toks/s]
Processed prompts:  54%|█████▍    | 139/256 [00:01<00:01, 107.05it/s, est. speed input: 125680.92 toks/s, output: 122.73 toks/s]
Processed prompts:  60%|█████▉    | 153/256 [00:01<00:01, 101.73it/s, est. speed input: 120919.48 toks/s, output: 118.08 toks/s]
Processed prompts:  64%|██████▍   | 165/256 [00:01<00:00, 98.17it/s, est. speed input: 117788.84 toks/s, output: 115.03 toks/s] 
Processed prompts:  69%|██████▉   | 176/256 [00:01<00:00, 93.30it/s, est. speed input: 114496.41 toks/s, output: 111.81 toks/s]
Processed prompts:  73%|███████▎  | 186/256 [00:01<00:00, 91.68it/s, est. speed input: 112697.66 toks/s, output: 110.06 toks/s]
Processed prompts:  77%|███████▋  | 196/256 [00:01<00:00, 90.20it/s, est. speed input: 111087.76 toks/s, output: 108.48 toks/s]
Processed prompts:  80%|████████  | 206/256 [00:01<00:00, 89.01it/s, est. speed input: 109668.88 toks/s, output: 107.10 toks/s]
Processed prompts:  84%|████████▍ | 216/256 [00:02<00:00, 88.24it/s, est. speed input: 108449.69 toks/s, output: 105.91 toks/s]
Processed prompts:  88%|████████▊ | 226/256 [00:02<00:00, 87.45it/s, est. speed input: 107312.18 toks/s, output: 104.80 toks/s]
Processed prompts:  92%|█████████▏| 236/256 [00:02<00:00, 87.03it/s, est. speed input: 106326.32 toks/s, output: 103.83 toks/s]
Processed prompts:  96%|█████████▌| 246/256 [00:02<00:00, 86.91it/s, est. speed input: 105472.83 toks/s, output: 103.00 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 86.65it/s, est. speed input: 104666.52 toks/s, output: 102.21 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 86.65it/s, est. speed input: 104666.52 toks/s, output: 102.21 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:02<00:00, 102.21it/s, est. speed input: 104666.52 toks/s, output: 102.21 toks/s]
[rank0]:[W126 13:52:07.203815120 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 34.1s

测试结果:
  Requests/s:   77.23
  Tokens/s:     79164.19
  Total Reqs:   256
  Elapsed:      3.31s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     79086.96

============================================================
[4/8] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:52:18 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=533362) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=533362) WARNING 01-26 13:52:31 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=533362) WARNING 01-26 13:52:38 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 161.32 requests/s, 165356.57 total tokens/s, 161.32 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-26 13:52:18] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:52:18] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:52:18] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:52:18] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:52:18] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:52:18] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:52:18] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:52:18] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:52:18] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:52:18] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:52:18] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:52:18] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:52:18] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:52:18] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:52:25] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:52:25] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:52:25] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:52:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:52:25] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:52:25] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:52:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:52:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:52:25] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:52:25] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:52:25] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:52:25] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:52:25] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:52:25] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=533362) [2026-01-26 13:52:26] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=533362) [2026-01-26 13:52:26] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=533362) [2026-01-26 13:52:26] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=533362) [2026-01-26 13:52:26] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=533362) [2026-01-26 13:52:26] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=533362) [2026-01-26 13:52:26] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=533362) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=533362) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.01it/s]
(EngineCore_DP0 pid=533362) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.01it/s]
(EngineCore_DP0 pid=533362) 
(EngineCore_DP0 pid=533362) [2026-01-26 13:52:26] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=533362) [2026-01-26 13:52:26] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6340608 bytes
(EngineCore_DP0 pid=533362) [2026-01-26 13:52:26] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=533362) [2026-01-26 13:52:26] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4227072 bytes
(EngineCore_DP0 pid=533362) [2026-01-26 13:52:26] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=533362) [2026-01-26 13:52:26] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 33816576 bytes
(EngineCore_DP0 pid=533362) [2026-01-26 13:52:26] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=533362) [2026-01-26 13:52:26] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16809984 bytes
(EngineCore_DP0 pid=533362) 2026-01-26 13:52:37,980 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=533362) 2026-01-26 13:52:38,002 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=533362) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  75%|███████▌  | 3/4 [00:00<00:00, 22.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 19.81it/s]
(EngineCore_DP0 pid=533362) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 22.89it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 22.87it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   6%|▋         | 33/512 [00:00<00:01, 329.61it/s]
Adding requests:  14%|█▍        | 74/512 [00:00<00:01, 373.23it/s]
Adding requests:  23%|██▎       | 117/512 [00:00<00:00, 396.79it/s]
Adding requests:  31%|███       | 157/512 [00:00<00:01, 346.89it/s]
Adding requests:  39%|███▉      | 200/512 [00:00<00:00, 373.08it/s]
Adding requests:  48%|████▊     | 247/512 [00:00<00:00, 401.43it/s]
Adding requests:  57%|█████▋    | 292/512 [00:00<00:00, 415.92it/s]
Adding requests:  66%|██████▌   | 337/512 [00:00<00:00, 423.68it/s]
Adding requests:  75%|███████▌  | 384/512 [00:00<00:00, 436.09it/s]
Adding requests:  84%|████████▍ | 432/512 [00:01<00:00, 447.47it/s]
Adding requests:  94%|█████████▎| 479/512 [00:01<00:00, 452.11it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 418.40it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  39%|███▊      | 198/512 [00:00<00:00, 1655.86it/s, est. speed input: 1695696.22 toks/s, output: 1655.88 toks/s]
Processed prompts:  71%|███████   | 364/512 [00:01<00:00, 292.80it/s, est. speed input: 346356.97 toks/s, output: 338.24 toks/s]   
Processed prompts:  87%|████████▋ | 443/512 [00:01<00:00, 245.67it/s, est. speed input: 295056.16 toks/s, output: 288.14 toks/s]
Processed prompts:  96%|█████████▋| 494/512 [00:01<00:00, 223.49it/s, est. speed input: 273691.09 toks/s, output: 267.27 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:01<00:00, 223.49it/s, est. speed input: 269047.78 toks/s, output: 262.74 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:01<00:00, 262.73it/s, est. speed input: 269047.78 toks/s, output: 262.74 toks/s]
[rank0]:[W126 13:52:43.773965901 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 35.6s

测试结果:
  Requests/s:   161.32
  Tokens/s:     165356.57
  Total Reqs:   512
  Elapsed:      3.17s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     165195.25

============================================================
[5/8] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:52:56 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=534442) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=534442) WARNING 01-26 13:53:09 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=534442) WARNING 01-26 13:53:16 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 302.61 requests/s, 310175.13 total tokens/s, 302.61 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-26 13:52:56] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:52:56] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:52:56] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:52:56] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:52:56] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:52:56] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:52:56] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:52:56] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:52:56] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:52:56] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:52:56] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:52:56] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:52:56] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:52:56] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:53:03] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:53:03] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:53:03] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:53:03] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:53:03] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:53:03] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:53:03] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:53:03] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:53:03] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:53:03] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:53:03] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:53:03] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:53:03] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:53:03] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=534442) [2026-01-26 13:53:04] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=534442) [2026-01-26 13:53:04] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=534442) [2026-01-26 13:53:04] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=534442) [2026-01-26 13:53:04] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=534442) [2026-01-26 13:53:04] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=534442) [2026-01-26 13:53:04] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=534442) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=534442) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.01it/s]
(EngineCore_DP0 pid=534442) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.00it/s]
(EngineCore_DP0 pid=534442) 
(EngineCore_DP0 pid=534442) [2026-01-26 13:53:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=534442) [2026-01-26 13:53:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6340608 bytes
(EngineCore_DP0 pid=534442) [2026-01-26 13:53:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=534442) [2026-01-26 13:53:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4227072 bytes
(EngineCore_DP0 pid=534442) [2026-01-26 13:53:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=534442) [2026-01-26 13:53:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 33816576 bytes
(EngineCore_DP0 pid=534442) [2026-01-26 13:53:05] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=534442) [2026-01-26 13:53:05] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16809984 bytes
(EngineCore_DP0 pid=534442) 2026-01-26 13:53:15,989 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=534442) 2026-01-26 13:53:16,012 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=534442) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00, 16.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 17.70it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 17.55it/s]
(EngineCore_DP0 pid=534442) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  75%|███████▌  | 3/4 [00:00<00:00, 23.86it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 23.75it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   4%|▍         | 40/1024 [00:00<00:02, 395.30it/s]
Adding requests:   8%|▊         | 85/1024 [00:00<00:02, 421.64it/s]
Adding requests:  13%|█▎        | 130/1024 [00:00<00:02, 431.18it/s]
Adding requests:  17%|█▋        | 174/1024 [00:00<00:01, 432.68it/s]
Adding requests:  21%|██▏       | 220/1024 [00:00<00:01, 439.64it/s]
Adding requests:  26%|██▌       | 268/1024 [00:00<00:01, 451.39it/s]
Adding requests:  31%|███       | 314/1024 [00:00<00:01, 449.50it/s]
Adding requests:  35%|███▌      | 360/1024 [00:00<00:01, 452.31it/s]
Adding requests:  40%|███▉      | 407/1024 [00:00<00:01, 455.42it/s]
Adding requests:  44%|████▍     | 454/1024 [00:01<00:01, 458.24it/s]
Adding requests:  49%|████▉     | 500/1024 [00:01<00:01, 457.43it/s]
Adding requests:  53%|█████▎    | 546/1024 [00:01<00:01, 453.96it/s]
Adding requests:  58%|█████▊    | 595/1024 [00:01<00:00, 463.03it/s]
Adding requests:  63%|██████▎   | 643/1024 [00:01<00:00, 465.82it/s]
Adding requests:  68%|██████▊   | 692/1024 [00:01<00:00, 472.87it/s]
Adding requests:  72%|███████▏  | 740/1024 [00:01<00:00, 473.47it/s]
Adding requests:  77%|███████▋  | 788/1024 [00:01<00:00, 470.52it/s]
Adding requests:  82%|████████▏ | 836/1024 [00:01<00:00, 458.38it/s]
Adding requests:  86%|████████▋ | 885/1024 [00:01<00:00, 465.93it/s]
Adding requests:  91%|█████████ | 934/1024 [00:02<00:00, 470.47it/s]
Adding requests:  96%|█████████▌| 982/1024 [00:02<00:00, 470.50it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 459.10it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  66%|██████▌   | 674/1024 [00:00<00:00, 5431.46it/s, est. speed input: 5562135.69 toks/s, output: 5431.54 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:01<00:00, 5431.46it/s, est. speed input: 910086.08 toks/s, output: 888.75 toks/s] 
Processed prompts: 100%|██████████| 1024/1024 [00:01<00:00, 888.67it/s, est. speed input: 910086.08 toks/s, output: 888.75 toks/s] 
[rank0]:[W126 13:53:22.228984171 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 38.4s

测试结果:
  Requests/s:   302.61
  Tokens/s:     310175.13
  Total Reqs:   1024
  Elapsed:      3.38s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     309872.52

============================================================
[6/8] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:53:39 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=535587) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=535587) WARNING 01-26 13:53:51 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=535587) WARNING 01-26 13:53:58 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 436.34 requests/s, 447248.66 total tokens/s, 436.34 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-26 13:53:39] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:53:39] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:53:39] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:53:39] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:53:39] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:53:39] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:53:39] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:53:39] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:53:39] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:53:39] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:53:39] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:53:39] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:53:39] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:53:39] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:53:45] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:53:46] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:53:46] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:53:46] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:53:46] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:53:46] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:53:46] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:53:46] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:53:46] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:53:46] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:53:46] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:53:46] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:53:46] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:53:46] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=535587) [2026-01-26 13:53:47] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=535587) [2026-01-26 13:53:47] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=535587) [2026-01-26 13:53:47] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=535587) [2026-01-26 13:53:47] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=535587) [2026-01-26 13:53:47] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=535587) [2026-01-26 13:53:47] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=535587) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=535587) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.04it/s]
(EngineCore_DP0 pid=535587) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.04it/s]
(EngineCore_DP0 pid=535587) 
(EngineCore_DP0 pid=535587) [2026-01-26 13:53:47] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=535587) [2026-01-26 13:53:47] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6340608 bytes
(EngineCore_DP0 pid=535587) [2026-01-26 13:53:47] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=535587) [2026-01-26 13:53:47] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4227072 bytes
(EngineCore_DP0 pid=535587) [2026-01-26 13:53:47] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=535587) [2026-01-26 13:53:47] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 33816576 bytes
(EngineCore_DP0 pid=535587) [2026-01-26 13:53:47] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=535587) [2026-01-26 13:53:47] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16809984 bytes
(EngineCore_DP0 pid=535587) 2026-01-26 13:53:58,455 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=535587) 2026-01-26 13:53:58,479 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=535587) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 3/7 [00:00<00:00, 21.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:00<00:00, 22.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 20.92it/s]
(EngineCore_DP0 pid=535587) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  60%|██████    | 3/5 [00:00<00:00, 20.46it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 20.12it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 41/2048 [00:00<00:04, 402.42it/s]
Adding requests:   4%|▍         | 86/2048 [00:00<00:04, 428.25it/s]
Adding requests:   6%|▋         | 131/2048 [00:00<00:04, 435.99it/s]
Adding requests:   9%|▊         | 175/2048 [00:00<00:04, 436.89it/s]
Adding requests:  11%|█         | 222/2048 [00:00<00:04, 446.43it/s]
Adding requests:  13%|█▎        | 270/2048 [00:00<00:03, 455.90it/s]
Adding requests:  15%|█▌        | 316/2048 [00:00<00:03, 451.84it/s]
Adding requests:  18%|█▊        | 364/2048 [00:00<00:03, 459.13it/s]
Adding requests:  20%|██        | 411/2048 [00:00<00:03, 461.41it/s]
Adding requests:  22%|██▏       | 459/2048 [00:01<00:03, 465.59it/s]
Adding requests:  25%|██▍       | 506/2048 [00:01<00:03, 461.24it/s]
Adding requests:  27%|██▋       | 553/2048 [00:01<00:03, 458.02it/s]
Adding requests:  29%|██▉       | 601/2048 [00:01<00:03, 462.89it/s]
Adding requests:  32%|███▏      | 650/2048 [00:01<00:02, 468.05it/s]
Adding requests:  34%|███▍      | 700/2048 [00:01<00:02, 476.05it/s]
Adding requests:  37%|███▋      | 748/2048 [00:01<00:02, 473.66it/s]
Adding requests:  39%|███▉      | 796/2048 [00:01<00:02, 473.84it/s]
Adding requests:  41%|████      | 844/2048 [00:01<00:02, 462.31it/s]
Adding requests:  44%|████▎     | 894/2048 [00:01<00:02, 472.90it/s]
Adding requests:  46%|████▌     | 942/2048 [00:02<00:02, 474.07it/s]
Adding requests:  48%|████▊     | 991/2048 [00:02<00:02, 477.78it/s]
Adding requests:  51%|█████     | 1040/2048 [00:02<00:02, 479.58it/s]
Adding requests:  53%|█████▎    | 1088/2048 [00:02<00:02, 475.46it/s]
Adding requests:  55%|█████▌    | 1136/2048 [00:02<00:01, 472.44it/s]
Adding requests:  58%|█████▊    | 1184/2048 [00:02<00:01, 472.44it/s]
Adding requests:  60%|██████    | 1233/2048 [00:02<00:01, 477.39it/s]
Adding requests:  63%|██████▎   | 1281/2048 [00:02<00:01, 473.23it/s]
Adding requests:  65%|██████▍   | 1330/2048 [00:02<00:01, 477.82it/s]
Adding requests:  67%|██████▋   | 1379/2048 [00:02<00:01, 479.55it/s]
Adding requests:  70%|██████▉   | 1427/2048 [00:03<00:01, 464.74it/s]
Adding requests:  72%|███████▏  | 1476/2048 [00:03<00:01, 469.44it/s]
Adding requests:  74%|███████▍  | 1524/2048 [00:03<00:01, 461.11it/s]
Adding requests:  77%|███████▋  | 1573/2048 [00:03<00:01, 467.04it/s]
Adding requests:  79%|███████▉  | 1622/2048 [00:03<00:00, 473.22it/s]
Adding requests:  82%|████████▏ | 1670/2048 [00:03<00:00, 472.09it/s]
Adding requests:  84%|████████▍ | 1719/2048 [00:03<00:00, 474.80it/s]
Adding requests:  86%|████████▋ | 1767/2048 [00:03<00:00, 473.26it/s]
Adding requests:  89%|████████▊ | 1815/2048 [00:03<00:00, 474.05it/s]
Adding requests:  91%|█████████ | 1863/2048 [00:03<00:00, 471.38it/s]
Adding requests:  93%|█████████▎| 1912/2048 [00:04<00:00, 474.60it/s]
Adding requests:  96%|█████████▌| 1961/2048 [00:04<00:00, 476.41it/s]
Adding requests:  98%|█████████▊| 2010/2048 [00:04<00:00, 480.10it/s]
Adding requests: 100%|██████████| 2048/2048 [00:04<00:00, 468.34it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  95%|█████████▌| 1954/2048 [00:00<00:00, 14838.46it/s, est. speed input: 15195385.99 toks/s, output: 14838.68 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:00<00:00, 14838.46it/s, est. speed input: 6572734.22 toks/s, output: 6418.59 toks/s]  
Processed prompts: 100%|██████████| 2048/2048 [00:00<00:00, 6416.34it/s, est. speed input: 6572734.22 toks/s, output: 6418.59 toks/s] 
[rank0]:[W126 13:54:05.768474129 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 43.6s

测试结果:
  Requests/s:   436.34
  Tokens/s:     447248.66
  Total Reqs:   2048
  Elapsed:      4.69s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     446812.32

============================================================
[7/8] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:54:32 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=536868) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=536868) WARNING 01-26 13:54:45 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=536868) WARNING 01-26 13:54:51 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 461.06 requests/s, 472586.65 total tokens/s, 461.06 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-26 13:54:32] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:54:32] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:54:32] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:54:32] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:54:32] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:54:32] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:54:32] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:54:32] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:54:32] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:54:32] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:54:32] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:54:32] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:54:32] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:54:32] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:54:38] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:54:39] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:54:39] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:54:39] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:54:39] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:54:39] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:54:39] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:54:39] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:54:39] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:54:39] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:54:39] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:54:39] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:54:39] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:54:39] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=536868) [2026-01-26 13:54:40] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=536868) [2026-01-26 13:54:40] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=536868) [2026-01-26 13:54:40] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=536868) [2026-01-26 13:54:40] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=536868) [2026-01-26 13:54:40] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=536868) [2026-01-26 13:54:40] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=536868) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=536868) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.00it/s]
(EngineCore_DP0 pid=536868) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.00it/s]
(EngineCore_DP0 pid=536868) 
(EngineCore_DP0 pid=536868) [2026-01-26 13:54:41] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=536868) [2026-01-26 13:54:41] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6340608 bytes
(EngineCore_DP0 pid=536868) [2026-01-26 13:54:41] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=536868) [2026-01-26 13:54:41] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4227072 bytes
(EngineCore_DP0 pid=536868) [2026-01-26 13:54:41] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=536868) [2026-01-26 13:54:41] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 33816576 bytes
(EngineCore_DP0 pid=536868) [2026-01-26 13:54:41] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=536868) [2026-01-26 13:54:41] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16809984 bytes
(EngineCore_DP0 pid=536868) [rank0]:W0126 13:54:48.588000 536868 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=536868) [rank0]:W0126 13:54:48.658000 536868 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=536868) [rank0]:W0126 13:54:49.467000 536868 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=536868) [rank0]:W0126 13:54:49.566000 536868 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=536868) 2026-01-26 13:54:51,885 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=536868) 2026-01-26 13:54:51,910 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=536868) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:00, 19.42it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 5/11 [00:00<00:00, 20.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:00<00:00, 21.96it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 18.25it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 19.07it/s]
(EngineCore_DP0 pid=536868) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00, 16.75it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00, 18.01it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 20.49it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 19.71it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 40/4096 [00:00<00:10, 396.91it/s]
Adding requests:   2%|▏         | 84/4096 [00:00<00:09, 419.24it/s]
Adding requests:   3%|▎         | 129/4096 [00:00<00:09, 428.49it/s]
Adding requests:   4%|▍         | 173/4096 [00:00<00:09, 431.72it/s]
Adding requests:   5%|▌         | 218/4096 [00:00<00:08, 436.90it/s]
Adding requests:   6%|▋         | 266/4096 [00:00<00:08, 449.45it/s]
Adding requests:   8%|▊         | 311/4096 [00:00<00:08, 447.84it/s]
Adding requests:   9%|▊         | 357/4096 [00:00<00:08, 451.40it/s]
Adding requests:  10%|▉         | 404/4096 [00:00<00:08, 454.96it/s]
Adding requests:  11%|█         | 451/4096 [00:01<00:07, 456.70it/s]
Adding requests:  12%|█▏        | 498/4096 [00:01<00:07, 458.91it/s]
Adding requests:  13%|█▎        | 544/4096 [00:01<00:07, 451.72it/s]
Adding requests:  14%|█▍        | 593/4096 [00:01<00:07, 461.74it/s]
Adding requests:  16%|█▌        | 641/4096 [00:01<00:07, 464.88it/s]
Adding requests:  17%|█▋        | 690/4096 [00:01<00:07, 471.64it/s]
Adding requests:  18%|█▊        | 739/4096 [00:01<00:07, 474.88it/s]
Adding requests:  19%|█▉        | 787/4096 [00:01<00:07, 471.52it/s]
Adding requests:  20%|██        | 835/4096 [00:01<00:07, 459.08it/s]
Adding requests:  22%|██▏       | 881/4096 [00:01<00:07, 449.93it/s]
Adding requests:  23%|██▎       | 929/4096 [00:02<00:06, 456.99it/s]
Adding requests:  24%|██▍       | 977/4096 [00:02<00:06, 462.09it/s]
Adding requests:  25%|██▌       | 1025/4096 [00:02<00:06, 466.99it/s]
Adding requests:  26%|██▌       | 1072/4096 [00:02<00:06, 462.91it/s]
Adding requests:  27%|██▋       | 1119/4096 [00:02<00:06, 463.32it/s]
Adding requests:  29%|██▊       | 1169/4096 [00:02<00:06, 470.80it/s]
Adding requests:  30%|██▉       | 1217/4096 [00:02<00:06, 472.07it/s]
Adding requests:  31%|███       | 1265/4096 [00:02<00:06, 467.25it/s]
Adding requests:  32%|███▏      | 1313/4096 [00:02<00:05, 469.49it/s]
Adding requests:  33%|███▎      | 1362/4096 [00:02<00:05, 474.04it/s]
Adding requests:  34%|███▍      | 1412/4096 [00:03<00:05, 479.82it/s]
Adding requests:  36%|███▌      | 1460/4096 [00:03<00:05, 477.59it/s]
Adding requests:  37%|███▋      | 1509/4096 [00:03<00:05, 479.10it/s]
Adding requests:  38%|███▊      | 1557/4096 [00:03<00:05, 476.76it/s]
Adding requests:  39%|███▉      | 1607/4096 [00:03<00:05, 481.34it/s]
Adding requests:  40%|████      | 1656/4096 [00:03<00:05, 477.05it/s]
Adding requests:  42%|████▏     | 1704/4096 [00:03<00:05, 473.91it/s]
Adding requests:  43%|████▎     | 1752/4096 [00:03<00:04, 473.28it/s]
Adding requests:  44%|████▍     | 1800/4096 [00:03<00:04, 473.08it/s]
Adding requests:  45%|████▌     | 1848/4096 [00:03<00:04, 472.76it/s]
Adding requests:  46%|████▋     | 1896/4096 [00:04<00:04, 453.88it/s]
Adding requests:  47%|████▋     | 1942/4096 [00:04<00:04, 444.77it/s]
Adding requests:  49%|████▊     | 1989/4096 [00:04<00:04, 451.15it/s]
Adding requests:  50%|████▉     | 2037/4096 [00:04<00:04, 457.64it/s]
Adding requests:  51%|█████     | 2085/4096 [00:04<00:04, 463.55it/s]
Adding requests:  52%|█████▏    | 2132/4096 [00:04<00:04, 460.63it/s]
Adding requests:  53%|█████▎    | 2179/4096 [00:04<00:04, 457.43it/s]
Adding requests:  54%|█████▍    | 2226/4096 [00:04<00:04, 460.99it/s]
Adding requests:  55%|█████▌    | 2273/4096 [00:04<00:03, 463.48it/s]
Adding requests:  57%|█████▋    | 2321/4096 [00:05<00:03, 467.57it/s]
Adding requests:  58%|█████▊    | 2368/4096 [00:05<00:03, 466.87it/s]
Adding requests:  59%|█████▉    | 2415/4096 [00:05<00:03, 467.75it/s]
Adding requests:  60%|██████    | 2463/4096 [00:05<00:03, 469.11it/s]
Adding requests:  61%|██████▏   | 2510/4096 [00:05<00:03, 469.02it/s]
Adding requests:  62%|██████▏   | 2559/4096 [00:05<00:03, 474.18it/s]
Adding requests:  64%|██████▎   | 2607/4096 [00:05<00:03, 472.90it/s]
Adding requests:  65%|██████▍   | 2656/4096 [00:05<00:03, 477.25it/s]
Adding requests:  66%|██████▌   | 2704/4096 [00:05<00:02, 470.02it/s]
Adding requests:  67%|██████▋   | 2752/4096 [00:05<00:02, 470.78it/s]
Adding requests:  68%|██████▊   | 2800/4096 [00:06<00:02, 467.23it/s]
Adding requests:  70%|██████▉   | 2847/4096 [00:06<00:02, 466.00it/s]
Adding requests:  71%|███████   | 2895/4096 [00:06<00:02, 468.19it/s]
Adding requests:  72%|███████▏  | 2942/4096 [00:06<00:02, 465.64it/s]
Adding requests:  73%|███████▎  | 2991/4096 [00:06<00:02, 470.92it/s]
Adding requests:  74%|███████▍  | 3039/4096 [00:06<00:02, 470.04it/s]
Adding requests:  75%|███████▌  | 3087/4096 [00:06<00:02, 459.36it/s]
Adding requests:  77%|███████▋  | 3135/4096 [00:06<00:02, 463.84it/s]
Adding requests:  78%|███████▊  | 3183/4096 [00:06<00:01, 464.06it/s]
Adding requests:  79%|███████▉  | 3231/4096 [00:06<00:01, 467.01it/s]
Adding requests:  80%|████████  | 3279/4096 [00:07<00:01, 469.04it/s]
Adding requests:  81%|████████  | 3326/4096 [00:07<00:01, 468.66it/s]
Adding requests:  82%|████████▏ | 3374/4096 [00:07<00:01, 471.14it/s]
Adding requests:  84%|████████▎ | 3423/4096 [00:07<00:01, 475.37it/s]
Adding requests:  85%|████████▍ | 3471/4096 [00:07<00:01, 464.06it/s]
Adding requests:  86%|████████▌ | 3519/4096 [00:07<00:01, 468.21it/s]
Adding requests:  87%|████████▋ | 3566/4096 [00:07<00:01, 462.57it/s]
Adding requests:  88%|████████▊ | 3613/4096 [00:07<00:01, 463.49it/s]
Adding requests:  89%|████████▉ | 3660/4096 [00:07<00:00, 461.22it/s]
Adding requests:  91%|█████████ | 3709/4096 [00:07<00:00, 467.67it/s]
Adding requests:  92%|█████████▏| 3756/4096 [00:08<00:00, 467.99it/s]
Adding requests:  93%|█████████▎| 3805/4096 [00:08<00:00, 474.49it/s]
Adding requests:  94%|█████████▍| 3854/4096 [00:08<00:00, 477.26it/s]
Adding requests:  95%|█████████▌| 3902/4096 [00:08<00:00, 477.81it/s]
Adding requests:  96%|█████████▋| 3950/4096 [00:08<00:00, 462.63it/s]
Adding requests:  98%|█████████▊| 3997/4096 [00:08<00:00, 462.33it/s]
Adding requests:  99%|█████████▊| 4044/4096 [00:08<00:00, 463.40it/s]
Adding requests: 100%|█████████▉| 4092/4096 [00:08<00:00, 468.10it/s]
Adding requests: 100%|██████████| 4096/4096 [00:08<00:00, 464.89it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:00<00:00, 57493.71it/s, est. speed input: 58885781.28 toks/s, output: 57501.80 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:00<00:00, 57408.99it/s, est. speed input: 58885781.28 toks/s, output: 57501.80 toks/s]
[rank0]:[W126 13:55:03.877534291 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 58.2s

测试结果:
  Requests/s:   461.06
  Tokens/s:     472586.65
  Total Reqs:   4096
  Elapsed:      8.88s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     472125.59

============================================================
[8/8] 测试 M=65536
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     Llama3.2-1B-INT8                                │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 65536
│   M_prefill     = 65536 (= 64 x 1024)
│   M_decode      = 64
│   batched_tokens = 65536 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 8192
│   --max-num-seqs           = 64
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 65536
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-26 13:55:48 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=538464) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=538464) WARNING 01-26 13:56:01 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=538464) WARNING 01-26 13:56:08 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 474.51 requests/s, 486369.11 total tokens/s, 474.51 output tokens/s
Total num prompt tokens:  8388608
Total num output tokens:  8192


─── STDERR ───
[2026-01-26 13:55:48] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:55:48] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:55:48] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:55:48] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:55:48] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:55:48] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:55:48] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:55:48] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:55:48] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:55:48] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:55:48] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:55:48] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:55:48] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:55:48] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-26 13:55:55] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-26 13:55:55] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'Llama3.2-1B-INT8'
[2026-01-26 13:55:55] INFO kernels.py:109: Loaded tuned kernel for model: Llama3.2-1B-INT8
[2026-01-26 13:55:55] INFO kernels.py:155: Dequant+bias kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:55:55] INFO kernels.py:224: FP8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:55:55] INFO kernels.py:348: INT8 quant kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:55:55] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:55:55] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: Llama3.2-1B-INT8
[2026-01-26 13:55:55] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'Llama3.2-1B-INT8'
[2026-01-26 13:55:55] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-26 13:55:55] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-26 13:55:55] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-26 13:55:55] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-26 13:55:55] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=538464) [2026-01-26 13:55:56] INFO SlideSparseLinearMethod_INT8.py:805: Wrapping CompressedTensorsW8A8Int8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=538464) [2026-01-26 13:55:56] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=538464) [2026-01-26 13:55:56] INFO SlideSparseLinearMethod_INT8.py:452: SlideSparseInt8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=538464) [2026-01-26 13:55:56] INFO SlideSparseLinearMethod_INT8.py:604: SlideSparseInt8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=538464) [2026-01-26 13:55:56] INFO SlideSparseLinearMethod_INT8.py:621: Preloaded INT8 Triton kernels for model: Llama3.2-1B-INT8
(EngineCore_DP0 pid=538464) [2026-01-26 13:55:56] INFO SlideSparseLinearMethod_INT8.py:625: SlideSparseInt8LinearMethod initialized, kernel=cuSPARSELt, symmetric=True
(EngineCore_DP0 pid=538464) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=538464) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.03it/s]
(EngineCore_DP0 pid=538464) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.03it/s]
(EngineCore_DP0 pid=538464) 
(EngineCore_DP0 pid=538464) [2026-01-26 13:55:57] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [3072, 3296] -> 1D uint8
(EngineCore_DP0 pid=538464) [2026-01-26 13:55:57] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 6340608 bytes
(EngineCore_DP0 pid=538464) [2026-01-26 13:55:57] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 3296] -> 1D uint8
(EngineCore_DP0 pid=538464) [2026-01-26 13:55:57] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 4227072 bytes
(EngineCore_DP0 pid=538464) [2026-01-26 13:55:57] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [16384, 3296] -> 1D uint8
(EngineCore_DP0 pid=538464) [2026-01-26 13:55:57] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 33816576 bytes
(EngineCore_DP0 pid=538464) [2026-01-26 13:55:57] INFO SlideSparseLinearMethod_INT8.py:719: cuSPARSELt INT8 compression: [2048, 13120] -> 1D uint8
(EngineCore_DP0 pid=538464) [2026-01-26 13:55:57] INFO SlideSparseLinearMethod_INT8.py:729: cuSPARSELt INT8 compression done: 16809984 bytes
(EngineCore_DP0 pid=538464) [rank0]:W0126 13:56:04.765000 538464 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=538464) [rank0]:W0126 13:56:04.835000 538464 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=538464) [rank0]:W0126 13:56:05.646000 538464 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=538464) [rank0]:W0126 13:56:05.745000 538464 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=538464) 2026-01-26 13:56:08,109 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=538464) 2026-01-26 13:56:08,134 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=538464) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|█         | 2/19 [00:00<00:00, 17.69it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  26%|██▋       | 5/19 [00:00<00:00, 19.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|████▏     | 8/19 [00:00<00:00, 20.74it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|█████▊    | 11/19 [00:00<00:00, 20.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|███████▎  | 14/19 [00:00<00:00, 21.18it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  89%|████████▉ | 17/19 [00:00<00:00, 21.89it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 19/19 [00:00<00:00, 20.25it/s]
(EngineCore_DP0 pid=538464) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  18%|█▊        | 2/11 [00:00<00:00, 18.02it/s]
Capturing CUDA graphs (decode, FULL):  36%|███▋      | 4/11 [00:00<00:00, 17.93it/s]
Capturing CUDA graphs (decode, FULL):  64%|██████▎   | 7/11 [00:00<00:00, 20.66it/s]
Capturing CUDA graphs (decode, FULL):  91%|█████████ | 10/11 [00:00<00:00, 21.93it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 11/11 [00:00<00:00, 21.18it/s]

Adding requests:   0%|          | 0/8192 [00:00<?, ?it/s]
Adding requests:   0%|          | 40/8192 [00:00<00:20, 399.04it/s]
Adding requests:   1%|          | 85/8192 [00:00<00:18, 427.23it/s]
Adding requests:   2%|▏         | 129/8192 [00:00<00:18, 432.49it/s]
Adding requests:   2%|▏         | 173/8192 [00:00<00:18, 434.70it/s]
Adding requests:   3%|▎         | 218/8192 [00:00<00:18, 439.49it/s]
Adding requests:   3%|▎         | 266/8192 [00:00<00:17, 453.21it/s]
Adding requests:   4%|▍         | 312/8192 [00:00<00:17, 452.03it/s]
Adding requests:   4%|▍         | 359/8192 [00:00<00:17, 456.03it/s]
Adding requests:   5%|▍         | 407/8192 [00:00<00:16, 461.51it/s]
Adding requests:   6%|▌         | 454/8192 [00:01<00:16, 460.07it/s]
Adding requests:   6%|▌         | 501/8192 [00:01<00:16, 461.74it/s]
Adding requests:   7%|▋         | 548/8192 [00:01<00:16, 457.52it/s]
Adding requests:   7%|▋         | 597/8192 [00:01<00:16, 466.30it/s]
Adding requests:   8%|▊         | 646/8192 [00:01<00:16, 470.31it/s]
Adding requests:   8%|▊         | 696/8192 [00:01<00:15, 476.82it/s]
Adding requests:   9%|▉         | 744/8192 [00:01<00:15, 476.12it/s]
Adding requests:  10%|▉         | 792/8192 [00:01<00:15, 474.86it/s]
Adding requests:  10%|█         | 840/8192 [00:01<00:15, 465.23it/s]
Adding requests:  11%|█         | 890/8192 [00:01<00:15, 473.02it/s]
Adding requests:  11%|█▏        | 939/8192 [00:02<00:15, 475.45it/s]
Adding requests:  12%|█▏        | 988/8192 [00:02<00:15, 477.36it/s]
Adding requests:  13%|█▎        | 1037/8192 [00:02<00:14, 479.26it/s]
Adding requests:  13%|█▎        | 1085/8192 [00:02<00:14, 475.20it/s]
Adding requests:  14%|█▍        | 1133/8192 [00:02<00:14, 472.70it/s]
Adding requests:  14%|█▍        | 1183/8192 [00:02<00:14, 479.91it/s]
Adding requests:  15%|█▌        | 1233/8192 [00:02<00:14, 483.44it/s]
Adding requests:  16%|█▌        | 1282/8192 [00:02<00:14, 478.35it/s]
Adding requests:  16%|█▌        | 1331/8192 [00:02<00:14, 480.94it/s]
Adding requests:  17%|█▋        | 1380/8192 [00:02<00:14, 482.17it/s]
Adding requests:  17%|█▋        | 1429/8192 [00:03<00:14, 483.05it/s]
Adding requests:  18%|█▊        | 1478/8192 [00:03<00:13, 484.56it/s]
Adding requests:  19%|█▊        | 1527/8192 [00:03<00:14, 473.89it/s]
Adding requests:  19%|█▉        | 1575/8192 [00:03<00:14, 468.02it/s]
Adding requests:  20%|█▉        | 1624/8192 [00:03<00:13, 473.03it/s]
Adding requests:  20%|██        | 1672/8192 [00:03<00:13, 470.50it/s]
Adding requests:  21%|██        | 1721/8192 [00:03<00:13, 475.76it/s]
Adding requests:  22%|██▏       | 1769/8192 [00:03<00:13, 474.38it/s]
Adding requests:  22%|██▏       | 1818/8192 [00:03<00:13, 477.70it/s]
Adding requests:  23%|██▎       | 1866/8192 [00:03<00:13, 475.14it/s]
Adding requests:  23%|██▎       | 1914/8192 [00:04<00:13, 476.34it/s]
Adding requests:  24%|██▍       | 1963/8192 [00:04<00:13, 478.19it/s]
Adding requests:  25%|██▍       | 2012/8192 [00:04<00:12, 481.43it/s]
Adding requests:  25%|██▌       | 2061/8192 [00:04<00:12, 483.25it/s]
Adding requests:  26%|██▌       | 2110/8192 [00:04<00:12, 485.21it/s]
Adding requests:  26%|██▋       | 2159/8192 [00:04<00:12, 475.08it/s]
Adding requests:  27%|██▋       | 2207/8192 [00:04<00:12, 472.99it/s]
Adding requests:  28%|██▊       | 2257/8192 [00:04<00:12, 480.27it/s]
Adding requests:  28%|██▊       | 2306/8192 [00:04<00:12, 479.53it/s]
Adding requests:  29%|██▊       | 2354/8192 [00:04<00:12, 479.28it/s]
Adding requests:  29%|██▉       | 2402/8192 [00:05<00:12, 477.78it/s]
Adding requests:  30%|██▉       | 2451/8192 [00:05<00:11, 479.72it/s]
Adding requests:  31%|███       | 2500/8192 [00:05<00:11, 481.27it/s]
Adding requests:  31%|███       | 2549/8192 [00:05<00:11, 481.97it/s]
Adding requests:  32%|███▏      | 2598/8192 [00:05<00:11, 472.79it/s]
Adding requests:  32%|███▏      | 2647/8192 [00:05<00:11, 477.03it/s]
Adding requests:  33%|███▎      | 2695/8192 [00:05<00:11, 476.71it/s]
Adding requests:  33%|███▎      | 2743/8192 [00:05<00:11, 476.42it/s]
Adding requests:  34%|███▍      | 2791/8192 [00:05<00:11, 473.58it/s]
Adding requests:  35%|███▍      | 2839/8192 [00:06<00:11, 464.55it/s]
Adding requests:  35%|███▌      | 2886/8192 [00:06<00:11, 462.05it/s]
Adding requests:  36%|███▌      | 2933/8192 [00:06<00:11, 461.95it/s]
Adding requests:  36%|███▋      | 2982/8192 [00:06<00:11, 467.02it/s]
Adding requests:  37%|███▋      | 3030/8192 [00:06<00:11, 469.19it/s]
Adding requests:  38%|███▊      | 3077/8192 [00:06<00:10, 468.75it/s]
Adding requests:  38%|███▊      | 3126/8192 [00:06<00:10, 474.56it/s]
Adding requests:  39%|███▊      | 3174/8192 [00:06<00:10, 472.59it/s]
Adding requests:  39%|███▉      | 3222/8192 [00:06<00:10, 472.22it/s]
Adding requests:  40%|███▉      | 3271/8192 [00:06<00:10, 476.80it/s]
Adding requests:  41%|████      | 3320/8192 [00:07<00:10, 477.94it/s]
Adding requests:  41%|████      | 3370/8192 [00:07<00:09, 482.48it/s]
Adding requests:  42%|████▏     | 3419/8192 [00:07<00:09, 482.60it/s]
Adding requests:  42%|████▏     | 3468/8192 [00:07<00:09, 476.90it/s]
Adding requests:  43%|████▎     | 3516/8192 [00:07<00:09, 476.64it/s]
Adding requests:  44%|████▎     | 3564/8192 [00:07<00:09, 475.01it/s]
Adding requests:  44%|████▍     | 3612/8192 [00:07<00:09, 475.94it/s]
Adding requests:  45%|████▍     | 3660/8192 [00:07<00:09, 473.38it/s]
Adding requests:  45%|████▌     | 3709/8192 [00:07<00:09, 477.80it/s]
Adding requests:  46%|████▌     | 3757/8192 [00:07<00:09, 476.56it/s]
Adding requests:  46%|████▋     | 3805/8192 [00:08<00:09, 470.16it/s]
Adding requests:  47%|████▋     | 3855/8192 [00:08<00:09, 476.53it/s]
Adding requests:  48%|████▊     | 3903/8192 [00:08<00:08, 476.83it/s]
Adding requests:  48%|████▊     | 3952/8192 [00:08<00:08, 477.80it/s]
Adding requests:  49%|████▉     | 4000/8192 [00:08<00:08, 475.67it/s]
Adding requests:  49%|████▉     | 4048/8192 [00:08<00:08, 475.27it/s]
Adding requests:  50%|█████     | 4097/8192 [00:08<00:08, 476.83it/s]
Adding requests:  51%|█████     | 4145/8192 [00:08<00:08, 471.43it/s]
Adding requests:  51%|█████     | 4195/8192 [00:08<00:08, 477.68it/s]
Adding requests:  52%|█████▏    | 4244/8192 [00:08<00:08, 480.14it/s]
Adding requests:  52%|█████▏    | 4293/8192 [00:09<00:08, 477.06it/s]
Adding requests:  53%|█████▎    | 4344/8192 [00:09<00:07, 484.80it/s]
Adding requests:  54%|█████▎    | 4394/8192 [00:09<00:07, 488.36it/s]
Adding requests:  54%|█████▍    | 4443/8192 [00:09<00:07, 488.20it/s]
Adding requests:  55%|█████▍    | 4492/8192 [00:09<00:07, 481.87it/s]
Adding requests:  55%|█████▌    | 4541/8192 [00:09<00:07, 481.53it/s]
Adding requests:  56%|█████▌    | 4590/8192 [00:09<00:07, 483.02it/s]
Adding requests:  57%|█████▋    | 4640/8192 [00:09<00:07, 487.11it/s]
Adding requests:  57%|█████▋    | 4689/8192 [00:09<00:07, 480.09it/s]
Adding requests:  58%|█████▊    | 4740/8192 [00:09<00:07, 485.56it/s]
Adding requests:  58%|█████▊    | 4789/8192 [00:10<00:07, 484.06it/s]
Adding requests:  59%|█████▉    | 4838/8192 [00:10<00:06, 484.22it/s]
Adding requests:  60%|█████▉    | 4887/8192 [00:10<00:06, 478.95it/s]
Adding requests:  60%|██████    | 4936/8192 [00:10<00:06, 480.73it/s]
Adding requests:  61%|██████    | 4985/8192 [00:10<00:06, 466.76it/s]
Adding requests:  61%|██████▏   | 5035/8192 [00:10<00:06, 474.52it/s]
Adding requests:  62%|██████▏   | 5084/8192 [00:10<00:06, 478.93it/s]
Adding requests:  63%|██████▎   | 5134/8192 [00:10<00:06, 484.19it/s]
Adding requests:  63%|██████▎   | 5183/8192 [00:10<00:06, 481.46it/s]
Adding requests:  64%|██████▍   | 5232/8192 [00:11<00:06, 481.77it/s]
Adding requests:  64%|██████▍   | 5281/8192 [00:11<00:06, 479.25it/s]
Adding requests:  65%|██████▌   | 5332/8192 [00:11<00:05, 485.46it/s]
Adding requests:  66%|██████▌   | 5381/8192 [00:11<00:05, 484.81it/s]
Adding requests:  66%|██████▋   | 5431/8192 [00:11<00:05, 486.75it/s]
Adding requests:  67%|██████▋   | 5480/8192 [00:11<00:05, 481.60it/s]
Adding requests:  67%|██████▋   | 5529/8192 [00:11<00:05, 479.62it/s]
Adding requests:  68%|██████▊   | 5578/8192 [00:11<00:05, 481.25it/s]
Adding requests:  69%|██████▊   | 5627/8192 [00:11<00:05, 477.24it/s]
Adding requests:  69%|██████▉   | 5675/8192 [00:11<00:05, 475.35it/s]
Adding requests:  70%|██████▉   | 5725/8192 [00:12<00:05, 480.55it/s]
Adding requests:  70%|███████   | 5774/8192 [00:12<00:05, 482.10it/s]
Adding requests:  71%|███████   | 5823/8192 [00:12<00:05, 473.46it/s]
Adding requests:  72%|███████▏  | 5872/8192 [00:12<00:04, 476.87it/s]
Adding requests:  72%|███████▏  | 5922/8192 [00:12<00:04, 480.67it/s]
Adding requests:  73%|███████▎  | 5971/8192 [00:12<00:04, 480.67it/s]
Adding requests:  73%|███████▎  | 6021/8192 [00:12<00:04, 484.08it/s]
Adding requests:  74%|███████▍  | 6071/8192 [00:12<00:04, 488.74it/s]
Adding requests:  75%|███████▍  | 6120/8192 [00:12<00:04, 472.58it/s]
Adding requests:  75%|███████▌  | 6168/8192 [00:12<00:04, 473.71it/s]
Adding requests:  76%|███████▌  | 6219/8192 [00:13<00:04, 483.57it/s]
Adding requests:  77%|███████▋  | 6270/8192 [00:13<00:03, 488.46it/s]
Adding requests:  77%|███████▋  | 6320/8192 [00:13<00:03, 491.37it/s]
Adding requests:  78%|███████▊  | 6370/8192 [00:13<00:03, 492.31it/s]
Adding requests:  78%|███████▊  | 6420/8192 [00:13<00:03, 491.43it/s]
Adding requests:  79%|███████▉  | 6470/8192 [00:13<00:03, 493.64it/s]
Adding requests:  80%|███████▉  | 6521/8192 [00:13<00:03, 498.40it/s]
Adding requests:  80%|████████  | 6571/8192 [00:13<00:03, 496.93it/s]
Adding requests:  81%|████████  | 6621/8192 [00:13<00:03, 490.34it/s]
Adding requests:  81%|████████▏ | 6671/8192 [00:13<00:03, 488.34it/s]
Adding requests:  82%|████████▏ | 6721/8192 [00:14<00:03, 488.93it/s]
Adding requests:  83%|████████▎ | 6770/8192 [00:14<00:02, 487.26it/s]
Adding requests:  83%|████████▎ | 6821/8192 [00:14<00:02, 492.18it/s]
Adding requests:  84%|████████▍ | 6872/8192 [00:14<00:02, 496.63it/s]
Adding requests:  84%|████████▍ | 6922/8192 [00:14<00:02, 494.44it/s]
Adding requests:  85%|████████▌ | 6973/8192 [00:14<00:02, 498.05it/s]
Adding requests:  86%|████████▌ | 7023/8192 [00:14<00:02, 489.42it/s]
Adding requests:  86%|████████▋ | 7072/8192 [00:14<00:02, 487.89it/s]
Adding requests:  87%|████████▋ | 7122/8192 [00:14<00:02, 491.36it/s]
Adding requests:  88%|████████▊ | 7172/8192 [00:15<00:02, 486.97it/s]
Adding requests:  88%|████████▊ | 7221/8192 [00:15<00:01, 486.61it/s]
Adding requests:  89%|████████▉ | 7272/8192 [00:15<00:01, 490.50it/s]
Adding requests:  89%|████████▉ | 7322/8192 [00:15<00:01, 479.53it/s]
Adding requests:  90%|████████▉ | 7371/8192 [00:15<00:01, 479.46it/s]
Adding requests:  91%|█████████ | 7422/8192 [00:15<00:01, 486.02it/s]
Adding requests:  91%|█████████ | 7472/8192 [00:15<00:01, 489.68it/s]
Adding requests:  92%|█████████▏| 7522/8192 [00:15<00:01, 487.62it/s]
Adding requests:  92%|█████████▏| 7571/8192 [00:15<00:01, 487.30it/s]
Adding requests:  93%|█████████▎| 7620/8192 [00:15<00:01, 483.32it/s]
Adding requests:  94%|█████████▎| 7671/8192 [00:16<00:01, 488.96it/s]
Adding requests:  94%|█████████▍| 7720/8192 [00:16<00:00, 488.25it/s]
Adding requests:  95%|█████████▍| 7769/8192 [00:16<00:00, 484.55it/s]
Adding requests:  95%|█████████▌| 7818/8192 [00:16<00:00, 482.57it/s]
Adding requests:  96%|█████████▌| 7867/8192 [00:16<00:00, 483.92it/s]
Adding requests:  97%|█████████▋| 7916/8192 [00:16<00:00, 479.48it/s]
Adding requests:  97%|█████████▋| 7964/8192 [00:16<00:00, 479.00it/s]
Adding requests:  98%|█████████▊| 8012/8192 [00:16<00:00, 476.26it/s]
Adding requests:  98%|█████████▊| 8060/8192 [00:16<00:00, 476.65it/s]
Adding requests:  99%|█████████▉| 8110/8192 [00:16<00:00, 481.51it/s]
Adding requests: 100%|█████████▉| 8159/8192 [00:17<00:00, 481.82it/s]
Adding requests: 100%|██████████| 8192/8192 [00:17<00:00, 478.41it/s]

Processed prompts:   0%|          | 0/8192 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  71%|███████▏  | 5842/8192 [00:00<00:00, 58384.70it/s, est. speed input: 59790490.56 toks/s, output: 58385.81 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [00:00<00:00, 58384.70it/s, est. speed input: 60871562.93 toks/s, output: 59442.73 toks/s]
Processed prompts: 100%|██████████| 8192/8192 [00:00<00:00, 59395.46it/s, est. speed input: 60871562.93 toks/s, output: 59442.73 toks/s]
[rank0]:[W126 13:56:28.174789904 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 85.4s

测试结果:
  Requests/s:   474.51
  Tokens/s:     486369.11
  Total Reqs:   8192
  Elapsed:      17.26s

  [Prefill 分析]
  Total Prefill Tokens: 8388608
  Prefill Tokens/s:     485894.60


------------------------------------------------------------
  生成 CSV: Llama3.2-1B-INT8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_INT8_py312_cu129_x86_64/cusparselt/2_10/Llama3.2-1B-INT8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,42.8383,21976.0463,2.9880
1024,1024,1,128,128,41.0587,42085.1923,3.1175
2048,1024,2,256,128,77.2334,79164.1906,3.3146
4096,1024,4,512,128,161.3235,165356.5729,3.1737
8192,1024,8,1024,128,302.6099,310175.1310,3.3839
16384,1024,16,2048,128,436.3402,447248.6606,4.6936
32768,1024,32,4096,128,461.0601,472586.6464,8.8839
65536,1024,64,8192,128,474.5064,486369.1063,17.2643

------------------------------------------------------------

[INFO] 完成: 8 成功, 0 失败


============================================================
  Benchmark 完成!
============================================================


总计: 40 成功, 0 失败
============================================================
