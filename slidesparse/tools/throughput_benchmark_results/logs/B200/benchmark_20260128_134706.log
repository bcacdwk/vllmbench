======================================================================
SlideSparse vLLM Throughput Benchmark Log
Created: 2026-01-28 13:47:06
======================================================================

原始命令:
  /root/vllmbench/slidesparse/tools/throughput_benchmark.py --model bitnet1.58-2b-fp8 --backend cublaslt,cusparselt --stage prefill --sparsity 2_4,2_6,2_8,2_10 --M 512,1024,2048,4096,8192,16384,32768

命令行参数:
  --model: bitnet1.58-2b-fp8
  --backend: cublaslt,cusparselt
  --sparsity: 2_4,2_6,2_8,2_10
  --stage: prefill
  --M: 512,1024,2048,4096,8192,16384,32768
  --N: None
  --inner-32: False
  --eager: False
  --gpu-id: 0
  --gpu-mem: 0.8
  --dry-run: False
  --list-models: False

硬件信息:
  GPU: B200
  Compute Capability: cc100
  VRAM: 178.4 GB
  CUDA: 12.9
  Python: py312

Backend 环境变量 (初始状态):
  DISABLE_SLIDESPARSE: 未设置
  USE_CUBLASLT: 未设置
  USE_CUSPARSELT: 未设置
  SPARSITY: 未设置
  INNER_DTYPE_32: 未设置

======================================================================


============================================================
  BitNet-2B-FP8 | cuBLASLt | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints/BitNet-2B-FP8
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cublaslt

============================================================
[1/7] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuBLASLt                                        │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 13:47:13 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3250109) WARNING 01-28 13:47:27 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=3250109) WARNING 01-28 13:47:38 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 32.62 requests/s, 16736.29 total tokens/s, 32.62 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-28 13:47:12] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:47:13] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 13:47:13] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 13:47:13] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:47:13] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:47:13] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:47:13] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:47:13] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:47:13] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 13:47:13] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:47:13] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:47:13] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:47:13] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:47:13] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 13:47:19] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:47:19] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 13:47:19] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 13:47:19] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:47:19] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:47:19] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:47:19] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:47:19] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:47:19] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 13:47:19] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:47:19] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:47:19] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:47:19] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:47:19] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3250109) [2026-01-28 13:47:20] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=3250109) [2026-01-28 13:47:20] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3250109) [2026-01-28 13:47:20] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=3250109) [2026-01-28 13:47:20] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3250109) [2026-01-28 13:47:20] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=3250109) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3250109) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.11it/s]
(EngineCore_DP0 pid=3250109) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.11it/s]
(EngineCore_DP0 pid=3250109) 
(EngineCore_DP0 pid=3250109) 2026-01-28 13:47:38,036 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3250109) 2026-01-28 13:47:38,064 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3250109) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  4.38it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  4.53it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  4.50it/s]
(EngineCore_DP0 pid=3250109) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 14.19it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  58%|█████▊    | 74/128 [00:00<00:00, 736.91it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 749.13it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   3%|▎         | 4/128 [00:00<00:03, 33.98it/s, est. speed input: 17396.53 toks/s, output: 33.98 toks/s]
Processed prompts:   6%|▋         | 8/128 [00:00<00:03, 34.10it/s, est. speed input: 17450.42 toks/s, output: 34.08 toks/s]
Processed prompts:   9%|▉         | 12/128 [00:00<00:03, 34.20it/s, est. speed input: 17492.00 toks/s, output: 34.16 toks/s]
Processed prompts:  12%|█▎        | 16/128 [00:00<00:03, 33.86it/s, est. speed input: 17385.72 toks/s, output: 33.96 toks/s]
Processed prompts:  16%|█▌        | 20/128 [00:00<00:03, 33.96it/s, est. speed input: 17404.82 toks/s, output: 33.99 toks/s]
Processed prompts:  19%|█▉        | 24/128 [00:00<00:03, 34.04it/s, est. speed input: 17420.47 toks/s, output: 34.02 toks/s]
Processed prompts:  22%|██▏       | 28/128 [00:00<00:02, 34.06it/s, est. speed input: 17425.62 toks/s, output: 34.03 toks/s]
Processed prompts:  25%|██▌       | 32/128 [00:00<00:02, 34.10it/s, est. speed input: 17435.12 toks/s, output: 34.05 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:01<00:02, 34.14it/s, est. speed input: 17445.53 toks/s, output: 34.07 toks/s]
Processed prompts:  31%|███▏      | 40/128 [00:01<00:02, 34.12it/s, est. speed input: 17445.76 toks/s, output: 34.07 toks/s]
Processed prompts:  34%|███▍      | 44/128 [00:01<00:02, 34.16it/s, est. speed input: 17454.36 toks/s, output: 34.09 toks/s]
Processed prompts:  38%|███▊      | 48/128 [00:01<00:02, 34.23it/s, est. speed input: 17466.39 toks/s, output: 34.11 toks/s]
Processed prompts:  41%|████      | 52/128 [00:01<00:02, 34.25it/s, est. speed input: 17473.61 toks/s, output: 34.13 toks/s]
Processed prompts:  44%|████▍     | 56/128 [00:01<00:02, 34.26it/s, est. speed input: 17478.79 toks/s, output: 34.14 toks/s]
Processed prompts:  47%|████▋     | 60/128 [00:01<00:02, 33.93it/s, est. speed input: 17445.54 toks/s, output: 34.07 toks/s]
Processed prompts:  50%|█████     | 64/128 [00:01<00:01, 34.01it/s, est. speed input: 17449.37 toks/s, output: 34.08 toks/s]
Processed prompts:  53%|█████▎    | 68/128 [00:01<00:01, 34.09it/s, est. speed input: 17455.73 toks/s, output: 34.09 toks/s]
Processed prompts:  56%|█████▋    | 72/128 [00:02<00:01, 34.15it/s, est. speed input: 17461.43 toks/s, output: 34.10 toks/s]
Processed prompts:  59%|█████▉    | 76/128 [00:02<00:01, 34.23it/s, est. speed input: 17469.35 toks/s, output: 34.12 toks/s]
Processed prompts:  62%|██████▎   | 80/128 [00:02<00:01, 34.06it/s, est. speed input: 17457.93 toks/s, output: 34.10 toks/s]
Processed prompts:  66%|██████▌   | 84/128 [00:02<00:01, 34.19it/s, est. speed input: 17467.83 toks/s, output: 34.12 toks/s]
Processed prompts:  69%|██████▉   | 88/128 [00:02<00:01, 34.21it/s, est. speed input: 17471.06 toks/s, output: 34.12 toks/s]
Processed prompts:  72%|███████▏  | 92/128 [00:02<00:01, 34.32it/s, est. speed input: 17481.16 toks/s, output: 34.14 toks/s]
Processed prompts:  75%|███████▌  | 96/128 [00:02<00:00, 34.39it/s, est. speed input: 17489.98 toks/s, output: 34.16 toks/s]
Processed prompts:  78%|███████▊  | 100/128 [00:02<00:00, 33.88it/s, est. speed input: 17459.51 toks/s, output: 34.10 toks/s]
Processed prompts:  81%|████████▏ | 104/128 [00:03<00:00, 34.02it/s, est. speed input: 17464.50 toks/s, output: 34.11 toks/s]
Processed prompts:  84%|████████▍ | 108/128 [00:03<00:00, 34.05it/s, est. speed input: 17464.62 toks/s, output: 34.11 toks/s]
Processed prompts:  88%|████████▊ | 112/128 [00:03<00:00, 34.11it/s, est. speed input: 17467.27 toks/s, output: 34.12 toks/s]
Processed prompts:  91%|█████████ | 116/128 [00:03<00:00, 34.18it/s, est. speed input: 17471.21 toks/s, output: 34.12 toks/s]
Processed prompts:  94%|█████████▍| 120/128 [00:03<00:00, 34.21it/s, est. speed input: 17473.78 toks/s, output: 34.13 toks/s]
Processed prompts:  97%|█████████▋| 124/128 [00:03<00:00, 34.08it/s, est. speed input: 17468.04 toks/s, output: 34.12 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 34.11it/s, est. speed input: 17469.15 toks/s, output: 34.12 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 34.11it/s, est. speed input: 17469.15 toks/s, output: 34.12 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 34.12it/s, est. speed input: 17469.15 toks/s, output: 34.12 toks/s]
[rank0]:[W128 13:47:44.453271443 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 39.9s

测试结果:
  Requests/s:   32.62
  Tokens/s:     16736.29
  Total Reqs:   128
  Elapsed:      3.92s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     16703.66

============================================================
[2/7] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuBLASLt                                        │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 13:47:53 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3251467) WARNING 01-28 13:48:08 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=3251467) WARNING 01-28 13:48:18 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 33.21 requests/s, 34045.18 total tokens/s, 33.21 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-28 13:47:52] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:47:53] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 13:47:53] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 13:47:53] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:47:53] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:47:53] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:47:53] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:47:53] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:47:53] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 13:47:53] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:47:53] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:47:53] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:47:53] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:47:53] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 13:47:59] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:47:59] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 13:47:59] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 13:47:59] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:47:59] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:47:59] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:47:59] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:47:59] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:47:59] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 13:47:59] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:47:59] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:47:59] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:47:59] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:47:59] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3251467) [2026-01-28 13:48:00] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=3251467) [2026-01-28 13:48:00] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3251467) [2026-01-28 13:48:00] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=3251467) [2026-01-28 13:48:00] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3251467) [2026-01-28 13:48:00] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=3251467) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3251467) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.22it/s]
(EngineCore_DP0 pid=3251467) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.22it/s]
(EngineCore_DP0 pid=3251467) 
(EngineCore_DP0 pid=3251467) 2026-01-28 13:48:18,156 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3251467) 2026-01-28 13:48:18,184 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3251467) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 14.10it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 14.08it/s]
(EngineCore_DP0 pid=3251467) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 14.17it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  32%|███▏      | 41/128 [00:00<00:00, 402.43it/s]
Adding requests:  67%|██████▋   | 86/128 [00:00<00:00, 428.88it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 430.21it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   9%|▉         | 12/128 [00:00<00:01, 107.76it/s, est. speed input: 110350.66 toks/s, output: 107.76 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:00<00:02, 47.60it/s, est. speed input: 53414.36 toks/s, output: 52.16 toks/s]   
Processed prompts:  23%|██▎       | 30/128 [00:00<00:02, 41.70it/s, est. speed input: 47320.35 toks/s, output: 46.21 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:00<00:02, 39.05it/s, est. speed input: 44616.60 toks/s, output: 43.57 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:00<00:02, 37.52it/s, est. speed input: 43082.62 toks/s, output: 42.07 toks/s]
Processed prompts:  36%|███▌      | 46/128 [00:01<00:02, 36.39it/s, est. speed input: 41942.73 toks/s, output: 40.96 toks/s]
Processed prompts:  39%|███▉      | 50/128 [00:01<00:02, 35.73it/s, est. speed input: 41238.33 toks/s, output: 40.27 toks/s]
Processed prompts:  42%|████▏     | 54/128 [00:01<00:02, 35.20it/s, est. speed input: 40653.85 toks/s, output: 39.70 toks/s]
Processed prompts:  45%|████▌     | 58/128 [00:01<00:02, 34.78it/s, est. speed input: 40157.51 toks/s, output: 39.22 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:01<00:01, 34.49it/s, est. speed input: 39741.25 toks/s, output: 38.81 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:01<00:01, 34.28it/s, est. speed input: 39384.96 toks/s, output: 38.46 toks/s]
Processed prompts:  55%|█████▍    | 70/128 [00:01<00:01, 34.18it/s, est. speed input: 39086.24 toks/s, output: 38.17 toks/s]
Processed prompts:  58%|█████▊    | 74/128 [00:01<00:01, 34.08it/s, est. speed input: 38817.28 toks/s, output: 37.91 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:02<00:01, 34.02it/s, est. speed input: 38581.38 toks/s, output: 37.68 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:02<00:01, 33.94it/s, est. speed input: 38364.34 toks/s, output: 37.47 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:02<00:01, 33.89it/s, est. speed input: 38170.76 toks/s, output: 37.28 toks/s]
Processed prompts:  70%|███████   | 90/128 [00:02<00:01, 33.50it/s, est. speed input: 37928.74 toks/s, output: 37.04 toks/s]
Processed prompts:  73%|███████▎  | 94/128 [00:02<00:01, 33.58it/s, est. speed input: 37773.60 toks/s, output: 36.89 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:02<00:00, 33.68it/s, est. speed input: 37638.30 toks/s, output: 36.76 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:02<00:00, 33.77it/s, est. speed input: 37519.35 toks/s, output: 36.64 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:02<00:00, 33.82it/s, est. speed input: 37406.23 toks/s, output: 36.53 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:03<00:00, 33.78it/s, est. speed input: 37291.90 toks/s, output: 36.42 toks/s]
Processed prompts:  89%|████████▉ | 114/128 [00:03<00:00, 33.69it/s, est. speed input: 37176.96 toks/s, output: 36.31 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:03<00:00, 33.74it/s, est. speed input: 37085.95 toks/s, output: 36.22 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:03<00:00, 33.69it/s, est. speed input: 36991.36 toks/s, output: 36.12 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:03<00:00, 33.67it/s, est. speed input: 36904.02 toks/s, output: 36.04 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 33.67it/s, est. speed input: 36868.27 toks/s, output: 36.00 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 36.00it/s, est. speed input: 36868.27 toks/s, output: 36.00 toks/s]
[rank0]:[W128 13:48:23.056875690 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 39.6s

测试结果:
  Requests/s:   33.21
  Tokens/s:     34045.18
  Total Reqs:   128
  Elapsed:      3.85s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     34011.97

============================================================
[3/7] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuBLASLt                                        │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 13:48:33 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3252633) WARNING 01-28 13:48:48 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=3252633) WARNING 01-28 13:48:58 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 65.15 requests/s, 66778.41 total tokens/s, 65.15 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-28 13:48:33] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:48:33] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 13:48:33] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 13:48:33] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:48:33] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:48:33] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:48:33] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:48:33] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:48:33] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 13:48:33] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:48:33] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:48:33] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:48:33] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:48:33] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 13:48:39] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:48:40] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 13:48:40] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 13:48:40] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:48:40] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:48:40] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:48:40] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:48:40] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:48:40] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 13:48:40] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:48:40] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:48:40] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:48:40] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:48:40] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3252633) [2026-01-28 13:48:40] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=3252633) [2026-01-28 13:48:41] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3252633) [2026-01-28 13:48:41] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=3252633) [2026-01-28 13:48:41] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3252633) [2026-01-28 13:48:41] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=3252633) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3252633) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.23it/s]
(EngineCore_DP0 pid=3252633) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.23it/s]
(EngineCore_DP0 pid=3252633) 
(EngineCore_DP0 pid=3252633) 2026-01-28 13:48:58,322 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3252633) 2026-01-28 13:48:58,351 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3252633) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:00<00:00, 11.17it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 11.03it/s]
(EngineCore_DP0 pid=3252633) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 15.13it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 15.12it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  16%|█▌        | 41/256 [00:00<00:00, 402.61it/s]
Adding requests:  34%|███▎      | 86/256 [00:00<00:00, 426.48it/s]
Adding requests:  51%|█████     | 131/256 [00:00<00:00, 436.35it/s]
Adding requests:  68%|██████▊   | 175/256 [00:00<00:00, 437.32it/s]
Adding requests:  86%|████████▋ | 221/256 [00:00<00:00, 443.96it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 442.86it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  15%|█▍        | 38/256 [00:00<00:00, 359.32it/s, est. speed input: 367973.50 toks/s, output: 359.33 toks/s]
Processed prompts:  29%|██▉       | 74/256 [00:00<00:01, 102.18it/s, est. speed input: 117593.62 toks/s, output: 114.84 toks/s]
Processed prompts:  36%|███▋      | 93/256 [00:00<00:01, 90.09it/s, est. speed input: 104328.74 toks/s, output: 101.88 toks/s] 
Processed prompts:  42%|████▏     | 107/256 [00:01<00:01, 83.46it/s, est. speed input: 97889.24 toks/s, output: 95.59 toks/s] 
Processed prompts:  46%|████▌     | 118/256 [00:01<00:01, 77.61it/s, est. speed input: 93108.65 toks/s, output: 90.93 toks/s]
Processed prompts:  50%|█████     | 128/256 [00:01<00:01, 74.85it/s, est. speed input: 90488.81 toks/s, output: 88.37 toks/s]
Processed prompts:  54%|█████▎    | 137/256 [00:01<00:01, 75.07it/s, est. speed input: 89520.86 toks/s, output: 87.42 toks/s]
Processed prompts:  57%|█████▋    | 146/256 [00:01<00:01, 71.12it/s, est. speed input: 87172.28 toks/s, output: 85.13 toks/s]
Processed prompts:  60%|██████    | 154/256 [00:01<00:01, 70.09it/s, est. speed input: 85953.18 toks/s, output: 83.94 toks/s]
Processed prompts:  63%|██████▎   | 162/256 [00:01<00:01, 69.42it/s, est. speed input: 84929.80 toks/s, output: 82.94 toks/s]
Processed prompts:  66%|██████▋   | 170/256 [00:02<00:01, 68.81it/s, est. speed input: 84000.52 toks/s, output: 82.03 toks/s]
Processed prompts:  70%|██████▉   | 178/256 [00:02<00:01, 68.41it/s, est. speed input: 83187.16 toks/s, output: 81.24 toks/s]
Processed prompts:  73%|███████▎  | 186/256 [00:02<00:01, 68.14it/s, est. speed input: 82462.97 toks/s, output: 80.53 toks/s]
Processed prompts:  76%|███████▌  | 194/256 [00:02<00:00, 67.98it/s, est. speed input: 81815.48 toks/s, output: 79.90 toks/s]
Processed prompts:  79%|███████▉  | 202/256 [00:02<00:00, 67.92it/s, est. speed input: 81240.26 toks/s, output: 79.34 toks/s]
Processed prompts:  82%|████████▏ | 210/256 [00:02<00:00, 67.09it/s, est. speed input: 80571.96 toks/s, output: 78.68 toks/s]
Processed prompts:  85%|████████▌ | 218/256 [00:02<00:00, 67.17it/s, est. speed input: 80078.51 toks/s, output: 78.20 toks/s]
Processed prompts:  88%|████████▊ | 226/256 [00:02<00:00, 67.27it/s, est. speed input: 79631.59 toks/s, output: 77.77 toks/s]
Processed prompts:  91%|█████████▏| 234/256 [00:03<00:00, 67.33it/s, est. speed input: 79218.60 toks/s, output: 77.36 toks/s]
Processed prompts:  95%|█████████▍| 242/256 [00:03<00:00, 67.31it/s, est. speed input: 78827.04 toks/s, output: 76.98 toks/s]
Processed prompts:  98%|█████████▊| 250/256 [00:03<00:00, 67.47it/s, est. speed input: 78489.96 toks/s, output: 76.65 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 67.47it/s, est. speed input: 78245.86 toks/s, output: 76.41 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 76.41it/s, est. speed input: 78245.86 toks/s, output: 76.41 toks/s]
[rank0]:[W128 13:49:04.715040179 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 40.5s

测试结果:
  Requests/s:   65.15
  Tokens/s:     66778.41
  Total Reqs:   256
  Elapsed:      3.93s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     66713.26

============================================================
[4/7] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuBLASLt                                        │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 13:49:15 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3253812) WARNING 01-28 13:49:29 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=3253812) WARNING 01-28 13:49:39 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 127.39 requests/s, 130571.73 total tokens/s, 127.39 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-28 13:49:14] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:49:15] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 13:49:15] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 13:49:15] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:49:15] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:49:15] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:49:15] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:49:15] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:49:15] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 13:49:15] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:49:15] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:49:15] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:49:15] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:49:15] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 13:49:21] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:49:21] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 13:49:21] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 13:49:21] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:49:21] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:49:21] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:49:21] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:49:21] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:49:21] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 13:49:21] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:49:21] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:49:21] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:49:21] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:49:21] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3253812) [2026-01-28 13:49:22] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=3253812) [2026-01-28 13:49:22] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3253812) [2026-01-28 13:49:22] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=3253812) [2026-01-28 13:49:22] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3253812) [2026-01-28 13:49:22] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=3253812) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3253812) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.21it/s]
(EngineCore_DP0 pid=3253812) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.21it/s]
(EngineCore_DP0 pid=3253812) 
(EngineCore_DP0 pid=3253812) 2026-01-28 13:49:39,668 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3253812) 2026-01-28 13:49:39,695 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3253812) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00, 14.70it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 13.48it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 13.65it/s]
(EngineCore_DP0 pid=3253812) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00, 15.32it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 15.48it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   8%|▊         | 40/512 [00:00<00:01, 396.83it/s]
Adding requests:  17%|█▋        | 85/512 [00:00<00:01, 423.01it/s]
Adding requests:  25%|██▌       | 128/512 [00:00<00:00, 419.76it/s]
Adding requests:  33%|███▎      | 171/512 [00:00<00:00, 421.99it/s]
Adding requests:  42%|████▏     | 216/512 [00:00<00:00, 431.28it/s]
Adding requests:  51%|█████▏    | 263/512 [00:00<00:00, 443.54it/s]
Adding requests:  60%|██████    | 308/512 [00:00<00:00, 444.19it/s]
Adding requests:  69%|██████▉   | 355/512 [00:00<00:00, 450.20it/s]
Adding requests:  79%|███████▊  | 402/512 [00:00<00:00, 454.51it/s]
Adding requests:  88%|████████▊ | 449/512 [00:01<00:00, 456.74it/s]
Adding requests:  97%|█████████▋| 496/512 [00:01<00:00, 458.97it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 445.00it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  28%|██▊       | 142/512 [00:00<00:00, 1270.45it/s, est. speed input: 1301011.05 toks/s, output: 1270.46 toks/s]
Processed prompts:  53%|█████▎    | 270/512 [00:01<00:01, 219.68it/s, est. speed input: 258710.34 toks/s, output: 252.65 toks/s]   
Processed prompts:  65%|██████▍   | 331/512 [00:01<00:00, 188.21it/s, est. speed input: 223883.28 toks/s, output: 218.64 toks/s]
Processed prompts:  72%|███████▏  | 370/512 [00:01<00:00, 171.89it/s, est. speed input: 208341.33 toks/s, output: 203.46 toks/s]
Processed prompts:  78%|███████▊  | 399/512 [00:02<00:00, 165.08it/s, est. speed input: 201595.71 toks/s, output: 196.87 toks/s]
Processed prompts:  83%|████████▎ | 423/512 [00:02<00:00, 157.87it/s, est. speed input: 195952.64 toks/s, output: 191.36 toks/s]
Processed prompts:  87%|████████▋ | 443/512 [00:02<00:00, 152.99it/s, est. speed input: 192146.51 toks/s, output: 187.64 toks/s]
Processed prompts:  90%|█████████ | 461/512 [00:02<00:00, 152.69it/s, est. speed input: 190363.01 toks/s, output: 185.90 toks/s]
Processed prompts:  93%|█████████▎| 478/512 [00:02<00:00, 143.33it/s, est. speed input: 186226.76 toks/s, output: 181.86 toks/s]
Processed prompts:  96%|█████████▋| 494/512 [00:02<00:00, 141.35it/s, est. speed input: 184113.76 toks/s, output: 179.80 toks/s]
Processed prompts: 100%|█████████▉| 510/512 [00:02<00:00, 139.46it/s, est. speed input: 182128.45 toks/s, output: 177.86 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:02<00:00, 139.46it/s, est. speed input: 182835.62 toks/s, output: 178.55 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:02<00:00, 178.55it/s, est. speed input: 182835.62 toks/s, output: 178.55 toks/s]
[rank0]:[W128 13:49:45.159789255 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 41.5s

测试结果:
  Requests/s:   127.39
  Tokens/s:     130571.73
  Total Reqs:   512
  Elapsed:      4.02s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     130444.35

============================================================
[5/7] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuBLASLt                                        │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 13:49:58 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3254997) WARNING 01-28 13:50:13 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=3254997) WARNING 01-28 13:50:23 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 236.46 requests/s, 242374.20 total tokens/s, 236.46 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-28 13:49:58] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:49:58] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 13:49:58] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 13:49:58] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:49:58] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:49:58] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:49:58] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:49:58] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:49:58] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 13:49:58] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:49:58] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:49:58] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:49:58] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:49:58] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 13:50:05] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:50:05] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 13:50:05] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 13:50:05] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:50:05] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:50:05] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:50:05] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:50:05] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:50:05] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 13:50:05] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:50:05] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:50:05] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:50:05] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:50:05] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3254997) [2026-01-28 13:50:06] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=3254997) [2026-01-28 13:50:06] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3254997) [2026-01-28 13:50:06] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=3254997) [2026-01-28 13:50:06] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3254997) [2026-01-28 13:50:06] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=3254997) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3254997) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.23it/s]
(EngineCore_DP0 pid=3254997) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.23it/s]
(EngineCore_DP0 pid=3254997) 
(EngineCore_DP0 pid=3254997) 2026-01-28 13:50:23,692 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3254997) 2026-01-28 13:50:23,722 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3254997) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00, 14.16it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00, 14.76it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 13.61it/s]
(EngineCore_DP0 pid=3254997) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00, 15.24it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 15.39it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 15.36it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   3%|▎         | 35/1024 [00:00<00:02, 346.33it/s]
Adding requests:   8%|▊         | 80/1024 [00:00<00:02, 403.00it/s]
Adding requests:  12%|█▏        | 125/1024 [00:00<00:02, 421.49it/s]
Adding requests:  17%|█▋        | 169/1024 [00:00<00:01, 428.47it/s]
Adding requests:  21%|██        | 214/1024 [00:00<00:01, 435.85it/s]
Adding requests:  26%|██▌       | 262/1024 [00:00<00:01, 449.44it/s]
Adding requests:  30%|██▉       | 307/1024 [00:00<00:01, 449.09it/s]
Adding requests:  35%|███▍      | 355/1024 [00:00<00:01, 458.04it/s]
Adding requests:  39%|███▉      | 402/1024 [00:00<00:01, 460.34it/s]
Adding requests:  44%|████▍     | 450/1024 [00:01<00:01, 464.28it/s]
Adding requests:  49%|████▊     | 497/1024 [00:01<00:01, 465.02it/s]
Adding requests:  53%|█████▎    | 544/1024 [00:01<00:01, 459.06it/s]
Adding requests:  58%|█████▊    | 590/1024 [00:01<00:00, 459.24it/s]
Adding requests:  62%|██████▏   | 638/1024 [00:01<00:00, 462.95it/s]
Adding requests:  67%|██████▋   | 687/1024 [00:01<00:00, 470.85it/s]
Adding requests:  72%|███████▏  | 737/1024 [00:01<00:00, 478.23it/s]
Adding requests:  77%|███████▋  | 785/1024 [00:01<00:00, 473.77it/s]
Adding requests:  81%|████████▏ | 833/1024 [00:01<00:00, 464.99it/s]
Adding requests:  86%|████████▌ | 881/1024 [00:01<00:00, 469.25it/s]
Adding requests:  91%|█████████ | 930/1024 [00:02<00:00, 475.35it/s]
Adding requests:  96%|█████████▌| 978/1024 [00:02<00:00, 476.04it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 460.59it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  49%|████▉     | 506/1024 [00:00<00:00, 4523.47it/s, est. speed input: 4632340.86 toks/s, output: 4523.54 toks/s]
Processed prompts:  94%|█████████▎| 959/1024 [00:01<00:00, 442.48it/s, est. speed input: 528591.96 toks/s, output: 516.20 toks/s]   
Processed prompts: 100%|██████████| 1024/1024 [00:02<00:00, 442.48it/s, est. speed input: 497925.56 toks/s, output: 486.25 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:02<00:00, 486.23it/s, est. speed input: 497925.56 toks/s, output: 486.25 toks/s]
[rank0]:[W128 13:50:30.624611298 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 44.5s

测试结果:
  Requests/s:   236.46
  Tokens/s:     242374.20
  Total Reqs:   1024
  Elapsed:      4.33s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     242137.74

============================================================
[6/7] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuBLASLt                                        │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 13:50:47 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3256342) WARNING 01-28 13:51:02 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=3256342) WARNING 01-28 13:51:12 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 284.53 requests/s, 291643.83 total tokens/s, 284.53 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-28 13:50:47] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:50:47] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 13:50:47] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 13:50:47] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:50:47] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:50:47] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:50:47] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:50:47] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:50:47] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 13:50:47] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:50:47] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:50:47] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:50:47] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:50:47] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 13:50:54] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:50:54] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 13:50:54] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 13:50:54] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:50:54] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:50:54] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:50:54] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:50:54] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:50:54] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 13:50:54] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:50:54] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:50:54] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:50:54] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:50:54] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3256342) [2026-01-28 13:50:55] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=3256342) [2026-01-28 13:50:55] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3256342) [2026-01-28 13:50:55] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=3256342) [2026-01-28 13:50:55] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3256342) [2026-01-28 13:50:55] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=3256342) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3256342) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.22it/s]
(EngineCore_DP0 pid=3256342) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.21it/s]
(EngineCore_DP0 pid=3256342) 
(EngineCore_DP0 pid=3256342) 2026-01-28 13:51:12,810 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3256342) 2026-01-28 13:51:12,885 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3256342) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00, 14.27it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00, 15.01it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:00<00:00, 15.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 14.27it/s]
(EngineCore_DP0 pid=3256342) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00, 12.25it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00, 13.46it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 13.40it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 41/2048 [00:00<00:05, 400.06it/s]
Adding requests:   4%|▍         | 86/2048 [00:00<00:04, 425.78it/s]
Adding requests:   6%|▋         | 131/2048 [00:00<00:04, 433.35it/s]
Adding requests:   9%|▊         | 175/2048 [00:00<00:04, 434.26it/s]
Adding requests:  11%|█         | 222/2048 [00:00<00:04, 444.28it/s]
Adding requests:  13%|█▎        | 270/2048 [00:00<00:03, 453.90it/s]
Adding requests:  15%|█▌        | 316/2048 [00:00<00:03, 450.92it/s]
Adding requests:  18%|█▊        | 363/2048 [00:00<00:03, 455.90it/s]
Adding requests:  20%|██        | 410/2048 [00:00<00:03, 459.30it/s]
Adding requests:  22%|██▏       | 457/2048 [00:01<00:03, 461.56it/s]
Adding requests:  25%|██▍       | 504/2048 [00:01<00:03, 461.54it/s]
Adding requests:  27%|██▋       | 551/2048 [00:01<00:03, 455.67it/s]
Adding requests:  29%|██▉       | 599/2048 [00:01<00:03, 462.39it/s]
Adding requests:  32%|███▏      | 646/2048 [00:01<00:03, 460.56it/s]
Adding requests:  34%|███▍      | 695/2048 [00:01<00:02, 467.62it/s]
Adding requests:  36%|███▋      | 743/2048 [00:01<00:02, 468.22it/s]
Adding requests:  39%|███▊      | 790/2048 [00:01<00:02, 466.88it/s]
Adding requests:  41%|████      | 837/2048 [00:01<00:02, 458.34it/s]
Adding requests:  43%|████▎     | 885/2048 [00:01<00:02, 464.06it/s]
Adding requests:  46%|████▌     | 934/2048 [00:02<00:02, 470.39it/s]
Adding requests:  48%|████▊     | 982/2048 [00:02<00:02, 470.05it/s]
Adding requests:  50%|█████     | 1031/2048 [00:02<00:02, 475.11it/s]
Adding requests:  53%|█████▎    | 1079/2048 [00:02<00:02, 470.61it/s]
Adding requests:  55%|█████▌    | 1127/2048 [00:02<00:01, 468.62it/s]
Adding requests:  57%|█████▋    | 1176/2048 [00:02<00:01, 474.34it/s]
Adding requests:  60%|█████▉    | 1226/2048 [00:02<00:01, 481.02it/s]
Adding requests:  62%|██████▏   | 1275/2048 [00:02<00:01, 474.87it/s]
Adding requests:  65%|██████▍   | 1325/2048 [00:02<00:01, 478.49it/s]
Adding requests:  67%|██████▋   | 1373/2048 [00:02<00:01, 478.86it/s]
Adding requests:  69%|██████▉   | 1422/2048 [00:03<00:01, 479.24it/s]
Adding requests:  72%|███████▏  | 1471/2048 [00:03<00:01, 480.40it/s]
Adding requests:  74%|███████▍  | 1520/2048 [00:03<00:01, 481.91it/s]
Adding requests:  77%|███████▋  | 1569/2048 [00:03<00:00, 480.76it/s]
Adding requests:  79%|███████▉  | 1618/2048 [00:03<00:00, 482.76it/s]
Adding requests:  81%|████████▏ | 1667/2048 [00:03<00:00, 479.99it/s]
Adding requests:  84%|████████▍ | 1716/2048 [00:03<00:00, 470.31it/s]
Adding requests:  86%|████████▌ | 1764/2048 [00:03<00:00, 471.71it/s]
Adding requests:  88%|████████▊ | 1812/2048 [00:03<00:00, 469.92it/s]
Adding requests:  91%|█████████ | 1860/2048 [00:03<00:00, 469.74it/s]
Adding requests:  93%|█████████▎| 1908/2048 [00:04<00:00, 470.50it/s]
Adding requests:  96%|█████████▌| 1956/2048 [00:04<00:00, 472.91it/s]
Adding requests:  98%|█████████▊| 2005/2048 [00:04<00:00, 476.21it/s]
Adding requests: 100%|██████████| 2048/2048 [00:04<00:00, 467.61it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  61%|██████    | 1250/2048 [00:00<00:00, 11196.99it/s, est. speed input: 11467606.62 toks/s, output: 11197.21 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:02<00:00, 11196.99it/s, est. speed input: 744563.44 toks/s, output: 727.11 toks/s]    
Processed prompts: 100%|██████████| 2048/2048 [00:02<00:00, 727.08it/s, est. speed input: 744563.44 toks/s, output: 727.11 toks/s]  
[rank0]:[W128 13:51:22.789271286 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 52.2s

测试结果:
  Requests/s:   284.53
  Tokens/s:     291643.83
  Total Reqs:   2048
  Elapsed:      7.20s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     291359.30

============================================================
[7/7] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuBLASLt                                        │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 13:51:49 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3257748) WARNING 01-28 13:52:04 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=3257748) WARNING 01-28 13:52:14 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 289.51 requests/s, 296747.69 total tokens/s, 289.51 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-28 13:51:48] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:51:49] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 13:51:49] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 13:51:49] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:51:49] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:51:49] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:51:49] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:51:49] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:51:49] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 13:51:49] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:51:49] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:51:49] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:51:49] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:51:49] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 13:51:55] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:51:56] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 13:51:56] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 13:51:56] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:51:56] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:51:56] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:51:56] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:51:56] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:51:56] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 13:51:56] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:51:56] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:51:56] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:51:56] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:51:56] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3257748) [2026-01-28 13:51:57] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuBLASLt)
(EngineCore_DP0 pid=3257748) [2026-01-28 13:51:57] INFO gemm_wrapper.py:870: cublaslt GEMM extension loaded: cublaslt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3257748) [2026-01-28 13:51:57] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuBLASLt)
(EngineCore_DP0 pid=3257748) [2026-01-28 13:51:57] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3257748) [2026-01-28 13:51:57] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuBLASLt
(EngineCore_DP0 pid=3257748) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3257748) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.22it/s]
(EngineCore_DP0 pid=3257748) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.22it/s]
(EngineCore_DP0 pid=3257748) 
(EngineCore_DP0 pid=3257748) [rank0]:W0128 13:52:09.624000 3257748 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3257748) [rank0]:W0128 13:52:09.709000 3257748 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3257748) [rank0]:W0128 13:52:10.661000 3257748 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3257748) [rank0]:W0128 13:52:10.791000 3257748 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3257748) 2026-01-28 13:52:14,583 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3257748) 2026-01-28 13:52:14,628 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3257748) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:00, 14.01it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:00<00:00, 14.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 6/11 [00:00<00:00, 14.90it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:00<00:00, 15.15it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 10/11 [00:00<00:00, 15.19it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 14.48it/s]
(EngineCore_DP0 pid=3257748) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00, 13.83it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00, 14.23it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00, 14.21it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 14.35it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 40/4096 [00:00<00:10, 397.71it/s]
Adding requests:   2%|▏         | 84/4096 [00:00<00:09, 421.37it/s]
Adding requests:   3%|▎         | 129/4096 [00:00<00:09, 430.73it/s]
Adding requests:   4%|▍         | 173/4096 [00:00<00:09, 433.71it/s]
Adding requests:   5%|▌         | 218/4096 [00:00<00:08, 438.56it/s]
Adding requests:   6%|▋         | 266/4096 [00:00<00:08, 450.86it/s]
Adding requests:   8%|▊         | 312/4096 [00:00<00:08, 449.07it/s]
Adding requests:   9%|▉         | 359/4096 [00:00<00:08, 452.93it/s]
Adding requests:  10%|▉         | 405/4096 [00:00<00:08, 447.66it/s]
Adding requests:  11%|█         | 452/4096 [00:01<00:08, 452.79it/s]
Adding requests:  12%|█▏        | 498/4096 [00:01<00:07, 453.19it/s]
Adding requests:  13%|█▎        | 544/4096 [00:01<00:07, 448.55it/s]
Adding requests:  14%|█▍        | 593/4096 [00:01<00:07, 458.64it/s]
Adding requests:  16%|█▌        | 641/4096 [00:01<00:07, 463.36it/s]
Adding requests:  17%|█▋        | 690/4096 [00:01<00:07, 469.34it/s]
Adding requests:  18%|█▊        | 739/4096 [00:01<00:07, 473.91it/s]
Adding requests:  19%|█▉        | 787/4096 [00:01<00:07, 470.31it/s]
Adding requests:  20%|██        | 835/4096 [00:01<00:07, 458.21it/s]
Adding requests:  22%|██▏       | 884/4096 [00:01<00:06, 465.37it/s]
Adding requests:  23%|██▎       | 931/4096 [00:02<00:06, 461.12it/s]
Adding requests:  24%|██▍       | 979/4096 [00:02<00:06, 464.12it/s]
Adding requests:  25%|██▌       | 1028/4096 [00:02<00:06, 469.18it/s]
Adding requests:  26%|██▌       | 1075/4096 [00:02<00:06, 466.07it/s]
Adding requests:  27%|██▋       | 1122/4096 [00:02<00:06, 465.06it/s]
Adding requests:  29%|██▊       | 1171/4096 [00:02<00:06, 472.15it/s]
Adding requests:  30%|██▉       | 1219/4096 [00:02<00:06, 462.74it/s]
Adding requests:  31%|███       | 1266/4096 [00:02<00:06, 461.05it/s]
Adding requests:  32%|███▏      | 1313/4096 [00:02<00:06, 462.87it/s]
Adding requests:  33%|███▎      | 1362/4096 [00:02<00:05, 468.87it/s]
Adding requests:  34%|███▍      | 1412/4096 [00:03<00:05, 475.22it/s]
Adding requests:  36%|███▌      | 1460/4096 [00:03<00:05, 474.20it/s]
Adding requests:  37%|███▋      | 1509/4096 [00:03<00:05, 477.57it/s]
Adding requests:  38%|███▊      | 1557/4096 [00:03<00:05, 476.70it/s]
Adding requests:  39%|███▉      | 1607/4096 [00:03<00:05, 482.70it/s]
Adding requests:  40%|████      | 1656/4096 [00:03<00:05, 479.37it/s]
Adding requests:  42%|████▏     | 1704/4096 [00:03<00:05, 476.68it/s]
Adding requests:  43%|████▎     | 1752/4096 [00:03<00:04, 471.07it/s]
Adding requests:  44%|████▍     | 1800/4096 [00:03<00:04, 464.40it/s]
Adding requests:  45%|████▌     | 1847/4096 [00:03<00:04, 462.30it/s]
Adding requests:  46%|████▌     | 1894/4096 [00:04<00:04, 457.77it/s]
Adding requests:  47%|████▋     | 1940/4096 [00:04<00:04, 457.70it/s]
Adding requests:  49%|████▊     | 1987/4096 [00:04<00:04, 459.51it/s]
Adding requests:  50%|████▉     | 2033/4096 [00:04<00:04, 448.56it/s]
Adding requests:  51%|█████     | 2081/4096 [00:04<00:04, 456.92it/s]
Adding requests:  52%|█████▏    | 2127/4096 [00:04<00:04, 455.69it/s]
Adding requests:  53%|█████▎    | 2173/4096 [00:04<00:04, 455.97it/s]
Adding requests:  54%|█████▍    | 2220/4096 [00:04<00:04, 457.80it/s]
Adding requests:  55%|█████▌    | 2267/4096 [00:04<00:03, 460.69it/s]
Adding requests:  57%|█████▋    | 2316/4096 [00:05<00:03, 467.74it/s]
Adding requests:  58%|█████▊    | 2363/4096 [00:05<00:03, 464.75it/s]
Adding requests:  59%|█████▉    | 2410/4096 [00:05<00:03, 465.66it/s]
Adding requests:  60%|██████    | 2458/4096 [00:05<00:03, 468.65it/s]
Adding requests:  61%|██████    | 2505/4096 [00:05<00:03, 468.03it/s]
Adding requests:  62%|██████▏   | 2554/4096 [00:05<00:03, 473.54it/s]
Adding requests:  64%|██████▎   | 2602/4096 [00:05<00:03, 472.81it/s]
Adding requests:  65%|██████▍   | 2650/4096 [00:05<00:03, 474.91it/s]
Adding requests:  66%|██████▌   | 2698/4096 [00:05<00:02, 472.50it/s]
Adding requests:  67%|██████▋   | 2746/4096 [00:05<00:02, 471.60it/s]
Adding requests:  68%|██████▊   | 2794/4096 [00:06<00:02, 470.01it/s]
Adding requests:  69%|██████▉   | 2842/4096 [00:06<00:02, 470.72it/s]
Adding requests:  71%|███████   | 2891/4096 [00:06<00:02, 474.09it/s]
Adding requests:  72%|███████▏  | 2939/4096 [00:06<00:02, 468.62it/s]
Adding requests:  73%|███████▎  | 2988/4096 [00:06<00:02, 472.19it/s]
Adding requests:  74%|███████▍  | 3036/4096 [00:06<00:02, 472.01it/s]
Adding requests:  75%|███████▌  | 3084/4096 [00:06<00:02, 470.01it/s]
Adding requests:  76%|███████▋  | 3133/4096 [00:06<00:02, 474.21it/s]
Adding requests:  78%|███████▊  | 3181/4096 [00:06<00:01, 475.33it/s]
Adding requests:  79%|███████▉  | 3229/4096 [00:06<00:01, 473.49it/s]
Adding requests:  80%|████████  | 3277/4096 [00:07<00:01, 463.33it/s]
Adding requests:  81%|████████  | 3325/4096 [00:07<00:01, 465.76it/s]
Adding requests:  82%|████████▏ | 3374/4096 [00:07<00:01, 471.02it/s]
Adding requests:  84%|████████▎ | 3423/4096 [00:07<00:01, 476.59it/s]
Adding requests:  85%|████████▍ | 3471/4096 [00:07<00:01, 465.57it/s]
Adding requests:  86%|████████▌ | 3520/4096 [00:07<00:01, 470.19it/s]
Adding requests:  87%|████████▋ | 3568/4096 [00:07<00:01, 468.89it/s]
Adding requests:  88%|████████▊ | 3615/4096 [00:07<00:01, 468.93it/s]
Adding requests:  89%|████████▉ | 3662/4096 [00:07<00:00, 468.83it/s]
Adding requests:  91%|█████████ | 3711/4096 [00:07<00:00, 473.52it/s]
Adding requests:  92%|█████████▏| 3759/4096 [00:08<00:00, 473.86it/s]
Adding requests:  93%|█████████▎| 3808/4096 [00:08<00:00, 476.79it/s]
Adding requests:  94%|█████████▍| 3858/4096 [00:08<00:00, 482.98it/s]
Adding requests:  95%|█████████▌| 3907/4096 [00:08<00:00, 480.13it/s]
Adding requests:  97%|█████████▋| 3956/4096 [00:08<00:00, 481.28it/s]
Adding requests:  98%|█████████▊| 4005/4096 [00:08<00:00, 477.94it/s]
Adding requests:  99%|█████████▉| 4053/4096 [00:08<00:00, 474.51it/s]
Adding requests: 100%|██████████| 4096/4096 [00:08<00:00, 466.20it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  62%|██████▏   | 2547/4096 [00:00<00:00, 15799.40it/s, est. speed input: 16179353.70 toks/s, output: 15799.57 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:05<00:00, 15799.40it/s, est. speed input: 782492.93 toks/s, output: 764.15 toks/s]    
Processed prompts: 100%|██████████| 4096/4096 [00:05<00:00, 764.13it/s, est. speed input: 782492.93 toks/s, output: 764.15 toks/s]  
[rank0]:[W128 13:52:31.080375403 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 69.4s

测试结果:
  Requests/s:   289.51
  Tokens/s:     296747.69
  Total Reqs:   4096
  Elapsed:      14.15s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     296458.18


------------------------------------------------------------
  生成 CSV: BitNet-2B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cublaslt/BitNet-2B-FP8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,32.6243,16736.2874,3.9235
1024,1024,1,128,128,33.2148,34045.1808,3.8537
2048,1024,2,256,128,65.1497,66778.4126,3.9294
4096,1024,4,512,128,127.3871,130571.7327,4.0192
8192,1024,8,1024,128,236.4626,242374.2025,4.3305
16384,1024,16,2048,128,284.5306,291643.8262,7.1978
32768,1024,32,4096,128,289.5099,296747.6906,14.1480

------------------------------------------------------------

[INFO] 完成: 7 成功, 0 失败

============================================================
  BitNet-2B-FP8 | cuSPARSELt (2_4) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_4
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_4

============================================================
[1/7] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 13:52:40 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3259020) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3259020) WARNING 01-28 13:52:55 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=3259020) WARNING 01-28 13:53:06 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 33.03 requests/s, 16942.80 total tokens/s, 33.03 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-28 13:52:40] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:52:40] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 13:52:40] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 13:52:40] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:52:40] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:52:40] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:52:40] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:52:40] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:52:40] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 13:52:40] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:52:40] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:52:40] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:52:40] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:52:40] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 13:52:46] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:52:47] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 13:52:47] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 13:52:47] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:52:47] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:52:47] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:52:47] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:52:47] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:52:47] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 13:52:47] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:52:47] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:52:47] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:52:47] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:52:47] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3259020) [2026-01-28 13:52:48] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3259020) [2026-01-28 13:52:48] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3259020) [2026-01-28 13:52:48] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3259020) [2026-01-28 13:52:48] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3259020) [2026-01-28 13:52:48] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3259020) [2026-01-28 13:52:48] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3259020) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3259020) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.87it/s]
(EngineCore_DP0 pid=3259020) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.87it/s]
(EngineCore_DP0 pid=3259020) 
(EngineCore_DP0 pid=3259020) [2026-01-28 13:52:49] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3259020) [2026-01-28 13:52:49] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6144000 bytes
(EngineCore_DP0 pid=3259020) [2026-01-28 13:52:49] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3259020) [2026-01-28 13:52:49] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4096000 bytes
(EngineCore_DP0 pid=3259020) [2026-01-28 13:52:49] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3259020) [2026-01-28 13:52:49] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 22118400 bytes
(EngineCore_DP0 pid=3259020) [2026-01-28 13:52:49] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3259020) [2026-01-28 13:52:49] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 11059200 bytes
(EngineCore_DP0 pid=3259020) 2026-01-28 13:53:06,427 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3259020) 2026-01-28 13:53:06,453 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3259020) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  3.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.67it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.66it/s]
(EngineCore_DP0 pid=3259020) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 14.78it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  59%|█████▊    | 75/128 [00:00<00:00, 742.79it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 753.19it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:20,  6.15it/s, est. speed input: 3151.33 toks/s, output: 6.15 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:05, 20.75it/s, est. speed input: 9299.72 toks/s, output: 18.16 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:04, 27.02it/s, est. speed input: 11923.64 toks/s, output: 23.29 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:03, 30.35it/s, est. speed input: 13378.29 toks/s, output: 26.13 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:03, 32.25it/s, est. speed input: 14286.59 toks/s, output: 27.90 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:03, 33.39it/s, est. speed input: 14905.55 toks/s, output: 29.11 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:00<00:03, 34.15it/s, est. speed input: 15362.67 toks/s, output: 30.00 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:00<00:02, 34.68it/s, est. speed input: 15715.23 toks/s, output: 30.69 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:01<00:02, 35.06it/s, est. speed input: 15996.95 toks/s, output: 31.24 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:01<00:02, 35.34it/s, est. speed input: 16227.20 toks/s, output: 31.69 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:01<00:02, 35.54it/s, est. speed input: 16419.25 toks/s, output: 32.07 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:01<00:02, 35.66it/s, est. speed input: 16577.73 toks/s, output: 32.38 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:01<00:02, 35.64it/s, est. speed input: 16700.77 toks/s, output: 32.62 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:01<00:02, 35.68it/s, est. speed input: 16812.73 toks/s, output: 32.84 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:01<00:01, 35.70it/s, est. speed input: 16909.60 toks/s, output: 33.03 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:01<00:01, 35.77it/s, est. speed input: 16999.07 toks/s, output: 33.20 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:01<00:01, 35.73it/s, est. speed input: 17071.41 toks/s, output: 33.34 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:02<00:01, 35.77it/s, est. speed input: 17141.04 toks/s, output: 33.48 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:02<00:01, 35.83it/s, est. speed input: 17206.12 toks/s, output: 33.61 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:02<00:01, 35.87it/s, est. speed input: 17264.99 toks/s, output: 33.72 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:02<00:01, 35.86it/s, est. speed input: 17315.45 toks/s, output: 33.82 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:02<00:01, 35.85it/s, est. speed input: 17361.65 toks/s, output: 33.91 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:02<00:01, 35.84it/s, est. speed input: 17403.21 toks/s, output: 33.99 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:02<00:00, 35.85it/s, est. speed input: 17442.75 toks/s, output: 34.07 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:02<00:00, 35.90it/s, est. speed input: 17481.61 toks/s, output: 34.14 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:02<00:00, 35.90it/s, est. speed input: 17515.28 toks/s, output: 34.21 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:03<00:00, 35.92it/s, est. speed input: 17548.08 toks/s, output: 34.27 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:03<00:00, 35.89it/s, est. speed input: 17575.80 toks/s, output: 34.33 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:03<00:00, 35.86it/s, est. speed input: 17601.18 toks/s, output: 34.38 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:03<00:00, 35.84it/s, est. speed input: 17625.12 toks/s, output: 34.42 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:03<00:00, 35.89it/s, est. speed input: 17650.87 toks/s, output: 34.47 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:03<00:00, 35.92it/s, est. speed input: 17674.44 toks/s, output: 34.52 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 35.92it/s, est. speed input: 17690.16 toks/s, output: 34.55 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 34.55it/s, est. speed input: 17690.16 toks/s, output: 34.55 toks/s]
[rank0]:[W128 13:53:12.974650369 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 40.8s

测试结果:
  Requests/s:   33.03
  Tokens/s:     16942.80
  Total Reqs:   128
  Elapsed:      3.88s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     16909.77

============================================================
[2/7] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 13:53:21 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3260361) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3260361) WARNING 01-28 13:53:37 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=3260361) WARNING 01-28 13:53:47 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 32.74 requests/s, 33560.56 total tokens/s, 32.74 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-28 13:53:21] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:53:21] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 13:53:21] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 13:53:21] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:53:21] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:53:21] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:53:21] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:53:21] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:53:21] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 13:53:21] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:53:21] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:53:21] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:53:21] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:53:21] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 13:53:28] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:53:28] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 13:53:28] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 13:53:28] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:53:28] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:53:28] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:53:28] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:53:28] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:53:28] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 13:53:28] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:53:28] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:53:28] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:53:28] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:53:28] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3260361) [2026-01-28 13:53:29] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3260361) [2026-01-28 13:53:29] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3260361) [2026-01-28 13:53:29] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3260361) [2026-01-28 13:53:29] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3260361) [2026-01-28 13:53:29] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3260361) [2026-01-28 13:53:29] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3260361) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3260361) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.00it/s]
(EngineCore_DP0 pid=3260361) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.00it/s]
(EngineCore_DP0 pid=3260361) 
(EngineCore_DP0 pid=3260361) [2026-01-28 13:53:30] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3260361) [2026-01-28 13:53:30] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6144000 bytes
(EngineCore_DP0 pid=3260361) [2026-01-28 13:53:30] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3260361) [2026-01-28 13:53:30] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4096000 bytes
(EngineCore_DP0 pid=3260361) [2026-01-28 13:53:30] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3260361) [2026-01-28 13:53:30] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 22118400 bytes
(EngineCore_DP0 pid=3260361) [2026-01-28 13:53:30] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3260361) [2026-01-28 13:53:30] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 11059200 bytes
(EngineCore_DP0 pid=3260361) 2026-01-28 13:53:47,159 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3260361) 2026-01-28 13:53:47,186 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3260361) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 14.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 14.60it/s]
(EngineCore_DP0 pid=3260361) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 14.56it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  32%|███▏      | 41/128 [00:00<00:00, 400.64it/s]
Adding requests:  67%|██████▋   | 86/128 [00:00<00:00, 426.83it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 429.97it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:02, 46.36it/s, est. speed input: 47481.08 toks/s, output: 46.37 toks/s]
Processed prompts:   8%|▊         | 10/128 [00:00<00:03, 39.07it/s, est. speed input: 40972.07 toks/s, output: 40.01 toks/s]
Processed prompts:  11%|█         | 14/128 [00:00<00:03, 37.46it/s, est. speed input: 39464.79 toks/s, output: 38.54 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:00<00:03, 36.64it/s, est. speed input: 38675.90 toks/s, output: 37.77 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:00<00:02, 36.16it/s, est. speed input: 38185.40 toks/s, output: 37.29 toks/s]
Processed prompts:  20%|██        | 26/128 [00:00<00:02, 35.86it/s, est. speed input: 37850.52 toks/s, output: 36.96 toks/s]
Processed prompts:  23%|██▎       | 30/128 [00:00<00:02, 35.61it/s, est. speed input: 37581.51 toks/s, output: 36.70 toks/s]
Processed prompts:  27%|██▋       | 34/128 [00:00<00:02, 35.44it/s, est. speed input: 37378.25 toks/s, output: 36.50 toks/s]
Processed prompts:  30%|██▉       | 38/128 [00:01<00:02, 35.36it/s, est. speed input: 37232.45 toks/s, output: 36.36 toks/s]
Processed prompts:  33%|███▎      | 42/128 [00:01<00:02, 35.38it/s, est. speed input: 37137.00 toks/s, output: 36.27 toks/s]
Processed prompts:  36%|███▌      | 46/128 [00:01<00:02, 35.36it/s, est. speed input: 37050.08 toks/s, output: 36.18 toks/s]
Processed prompts:  39%|███▉      | 50/128 [00:01<00:02, 35.30it/s, est. speed input: 36964.56 toks/s, output: 36.10 toks/s]
Processed prompts:  42%|████▏     | 54/128 [00:01<00:02, 35.32it/s, est. speed input: 36907.32 toks/s, output: 36.04 toks/s]
Processed prompts:  45%|████▌     | 58/128 [00:01<00:01, 35.31it/s, est. speed input: 36852.78 toks/s, output: 35.99 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:01<00:01, 35.31it/s, est. speed input: 36807.22 toks/s, output: 35.94 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:01<00:01, 34.78it/s, est. speed input: 36653.32 toks/s, output: 35.79 toks/s]
Processed prompts:  55%|█████▍    | 70/128 [00:01<00:01, 34.97it/s, est. speed input: 36630.96 toks/s, output: 35.77 toks/s]
Processed prompts:  58%|█████▊    | 74/128 [00:02<00:01, 35.05it/s, est. speed input: 36601.38 toks/s, output: 35.74 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:02<00:01, 35.17it/s, est. speed input: 36585.67 toks/s, output: 35.73 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:02<00:01, 35.19it/s, est. speed input: 36560.60 toks/s, output: 35.70 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:02<00:01, 35.20it/s, est. speed input: 36537.54 toks/s, output: 35.68 toks/s]
Processed prompts:  70%|███████   | 90/128 [00:02<00:01, 35.03it/s, est. speed input: 36488.29 toks/s, output: 35.63 toks/s]
Processed prompts:  73%|███████▎  | 94/128 [00:02<00:00, 35.13it/s, est. speed input: 36477.34 toks/s, output: 35.62 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:02<00:00, 35.12it/s, est. speed input: 36455.00 toks/s, output: 35.60 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:02<00:00, 35.11it/s, est. speed input: 36434.55 toks/s, output: 35.58 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:02<00:00, 35.09it/s, est. speed input: 36413.62 toks/s, output: 35.56 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:03<00:00, 35.02it/s, est. speed input: 36386.64 toks/s, output: 35.53 toks/s]
Processed prompts:  89%|████████▉ | 114/128 [00:03<00:00, 35.11it/s, est. speed input: 36378.63 toks/s, output: 35.53 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:03<00:00, 35.17it/s, est. speed input: 36371.19 toks/s, output: 35.52 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:03<00:00, 35.18it/s, est. speed input: 36360.99 toks/s, output: 35.51 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:03<00:00, 35.20it/s, est. speed input: 36352.10 toks/s, output: 35.50 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 35.20it/s, est. speed input: 36301.63 toks/s, output: 35.45 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 35.45it/s, est. speed input: 36301.63 toks/s, output: 35.45 toks/s]
[rank0]:[W128 13:53:52.048743598 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 40.0s

测试结果:
  Requests/s:   32.74
  Tokens/s:     33560.56
  Total Reqs:   128
  Elapsed:      3.91s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     33527.82

============================================================
[3/7] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 13:54:02 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3261597) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3261597) WARNING 01-28 13:54:17 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=3261597) WARNING 01-28 13:54:27 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 67.98 requests/s, 69677.78 total tokens/s, 67.98 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-28 13:54:01] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:54:02] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 13:54:02] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 13:54:02] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:54:02] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:54:02] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:54:02] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:54:02] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:54:02] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 13:54:02] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:54:02] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:54:02] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:54:02] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:54:02] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 13:54:08] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:54:08] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 13:54:08] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 13:54:08] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:54:08] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:54:08] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:54:08] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:54:08] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:54:08] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 13:54:08] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:54:08] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:54:08] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:54:08] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:54:08] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3261597) [2026-01-28 13:54:09] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3261597) [2026-01-28 13:54:09] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3261597) [2026-01-28 13:54:09] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3261597) [2026-01-28 13:54:09] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3261597) [2026-01-28 13:54:09] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3261597) [2026-01-28 13:54:09] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3261597) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3261597) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.79it/s]
(EngineCore_DP0 pid=3261597) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.79it/s]
(EngineCore_DP0 pid=3261597) 
(EngineCore_DP0 pid=3261597) [2026-01-28 13:54:10] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3261597) [2026-01-28 13:54:11] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6144000 bytes
(EngineCore_DP0 pid=3261597) [2026-01-28 13:54:11] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3261597) [2026-01-28 13:54:11] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4096000 bytes
(EngineCore_DP0 pid=3261597) [2026-01-28 13:54:11] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3261597) [2026-01-28 13:54:11] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 22118400 bytes
(EngineCore_DP0 pid=3261597) [2026-01-28 13:54:11] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3261597) [2026-01-28 13:54:11] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 11059200 bytes
(EngineCore_DP0 pid=3261597) 2026-01-28 13:54:27,490 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3261597) 2026-01-28 13:54:27,515 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3261597) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:00<00:00, 15.25it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 13.52it/s]
(EngineCore_DP0 pid=3261597) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 15.89it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 15.87it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  16%|█▌        | 40/256 [00:00<00:00, 397.75it/s]
Adding requests:  33%|███▎      | 84/256 [00:00<00:00, 420.11it/s]
Adding requests:  50%|█████     | 129/256 [00:00<00:00, 430.72it/s]
Adding requests:  68%|██████▊   | 173/256 [00:00<00:00, 433.92it/s]
Adding requests:  85%|████████▌ | 218/256 [00:00<00:00, 439.47it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 432.90it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  16%|█▌        | 40/256 [00:00<00:00, 390.48it/s, est. speed input: 399874.99 toks/s, output: 390.48 toks/s]
Processed prompts:  31%|███▏      | 80/256 [00:00<00:01, 106.77it/s, est. speed input: 122701.56 toks/s, output: 119.82 toks/s]
Processed prompts:  39%|███▉      | 101/256 [00:00<00:01, 94.05it/s, est. speed input: 108734.09 toks/s, output: 106.19 toks/s]
Processed prompts:  45%|████▌     | 116/256 [00:01<00:01, 85.01it/s, est. speed input: 100501.67 toks/s, output: 98.15 toks/s] 
Processed prompts:  50%|█████     | 128/256 [00:01<00:01, 81.56it/s, est. speed input: 97018.98 toks/s, output: 94.74 toks/s] 
Processed prompts:  54%|█████▍    | 138/256 [00:01<00:01, 79.06it/s, est. speed input: 94665.48 toks/s, output: 92.45 toks/s]
Processed prompts:  57%|█████▋    | 147/256 [00:01<00:01, 79.15it/s, est. speed input: 93732.81 toks/s, output: 91.54 toks/s]
Processed prompts:  61%|██████    | 156/256 [00:01<00:01, 74.60it/s, est. speed input: 91280.86 toks/s, output: 89.14 toks/s]
Processed prompts:  64%|██████▍   | 164/256 [00:01<00:01, 73.66it/s, est. speed input: 90126.75 toks/s, output: 88.01 toks/s]
Processed prompts:  67%|██████▋   | 172/256 [00:01<00:01, 72.85it/s, est. speed input: 89091.37 toks/s, output: 87.00 toks/s]
Processed prompts:  70%|███████   | 180/256 [00:02<00:01, 72.29it/s, est. speed input: 88187.84 toks/s, output: 86.12 toks/s]
Processed prompts:  73%|███████▎  | 188/256 [00:02<00:00, 71.84it/s, est. speed input: 87373.94 toks/s, output: 85.33 toks/s]
Processed prompts:  77%|███████▋  | 196/256 [00:02<00:00, 71.15it/s, est. speed input: 86563.66 toks/s, output: 84.53 toks/s]
Processed prompts:  80%|███████▉  | 204/256 [00:02<00:00, 70.90it/s, est. speed input: 85881.46 toks/s, output: 83.87 toks/s]
Processed prompts:  83%|████████▎ | 212/256 [00:02<00:00, 70.76it/s, est. speed input: 85265.50 toks/s, output: 83.27 toks/s]
Processed prompts:  86%|████████▌ | 220/256 [00:02<00:00, 70.80it/s, est. speed input: 84728.17 toks/s, output: 82.74 toks/s]
Processed prompts:  89%|████████▉ | 228/256 [00:02<00:00, 70.92it/s, est. speed input: 84250.25 toks/s, output: 82.28 toks/s]
Processed prompts:  92%|█████████▏| 236/256 [00:02<00:00, 69.74it/s, est. speed input: 83607.99 toks/s, output: 81.65 toks/s]
Processed prompts:  95%|█████████▌| 244/256 [00:03<00:00, 70.04it/s, est. speed input: 83188.09 toks/s, output: 81.24 toks/s]
Processed prompts:  98%|█████████▊| 252/256 [00:03<00:00, 70.19it/s, est. speed input: 82790.24 toks/s, output: 80.85 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 70.19it/s, est. speed input: 82607.97 toks/s, output: 80.67 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 80.67it/s, est. speed input: 82607.97 toks/s, output: 80.67 toks/s]
[rank0]:[W128 13:54:33.662184334 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 40.6s

测试结果:
  Requests/s:   67.98
  Tokens/s:     69677.78
  Total Reqs:   256
  Elapsed:      3.77s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     69609.80

============================================================
[4/7] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 13:54:44 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3262761) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3262761) WARNING 01-28 13:54:59 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=3262761) WARNING 01-28 13:55:09 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 131.07 requests/s, 134351.42 total tokens/s, 131.07 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-28 13:54:43] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:54:44] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 13:54:44] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 13:54:44] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:54:44] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:54:44] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:54:44] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:54:44] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:54:44] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 13:54:44] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:54:44] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:54:44] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:54:44] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:54:44] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 13:54:50] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:54:50] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 13:54:50] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 13:54:50] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:54:50] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:54:50] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:54:50] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:54:50] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:54:50] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 13:54:50] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:54:50] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:54:50] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:54:50] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:54:50] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3262761) [2026-01-28 13:54:51] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3262761) [2026-01-28 13:54:51] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3262761) [2026-01-28 13:54:51] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3262761) [2026-01-28 13:54:51] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3262761) [2026-01-28 13:54:51] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3262761) [2026-01-28 13:54:51] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3262761) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3262761) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.96it/s]
(EngineCore_DP0 pid=3262761) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.96it/s]
(EngineCore_DP0 pid=3262761) 
(EngineCore_DP0 pid=3262761) [2026-01-28 13:54:52] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3262761) [2026-01-28 13:54:52] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6144000 bytes
(EngineCore_DP0 pid=3262761) [2026-01-28 13:54:52] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3262761) [2026-01-28 13:54:52] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4096000 bytes
(EngineCore_DP0 pid=3262761) [2026-01-28 13:54:52] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3262761) [2026-01-28 13:54:52] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 22118400 bytes
(EngineCore_DP0 pid=3262761) [2026-01-28 13:54:52] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3262761) [2026-01-28 13:54:52] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 11059200 bytes
(EngineCore_DP0 pid=3262761) 2026-01-28 13:55:09,555 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3262761) 2026-01-28 13:55:09,581 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3262761) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00, 15.04it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 13.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 13.99it/s]
(EngineCore_DP0 pid=3262761) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00, 15.85it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 16.02it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   7%|▋         | 37/512 [00:00<00:01, 368.32it/s]
Adding requests:  16%|█▌        | 81/512 [00:00<00:01, 408.37it/s]
Adding requests:  25%|██▍       | 126/512 [00:00<00:00, 425.78it/s]
Adding requests:  33%|███▎      | 170/512 [00:00<00:00, 430.04it/s]
Adding requests:  42%|████▏     | 216/512 [00:00<00:00, 438.36it/s]
Adding requests:  51%|█████     | 260/512 [00:00<00:00, 431.75it/s]
Adding requests:  60%|█████▉    | 305/512 [00:00<00:00, 435.78it/s]
Adding requests:  69%|██████▉   | 352/512 [00:00<00:00, 445.66it/s]
Adding requests:  78%|███████▊  | 400/512 [00:00<00:00, 454.18it/s]
Adding requests:  87%|████████▋ | 447/512 [00:01<00:00, 457.16it/s]
Adding requests:  96%|█████████▋| 494/512 [00:01<00:00, 460.54it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 443.60it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  29%|██▊       | 146/512 [00:00<00:00, 1386.99it/s, est. speed input: 1420370.63 toks/s, output: 1387.01 toks/s]
Processed prompts:  56%|█████▌    | 285/512 [00:01<00:00, 229.73it/s, est. speed input: 269841.89 toks/s, output: 263.52 toks/s]   
Processed prompts:  69%|██████▊   | 351/512 [00:01<00:00, 190.04it/s, est. speed input: 227149.50 toks/s, output: 221.82 toks/s]
Processed prompts:  77%|███████▋  | 393/512 [00:01<00:00, 179.13it/s, est. speed input: 215263.93 toks/s, output: 210.22 toks/s]
Processed prompts:  83%|████████▎ | 424/512 [00:02<00:00, 169.15it/s, est. speed input: 206720.07 toks/s, output: 201.87 toks/s]
Processed prompts:  88%|████████▊ | 449/512 [00:02<00:00, 164.51it/s, est. speed input: 202297.56 toks/s, output: 197.56 toks/s]
Processed prompts:  92%|█████████▏| 471/512 [00:02<00:00, 154.84it/s, est. speed input: 196525.28 toks/s, output: 191.92 toks/s]
Processed prompts:  96%|█████████▌| 490/512 [00:02<00:00, 150.17it/s, est. speed input: 193110.49 toks/s, output: 188.58 toks/s]
Processed prompts:  99%|█████████▉| 507/512 [00:02<00:00, 149.73it/s, est. speed input: 191341.94 toks/s, output: 186.86 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:02<00:00, 149.73it/s, est. speed input: 190595.67 toks/s, output: 186.13 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:02<00:00, 186.12it/s, est. speed input: 190595.67 toks/s, output: 186.13 toks/s]
[rank0]:[W128 13:55:15.958884660 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 42.3s

测试结果:
  Requests/s:   131.07
  Tokens/s:     134351.42
  Total Reqs:   512
  Elapsed:      3.91s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     134220.35

============================================================
[5/7] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 13:55:28 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3263943) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3263943) WARNING 01-28 13:55:43 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=3263943) WARNING 01-28 13:55:53 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 246.67 requests/s, 252839.22 total tokens/s, 246.67 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-28 13:55:28] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:55:28] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 13:55:28] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 13:55:28] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:55:28] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:55:28] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:55:28] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:55:28] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:55:28] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 13:55:28] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:55:28] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:55:28] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:55:28] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:55:28] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 13:55:34] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:55:35] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 13:55:35] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 13:55:35] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:55:35] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:55:35] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:55:35] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:55:35] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:55:35] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 13:55:35] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:55:35] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:55:35] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:55:35] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:55:35] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3263943) [2026-01-28 13:55:36] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3263943) [2026-01-28 13:55:36] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3263943) [2026-01-28 13:55:36] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3263943) [2026-01-28 13:55:36] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3263943) [2026-01-28 13:55:36] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3263943) [2026-01-28 13:55:36] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3263943) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3263943) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.10it/s]
(EngineCore_DP0 pid=3263943) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.10it/s]
(EngineCore_DP0 pid=3263943) 
(EngineCore_DP0 pid=3263943) [2026-01-28 13:55:37] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3263943) [2026-01-28 13:55:37] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6144000 bytes
(EngineCore_DP0 pid=3263943) [2026-01-28 13:55:37] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3263943) [2026-01-28 13:55:37] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4096000 bytes
(EngineCore_DP0 pid=3263943) [2026-01-28 13:55:37] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3263943) [2026-01-28 13:55:37] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 22118400 bytes
(EngineCore_DP0 pid=3263943) [2026-01-28 13:55:37] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3263943) [2026-01-28 13:55:37] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 11059200 bytes
(EngineCore_DP0 pid=3263943) 2026-01-28 13:55:53,684 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3263943) 2026-01-28 13:55:53,714 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3263943) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00, 14.94it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00, 15.51it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 13.63it/s]
(EngineCore_DP0 pid=3263943) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00, 13.05it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 13.21it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 13.18it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   4%|▍         | 41/1024 [00:00<00:02, 401.96it/s]
Adding requests:   8%|▊         | 86/1024 [00:00<00:02, 428.55it/s]
Adding requests:  13%|█▎        | 131/1024 [00:00<00:02, 437.16it/s]
Adding requests:  17%|█▋        | 176/1024 [00:00<00:01, 439.39it/s]
Adding requests:  22%|██▏       | 223/1024 [00:00<00:01, 448.41it/s]
Adding requests:  26%|██▋       | 271/1024 [00:00<00:01, 458.97it/s]
Adding requests:  31%|███       | 317/1024 [00:00<00:01, 455.14it/s]
Adding requests:  36%|███▌      | 366/1024 [00:00<00:01, 463.96it/s]
Adding requests:  40%|████      | 413/1024 [00:00<00:01, 464.02it/s]
Adding requests:  45%|████▌     | 461/1024 [00:01<00:01, 468.56it/s]
Adding requests:  50%|████▉     | 508/1024 [00:01<00:01, 466.46it/s]
Adding requests:  54%|█████▍    | 555/1024 [00:01<00:01, 465.25it/s]
Adding requests:  59%|█████▉    | 602/1024 [00:01<00:00, 466.64it/s]
Adding requests:  64%|██████▎   | 652/1024 [00:01<00:00, 475.43it/s]
Adding requests:  69%|██████▊   | 702/1024 [00:01<00:00, 482.15it/s]
Adding requests:  73%|███████▎  | 751/1024 [00:01<00:00, 480.60it/s]
Adding requests:  78%|███████▊  | 800/1024 [00:01<00:00, 476.73it/s]
Adding requests:  83%|████████▎ | 848/1024 [00:01<00:00, 467.51it/s]
Adding requests:  88%|████████▊ | 898/1024 [00:01<00:00, 475.58it/s]
Adding requests:  92%|█████████▏| 946/1024 [00:02<00:00, 475.99it/s]
Adding requests:  97%|█████████▋| 995/1024 [00:02<00:00, 477.87it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 466.92it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  51%|█████     | 522/1024 [00:00<00:00, 4527.15it/s, est. speed input: 4636102.75 toks/s, output: 4527.22 toks/s]
Processed prompts:  95%|█████████▌| 975/1024 [00:01<00:00, 470.97it/s, est. speed input: 563342.75 toks/s, output: 550.14 toks/s]   
Processed prompts: 100%|██████████| 1024/1024 [00:01<00:00, 470.97it/s, est. speed input: 535822.88 toks/s, output: 523.26 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:01<00:00, 523.24it/s, est. speed input: 535822.88 toks/s, output: 523.26 toks/s]
[rank0]:[W128 13:56:00.558608128 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 44.6s

测试结果:
  Requests/s:   246.67
  Tokens/s:     252839.22
  Total Reqs:   1024
  Elapsed:      4.15s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     252592.54

============================================================
[6/7] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 13:56:17 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3265208) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3265208) WARNING 01-28 13:56:33 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=3265208) WARNING 01-28 13:56:43 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 312.03 requests/s, 319832.27 total tokens/s, 312.03 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-28 13:56:17] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:56:17] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 13:56:17] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 13:56:17] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:56:17] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:56:17] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:56:17] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:56:17] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:56:17] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 13:56:17] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:56:17] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:56:17] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:56:17] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:56:17] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 13:56:23] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:56:24] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 13:56:24] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 13:56:24] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:56:24] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:56:24] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:56:24] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:56:24] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:56:24] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 13:56:24] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:56:24] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:56:24] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:56:24] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:56:24] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3265208) [2026-01-28 13:56:25] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3265208) [2026-01-28 13:56:25] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3265208) [2026-01-28 13:56:25] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3265208) [2026-01-28 13:56:25] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3265208) [2026-01-28 13:56:25] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3265208) [2026-01-28 13:56:25] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3265208) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3265208) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.97it/s]
(EngineCore_DP0 pid=3265208) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.97it/s]
(EngineCore_DP0 pid=3265208) 
(EngineCore_DP0 pid=3265208) [2026-01-28 13:56:26] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3265208) [2026-01-28 13:56:26] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6144000 bytes
(EngineCore_DP0 pid=3265208) [2026-01-28 13:56:26] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3265208) [2026-01-28 13:56:26] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4096000 bytes
(EngineCore_DP0 pid=3265208) [2026-01-28 13:56:26] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3265208) [2026-01-28 13:56:26] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 22118400 bytes
(EngineCore_DP0 pid=3265208) [2026-01-28 13:56:26] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3265208) [2026-01-28 13:56:26] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 11059200 bytes
(EngineCore_DP0 pid=3265208) 2026-01-28 13:56:43,069 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3265208) 2026-01-28 13:56:43,095 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3265208) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00, 14.16it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00, 13.39it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:00<00:00, 13.73it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 13.27it/s]
(EngineCore_DP0 pid=3265208) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00, 16.08it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00, 16.46it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 16.44it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 41/2048 [00:00<00:05, 400.49it/s]
Adding requests:   4%|▍         | 86/2048 [00:00<00:04, 426.19it/s]
Adding requests:   6%|▋         | 131/2048 [00:00<00:04, 433.78it/s]
Adding requests:   9%|▊         | 175/2048 [00:00<00:04, 434.97it/s]
Adding requests:  11%|█         | 219/2048 [00:00<00:04, 421.38it/s]
Adding requests:  13%|█▎        | 262/2048 [00:00<00:04, 421.24it/s]
Adding requests:  15%|█▍        | 305/2048 [00:00<00:04, 419.60it/s]
Adding requests:  17%|█▋        | 347/2048 [00:00<00:04, 416.67it/s]
Adding requests:  19%|█▉        | 393/2048 [00:00<00:03, 427.24it/s]
Adding requests:  21%|██▏       | 439/2048 [00:01<00:03, 434.30it/s]
Adding requests:  24%|██▎       | 486/2048 [00:01<00:03, 443.92it/s]
Adding requests:  26%|██▌       | 531/2048 [00:01<00:03, 430.36it/s]
Adding requests:  28%|██▊       | 575/2048 [00:01<00:03, 432.13it/s]
Adding requests:  30%|███       | 622/2048 [00:01<00:03, 441.36it/s]
Adding requests:  33%|███▎      | 670/2048 [00:01<00:03, 451.01it/s]
Adding requests:  35%|███▌      | 719/2048 [00:01<00:02, 460.98it/s]
Adding requests:  37%|███▋      | 766/2048 [00:01<00:02, 463.27it/s]
Adding requests:  40%|███▉      | 813/2048 [00:01<00:02, 455.80it/s]
Adding requests:  42%|████▏     | 860/2048 [00:01<00:02, 457.74it/s]
Adding requests:  44%|████▍     | 909/2048 [00:02<00:02, 465.39it/s]
Adding requests:  47%|████▋     | 956/2048 [00:02<00:02, 466.56it/s]
Adding requests:  49%|████▉     | 1005/2048 [00:02<00:02, 470.40it/s]
Adding requests:  51%|█████▏    | 1054/2048 [00:02<00:02, 473.41it/s]
Adding requests:  54%|█████▍    | 1102/2048 [00:02<00:02, 471.29it/s]
Adding requests:  56%|█████▌    | 1150/2048 [00:02<00:01, 467.85it/s]
Adding requests:  59%|█████▊    | 1201/2048 [00:02<00:01, 478.90it/s]
Adding requests:  61%|██████    | 1249/2048 [00:02<00:01, 478.31it/s]
Adding requests:  63%|██████▎   | 1297/2048 [00:02<00:01, 475.26it/s]
Adding requests:  66%|██████▌   | 1346/2048 [00:02<00:01, 478.41it/s]
Adding requests:  68%|██████▊   | 1395/2048 [00:03<00:01, 481.38it/s]
Adding requests:  71%|███████   | 1444/2048 [00:03<00:01, 478.05it/s]
Adding requests:  73%|███████▎  | 1493/2048 [00:03<00:01, 481.55it/s]
Adding requests:  75%|███████▌  | 1542/2048 [00:03<00:01, 481.78it/s]
Adding requests:  78%|███████▊  | 1591/2048 [00:03<00:01, 452.75it/s]
Adding requests:  80%|████████  | 1640/2048 [00:03<00:00, 462.38it/s]
Adding requests:  82%|████████▏ | 1687/2048 [00:03<00:00, 461.61it/s]
Adding requests:  85%|████████▍ | 1736/2048 [00:03<00:00, 467.00it/s]
Adding requests:  87%|████████▋ | 1783/2048 [00:03<00:00, 461.87it/s]
Adding requests:  89%|████████▉ | 1832/2048 [00:04<00:00, 468.32it/s]
Adding requests:  92%|█████████▏| 1880/2048 [00:04<00:00, 469.56it/s]
Adding requests:  94%|█████████▍| 1928/2048 [00:04<00:00, 470.71it/s]
Adding requests:  96%|█████████▋| 1976/2048 [00:04<00:00, 472.20it/s]
Adding requests:  99%|█████████▉| 2024/2048 [00:04<00:00, 471.88it/s]
Adding requests: 100%|██████████| 2048/2048 [00:04<00:00, 458.12it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  69%|██████▉   | 1410/2048 [00:00<00:00, 10791.87it/s, est. speed input: 11051443.84 toks/s, output: 10792.01 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:02<00:00, 10791.87it/s, est. speed input: 1002721.89 toks/s, output: 979.22 toks/s]   
Processed prompts: 100%|██████████| 2048/2048 [00:02<00:00, 979.16it/s, est. speed input: 1002721.89 toks/s, output: 979.22 toks/s]  
[rank0]:[W128 13:56:52.328870732 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 51.9s

测试结果:
  Requests/s:   312.03
  Tokens/s:     319832.27
  Total Reqs:   2048
  Elapsed:      6.56s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     319520.24

============================================================
[7/7] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:4)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 13:57:18 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3266600) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3266600) WARNING 01-28 13:57:34 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=3266600) WARNING 01-28 13:57:44 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 328.17 requests/s, 336374.14 total tokens/s, 328.17 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-28 13:57:18] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:57:18] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 13:57:18] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 13:57:18] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:57:18] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:57:18] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:57:18] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:57:18] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:57:18] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 13:57:18] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:57:18] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:57:18] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:57:18] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:57:18] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 13:57:25] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:57:25] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 13:57:25] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 13:57:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:57:25] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:57:25] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:57:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:57:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:57:25] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 13:57:25] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:57:25] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:57:25] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:57:25] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:57:25] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3266600) [2026-01-28 13:57:26] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3266600) [2026-01-28 13:57:26] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3266600) [2026-01-28 13:57:26] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3266600) [2026-01-28 13:57:26] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:4, expand_ratio=2.000
(EngineCore_DP0 pid=3266600) [2026-01-28 13:57:26] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3266600) [2026-01-28 13:57:26] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3266600) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3266600) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.07it/s]
(EngineCore_DP0 pid=3266600) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.07it/s]
(EngineCore_DP0 pid=3266600) 
(EngineCore_DP0 pid=3266600) [2026-01-28 13:57:27] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 2560] -> 1D uint8
(EngineCore_DP0 pid=3266600) [2026-01-28 13:57:27] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6144000 bytes
(EngineCore_DP0 pid=3266600) [2026-01-28 13:57:27] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 2560] -> 1D uint8
(EngineCore_DP0 pid=3266600) [2026-01-28 13:57:27] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 4096000 bytes
(EngineCore_DP0 pid=3266600) [2026-01-28 13:57:27] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 2560] -> 1D uint8
(EngineCore_DP0 pid=3266600) [2026-01-28 13:57:27] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 22118400 bytes
(EngineCore_DP0 pid=3266600) [2026-01-28 13:57:27] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 6912] -> 1D uint8
(EngineCore_DP0 pid=3266600) [2026-01-28 13:57:27] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 11059200 bytes
(EngineCore_DP0 pid=3266600) [rank0]:W0128 13:57:39.385000 3266600 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3266600) [rank0]:W0128 13:57:39.469000 3266600 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3266600) [rank0]:W0128 13:57:40.575000 3266600 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3266600) [rank0]:W0128 13:57:40.703000 3266600 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3266600) 2026-01-28 13:57:44,680 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3266600) 2026-01-28 13:57:44,709 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3266600) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:00, 13.63it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:00<00:00, 14.21it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 6/11 [00:00<00:00, 14.42it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:00<00:00, 13.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 10/11 [00:00<00:00, 13.77it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 13.58it/s]
(EngineCore_DP0 pid=3266600) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00, 15.63it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00, 15.91it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00, 15.98it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 15.96it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 40/4096 [00:00<00:10, 397.40it/s]
Adding requests:   2%|▏         | 84/4096 [00:00<00:09, 420.11it/s]
Adding requests:   3%|▎         | 129/4096 [00:00<00:09, 428.94it/s]
Adding requests:   4%|▍         | 173/4096 [00:00<00:09, 432.43it/s]
Adding requests:   5%|▌         | 218/4096 [00:00<00:08, 437.16it/s]
Adding requests:   6%|▋         | 266/4096 [00:00<00:08, 449.33it/s]
Adding requests:   8%|▊         | 311/4096 [00:00<00:08, 447.89it/s]
Adding requests:   9%|▊         | 356/4096 [00:00<00:08, 442.60it/s]
Adding requests:  10%|▉         | 402/4096 [00:00<00:08, 447.65it/s]
Adding requests:  11%|█         | 449/4096 [00:01<00:08, 453.20it/s]
Adding requests:  12%|█▏        | 496/4096 [00:01<00:07, 455.45it/s]
Adding requests:  13%|█▎        | 542/4096 [00:01<00:07, 449.63it/s]
Adding requests:  14%|█▍        | 591/4096 [00:01<00:07, 460.70it/s]
Adding requests:  16%|█▌        | 638/4096 [00:01<00:07, 462.66it/s]
Adding requests:  17%|█▋        | 687/4096 [00:01<00:07, 468.88it/s]
Adding requests:  18%|█▊        | 736/4096 [00:01<00:07, 474.30it/s]
Adding requests:  19%|█▉        | 784/4096 [00:01<00:07, 469.84it/s]
Adding requests:  20%|██        | 832/4096 [00:01<00:07, 460.64it/s]
Adding requests:  21%|██▏       | 880/4096 [00:01<00:06, 464.99it/s]
Adding requests:  23%|██▎       | 927/4096 [00:02<00:06, 461.52it/s]
Adding requests:  24%|██▍       | 975/4096 [00:02<00:06, 465.82it/s]
Adding requests:  25%|██▍       | 1023/4096 [00:02<00:06, 469.43it/s]
Adding requests:  26%|██▌       | 1070/4096 [00:02<00:06, 466.73it/s]
Adding requests:  27%|██▋       | 1117/4096 [00:02<00:06, 435.49it/s]
Adding requests:  28%|██▊       | 1165/4096 [00:02<00:06, 446.70it/s]
Adding requests:  30%|██▉       | 1213/4096 [00:02<00:06, 456.24it/s]
Adding requests:  31%|███       | 1259/4096 [00:02<00:06, 450.19it/s]
Adding requests:  32%|███▏      | 1306/4096 [00:02<00:06, 453.07it/s]
Adding requests:  33%|███▎      | 1354/4096 [00:02<00:05, 458.14it/s]
Adding requests:  34%|███▍      | 1403/4096 [00:03<00:05, 464.54it/s]
Adding requests:  35%|███▌      | 1450/4096 [00:03<00:05, 463.62it/s]
Adding requests:  37%|███▋      | 1499/4096 [00:03<00:05, 469.93it/s]
Adding requests:  38%|███▊      | 1547/4096 [00:03<00:05, 469.37it/s]
Adding requests:  39%|███▉      | 1596/4096 [00:03<00:05, 474.98it/s]
Adding requests:  40%|████      | 1644/4096 [00:03<00:05, 475.39it/s]
Adding requests:  41%|████▏     | 1692/4096 [00:03<00:05, 470.56it/s]
Adding requests:  42%|████▏     | 1740/4096 [00:03<00:04, 471.99it/s]
Adding requests:  44%|████▎     | 1788/4096 [00:03<00:04, 468.47it/s]
Adding requests:  45%|████▍     | 1835/4096 [00:03<00:04, 468.71it/s]
Adding requests:  46%|████▌     | 1882/4096 [00:04<00:04, 468.35it/s]
Adding requests:  47%|████▋     | 1929/4096 [00:04<00:04, 467.24it/s]
Adding requests:  48%|████▊     | 1977/4096 [00:04<00:04, 468.59it/s]
Adding requests:  49%|████▉     | 2024/4096 [00:04<00:04, 458.87it/s]
Adding requests:  51%|█████     | 2072/4096 [00:04<00:04, 464.73it/s]
Adding requests:  52%|█████▏    | 2119/4096 [00:04<00:04, 463.90it/s]
Adding requests:  53%|█████▎    | 2166/4096 [00:04<00:04, 458.62it/s]
Adding requests:  54%|█████▍    | 2212/4096 [00:04<00:04, 456.47it/s]
Adding requests:  55%|█████▌    | 2260/4096 [00:04<00:03, 461.59it/s]
Adding requests:  56%|█████▋    | 2308/4096 [00:05<00:03, 466.03it/s]
Adding requests:  57%|█████▋    | 2355/4096 [00:05<00:03, 464.38it/s]
Adding requests:  59%|█████▊    | 2402/4096 [00:05<00:03, 463.65it/s]
Adding requests:  60%|█████▉    | 2450/4096 [00:05<00:03, 465.75it/s]
Adding requests:  61%|██████    | 2498/4096 [00:05<00:03, 469.90it/s]
Adding requests:  62%|██████▏   | 2545/4096 [00:05<00:03, 467.31it/s]
Adding requests:  63%|██████▎   | 2593/4096 [00:05<00:03, 470.48it/s]
Adding requests:  64%|██████▍   | 2641/4096 [00:05<00:03, 471.27it/s]
Adding requests:  66%|██████▌   | 2689/4096 [00:05<00:03, 467.54it/s]
Adding requests:  67%|██████▋   | 2736/4096 [00:05<00:02, 465.32it/s]
Adding requests:  68%|██████▊   | 2784/4096 [00:06<00:02, 468.20it/s]
Adding requests:  69%|██████▉   | 2831/4096 [00:06<00:02, 459.48it/s]
Adding requests:  70%|███████   | 2879/4096 [00:06<00:02, 464.46it/s]
Adding requests:  71%|███████▏  | 2926/4096 [00:06<00:02, 464.76it/s]
Adding requests:  73%|███████▎  | 2973/4096 [00:06<00:02, 465.26it/s]
Adding requests:  74%|███████▎  | 3020/4096 [00:06<00:02, 461.59it/s]
Adding requests:  75%|███████▍  | 3067/4096 [00:06<00:02, 458.52it/s]
Adding requests:  76%|███████▌  | 3115/4096 [00:06<00:02, 463.57it/s]
Adding requests:  77%|███████▋  | 3162/4096 [00:06<00:02, 443.89it/s]
Adding requests:  78%|███████▊  | 3209/4096 [00:06<00:01, 450.24it/s]
Adding requests:  79%|███████▉  | 3255/4096 [00:07<00:01, 442.71it/s]
Adding requests:  81%|████████  | 3303/4096 [00:07<00:01, 450.80it/s]
Adding requests:  82%|████████▏ | 3351/4096 [00:07<00:01, 458.20it/s]
Adding requests:  83%|████████▎ | 3397/4096 [00:07<00:01, 458.22it/s]
Adding requests:  84%|████████▍ | 3445/4096 [00:07<00:01, 460.55it/s]
Adding requests:  85%|████████▌ | 3492/4096 [00:07<00:01, 459.24it/s]
Adding requests:  86%|████████▋ | 3539/4096 [00:07<00:01, 462.16it/s]
Adding requests:  88%|████████▊ | 3586/4096 [00:07<00:01, 464.10it/s]
Adding requests:  89%|████████▊ | 3633/4096 [00:07<00:00, 463.12it/s]
Adding requests:  90%|████████▉ | 3681/4096 [00:07<00:00, 465.55it/s]
Adding requests:  91%|█████████ | 3729/4096 [00:08<00:00, 466.51it/s]
Adding requests:  92%|█████████▏| 3778/4096 [00:08<00:00, 473.28it/s]
Adding requests:  93%|█████████▎| 3826/4096 [00:08<00:00, 472.84it/s]
Adding requests:  95%|█████████▍| 3876/4096 [00:08<00:00, 478.19it/s]
Adding requests:  96%|█████████▌| 3924/4096 [00:08<00:00, 477.17it/s]
Adding requests:  97%|█████████▋| 3972/4096 [00:08<00:00, 474.64it/s]
Adding requests:  98%|█████████▊| 4020/4096 [00:08<00:00, 475.13it/s]
Adding requests:  99%|█████████▉| 4068/4096 [00:08<00:00, 467.07it/s]
Adding requests: 100%|██████████| 4096/4096 [00:08<00:00, 461.69it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  72%|███████▏  | 2932/4096 [00:00<00:00, 15017.34it/s, est. speed input: 15378394.16 toks/s, output: 15017.49 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:03<00:00, 15017.34it/s, est. speed input: 1162668.43 toks/s, output: 1135.42 toks/s]  
Processed prompts: 100%|██████████| 4096/4096 [00:03<00:00, 1135.37it/s, est. speed input: 1162668.43 toks/s, output: 1135.42 toks/s] 
[rank0]:[W128 13:58:00.518425343 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 68.3s

测试结果:
  Requests/s:   328.17
  Tokens/s:     336374.14
  Total Reqs:   4096
  Elapsed:      12.48s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     336045.97


------------------------------------------------------------
  生成 CSV: BitNet-2B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_4/BitNet-2B-FP8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,33.0269,16942.7963,3.8756
1024,1024,1,128,128,32.7420,33560.5641,3.9094
2048,1024,2,256,128,67.9783,69677.7760,3.7659
4096,1024,4,512,128,131.0746,134351.4206,3.9062
8192,1024,8,1024,128,246.6724,252839.2173,4.1513
16384,1024,16,2048,128,312.0315,319832.2736,6.5634
32768,1024,32,4096,128,328.1699,336374.1400,12.4813

------------------------------------------------------------

[INFO] 完成: 7 成功, 0 失败

============================================================
  BitNet-2B-FP8 | cuSPARSELt (2_6) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_6
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6

============================================================
[1/7] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 13:58:09 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3267849) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3267849) WARNING 01-28 13:58:24 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=3267849) WARNING 01-28 13:58:34 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 32.70 requests/s, 16776.89 total tokens/s, 32.70 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-28 13:58:08] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:58:09] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 13:58:09] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 13:58:09] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:58:09] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:58:09] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:58:09] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:58:09] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:58:09] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 13:58:09] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:58:09] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:58:09] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:58:09] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:58:09] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 13:58:15] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:58:15] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 13:58:15] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 13:58:15] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:58:15] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:58:15] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:58:15] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:58:15] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:58:15] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 13:58:15] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:58:15] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:58:15] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:58:15] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:58:15] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3267849) [2026-01-28 13:58:16] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3267849) [2026-01-28 13:58:16] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3267849) [2026-01-28 13:58:16] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3267849) [2026-01-28 13:58:16] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3267849) [2026-01-28 13:58:16] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3267849) [2026-01-28 13:58:16] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3267849) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3267849) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.68it/s]
(EngineCore_DP0 pid=3267849) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.68it/s]
(EngineCore_DP0 pid=3267849) 
(EngineCore_DP0 pid=3267849) [2026-01-28 13:58:17] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=3267849) [2026-01-28 13:58:17] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8232960 bytes
(EngineCore_DP0 pid=3267849) [2026-01-28 13:58:17] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=3267849) [2026-01-28 13:58:17] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5488640 bytes
(EngineCore_DP0 pid=3267849) [2026-01-28 13:58:17] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=3267849) [2026-01-28 13:58:17] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 29638656 bytes
(EngineCore_DP0 pid=3267849) [2026-01-28 13:58:17] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=3267849) [2026-01-28 13:58:17] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14745600 bytes
(EngineCore_DP0 pid=3267849) 2026-01-28 13:58:34,612 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3267849) 2026-01-28 13:58:34,639 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3267849) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  3.55it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.57it/s]
(EngineCore_DP0 pid=3267849) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 14.20it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  59%|█████▊    | 75/128 [00:00<00:00, 743.07it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 756.30it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:17,  7.28it/s, est. speed input: 3729.09 toks/s, output: 7.28 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:05, 22.23it/s, est. speed input: 10132.79 toks/s, output: 19.79 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:04, 27.87it/s, est. speed input: 12583.49 toks/s, output: 24.58 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:03, 30.74it/s, est. speed input: 13887.71 toks/s, output: 27.12 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:03, 32.25it/s, est. speed input: 14663.83 toks/s, output: 28.64 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:03, 33.20it/s, est. speed input: 15195.78 toks/s, output: 29.68 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:00<00:03, 33.84it/s, est. speed input: 15585.25 toks/s, output: 30.44 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:00<00:02, 34.29it/s, est. speed input: 15884.39 toks/s, output: 31.02 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:01<00:02, 34.60it/s, est. speed input: 16120.70 toks/s, output: 31.49 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:01<00:02, 34.83it/s, est. speed input: 16313.33 toks/s, output: 31.86 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:01<00:02, 34.93it/s, est. speed input: 16464.32 toks/s, output: 32.16 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:01<00:02, 35.05it/s, est. speed input: 16596.97 toks/s, output: 32.42 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:01<00:02, 35.08it/s, est. speed input: 16702.80 toks/s, output: 32.62 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:01<00:02, 35.17it/s, est. speed input: 16801.25 toks/s, output: 32.81 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:01<00:02, 35.31it/s, est. speed input: 16895.15 toks/s, output: 33.00 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:01<00:01, 35.37it/s, est. speed input: 16973.74 toks/s, output: 33.15 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:01<00:01, 35.36it/s, est. speed input: 17038.48 toks/s, output: 33.28 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:02<00:01, 35.37it/s, est. speed input: 17098.13 toks/s, output: 33.39 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:02<00:01, 34.92it/s, est. speed input: 17112.44 toks/s, output: 33.42 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:02<00:01, 35.07it/s, est. speed input: 17162.85 toks/s, output: 33.52 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:02<00:01, 35.19it/s, est. speed input: 17209.41 toks/s, output: 33.61 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:02<00:01, 35.24it/s, est. speed input: 17249.53 toks/s, output: 33.69 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:02<00:01, 35.35it/s, est. speed input: 17291.16 toks/s, output: 33.77 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:02<00:00, 35.14it/s, est. speed input: 17310.16 toks/s, output: 33.81 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:02<00:00, 35.18it/s, est. speed input: 17340.29 toks/s, output: 33.87 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:02<00:00, 35.25it/s, est. speed input: 17370.32 toks/s, output: 33.93 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:03<00:00, 35.27it/s, est. speed input: 17396.60 toks/s, output: 33.98 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:03<00:00, 35.29it/s, est. speed input: 17421.02 toks/s, output: 34.03 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:03<00:00, 35.12it/s, est. speed input: 17433.51 toks/s, output: 34.05 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:03<00:00, 35.21it/s, est. speed input: 17456.47 toks/s, output: 34.09 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:03<00:00, 35.19it/s, est. speed input: 17473.61 toks/s, output: 34.13 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:03<00:00, 35.23it/s, est. speed input: 17492.67 toks/s, output: 34.17 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 35.23it/s, est. speed input: 17505.72 toks/s, output: 34.19 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 34.19it/s, est. speed input: 17505.72 toks/s, output: 34.19 toks/s]
[rank0]:[W128 13:58:40.204467028 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 40.6s

测试结果:
  Requests/s:   32.70
  Tokens/s:     16776.89
  Total Reqs:   128
  Elapsed:      3.91s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     16744.19

============================================================
[2/7] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 13:58:49 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3269041) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3269041) WARNING 01-28 13:59:05 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=3269041) WARNING 01-28 13:59:15 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 32.78 requests/s, 33602.09 total tokens/s, 32.78 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-28 13:58:49] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:58:49] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 13:58:49] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 13:58:49] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:58:49] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:58:49] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:58:49] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:58:49] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:58:49] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 13:58:49] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:58:49] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:58:49] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:58:49] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:58:49] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 13:58:56] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:58:56] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 13:58:56] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 13:58:56] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:58:56] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:58:56] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:58:56] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:58:56] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:58:56] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 13:58:56] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:58:56] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:58:56] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:58:56] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:58:56] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3269041) [2026-01-28 13:58:57] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3269041) [2026-01-28 13:58:57] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3269041) [2026-01-28 13:58:57] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3269041) [2026-01-28 13:58:57] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3269041) [2026-01-28 13:58:57] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3269041) [2026-01-28 13:58:57] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3269041) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3269041) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.70it/s]
(EngineCore_DP0 pid=3269041) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.70it/s]
(EngineCore_DP0 pid=3269041) 
(EngineCore_DP0 pid=3269041) [2026-01-28 13:58:58] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=3269041) [2026-01-28 13:58:58] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8232960 bytes
(EngineCore_DP0 pid=3269041) [2026-01-28 13:58:58] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=3269041) [2026-01-28 13:58:58] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5488640 bytes
(EngineCore_DP0 pid=3269041) [2026-01-28 13:58:58] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=3269041) [2026-01-28 13:58:58] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 29638656 bytes
(EngineCore_DP0 pid=3269041) [2026-01-28 13:58:58] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=3269041) [2026-01-28 13:58:58] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14745600 bytes
(EngineCore_DP0 pid=3269041) 2026-01-28 13:59:15,463 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3269041) 2026-01-28 13:59:15,489 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3269041) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 14.58it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 14.57it/s]
(EngineCore_DP0 pid=3269041) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 14.65it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  31%|███▏      | 40/128 [00:00<00:00, 396.79it/s]
Adding requests:  66%|██████▋   | 85/128 [00:00<00:00, 424.45it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 224.43it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 255.36it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   9%|▊         | 11/128 [00:00<00:01, 109.83it/s, est. speed input: 112471.17 toks/s, output: 109.83 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:00<00:02, 49.22it/s, est. speed input: 54948.47 toks/s, output: 53.66 toks/s]   
Processed prompts:  23%|██▎       | 29/128 [00:00<00:02, 43.32it/s, est. speed input: 48861.91 toks/s, output: 47.72 toks/s]
Processed prompts:  27%|██▋       | 35/128 [00:00<00:02, 40.60it/s, est. speed input: 46107.47 toks/s, output: 45.03 toks/s]
Processed prompts:  31%|███▏      | 40/128 [00:00<00:02, 39.08it/s, est. speed input: 44585.91 toks/s, output: 43.54 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:01<00:02, 38.02it/s, est. speed input: 43483.63 toks/s, output: 42.46 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:01<00:02, 37.34it/s, est. speed input: 42775.00 toks/s, output: 41.77 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:01<00:02, 36.81it/s, est. speed input: 42196.86 toks/s, output: 41.21 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:01<00:01, 36.45it/s, est. speed input: 41725.92 toks/s, output: 40.75 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:01<00:01, 36.13it/s, est. speed input: 41306.59 toks/s, output: 40.34 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:01<00:01, 35.85it/s, est. speed input: 40936.54 toks/s, output: 39.98 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:01<00:01, 35.74it/s, est. speed input: 40636.02 toks/s, output: 39.68 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:01<00:01, 35.68it/s, est. speed input: 40377.97 toks/s, output: 39.43 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:01<00:01, 35.66it/s, est. speed input: 40155.47 toks/s, output: 39.21 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:02<00:01, 35.55it/s, est. speed input: 39936.09 toks/s, output: 39.00 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:02<00:01, 35.50it/s, est. speed input: 39744.62 toks/s, output: 38.81 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:02<00:01, 35.53it/s, est. speed input: 39583.98 toks/s, output: 38.66 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:02<00:00, 35.52it/s, est. speed input: 39432.44 toks/s, output: 38.51 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:02<00:00, 35.48it/s, est. speed input: 39290.56 toks/s, output: 38.37 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:02<00:00, 35.50it/s, est. speed input: 39166.98 toks/s, output: 38.25 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:02<00:00, 35.45it/s, est. speed input: 39044.87 toks/s, output: 38.13 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:02<00:00, 35.42it/s, est. speed input: 38932.02 toks/s, output: 38.02 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:02<00:00, 35.40it/s, est. speed input: 38827.97 toks/s, output: 37.92 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:03<00:00, 35.45it/s, est. speed input: 38740.83 toks/s, output: 37.83 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:03<00:00, 35.49it/s, est. speed input: 38659.35 toks/s, output: 37.75 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:03<00:00, 35.50it/s, est. speed input: 38581.77 toks/s, output: 37.68 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 35.50it/s, est. speed input: 38524.69 toks/s, output: 37.62 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 37.62it/s, est. speed input: 38524.69 toks/s, output: 37.62 toks/s]
[rank0]:[W128 13:59:21.347724730 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 40.0s

测试结果:
  Requests/s:   32.78
  Tokens/s:     33602.09
  Total Reqs:   128
  Elapsed:      3.90s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     33569.31

============================================================
[3/7] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 13:59:30 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3270166) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3270166) WARNING 01-28 13:59:45 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=3270166) WARNING 01-28 13:59:55 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 68.15 requests/s, 69858.67 total tokens/s, 68.15 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-28 13:59:30] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:59:30] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 13:59:30] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 13:59:30] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:59:30] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:59:30] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:59:30] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:59:30] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:59:30] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 13:59:30] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:59:30] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:59:30] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:59:30] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:59:30] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 13:59:36] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 13:59:37] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 13:59:37] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 13:59:37] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:59:37] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:59:37] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:59:37] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:59:37] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 13:59:37] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 13:59:37] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 13:59:37] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 13:59:37] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 13:59:37] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 13:59:37] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3270166) [2026-01-28 13:59:37] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3270166) [2026-01-28 13:59:38] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3270166) [2026-01-28 13:59:38] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3270166) [2026-01-28 13:59:38] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3270166) [2026-01-28 13:59:38] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3270166) [2026-01-28 13:59:38] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3270166) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3270166) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.69it/s]
(EngineCore_DP0 pid=3270166) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.69it/s]
(EngineCore_DP0 pid=3270166) 
(EngineCore_DP0 pid=3270166) [2026-01-28 13:59:39] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=3270166) [2026-01-28 13:59:39] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8232960 bytes
(EngineCore_DP0 pid=3270166) [2026-01-28 13:59:39] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=3270166) [2026-01-28 13:59:39] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5488640 bytes
(EngineCore_DP0 pid=3270166) [2026-01-28 13:59:39] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=3270166) [2026-01-28 13:59:39] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 29638656 bytes
(EngineCore_DP0 pid=3270166) [2026-01-28 13:59:39] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=3270166) [2026-01-28 13:59:39] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14745600 bytes
(EngineCore_DP0 pid=3270166) 2026-01-28 13:59:55,710 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3270166) 2026-01-28 13:59:55,736 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3270166) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:00<00:00, 15.07it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 11.84it/s]
(EngineCore_DP0 pid=3270166) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 13.60it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 13.59it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:  16%|█▌        | 40/256 [00:00<00:00, 396.69it/s]
Adding requests:  33%|███▎      | 84/256 [00:00<00:00, 421.46it/s]
Adding requests:  50%|█████     | 129/256 [00:00<00:00, 430.61it/s]
Adding requests:  68%|██████▊   | 173/256 [00:00<00:00, 432.76it/s]
Adding requests:  85%|████████▌ | 218/256 [00:00<00:00, 438.34it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 438.28it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  16%|█▌        | 40/256 [00:00<00:00, 363.48it/s, est. speed input: 372227.65 toks/s, output: 363.49 toks/s]
Processed prompts:  30%|███       | 77/256 [00:00<00:01, 111.03it/s, est. speed input: 127493.19 toks/s, output: 124.50 toks/s]
Processed prompts:  38%|███▊      | 97/256 [00:00<00:01, 94.60it/s, est. speed input: 110242.85 toks/s, output: 107.66 toks/s] 
Processed prompts:  43%|████▎     | 111/256 [00:01<00:01, 87.81it/s, est. speed input: 103526.18 toks/s, output: 101.10 toks/s]
Processed prompts:  48%|████▊     | 123/256 [00:01<00:01, 83.36it/s, est. speed input: 99380.29 toks/s, output: 97.05 toks/s]  
Processed prompts:  52%|█████▏    | 133/256 [00:01<00:01, 80.39it/s, est. speed input: 96697.24 toks/s, output: 94.43 toks/s]
Processed prompts:  55%|█████▌    | 142/256 [00:01<00:01, 75.52it/s, est. speed input: 93546.09 toks/s, output: 91.35 toks/s]
Processed prompts:  59%|█████▊    | 150/256 [00:01<00:01, 74.44it/s, est. speed input: 92110.43 toks/s, output: 89.95 toks/s]
Processed prompts:  62%|██████▏   | 158/256 [00:01<00:01, 73.61it/s, est. speed input: 90881.14 toks/s, output: 88.75 toks/s]
Processed prompts:  65%|██████▍   | 166/256 [00:01<00:01, 72.98it/s, est. speed input: 89809.87 toks/s, output: 87.70 toks/s]
Processed prompts:  68%|██████▊   | 174/256 [00:02<00:01, 72.54it/s, est. speed input: 88873.07 toks/s, output: 86.79 toks/s]
Processed prompts:  71%|███████   | 182/256 [00:02<00:01, 72.13it/s, est. speed input: 88014.83 toks/s, output: 85.95 toks/s]
Processed prompts:  74%|███████▍  | 190/256 [00:02<00:00, 71.41it/s, est. speed input: 87155.21 toks/s, output: 85.11 toks/s]
Processed prompts:  77%|███████▋  | 198/256 [00:02<00:00, 71.31it/s, est. speed input: 86463.76 toks/s, output: 84.44 toks/s]
Processed prompts:  80%|████████  | 206/256 [00:02<00:00, 71.27it/s, est. speed input: 85842.57 toks/s, output: 83.83 toks/s]
Processed prompts:  84%|████████▎ | 214/256 [00:02<00:00, 71.08it/s, est. speed input: 85247.54 toks/s, output: 83.25 toks/s]
Processed prompts:  87%|████████▋ | 222/256 [00:02<00:00, 71.09it/s, est. speed input: 84726.10 toks/s, output: 82.74 toks/s]
Processed prompts:  90%|████████▉ | 230/256 [00:02<00:00, 70.99it/s, est. speed input: 84230.37 toks/s, output: 82.26 toks/s]
Processed prompts:  93%|█████████▎| 238/256 [00:02<00:00, 69.68it/s, est. speed input: 83577.98 toks/s, output: 81.62 toks/s]
Processed prompts:  96%|█████████▌| 246/256 [00:03<00:00, 69.94it/s, est. speed input: 83153.99 toks/s, output: 81.20 toks/s]
Processed prompts:  99%|█████████▉| 254/256 [00:03<00:00, 70.23it/s, est. speed input: 82775.86 toks/s, output: 80.84 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 70.23it/s, est. speed input: 82670.26 toks/s, output: 80.73 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 80.73it/s, est. speed input: 82670.26 toks/s, output: 80.73 toks/s]
[rank0]:[W128 14:00:01.838582338 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 40.5s

测试结果:
  Requests/s:   68.15
  Tokens/s:     69858.67
  Total Reqs:   256
  Elapsed:      3.76s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     69790.51

============================================================
[4/7] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 14:00:12 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3271297) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3271297) WARNING 01-28 14:00:27 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=3271297) WARNING 01-28 14:00:37 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 131.51 requests/s, 134799.96 total tokens/s, 131.51 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-28 14:00:11] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 14:00:12] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 14:00:12] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 14:00:12] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:00:12] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:00:12] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:00:12] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:00:12] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:00:12] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 14:00:12] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 14:00:12] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 14:00:12] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 14:00:12] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 14:00:12] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 14:00:18] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 14:00:18] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 14:00:18] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 14:00:18] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:00:18] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:00:18] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:00:18] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:00:18] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:00:18] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 14:00:18] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 14:00:18] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 14:00:18] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 14:00:18] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 14:00:18] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3271297) [2026-01-28 14:00:19] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3271297) [2026-01-28 14:00:19] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3271297) [2026-01-28 14:00:19] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3271297) [2026-01-28 14:00:19] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3271297) [2026-01-28 14:00:19] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3271297) [2026-01-28 14:00:19] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3271297) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3271297) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.68it/s]
(EngineCore_DP0 pid=3271297) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.68it/s]
(EngineCore_DP0 pid=3271297) 
(EngineCore_DP0 pid=3271297) [2026-01-28 14:00:20] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=3271297) [2026-01-28 14:00:20] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8232960 bytes
(EngineCore_DP0 pid=3271297) [2026-01-28 14:00:20] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=3271297) [2026-01-28 14:00:20] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5488640 bytes
(EngineCore_DP0 pid=3271297) [2026-01-28 14:00:20] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=3271297) [2026-01-28 14:00:20] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 29638656 bytes
(EngineCore_DP0 pid=3271297) [2026-01-28 14:00:20] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=3271297) [2026-01-28 14:00:20] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14745600 bytes
(EngineCore_DP0 pid=3271297) 2026-01-28 14:00:37,361 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3271297) 2026-01-28 14:00:37,387 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3271297) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00, 12.74it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 12.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 12.93it/s]
(EngineCore_DP0 pid=3271297) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00, 15.90it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 16.07it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   8%|▊         | 40/512 [00:00<00:01, 396.89it/s]
Adding requests:  16%|█▋        | 84/512 [00:00<00:01, 421.22it/s]
Adding requests:  25%|██▌       | 129/512 [00:00<00:00, 431.06it/s]
Adding requests:  34%|███▍      | 173/512 [00:00<00:00, 432.82it/s]
Adding requests:  43%|████▎     | 219/512 [00:00<00:00, 440.14it/s]
Adding requests:  52%|█████▏    | 267/512 [00:00<00:00, 450.27it/s]
Adding requests:  61%|██████    | 313/512 [00:00<00:00, 449.14it/s]
Adding requests:  70%|███████   | 359/512 [00:00<00:00, 450.31it/s]
Adding requests:  79%|███████▉  | 407/512 [00:00<00:00, 456.53it/s]
Adding requests:  88%|████████▊ | 453/512 [00:01<00:00, 457.50it/s]
Adding requests:  98%|█████████▊| 500/512 [00:01<00:00, 457.62it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 448.26it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  29%|██▊       | 146/512 [00:00<00:00, 1290.58it/s, est. speed input: 1321631.46 toks/s, output: 1290.60 toks/s]
Processed prompts:  54%|█████▍    | 276/512 [00:01<00:01, 233.32it/s, est. speed input: 274626.62 toks/s, output: 268.19 toks/s]   
Processed prompts:  66%|██████▌   | 338/512 [00:01<00:00, 194.81it/s, est. speed input: 233058.71 toks/s, output: 227.60 toks/s]
Processed prompts:  74%|███████▍  | 378/512 [00:01<00:00, 178.04it/s, est. speed input: 216756.78 toks/s, output: 211.68 toks/s]
Processed prompts:  80%|███████▉  | 408/512 [00:01<00:00, 172.19it/s, est. speed input: 210334.94 toks/s, output: 205.40 toks/s]
Processed prompts:  85%|████████▍ | 433/512 [00:02<00:00, 166.89it/s, est. speed input: 205483.80 toks/s, output: 200.67 toks/s]
Processed prompts:  89%|████████▊ | 454/512 [00:02<00:00, 154.58it/s, est. speed input: 198565.54 toks/s, output: 193.91 toks/s]
Processed prompts:  92%|█████████▏| 472/512 [00:02<00:00, 155.07it/s, est. speed input: 196822.52 toks/s, output: 192.21 toks/s]
Processed prompts:  96%|█████████▌| 490/512 [00:02<00:00, 148.19it/s, est. speed input: 193088.40 toks/s, output: 188.56 toks/s]
Processed prompts:  99%|█████████▉| 506/512 [00:02<00:00, 146.44it/s, est. speed input: 190989.03 toks/s, output: 186.51 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:02<00:00, 146.44it/s, est. speed input: 190658.73 toks/s, output: 186.19 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:02<00:00, 186.18it/s, est. speed input: 190658.73 toks/s, output: 186.19 toks/s]
[rank0]:[W128 14:00:43.744558176 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 41.9s

测试结果:
  Requests/s:   131.51
  Tokens/s:     134799.96
  Total Reqs:   512
  Elapsed:      3.89s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     134668.44

============================================================
[5/7] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 14:00:56 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3272474) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3272474) WARNING 01-28 14:01:11 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=3272474) WARNING 01-28 14:01:21 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 236.74 requests/s, 242662.10 total tokens/s, 236.74 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-28 14:00:55] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 14:00:56] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 14:00:56] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 14:00:56] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:00:56] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:00:56] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:00:56] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:00:56] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:00:56] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 14:00:56] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 14:00:56] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 14:00:56] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 14:00:56] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 14:00:56] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 14:01:02] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 14:01:03] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 14:01:03] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 14:01:03] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:01:03] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:01:03] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:01:03] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:01:03] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:01:03] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 14:01:03] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 14:01:03] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 14:01:03] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 14:01:03] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 14:01:03] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3272474) [2026-01-28 14:01:03] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3272474) [2026-01-28 14:01:03] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3272474) [2026-01-28 14:01:03] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3272474) [2026-01-28 14:01:03] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3272474) [2026-01-28 14:01:03] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3272474) [2026-01-28 14:01:03] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3272474) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3272474) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.70it/s]
(EngineCore_DP0 pid=3272474) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.70it/s]
(EngineCore_DP0 pid=3272474) 
(EngineCore_DP0 pid=3272474) [2026-01-28 14:01:04] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=3272474) [2026-01-28 14:01:04] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8232960 bytes
(EngineCore_DP0 pid=3272474) [2026-01-28 14:01:04] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=3272474) [2026-01-28 14:01:04] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5488640 bytes
(EngineCore_DP0 pid=3272474) [2026-01-28 14:01:04] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=3272474) [2026-01-28 14:01:04] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 29638656 bytes
(EngineCore_DP0 pid=3272474) [2026-01-28 14:01:04] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=3272474) [2026-01-28 14:01:04] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14745600 bytes
(EngineCore_DP0 pid=3272474) 2026-01-28 14:01:21,256 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3272474) 2026-01-28 14:01:21,283 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3272474) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00, 15.11it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00, 15.75it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 14.42it/s]
(EngineCore_DP0 pid=3272474) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00, 15.87it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 16.01it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 15.98it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   4%|▍         | 40/1024 [00:00<00:02, 398.18it/s]
Adding requests:   8%|▊         | 85/1024 [00:00<00:02, 424.85it/s]
Adding requests:  13%|█▎        | 130/1024 [00:00<00:02, 434.23it/s]
Adding requests:  17%|█▋        | 174/1024 [00:00<00:01, 435.38it/s]
Adding requests:  21%|██▏       | 220/1024 [00:00<00:01, 441.74it/s]
Adding requests:  26%|██▌       | 268/1024 [00:00<00:01, 452.75it/s]
Adding requests:  31%|███       | 314/1024 [00:00<00:01, 450.44it/s]
Adding requests:  35%|███▌      | 360/1024 [00:00<00:01, 452.22it/s]
Adding requests:  40%|███▉      | 406/1024 [00:00<00:01, 454.23it/s]
Adding requests:  44%|████▍     | 453/1024 [00:01<00:01, 456.90it/s]
Adding requests:  49%|████▉     | 500/1024 [00:01<00:01, 455.87it/s]
Adding requests:  53%|█████▎    | 546/1024 [00:01<00:01, 451.45it/s]
Adding requests:  58%|█████▊    | 595/1024 [00:01<00:00, 460.57it/s]
Adding requests:  63%|██████▎   | 643/1024 [00:01<00:00, 463.41it/s]
Adding requests:  68%|██████▊   | 692/1024 [00:01<00:00, 469.82it/s]
Adding requests:  72%|███████▏  | 740/1024 [00:01<00:00, 470.35it/s]
Adding requests:  77%|███████▋  | 788/1024 [00:01<00:00, 467.70it/s]
Adding requests:  82%|████████▏ | 835/1024 [00:01<00:00, 457.33it/s]
Adding requests:  86%|████████▌ | 883/1024 [00:01<00:00, 462.89it/s]
Adding requests:  91%|█████████ | 932/1024 [00:02<00:00, 468.57it/s]
Adding requests:  96%|█████████▌| 979/1024 [00:02<00:00, 468.34it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 458.20it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  52%|█████▏    | 530/1024 [00:00<00:00, 4870.30it/s, est. speed input: 4987516.96 toks/s, output: 4870.40 toks/s]
Processed prompts:  99%|█████████▉| 1018/1024 [00:02<00:00, 417.71it/s, est. speed input: 498985.97 toks/s, output: 487.29 toks/s]  
Processed prompts: 100%|██████████| 1024/1024 [00:02<00:00, 417.71it/s, est. speed input: 501888.57 toks/s, output: 490.12 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:02<00:00, 490.10it/s, est. speed input: 501888.57 toks/s, output: 490.12 toks/s]
[rank0]:[W128 14:01:28.257615850 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 44.5s

测试结果:
  Requests/s:   236.74
  Tokens/s:     242662.10
  Total Reqs:   1024
  Elapsed:      4.33s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     242425.36

============================================================
[6/7] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 14:01:45 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3273697) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3273697) WARNING 01-28 14:02:00 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=3273697) WARNING 01-28 14:02:10 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 270.54 requests/s, 277299.77 total tokens/s, 270.54 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-28 14:01:44] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 14:01:45] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 14:01:45] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 14:01:45] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:01:45] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:01:45] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:01:45] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:01:45] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:01:45] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 14:01:45] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 14:01:45] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 14:01:45] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 14:01:45] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 14:01:45] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 14:01:51] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 14:01:52] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 14:01:52] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 14:01:52] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:01:52] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:01:52] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:01:52] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:01:52] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:01:52] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 14:01:52] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 14:01:52] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 14:01:52] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 14:01:52] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 14:01:52] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3273697) [2026-01-28 14:01:53] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3273697) [2026-01-28 14:01:53] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3273697) [2026-01-28 14:01:53] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3273697) [2026-01-28 14:01:53] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3273697) [2026-01-28 14:01:53] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3273697) [2026-01-28 14:01:53] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3273697) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3273697) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.71it/s]
(EngineCore_DP0 pid=3273697) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.71it/s]
(EngineCore_DP0 pid=3273697) 
(EngineCore_DP0 pid=3273697) [2026-01-28 14:01:54] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=3273697) [2026-01-28 14:01:54] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8232960 bytes
(EngineCore_DP0 pid=3273697) [2026-01-28 14:01:54] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=3273697) [2026-01-28 14:01:54] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5488640 bytes
(EngineCore_DP0 pid=3273697) [2026-01-28 14:01:54] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=3273697) [2026-01-28 14:01:54] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 29638656 bytes
(EngineCore_DP0 pid=3273697) [2026-01-28 14:01:54] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=3273697) [2026-01-28 14:01:54] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14745600 bytes
(EngineCore_DP0 pid=3273697) 2026-01-28 14:02:10,520 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3273697) 2026-01-28 14:02:10,548 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3273697) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00, 13.10it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00, 14.06it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:00<00:00, 14.96it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 13.94it/s]
(EngineCore_DP0 pid=3273697) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00, 14.20it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00, 15.03it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 15.17it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 39/2048 [00:00<00:05, 388.18it/s]
Adding requests:   4%|▍         | 81/2048 [00:00<00:04, 402.45it/s]
Adding requests:   6%|▌         | 123/2048 [00:00<00:04, 407.23it/s]
Adding requests:   8%|▊         | 165/2048 [00:00<00:04, 411.51it/s]
Adding requests:  10%|█         | 210/2048 [00:00<00:04, 422.53it/s]
Adding requests:  13%|█▎        | 257/2048 [00:00<00:04, 437.42it/s]
Adding requests:  15%|█▍        | 301/2048 [00:00<00:03, 437.91it/s]
Adding requests:  17%|█▋        | 348/2048 [00:00<00:03, 445.42it/s]
Adding requests:  19%|█▉        | 393/2048 [00:00<00:03, 444.69it/s]
Adding requests:  21%|██▏       | 439/2048 [00:01<00:03, 448.11it/s]
Adding requests:  24%|██▍       | 487/2048 [00:01<00:03, 455.31it/s]
Adding requests:  26%|██▌       | 533/2048 [00:01<00:03, 446.03it/s]
Adding requests:  28%|██▊       | 582/2048 [00:01<00:03, 458.34it/s]
Adding requests:  31%|███       | 628/2048 [00:01<00:03, 458.59it/s]
Adding requests:  33%|███▎      | 676/2048 [00:01<00:02, 464.88it/s]
Adding requests:  35%|███▌      | 725/2048 [00:01<00:02, 470.03it/s]
Adding requests:  38%|███▊      | 773/2048 [00:01<00:02, 467.47it/s]
Adding requests:  40%|████      | 820/2048 [00:01<00:02, 460.83it/s]
Adding requests:  42%|████▏     | 867/2048 [00:01<00:02, 462.64it/s]
Adding requests:  45%|████▍     | 915/2048 [00:02<00:02, 467.76it/s]
Adding requests:  47%|████▋     | 964/2048 [00:02<00:02, 471.26it/s]
Adding requests:  49%|████▉     | 1012/2048 [00:02<00:02, 472.75it/s]
Adding requests:  52%|█████▏    | 1060/2048 [00:02<00:02, 471.81it/s]
Adding requests:  54%|█████▍    | 1108/2048 [00:02<00:02, 468.84it/s]
Adding requests:  56%|█████▋    | 1156/2048 [00:02<00:01, 470.17it/s]
Adding requests:  59%|█████▉    | 1207/2048 [00:02<00:01, 481.28it/s]
Adding requests:  61%|██████▏   | 1256/2048 [00:02<00:01, 475.95it/s]
Adding requests:  64%|██████▎   | 1304/2048 [00:02<00:01, 476.63it/s]
Adding requests:  66%|██████▌   | 1353/2048 [00:02<00:01, 478.16it/s]
Adding requests:  69%|██████▊   | 1403/2048 [00:03<00:01, 482.48it/s]
Adding requests:  71%|███████   | 1452/2048 [00:03<00:01, 460.80it/s]
Adding requests:  73%|███████▎  | 1502/2048 [00:03<00:01, 471.05it/s]
Adding requests:  76%|███████▌  | 1550/2048 [00:03<00:01, 471.57it/s]
Adding requests:  78%|███████▊  | 1600/2048 [00:03<00:00, 478.25it/s]
Adding requests:  80%|████████  | 1648/2048 [00:03<00:00, 478.03it/s]
Adding requests:  83%|████████▎ | 1696/2048 [00:03<00:00, 474.85it/s]
Adding requests:  85%|████████▌ | 1744/2048 [00:03<00:00, 463.09it/s]
Adding requests:  87%|████████▋ | 1791/2048 [00:03<00:00, 463.50it/s]
Adding requests:  90%|████████▉ | 1839/2048 [00:03<00:00, 465.93it/s]
Adding requests:  92%|█████████▏| 1887/2048 [00:04<00:00, 468.44it/s]
Adding requests:  94%|█████████▍| 1935/2048 [00:04<00:00, 469.97it/s]
Adding requests:  97%|█████████▋| 1983/2048 [00:04<00:00, 470.81it/s]
Adding requests:  99%|█████████▉| 2031/2048 [00:04<00:00, 469.63it/s]
Adding requests: 100%|██████████| 2048/2048 [00:04<00:00, 461.88it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  59%|█████▊    | 1202/2048 [00:00<00:00, 10958.59it/s, est. speed input: 11222399.56 toks/s, output: 10958.76 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:03<00:00, 10958.59it/s, est. speed input: 669072.67 toks/s, output: 653.39 toks/s]    
Processed prompts: 100%|██████████| 2048/2048 [00:03<00:00, 653.36it/s, est. speed input: 669072.67 toks/s, output: 653.39 toks/s]  
[rank0]:[W128 14:02:20.866078472 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 52.7s

测试结果:
  Requests/s:   270.54
  Tokens/s:     277299.77
  Total Reqs:   2048
  Elapsed:      7.57s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     277029.24

============================================================
[7/7] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:6)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 14:02:47 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3275109) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3275109) WARNING 01-28 14:03:03 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=3275109) WARNING 01-28 14:03:13 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 284.01 requests/s, 291105.61 total tokens/s, 284.01 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-28 14:02:46] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 14:02:47] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 14:02:47] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 14:02:47] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:02:47] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:02:47] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:02:47] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:02:47] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:02:47] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 14:02:47] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 14:02:47] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 14:02:47] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 14:02:47] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 14:02:47] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 14:02:53] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 14:02:54] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 14:02:54] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 14:02:54] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:02:54] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:02:54] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:02:54] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:02:54] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:02:54] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 14:02:54] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 14:02:54] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 14:02:54] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 14:02:54] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 14:02:54] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3275109) [2026-01-28 14:02:55] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3275109) [2026-01-28 14:02:55] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3275109) [2026-01-28 14:02:55] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3275109) [2026-01-28 14:02:55] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:6, expand_ratio=1.500
(EngineCore_DP0 pid=3275109) [2026-01-28 14:02:55] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3275109) [2026-01-28 14:02:55] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3275109) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3275109) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.71it/s]
(EngineCore_DP0 pid=3275109) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.71it/s]
(EngineCore_DP0 pid=3275109) 
(EngineCore_DP0 pid=3275109) [2026-01-28 14:02:56] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3424] -> 1D uint8
(EngineCore_DP0 pid=3275109) [2026-01-28 14:02:56] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 8232960 bytes
(EngineCore_DP0 pid=3275109) [2026-01-28 14:02:56] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3424] -> 1D uint8
(EngineCore_DP0 pid=3275109) [2026-01-28 14:02:56] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 5488640 bytes
(EngineCore_DP0 pid=3275109) [2026-01-28 14:02:56] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3424] -> 1D uint8
(EngineCore_DP0 pid=3275109) [2026-01-28 14:02:56] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 29638656 bytes
(EngineCore_DP0 pid=3275109) [2026-01-28 14:02:56] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 9216] -> 1D uint8
(EngineCore_DP0 pid=3275109) [2026-01-28 14:02:56] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 14745600 bytes
(EngineCore_DP0 pid=3275109) [rank0]:W0128 14:03:08.348000 3275109 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3275109) [rank0]:W0128 14:03:08.431000 3275109 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3275109) [rank0]:W0128 14:03:09.543000 3275109 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3275109) [rank0]:W0128 14:03:09.675000 3275109 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3275109) 2026-01-28 14:03:13,329 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3275109) 2026-01-28 14:03:13,357 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3275109) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|▉         | 1/11 [00:00<00:01,  6.06it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|██▋       | 3/11 [00:00<00:00,  9.32it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|████▌     | 5/11 [00:00<00:00, 11.57it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  64%|██████▎   | 7/11 [00:00<00:00, 12.31it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  82%|████████▏ | 9/11 [00:00<00:00, 13.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 13.13it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 12.15it/s]
(EngineCore_DP0 pid=3275109) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00, 15.84it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00, 16.19it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00, 15.55it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 15.65it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 40/4096 [00:00<00:10, 397.11it/s]
Adding requests:   2%|▏         | 84/4096 [00:00<00:09, 420.41it/s]
Adding requests:   3%|▎         | 128/4096 [00:00<00:09, 429.10it/s]
Adding requests:   4%|▍         | 172/4096 [00:00<00:09, 430.93it/s]
Adding requests:   5%|▌         | 217/4096 [00:00<00:08, 434.97it/s]
Adding requests:   6%|▋         | 265/4096 [00:00<00:08, 448.54it/s]
Adding requests:   8%|▊         | 310/4096 [00:00<00:08, 447.26it/s]
Adding requests:   9%|▊         | 357/4096 [00:00<00:08, 451.70it/s]
Adding requests:  10%|▉         | 404/4096 [00:00<00:08, 455.69it/s]
Adding requests:  11%|█         | 451/4096 [00:01<00:07, 458.03it/s]
Adding requests:  12%|█▏        | 497/4096 [00:01<00:07, 458.62it/s]
Adding requests:  13%|█▎        | 543/4096 [00:01<00:07, 451.71it/s]
Adding requests:  14%|█▍        | 592/4096 [00:01<00:07, 461.10it/s]
Adding requests:  16%|█▌        | 640/4096 [00:01<00:07, 463.87it/s]
Adding requests:  17%|█▋        | 689/4096 [00:01<00:07, 468.76it/s]
Adding requests:  18%|█▊        | 738/4096 [00:01<00:07, 474.07it/s]
Adding requests:  19%|█▉        | 786/4096 [00:01<00:07, 468.83it/s]
Adding requests:  20%|██        | 833/4096 [00:01<00:07, 459.22it/s]
Adding requests:  22%|██▏       | 881/4096 [00:01<00:06, 463.90it/s]
Adding requests:  23%|██▎       | 928/4096 [00:02<00:06, 460.58it/s]
Adding requests:  24%|██▍       | 976/4096 [00:02<00:06, 465.03it/s]
Adding requests:  25%|██▌       | 1024/4096 [00:02<00:06, 467.77it/s]
Adding requests:  26%|██▌       | 1071/4096 [00:02<00:06, 463.28it/s]
Adding requests:  27%|██▋       | 1118/4096 [00:02<00:06, 462.00it/s]
Adding requests:  28%|██▊       | 1167/4096 [00:02<00:06, 469.87it/s]
Adding requests:  30%|██▉       | 1216/4096 [00:02<00:06, 474.54it/s]
Adding requests:  31%|███       | 1264/4096 [00:02<00:06, 468.99it/s]
Adding requests:  32%|███▏      | 1312/4096 [00:02<00:05, 469.29it/s]
Adding requests:  33%|███▎      | 1361/4096 [00:02<00:05, 473.70it/s]
Adding requests:  34%|███▍      | 1411/4096 [00:03<00:05, 478.85it/s]
Adding requests:  36%|███▌      | 1459/4096 [00:03<00:05, 476.54it/s]
Adding requests:  37%|███▋      | 1508/4096 [00:03<00:05, 480.04it/s]
Adding requests:  38%|███▊      | 1557/4096 [00:03<00:05, 478.73it/s]
Adding requests:  39%|███▉      | 1605/4096 [00:03<00:05, 473.47it/s]
Adding requests:  40%|████      | 1653/4096 [00:03<00:05, 471.96it/s]
Adding requests:  42%|████▏     | 1701/4096 [00:03<00:05, 469.76it/s]
Adding requests:  43%|████▎     | 1749/4096 [00:03<00:04, 471.48it/s]
Adding requests:  44%|████▍     | 1797/4096 [00:03<00:04, 473.60it/s]
Adding requests:  45%|████▌     | 1845/4096 [00:03<00:04, 474.60it/s]
Adding requests:  46%|████▌     | 1893/4096 [00:04<00:04, 472.50it/s]
Adding requests:  47%|████▋     | 1941/4096 [00:04<00:04, 472.62it/s]
Adding requests:  49%|████▊     | 1989/4096 [00:04<00:04, 473.55it/s]
Adding requests:  50%|████▉     | 2037/4096 [00:04<00:04, 464.57it/s]
Adding requests:  51%|█████     | 2086/4096 [00:04<00:04, 469.54it/s]
Adding requests:  52%|█████▏    | 2133/4096 [00:04<00:04, 466.75it/s]
Adding requests:  53%|█████▎    | 2180/4096 [00:04<00:04, 461.41it/s]
Adding requests:  54%|█████▍    | 2228/4096 [00:04<00:04, 465.18it/s]
Adding requests:  56%|█████▌    | 2275/4096 [00:04<00:03, 465.24it/s]
Adding requests:  57%|█████▋    | 2323/4096 [00:05<00:03, 466.34it/s]
Adding requests:  58%|█████▊    | 2370/4096 [00:05<00:03, 464.45it/s]
Adding requests:  59%|█████▉    | 2417/4096 [00:05<00:03, 464.80it/s]
Adding requests:  60%|██████    | 2464/4096 [00:05<00:03, 465.03it/s]
Adding requests:  61%|██████▏   | 2511/4096 [00:05<00:03, 465.00it/s]
Adding requests:  62%|██████▏   | 2559/4096 [00:05<00:03, 468.66it/s]
Adding requests:  64%|██████▎   | 2607/4096 [00:05<00:03, 469.20it/s]
Adding requests:  65%|██████▍   | 2655/4096 [00:05<00:03, 472.00it/s]
Adding requests:  66%|██████▌   | 2703/4096 [00:05<00:02, 467.63it/s]
Adding requests:  67%|██████▋   | 2750/4096 [00:05<00:02, 467.57it/s]
Adding requests:  68%|██████▊   | 2797/4096 [00:06<00:02, 467.05it/s]
Adding requests:  69%|██████▉   | 2845/4096 [00:06<00:02, 469.27it/s]
Adding requests:  71%|███████   | 2893/4096 [00:06<00:02, 470.50it/s]
Adding requests:  72%|███████▏  | 2941/4096 [00:06<00:02, 467.68it/s]
Adding requests:  73%|███████▎  | 2990/4096 [00:06<00:02, 471.97it/s]
Adding requests:  74%|███████▍  | 3038/4096 [00:06<00:02, 470.56it/s]
Adding requests:  75%|███████▌  | 3086/4096 [00:06<00:02, 469.79it/s]
Adding requests:  77%|███████▋  | 3135/4096 [00:06<00:02, 473.24it/s]
Adding requests:  78%|███████▊  | 3183/4096 [00:06<00:01, 471.53it/s]
Adding requests:  79%|███████▉  | 3231/4096 [00:06<00:01, 456.53it/s]
Adding requests:  80%|████████  | 3277/4096 [00:07<00:01, 446.01it/s]
Adding requests:  81%|████████  | 3324/4096 [00:07<00:01, 452.10it/s]
Adding requests:  82%|████████▏ | 3372/4096 [00:07<00:01, 459.20it/s]
Adding requests:  84%|████████▎ | 3421/4096 [00:07<00:01, 466.94it/s]
Adding requests:  85%|████████▍ | 3468/4096 [00:07<00:01, 461.88it/s]
Adding requests:  86%|████████▌ | 3515/4096 [00:07<00:01, 462.73it/s]
Adding requests:  87%|████████▋ | 3562/4096 [00:07<00:01, 462.38it/s]
Adding requests:  88%|████████▊ | 3610/4096 [00:07<00:01, 464.70it/s]
Adding requests:  89%|████████▉ | 3657/4096 [00:07<00:00, 464.26it/s]
Adding requests:  90%|█████████ | 3705/4096 [00:07<00:00, 468.77it/s]
Adding requests:  92%|█████████▏| 3753/4096 [00:08<00:00, 469.94it/s]
Adding requests:  93%|█████████▎| 3802/4096 [00:08<00:00, 475.65it/s]
Adding requests:  94%|█████████▍| 3851/4096 [00:08<00:00, 479.00it/s]
Adding requests:  95%|█████████▌| 3899/4096 [00:08<00:00, 478.05it/s]
Adding requests:  96%|█████████▋| 3948/4096 [00:08<00:00, 479.42it/s]
Adding requests:  98%|█████████▊| 3996/4096 [00:08<00:00, 474.38it/s]
Adding requests:  99%|█████████▊| 4044/4096 [00:08<00:00, 473.48it/s]
Adding requests: 100%|█████████▉| 4093/4096 [00:08<00:00, 476.83it/s]
Adding requests: 100%|██████████| 4096/4096 [00:08<00:00, 466.06it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  61%|██████    | 2485/4096 [00:00<00:00, 22115.99it/s, est. speed input: 22648454.80 toks/s, output: 22116.46 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:05<00:00, 22115.99it/s, est. speed input: 744781.63 toks/s, output: 727.33 toks/s]    
Processed prompts: 100%|██████████| 4096/4096 [00:05<00:00, 727.31it/s, est. speed input: 744781.63 toks/s, output: 727.33 toks/s]  
[rank0]:[W128 14:03:30.218783092 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 70.4s

测试结果:
  Requests/s:   284.01
  Tokens/s:     291105.61
  Total Reqs:   4096
  Elapsed:      14.42s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     290821.60


------------------------------------------------------------
  生成 CSV: BitNet-2B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_6/BitNet-2B-FP8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,32.7035,16776.8935,3.9140
1024,1024,1,128,128,32.7825,33602.0943,3.9045
2048,1024,2,256,128,68.1548,69858.6668,3.7562
4096,1024,4,512,128,131.5122,134799.9568,3.8932
8192,1024,8,1024,128,236.7435,242662.0994,4.3254
16384,1024,16,2048,128,270.5364,277299.7738,7.5701
32768,1024,32,4096,128,284.0055,291105.6103,14.4223

------------------------------------------------------------

[INFO] 完成: 7 成功, 0 失败

============================================================
  BitNet-2B-FP8 | cuSPARSELt (2_8) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_8
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8

============================================================
[1/7] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 14:03:39 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3276391) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3276391) WARNING 01-28 14:03:55 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=3276391) WARNING 01-28 14:04:05 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 32.91 requests/s, 16880.48 total tokens/s, 32.91 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-28 14:03:39] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 14:03:39] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 14:03:39] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 14:03:39] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:03:39] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:03:39] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:03:39] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:03:39] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:03:39] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 14:03:39] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 14:03:39] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 14:03:39] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 14:03:39] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 14:03:39] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 14:03:45] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 14:03:46] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 14:03:46] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 14:03:46] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:03:46] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:03:46] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:03:46] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:03:46] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:03:46] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 14:03:46] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 14:03:46] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 14:03:46] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 14:03:46] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 14:03:46] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3276391) [2026-01-28 14:03:47] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3276391) [2026-01-28 14:03:47] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3276391) [2026-01-28 14:03:47] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3276391) [2026-01-28 14:03:47] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3276391) [2026-01-28 14:03:47] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3276391) [2026-01-28 14:03:47] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3276391) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3276391) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.57it/s]
(EngineCore_DP0 pid=3276391) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.57it/s]
(EngineCore_DP0 pid=3276391) 
(EngineCore_DP0 pid=3276391) [2026-01-28 14:03:48] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=3276391) [2026-01-28 14:03:48] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9216000 bytes
(EngineCore_DP0 pid=3276391) [2026-01-28 14:03:48] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=3276391) [2026-01-28 14:03:48] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6144000 bytes
(EngineCore_DP0 pid=3276391) [2026-01-28 14:03:48] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=3276391) [2026-01-28 14:03:48] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 33177600 bytes
(EngineCore_DP0 pid=3276391) [2026-01-28 14:03:48] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=3276391) [2026-01-28 14:03:48] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16588800 bytes
(EngineCore_DP0 pid=3276391) 2026-01-28 14:04:05,145 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3276391) 2026-01-28 14:04:05,171 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3276391) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  3.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.54it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.53it/s]
(EngineCore_DP0 pid=3276391) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 14.58it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  59%|█████▊    | 75/128 [00:00<00:00, 748.90it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 757.23it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:16,  7.53it/s, est. speed input: 3853.63 toks/s, output: 7.53 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:05, 22.46it/s, est. speed input: 10277.05 toks/s, output: 20.07 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:04, 28.04it/s, est. speed input: 12708.81 toks/s, output: 24.82 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:03, 30.95it/s, est. speed input: 14019.75 toks/s, output: 27.38 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:03, 32.45it/s, est. speed input: 14791.16 toks/s, output: 28.89 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:03, 33.44it/s, est. speed input: 15327.86 toks/s, output: 29.94 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:00<00:03, 34.09it/s, est. speed input: 15719.15 toks/s, output: 30.70 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:00<00:02, 34.53it/s, est. speed input: 16016.42 toks/s, output: 31.28 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:01<00:02, 34.82it/s, est. speed input: 16248.34 toks/s, output: 31.73 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:01<00:02, 35.02it/s, est. speed input: 16435.74 toks/s, output: 32.10 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:01<00:02, 35.20it/s, est. speed input: 16595.79 toks/s, output: 32.41 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:01<00:02, 35.26it/s, est. speed input: 16720.59 toks/s, output: 32.66 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:01<00:02, 35.31it/s, est. speed input: 16828.00 toks/s, output: 32.87 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:01<00:02, 35.35it/s, est. speed input: 16921.20 toks/s, output: 33.05 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:01<00:02, 35.40it/s, est. speed input: 17003.53 toks/s, output: 33.21 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:01<00:01, 35.38it/s, est. speed input: 17070.99 toks/s, output: 33.34 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:01<00:01, 35.34it/s, est. speed input: 17128.01 toks/s, output: 33.45 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:02<00:01, 35.33it/s, est. speed input: 17179.95 toks/s, output: 33.55 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:02<00:01, 35.34it/s, est. speed input: 17228.73 toks/s, output: 33.65 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:02<00:01, 35.40it/s, est. speed input: 17275.89 toks/s, output: 33.74 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:02<00:01, 35.34it/s, est. speed input: 17311.91 toks/s, output: 33.81 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:02<00:01, 35.35it/s, est. speed input: 17347.73 toks/s, output: 33.88 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:02<00:01, 35.44it/s, est. speed input: 17386.58 toks/s, output: 33.96 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:02<00:00, 35.42it/s, est. speed input: 17416.76 toks/s, output: 34.02 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:02<00:00, 35.42it/s, est. speed input: 17444.76 toks/s, output: 34.07 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:02<00:00, 35.48it/s, est. speed input: 17474.77 toks/s, output: 34.13 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:03<00:00, 35.49it/s, est. speed input: 17500.66 toks/s, output: 34.18 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:03<00:00, 35.47it/s, est. speed input: 17523.16 toks/s, output: 34.22 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:03<00:00, 35.42it/s, est. speed input: 17542.50 toks/s, output: 34.26 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:03<00:00, 35.48it/s, est. speed input: 17565.46 toks/s, output: 34.31 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:03<00:00, 35.47it/s, est. speed input: 17584.08 toks/s, output: 34.34 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:03<00:00, 35.52it/s, est. speed input: 17604.51 toks/s, output: 34.38 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 35.52it/s, est. speed input: 17617.72 toks/s, output: 34.41 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 34.41it/s, est. speed input: 17617.72 toks/s, output: 34.41 toks/s]
[rank0]:[W128 14:04:11.777875665 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 40.5s

测试结果:
  Requests/s:   32.91
  Tokens/s:     16880.48
  Total Reqs:   128
  Elapsed:      3.89s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     16847.57

============================================================
[2/7] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 14:04:20 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3277589) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3277589) WARNING 01-28 14:04:35 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=3277589) WARNING 01-28 14:04:45 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 32.59 requests/s, 33408.87 total tokens/s, 32.59 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-28 14:04:20] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 14:04:20] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 14:04:20] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 14:04:20] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:04:20] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:04:20] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:04:20] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:04:20] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:04:20] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 14:04:20] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 14:04:20] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 14:04:20] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 14:04:20] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 14:04:20] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 14:04:26] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 14:04:27] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 14:04:27] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 14:04:27] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:04:27] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:04:27] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:04:27] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:04:27] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:04:27] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 14:04:27] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 14:04:27] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 14:04:27] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 14:04:27] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 14:04:27] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3277589) [2026-01-28 14:04:28] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3277589) [2026-01-28 14:04:28] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3277589) [2026-01-28 14:04:28] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3277589) [2026-01-28 14:04:28] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3277589) [2026-01-28 14:04:28] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3277589) [2026-01-28 14:04:28] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3277589) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3277589) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.59it/s]
(EngineCore_DP0 pid=3277589) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.59it/s]
(EngineCore_DP0 pid=3277589) 
(EngineCore_DP0 pid=3277589) [2026-01-28 14:04:29] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=3277589) [2026-01-28 14:04:29] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9216000 bytes
(EngineCore_DP0 pid=3277589) [2026-01-28 14:04:29] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=3277589) [2026-01-28 14:04:29] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6144000 bytes
(EngineCore_DP0 pid=3277589) [2026-01-28 14:04:29] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=3277589) [2026-01-28 14:04:29] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 33177600 bytes
(EngineCore_DP0 pid=3277589) [2026-01-28 14:04:29] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=3277589) [2026-01-28 14:04:29] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16588800 bytes
(EngineCore_DP0 pid=3277589) 2026-01-28 14:04:45,840 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3277589) 2026-01-28 14:04:45,867 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3277589) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 14.47it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 14.45it/s]
(EngineCore_DP0 pid=3277589) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 12.32it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  31%|███▏      | 40/128 [00:00<00:00, 395.11it/s]
Adding requests:  66%|██████▋   | 85/128 [00:00<00:00, 423.33it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 222.46it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 253.35it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   9%|▉         | 12/128 [00:00<00:01, 103.75it/s, est. speed input: 106251.68 toks/s, output: 103.76 toks/s]
Processed prompts:  18%|█▊        | 23/128 [00:00<00:02, 49.11it/s, est. speed input: 54808.52 toks/s, output: 53.52 toks/s]   
Processed prompts:  23%|██▎       | 30/128 [00:00<00:02, 43.20it/s, est. speed input: 48796.15 toks/s, output: 47.65 toks/s]
Processed prompts:  28%|██▊       | 36/128 [00:00<00:02, 40.41it/s, est. speed input: 46015.95 toks/s, output: 44.94 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:00<00:02, 38.80it/s, est. speed input: 44446.47 toks/s, output: 43.40 toks/s]
Processed prompts:  36%|███▌      | 46/128 [00:01<00:02, 37.72it/s, est. speed input: 43326.85 toks/s, output: 42.31 toks/s]
Processed prompts:  39%|███▉      | 50/128 [00:01<00:02, 37.06it/s, est. speed input: 42624.99 toks/s, output: 41.63 toks/s]
Processed prompts:  42%|████▏     | 54/128 [00:01<00:02, 36.51it/s, est. speed input: 42030.79 toks/s, output: 41.05 toks/s]
Processed prompts:  45%|████▌     | 58/128 [00:01<00:01, 36.11it/s, est. speed input: 41539.80 toks/s, output: 40.57 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:01<00:01, 35.82it/s, est. speed input: 41125.16 toks/s, output: 40.16 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:01<00:01, 35.65it/s, est. speed input: 40779.46 toks/s, output: 39.82 toks/s]
Processed prompts:  55%|█████▍    | 70/128 [00:01<00:01, 35.45it/s, est. speed input: 40457.38 toks/s, output: 39.51 toks/s]
Processed prompts:  58%|█████▊    | 74/128 [00:01<00:01, 35.38it/s, est. speed input: 40193.14 toks/s, output: 39.25 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:01<00:01, 35.33it/s, est. speed input: 39956.48 toks/s, output: 39.02 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:02<00:01, 35.22it/s, est. speed input: 39732.57 toks/s, output: 38.80 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:02<00:01, 35.18it/s, est. speed input: 39537.56 toks/s, output: 38.61 toks/s]
Processed prompts:  70%|███████   | 90/128 [00:02<00:01, 35.19it/s, est. speed input: 39368.37 toks/s, output: 38.45 toks/s]
Processed prompts:  73%|███████▎  | 94/128 [00:02<00:00, 35.21it/s, est. speed input: 39218.26 toks/s, output: 38.30 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:02<00:00, 35.21it/s, est. speed input: 39077.82 toks/s, output: 38.16 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:02<00:00, 35.23it/s, est. speed input: 38953.38 toks/s, output: 38.04 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:02<00:00, 35.21it/s, est. speed input: 38832.99 toks/s, output: 37.92 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:02<00:00, 35.21it/s, est. speed input: 38725.00 toks/s, output: 37.82 toks/s]
Processed prompts:  89%|████████▉ | 114/128 [00:03<00:00, 35.19it/s, est. speed input: 38621.11 toks/s, output: 37.72 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:03<00:00, 35.18it/s, est. speed input: 38526.30 toks/s, output: 37.62 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:03<00:00, 35.15it/s, est. speed input: 38434.38 toks/s, output: 37.53 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:03<00:00, 35.17it/s, est. speed input: 38354.70 toks/s, output: 37.46 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 35.17it/s, est. speed input: 38315.17 toks/s, output: 37.42 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 37.42it/s, est. speed input: 38315.17 toks/s, output: 37.42 toks/s]
[rank0]:[W128 14:04:51.760976892 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 39.9s

测试结果:
  Requests/s:   32.59
  Tokens/s:     33408.87
  Total Reqs:   128
  Elapsed:      3.93s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     33376.28

============================================================
[3/7] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 14:05:01 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3278706) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3278706) WARNING 01-28 14:05:16 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=3278706) WARNING 01-28 14:05:26 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 65.54 requests/s, 67176.65 total tokens/s, 65.54 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-28 14:05:00] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 14:05:01] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 14:05:01] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 14:05:01] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:05:01] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:05:01] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:05:01] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:05:01] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:05:01] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 14:05:01] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 14:05:01] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 14:05:01] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 14:05:01] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 14:05:01] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 14:05:07] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 14:05:07] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 14:05:07] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 14:05:07] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:05:07] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:05:07] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:05:07] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:05:07] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:05:07] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 14:05:07] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 14:05:07] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 14:05:07] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 14:05:07] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 14:05:07] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3278706) [2026-01-28 14:05:08] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3278706) [2026-01-28 14:05:08] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3278706) [2026-01-28 14:05:08] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3278706) [2026-01-28 14:05:08] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3278706) [2026-01-28 14:05:08] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3278706) [2026-01-28 14:05:08] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3278706) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3278706) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.58it/s]
(EngineCore_DP0 pid=3278706) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.58it/s]
(EngineCore_DP0 pid=3278706) 
(EngineCore_DP0 pid=3278706) [2026-01-28 14:05:09] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=3278706) [2026-01-28 14:05:09] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9216000 bytes
(EngineCore_DP0 pid=3278706) [2026-01-28 14:05:09] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=3278706) [2026-01-28 14:05:09] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6144000 bytes
(EngineCore_DP0 pid=3278706) [2026-01-28 14:05:09] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=3278706) [2026-01-28 14:05:09] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 33177600 bytes
(EngineCore_DP0 pid=3278706) [2026-01-28 14:05:09] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=3278706) [2026-01-28 14:05:09] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16588800 bytes
(EngineCore_DP0 pid=3278706) 2026-01-28 14:05:26,010 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3278706) 2026-01-28 14:05:26,036 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3278706) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:00<00:00, 12.96it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 11.89it/s]
(EngineCore_DP0 pid=3278706) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 15.21it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 15.19it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   0%|          | 1/256 [00:00<00:51,  4.97it/s]
Adding requests:  16%|█▋        | 42/256 [00:00<00:01, 172.03it/s]
Adding requests:  34%|███▎      | 86/256 [00:00<00:00, 271.16it/s]
Adding requests:  51%|█████     | 131/256 [00:00<00:00, 332.10it/s]
Adding requests:  68%|██████▊   | 175/256 [00:00<00:00, 366.47it/s]
Adding requests:  86%|████████▌ | 220/256 [00:00<00:00, 392.31it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 327.21it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  16%|█▋        | 42/256 [00:00<00:00, 337.18it/s, est. speed input: 345292.94 toks/s, output: 337.19 toks/s]
Processed prompts:  30%|██▉       | 76/256 [00:00<00:01, 112.33it/s, est. speed input: 129325.28 toks/s, output: 126.29 toks/s]
Processed prompts:  37%|███▋      | 95/256 [00:00<00:01, 97.96it/s, est. speed input: 113949.87 toks/s, output: 111.28 toks/s] 
Processed prompts:  43%|████▎     | 109/256 [00:01<00:01, 89.87it/s, est. speed input: 106277.72 toks/s, output: 103.79 toks/s]
Processed prompts:  47%|████▋     | 120/256 [00:01<00:01, 83.25it/s, est. speed input: 100840.75 toks/s, output: 98.48 toks/s] 
Processed prompts:  51%|█████     | 130/256 [00:01<00:01, 80.35it/s, est. speed input: 97975.14 toks/s, output: 95.68 toks/s] 
Processed prompts:  54%|█████▍    | 139/256 [00:01<00:01, 80.33it/s, est. speed input: 96771.77 toks/s, output: 94.50 toks/s]
Processed prompts:  58%|█████▊    | 148/256 [00:01<00:01, 75.81it/s, est. speed input: 94069.75 toks/s, output: 91.86 toks/s]
Processed prompts:  61%|██████    | 156/256 [00:01<00:01, 74.70it/s, est. speed input: 92693.72 toks/s, output: 90.52 toks/s]
Processed prompts:  64%|██████▍   | 164/256 [00:01<00:01, 73.88it/s, est. speed input: 91505.47 toks/s, output: 89.36 toks/s]
Processed prompts:  67%|██████▋   | 172/256 [00:01<00:01, 73.24it/s, est. speed input: 90454.34 toks/s, output: 88.33 toks/s]
Processed prompts:  70%|███████   | 180/256 [00:02<00:01, 72.67it/s, est. speed input: 89495.63 toks/s, output: 87.40 toks/s]
Processed prompts:  73%|███████▎  | 188/256 [00:02<00:00, 72.32it/s, est. speed input: 88651.88 toks/s, output: 86.57 toks/s]
Processed prompts:  77%|███████▋  | 196/256 [00:02<00:00, 72.13it/s, est. speed input: 87905.97 toks/s, output: 85.85 toks/s]
Processed prompts:  80%|███████▉  | 204/256 [00:02<00:00, 71.95it/s, est. speed input: 87219.99 toks/s, output: 85.18 toks/s]
Processed prompts:  83%|████████▎ | 212/256 [00:02<00:00, 71.77it/s, est. speed input: 86586.56 toks/s, output: 84.56 toks/s]
Processed prompts:  86%|████████▌ | 220/256 [00:02<00:00, 71.65it/s, est. speed input: 86008.68 toks/s, output: 83.99 toks/s]
Processed prompts:  89%|████████▉ | 228/256 [00:02<00:00, 71.54it/s, est. speed input: 85472.75 toks/s, output: 83.47 toks/s]
Processed prompts:  92%|█████████▏| 236/256 [00:02<00:00, 71.60it/s, est. speed input: 85001.74 toks/s, output: 83.01 toks/s]
Processed prompts:  95%|█████████▌| 244/256 [00:02<00:00, 71.63it/s, est. speed input: 84564.26 toks/s, output: 82.58 toks/s]
Processed prompts:  98%|█████████▊| 252/256 [00:03<00:00, 71.53it/s, est. speed input: 84142.21 toks/s, output: 82.17 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 71.53it/s, est. speed input: 83948.14 toks/s, output: 81.98 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 81.98it/s, est. speed input: 83948.14 toks/s, output: 81.98 toks/s]
[rank0]:[W128 14:05:31.036121218 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 40.2s

测试结果:
  Requests/s:   65.54
  Tokens/s:     67176.65
  Total Reqs:   256
  Elapsed:      3.91s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     67111.11

============================================================
[4/7] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 14:05:42 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3279829) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3279829) WARNING 01-28 14:05:57 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=3279829) WARNING 01-28 14:06:07 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 129.89 requests/s, 133138.59 total tokens/s, 129.89 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-28 14:05:41] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 14:05:42] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 14:05:42] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 14:05:42] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:05:42] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:05:42] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:05:42] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:05:42] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:05:42] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 14:05:42] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 14:05:42] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 14:05:42] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 14:05:42] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 14:05:42] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 14:05:48] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 14:05:48] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 14:05:48] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 14:05:48] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:05:48] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:05:48] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:05:48] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:05:48] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:05:48] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 14:05:48] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 14:05:48] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 14:05:48] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 14:05:48] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 14:05:48] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3279829) [2026-01-28 14:05:49] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3279829) [2026-01-28 14:05:49] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3279829) [2026-01-28 14:05:49] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3279829) [2026-01-28 14:05:49] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3279829) [2026-01-28 14:05:49] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3279829) [2026-01-28 14:05:49] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3279829) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3279829) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.58it/s]
(EngineCore_DP0 pid=3279829) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.58it/s]
(EngineCore_DP0 pid=3279829) 
(EngineCore_DP0 pid=3279829) [2026-01-28 14:05:50] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=3279829) [2026-01-28 14:05:50] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9216000 bytes
(EngineCore_DP0 pid=3279829) [2026-01-28 14:05:50] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=3279829) [2026-01-28 14:05:50] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6144000 bytes
(EngineCore_DP0 pid=3279829) [2026-01-28 14:05:50] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=3279829) [2026-01-28 14:05:50] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 33177600 bytes
(EngineCore_DP0 pid=3279829) [2026-01-28 14:05:50] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=3279829) [2026-01-28 14:05:50] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16588800 bytes
(EngineCore_DP0 pid=3279829) 2026-01-28 14:06:07,580 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3279829) 2026-01-28 14:06:07,606 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3279829) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00, 13.85it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 13.34it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 13.41it/s]
(EngineCore_DP0 pid=3279829) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00, 15.80it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 15.98it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   8%|▊         | 40/512 [00:00<00:01, 398.74it/s]
Adding requests:  16%|█▋        | 84/512 [00:00<00:01, 422.94it/s]
Adding requests:  25%|██▌       | 129/512 [00:00<00:00, 431.18it/s]
Adding requests:  34%|███▍      | 173/512 [00:00<00:00, 432.73it/s]
Adding requests:  43%|████▎     | 219/512 [00:00<00:00, 439.86it/s]
Adding requests:  52%|█████▏    | 267/512 [00:00<00:00, 450.75it/s]
Adding requests:  61%|██████    | 313/512 [00:00<00:00, 451.50it/s]
Adding requests:  70%|███████   | 359/512 [00:00<00:00, 452.68it/s]
Adding requests:  79%|███████▉  | 407/512 [00:00<00:00, 458.70it/s]
Adding requests:  89%|████████▊ | 454/512 [00:01<00:00, 459.73it/s]
Adding requests:  98%|█████████▊| 500/512 [00:01<00:00, 459.55it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 449.80it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  29%|██▊       | 146/512 [00:00<00:00, 1264.55it/s, est. speed input: 1294979.28 toks/s, output: 1264.57 toks/s]
Processed prompts:  53%|█████▎    | 273/512 [00:01<00:01, 236.18it/s, est. speed input: 278137.74 toks/s, output: 271.62 toks/s]   
Processed prompts:  65%|██████▌   | 334/512 [00:01<00:00, 194.98it/s, est. speed input: 234020.21 toks/s, output: 228.53 toks/s]
Processed prompts:  73%|███████▎  | 373/512 [00:01<00:00, 181.97it/s, est. speed input: 220444.16 toks/s, output: 215.28 toks/s]
Processed prompts:  79%|███████▊  | 403/512 [00:01<00:00, 170.19it/s, est. speed input: 210484.92 toks/s, output: 205.55 toks/s]
Processed prompts:  83%|████████▎ | 427/512 [00:02<00:00, 164.13it/s, est. speed input: 205088.46 toks/s, output: 200.28 toks/s]
Processed prompts:  88%|████████▊ | 448/512 [00:02<00:00, 160.82it/s, est. speed input: 201661.06 toks/s, output: 196.93 toks/s]
Processed prompts:  91%|█████████ | 467/512 [00:02<00:00, 151.81it/s, est. speed input: 196729.42 toks/s, output: 192.12 toks/s]
Processed prompts:  95%|█████████▍| 484/512 [00:02<00:00, 151.34it/s, est. speed input: 194773.63 toks/s, output: 190.21 toks/s]
Processed prompts:  98%|█████████▊| 500/512 [00:02<00:00, 149.07it/s, est. speed input: 192616.22 toks/s, output: 188.10 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:02<00:00, 149.07it/s, est. speed input: 187088.36 toks/s, output: 182.70 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:02<00:00, 182.70it/s, est. speed input: 187088.36 toks/s, output: 182.70 toks/s]
[rank0]:[W128 14:06:13.032486378 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 42.0s

测试结果:
  Requests/s:   129.89
  Tokens/s:     133138.59
  Total Reqs:   512
  Elapsed:      3.94s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     133008.70

============================================================
[5/7] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 14:06:26 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3280988) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3280988) WARNING 01-28 14:06:41 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=3280988) WARNING 01-28 14:06:51 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 229.08 requests/s, 234808.75 total tokens/s, 229.08 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-28 14:06:26] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 14:06:26] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 14:06:26] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 14:06:26] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:06:26] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:06:26] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:06:26] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:06:26] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:06:26] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 14:06:26] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 14:06:26] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 14:06:26] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 14:06:26] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 14:06:26] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 14:06:32] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 14:06:33] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 14:06:33] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 14:06:33] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:06:33] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:06:33] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:06:33] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:06:33] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:06:33] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 14:06:33] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 14:06:33] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 14:06:33] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 14:06:33] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 14:06:33] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3280988) [2026-01-28 14:06:34] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3280988) [2026-01-28 14:06:34] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3280988) [2026-01-28 14:06:34] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3280988) [2026-01-28 14:06:34] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3280988) [2026-01-28 14:06:34] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3280988) [2026-01-28 14:06:34] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3280988) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3280988) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.58it/s]
(EngineCore_DP0 pid=3280988) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.58it/s]
(EngineCore_DP0 pid=3280988) 
(EngineCore_DP0 pid=3280988) [2026-01-28 14:06:35] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=3280988) [2026-01-28 14:06:35] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9216000 bytes
(EngineCore_DP0 pid=3280988) [2026-01-28 14:06:35] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=3280988) [2026-01-28 14:06:35] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6144000 bytes
(EngineCore_DP0 pid=3280988) [2026-01-28 14:06:35] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=3280988) [2026-01-28 14:06:35] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 33177600 bytes
(EngineCore_DP0 pid=3280988) [2026-01-28 14:06:35] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=3280988) [2026-01-28 14:06:35] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16588800 bytes
(EngineCore_DP0 pid=3280988) 2026-01-28 14:06:51,625 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3280988) 2026-01-28 14:06:51,651 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3280988) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00, 15.23it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00, 15.82it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 14.51it/s]
(EngineCore_DP0 pid=3280988) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00, 16.17it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 16.35it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 16.31it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   4%|▍         | 40/1024 [00:00<00:02, 397.43it/s]
Adding requests:   8%|▊         | 85/1024 [00:00<00:02, 423.95it/s]
Adding requests:  13%|█▎        | 130/1024 [00:00<00:02, 432.72it/s]
Adding requests:  17%|█▋        | 174/1024 [00:00<00:01, 434.10it/s]
Adding requests:  21%|██▏       | 219/1024 [00:00<00:01, 439.54it/s]
Adding requests:  26%|██▌       | 267/1024 [00:00<00:01, 451.17it/s]
Adding requests:  31%|███       | 313/1024 [00:00<00:01, 447.89it/s]
Adding requests:  35%|███▌      | 359/1024 [00:00<00:01, 450.85it/s]
Adding requests:  40%|███▉      | 406/1024 [00:00<00:01, 456.25it/s]
Adding requests:  44%|████▍     | 453/1024 [00:01<00:01, 458.96it/s]
Adding requests:  49%|████▉     | 500/1024 [00:01<00:01, 458.68it/s]
Adding requests:  53%|█████▎    | 546/1024 [00:01<00:01, 455.67it/s]
Adding requests:  58%|█████▊    | 595/1024 [00:01<00:00, 465.20it/s]
Adding requests:  63%|██████▎   | 643/1024 [00:01<00:00, 467.88it/s]
Adding requests:  68%|██████▊   | 692/1024 [00:01<00:00, 474.47it/s]
Adding requests:  72%|███████▏  | 740/1024 [00:01<00:00, 474.59it/s]
Adding requests:  77%|███████▋  | 788/1024 [00:01<00:00, 471.93it/s]
Adding requests:  82%|████████▏ | 836/1024 [00:01<00:00, 461.03it/s]
Adding requests:  86%|████████▋ | 885/1024 [00:01<00:00, 467.57it/s]
Adding requests:  91%|█████████ | 934/1024 [00:02<00:00, 473.04it/s]
Adding requests:  96%|█████████▌| 982/1024 [00:02<00:00, 472.38it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 460.53it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  50%|█████     | 514/1024 [00:00<00:00, 3876.92it/s, est. speed input: 3970184.57 toks/s, output: 3876.98 toks/s]
Processed prompts:  88%|████████▊ | 902/1024 [00:01<00:00, 439.07it/s, est. speed input: 529945.78 toks/s, output: 517.52 toks/s]   
Processed prompts: 100%|██████████| 1024/1024 [00:02<00:00, 439.07it/s, est. speed input: 467019.61 toks/s, output: 456.07 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:02<00:00, 456.05it/s, est. speed input: 467019.61 toks/s, output: 456.07 toks/s]
[rank0]:[W128 14:06:58.723116534 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 44.7s

测试结果:
  Requests/s:   229.08
  Tokens/s:     234808.75
  Total Reqs:   1024
  Elapsed:      4.47s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     234579.66

============================================================
[6/7] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 14:07:15 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3282210) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3282210) WARNING 01-28 14:07:31 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=3282210) WARNING 01-28 14:07:41 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 264.99 requests/s, 271618.86 total tokens/s, 264.99 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-28 14:07:15] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 14:07:15] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 14:07:15] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 14:07:15] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:07:15] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:07:15] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:07:15] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:07:15] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:07:15] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 14:07:15] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 14:07:15] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 14:07:15] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 14:07:15] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 14:07:15] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 14:07:22] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 14:07:22] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 14:07:22] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 14:07:22] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:07:22] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:07:22] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:07:22] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:07:22] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:07:22] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 14:07:22] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 14:07:22] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 14:07:22] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 14:07:22] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 14:07:22] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3282210) [2026-01-28 14:07:23] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3282210) [2026-01-28 14:07:23] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3282210) [2026-01-28 14:07:23] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3282210) [2026-01-28 14:07:23] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3282210) [2026-01-28 14:07:23] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3282210) [2026-01-28 14:07:23] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3282210) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3282210) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.58it/s]
(EngineCore_DP0 pid=3282210) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.58it/s]
(EngineCore_DP0 pid=3282210) 
(EngineCore_DP0 pid=3282210) [2026-01-28 14:07:24] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=3282210) [2026-01-28 14:07:24] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9216000 bytes
(EngineCore_DP0 pid=3282210) [2026-01-28 14:07:24] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=3282210) [2026-01-28 14:07:24] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6144000 bytes
(EngineCore_DP0 pid=3282210) [2026-01-28 14:07:24] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=3282210) [2026-01-28 14:07:24] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 33177600 bytes
(EngineCore_DP0 pid=3282210) [2026-01-28 14:07:24] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=3282210) [2026-01-28 14:07:24] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16588800 bytes
(EngineCore_DP0 pid=3282210) 2026-01-28 14:07:41,492 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3282210) 2026-01-28 14:07:41,519 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3282210) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  29%|██▊       | 2/7 [00:00<00:00, 11.32it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  57%|█████▋    | 4/7 [00:00<00:00, 12.62it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  86%|████████▌ | 6/7 [00:00<00:00, 14.04it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 13.05it/s]
(EngineCore_DP0 pid=3282210) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00, 15.91it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00, 16.19it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 15.92it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 41/2048 [00:00<00:05, 400.88it/s]
Adding requests:   4%|▍         | 86/2048 [00:00<00:04, 425.27it/s]
Adding requests:   6%|▋         | 131/2048 [00:00<00:04, 432.23it/s]
Adding requests:   9%|▊         | 175/2048 [00:00<00:04, 433.54it/s]
Adding requests:  11%|█         | 221/2048 [00:00<00:04, 442.95it/s]
Adding requests:  13%|█▎        | 269/2048 [00:00<00:03, 453.25it/s]
Adding requests:  15%|█▌        | 315/2048 [00:00<00:03, 450.46it/s]
Adding requests:  18%|█▊        | 362/2048 [00:00<00:03, 455.15it/s]
Adding requests:  20%|██        | 410/2048 [00:00<00:03, 460.18it/s]
Adding requests:  22%|██▏       | 457/2048 [00:01<00:03, 462.63it/s]
Adding requests:  25%|██▍       | 504/2048 [00:01<00:03, 463.03it/s]
Adding requests:  27%|██▋       | 551/2048 [00:01<00:03, 457.96it/s]
Adding requests:  29%|██▉       | 600/2048 [00:01<00:03, 462.71it/s]
Adding requests:  32%|███▏      | 648/2048 [00:01<00:03, 465.57it/s]
Adding requests:  34%|███▍      | 697/2048 [00:01<00:02, 472.34it/s]
Adding requests:  36%|███▋      | 745/2048 [00:01<00:02, 471.62it/s]
Adding requests:  39%|███▊      | 793/2048 [00:01<00:02, 469.71it/s]
Adding requests:  41%|████      | 840/2048 [00:01<00:02, 460.89it/s]
Adding requests:  43%|████▎     | 888/2048 [00:01<00:02, 466.03it/s]
Adding requests:  46%|████▌     | 937/2048 [00:02<00:02, 470.27it/s]
Adding requests:  48%|████▊     | 985/2048 [00:02<00:02, 471.88it/s]
Adding requests:  50%|█████     | 1034/2048 [00:02<00:02, 474.84it/s]
Adding requests:  53%|█████▎    | 1082/2048 [00:02<00:02, 471.05it/s]
Adding requests:  55%|█████▌    | 1130/2048 [00:02<00:01, 468.91it/s]
Adding requests:  58%|█████▊    | 1179/2048 [00:02<00:01, 474.41it/s]
Adding requests:  60%|██████    | 1229/2048 [00:02<00:01, 479.88it/s]
Adding requests:  62%|██████▏   | 1278/2048 [00:02<00:01, 472.86it/s]
Adding requests:  65%|██████▍   | 1327/2048 [00:02<00:01, 477.28it/s]
Adding requests:  67%|██████▋   | 1376/2048 [00:02<00:01, 478.21it/s]
Adding requests:  70%|██████▉   | 1425/2048 [00:03<00:01, 477.79it/s]
Adding requests:  72%|███████▏  | 1474/2048 [00:03<00:01, 479.97it/s]
Adding requests:  74%|███████▍  | 1523/2048 [00:03<00:01, 480.85it/s]
Adding requests:  77%|███████▋  | 1572/2048 [00:03<00:00, 482.24it/s]
Adding requests:  79%|███████▉  | 1622/2048 [00:03<00:00, 484.64it/s]
Adding requests:  82%|████████▏ | 1671/2048 [00:03<00:00, 480.83it/s]
Adding requests:  84%|████████▍ | 1720/2048 [00:03<00:00, 472.34it/s]
Adding requests:  86%|████████▋ | 1768/2048 [00:03<00:00, 472.05it/s]
Adding requests:  89%|████████▊ | 1816/2048 [00:03<00:00, 473.06it/s]
Adding requests:  91%|█████████ | 1864/2048 [00:03<00:00, 470.24it/s]
Adding requests:  93%|█████████▎| 1913/2048 [00:04<00:00, 473.42it/s]
Adding requests:  96%|█████████▌| 1962/2048 [00:04<00:00, 475.68it/s]
Adding requests:  98%|█████████▊| 2011/2048 [00:04<00:00, 478.77it/s]
Adding requests: 100%|██████████| 2048/2048 [00:04<00:00, 468.57it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  57%|█████▋    | 1170/2048 [00:00<00:00, 7855.55it/s, est. speed input: 8044464.87 toks/s, output: 7855.65 toks/s]
Processed prompts:  96%|█████████▌| 1956/2048 [00:03<00:00, 532.23it/s, est. speed input: 654496.40 toks/s, output: 639.16 toks/s]   
Processed prompts: 100%|██████████| 2048/2048 [00:03<00:00, 532.23it/s, est. speed input: 624841.09 toks/s, output: 610.20 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:03<00:00, 610.18it/s, est. speed input: 624841.09 toks/s, output: 610.20 toks/s]
[rank0]:[W128 14:07:51.945721088 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 53.2s

测试结果:
  Requests/s:   264.99
  Tokens/s:     271618.86
  Total Reqs:   2048
  Elapsed:      7.73s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     271353.86

============================================================
[7/7] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:8)                                │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 14:08:18 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3283626) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3283626) WARNING 01-28 14:08:33 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=3283626) WARNING 01-28 14:08:44 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 278.09 requests/s, 285042.24 total tokens/s, 278.09 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-28 14:08:17] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 14:08:18] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 14:08:18] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 14:08:18] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:08:18] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:08:18] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:08:18] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:08:18] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:08:18] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 14:08:18] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 14:08:18] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 14:08:18] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 14:08:18] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 14:08:18] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 14:08:24] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 14:08:25] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 14:08:25] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 14:08:25] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:08:25] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:08:25] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:08:25] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:08:25] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:08:25] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 14:08:25] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 14:08:25] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 14:08:25] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 14:08:25] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 14:08:25] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3283626) [2026-01-28 14:08:26] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3283626) [2026-01-28 14:08:26] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3283626) [2026-01-28 14:08:26] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3283626) [2026-01-28 14:08:26] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:8, expand_ratio=1.333
(EngineCore_DP0 pid=3283626) [2026-01-28 14:08:26] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3283626) [2026-01-28 14:08:26] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3283626) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3283626) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.59it/s]
(EngineCore_DP0 pid=3283626) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.59it/s]
(EngineCore_DP0 pid=3283626) 
(EngineCore_DP0 pid=3283626) [2026-01-28 14:08:27] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 3840] -> 1D uint8
(EngineCore_DP0 pid=3283626) [2026-01-28 14:08:27] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9216000 bytes
(EngineCore_DP0 pid=3283626) [2026-01-28 14:08:27] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 3840] -> 1D uint8
(EngineCore_DP0 pid=3283626) [2026-01-28 14:08:27] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6144000 bytes
(EngineCore_DP0 pid=3283626) [2026-01-28 14:08:27] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 3840] -> 1D uint8
(EngineCore_DP0 pid=3283626) [2026-01-28 14:08:27] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 33177600 bytes
(EngineCore_DP0 pid=3283626) [2026-01-28 14:08:27] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 10368] -> 1D uint8
(EngineCore_DP0 pid=3283626) [2026-01-28 14:08:27] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 16588800 bytes
(EngineCore_DP0 pid=3283626) [rank0]:W0128 14:08:38.873000 3283626 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3283626) [rank0]:W0128 14:08:38.956000 3283626 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3283626) [rank0]:W0128 14:08:40.073000 3283626 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3283626) [rank0]:W0128 14:08:40.204000 3283626 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3283626) 2026-01-28 14:08:43,935 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3283626) 2026-01-28 14:08:43,965 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3283626) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:00, 13.61it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:00<00:00, 12.83it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 6/11 [00:00<00:00, 13.60it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:00<00:00, 14.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 10/11 [00:00<00:00, 14.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 13.63it/s]
(EngineCore_DP0 pid=3283626) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00, 13.47it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00, 14.93it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00, 15.48it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 15.31it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 41/4096 [00:00<00:10, 403.84it/s]
Adding requests:   2%|▏         | 86/4096 [00:00<00:09, 425.82it/s]
Adding requests:   3%|▎         | 131/4096 [00:00<00:09, 436.02it/s]
Adding requests:   4%|▍         | 176/4096 [00:00<00:08, 438.63it/s]
Adding requests:   5%|▌         | 223/4096 [00:00<00:08, 447.84it/s]
Adding requests:   7%|▋         | 271/4096 [00:00<00:08, 457.81it/s]
Adding requests:   8%|▊         | 317/4096 [00:00<00:08, 453.24it/s]
Adding requests:   9%|▉         | 365/4096 [00:00<00:08, 460.89it/s]
Adding requests:  10%|█         | 412/4096 [00:00<00:08, 453.55it/s]
Adding requests:  11%|█         | 459/4096 [00:01<00:07, 457.76it/s]
Adding requests:  12%|█▏        | 506/4096 [00:01<00:07, 459.43it/s]
Adding requests:  13%|█▎        | 552/4096 [00:01<00:07, 455.44it/s]
Adding requests:  15%|█▍        | 600/4096 [00:01<00:07, 460.25it/s]
Adding requests:  16%|█▌        | 649/4096 [00:01<00:07, 468.59it/s]
Adding requests:  17%|█▋        | 698/4096 [00:01<00:07, 474.53it/s]
Adding requests:  18%|█▊        | 746/4096 [00:01<00:07, 474.51it/s]
Adding requests:  19%|█▉        | 794/4096 [00:01<00:06, 472.91it/s]
Adding requests:  21%|██        | 842/4096 [00:01<00:07, 463.54it/s]
Adding requests:  22%|██▏       | 892/4096 [00:01<00:06, 473.42it/s]
Adding requests:  23%|██▎       | 940/4096 [00:02<00:06, 465.69it/s]
Adding requests:  24%|██▍       | 989/4096 [00:02<00:06, 471.12it/s]
Adding requests:  25%|██▌       | 1038/4096 [00:02<00:06, 474.34it/s]
Adding requests:  27%|██▋       | 1086/4096 [00:02<00:06, 471.48it/s]
Adding requests:  28%|██▊       | 1134/4096 [00:02<00:06, 467.85it/s]
Adding requests:  29%|██▉       | 1185/4096 [00:02<00:06, 478.62it/s]
Adding requests:  30%|███       | 1233/4096 [00:02<00:06, 466.43it/s]
Adding requests:  31%|███▏      | 1280/4096 [00:02<00:06, 462.80it/s]
Adding requests:  32%|███▏      | 1329/4096 [00:02<00:05, 468.37it/s]
Adding requests:  34%|███▎      | 1378/4096 [00:02<00:05, 473.29it/s]
Adding requests:  35%|███▍      | 1427/4096 [00:03<00:05, 475.96it/s]
Adding requests:  36%|███▌      | 1476/4096 [00:03<00:05, 478.21it/s]
Adding requests:  37%|███▋      | 1525/4096 [00:03<00:05, 481.25it/s]
Adding requests:  38%|███▊      | 1574/4096 [00:03<00:05, 482.30it/s]
Adding requests:  40%|███▉      | 1624/4096 [00:03<00:05, 487.46it/s]
Adding requests:  41%|████      | 1673/4096 [00:03<00:05, 482.08it/s]
Adding requests:  42%|████▏     | 1722/4096 [00:03<00:04, 483.81it/s]
Adding requests:  43%|████▎     | 1771/4096 [00:03<00:04, 479.96it/s]
Adding requests:  44%|████▍     | 1820/4096 [00:03<00:04, 481.39it/s]
Adding requests:  46%|████▌     | 1869/4096 [00:03<00:04, 476.89it/s]
Adding requests:  47%|████▋     | 1918/4096 [00:04<00:04, 479.73it/s]
Adding requests:  48%|████▊     | 1966/4096 [00:04<00:04, 478.14it/s]
Adding requests:  49%|████▉     | 2015/4096 [00:04<00:04, 480.88it/s]
Adding requests:  50%|█████     | 2064/4096 [00:04<00:04, 471.31it/s]
Adding requests:  52%|█████▏    | 2112/4096 [00:04<00:04, 471.79it/s]
Adding requests:  53%|█████▎    | 2160/4096 [00:04<00:04, 465.16it/s]
Adding requests:  54%|█████▍    | 2207/4096 [00:04<00:04, 465.76it/s]
Adding requests:  55%|█████▌    | 2256/4096 [00:04<00:03, 472.44it/s]
Adding requests:  56%|█████▋    | 2304/4096 [00:04<00:03, 474.21it/s]
Adding requests:  57%|█████▋    | 2352/4096 [00:05<00:03, 473.61it/s]
Adding requests:  59%|█████▊    | 2401/4096 [00:05<00:03, 473.65it/s]
Adding requests:  60%|█████▉    | 2449/4096 [00:05<00:03, 475.44it/s]
Adding requests:  61%|██████    | 2499/4096 [00:05<00:03, 480.08it/s]
Adding requests:  62%|██████▏   | 2548/4096 [00:05<00:03, 479.62it/s]
Adding requests:  63%|██████▎   | 2597/4096 [00:05<00:03, 480.84it/s]
Adding requests:  65%|██████▍   | 2646/4096 [00:05<00:03, 482.95it/s]
Adding requests:  66%|██████▌   | 2695/4096 [00:05<00:02, 479.61it/s]
Adding requests:  67%|██████▋   | 2743/4096 [00:05<00:02, 479.31it/s]
Adding requests:  68%|██████▊   | 2791/4096 [00:05<00:02, 474.84it/s]
Adding requests:  69%|██████▉   | 2839/4096 [00:06<00:02, 476.16it/s]
Adding requests:  71%|███████   | 2889/4096 [00:06<00:02, 480.78it/s]
Adding requests:  72%|███████▏  | 2938/4096 [00:06<00:02, 474.64it/s]
Adding requests:  73%|███████▎  | 2987/4096 [00:06<00:02, 477.98it/s]
Adding requests:  74%|███████▍  | 3035/4096 [00:06<00:02, 476.50it/s]
Adding requests:  75%|███████▌  | 3083/4096 [00:06<00:02, 474.71it/s]
Adding requests:  76%|███████▋  | 3132/4096 [00:06<00:02, 478.69it/s]
Adding requests:  78%|███████▊  | 3181/4096 [00:06<00:01, 479.97it/s]
Adding requests:  79%|███████▉  | 3230/4096 [00:06<00:01, 478.35it/s]
Adding requests:  80%|████████  | 3278/4096 [00:06<00:01, 467.64it/s]
Adding requests:  81%|████████  | 3326/4096 [00:07<00:01, 469.40it/s]
Adding requests:  82%|████████▏ | 3373/4096 [00:07<00:01, 466.99it/s]
Adding requests:  84%|████████▎ | 3421/4096 [00:07<00:01, 470.76it/s]
Adding requests:  85%|████████▍ | 3469/4096 [00:07<00:01, 462.75it/s]
Adding requests:  86%|████████▌ | 3517/4096 [00:07<00:01, 466.72it/s]
Adding requests:  87%|████████▋ | 3564/4096 [00:07<00:01, 466.71it/s]
Adding requests:  88%|████████▊ | 3612/4096 [00:07<00:01, 468.70it/s]
Adding requests:  89%|████████▉ | 3659/4096 [00:07<00:00, 467.39it/s]
Adding requests:  91%|█████████ | 3708/4096 [00:07<00:00, 472.63it/s]
Adding requests:  92%|█████████▏| 3756/4096 [00:07<00:00, 473.46it/s]
Adding requests:  93%|█████████▎| 3806/4096 [00:08<00:00, 478.50it/s]
Adding requests:  94%|█████████▍| 3856/4096 [00:08<00:00, 483.26it/s]
Adding requests:  95%|█████████▌| 3905/4096 [00:08<00:00, 481.25it/s]
Adding requests:  97%|█████████▋| 3954/4096 [00:08<00:00, 481.56it/s]
Adding requests:  98%|█████████▊| 4003/4096 [00:08<00:00, 480.39it/s]
Adding requests:  99%|█████████▉| 4052/4096 [00:08<00:00, 476.37it/s]
Adding requests: 100%|██████████| 4096/4096 [00:08<00:00, 471.88it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  59%|█████▉    | 2422/4096 [00:00<00:00, 13190.35it/s, est. speed input: 13507589.49 toks/s, output: 13190.53 toks/s]
Processed prompts:  91%|█████████▏| 3742/4096 [00:04<00:00, 627.97it/s, est. speed input: 788938.77 toks/s, output: 770.45 toks/s]      
Processed prompts: 100%|██████████| 4096/4096 [00:06<00:00, 627.97it/s, est. speed input: 693634.60 toks/s, output: 677.38 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:06<00:00, 677.36it/s, est. speed input: 693634.60 toks/s, output: 677.38 toks/s]
[rank0]:[W128 14:09:01.070982911 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 70.3s

测试结果:
  Requests/s:   278.09
  Tokens/s:     285042.24
  Total Reqs:   4096
  Elapsed:      14.73s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     284764.15


------------------------------------------------------------
  生成 CSV: BitNet-2B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_8/BitNet-2B-FP8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,32.9054,16880.4755,3.8899
1024,1024,1,128,128,32.5940,33408.8735,3.9271
2048,1024,2,256,128,65.5382,67176.6468,3.9061
4096,1024,4,512,128,129.8913,133138.5936,3.9418
8192,1024,8,1024,128,229.0817,234808.7460,4.4700
16384,1024,16,2048,128,264.9940,271618.8574,7.7285
32768,1024,32,4096,128,278.0900,285042.2439,14.7290

------------------------------------------------------------

[INFO] 完成: 7 成功, 0 失败

============================================================
  BitNet-2B-FP8 | cuSPARSELt (2_10) | prefill
============================================================

[INFO] Checkpoint: /root/vllmbench/checkpoints_slidesparse/BitNet-2B-FP8-SlideSparse-2_10
[INFO] Output: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10

============================================================
[1/7] 测试 M=512
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 512
│   M_prefill     = 512 (= 1 x 512)
│   M_decode      = 1
│   batched_tokens = 513 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 512
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 513
│   --max-num-batched-tokens = 513
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 14:09:10 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3284917) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3284917) WARNING 01-28 14:09:26 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=3284917) WARNING 01-28 14:09:36 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 33.01 requests/s, 16936.19 total tokens/s, 33.01 output tokens/s
Total num prompt tokens:  65536
Total num output tokens:  128


─── STDERR ───
[2026-01-28 14:09:10] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 14:09:10] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 14:09:10] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 14:09:10] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:09:10] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:09:10] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:09:10] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:09:10] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:09:10] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 14:09:10] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 14:09:10] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 14:09:10] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 14:09:10] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 14:09:10] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 14:09:16] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 14:09:17] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 14:09:17] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 14:09:17] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:09:17] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:09:17] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:09:17] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:09:17] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:09:17] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 14:09:17] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 14:09:17] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 14:09:17] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 14:09:17] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 14:09:17] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3284917) [2026-01-28 14:09:18] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3284917) [2026-01-28 14:09:18] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3284917) [2026-01-28 14:09:18] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3284917) [2026-01-28 14:09:18] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3284917) [2026-01-28 14:09:18] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3284917) [2026-01-28 14:09:18] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3284917) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3284917) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.48it/s]
(EngineCore_DP0 pid=3284917) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.48it/s]
(EngineCore_DP0 pid=3284917) 
(EngineCore_DP0 pid=3284917) [2026-01-28 14:09:19] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=3284917) [2026-01-28 14:09:19] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9830400 bytes
(EngineCore_DP0 pid=3284917) [2026-01-28 14:09:19] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=3284917) [2026-01-28 14:09:19] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6553600 bytes
(EngineCore_DP0 pid=3284917) [2026-01-28 14:09:19] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=3284917) [2026-01-28 14:09:19] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 35389440 bytes
(EngineCore_DP0 pid=3284917) [2026-01-28 14:09:19] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=3284917) [2026-01-28 14:09:19] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 17735680 bytes
(EngineCore_DP0 pid=3284917) 2026-01-28 14:09:36,359 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3284917) 2026-01-28 14:09:36,386 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3284917) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 1/2 [00:00<00:00,  3.55it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.56it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00,  3.56it/s]
(EngineCore_DP0 pid=3284917) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 13.43it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  59%|█████▊    | 75/128 [00:00<00:00, 745.45it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 757.08it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   1%|          | 1/128 [00:00<00:16,  7.49it/s, est. speed input: 3834.94 toks/s, output: 7.49 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:05, 22.53it/s, est. speed input: 10296.00 toks/s, output: 20.11 toks/s]
Processed prompts:   7%|▋         | 9/128 [00:00<00:04, 28.11it/s, est. speed input: 12730.17 toks/s, output: 24.86 toks/s]
Processed prompts:  10%|█         | 13/128 [00:00<00:03, 30.91it/s, est. speed input: 14014.10 toks/s, output: 27.37 toks/s]
Processed prompts:  13%|█▎        | 17/128 [00:00<00:03, 32.59it/s, est. speed input: 14826.59 toks/s, output: 28.96 toks/s]
Processed prompts:  16%|█▋        | 21/128 [00:00<00:03, 33.55it/s, est. speed input: 15362.55 toks/s, output: 30.00 toks/s]
Processed prompts:  20%|█▉        | 25/128 [00:00<00:03, 34.21it/s, est. speed input: 15758.36 toks/s, output: 30.78 toks/s]
Processed prompts:  23%|██▎       | 29/128 [00:00<00:02, 34.71it/s, est. speed input: 16069.58 toks/s, output: 31.39 toks/s]
Processed prompts:  26%|██▌       | 33/128 [00:01<00:02, 35.02it/s, est. speed input: 16308.50 toks/s, output: 31.85 toks/s]
Processed prompts:  29%|██▉       | 37/128 [00:01<00:02, 35.18it/s, est. speed input: 16493.67 toks/s, output: 32.21 toks/s]
Processed prompts:  32%|███▏      | 41/128 [00:01<00:02, 35.31it/s, est. speed input: 16647.86 toks/s, output: 32.52 toks/s]
Processed prompts:  35%|███▌      | 45/128 [00:01<00:02, 35.38it/s, est. speed input: 16774.64 toks/s, output: 32.76 toks/s]
Processed prompts:  38%|███▊      | 49/128 [00:01<00:02, 35.40it/s, est. speed input: 16879.34 toks/s, output: 32.97 toks/s]
Processed prompts:  41%|████▏     | 53/128 [00:01<00:02, 35.48it/s, est. speed input: 16975.61 toks/s, output: 33.16 toks/s]
Processed prompts:  45%|████▍     | 57/128 [00:01<00:01, 35.52it/s, est. speed input: 17058.66 toks/s, output: 33.32 toks/s]
Processed prompts:  48%|████▊     | 61/128 [00:01<00:01, 35.58it/s, est. speed input: 17133.77 toks/s, output: 33.46 toks/s]
Processed prompts:  51%|█████     | 65/128 [00:01<00:01, 35.57it/s, est. speed input: 17195.60 toks/s, output: 33.59 toks/s]
Processed prompts:  54%|█████▍    | 69/128 [00:02<00:01, 35.63it/s, est. speed input: 17257.10 toks/s, output: 33.71 toks/s]
Processed prompts:  57%|█████▋    | 73/128 [00:02<00:01, 35.61it/s, est. speed input: 17306.40 toks/s, output: 33.80 toks/s]
Processed prompts:  60%|██████    | 77/128 [00:02<00:01, 35.56it/s, est. speed input: 17348.37 toks/s, output: 33.88 toks/s]
Processed prompts:  63%|██████▎   | 81/128 [00:02<00:01, 35.57it/s, est. speed input: 17389.88 toks/s, output: 33.96 toks/s]
Processed prompts:  66%|██████▋   | 85/128 [00:02<00:01, 35.51it/s, est. speed input: 17422.24 toks/s, output: 34.03 toks/s]
Processed prompts:  70%|██████▉   | 89/128 [00:02<00:01, 35.55it/s, est. speed input: 17457.53 toks/s, output: 34.10 toks/s]
Processed prompts:  73%|███████▎  | 93/128 [00:02<00:00, 35.54it/s, est. speed input: 17487.60 toks/s, output: 34.16 toks/s]
Processed prompts:  76%|███████▌  | 97/128 [00:02<00:00, 35.53it/s, est. speed input: 17515.16 toks/s, output: 34.21 toks/s]
Processed prompts:  79%|███████▉  | 101/128 [00:02<00:00, 35.53it/s, est. speed input: 17540.98 toks/s, output: 34.26 toks/s]
Processed prompts:  82%|████████▏ | 105/128 [00:03<00:00, 35.52it/s, est. speed input: 17564.20 toks/s, output: 34.31 toks/s]
Processed prompts:  85%|████████▌ | 109/128 [00:03<00:00, 35.52it/s, est. speed input: 17586.14 toks/s, output: 34.35 toks/s]
Processed prompts:  88%|████████▊ | 113/128 [00:03<00:00, 35.50it/s, est. speed input: 17605.84 toks/s, output: 34.39 toks/s]
Processed prompts:  91%|█████████▏| 117/128 [00:03<00:00, 35.51it/s, est. speed input: 17625.23 toks/s, output: 34.42 toks/s]
Processed prompts:  95%|█████████▍| 121/128 [00:03<00:00, 35.56it/s, est. speed input: 17645.58 toks/s, output: 34.46 toks/s]
Processed prompts:  98%|█████████▊| 125/128 [00:03<00:00, 35.61it/s, est. speed input: 17665.50 toks/s, output: 34.50 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 35.61it/s, est. speed input: 17678.79 toks/s, output: 34.53 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 34.53it/s, est. speed input: 17678.79 toks/s, output: 34.53 toks/s]
[rank0]:[W128 14:09:42.941342709 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 40.7s

测试结果:
  Requests/s:   33.01
  Tokens/s:     16936.19
  Total Reqs:   128
  Elapsed:      3.88s

  [Prefill 分析]
  Total Prefill Tokens: 65536
  Prefill Tokens/s:     16903.18

============================================================
[2/7] 测试 M=1024
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 1024
│   M_prefill     = 1024 (= 1 x 1024)
│   M_decode      = 1
│   batched_tokens = 1025 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 128
│   --max-num-seqs           = 1
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 1025
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 14:09:51 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3286125) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3286125) WARNING 01-28 14:10:07 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=3286125) WARNING 01-28 14:10:17 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 32.66 requests/s, 33473.92 total tokens/s, 32.66 output tokens/s
Total num prompt tokens:  131072
Total num output tokens:  128


─── STDERR ───
[2026-01-28 14:09:51] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 14:09:51] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 14:09:51] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 14:09:51] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:09:51] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:09:51] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:09:51] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:09:51] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:09:51] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 14:09:51] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 14:09:51] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 14:09:51] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 14:09:51] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 14:09:51] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 14:09:57] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 14:09:58] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 14:09:58] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 14:09:58] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:09:58] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:09:58] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:09:58] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:09:58] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:09:58] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 14:09:58] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 14:09:58] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 14:09:58] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 14:09:58] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 14:09:58] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3286125) [2026-01-28 14:09:59] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3286125) [2026-01-28 14:09:59] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3286125) [2026-01-28 14:09:59] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3286125) [2026-01-28 14:09:59] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3286125) [2026-01-28 14:09:59] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3286125) [2026-01-28 14:09:59] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3286125) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3286125) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.51it/s]
(EngineCore_DP0 pid=3286125) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.51it/s]
(EngineCore_DP0 pid=3286125) 
(EngineCore_DP0 pid=3286125) [2026-01-28 14:10:00] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=3286125) [2026-01-28 14:10:00] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9830400 bytes
(EngineCore_DP0 pid=3286125) [2026-01-28 14:10:00] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=3286125) [2026-01-28 14:10:00] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6553600 bytes
(EngineCore_DP0 pid=3286125) [2026-01-28 14:10:00] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=3286125) [2026-01-28 14:10:00] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 35389440 bytes
(EngineCore_DP0 pid=3286125) [2026-01-28 14:10:00] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=3286125) [2026-01-28 14:10:00] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 17735680 bytes
(EngineCore_DP0 pid=3286125) 2026-01-28 14:10:17,461 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3286125) 2026-01-28 14:10:17,487 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3286125) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 10.74it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 2/2 [00:00<00:00, 10.73it/s]
(EngineCore_DP0 pid=3286125) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/1 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 1/1 [00:00<00:00, 10.66it/s]

Adding requests:   0%|          | 0/128 [00:00<?, ?it/s]
Adding requests:  32%|███▏      | 41/128 [00:00<00:00, 401.45it/s]
Adding requests:  67%|██████▋   | 86/128 [00:00<00:00, 426.47it/s]
Adding requests: 100%|██████████| 128/128 [00:00<00:00, 427.55it/s]

Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:   4%|▍         | 5/128 [00:00<00:02, 44.73it/s, est. speed input: 45804.97 toks/s, output: 44.73 toks/s]
Processed prompts:   8%|▊         | 10/128 [00:00<00:03, 38.58it/s, est. speed input: 40334.39 toks/s, output: 39.39 toks/s]
Processed prompts:  11%|█         | 14/128 [00:00<00:03, 37.19it/s, est. speed input: 39042.93 toks/s, output: 38.13 toks/s]
Processed prompts:  14%|█▍        | 18/128 [00:00<00:03, 36.40it/s, est. speed input: 38310.87 toks/s, output: 37.41 toks/s]
Processed prompts:  17%|█▋        | 22/128 [00:00<00:02, 35.96it/s, est. speed input: 37868.09 toks/s, output: 36.98 toks/s]
Processed prompts:  20%|██        | 26/128 [00:00<00:02, 35.76it/s, est. speed input: 37599.42 toks/s, output: 36.72 toks/s]
Processed prompts:  23%|██▎       | 30/128 [00:00<00:02, 35.54it/s, est. speed input: 37368.45 toks/s, output: 36.49 toks/s]
Processed prompts:  27%|██▋       | 34/128 [00:00<00:02, 35.45it/s, est. speed input: 37215.22 toks/s, output: 36.34 toks/s]
Processed prompts:  30%|██▉       | 38/128 [00:01<00:02, 35.37it/s, est. speed input: 37085.42 toks/s, output: 36.22 toks/s]
Processed prompts:  33%|███▎      | 42/128 [00:01<00:02, 35.29it/s, est. speed input: 36974.72 toks/s, output: 36.11 toks/s]
Processed prompts:  36%|███▌      | 46/128 [00:01<00:02, 35.27it/s, est. speed input: 36894.99 toks/s, output: 36.03 toks/s]
Processed prompts:  39%|███▉      | 50/128 [00:01<00:02, 35.22it/s, est. speed input: 36818.00 toks/s, output: 35.95 toks/s]
Processed prompts:  42%|████▏     | 54/128 [00:01<00:02, 35.21it/s, est. speed input: 36757.52 toks/s, output: 35.90 toks/s]
Processed prompts:  45%|████▌     | 58/128 [00:01<00:02, 34.75it/s, est. speed input: 36595.55 toks/s, output: 35.74 toks/s]
Processed prompts:  48%|████▊     | 62/128 [00:01<00:01, 34.88it/s, est. speed input: 36557.72 toks/s, output: 35.70 toks/s]
Processed prompts:  52%|█████▏    | 66/128 [00:01<00:01, 34.99it/s, est. speed input: 36530.13 toks/s, output: 35.67 toks/s]
Processed prompts:  55%|█████▍    | 70/128 [00:01<00:01, 35.03it/s, est. speed input: 36496.27 toks/s, output: 35.64 toks/s]
Processed prompts:  58%|█████▊    | 74/128 [00:02<00:01, 35.05it/s, est. speed input: 36467.04 toks/s, output: 35.61 toks/s]
Processed prompts:  61%|██████    | 78/128 [00:02<00:01, 34.86it/s, est. speed input: 36402.21 toks/s, output: 35.55 toks/s]
Processed prompts:  64%|██████▍   | 82/128 [00:02<00:01, 34.95it/s, est. speed input: 36383.37 toks/s, output: 35.53 toks/s]
Processed prompts:  67%|██████▋   | 86/128 [00:02<00:01, 35.00it/s, est. speed input: 36362.74 toks/s, output: 35.51 toks/s]
Processed prompts:  70%|███████   | 90/128 [00:02<00:01, 35.09it/s, est. speed input: 36353.71 toks/s, output: 35.50 toks/s]
Processed prompts:  73%|███████▎  | 94/128 [00:02<00:00, 35.02it/s, est. speed input: 36324.53 toks/s, output: 35.47 toks/s]
Processed prompts:  77%|███████▋  | 98/128 [00:02<00:00, 35.10it/s, est. speed input: 36317.04 toks/s, output: 35.47 toks/s]
Processed prompts:  80%|███████▉  | 102/128 [00:02<00:00, 35.15it/s, est. speed input: 36309.58 toks/s, output: 35.46 toks/s]
Processed prompts:  83%|████████▎ | 106/128 [00:02<00:00, 35.12it/s, est. speed input: 36293.10 toks/s, output: 35.44 toks/s]
Processed prompts:  86%|████████▌ | 110/128 [00:03<00:00, 35.15it/s, est. speed input: 36284.86 toks/s, output: 35.43 toks/s]
Processed prompts:  89%|████████▉ | 114/128 [00:03<00:00, 35.17it/s, est. speed input: 36277.54 toks/s, output: 35.43 toks/s]
Processed prompts:  92%|█████████▏| 118/128 [00:03<00:00, 35.20it/s, est. speed input: 36272.00 toks/s, output: 35.42 toks/s]
Processed prompts:  95%|█████████▌| 122/128 [00:03<00:00, 35.23it/s, est. speed input: 36268.03 toks/s, output: 35.42 toks/s]
Processed prompts:  98%|█████████▊| 126/128 [00:03<00:00, 34.85it/s, est. speed input: 36219.31 toks/s, output: 35.37 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 34.85it/s, est. speed input: 36217.09 toks/s, output: 35.37 toks/s]
Processed prompts: 100%|██████████| 128/128 [00:03<00:00, 35.37it/s, est. speed input: 36217.09 toks/s, output: 35.37 toks/s]
[rank0]:[W128 14:10:23.460162411 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 40.6s

测试结果:
  Requests/s:   32.66
  Tokens/s:     33473.92
  Total Reqs:   128
  Elapsed:      3.92s

  [Prefill 分析]
  Total Prefill Tokens: 131072
  Prefill Tokens/s:     33441.26

============================================================
[3/7] 测试 M=2048
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 2048
│   M_prefill     = 2048 (= 2 x 1024)
│   M_decode      = 2
│   batched_tokens = 2048 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 256
│   --max-num-seqs           = 2
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 2048
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 14:10:32 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3287262) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3287262) WARNING 01-28 14:10:48 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=3287262) WARNING 01-28 14:10:58 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 63.83 requests/s, 65421.11 total tokens/s, 63.83 output tokens/s
Total num prompt tokens:  262144
Total num output tokens:  256


─── STDERR ───
[2026-01-28 14:10:32] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 14:10:32] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 14:10:32] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 14:10:32] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:10:32] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:10:32] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:10:32] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:10:32] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:10:32] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 14:10:32] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 14:10:32] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 14:10:32] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 14:10:32] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 14:10:32] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 14:10:39] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 14:10:39] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 14:10:39] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 14:10:39] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:10:39] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:10:39] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:10:39] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:10:39] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:10:39] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 14:10:39] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 14:10:39] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 14:10:39] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 14:10:39] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 14:10:39] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3287262) [2026-01-28 14:10:40] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3287262) [2026-01-28 14:10:40] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3287262) [2026-01-28 14:10:40] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3287262) [2026-01-28 14:10:40] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3287262) [2026-01-28 14:10:40] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3287262) [2026-01-28 14:10:40] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3287262) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3287262) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.49it/s]
(EngineCore_DP0 pid=3287262) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.49it/s]
(EngineCore_DP0 pid=3287262) 
(EngineCore_DP0 pid=3287262) [2026-01-28 14:10:41] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=3287262) [2026-01-28 14:10:41] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9830400 bytes
(EngineCore_DP0 pid=3287262) [2026-01-28 14:10:41] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=3287262) [2026-01-28 14:10:41] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6553600 bytes
(EngineCore_DP0 pid=3287262) [2026-01-28 14:10:41] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=3287262) [2026-01-28 14:10:41] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 35389440 bytes
(EngineCore_DP0 pid=3287262) [2026-01-28 14:10:41] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=3287262) [2026-01-28 14:10:41] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 17735680 bytes
(EngineCore_DP0 pid=3287262) 2026-01-28 14:10:58,108 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3287262) 2026-01-28 14:10:58,135 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3287262) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|██████▋   | 2/3 [00:00<00:00, 14.98it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 3/3 [00:00<00:00, 13.36it/s]
(EngineCore_DP0 pid=3287262) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/2 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 15.69it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 2/2 [00:00<00:00, 15.67it/s]

Adding requests:   0%|          | 0/256 [00:00<?, ?it/s]
Adding requests:   0%|          | 1/256 [00:00<00:51,  5.00it/s]
Adding requests:  16%|█▋        | 42/256 [00:00<00:01, 172.57it/s]
Adding requests:  34%|███▎      | 86/256 [00:00<00:00, 271.56it/s]
Adding requests:  51%|█████     | 131/256 [00:00<00:00, 332.11it/s]
Adding requests:  68%|██████▊   | 175/256 [00:00<00:00, 366.36it/s]
Adding requests:  86%|████████▋ | 221/256 [00:00<00:00, 395.07it/s]
Adding requests: 100%|██████████| 256/256 [00:00<00:00, 328.69it/s]

Processed prompts:   0%|          | 0/256 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  16%|█▌        | 40/256 [00:00<00:00, 357.71it/s, est. speed input: 366319.45 toks/s, output: 357.72 toks/s]
Processed prompts:  30%|██▉       | 76/256 [00:00<00:01, 107.45it/s, est. speed input: 123697.68 toks/s, output: 120.80 toks/s]
Processed prompts:  37%|███▋      | 95/256 [00:00<00:01, 94.26it/s, est. speed input: 109417.53 toks/s, output: 106.85 toks/s] 
Processed prompts:  43%|████▎     | 109/256 [00:01<00:01, 86.37it/s, est. speed input: 101985.93 toks/s, output: 99.60 toks/s]
Processed prompts:  47%|████▋     | 120/256 [00:01<00:01, 80.31it/s, est. speed input: 96964.99 toks/s, output: 94.69 toks/s] 
Processed prompts:  51%|█████     | 130/256 [00:01<00:01, 77.71it/s, est. speed input: 94336.10 toks/s, output: 92.12 toks/s]
Processed prompts:  54%|█████▍    | 139/256 [00:01<00:01, 77.78it/s, est. speed input: 93242.97 toks/s, output: 91.06 toks/s]
Processed prompts:  58%|█████▊    | 148/256 [00:01<00:01, 73.56it/s, est. speed input: 90737.11 toks/s, output: 88.61 toks/s]
Processed prompts:  61%|██████    | 156/256 [00:01<00:01, 72.26it/s, est. speed input: 89366.68 toks/s, output: 87.27 toks/s]
Processed prompts:  64%|██████▍   | 164/256 [00:01<00:01, 71.56it/s, est. speed input: 88268.59 toks/s, output: 86.20 toks/s]
Processed prompts:  67%|██████▋   | 172/256 [00:02<00:01, 71.06it/s, est. speed input: 87306.57 toks/s, output: 85.26 toks/s]
Processed prompts:  70%|███████   | 180/256 [00:02<00:01, 70.69it/s, est. speed input: 86450.55 toks/s, output: 84.42 toks/s]
Processed prompts:  73%|███████▎  | 188/256 [00:02<00:00, 70.29it/s, est. speed input: 85654.03 toks/s, output: 83.65 toks/s]
Processed prompts:  77%|███████▋  | 196/256 [00:02<00:00, 69.53it/s, est. speed input: 84838.60 toks/s, output: 82.85 toks/s]
Processed prompts:  80%|███████▉  | 204/256 [00:02<00:00, 69.54it/s, est. speed input: 84207.48 toks/s, output: 82.23 toks/s]
Processed prompts:  83%|████████▎ | 212/256 [00:02<00:00, 69.53it/s, est. speed input: 83629.53 toks/s, output: 81.67 toks/s]
Processed prompts:  86%|████████▌ | 220/256 [00:02<00:00, 69.44it/s, est. speed input: 83087.10 toks/s, output: 81.14 toks/s]
Processed prompts:  89%|████████▉ | 228/256 [00:02<00:00, 69.40it/s, est. speed input: 82591.64 toks/s, output: 80.66 toks/s]
Processed prompts:  92%|█████████▏| 236/256 [00:02<00:00, 69.03it/s, est. speed input: 82082.70 toks/s, output: 80.16 toks/s]
Processed prompts:  95%|█████████▌| 244/256 [00:03<00:00, 69.23it/s, est. speed input: 81681.30 toks/s, output: 79.77 toks/s]
Processed prompts:  98%|█████████▊| 252/256 [00:03<00:00, 69.39it/s, est. speed input: 81310.36 toks/s, output: 79.40 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 69.39it/s, est. speed input: 81132.36 toks/s, output: 79.23 toks/s]
Processed prompts: 100%|██████████| 256/256 [00:03<00:00, 79.23it/s, est. speed input: 81132.36 toks/s, output: 79.23 toks/s]
[rank0]:[W128 14:11:04.337392897 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 40.8s

测试结果:
  Requests/s:   63.83
  Tokens/s:     65421.11
  Total Reqs:   256
  Elapsed:      4.01s

  [Prefill 分析]
  Total Prefill Tokens: 262144
  Prefill Tokens/s:     65357.28

============================================================
[4/7] 测试 M=4096
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 4096
│   M_prefill     = 4096 (= 4 x 1024)
│   M_decode      = 4
│   batched_tokens = 4096 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 512
│   --max-num-seqs           = 4
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 4096
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 14:11:14 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3288402) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3288402) WARNING 01-28 14:11:30 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=3288402) WARNING 01-28 14:11:39 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 131.72 requests/s, 135015.78 total tokens/s, 131.72 output tokens/s
Total num prompt tokens:  524288
Total num output tokens:  512


─── STDERR ───
[2026-01-28 14:11:14] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 14:11:14] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 14:11:14] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 14:11:14] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:11:14] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:11:14] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:11:14] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:11:14] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:11:14] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 14:11:14] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 14:11:14] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 14:11:14] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 14:11:14] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 14:11:14] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 14:11:20] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 14:11:21] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 14:11:21] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 14:11:21] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:11:21] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:11:21] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:11:21] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:11:21] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:11:21] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 14:11:21] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 14:11:21] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 14:11:21] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 14:11:21] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 14:11:21] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3288402) [2026-01-28 14:11:22] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3288402) [2026-01-28 14:11:22] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3288402) [2026-01-28 14:11:22] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3288402) [2026-01-28 14:11:22] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3288402) [2026-01-28 14:11:22] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3288402) [2026-01-28 14:11:22] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3288402) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3288402) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.49it/s]
(EngineCore_DP0 pid=3288402) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.49it/s]
(EngineCore_DP0 pid=3288402) 
(EngineCore_DP0 pid=3288402) [2026-01-28 14:11:23] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=3288402) [2026-01-28 14:11:23] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9830400 bytes
(EngineCore_DP0 pid=3288402) [2026-01-28 14:11:23] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=3288402) [2026-01-28 14:11:23] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6553600 bytes
(EngineCore_DP0 pid=3288402) [2026-01-28 14:11:23] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=3288402) [2026-01-28 14:11:23] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 35389440 bytes
(EngineCore_DP0 pid=3288402) [2026-01-28 14:11:23] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=3288402) [2026-01-28 14:11:23] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 17735680 bytes
(EngineCore_DP0 pid=3288402) 2026-01-28 14:11:39,939 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3288402) 2026-01-28 14:11:39,965 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3288402) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  50%|█████     | 2/4 [00:00<00:00, 15.20it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 13.93it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 4/4 [00:00<00:00, 14.10it/s]
(EngineCore_DP0 pid=3288402) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/3 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  67%|██████▋   | 2/3 [00:00<00:00, 15.89it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 3/3 [00:00<00:00, 15.34it/s]

Adding requests:   0%|          | 0/512 [00:00<?, ?it/s]
Adding requests:   8%|▊         | 40/512 [00:00<00:01, 397.70it/s]
Adding requests:  17%|█▋        | 85/512 [00:00<00:01, 422.66it/s]
Adding requests:  25%|██▌       | 130/512 [00:00<00:00, 433.26it/s]
Adding requests:  34%|███▍      | 174/512 [00:00<00:00, 433.47it/s]
Adding requests:  43%|████▎     | 220/512 [00:00<00:00, 441.29it/s]
Adding requests:  52%|█████▏    | 268/512 [00:00<00:00, 451.82it/s]
Adding requests:  61%|██████▏   | 314/512 [00:00<00:00, 451.00it/s]
Adding requests:  70%|███████   | 360/512 [00:00<00:00, 452.66it/s]
Adding requests:  80%|███████▉  | 408/512 [00:00<00:00, 459.09it/s]
Adding requests:  89%|████████▉ | 455/512 [00:01<00:00, 460.58it/s]
Adding requests:  98%|█████████▊| 502/512 [00:01<00:00, 460.53it/s]
Adding requests: 100%|██████████| 512/512 [00:01<00:00, 450.08it/s]

Processed prompts:   0%|          | 0/512 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  29%|██▊       | 146/512 [00:00<00:00, 1140.40it/s, est. speed input: 1167827.96 toks/s, output: 1140.41 toks/s]
Processed prompts:  51%|█████     | 261/512 [00:00<00:01, 244.92it/s, est. speed input: 288860.29 toks/s, output: 282.09 toks/s]   
Processed prompts:  62%|██████▏   | 317/512 [00:01<00:00, 204.14it/s, est. speed input: 245257.33 toks/s, output: 239.51 toks/s]
Processed prompts:  69%|██████▉   | 354/512 [00:01<00:00, 180.13it/s, est. speed input: 223207.81 toks/s, output: 217.98 toks/s]
Processed prompts:  74%|███████▍  | 381/512 [00:01<00:00, 175.97it/s, est. speed input: 217440.14 toks/s, output: 212.34 toks/s]
Processed prompts:  79%|███████▉  | 404/512 [00:01<00:00, 167.09it/s, est. speed input: 210571.67 toks/s, output: 205.64 toks/s]
Processed prompts:  83%|████████▎ | 424/512 [00:02<00:00, 161.73it/s, est. speed input: 206108.81 toks/s, output: 201.28 toks/s]
Processed prompts:  86%|████████▋ | 442/512 [00:02<00:00, 153.70it/s, est. speed input: 201250.30 toks/s, output: 196.53 toks/s]
Processed prompts:  90%|████████▉ | 459/512 [00:02<00:00, 150.52it/s, est. speed input: 198218.92 toks/s, output: 193.57 toks/s]
Processed prompts:  93%|█████████▎| 475/512 [00:02<00:00, 148.25it/s, est. speed input: 195724.69 toks/s, output: 191.14 toks/s]
Processed prompts:  96%|█████████▌| 491/512 [00:02<00:00, 146.60it/s, est. speed input: 193506.58 toks/s, output: 188.97 toks/s]
Processed prompts:  99%|█████████▉| 506/512 [00:02<00:00, 142.98it/s, est. speed input: 191094.22 toks/s, output: 186.62 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:02<00:00, 142.98it/s, est. speed input: 190771.32 toks/s, output: 186.30 toks/s]
Processed prompts: 100%|██████████| 512/512 [00:02<00:00, 186.29it/s, est. speed input: 190771.32 toks/s, output: 186.30 toks/s]
[rank0]:[W128 14:11:46.236968145 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 41.9s

测试结果:
  Requests/s:   131.72
  Tokens/s:     135015.78
  Total Reqs:   512
  Elapsed:      3.89s

  [Prefill 分析]
  Total Prefill Tokens: 524288
  Prefill Tokens/s:     134884.05

============================================================
[5/7] 测试 M=8192
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 8192
│   M_prefill     = 8192 (= 8 x 1024)
│   M_decode      = 8
│   batched_tokens = 8192 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 1024
│   --max-num-seqs           = 8
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 8192
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 14:11:58 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3289556) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3289556) WARNING 01-28 14:12:14 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=3289556) WARNING 01-28 14:12:24 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 231.04 requests/s, 236810.98 total tokens/s, 231.04 output tokens/s
Total num prompt tokens:  1048576
Total num output tokens:  1024


─── STDERR ───
[2026-01-28 14:11:58] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 14:11:58] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 14:11:58] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 14:11:58] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:11:58] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:11:58] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:11:58] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:11:58] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:11:58] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 14:11:58] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 14:11:58] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 14:11:58] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 14:11:58] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 14:11:58] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 14:12:05] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 14:12:05] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 14:12:05] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 14:12:05] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:12:05] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:12:05] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:12:05] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:12:05] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:12:05] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 14:12:05] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 14:12:05] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 14:12:05] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 14:12:05] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 14:12:05] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3289556) [2026-01-28 14:12:06] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3289556) [2026-01-28 14:12:06] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3289556) [2026-01-28 14:12:06] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3289556) [2026-01-28 14:12:06] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3289556) [2026-01-28 14:12:06] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3289556) [2026-01-28 14:12:06] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3289556) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3289556) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.50it/s]
(EngineCore_DP0 pid=3289556) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.50it/s]
(EngineCore_DP0 pid=3289556) 
(EngineCore_DP0 pid=3289556) [2026-01-28 14:12:07] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=3289556) [2026-01-28 14:12:07] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9830400 bytes
(EngineCore_DP0 pid=3289556) [2026-01-28 14:12:07] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=3289556) [2026-01-28 14:12:07] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6553600 bytes
(EngineCore_DP0 pid=3289556) [2026-01-28 14:12:07] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=3289556) [2026-01-28 14:12:07] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 35389440 bytes
(EngineCore_DP0 pid=3289556) [2026-01-28 14:12:07] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=3289556) [2026-01-28 14:12:07] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 17735680 bytes
(EngineCore_DP0 pid=3289556) 2026-01-28 14:12:24,049 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3289556) 2026-01-28 14:12:24,076 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3289556) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|████      | 2/5 [00:00<00:00, 12.52it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  80%|████████  | 4/5 [00:00<00:00, 14.48it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 13.39it/s]
(EngineCore_DP0 pid=3289556) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/4 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  50%|█████     | 2/4 [00:00<00:00, 16.13it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 16.25it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 4/4 [00:00<00:00, 16.22it/s]

Adding requests:   0%|          | 0/1024 [00:00<?, ?it/s]
Adding requests:   4%|▎         | 36/1024 [00:00<00:02, 359.05it/s]
Adding requests:   8%|▊         | 80/1024 [00:00<00:02, 405.30it/s]
Adding requests:  12%|█▏        | 125/1024 [00:00<00:02, 422.28it/s]
Adding requests:  17%|█▋        | 169/1024 [00:00<00:01, 428.44it/s]
Adding requests:  21%|██        | 213/1024 [00:00<00:01, 431.37it/s]
Adding requests:  25%|██▌       | 261/1024 [00:00<00:01, 446.60it/s]
Adding requests:  30%|██▉       | 306/1024 [00:00<00:01, 442.64it/s]
Adding requests:  34%|███▍      | 353/1024 [00:00<00:01, 450.32it/s]
Adding requests:  39%|███▉      | 400/1024 [00:00<00:01, 454.86it/s]
Adding requests:  44%|████▎     | 447/1024 [00:01<00:01, 457.91it/s]
Adding requests:  48%|████▊     | 494/1024 [00:01<00:01, 460.15it/s]
Adding requests:  53%|█████▎    | 541/1024 [00:01<00:01, 452.35it/s]
Adding requests:  58%|█████▊    | 590/1024 [00:01<00:00, 462.54it/s]
Adding requests:  62%|██████▏   | 637/1024 [00:01<00:00, 464.60it/s]
Adding requests:  67%|██████▋   | 686/1024 [00:01<00:00, 471.45it/s]
Adding requests:  72%|███████▏  | 735/1024 [00:01<00:00, 476.56it/s]
Adding requests:  76%|███████▋  | 783/1024 [00:01<00:00, 469.68it/s]
Adding requests:  81%|████████  | 831/1024 [00:01<00:00, 460.98it/s]
Adding requests:  86%|████████▌ | 879/1024 [00:01<00:00, 465.02it/s]
Adding requests:  91%|█████████ | 928/1024 [00:02<00:00, 470.56it/s]
Adding requests:  95%|█████████▌| 976/1024 [00:02<00:00, 472.10it/s]
Adding requests: 100%|██████████| 1024/1024 [00:02<00:00, 457.32it/s]

Processed prompts:   0%|          | 0/1024 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  51%|█████     | 522/1024 [00:00<00:00, 4218.67it/s, est. speed input: 4320193.25 toks/s, output: 4218.74 toks/s]
Processed prompts:  92%|█████████▏| 944/1024 [00:01<00:00, 433.27it/s, est. speed input: 521265.12 toks/s, output: 509.05 toks/s]   
Processed prompts: 100%|██████████| 1024/1024 [00:02<00:00, 433.27it/s, est. speed input: 478396.62 toks/s, output: 467.18 toks/s]
Processed prompts: 100%|██████████| 1024/1024 [00:02<00:00, 467.16it/s, est. speed input: 478396.62 toks/s, output: 467.18 toks/s]
[rank0]:[W128 14:12:30.206213654 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 45.0s

测试结果:
  Requests/s:   231.04
  Tokens/s:     236810.98
  Total Reqs:   1024
  Elapsed:      4.43s

  [Prefill 分析]
  Total Prefill Tokens: 1048576
  Prefill Tokens/s:     236579.95

============================================================
[6/7] 测试 M=16384
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 16384
│   M_prefill     = 16384 (= 16 x 1024)
│   M_decode      = 16
│   batched_tokens = 16384 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 2048
│   --max-num-seqs           = 16
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 16384
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 14:12:48 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3290788) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3290788) WARNING 01-28 14:13:04 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=3290788) WARNING 01-28 14:13:13 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 257.75 requests/s, 264197.64 total tokens/s, 257.75 output tokens/s
Total num prompt tokens:  2097152
Total num output tokens:  2048


─── STDERR ───
[2026-01-28 14:12:48] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 14:12:48] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 14:12:48] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 14:12:48] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:12:48] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:12:48] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:12:48] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:12:48] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:12:48] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 14:12:48] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 14:12:48] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 14:12:48] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 14:12:48] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 14:12:48] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 14:12:54] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 14:12:55] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 14:12:55] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 14:12:55] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:12:55] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:12:55] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:12:55] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:12:55] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:12:55] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 14:12:55] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 14:12:55] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 14:12:55] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 14:12:55] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 14:12:55] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3290788) [2026-01-28 14:12:56] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3290788) [2026-01-28 14:12:56] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3290788) [2026-01-28 14:12:56] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3290788) [2026-01-28 14:12:56] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3290788) [2026-01-28 14:12:56] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3290788) [2026-01-28 14:12:56] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3290788) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3290788) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.50it/s]
(EngineCore_DP0 pid=3290788) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.50it/s]
(EngineCore_DP0 pid=3290788) 
(EngineCore_DP0 pid=3290788) [2026-01-28 14:12:57] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=3290788) [2026-01-28 14:12:57] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9830400 bytes
(EngineCore_DP0 pid=3290788) [2026-01-28 14:12:57] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=3290788) [2026-01-28 14:12:57] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6553600 bytes
(EngineCore_DP0 pid=3290788) [2026-01-28 14:12:57] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=3290788) [2026-01-28 14:12:57] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 35389440 bytes
(EngineCore_DP0 pid=3290788) [2026-01-28 14:12:57] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=3290788) [2026-01-28 14:12:57] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 17735680 bytes
(EngineCore_DP0 pid=3290788) 2026-01-28 14:13:13,742 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3290788) 2026-01-28 14:13:13,769 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3290788) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  14%|█▍        | 1/7 [00:00<00:00,  7.91it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  43%|████▎     | 3/7 [00:00<00:00, 11.45it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  71%|███████▏  | 5/7 [00:00<00:00, 13.49it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 12.29it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 7/7 [00:00<00:00, 12.08it/s]
(EngineCore_DP0 pid=3290788) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/5 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  40%|████      | 2/5 [00:00<00:00, 15.87it/s]
Capturing CUDA graphs (decode, FULL):  80%|████████  | 4/5 [00:00<00:00, 16.27it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 5/5 [00:00<00:00, 16.24it/s]

Adding requests:   0%|          | 0/2048 [00:00<?, ?it/s]
Adding requests:   2%|▏         | 40/2048 [00:00<00:05, 399.79it/s]
Adding requests:   4%|▍         | 85/2048 [00:00<00:04, 424.23it/s]
Adding requests:   6%|▋         | 129/2048 [00:00<00:04, 430.39it/s]
Adding requests:   8%|▊         | 173/2048 [00:00<00:04, 433.32it/s]
Adding requests:  11%|█         | 219/2048 [00:00<00:04, 440.98it/s]
Adding requests:  13%|█▎        | 267/2048 [00:00<00:03, 451.44it/s]
Adding requests:  15%|█▌        | 313/2048 [00:00<00:03, 450.85it/s]
Adding requests:  18%|█▊        | 359/2048 [00:00<00:03, 435.82it/s]
Adding requests:  20%|█▉        | 407/2048 [00:00<00:03, 446.26it/s]
Adding requests:  22%|██▏       | 453/2048 [00:01<00:03, 450.17it/s]
Adding requests:  24%|██▍       | 500/2048 [00:01<00:03, 453.18it/s]
Adding requests:  27%|██▋       | 546/2048 [00:01<00:03, 450.58it/s]
Adding requests:  29%|██▉       | 595/2048 [00:01<00:03, 462.20it/s]
Adding requests:  31%|███▏      | 642/2048 [00:01<00:03, 459.95it/s]
Adding requests:  34%|███▎      | 691/2048 [00:01<00:02, 467.84it/s]
Adding requests:  36%|███▌      | 740/2048 [00:01<00:02, 469.95it/s]
Adding requests:  38%|███▊      | 788/2048 [00:01<00:02, 470.05it/s]
Adding requests:  41%|████      | 836/2048 [00:01<00:02, 459.26it/s]
Adding requests:  43%|████▎     | 885/2048 [00:01<00:02, 466.23it/s]
Adding requests:  46%|████▌     | 934/2048 [00:02<00:02, 471.92it/s]
Adding requests:  48%|████▊     | 982/2048 [00:02<00:02, 467.07it/s]
Adding requests:  50%|█████     | 1030/2048 [00:02<00:02, 468.78it/s]
Adding requests:  53%|█████▎    | 1077/2048 [00:02<00:02, 463.94it/s]
Adding requests:  55%|█████▍    | 1124/2048 [00:02<00:01, 462.74it/s]
Adding requests:  57%|█████▋    | 1173/2048 [00:02<00:01, 469.20it/s]
Adding requests:  60%|█████▉    | 1223/2048 [00:02<00:01, 475.91it/s]
Adding requests:  62%|██████▏   | 1271/2048 [00:02<00:01, 470.04it/s]
Adding requests:  64%|██████▍   | 1319/2048 [00:02<00:01, 472.71it/s]
Adding requests:  67%|██████▋   | 1367/2048 [00:02<00:01, 473.96it/s]
Adding requests:  69%|██████▉   | 1416/2048 [00:03<00:01, 475.65it/s]
Adding requests:  72%|███████▏  | 1465/2048 [00:03<00:01, 477.15it/s]
Adding requests:  74%|███████▍  | 1514/2048 [00:03<00:01, 479.47it/s]
Adding requests:  76%|███████▋  | 1562/2048 [00:03<00:01, 477.53it/s]
Adding requests:  79%|███████▊  | 1611/2048 [00:03<00:00, 479.91it/s]
Adding requests:  81%|████████  | 1659/2048 [00:03<00:00, 479.69it/s]
Adding requests:  83%|████████▎ | 1707/2048 [00:03<00:00, 467.34it/s]
Adding requests:  86%|████████▌ | 1755/2048 [00:03<00:00, 470.23it/s]
Adding requests:  88%|████████▊ | 1803/2048 [00:03<00:00, 469.06it/s]
Adding requests:  90%|█████████ | 1851/2048 [00:03<00:00, 471.35it/s]
Adding requests:  93%|█████████▎| 1899/2048 [00:04<00:00, 473.00it/s]
Adding requests:  95%|█████████▌| 1947/2048 [00:04<00:00, 473.46it/s]
Adding requests:  97%|█████████▋| 1995/2048 [00:04<00:00, 472.46it/s]
Adding requests: 100%|█████████▉| 2044/2048 [00:04<00:00, 476.06it/s]
Adding requests: 100%|██████████| 2048/2048 [00:04<00:00, 464.75it/s]

Processed prompts:   0%|          | 0/2048 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  56%|█████▌    | 1138/2048 [00:00<00:00, 9382.55it/s, est. speed input: 9608352.40 toks/s, output: 9382.75 toks/s]
Processed prompts: 100%|██████████| 2048/2048 [00:03<00:00, 9382.55it/s, est. speed input: 592850.29 toks/s, output: 578.95 toks/s]  
Processed prompts: 100%|██████████| 2048/2048 [00:03<00:00, 578.93it/s, est. speed input: 592850.29 toks/s, output: 578.95 toks/s] 
[rank0]:[W128 14:13:24.440038038 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 53.3s

测试结果:
  Requests/s:   257.75
  Tokens/s:     264197.64
  Total Reqs:   2048
  Elapsed:      7.95s

  [Prefill 分析]
  Total Prefill Tokens: 2097152
  Prefill Tokens/s:     263939.88

============================================================
[7/7] 测试 M=32768
============================================================

┌─────────────────────────────────────────────────────────────┐
│                    测试参数                                  │
├─────────────────────────────────────────────────────────────┤
│ 模型:     BitNet-2B-FP8                                   │
│ Backend:  cuSPARSELt (2:10)                               │
│ 阶段:     prefill                                         │
├─────────────────────────────────────────────────────────────┤
│ GEMM M 维度 (精确控制):
│   目标 M        = 32768
│   M_prefill     = 32768 (= 32 x 1024)
│   M_decode      = 32
│   batched_tokens = 32768 (控制 M 的关键参数)
├─────────────────────────────────────────────────────────────┤
│ vLLM 参数:
│   --input-len              = 1024
│   --output-len             = 1
│   --num-prompts            = 4096
│   --max-num-seqs           = 32
│   --max-model-len          = 1025
│   --max-num-batched-tokens = 32768
│   --no-enable-chunked-prefill
├─────────────────────────────────────────────────────────────┤
│ 迭代次数:
│   N_prefill = 128
│   N_decode  = 0
└─────────────────────────────────────────────────────────────┘

[INFO] 开始测试...

─── STDOUT ───
When dataset path is not set, it will default to random dataset
WARNING 01-28 14:13:51 [arg_utils.py:1869] This model does not officially support disabling chunked prefill. Disabling this manually may cause the engine to crash or produce incorrect outputs.
(EngineCore_DP0 pid=3292190) [INFO] Loading compress extension: cusparselt_compress_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3292190) WARNING 01-28 14:14:07 [backends.py:609] Failed to read file <frozen os>
(EngineCore_DP0 pid=3292190) WARNING 01-28 14:14:17 [flashinfer.py:363] Using TRTLLM prefill attention (auto-detected).
Throughput: 269.36 requests/s, 276094.47 total tokens/s, 269.36 output tokens/s
Total num prompt tokens:  4194304
Total num output tokens:  4096


─── STDERR ───
[2026-01-28 14:13:50] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 14:13:51] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 14:13:51] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 14:13:51] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:13:51] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:13:51] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:13:51] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:13:51] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:13:51] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 14:13:51] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 14:13:51] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 14:13:51] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 14:13:51] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 14:13:51] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
[2026-01-28 14:13:57] INFO kernels.py:578: Triton kernel custom ops registered
[2026-01-28 14:13:58] INFO kernels.py:627: Optimization enabled: Only preloading kernels for model 'BitNet-2B-FP8'
[2026-01-28 14:13:58] INFO kernels.py:109: Loaded tuned kernel for model: BitNet-2B-FP8
[2026-01-28 14:13:58] INFO kernels.py:155: Dequant+bias kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:13:58] INFO kernels.py:224: FP8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:13:58] INFO kernels.py:348: INT8 quant kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:13:58] INFO kernels.py:284: FP8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:13:58] INFO kernels.py:408: INT8 quant+slide kernel loaded for model: BitNet-2B-FP8
[2026-01-28 14:13:58] INFO gemm_wrapper.py:157: Optimization enabled: Only loading GEMM configs for model 'BitNet-2B-FP8'
[2026-01-28 14:13:58] INFO gemm_wrapper.py:172: Loaded algorithm configs: cuBLASLt=1, cuSPARSELt=1 models
[2026-01-28 14:13:58] INFO gemm_wrapper.py:944: cuBLASLt FP8 custom op registered: slidesparse::cublaslt_fp8_mm
[2026-01-28 14:13:58] INFO gemm_wrapper.py:1009: cuSPARSELt FP8 custom op registered: slidesparse::cusparselt_fp8_mm
[2026-01-28 14:13:58] INFO gemm_wrapper.py:1075: cuBLASLt INT8 custom op registered: slidesparse::cublaslt_int8_mm
[2026-01-28 14:13:58] INFO gemm_wrapper.py:1141: cuSPARSELt INT8 custom op registered: slidesparse::cusparselt_int8_mm
(EngineCore_DP0 pid=3292190) [2026-01-28 14:13:59] INFO SlideSparseLinearMethod_FP8.py:734: Wrapping CompressedTensorsW8A8Fp8 with SlideSparse (cuSPARSELt)
(EngineCore_DP0 pid=3292190) [2026-01-28 14:13:59] INFO gemm_wrapper.py:870: cusparselt GEMM extension loaded: cusparselt_gemm_B200_cc100_py312_cu129_x86_64.so
(EngineCore_DP0 pid=3292190) [2026-01-28 14:13:59] INFO SlideSparseLinearMethod_FP8.py:392: SlideSparseFp8LinearOp initialized (kernel=cuSPARSELt)
(EngineCore_DP0 pid=3292190) [2026-01-28 14:13:59] INFO SlideSparseLinearMethod_FP8.py:539: SlideSparseFp8LinearMethod: cuSPARSELt sparsity=2:10, expand_ratio=1.250
(EngineCore_DP0 pid=3292190) [2026-01-28 14:13:59] INFO SlideSparseLinearMethod_FP8.py:556: Preloaded FP8 Triton kernels for model: BitNet-2B-FP8
(EngineCore_DP0 pid=3292190) [2026-01-28 14:13:59] INFO SlideSparseLinearMethod_FP8.py:560: SlideSparseFp8LinearMethod initialized, kernel=cuSPARSELt
(EngineCore_DP0 pid=3292190) 
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
(EngineCore_DP0 pid=3292190) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.38it/s]
(EngineCore_DP0 pid=3292190) 
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.38it/s]
(EngineCore_DP0 pid=3292190) 
(EngineCore_DP0 pid=3292190) [2026-01-28 14:14:00] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [3840, 4096] -> 1D uint8
(EngineCore_DP0 pid=3292190) [2026-01-28 14:14:00] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 9830400 bytes
(EngineCore_DP0 pid=3292190) [2026-01-28 14:14:00] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 4096] -> 1D uint8
(EngineCore_DP0 pid=3292190) [2026-01-28 14:14:00] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 6553600 bytes
(EngineCore_DP0 pid=3292190) [2026-01-28 14:14:00] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [13824, 4096] -> 1D uint8
(EngineCore_DP0 pid=3292190) [2026-01-28 14:14:00] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 35389440 bytes
(EngineCore_DP0 pid=3292190) [2026-01-28 14:14:00] INFO SlideSparseLinearMethod_FP8.py:660: cuSPARSELt compression: [2560, 11072] -> 1D uint8
(EngineCore_DP0 pid=3292190) [2026-01-28 14:14:00] INFO SlideSparseLinearMethod_FP8.py:670: cuSPARSELt compression done: 17735680 bytes
(EngineCore_DP0 pid=3292190) [rank0]:W0128 14:14:12.101000 3292190 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3292190) [rank0]:W0128 14:14:12.182000 3292190 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3292190) [rank0]:W0128 14:14:13.258000 3292190 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3292190) [rank0]:W0128 14:14:13.386000 3292190 torch/_inductor/codegen/triton_combo_kernel.py:110] ComboKernels: 1 large pointwise nodes are separated
(EngineCore_DP0 pid=3292190) 2026-01-28 14:14:16,985 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
(EngineCore_DP0 pid=3292190) 2026-01-28 14:14:17,014 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
(EngineCore_DP0 pid=3292190) 
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/11 [00:00<?, ?it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|█▊        | 2/11 [00:00<00:00, 10.39it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|███▋      | 4/11 [00:00<00:00, 11.40it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  55%|█████▍    | 6/11 [00:00<00:00, 12.78it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  73%|███████▎  | 8/11 [00:00<00:00, 13.95it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  91%|█████████ | 10/11 [00:00<00:00, 14.65it/s]
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 11/11 [00:00<00:00, 13.28it/s]
(EngineCore_DP0 pid=3292190) 
Capturing CUDA graphs (decode, FULL):   0%|          | 0/7 [00:00<?, ?it/s]
Capturing CUDA graphs (decode, FULL):  29%|██▊       | 2/7 [00:00<00:00, 15.79it/s]
Capturing CUDA graphs (decode, FULL):  57%|█████▋    | 4/7 [00:00<00:00, 16.11it/s]
Capturing CUDA graphs (decode, FULL):  86%|████████▌ | 6/7 [00:00<00:00, 16.20it/s]
Capturing CUDA graphs (decode, FULL): 100%|██████████| 7/7 [00:00<00:00, 16.16it/s]

Adding requests:   0%|          | 0/4096 [00:00<?, ?it/s]
Adding requests:   1%|          | 40/4096 [00:00<00:10, 398.54it/s]
Adding requests:   2%|▏         | 84/4096 [00:00<00:09, 420.90it/s]
Adding requests:   3%|▎         | 129/4096 [00:00<00:09, 430.58it/s]
Adding requests:   4%|▍         | 174/4096 [00:00<00:09, 434.49it/s]
Adding requests:   5%|▌         | 220/4096 [00:00<00:08, 440.89it/s]
Adding requests:   7%|▋         | 268/4096 [00:00<00:08, 453.46it/s]
Adding requests:   8%|▊         | 314/4096 [00:00<00:08, 451.95it/s]
Adding requests:   9%|▉         | 360/4096 [00:00<00:08, 454.41it/s]
Adding requests:  10%|▉         | 407/4096 [00:00<00:08, 458.65it/s]
Adding requests:  11%|█         | 454/4096 [00:01<00:07, 461.16it/s]
Adding requests:  12%|█▏        | 501/4096 [00:01<00:07, 460.93it/s]
Adding requests:  13%|█▎        | 548/4096 [00:01<00:07, 457.39it/s]
Adding requests:  15%|█▍        | 597/4096 [00:01<00:07, 465.03it/s]
Adding requests:  16%|█▌        | 646/4096 [00:01<00:07, 469.95it/s]
Adding requests:  17%|█▋        | 695/4096 [00:01<00:07, 475.09it/s]
Adding requests:  18%|█▊        | 743/4096 [00:01<00:07, 474.80it/s]
Adding requests:  19%|█▉        | 791/4096 [00:01<00:06, 472.67it/s]
Adding requests:  20%|██        | 839/4096 [00:01<00:07, 455.70it/s]
Adding requests:  22%|██▏       | 886/4096 [00:01<00:07, 457.49it/s]
Adding requests:  23%|██▎       | 934/4096 [00:02<00:06, 462.34it/s]
Adding requests:  24%|██▍       | 982/4096 [00:02<00:06, 464.52it/s]
Adding requests:  25%|██▌       | 1031/4096 [00:02<00:06, 469.81it/s]
Adding requests:  26%|██▋       | 1079/4096 [00:02<00:06, 467.83it/s]
Adding requests:  27%|██▋       | 1126/4096 [00:02<00:06, 466.57it/s]
Adding requests:  29%|██▊       | 1176/4096 [00:02<00:06, 474.04it/s]
Adding requests:  30%|██▉       | 1226/4096 [00:02<00:05, 480.55it/s]
Adding requests:  31%|███       | 1275/4096 [00:02<00:05, 476.61it/s]
Adding requests:  32%|███▏      | 1324/4096 [00:02<00:05, 480.33it/s]
Adding requests:  34%|███▎      | 1373/4096 [00:02<00:05, 482.17it/s]
Adding requests:  35%|███▍      | 1422/4096 [00:03<00:05, 481.90it/s]
Adding requests:  36%|███▌      | 1471/4096 [00:03<00:05, 483.65it/s]
Adding requests:  37%|███▋      | 1520/4096 [00:03<00:05, 485.06it/s]
Adding requests:  38%|███▊      | 1569/4096 [00:03<00:05, 482.60it/s]
Adding requests:  40%|███▉      | 1618/4096 [00:03<00:05, 482.81it/s]
Adding requests:  41%|████      | 1667/4096 [00:03<00:05, 481.59it/s]
Adding requests:  42%|████▏     | 1717/4096 [00:03<00:04, 484.11it/s]
Adding requests:  43%|████▎     | 1766/4096 [00:03<00:04, 480.85it/s]
Adding requests:  44%|████▍     | 1816/4096 [00:03<00:04, 484.57it/s]
Adding requests:  46%|████▌     | 1865/4096 [00:03<00:04, 481.03it/s]
Adding requests:  47%|████▋     | 1914/4096 [00:04<00:04, 462.96it/s]
Adding requests:  48%|████▊     | 1961/4096 [00:04<00:04, 455.92it/s]
Adding requests:  49%|████▉     | 2011/4096 [00:04<00:04, 466.02it/s]
Adding requests:  50%|█████     | 2060/4096 [00:04<00:04, 471.65it/s]
Adding requests:  52%|█████▏    | 2110/4096 [00:04<00:04, 479.03it/s]
Adding requests:  53%|█████▎    | 2158/4096 [00:04<00:04, 469.42it/s]
Adding requests:  54%|█████▍    | 2206/4096 [00:04<00:04, 469.37it/s]
Adding requests:  55%|█████▌    | 2256/4096 [00:04<00:03, 477.39it/s]
Adding requests:  56%|█████▋    | 2305/4096 [00:04<00:03, 478.91it/s]
Adding requests:  57%|█████▋    | 2353/4096 [00:05<00:03, 477.73it/s]
Adding requests:  59%|█████▊    | 2401/4096 [00:05<00:03, 478.10it/s]
Adding requests:  60%|█████▉    | 2450/4096 [00:05<00:03, 479.73it/s]
Adding requests:  61%|██████    | 2500/4096 [00:05<00:03, 482.74it/s]
Adding requests:  62%|██████▏   | 2549/4096 [00:05<00:03, 483.70it/s]
Adding requests:  63%|██████▎   | 2598/4096 [00:05<00:03, 484.32it/s]
Adding requests:  65%|██████▍   | 2648/4096 [00:05<00:02, 486.30it/s]
Adding requests:  66%|██████▌   | 2697/4096 [00:05<00:02, 482.22it/s]
Adding requests:  67%|██████▋   | 2746/4096 [00:05<00:02, 482.09it/s]
Adding requests:  68%|██████▊   | 2795/4096 [00:05<00:02, 478.47it/s]
Adding requests:  69%|██████▉   | 2844/4096 [00:06<00:02, 480.32it/s]
Adding requests:  71%|███████   | 2893/4096 [00:06<00:02, 481.05it/s]
Adding requests:  72%|███████▏  | 2942/4096 [00:06<00:02, 473.43it/s]
Adding requests:  73%|███████▎  | 2991/4096 [00:06<00:02, 477.59it/s]
Adding requests:  74%|███████▍  | 3039/4096 [00:06<00:02, 476.72it/s]
Adding requests:  75%|███████▌  | 3087/4096 [00:06<00:02, 476.93it/s]
Adding requests:  77%|███████▋  | 3136/4096 [00:06<00:02, 477.90it/s]
Adding requests:  78%|███████▊  | 3184/4096 [00:06<00:01, 469.22it/s]
Adding requests:  79%|███████▉  | 3233/4096 [00:06<00:01, 473.82it/s]
Adding requests:  80%|████████  | 3283/4096 [00:06<00:01, 479.63it/s]
Adding requests:  81%|████████▏ | 3331/4096 [00:07<00:01, 477.68it/s]
Adding requests:  83%|████████▎ | 3381/4096 [00:07<00:01, 482.73it/s]
Adding requests:  84%|████████▎ | 3430/4096 [00:07<00:01, 484.53it/s]
Adding requests:  85%|████████▍ | 3479/4096 [00:07<00:01, 474.64it/s]
Adding requests:  86%|████████▌ | 3527/4096 [00:07<00:01, 475.55it/s]
Adding requests:  87%|████████▋ | 3576/4096 [00:07<00:01, 479.12it/s]
Adding requests:  88%|████████▊ | 3624/4096 [00:07<00:00, 473.20it/s]
Adding requests:  90%|████████▉ | 3672/4096 [00:07<00:00, 473.84it/s]
Adding requests:  91%|█████████ | 3720/4096 [00:07<00:00, 473.99it/s]
Adding requests:  92%|█████████▏| 3770/4096 [00:07<00:00, 479.82it/s]
Adding requests:  93%|█████████▎| 3820/4096 [00:08<00:00, 482.98it/s]
Adding requests:  95%|█████████▍| 3871/4096 [00:08<00:00, 490.14it/s]
Adding requests:  96%|█████████▌| 3921/4096 [00:08<00:00, 487.60it/s]
Adding requests:  97%|█████████▋| 3970/4096 [00:08<00:00, 486.53it/s]
Adding requests:  98%|█████████▊| 4019/4096 [00:08<00:00, 469.85it/s]
Adding requests:  99%|█████████▉| 4067/4096 [00:08<00:00, 466.95it/s]
Adding requests: 100%|██████████| 4096/4096 [00:08<00:00, 472.94it/s]

Processed prompts:   0%|          | 0/4096 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]
Processed prompts:  57%|█████▋    | 2325/4096 [00:00<00:00, 14385.10it/s, est. speed input: 14731141.61 toks/s, output: 14385.31 toks/s]
Processed prompts:  92%|█████████▏| 3764/4096 [00:05<00:00, 581.71it/s, est. speed input: 724494.06 toks/s, output: 707.51 toks/s]      
Processed prompts: 100%|██████████| 4096/4096 [00:06<00:00, 581.71it/s, est. speed input: 640963.33 toks/s, output: 625.94 toks/s]
Processed prompts: 100%|██████████| 4096/4096 [00:06<00:00, 625.93it/s, est. speed input: 640963.33 toks/s, output: 625.94 toks/s]
[rank0]:[W128 14:14:35.478573507 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

[SUCCESS] 测试完成! 耗时: 71.2s

测试结果:
  Requests/s:   269.36
  Tokens/s:     276094.47
  Total Reqs:   4096
  Elapsed:      15.21s

  [Prefill 分析]
  Total Prefill Tokens: 4194304
  Prefill Tokens/s:     275825.11


------------------------------------------------------------
  生成 CSV: BitNet-2B-FP8
------------------------------------------------------------
[SUCCESS] CSV 保存到: /root/vllmbench/slidesparse/tools/throughput_benchmark_results/prefill/B200_cc100_FP8E4M3_py312_cu129_x86_64/cusparselt/2_10/BitNet-2B-FP8_prefill.csv

预览:
------------------------------------------------------------
M_prefill,prompt_len,max_num_seqs,num_prompts,N_prefill,requests_per_s,tokens_per_s,elapsed_time_s
512,512,1,128,128,33.0140,16936.1909,3.8771
1024,1024,1,128,128,32.6575,33473.9171,3.9195
2048,1024,2,256,128,63.8255,65421.1093,4.0109
4096,1024,4,512,128,131.7227,135015.7754,3.8870
8192,1024,8,1024,128,231.0351,236810.9803,4.4322
16384,1024,16,2048,128,257.7538,264197.6385,7.9456
32768,1024,32,4096,128,269.3605,276094.4745,15.2064

------------------------------------------------------------

[INFO] 完成: 7 成功, 0 失败


============================================================
  Benchmark 完成!
============================================================


总计: 35 成功, 0 失败
============================================================
